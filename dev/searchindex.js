Search.setIndex({"alltitles": {"(One-Way) Cluster Robust Double Machine Learing": [[50, "(One-Way)-Cluster-Robust-Double-Machine-Learing"], [71, "(One-Way)-Cluster-Robust-Double-Machine-Learing"]], "0. Problem Formulation": [[117, "problem-formulation"]], "1. Data-Backend": [[117, "data-backend"]], "1. Formulation of Causal Model & Identification Assumptions": [[79, "1.-Formulation-of-Causal-Model-&-Identification-Assumptions"]], "2. Causal Model": [[117, "causal-model"]], "2. Estimation of Causal Effect": [[79, "2.-Estimation-of-Causal-Effect"]], "3. ML Methods": [[117, "ml-methods"]], "3. Sensitivity Analysis": [[79, "3.-Sensitivity-Analysis"]], "4. Benchmarking Analysis": [[79, "4.-Benchmarking-Analysis"]], "4. DML Specifications": [[117, "dml-specifications"]], "5. Conclusion": [[79, "5.-Conclusion"]], "5. Estimation": [[117, "estimation"]], "6. Inference": [[117, "inference"]], "7. Sensitivity Analysis": [[117, "sensitivity-analysis"]], "A Motivating Example: Two-Way Cluster Robust DML": [[50, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"], [71, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"]], "API Reference": [[0, null]], "ATE Estimation and Sensitivity": [[67, "ATE-Estimation-and-Sensitivity"]], "ATE estimates distribution": [[53, "ATE-estimates-distribution"], [53, "id3"], [80, "ATE-estimates-distribution"], [80, "id3"]], "ATTE Estimation": [[62, "ATTE-Estimation"], [62, "id2"]], "Acknowledgements": [[112, "acknowledgements"]], "Acknowledgements and Final Remarks": [[49, "Acknowledgements-and-Final-Remarks"]], "Additional Results: CATE estimates": [[74, "Additional-Results:-CATE-estimates"]], "Advanced: External Predictions": [[86, "advanced-external-predictions"]], "Advanced: Global and Local Learners, Stacked Ensembles": [[77, "Advanced:-Global-and-Local-Learners,-Stacked-Ensembles"]], "Algorithm DML1": [[81, "algorithm-dml1"]], "Algorithm DML2": [[81, "algorithm-dml2"]], "Application Results": [[50, "Application-Results"], [71, "Application-Results"]], "Application: 401(k)": [[78, "Application:-401(k)"]], "AutoML with less Computation time": [[70, "AutoML-with-less-Computation-time"]], "Average Potential Outcome (APOs)": [[56, "Average-Potential-Outcome-(APOs)"]], "Average Potential Outcomes (APOs)": [[87, "average-potential-outcomes-apos"], [89, "average-potential-outcomes-apos"], [103, "average-potential-outcomes-apos"]], "Average Potential Outcomes (APOs) for Multiple Treatment Levels": [[87, "average-potential-outcomes-apos-for-multiple-treatment-levels"]], "Average Treatment Effect": [[68, "Average-Treatment-Effect"]], "Average Treatment Effect on the Treated": [[68, "Average-Treatment-Effect-on-the-Treated"]], "Benchmarking": [[103, "benchmarking"]], "Benchmarking Analysis": [[78, "Benchmarking-Analysis"]], "Binary Interactive Regression Model (IRM)": [[87, "binary-interactive-regression-model-irm"], [89, "binary-interactive-regression-model-irm"]], "CATEs for IRM models": [[85, "cates-for-irm-models"]], "CATEs for PLR models": [[85, "cates-for-plr-models"]], "CVaR Treatment Effects": [[61, "CVaR-Treatment-Effects"]], "CVaR of potential outcomes": [[85, "cvar-of-potential-outcomes"]], "CVaR treatment effects": [[85, "cvar-treatment-effects"]], "Causal Analysis with DoubleML": [[79, "Causal-Analysis-with-DoubleML"]], "Causal Contrasts": [[56, "Causal-Contrasts"]], "Causal estimation vs. lasso penalty \\lambda": [[64, "Causal-estimation-vs.-lasso-penalty-\\lambda"]], "Chernozhukov et al. (2023): Benchmarking": [[79, "Chernozhukov-et-al.-(2023):-Benchmarking"]], "Citation": [[112, "citation"]], "Cluster Robust Cross Fitting": [[50, "Cluster-Robust-Cross-Fitting"], [71, "Cluster-Robust-Cross-Fitting"]], "Cluster Robust Standard Errors": [[50, "Cluster-Robust-Standard-Errors"], [71, "Cluster-Robust-Standard-Errors"]], "Clustering and double machine learning": [[50, "Clustering-and-double-machine-learning"], [71, "Clustering-and-double-machine-learning"]], "Combined loss vs. lasso penalty \\lambda": [[64, "Combined-loss-vs.-lasso-penalty-\\lambda"]], "Compare Metrics for Nuisance Estimation": [[70, "Compare-Metrics-for-Nuisance-Estimation"]], "Comparing different learners": [[69, "Comparing-different-learners"]], "Comparison and summary": [[70, "Comparison-and-summary"]], "Comparison to AutoML with less Computation time and Untuned XGBoost Learners": [[70, "Comparison-to-AutoML-with-less-Computation-time-and-Untuned-XGBoost-Learners"]], "Comparison to did package": [[49, "Comparison-to-did-package"]], "Computation time": [[69, "Computation-time"]], "Conclusion": [[70, "Conclusion"]], "Conditional Value at Risk (CVaR)": [[61, "Conditional-Value-at-Risk-(CVaR)"]], "Conditional average treatment effects (CATEs)": [[85, "conditional-average-treatment-effects-cates"]], "Conditional value at risk (CVaR)": [[85, "conditional-value-at-risk-cvar"], [89, "conditional-value-at-risk-cvar"]], "Confidence bands and multiplier bootstrap for valid simultaneous inference": [[102, "confidence-bands-and-multiplier-bootstrap-for-valid-simultaneous-inference"]], "Coverage Simulation": [[62, "Coverage-Simulation"], [62, "id3"]], "Cross-fitting with K folds": [[88, "cross-fitting-with-k-folds"]], "Cross-fitting, DML algorithms and Neyman-orthogonal score functions": [[114, "cross-fitting-dml-algorithms-and-neyman-orthogonal-score-functions"]], "Custom evaluation metrics": [[69, "Custom-evaluation-metrics"]], "DML: Bonus Data": [[54, null]], "Data": [[51, "Data"], [53, "Data"], [53, "id1"], [59, "Data"], [60, "Data"], [61, "Data"], [62, "Data"], [62, "id1"], [65, "Data"], [66, "Data"], [67, "Data"], [68, "Data"], [72, "Data"], [73, "Data"], [75, "Data"], [76, "Data"], [76, "id1"], [78, "Data"], [80, "Data"], [80, "id1"], [114, "data"]], "Data Generating Process (DGP)": [[48, "Data-Generating-Process-(DGP)"], [56, "Data-Generating-Process-(DGP)"], [58, "Data-Generating-Process-(DGP)"]], "Data Generation": [[70, "Data-Generation"]], "Data Simulation": [[47, "Data-Simulation"], [57, "Data-Simulation"]], "Data and Effect Estimation": [[78, "Data-and-Effect-Estimation"]], "Data generating process": [[82, "data-generating-process"]], "Data preprocessing": [[52, "Data-preprocessing"]], "Data-Backend for Cluster Data": [[50, "Data-Backend-for-Cluster-Data"], [71, "Data-Backend-for-Cluster-Data"]], "Dataset Generators": [[2, "dataset-generators"]], "Dataset Loaders": [[2, "dataset-loaders"]], "Datasets": [[2, null]], "Define Helper Functions for Plotting": [[50, "Define-Helper-Functions-for-Plotting"], [71, "Define-Helper-Functions-for-Plotting"]], "Demo Example from did": [[49, "Demo-Example-from-did"]], "Details on Predictive Performance": [[49, "Details-on-Predictive-Performance"]], "Difference-in-Differences Models": [[89, "difference-in-differences-models"]], "Difference-in-Differences Models (DID)": [[87, "difference-in-differences-models-did"]], "Difference-in-Differences for Panel Data": [[103, "difference-in-differences-for-panel-data"]], "Difference-in-Differences for repeated cross-sections": [[103, "difference-in-differences-for-repeated-cross-sections"]], "Disclaimer": [[79, "Disclaimer"]], "Double Machine Learning Algorithm": [[112, "double-machine-learning-algorithm"]], "Double Machine Learning Literature": [[115, null]], "Double machine learning algorithms": [[81, null]], "Double/debiased machine learning": [[48, "Double/debiased-machine-learning"], [58, "Double/debiased-machine-learning"], [82, "double-debiased-machine-learning"]], "DoubleML": [[112, null]], "DoubleML Data Class": [[1, null]], "DoubleML Models": [[3, null]], "DoubleML Object": [[78, "DoubleML-Object"]], "DoubleML Workflow": [[117, null]], "DoubleML meets FLAML - How to tune learners automatically within DoubleML": [[70, null]], "DoubleMLData from arrays and matrices": [[83, "doublemldata-from-arrays-and-matrices"]], "DoubleMLData from dataframes": [[83, "doublemldata-from-dataframes"]], "Effect Heterogeneity": [[55, "effect-heterogeneity"], [68, "Effect-Heterogeneity"]], "Empirical coverage vs. lasso penalty \\lambda": [[64, "Empirical-coverage-vs.-lasso-penalty-\\lambda"]], "Estimate DML models without sample-splitting": [[88, "estimate-dml-models-without-sample-splitting"]], "Estimate double/debiased machine learning models": [[114, "estimate-double-debiased-machine-learning-models"]], "Estimating Potential Quantiles and Quantile Treatment Effects": [[73, "Estimating-Potential-Quantiles-and-Quantile-Treatment-Effects"]], "Estimating local quantile treatment effects (LQTEs)": [[73, "Estimating-local-quantile-treatment-effects-(LQTEs)"]], "Estimating the Average Treatment Effect of 401(k) Eligibility on Net Financial Assets": [[51, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"], [72, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"]], "Estimating the treatment effect on the Conditional Value a Risk (CVaR)": [[73, "Estimating-the-treatment-effect-on-the-Conditional-Value-a-Risk-(CVaR)"]], "Estimation": [[53, "Estimation"], [53, "id2"], [80, "Estimation"], [80, "id2"]], "Estimation quality vs. \\lambda": [[64, "Estimation-quality-vs.-\\lambda"]], "Evaluate learners": [[86, "evaluate-learners"]], "Example: Sensitivity Analysis for Causal ML": [[79, null]], "Examples": [[55, null]], "Exploiting the Functionalities of did": [[49, "Exploiting-the-Functionalities-of-did"]], "Externally provide a sample splitting / partition": [[88, "externally-provide-a-sample-splitting-partition"]], "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)": [[77, null]], "Fuzzy RDD": [[77, "Fuzzy-RDD"]], "Fuzzy RDD Without Adjustment": [[77, "Fuzzy-RDD-Without-Adjustment"]], "Fuzzy RDD with Flexible Adjustment": [[77, "Fuzzy-RDD-with-Flexible-Adjustment"]], "Fuzzy RDD with Linear Adjustment": [[77, "Fuzzy-RDD-with-Linear-Adjustment"]], "Fuzzy Regression Discontinuity Design": [[87, "fuzzy-regression-discontinuity-design"]], "GATE Estimation and Sensitivity": [[67, "GATE-Estimation-and-Sensitivity"]], "GATET Estimation and Sensitivity": [[67, "GATET-Estimation-and-Sensitivity"]], "GATEs for IRM models": [[85, "gates-for-irm-models"]], "GATEs for PLR models": [[85, "gates-for-plr-models"]], "General Examples": [[55, "general-examples"]], "General algorithm": [[103, "general-algorithm"]], "Generate Fuzzy Data": [[77, "Generate-Fuzzy-Data"]], "Generate Sharp Data": [[77, "Generate-Sharp-Data"]], "Getting Started": [[114, null]], "Group Average Treatment Effects (GATEs)": [[65, "Group-Average-Treatment-Effects-(GATEs)"], [66, "Group-Average-Treatment-Effects-(GATEs)"]], "Group average treatment effects (GATEs)": [[85, "group-average-treatment-effects-gates"]], "Heterogeneous treatment effects": [[85, null]], "How to exploit more features of mlr3pipelines in DoubleML": [[52, "How-to-exploit-more-features-of-mlr3pipelines-in-DoubleML"]], "Hyperparameter tuning": [[86, "hyperparameter-tuning"], [86, "id16"]], "Hyperparameter tuning with pipelines": [[86, "hyperparameter-tuning-with-pipelines"]], "Implementation": [[103, "implementation"]], "Implementation Details": [[87, "implementation-details"]], "Implementation of the double machine learning algorithms": [[81, "implementation-of-the-double-machine-learning-algorithms"]], "Implementation of the score function and the estimate of the causal parameter": [[89, "implementation-of-the-score-function-and-the-estimate-of-the-causal-parameter"]], "Implemented Neyman orthogonal score functions": [[89, "implemented-neyman-orthogonal-score-functions"]], "Initialize DoubleMLClusterData object": [[50, "Initialize-DoubleMLClusterData-object"], [71, "Initialize-DoubleMLClusterData-object"]], "Initialize the objects of class DoubleMLPLIV": [[50, "Initialize-the-objects-of-class-DoubleMLPLIV"], [71, "Initialize-the-objects-of-class-DoubleMLPLIV"]], "Installing DoubleML": [[113, null]], "Instrumental Variables Directed Acyclic Graph (IV - DAG)": [[47, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"], [57, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"]], "Interactive IV Model (IIVM)": [[51, "Interactive-IV-Model-(IIVM)"], [72, "Interactive-IV-Model-(IIVM)"]], "Interactive IV model (IIVM)": [[87, "interactive-iv-model-iivm"], [89, "interactive-iv-model-iivm"]], "Interactive Regression Model (IRM)": [[51, "Interactive-Regression-Model-(IRM)"], [65, "Interactive-Regression-Model-(IRM)"], [72, "Interactive-Regression-Model-(IRM)"], [75, "Interactive-Regression-Model-(IRM)"]], "Interactive regression model (IRM)": [[103, "interactive-regression-model-irm"]], "Interactive regression models (IRM)": [[87, "interactive-regression-models-irm"], [89, "interactive-regression-models-irm"]], "Learners and Hyperparameters": [[68, "Learners-and-Hyperparameters"]], "Learners to estimate the nuisance models": [[114, "learners-to-estimate-the-nuisance-models"]], "Learners, hyperparameters and hyperparameter tuning": [[86, null]], "Load Data": [[79, "Load-Data"]], "Load and Process Data": [[50, "Load-and-Process-Data"], [71, "Load-and-Process-Data"]], "Load bonus data using the dml datasets module": [[54, "Load-bonus-data-using-the-dml-datasets-module"]], "Local Average Treatment Effects of 401(k) Participation on Net Financial Assets": [[51, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"], [72, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"]], "Local Potential Quantile Estimation": [[76, "Local-Potential-Quantile-Estimation"]], "Local Potential Quantiles (LPQs)": [[76, "Local-Potential-Quantiles-(LPQs)"]], "Local Quantile Treatment Effects (LQTEs)": [[76, "Local-Quantile-Treatment-Effects-(LQTEs)"]], "Local potential quantiles (LPQs)": [[89, "local-potential-quantiles-lpqs"]], "Main Features": [[112, "main-features"]], "Minimum requirements for learners": [[86, "minimum-requirements-for-learners"], [86, "id2"]], "Missingness at Random": [[87, "missingness-at-random"], [89, "missingness-at-random"]], "Model-specific implementations": [[103, "model-specific-implementations"]], "Models": [[87, null]], "Motivation": [[50, "Motivation"], [71, "Motivation"]], "Multiple Average Potential Outcome Models (APOS)": [[56, "Multiple-Average-Potential-Outcome-Models-(APOS)"]], "Naive estimation": [[47, "Naive-estimation"], [57, "Naive-estimation"]], "No Clustering / Zero-Way Clustering": [[50, "No-Clustering-/-Zero-Way-Clustering"], [71, "No-Clustering-/-Zero-Way-Clustering"]], "Nonignorable Nonresponse": [[87, "nonignorable-nonresponse"], [89, "nonignorable-nonresponse"]], "One-Way Clustering with Respect to the Market": [[50, "One-Way-Clustering-with-Respect-to-the-Market"], [71, "One-Way-Clustering-with-Respect-to-the-Market"]], "One-Way Clustering with Respect to the Product": [[50, "One-Way-Clustering-with-Respect-to-the-Product"], [71, "One-Way-Clustering-with-Respect-to-the-Product"]], "One-dimensional Example": [[59, "One-dimensional-Example"], [60, "One-dimensional-Example"]], "Other models": [[45, null]], "Outcome missing at random (MAR)": [[53, "Outcome-missing-at-random-(MAR)"], [80, "Outcome-missing-at-random-(MAR)"]], "Outcome missing under nonignorable nonresponse": [[53, "Outcome-missing-under-nonignorable-nonresponse"], [80, "Outcome-missing-under-nonignorable-nonresponse"]], "Overcoming regularization bias by orthogonalization": [[48, "Overcoming-regularization-bias-by-orthogonalization"], [58, "Overcoming-regularization-bias-by-orthogonalization"], [82, "overcoming-regularization-bias-by-orthogonalization"]], "Panel Data": [[89, "panel-data"]], "Panel Data (Repeated Outcomes)": [[62, "Panel-Data-(Repeated-Outcomes)"]], "Panel data": [[87, "panel-data"]], "Parameter tuning": [[52, "Parameter-tuning"]], "Partialling out score": [[48, "Partialling-out-score"], [58, "Partialling-out-score"], [82, "partialling-out-score"]], "Partially Linear Regression Model (PLR)": [[51, "Partially-Linear-Regression-Model-(PLR)"], [66, "Partially-Linear-Regression-Model-(PLR)"], [72, "Partially-Linear-Regression-Model-(PLR)"]], "Partially linear IV regression model (PLIV)": [[87, "partially-linear-iv-regression-model-pliv"], [89, "partially-linear-iv-regression-model-pliv"]], "Partially linear models (PLM)": [[87, "partially-linear-models-plm"], [89, "partially-linear-models-plm"]], "Partially linear regression model (PLR)": [[87, "partially-linear-regression-model-plr"], [89, "partially-linear-regression-model-plr"], [103, "partially-linear-regression-model-plr"]], "Plot Coefficients and 95% Confidence Intervals": [[70, "Plot-Coefficients-and-95%-Confidence-Intervals"]], "Policy Learning with Trees": [[75, "Policy-Learning-with-Trees"], [85, "policy-learning-with-trees"]], "Potential Quantile Estimation": [[76, "Potential-Quantile-Estimation"]], "Potential Quantiles (PQs)": [[76, "Potential-Quantiles-(PQs)"]], "Potential quantiles (PQs)": [[85, "potential-quantiles-pqs"], [89, "potential-quantiles-pqs"]], "Python: Average Potential Outcome (APO) Models": [[56, null]], "Python: Basic Instrumental Variables calculation": [[57, null]], "Python: Basics of Double Machine Learning": [[58, null]], "Python: Building the package from source": [[113, "python-building-the-package-from-source"]], "Python: Case studies": [[55, "python-case-studies"]], "Python: Choice of learners": [[69, null]], "Python: Cluster Robust Double Machine Learning": [[71, null]], "Python: Conditional Average Treatment Effects (CATEs) for IRM models": [[59, null]], "Python: Conditional Average Treatment Effects (CATEs) for PLR models": [[60, null]], "Python: Conditional Value at Risk of potential outcomes": [[61, null]], "Python: Difference-in-Differences": [[62, null]], "Python: Difference-in-Differences Pre-Testing": [[63, null]], "Python: First Stage and Causal Estimation": [[64, null]], "Python: GATE Sensitivity Analysis": [[67, null]], "Python: Group Average Treatment Effects (GATEs) for IRM models": [[65, null]], "Python: Group Average Treatment Effects (GATEs) for PLR models": [[66, null]], "Python: IRM and APO Model Comparison": [[68, null]], "Python: Impact of 401(k) on Financial Wealth": [[72, null]], "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)": [[73, null]], "Python: Installing DoubleML": [[113, "python-installing-doubleml"]], "Python: Installing a released version from a .whl file": [[113, "python-installing-a-released-version-from-a-whl-file"]], "Python: Installing the latest release from pip or conda": [[113, "python-installing-the-latest-release-from-pip-or-conda"]], "Python: Learners and hyperparameters": [[86, "python-learners-and-hyperparameters"]], "Python: Optional Dependencies": [[113, "python-optional-dependencies"]], "Python: PLM and IRM for Multiple Treatments": [[74, null]], "Python: Policy Learning with Trees": [[75, null]], "Python: Potential Quantiles and Quantile Treatment Effects": [[76, null]], "Python: Sample Selection Models": [[80, null]], "Python: Sensitivity Analysis": [[78, null]], "Quantile Treatment Effects (QTEs)": [[76, "Quantile-Treatment-Effects-(QTEs)"]], "Quantile treatment effects (QTEs)": [[85, "quantile-treatment-effects-qtes"]], "Quantiles": [[85, "quantiles"]], "R: Basic Instrumental Variables Calculation": [[47, null]], "R: Basics of Double Machine Learning": [[48, null]], "R: Case studies": [[55, "r-case-studies"]], "R: Cluster Robust Double Machine Learning": [[50, null]], "R: DoubleML for Difference-in-Differences": [[49, null]], "R: Ensemble Learners and More with mlr3pipelines": [[52, null]], "R: Impact of 401(k) on Financial Wealth": [[51, null]], "R: Installing DoubleML": [[113, "r-installing-doubleml"]], "R: Installing the development version from GitHub": [[113, "r-installing-the-development-version-from-github"]], "R: Installing the latest release from CRAN": [[113, "r-installing-the-latest-release-from-cran"]], "R: Learners and hyperparameters": [[86, "r-learners-and-hyperparameters"]], "R: Sample Selection Models": [[53, null]], "Ranking Treatment Effects under Treatment Propensity and Treatment Effect Heterogeneity": [[74, "Ranking-Treatment-Effects-under-Treatment-Propensity-and-Treatment-Effect-Heterogeneity"]], "Real-Data Application": [[50, "Real-Data-Application"], [71, "Real-Data-Application"]], "References": [[47, "References"], [49, "References"], [50, "References"], [52, "References"], [57, "References"], [64, "References"], [69, "References"], [70, "References"], [71, "References"], [74, "References"], [79, "References"], [82, "references"], [86, "references"], [88, "references"], [102, "references"], [112, "references"], [114, "references"]], "Regression Discontinuity Designs (RDD)": [[87, "regression-discontinuity-designs-rdd"]], "Regularization Bias in Simple ML-Approaches": [[48, "Regularization-Bias-in-Simple-ML-Approaches"], [58, "Regularization-Bias-in-Simple-ML-Approaches"]], "Regularization bias in simple ML-approaches": [[82, "regularization-bias-in-simple-ml-approaches"]], "Release Notes": [[116, null]], "Repeated Cross-Sectional Data": [[62, "Repeated-Cross-Sectional-Data"], [89, "repeated-cross-sectional-data"]], "Repeated cross-fitting with K folds and M repetitions": [[88, "repeated-cross-fitting-with-k-folds-and-m-repetitions"]], "Repeated cross-sections": [[87, "repeated-cross-sections"]], "Sample Selection Models": [[89, "sample-selection-models"]], "Sample Selection Models (SSM)": [[87, "sample-selection-models-ssm"]], "Sample splitting to remove bias induced by overfitting": [[48, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [58, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [82, "sample-splitting-to-remove-bias-induced-by-overfitting"]], "Sample-splitting without cross-fitting": [[88, "sample-splitting-without-cross-fitting"]], "Sample-splitting, cross-fitting and repeated cross-fitting": [[88, null]], "Sandbox": [[55, "sandbox"]], "Score Mixin Classes for DoubleML Models": [[44, null]], "Score functions": [[89, null]], "Sensitivity Analysis": [[56, "Sensitivity-Analysis"], [68, "Sensitivity-Analysis"], [78, "Sensitivity-Analysis"], [78, "id1"]], "Sensitivity Analysis with IRM": [[78, "Sensitivity-Analysis-with-IRM"]], "Sensitivity analysis": [[103, null]], "Set up learners based on mlr3pipelines": [[52, "Set-up-learners-based-on-mlr3pipelines"]], "Sharp RDD": [[77, "Sharp-RDD"]], "Sharp RDD Without Adjustment": [[77, "Sharp-RDD-Without-Adjustment"]], "Sharp RDD with Flexible Adjustment": [[77, "Sharp-RDD-with-Flexible-Adjustment"]], "Sharp RDD with Linear Adjustment": [[77, "Sharp-RDD-with-Linear-Adjustment"]], "Sharp Regression Discontinuity Design": [[87, "sharp-regression-discontinuity-design"]], "Simulate two-way cluster data": [[50, "Simulate-two-way-cluster-data"], [71, "Simulate-two-way-cluster-data"]], "Simulation Example": [[78, "Simulation-Example"]], "Simultaneous inference over different DoubleML models (advanced)": [[102, "simultaneous-inference-over-different-doubleml-models-advanced"]], "Single Average Potential Outcome Models (APO)": [[56, "Single-Average-Potential-Outcome-Models-(APO)"]], "Source code and maintenance": [[112, "source-code-and-maintenance"]], "Specify learner and estimate causal parameter: IRM model with Lasso as learner": [[54, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: IRM model with random forest as learner": [[54, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-random-forest-as-learner"]], "Specify learner and estimate causal parameter: PLR model with Lasso as learner": [[54, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: PLR model with random forest as learner": [[54, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-random-forest-as-learner"]], "Specifying alternative score functions via callables": [[89, "specifying-alternative-score-functions-via-callables"]], "Specifying learners and set hyperparameters": [[86, "specifying-learners-and-set-hyperparameters"], [86, "id9"]], "Standard approach": [[69, "Standard-approach"]], "Step 1: Custom API for FLAML Models within DoubleML": [[70, "Step-1:-Custom-API-for-FLAML-Models-within-DoubleML"]], "Step 1: Initialize and Train the AutoML Models:": [[70, "Step-1:-Initialize-and-Train-the-AutoML-Models:"]], "Step 2: Evaluate the Tuned Models": [[70, "Step-2:-Evaluate-the-Tuned-Models"]], "Step 2: Using the API when calling DoubleML\u2019s .fit() Method": [[70, "Step-2:-Using-the-API-when-calling-DoubleML's-.fit()-Method"]], "Step 3: Create and Fit DoubleML Model": [[70, "Step-3:-Create-and-Fit-DoubleML-Model"]], "Summary Figure": [[74, "Summary-Figure"]], "Summary of Results": [[51, "Summary-of-Results"], [72, "Summary-of-Results"]], "The Augmented Inverse Propensity Weighting Model estimates the ATE under arbitrary effect and propensity score heterogeneity": [[74, "The-Augmented-Inverse-Propensity-Weighting-Model-estimates-the-ATE-under-arbitrary-effect-and-propensity-score-heterogeneity"]], "The Data Backend: DoubleMLData": [[51, "The-Data-Backend:-DoubleMLData"], [72, "The-Data-Backend:-DoubleMLData"]], "The DoubleML package": [[51, "The-DoubleML-package"], [72, "The-DoubleML-package"]], "The Partially Linear Model performs overlap weighting": [[74, "The-Partially-Linear-Model-performs-overlap-weighting"]], "The basics of double/debiased machine learning": [[82, null]], "The causal model": [[114, "the-causal-model"]], "The data-backend DoubleMLData": [[83, null], [114, "the-data-backend-doublemldata"]], "Theory": [[103, "theory"]], "Tuning on the Folds": [[70, "Tuning-on-the-Folds"]], "Tuning on the full Sample": [[70, "Tuning-on-the-full-Sample"]], "Two-Dimensional Example": [[59, "Two-Dimensional-Example"], [60, "Two-Dimensional-Example"]], "Two-Way Clustering with Respect to Product and Market": [[50, "Two-Way-Clustering-with-Respect-to-Product-and-Market"], [71, "Two-Way-Clustering-with-Respect-to-Product-and-Market"]], "Untuned (default parameter) XGBoost": [[70, "Untuned-(default-parameter)-XGBoost"]], "Use ensemble learners based on mlr3pipelines": [[52, "Use-ensemble-learners-based-on-mlr3pipelines"]], "User Guide": [[84, null]], "Using DoubleML": [[47, "Using-DoubleML"], [57, "Using-DoubleML"]], "Using ML for DiD: Integrating DoubleML in did": [[49, "Using-ML-for-DiD:-Integrating-DoubleML-in-did"]], "Using learners from mlr3, mlr3learners and mlr3extralearners": [[52, "Using-learners-from-mlr3,-mlr3learners-and-mlr3extralearners"]], "Using pipelines to construct learners": [[86, "using-pipelines-to-construct-learners"]], "Utility Classes": [[46, "utility-classes"]], "Utility Classes and Functions": [[46, null]], "Utility Functions": [[46, "utility-functions"]], "VanderWeele and Arah (2011): Benchmarking": [[79, "VanderWeele-and-Arah-(2011):-Benchmarking"]], "Variance estimation": [[102, "variance-estimation"]], "Variance estimation and confidence intervals": [[102, null]], "Weighted Average Treatment Effects": [[85, "weighted-average-treatment-effects"]], "doubleml.DoubleMLAPO": [[4, null]], "doubleml.DoubleMLAPOS": [[5, null]], "doubleml.DoubleMLCVAR": [[6, null]], "doubleml.DoubleMLClusterData": [[7, null]], "doubleml.DoubleMLDID": [[8, null]], "doubleml.DoubleMLDIDCS": [[9, null]], "doubleml.DoubleMLData": [[10, null]], "doubleml.DoubleMLIIVM": [[11, null]], "doubleml.DoubleMLIRM": [[12, null]], "doubleml.DoubleMLLPQ": [[13, null]], "doubleml.DoubleMLPLIV": [[14, null]], "doubleml.DoubleMLPLR": [[15, null]], "doubleml.DoubleMLPQ": [[16, null]], "doubleml.DoubleMLQTE": [[17, null]], "doubleml.DoubleMLSSM": [[18, null]], "doubleml.datasets.fetch_401K": [[19, null]], "doubleml.datasets.fetch_bonus": [[20, null]], "doubleml.datasets.make_confounded_irm_data": [[21, null]], "doubleml.datasets.make_confounded_plr_data": [[22, null]], "doubleml.datasets.make_did_SZ2020": [[23, null]], "doubleml.datasets.make_heterogeneous_data": [[24, null]], "doubleml.datasets.make_iivm_data": [[25, null]], "doubleml.datasets.make_irm_data": [[26, null]], "doubleml.datasets.make_irm_data_discrete_treatments": [[27, null]], "doubleml.datasets.make_pliv_CHS2015": [[28, null]], "doubleml.datasets.make_pliv_multiway_cluster_CKMS2021": [[29, null]], "doubleml.datasets.make_plr_CCDDHNR2018": [[30, null]], "doubleml.datasets.make_plr_turrell2018": [[31, null]], "doubleml.datasets.make_ssm_data": [[32, null]], "doubleml.double_ml_score_mixins.LinearScoreMixin": [[33, null]], "doubleml.double_ml_score_mixins.NonLinearScoreMixin": [[34, null]], "doubleml.rdd.RDFlex": [[35, null]], "doubleml.rdd.datasets.make_simple_rdd_data": [[36, null]], "doubleml.utils.DMLDummyClassifier": [[37, null]], "doubleml.utils.DMLDummyRegressor": [[38, null]], "doubleml.utils.DoubleMLBLP": [[39, null]], "doubleml.utils.DoubleMLPolicyTree": [[40, null]], "doubleml.utils.GlobalClassifier": [[41, null]], "doubleml.utils.GlobalRegressor": [[42, null]], "doubleml.utils.gain_statistics": [[43, null]]}, "docnames": ["api/api", "api/data_class", "api/datasets", "api/dml_models", "api/generated/doubleml.DoubleMLAPO", "api/generated/doubleml.DoubleMLAPOS", "api/generated/doubleml.DoubleMLCVAR", "api/generated/doubleml.DoubleMLClusterData", "api/generated/doubleml.DoubleMLDID", "api/generated/doubleml.DoubleMLDIDCS", "api/generated/doubleml.DoubleMLData", "api/generated/doubleml.DoubleMLIIVM", "api/generated/doubleml.DoubleMLIRM", "api/generated/doubleml.DoubleMLLPQ", "api/generated/doubleml.DoubleMLPLIV", "api/generated/doubleml.DoubleMLPLR", "api/generated/doubleml.DoubleMLPQ", "api/generated/doubleml.DoubleMLQTE", "api/generated/doubleml.DoubleMLSSM", "api/generated/doubleml.datasets.fetch_401K", "api/generated/doubleml.datasets.fetch_bonus", "api/generated/doubleml.datasets.make_confounded_irm_data", "api/generated/doubleml.datasets.make_confounded_plr_data", "api/generated/doubleml.datasets.make_did_SZ2020", "api/generated/doubleml.datasets.make_heterogeneous_data", "api/generated/doubleml.datasets.make_iivm_data", "api/generated/doubleml.datasets.make_irm_data", "api/generated/doubleml.datasets.make_irm_data_discrete_treatments", "api/generated/doubleml.datasets.make_pliv_CHS2015", "api/generated/doubleml.datasets.make_pliv_multiway_cluster_CKMS2021", "api/generated/doubleml.datasets.make_plr_CCDDHNR2018", "api/generated/doubleml.datasets.make_plr_turrell2018", "api/generated/doubleml.datasets.make_ssm_data", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin", "api/generated/doubleml.rdd.RDFlex", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data", "api/generated/doubleml.utils.DMLDummyClassifier", "api/generated/doubleml.utils.DMLDummyRegressor", "api/generated/doubleml.utils.DoubleMLBLP", "api/generated/doubleml.utils.DoubleMLPolicyTree", "api/generated/doubleml.utils.GlobalClassifier", "api/generated/doubleml.utils.GlobalRegressor", "api/generated/doubleml.utils.gain_statistics", "api/mixins", "api/other_models", "api/utility", "examples/R_double_ml_basic_iv", "examples/R_double_ml_basics", "examples/R_double_ml_did", "examples/R_double_ml_multiway_cluster", "examples/R_double_ml_pension", "examples/R_double_ml_pipeline", "examples/R_double_ml_ssm", "examples/double_ml_bonus_data", "examples/index", "examples/py_double_ml_apo", "examples/py_double_ml_basic_iv", "examples/py_double_ml_basics", "examples/py_double_ml_cate", "examples/py_double_ml_cate_plr", "examples/py_double_ml_cvar", "examples/py_double_ml_did", "examples/py_double_ml_did_pretest", "examples/py_double_ml_firststage", "examples/py_double_ml_gate", "examples/py_double_ml_gate_plr", "examples/py_double_ml_gate_sensitivity", "examples/py_double_ml_irm_vs_apo", "examples/py_double_ml_learner", "examples/py_double_ml_meets_flaml", "examples/py_double_ml_multiway_cluster", "examples/py_double_ml_pension", "examples/py_double_ml_pension_qte", "examples/py_double_ml_plm_irm_hetfx", "examples/py_double_ml_policy_tree", "examples/py_double_ml_pq", "examples/py_double_ml_rdflex", "examples/py_double_ml_sensitivity", "examples/py_double_ml_sensitivity_booking", "examples/py_double_ml_ssm", "guide/algorithms", "guide/basics", "guide/data_backend", "guide/guide", "guide/heterogeneity", "guide/learners", "guide/models", "guide/resampling", "guide/scores", "guide/scores/apo_score", "guide/scores/cvar_score", "guide/scores/did_score", "guide/scores/didcs_score", "guide/scores/iivm_score", "guide/scores/irm_score", "guide/scores/lpq_score", "guide/scores/mar_score", "guide/scores/nr_score", "guide/scores/pliv_score", "guide/scores/plr_score", "guide/scores/pq_score", "guide/se_confint", "guide/sensitivity", "guide/sensitivity/apo_sensitivity", "guide/sensitivity/benchmarking", "guide/sensitivity/did_cs_sensitivity", "guide/sensitivity/did_sensitivity", "guide/sensitivity/implementation", "guide/sensitivity/irm_sensitivity", "guide/sensitivity/plr_sensitivity", "guide/sensitivity/theory", "index", "intro/install", "intro/intro", "literature/literature", "release/release", "workflow/workflow"], "envversion": {"nbsphinx": 4, "sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1}, "filenames": ["api/api.rst", "api/data_class.rst", "api/datasets.rst", "api/dml_models.rst", "api/generated/doubleml.DoubleMLAPO.rst", "api/generated/doubleml.DoubleMLAPOS.rst", "api/generated/doubleml.DoubleMLCVAR.rst", "api/generated/doubleml.DoubleMLClusterData.rst", "api/generated/doubleml.DoubleMLDID.rst", "api/generated/doubleml.DoubleMLDIDCS.rst", "api/generated/doubleml.DoubleMLData.rst", "api/generated/doubleml.DoubleMLIIVM.rst", "api/generated/doubleml.DoubleMLIRM.rst", "api/generated/doubleml.DoubleMLLPQ.rst", "api/generated/doubleml.DoubleMLPLIV.rst", "api/generated/doubleml.DoubleMLPLR.rst", "api/generated/doubleml.DoubleMLPQ.rst", "api/generated/doubleml.DoubleMLQTE.rst", "api/generated/doubleml.DoubleMLSSM.rst", "api/generated/doubleml.datasets.fetch_401K.rst", "api/generated/doubleml.datasets.fetch_bonus.rst", "api/generated/doubleml.datasets.make_confounded_irm_data.rst", "api/generated/doubleml.datasets.make_confounded_plr_data.rst", "api/generated/doubleml.datasets.make_did_SZ2020.rst", "api/generated/doubleml.datasets.make_heterogeneous_data.rst", "api/generated/doubleml.datasets.make_iivm_data.rst", "api/generated/doubleml.datasets.make_irm_data.rst", "api/generated/doubleml.datasets.make_irm_data_discrete_treatments.rst", "api/generated/doubleml.datasets.make_pliv_CHS2015.rst", "api/generated/doubleml.datasets.make_pliv_multiway_cluster_CKMS2021.rst", "api/generated/doubleml.datasets.make_plr_CCDDHNR2018.rst", "api/generated/doubleml.datasets.make_plr_turrell2018.rst", "api/generated/doubleml.datasets.make_ssm_data.rst", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin.rst", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin.rst", "api/generated/doubleml.rdd.RDFlex.rst", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data.rst", "api/generated/doubleml.utils.DMLDummyClassifier.rst", "api/generated/doubleml.utils.DMLDummyRegressor.rst", "api/generated/doubleml.utils.DoubleMLBLP.rst", "api/generated/doubleml.utils.DoubleMLPolicyTree.rst", "api/generated/doubleml.utils.GlobalClassifier.rst", "api/generated/doubleml.utils.GlobalRegressor.rst", "api/generated/doubleml.utils.gain_statistics.rst", "api/mixins.rst", "api/other_models.rst", "api/utility.rst", "examples/R_double_ml_basic_iv.ipynb", "examples/R_double_ml_basics.ipynb", "examples/R_double_ml_did.ipynb", "examples/R_double_ml_multiway_cluster.ipynb", "examples/R_double_ml_pension.ipynb", "examples/R_double_ml_pipeline.ipynb", "examples/R_double_ml_ssm.ipynb", "examples/double_ml_bonus_data.ipynb", "examples/index.rst", "examples/py_double_ml_apo.ipynb", "examples/py_double_ml_basic_iv.ipynb", "examples/py_double_ml_basics.ipynb", "examples/py_double_ml_cate.ipynb", "examples/py_double_ml_cate_plr.ipynb", "examples/py_double_ml_cvar.ipynb", "examples/py_double_ml_did.ipynb", "examples/py_double_ml_did_pretest.ipynb", "examples/py_double_ml_firststage.ipynb", "examples/py_double_ml_gate.ipynb", "examples/py_double_ml_gate_plr.ipynb", "examples/py_double_ml_gate_sensitivity.ipynb", "examples/py_double_ml_irm_vs_apo.ipynb", "examples/py_double_ml_learner.ipynb", "examples/py_double_ml_meets_flaml.ipynb", "examples/py_double_ml_multiway_cluster.ipynb", "examples/py_double_ml_pension.ipynb", "examples/py_double_ml_pension_qte.ipynb", "examples/py_double_ml_plm_irm_hetfx.ipynb", "examples/py_double_ml_policy_tree.ipynb", "examples/py_double_ml_pq.ipynb", "examples/py_double_ml_rdflex.ipynb", "examples/py_double_ml_sensitivity.ipynb", "examples/py_double_ml_sensitivity_booking.ipynb", "examples/py_double_ml_ssm.ipynb", "guide/algorithms.rst", "guide/basics.rst", "guide/data_backend.rst", "guide/guide.rst", "guide/heterogeneity.rst", "guide/learners.rst", "guide/models.rst", "guide/resampling.rst", "guide/scores.rst", "guide/scores/apo_score.rst", "guide/scores/cvar_score.rst", "guide/scores/did_score.rst", "guide/scores/didcs_score.rst", "guide/scores/iivm_score.rst", "guide/scores/irm_score.rst", "guide/scores/lpq_score.rst", "guide/scores/mar_score.rst", "guide/scores/nr_score.rst", "guide/scores/pliv_score.rst", "guide/scores/plr_score.rst", "guide/scores/pq_score.rst", "guide/se_confint.rst", "guide/sensitivity.rst", "guide/sensitivity/apo_sensitivity.rst", "guide/sensitivity/benchmarking.rst", "guide/sensitivity/did_cs_sensitivity.rst", "guide/sensitivity/did_sensitivity.rst", "guide/sensitivity/implementation.rst", "guide/sensitivity/irm_sensitivity.rst", "guide/sensitivity/plr_sensitivity.rst", "guide/sensitivity/theory.rst", "index.rst", "intro/install.rst", "intro/intro.rst", "literature/literature.rst", "release/release.rst", "workflow/workflow.rst"], "indexentries": {"aggregate_over_splits() (doubleml.rdd.rdflex method)": [[35, "doubleml.rdd.RDFlex.aggregate_over_splits", false]], "bootstrap() (doubleml.doublemlapo method)": [[4, "doubleml.DoubleMLAPO.bootstrap", false]], "bootstrap() (doubleml.doublemlapos method)": [[5, "doubleml.DoubleMLAPOS.bootstrap", false]], "bootstrap() (doubleml.doublemlcvar method)": [[6, "doubleml.DoubleMLCVAR.bootstrap", false]], "bootstrap() (doubleml.doublemldid method)": [[8, "doubleml.DoubleMLDID.bootstrap", false]], "bootstrap() (doubleml.doublemldidcs method)": [[9, "doubleml.DoubleMLDIDCS.bootstrap", false]], "bootstrap() (doubleml.doublemliivm method)": [[11, "doubleml.DoubleMLIIVM.bootstrap", false]], "bootstrap() (doubleml.doublemlirm method)": [[12, "doubleml.DoubleMLIRM.bootstrap", false]], "bootstrap() (doubleml.doublemllpq method)": [[13, "doubleml.DoubleMLLPQ.bootstrap", false]], "bootstrap() (doubleml.doublemlpliv method)": [[14, "doubleml.DoubleMLPLIV.bootstrap", false]], "bootstrap() (doubleml.doublemlplr method)": [[15, "doubleml.DoubleMLPLR.bootstrap", false]], "bootstrap() (doubleml.doublemlpq method)": [[16, "doubleml.DoubleMLPQ.bootstrap", false]], "bootstrap() (doubleml.doublemlqte method)": [[17, "doubleml.DoubleMLQTE.bootstrap", false]], "bootstrap() (doubleml.doublemlssm method)": [[18, "doubleml.DoubleMLSSM.bootstrap", false]], "capo() (doubleml.doublemlapo method)": [[4, "doubleml.DoubleMLAPO.capo", false]], "cate() (doubleml.doublemlirm method)": [[12, "doubleml.DoubleMLIRM.cate", false]], "cate() (doubleml.doublemlplr method)": [[15, "doubleml.DoubleMLPLR.cate", false]], "causal_contrast() (doubleml.doublemlapos method)": [[5, "doubleml.DoubleMLAPOS.causal_contrast", false]], "confint() (doubleml.doublemlapo method)": [[4, "doubleml.DoubleMLAPO.confint", false]], "confint() (doubleml.doublemlapos method)": [[5, "doubleml.DoubleMLAPOS.confint", false]], "confint() (doubleml.doublemlcvar method)": [[6, "doubleml.DoubleMLCVAR.confint", false]], "confint() (doubleml.doublemldid method)": [[8, "doubleml.DoubleMLDID.confint", false]], "confint() (doubleml.doublemldidcs method)": [[9, "doubleml.DoubleMLDIDCS.confint", false]], "confint() (doubleml.doublemliivm method)": [[11, "doubleml.DoubleMLIIVM.confint", false]], "confint() (doubleml.doublemlirm method)": [[12, "doubleml.DoubleMLIRM.confint", false]], "confint() (doubleml.doublemllpq method)": [[13, "doubleml.DoubleMLLPQ.confint", false]], "confint() (doubleml.doublemlpliv method)": [[14, "doubleml.DoubleMLPLIV.confint", false]], "confint() (doubleml.doublemlplr method)": [[15, "doubleml.DoubleMLPLR.confint", false]], "confint() (doubleml.doublemlpq method)": [[16, "doubleml.DoubleMLPQ.confint", false]], "confint() (doubleml.doublemlqte method)": [[17, "doubleml.DoubleMLQTE.confint", false]], "confint() (doubleml.doublemlssm method)": [[18, "doubleml.DoubleMLSSM.confint", false]], "confint() (doubleml.rdd.rdflex method)": [[35, "doubleml.rdd.RDFlex.confint", false]], "confint() (doubleml.utils.doublemlblp method)": [[39, "doubleml.utils.DoubleMLBLP.confint", false]], "construct_framework() (doubleml.doublemlapo method)": [[4, "doubleml.DoubleMLAPO.construct_framework", false]], "construct_framework() (doubleml.doublemlcvar method)": [[6, "doubleml.DoubleMLCVAR.construct_framework", false]], "construct_framework() (doubleml.doublemldid method)": [[8, "doubleml.DoubleMLDID.construct_framework", false]], "construct_framework() (doubleml.doublemldidcs method)": [[9, "doubleml.DoubleMLDIDCS.construct_framework", false]], "construct_framework() (doubleml.doublemliivm method)": [[11, "doubleml.DoubleMLIIVM.construct_framework", false]], "construct_framework() (doubleml.doublemlirm method)": [[12, "doubleml.DoubleMLIRM.construct_framework", false]], "construct_framework() (doubleml.doublemllpq method)": [[13, "doubleml.DoubleMLLPQ.construct_framework", false]], "construct_framework() (doubleml.doublemlpliv method)": [[14, "doubleml.DoubleMLPLIV.construct_framework", false]], "construct_framework() (doubleml.doublemlplr method)": [[15, "doubleml.DoubleMLPLR.construct_framework", false]], "construct_framework() (doubleml.doublemlpq method)": [[16, "doubleml.DoubleMLPQ.construct_framework", false]], "construct_framework() (doubleml.doublemlssm method)": [[18, "doubleml.DoubleMLSSM.construct_framework", false]], "dmldummyclassifier (class in doubleml.utils)": [[37, "doubleml.utils.DMLDummyClassifier", false]], "dmldummyregressor (class in doubleml.utils)": [[38, "doubleml.utils.DMLDummyRegressor", false]], "doublemlapo (class in doubleml)": [[4, "doubleml.DoubleMLAPO", false]], "doublemlapos (class in doubleml)": [[5, "doubleml.DoubleMLAPOS", false]], "doublemlblp (class in doubleml.utils)": [[39, "doubleml.utils.DoubleMLBLP", false]], "doublemlclusterdata (class in doubleml)": [[7, "doubleml.DoubleMLClusterData", false]], "doublemlcvar (class in doubleml)": [[6, "doubleml.DoubleMLCVAR", false]], "doublemldata (class in doubleml)": [[10, "doubleml.DoubleMLData", false]], "doublemldid (class in doubleml)": [[8, "doubleml.DoubleMLDID", false]], "doublemldidcs (class in doubleml)": [[9, "doubleml.DoubleMLDIDCS", false]], "doublemliivm (class in doubleml)": [[11, "doubleml.DoubleMLIIVM", false]], "doublemlirm (class in doubleml)": [[12, "doubleml.DoubleMLIRM", false]], "doublemllpq (class in doubleml)": [[13, "doubleml.DoubleMLLPQ", false]], "doublemlpliv (class in doubleml)": [[14, "doubleml.DoubleMLPLIV", false]], "doublemlplr (class in doubleml)": [[15, "doubleml.DoubleMLPLR", false]], "doublemlpolicytree (class in doubleml.utils)": [[40, "doubleml.utils.DoubleMLPolicyTree", false]], "doublemlpq (class in doubleml)": [[16, "doubleml.DoubleMLPQ", false]], "doublemlqte (class in doubleml)": [[17, "doubleml.DoubleMLQTE", false]], "doublemlssm (class in doubleml)": [[18, "doubleml.DoubleMLSSM", false]], "draw_sample_splitting() (doubleml.doublemlapo method)": [[4, "doubleml.DoubleMLAPO.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.doublemlapos method)": [[5, "doubleml.DoubleMLAPOS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.doublemlcvar method)": [[6, "doubleml.DoubleMLCVAR.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.doublemldid method)": [[8, "doubleml.DoubleMLDID.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.doublemldidcs method)": [[9, "doubleml.DoubleMLDIDCS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.doublemliivm method)": [[11, "doubleml.DoubleMLIIVM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.doublemlirm method)": [[12, "doubleml.DoubleMLIRM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.doublemllpq method)": [[13, "doubleml.DoubleMLLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.doublemlpliv method)": [[14, "doubleml.DoubleMLPLIV.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.doublemlplr method)": [[15, "doubleml.DoubleMLPLR.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.doublemlpq method)": [[16, "doubleml.DoubleMLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.doublemlqte method)": [[17, "doubleml.DoubleMLQTE.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.doublemlssm method)": [[18, "doubleml.DoubleMLSSM.draw_sample_splitting", false]], "evaluate_learners() (doubleml.doublemlapo method)": [[4, "doubleml.DoubleMLAPO.evaluate_learners", false]], "evaluate_learners() (doubleml.doublemlcvar method)": [[6, "doubleml.DoubleMLCVAR.evaluate_learners", false]], "evaluate_learners() (doubleml.doublemldid method)": [[8, "doubleml.DoubleMLDID.evaluate_learners", false]], "evaluate_learners() (doubleml.doublemldidcs method)": [[9, "doubleml.DoubleMLDIDCS.evaluate_learners", false]], "evaluate_learners() (doubleml.doublemliivm method)": [[11, "doubleml.DoubleMLIIVM.evaluate_learners", false]], "evaluate_learners() (doubleml.doublemlirm method)": [[12, "doubleml.DoubleMLIRM.evaluate_learners", false]], "evaluate_learners() (doubleml.doublemllpq method)": [[13, "doubleml.DoubleMLLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.doublemlpliv method)": [[14, "doubleml.DoubleMLPLIV.evaluate_learners", false]], "evaluate_learners() (doubleml.doublemlplr method)": [[15, "doubleml.DoubleMLPLR.evaluate_learners", false]], "evaluate_learners() (doubleml.doublemlpq method)": [[16, "doubleml.DoubleMLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.doublemlssm method)": [[18, "doubleml.DoubleMLSSM.evaluate_learners", false]], "fetch_401k() (in module doubleml.datasets)": [[19, "doubleml.datasets.fetch_401K", false]], "fetch_bonus() (in module doubleml.datasets)": [[20, "doubleml.datasets.fetch_bonus", false]], "fit() (doubleml.doublemlapo method)": [[4, "doubleml.DoubleMLAPO.fit", false]], "fit() (doubleml.doublemlapos method)": [[5, "doubleml.DoubleMLAPOS.fit", false]], "fit() (doubleml.doublemlcvar method)": [[6, "doubleml.DoubleMLCVAR.fit", false]], "fit() (doubleml.doublemldid method)": [[8, "doubleml.DoubleMLDID.fit", false]], "fit() (doubleml.doublemldidcs method)": [[9, "doubleml.DoubleMLDIDCS.fit", false]], "fit() (doubleml.doublemliivm method)": [[11, "doubleml.DoubleMLIIVM.fit", false]], "fit() (doubleml.doublemlirm method)": [[12, "doubleml.DoubleMLIRM.fit", false]], "fit() (doubleml.doublemllpq method)": [[13, "doubleml.DoubleMLLPQ.fit", false]], "fit() (doubleml.doublemlpliv method)": [[14, "doubleml.DoubleMLPLIV.fit", false]], "fit() (doubleml.doublemlplr method)": [[15, "doubleml.DoubleMLPLR.fit", false]], "fit() (doubleml.doublemlpq method)": [[16, "doubleml.DoubleMLPQ.fit", false]], "fit() (doubleml.doublemlqte method)": [[17, "doubleml.DoubleMLQTE.fit", false]], "fit() (doubleml.doublemlssm method)": [[18, "doubleml.DoubleMLSSM.fit", false]], "fit() (doubleml.rdd.rdflex method)": [[35, "doubleml.rdd.RDFlex.fit", false]], "fit() (doubleml.utils.dmldummyclassifier method)": [[37, "doubleml.utils.DMLDummyClassifier.fit", false]], "fit() (doubleml.utils.dmldummyregressor method)": [[38, "doubleml.utils.DMLDummyRegressor.fit", false]], "fit() (doubleml.utils.doublemlblp method)": [[39, "doubleml.utils.DoubleMLBLP.fit", false]], "fit() (doubleml.utils.doublemlpolicytree method)": [[40, "doubleml.utils.DoubleMLPolicyTree.fit", false]], "fit() (doubleml.utils.globalclassifier method)": [[41, "doubleml.utils.GlobalClassifier.fit", false]], "fit() (doubleml.utils.globalregressor method)": [[42, "doubleml.utils.GlobalRegressor.fit", false]], "from_arrays() (doubleml.doublemlclusterdata class method)": [[7, "doubleml.DoubleMLClusterData.from_arrays", false]], "from_arrays() (doubleml.doublemldata class method)": [[10, "doubleml.DoubleMLData.from_arrays", false]], "gain_statistics() (in module doubleml.utils)": [[43, "doubleml.utils.gain_statistics", false]], "gapo() (doubleml.doublemlapo method)": [[4, "doubleml.DoubleMLAPO.gapo", false]], "gate() (doubleml.doublemlirm method)": [[12, "doubleml.DoubleMLIRM.gate", false]], "gate() (doubleml.doublemlplr method)": [[15, "doubleml.DoubleMLPLR.gate", false]], "get_metadata_routing() (doubleml.utils.dmldummyclassifier method)": [[37, "doubleml.utils.DMLDummyClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.dmldummyregressor method)": [[38, "doubleml.utils.DMLDummyRegressor.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalclassifier method)": [[41, "doubleml.utils.GlobalClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalregressor method)": [[42, "doubleml.utils.GlobalRegressor.get_metadata_routing", false]], "get_params() (doubleml.doublemlapo method)": [[4, "doubleml.DoubleMLAPO.get_params", false]], "get_params() (doubleml.doublemlcvar method)": [[6, "doubleml.DoubleMLCVAR.get_params", false]], "get_params() (doubleml.doublemldid method)": [[8, "doubleml.DoubleMLDID.get_params", false]], "get_params() (doubleml.doublemldidcs method)": [[9, "doubleml.DoubleMLDIDCS.get_params", false]], "get_params() (doubleml.doublemliivm method)": [[11, "doubleml.DoubleMLIIVM.get_params", false]], "get_params() (doubleml.doublemlirm method)": [[12, "doubleml.DoubleMLIRM.get_params", false]], "get_params() (doubleml.doublemllpq method)": [[13, "doubleml.DoubleMLLPQ.get_params", false]], "get_params() (doubleml.doublemlpliv method)": [[14, "doubleml.DoubleMLPLIV.get_params", false]], "get_params() (doubleml.doublemlplr method)": [[15, "doubleml.DoubleMLPLR.get_params", false]], "get_params() (doubleml.doublemlpq method)": [[16, "doubleml.DoubleMLPQ.get_params", false]], "get_params() (doubleml.doublemlssm method)": [[18, "doubleml.DoubleMLSSM.get_params", false]], "get_params() (doubleml.utils.dmldummyclassifier method)": [[37, "doubleml.utils.DMLDummyClassifier.get_params", false]], "get_params() (doubleml.utils.dmldummyregressor method)": [[38, "doubleml.utils.DMLDummyRegressor.get_params", false]], "get_params() (doubleml.utils.globalclassifier method)": [[41, "doubleml.utils.GlobalClassifier.get_params", false]], "get_params() (doubleml.utils.globalregressor method)": [[42, "doubleml.utils.GlobalRegressor.get_params", false]], "globalclassifier (class in doubleml.utils)": [[41, "doubleml.utils.GlobalClassifier", false]], "globalregressor (class in doubleml.utils)": [[42, "doubleml.utils.GlobalRegressor", false]], "linearscoremixin (class in doubleml.double_ml_score_mixins)": [[33, "doubleml.double_ml_score_mixins.LinearScoreMixin", false]], "make_confounded_irm_data() (in module doubleml.datasets)": [[21, "doubleml.datasets.make_confounded_irm_data", false]], "make_confounded_plr_data() (in module doubleml.datasets)": [[22, "doubleml.datasets.make_confounded_plr_data", false]], "make_did_sz2020() (in module doubleml.datasets)": [[23, "doubleml.datasets.make_did_SZ2020", false]], "make_heterogeneous_data() (in module doubleml.datasets)": [[24, "doubleml.datasets.make_heterogeneous_data", false]], "make_iivm_data() (in module doubleml.datasets)": [[25, "doubleml.datasets.make_iivm_data", false]], "make_irm_data() (in module doubleml.datasets)": [[26, "doubleml.datasets.make_irm_data", false]], "make_irm_data_discrete_treatments() (in module doubleml.datasets)": [[27, "doubleml.datasets.make_irm_data_discrete_treatments", false]], "make_pliv_chs2015() (in module doubleml.datasets)": [[28, "doubleml.datasets.make_pliv_CHS2015", false]], "make_pliv_multiway_cluster_ckms2021() (in module doubleml.datasets)": [[29, "doubleml.datasets.make_pliv_multiway_cluster_CKMS2021", false]], "make_plr_ccddhnr2018() (in module doubleml.datasets)": [[30, "doubleml.datasets.make_plr_CCDDHNR2018", false]], "make_plr_turrell2018() (in module doubleml.datasets)": [[31, "doubleml.datasets.make_plr_turrell2018", false]], "make_simple_rdd_data() (in module doubleml.rdd.datasets)": [[36, "doubleml.rdd.datasets.make_simple_rdd_data", false]], "make_ssm_data() (in module doubleml.datasets)": [[32, "doubleml.datasets.make_ssm_data", false]], "nonlinearscoremixin (class in doubleml.double_ml_score_mixins)": [[34, "doubleml.double_ml_score_mixins.NonLinearScoreMixin", false]], "p_adjust() (doubleml.doublemlapo method)": [[4, "doubleml.DoubleMLAPO.p_adjust", false]], "p_adjust() (doubleml.doublemlcvar method)": [[6, "doubleml.DoubleMLCVAR.p_adjust", false]], "p_adjust() (doubleml.doublemldid method)": [[8, "doubleml.DoubleMLDID.p_adjust", false]], "p_adjust() (doubleml.doublemldidcs method)": [[9, "doubleml.DoubleMLDIDCS.p_adjust", false]], "p_adjust() (doubleml.doublemliivm method)": [[11, "doubleml.DoubleMLIIVM.p_adjust", false]], "p_adjust() (doubleml.doublemlirm method)": [[12, "doubleml.DoubleMLIRM.p_adjust", false]], "p_adjust() (doubleml.doublemllpq method)": [[13, "doubleml.DoubleMLLPQ.p_adjust", false]], "p_adjust() (doubleml.doublemlpliv method)": [[14, "doubleml.DoubleMLPLIV.p_adjust", false]], "p_adjust() (doubleml.doublemlplr method)": [[15, "doubleml.DoubleMLPLR.p_adjust", false]], "p_adjust() (doubleml.doublemlpq method)": [[16, "doubleml.DoubleMLPQ.p_adjust", false]], "p_adjust() (doubleml.doublemlqte method)": [[17, "doubleml.DoubleMLQTE.p_adjust", false]], "p_adjust() (doubleml.doublemlssm method)": [[18, "doubleml.DoubleMLSSM.p_adjust", false]], "plot_tree() (doubleml.utils.doublemlpolicytree method)": [[40, "doubleml.utils.DoubleMLPolicyTree.plot_tree", false]], "policy_tree() (doubleml.doublemlirm method)": [[12, "doubleml.DoubleMLIRM.policy_tree", false]], "predict() (doubleml.utils.dmldummyclassifier method)": [[37, "doubleml.utils.DMLDummyClassifier.predict", false]], "predict() (doubleml.utils.dmldummyregressor method)": [[38, "doubleml.utils.DMLDummyRegressor.predict", false]], "predict() (doubleml.utils.doublemlpolicytree method)": [[40, "doubleml.utils.DoubleMLPolicyTree.predict", false]], "predict() (doubleml.utils.globalclassifier method)": [[41, "doubleml.utils.GlobalClassifier.predict", false]], "predict() (doubleml.utils.globalregressor method)": [[42, "doubleml.utils.GlobalRegressor.predict", false]], "predict_proba() (doubleml.utils.dmldummyclassifier method)": [[37, "doubleml.utils.DMLDummyClassifier.predict_proba", false]], "predict_proba() (doubleml.utils.globalclassifier method)": [[41, "doubleml.utils.GlobalClassifier.predict_proba", false]], "rdflex (class in doubleml.rdd)": [[35, "doubleml.rdd.RDFlex", false]], "score() (doubleml.utils.dmldummyclassifier method)": [[37, "doubleml.utils.DMLDummyClassifier.score", false]], "score() (doubleml.utils.dmldummyregressor method)": [[38, "doubleml.utils.DMLDummyRegressor.score", false]], "score() (doubleml.utils.globalclassifier method)": [[41, "doubleml.utils.GlobalClassifier.score", false]], "score() (doubleml.utils.globalregressor method)": [[42, "doubleml.utils.GlobalRegressor.score", false]], "sensitivity_analysis() (doubleml.doublemlapo method)": [[4, "doubleml.DoubleMLAPO.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.doublemlapos method)": [[5, "doubleml.DoubleMLAPOS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.doublemlcvar method)": [[6, "doubleml.DoubleMLCVAR.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.doublemldid method)": [[8, "doubleml.DoubleMLDID.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.doublemldidcs method)": [[9, "doubleml.DoubleMLDIDCS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.doublemliivm method)": [[11, "doubleml.DoubleMLIIVM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.doublemlirm method)": [[12, "doubleml.DoubleMLIRM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.doublemllpq method)": [[13, "doubleml.DoubleMLLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.doublemlpliv method)": [[14, "doubleml.DoubleMLPLIV.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.doublemlplr method)": [[15, "doubleml.DoubleMLPLR.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.doublemlpq method)": [[16, "doubleml.DoubleMLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.doublemlssm method)": [[18, "doubleml.DoubleMLSSM.sensitivity_analysis", false]], "sensitivity_benchmark() (doubleml.doublemlapo method)": [[4, "doubleml.DoubleMLAPO.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.doublemlapos method)": [[5, "doubleml.DoubleMLAPOS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.doublemlcvar method)": [[6, "doubleml.DoubleMLCVAR.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.doublemldid method)": [[8, "doubleml.DoubleMLDID.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.doublemldidcs method)": [[9, "doubleml.DoubleMLDIDCS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.doublemliivm method)": [[11, "doubleml.DoubleMLIIVM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.doublemlirm method)": [[12, "doubleml.DoubleMLIRM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.doublemllpq method)": [[13, "doubleml.DoubleMLLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.doublemlpliv method)": [[14, "doubleml.DoubleMLPLIV.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.doublemlplr method)": [[15, "doubleml.DoubleMLPLR.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.doublemlpq method)": [[16, "doubleml.DoubleMLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.doublemlssm method)": [[18, "doubleml.DoubleMLSSM.sensitivity_benchmark", false]], "sensitivity_plot() (doubleml.doublemlapo method)": [[4, "doubleml.DoubleMLAPO.sensitivity_plot", false]], "sensitivity_plot() (doubleml.doublemlapos method)": [[5, "doubleml.DoubleMLAPOS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.doublemlcvar method)": [[6, "doubleml.DoubleMLCVAR.sensitivity_plot", false]], "sensitivity_plot() (doubleml.doublemldid method)": [[8, "doubleml.DoubleMLDID.sensitivity_plot", false]], "sensitivity_plot() (doubleml.doublemldidcs method)": [[9, "doubleml.DoubleMLDIDCS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.doublemliivm method)": [[11, "doubleml.DoubleMLIIVM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.doublemlirm method)": [[12, "doubleml.DoubleMLIRM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.doublemllpq method)": [[13, "doubleml.DoubleMLLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.doublemlpliv method)": [[14, "doubleml.DoubleMLPLIV.sensitivity_plot", false]], "sensitivity_plot() (doubleml.doublemlplr method)": [[15, "doubleml.DoubleMLPLR.sensitivity_plot", false]], "sensitivity_plot() (doubleml.doublemlpq method)": [[16, "doubleml.DoubleMLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.doublemlssm method)": [[18, "doubleml.DoubleMLSSM.sensitivity_plot", false]], "set_fit_request() (doubleml.utils.globalclassifier method)": [[41, "doubleml.utils.GlobalClassifier.set_fit_request", false]], "set_fit_request() (doubleml.utils.globalregressor method)": [[42, "doubleml.utils.GlobalRegressor.set_fit_request", false]], "set_ml_nuisance_params() (doubleml.doublemlapo method)": [[4, "doubleml.DoubleMLAPO.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.doublemlcvar method)": [[6, "doubleml.DoubleMLCVAR.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.doublemldid method)": [[8, "doubleml.DoubleMLDID.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.doublemldidcs method)": [[9, "doubleml.DoubleMLDIDCS.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.doublemliivm method)": [[11, "doubleml.DoubleMLIIVM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.doublemlirm method)": [[12, "doubleml.DoubleMLIRM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.doublemllpq method)": [[13, "doubleml.DoubleMLLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.doublemlpliv method)": [[14, "doubleml.DoubleMLPLIV.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.doublemlplr method)": [[15, "doubleml.DoubleMLPLR.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.doublemlpq method)": [[16, "doubleml.DoubleMLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.doublemlssm method)": [[18, "doubleml.DoubleMLSSM.set_ml_nuisance_params", false]], "set_params() (doubleml.utils.dmldummyclassifier method)": [[37, "doubleml.utils.DMLDummyClassifier.set_params", false]], "set_params() (doubleml.utils.dmldummyregressor method)": [[38, "doubleml.utils.DMLDummyRegressor.set_params", false]], "set_params() (doubleml.utils.globalclassifier method)": [[41, "doubleml.utils.GlobalClassifier.set_params", false]], "set_params() (doubleml.utils.globalregressor method)": [[42, "doubleml.utils.GlobalRegressor.set_params", false]], "set_sample_splitting() (doubleml.doublemlapo method)": [[4, "doubleml.DoubleMLAPO.set_sample_splitting", false]], "set_sample_splitting() (doubleml.doublemlapos method)": [[5, "doubleml.DoubleMLAPOS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.doublemlcvar method)": [[6, "doubleml.DoubleMLCVAR.set_sample_splitting", false]], "set_sample_splitting() (doubleml.doublemldid method)": [[8, "doubleml.DoubleMLDID.set_sample_splitting", false]], "set_sample_splitting() (doubleml.doublemldidcs method)": [[9, "doubleml.DoubleMLDIDCS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.doublemliivm method)": [[11, "doubleml.DoubleMLIIVM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.doublemlirm method)": [[12, "doubleml.DoubleMLIRM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.doublemllpq method)": [[13, "doubleml.DoubleMLLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.doublemlpliv method)": [[14, "doubleml.DoubleMLPLIV.set_sample_splitting", false]], "set_sample_splitting() (doubleml.doublemlplr method)": [[15, "doubleml.DoubleMLPLR.set_sample_splitting", false]], "set_sample_splitting() (doubleml.doublemlpq method)": [[16, "doubleml.DoubleMLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.doublemlqte method)": [[17, "doubleml.DoubleMLQTE.set_sample_splitting", false]], "set_sample_splitting() (doubleml.doublemlssm method)": [[18, "doubleml.DoubleMLSSM.set_sample_splitting", false]], "set_score_request() (doubleml.utils.dmldummyclassifier method)": [[37, "doubleml.utils.DMLDummyClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.dmldummyregressor method)": [[38, "doubleml.utils.DMLDummyRegressor.set_score_request", false]], "set_score_request() (doubleml.utils.globalclassifier method)": [[41, "doubleml.utils.GlobalClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.globalregressor method)": [[42, "doubleml.utils.GlobalRegressor.set_score_request", false]], "set_x_d() (doubleml.doublemlclusterdata method)": [[7, "doubleml.DoubleMLClusterData.set_x_d", false]], "set_x_d() (doubleml.doublemldata method)": [[10, "doubleml.DoubleMLData.set_x_d", false]], "tune() (doubleml.doublemlapo method)": [[4, "doubleml.DoubleMLAPO.tune", false]], "tune() (doubleml.doublemlcvar method)": [[6, "doubleml.DoubleMLCVAR.tune", false]], "tune() (doubleml.doublemldid method)": [[8, "doubleml.DoubleMLDID.tune", false]], "tune() (doubleml.doublemldidcs method)": [[9, "doubleml.DoubleMLDIDCS.tune", false]], "tune() (doubleml.doublemliivm method)": [[11, "doubleml.DoubleMLIIVM.tune", false]], "tune() (doubleml.doublemlirm method)": [[12, "doubleml.DoubleMLIRM.tune", false]], "tune() (doubleml.doublemllpq method)": [[13, "doubleml.DoubleMLLPQ.tune", false]], "tune() (doubleml.doublemlpliv method)": [[14, "doubleml.DoubleMLPLIV.tune", false]], "tune() (doubleml.doublemlplr method)": [[15, "doubleml.DoubleMLPLR.tune", false]], "tune() (doubleml.doublemlpq method)": [[16, "doubleml.DoubleMLPQ.tune", false]], "tune() (doubleml.doublemlssm method)": [[18, "doubleml.DoubleMLSSM.tune", false]]}, "objects": {"doubleml": [[4, 0, 1, "", "DoubleMLAPO"], [5, 0, 1, "", "DoubleMLAPOS"], [6, 0, 1, "", "DoubleMLCVAR"], [7, 0, 1, "", "DoubleMLClusterData"], [8, 0, 1, "", "DoubleMLDID"], [9, 0, 1, "", "DoubleMLDIDCS"], [10, 0, 1, "", "DoubleMLData"], [11, 0, 1, "", "DoubleMLIIVM"], [12, 0, 1, "", "DoubleMLIRM"], [13, 0, 1, "", "DoubleMLLPQ"], [14, 0, 1, "", "DoubleMLPLIV"], [15, 0, 1, "", "DoubleMLPLR"], [16, 0, 1, "", "DoubleMLPQ"], [17, 0, 1, "", "DoubleMLQTE"], [18, 0, 1, "", "DoubleMLSSM"]], "doubleml.DoubleMLAPO": [[4, 1, 1, "", "bootstrap"], [4, 1, 1, "", "capo"], [4, 1, 1, "", "confint"], [4, 1, 1, "", "construct_framework"], [4, 1, 1, "", "draw_sample_splitting"], [4, 1, 1, "", "evaluate_learners"], [4, 1, 1, "", "fit"], [4, 1, 1, "", "gapo"], [4, 1, 1, "", "get_params"], [4, 1, 1, "", "p_adjust"], [4, 1, 1, "", "sensitivity_analysis"], [4, 1, 1, "", "sensitivity_benchmark"], [4, 1, 1, "", "sensitivity_plot"], [4, 1, 1, "", "set_ml_nuisance_params"], [4, 1, 1, "", "set_sample_splitting"], [4, 1, 1, "", "tune"]], "doubleml.DoubleMLAPOS": [[5, 1, 1, "", "bootstrap"], [5, 1, 1, "", "causal_contrast"], [5, 1, 1, "", "confint"], [5, 1, 1, "", "draw_sample_splitting"], [5, 1, 1, "", "fit"], [5, 1, 1, "", "sensitivity_analysis"], [5, 1, 1, "", "sensitivity_benchmark"], [5, 1, 1, "", "sensitivity_plot"], [5, 1, 1, "", "set_sample_splitting"]], "doubleml.DoubleMLCVAR": [[6, 1, 1, "", "bootstrap"], [6, 1, 1, "", "confint"], [6, 1, 1, "", "construct_framework"], [6, 1, 1, "", "draw_sample_splitting"], [6, 1, 1, "", "evaluate_learners"], [6, 1, 1, "", "fit"], [6, 1, 1, "", "get_params"], [6, 1, 1, "", "p_adjust"], [6, 1, 1, "", "sensitivity_analysis"], [6, 1, 1, "", "sensitivity_benchmark"], [6, 1, 1, "", "sensitivity_plot"], [6, 1, 1, "", "set_ml_nuisance_params"], [6, 1, 1, "", "set_sample_splitting"], [6, 1, 1, "", "tune"]], "doubleml.DoubleMLClusterData": [[7, 1, 1, "", "from_arrays"], [7, 1, 1, "", "set_x_d"]], "doubleml.DoubleMLDID": [[8, 1, 1, "", "bootstrap"], [8, 1, 1, "", "confint"], [8, 1, 1, "", "construct_framework"], [8, 1, 1, "", "draw_sample_splitting"], [8, 1, 1, "", "evaluate_learners"], [8, 1, 1, "", "fit"], [8, 1, 1, "", "get_params"], [8, 1, 1, "", "p_adjust"], [8, 1, 1, "", "sensitivity_analysis"], [8, 1, 1, "", "sensitivity_benchmark"], [8, 1, 1, "", "sensitivity_plot"], [8, 1, 1, "", "set_ml_nuisance_params"], [8, 1, 1, "", "set_sample_splitting"], [8, 1, 1, "", "tune"]], "doubleml.DoubleMLDIDCS": [[9, 1, 1, "", "bootstrap"], [9, 1, 1, "", "confint"], [9, 1, 1, "", "construct_framework"], [9, 1, 1, "", "draw_sample_splitting"], [9, 1, 1, "", "evaluate_learners"], [9, 1, 1, "", "fit"], [9, 1, 1, "", "get_params"], [9, 1, 1, "", "p_adjust"], [9, 1, 1, "", "sensitivity_analysis"], [9, 1, 1, "", "sensitivity_benchmark"], [9, 1, 1, "", "sensitivity_plot"], [9, 1, 1, "", "set_ml_nuisance_params"], [9, 1, 1, "", "set_sample_splitting"], [9, 1, 1, "", "tune"]], "doubleml.DoubleMLData": [[10, 1, 1, "", "from_arrays"], [10, 1, 1, "", "set_x_d"]], "doubleml.DoubleMLIIVM": [[11, 1, 1, "", "bootstrap"], [11, 1, 1, "", "confint"], [11, 1, 1, "", "construct_framework"], [11, 1, 1, "", "draw_sample_splitting"], [11, 1, 1, "", "evaluate_learners"], [11, 1, 1, "", "fit"], [11, 1, 1, "", "get_params"], [11, 1, 1, "", "p_adjust"], [11, 1, 1, "", "sensitivity_analysis"], [11, 1, 1, "", "sensitivity_benchmark"], [11, 1, 1, "", "sensitivity_plot"], [11, 1, 1, "", "set_ml_nuisance_params"], [11, 1, 1, "", "set_sample_splitting"], [11, 1, 1, "", "tune"]], "doubleml.DoubleMLIRM": [[12, 1, 1, "", "bootstrap"], [12, 1, 1, "", "cate"], [12, 1, 1, "", "confint"], [12, 1, 1, "", "construct_framework"], [12, 1, 1, "", "draw_sample_splitting"], [12, 1, 1, "", "evaluate_learners"], [12, 1, 1, "", "fit"], [12, 1, 1, "", "gate"], [12, 1, 1, "", "get_params"], [12, 1, 1, "", "p_adjust"], [12, 1, 1, "", "policy_tree"], [12, 1, 1, "", "sensitivity_analysis"], [12, 1, 1, "", "sensitivity_benchmark"], [12, 1, 1, "", "sensitivity_plot"], [12, 1, 1, "", "set_ml_nuisance_params"], [12, 1, 1, "", "set_sample_splitting"], [12, 1, 1, "", "tune"]], "doubleml.DoubleMLLPQ": [[13, 1, 1, "", "bootstrap"], [13, 1, 1, "", "confint"], [13, 1, 1, "", "construct_framework"], [13, 1, 1, "", "draw_sample_splitting"], [13, 1, 1, "", "evaluate_learners"], [13, 1, 1, "", "fit"], [13, 1, 1, "", "get_params"], [13, 1, 1, "", "p_adjust"], [13, 1, 1, "", "sensitivity_analysis"], [13, 1, 1, "", "sensitivity_benchmark"], [13, 1, 1, "", "sensitivity_plot"], [13, 1, 1, "", "set_ml_nuisance_params"], [13, 1, 1, "", "set_sample_splitting"], [13, 1, 1, "", "tune"]], "doubleml.DoubleMLPLIV": [[14, 1, 1, "", "bootstrap"], [14, 1, 1, "", "confint"], [14, 1, 1, "", "construct_framework"], [14, 1, 1, "", "draw_sample_splitting"], [14, 1, 1, "", "evaluate_learners"], [14, 1, 1, "", "fit"], [14, 1, 1, "", "get_params"], [14, 1, 1, "", "p_adjust"], [14, 1, 1, "", "sensitivity_analysis"], [14, 1, 1, "", "sensitivity_benchmark"], [14, 1, 1, "", "sensitivity_plot"], [14, 1, 1, "", "set_ml_nuisance_params"], [14, 1, 1, "", "set_sample_splitting"], [14, 1, 1, "", "tune"]], "doubleml.DoubleMLPLR": [[15, 1, 1, "", "bootstrap"], [15, 1, 1, "", "cate"], [15, 1, 1, "", "confint"], [15, 1, 1, "", "construct_framework"], [15, 1, 1, "", "draw_sample_splitting"], [15, 1, 1, "", "evaluate_learners"], [15, 1, 1, "", "fit"], [15, 1, 1, "", "gate"], [15, 1, 1, "", "get_params"], [15, 1, 1, "", "p_adjust"], [15, 1, 1, "", "sensitivity_analysis"], [15, 1, 1, "", "sensitivity_benchmark"], [15, 1, 1, "", "sensitivity_plot"], [15, 1, 1, "", "set_ml_nuisance_params"], [15, 1, 1, "", "set_sample_splitting"], [15, 1, 1, "", "tune"]], "doubleml.DoubleMLPQ": [[16, 1, 1, "", "bootstrap"], [16, 1, 1, "", "confint"], [16, 1, 1, "", "construct_framework"], [16, 1, 1, "", "draw_sample_splitting"], [16, 1, 1, "", "evaluate_learners"], [16, 1, 1, "", "fit"], [16, 1, 1, "", "get_params"], [16, 1, 1, "", "p_adjust"], [16, 1, 1, "", "sensitivity_analysis"], [16, 1, 1, "", "sensitivity_benchmark"], [16, 1, 1, "", "sensitivity_plot"], [16, 1, 1, "", "set_ml_nuisance_params"], [16, 1, 1, "", "set_sample_splitting"], [16, 1, 1, "", "tune"]], "doubleml.DoubleMLQTE": [[17, 1, 1, "", "bootstrap"], [17, 1, 1, "", "confint"], [17, 1, 1, "", "draw_sample_splitting"], [17, 1, 1, "", "fit"], [17, 1, 1, "", "p_adjust"], [17, 1, 1, "", "set_sample_splitting"]], "doubleml.DoubleMLSSM": [[18, 1, 1, "", "bootstrap"], [18, 1, 1, "", "confint"], [18, 1, 1, "", "construct_framework"], [18, 1, 1, "", "draw_sample_splitting"], [18, 1, 1, "", "evaluate_learners"], [18, 1, 1, "", "fit"], [18, 1, 1, "", "get_params"], [18, 1, 1, "", "p_adjust"], [18, 1, 1, "", "sensitivity_analysis"], [18, 1, 1, "", "sensitivity_benchmark"], [18, 1, 1, "", "sensitivity_plot"], [18, 1, 1, "", "set_ml_nuisance_params"], [18, 1, 1, "", "set_sample_splitting"], [18, 1, 1, "", "tune"]], "doubleml.datasets": [[19, 2, 1, "", "fetch_401K"], [20, 2, 1, "", "fetch_bonus"], [21, 2, 1, "", "make_confounded_irm_data"], [22, 2, 1, "", "make_confounded_plr_data"], [23, 2, 1, "", "make_did_SZ2020"], [24, 2, 1, "", "make_heterogeneous_data"], [25, 2, 1, "", "make_iivm_data"], [26, 2, 1, "", "make_irm_data"], [27, 2, 1, "", "make_irm_data_discrete_treatments"], [28, 2, 1, "", "make_pliv_CHS2015"], [29, 2, 1, "", "make_pliv_multiway_cluster_CKMS2021"], [30, 2, 1, "", "make_plr_CCDDHNR2018"], [31, 2, 1, "", "make_plr_turrell2018"], [32, 2, 1, "", "make_ssm_data"]], "doubleml.double_ml_score_mixins": [[33, 0, 1, "", "LinearScoreMixin"], [34, 0, 1, "", "NonLinearScoreMixin"]], "doubleml.rdd": [[35, 0, 1, "", "RDFlex"]], "doubleml.rdd.RDFlex": [[35, 1, 1, "", "aggregate_over_splits"], [35, 1, 1, "", "confint"], [35, 1, 1, "", "fit"]], "doubleml.rdd.datasets": [[36, 2, 1, "", "make_simple_rdd_data"]], "doubleml.utils": [[37, 0, 1, "", "DMLDummyClassifier"], [38, 0, 1, "", "DMLDummyRegressor"], [39, 0, 1, "", "DoubleMLBLP"], [40, 0, 1, "", "DoubleMLPolicyTree"], [41, 0, 1, "", "GlobalClassifier"], [42, 0, 1, "", "GlobalRegressor"], [43, 2, 1, "", "gain_statistics"]], "doubleml.utils.DMLDummyClassifier": [[37, 1, 1, "", "fit"], [37, 1, 1, "", "get_metadata_routing"], [37, 1, 1, "", "get_params"], [37, 1, 1, "", "predict"], [37, 1, 1, "", "predict_proba"], [37, 1, 1, "", "score"], [37, 1, 1, "", "set_params"], [37, 1, 1, "", "set_score_request"]], "doubleml.utils.DMLDummyRegressor": [[38, 1, 1, "", "fit"], [38, 1, 1, "", "get_metadata_routing"], [38, 1, 1, "", "get_params"], [38, 1, 1, "", "predict"], [38, 1, 1, "", "score"], [38, 1, 1, "", "set_params"], [38, 1, 1, "", "set_score_request"]], "doubleml.utils.DoubleMLBLP": [[39, 1, 1, "", "confint"], [39, 1, 1, "", "fit"]], "doubleml.utils.DoubleMLPolicyTree": [[40, 1, 1, "", "fit"], [40, 1, 1, "", "plot_tree"], [40, 1, 1, "", "predict"]], "doubleml.utils.GlobalClassifier": [[41, 1, 1, "", "fit"], [41, 1, 1, "", "get_metadata_routing"], [41, 1, 1, "", "get_params"], [41, 1, 1, "", "predict"], [41, 1, 1, "", "predict_proba"], [41, 1, 1, "", "score"], [41, 1, 1, "", "set_fit_request"], [41, 1, 1, "", "set_params"], [41, 1, 1, "", "set_score_request"]], "doubleml.utils.GlobalRegressor": [[42, 1, 1, "", "fit"], [42, 1, 1, "", "get_metadata_routing"], [42, 1, 1, "", "get_params"], [42, 1, 1, "", "predict"], [42, 1, 1, "", "score"], [42, 1, 1, "", "set_fit_request"], [42, 1, 1, "", "set_params"], [42, 1, 1, "", "set_score_request"]]}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function"}, "terms": {"": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 23, 25, 26, 28, 29, 30, 31, 32, 35, 39, 41, 42, 47, 49, 50, 51, 52, 53, 54, 56, 62, 64, 65, 66, 67, 69, 71, 72, 73, 77, 78, 79, 80, 81, 83, 86, 87, 89, 97, 98, 102, 103, 105, 112, 114, 115, 116, 117], "0": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 42, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 101, 102, 103, 105, 106, 107, 108, 109, 113, 114, 116], "00": [66, 68, 72, 73, 88], "000": [74, 102, 117], "00000": 68, "000000": [54, 56, 68, 72, 73, 83, 85, 114], "0000000": 102, "0000000000000010000100": [52, 83, 114], "000000e": [66, 68, 72, 73], "00000591": 76, "000006": [56, 76], "000017": 76, "000025": 71, "000034": 72, "000039": 71, "000064": 57, "000067": 71, "000076": 87, "000091": 71, "0001": [54, 72], "0001095602": 88, "0001224837": 88, "000204": 88, "000219": [16, 85], "000242": [17, 85], "000341": 71, "000442": 71, "00047580260495": 47, "000488": 71, "000494": 67, "0005": 54, "000522": 71, "0005a80b528f": 52, "000670": 71, "000743": 78, "000915799": 102, "0009157990": 102, "000943": [59, 60], "001": [47, 49, 50, 51, 52, 53, 58, 74, 86, 87, 88, 89, 102, 114, 117], "001051": 71, "001234": 73, "00133": 52, "00138944": [81, 89], "001403": 77, "001471": 68, "001494": [85, 86, 87], "0016": [51, 72], "001698": 68, "001714": 85, "0018": [51, 72], "0019": 54, "001907": 68, "002": 74, "002169338": 102, "0021693380": 102, "0021693381": 102, "002277": 59, "002290": 63, "0023": 49, "002388": 70, "002436": 67, "0026": 54, "002779": 78, "0028": [49, 51, 72], "002821": 79, "0028213335041910427": 79, "002866752": 88, "002983": 71, "003": [21, 22, 23], "003045": 68, "003074": 87, "003134": 76, "003187": 59, "003220": 56, "003328": 76, "0034": 64, "003404": 56, "003415": 56, "003427": 71, "003607": 60, "003779": 67, "003836": 76, "003924": 67, "003944": 59, "003975": 59, "004": 88, "00409412": [81, 89], "0042": [51, 72], "004253": 56, "004392": 67, "004526": 56, "004542": 68, "004688": 11, "0047": [51, 72], "004846": 79, "005339": [59, 60], "005857": 71, "005e": 87, "006055": 56, "006267": 60, "006425": 73, "0068101213851626": 70, "006922": 54, "006958": [59, 60], "007200016": 88, "007210e": 73, "00728": 114, "0073": 54, "007332": 61, "007332393760465": 61, "007421": 85, "00778625": 88, "0078540263583833": 70, "008": 79, "008023": 73, "008223": [59, 60], "008266e": 73, "008487": 54, "0084871742256079": 70, "008642": 85, "008883698": 89, "00888458890362062": 81, "008884589": 81, "008dbd": 74, "008e80": 74, "009": [74, 79], "009122": 76, "009255": 59, "009329847": 89, "009428": 61, "00944171905420782": 79, "00950122695463054": 81, "009501226954630540": 81, "009501227": 81, "009538": 74, "009645422": 50, "009656": 76, "00972": 54, "009790": 73, "009986": 76, "01": [4, 5, 6, 8, 9, 11, 12, 13, 16, 17, 18, 40, 47, 50, 51, 52, 53, 59, 60, 68, 72, 73, 74, 75, 76, 77, 86, 87, 88, 89, 102, 114, 117], "010213": 78, "010269": 71, "010277": 74, "010450": 50, "010940": 71, "011131": 76, "01119132": 88, "0112": 49, "011204": 68, "01128": 54, "011598": 76, "0118095": 50, "011823": 78, "01188": 74, "011988e": 76, "012002": 74, "01219": 52, "01274": 79, "012780": 73, "012831": 79, "013034": 79, "013128": 59, "013195": 74, "013313": 70, "013450": 68, "01351638": 50, "013593": 78, "013617": 73, "013677": 77, "0137": 88, "013712": 59, "01398951": 50, "013990": 102, "014": 77, "01403089": 50, "014080": [59, 60], "01439637": 88, "014432": 63, "014637": 71, "014681": 78, "014873e": 59, "015": 52, "015038": 61, "015552": 59, "015565": 76, "0156853566737638": 70, "015698": 76, "01574297": 76, "015743": 76, "015831": 59, "016011": 60, "016154": 71, "016200": [59, 60], "016315": 65, "01643": 115, "017": 52, "017140": 59, "017393e": 102, "017660": 68, "01772": 105, "017777e": 60, "017800092": 102, "0178000920": 102, "018": [52, 74], "018023": 75, "018092": 85, "018148": 76, "018508": 59, "019": 74, "01903": [52, 86, 112, 114], "01916030e": 88, "01925597": 50, "019439633": 102, "0194396330": 102, "0194396331": 102, "019596": 61, "019660": [17, 85], "01990373": 80, "019974": 73, "02": [59, 60, 72, 73, 76, 85, 87, 88], "020": 74, "02016117": 114, "020166": 76, "020271": 71, "020272": 68, "020360838": 102, "0203608380": 102, "0203608381": 102, "02052929": [81, 89], "02079162e": 88, "020819": 85, "02092": 114, "021269": [65, 66], "02128017": 88, "02152563": 88, "02163217": 50, "021690": 66, "021823": 70, "021866": 75, "021926": 61, "022181": 59, "022258": 68, "022295e": 59, "02247976": 50, "022768": 54, "022783": 78, "022915": 71, "022969": 73, "023020e": [72, 73], "023052": 60, "023256": 76, "023537": 70, "023563": 102, "0239059": 88, "023955": 73, "024266": 68, "024346": 59, "024355": 63, "024364": 103, "024401": [65, 66], "024604": 71, "024782": 76, "024926": 63, "025": [59, 60, 65, 66, 68, 74], "025077": [60, 102], "02528067": 69, "0253": 52, "025300e": 60, "0253678258": 88, "025443": 54, "025496": 59, "0257": 49, "025813114": 102, "0258131140": 102, "02584": 52, "025841": 68, "025964": 68, "026": 47, "026669": 73, "026723": 61, "026822": 70, "026966": 68, "02791": 54, "0281": 52, "028520": [59, 60], "028731": 85, "02897287": 62, "02900983": 76, "029010": 76, "029022": 60, "029209": 117, "029364": [103, 108], "029831": 76, "029910e": [72, 73], "02e": 51, "03": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 53, 56, 59, 60, 61, 67, 68, 72, 73, 76, 77, 78, 79, 88, 103, 108, 117], "030": 47, "030059": 85, "0301": 52, "03018": 13, "030346": 114, "03045": 53, "0307": 52, "030934": 76, "030962": 76, "031007": 70, "03113": 80, "031134": 86, "031156": 60, "031269": 54, "031639": 76, "031820": 60, "03191": 115, "03220": 116, "0323": 49, "032411": 74, "03244552": 86, "0325": 114, "03258": 70, "032580": 70, "032738": 70, "032941": 59, "032953": 78, "033265": 70, "033756": 61, "033946": [65, 66], "03395788": 88, "034065": 60, "03411": 114, "034226": 73, "03438": 53, "034690": 61, "034812763": 102, "0348127630": 102, "0348127631": 102, "034846": 72, "03489": [29, 50, 71], "035119185": 102, "0351191850": 102, "0351191851": 102, "035264": 60, "03536": 114, "03538": 52, "03539": 52, "035391": 54, "0354": 52, "035411": 114, "035441": 60, "03545": 52, "035471411": 88, "035545": 54, "035572": 54, "035730": 76, "03574": 54, "035762": 76, "035785": 60, "0359": 52, "036129015": 102, "0361290150": 102, "0361290151": 102, "036143": 76, "036147": 76, "036240": 56, "036729": 71, "0368": 49, "036945": 73, "03698487": 76, "036985": 76, "037008": [65, 66], "037114": 68, "0374": 52, "037504": 60, "037509": 80, "037529": 68, "037747": [59, 60], "038103": 68, "038845": 59, "039036": 59, "039141": 56, "03917696": [89, 102], "03920960e": 88, "039310e": 61, "039661": 68, "039895": 68, "039966": 74, "039991e": 59, "04": [22, 35, 51, 56, 59, 60, 72, 73, 76, 77, 78, 88, 117], "040010": 68, "040079": 60, "040112": 102, "040139": [59, 60], "040533": [89, 102], "04053339": 102, "040562": 59, "040629": 15, "040688": 59, "040784": 56, "0408": 59, "040912": 60, "040919": 60, "04096245": 88, "041147": 61, "04123521": 88, "041284": 61, "04129480": 88, "041387": 61, "041459": 73, "041491e": 61, "04165": 87, "0418": 49, "041831": 61, "041925": 59, "042": 74, "042034": 79, "042249": 60, "042265": 61, "0425": 86, "0428": 80, "042804": 68, "042822e": 73, "042844e": 76, "043108": 73, "0433": 49, "0434e374": 52, "04387": 86, "043998": 60, "044113": 61, "04415": 52, "044176": 68, "044239": 68, "04424": 52, "04444978": 102, "044449780": 102, "0445": 86, "04465": 50, "044704": 60, "04486": 114, "04487585": [103, 108], "04491": 87, "044929": 68, "04497975": [103, 108], "04501612": 102, "04502": [86, 89, 102], "045144": 71, "045172": 68, "045313": 59, "045379": 114, "04552": 71, "045553": 61, "045624": 63, "04563": 86, "045638": 59, "04573173": 88, "045754": 76, "04586": 86, "045932": 76, "045984": 68, "045993": 86, "046": 74, "04625": 86, "046451": 68, "046525": 85, "046527": 61, "04653976": 76, "046540": 76, "046587": 60, "0466028": 50, "04671050": 88, "046728": 78, "04682310e": 88, "046922": 86, "047": 74, "047194": 11, "047239": 68, "047288": 85, "047652e": 60, "047724": 59, "047873": 68, "047954": 71, "048220": 70, "048308": 66, "048476": 68, "048699": 80, "048723": 86, "048853": 60, "049264": 56, "04973": 60, "05": [35, 47, 49, 50, 51, 52, 53, 59, 60, 61, 64, 69, 71, 72, 73, 74, 76, 77, 79, 86, 87, 88, 89, 102, 114, 117], "05039": 78, "050494e": 60, "050538": 60, "051": 52, "05128508": 88, "051644": 74, "051651": 77, "051867e": 61, "05191142": 88, "052": 77, "052000e": 73, "052023": 68, "052298": 76, "052380": 59, "052488": 66, "052502": 76, "052745": 61, "053": [52, 87], "053049": 60, "0533": 49, "053331": 61, "053342": 73, "053389": 102, "053436": 12, "053541": 76, "053558": 61, "053849e": 59, "054": [47, 52, 77], "054068": 71, "054162": 71, "054348": 102, "054370": 61, "054529": 102, "054556934": 88, "054771e": 76, "055165": 78, "055171": 60, "055439": [70, 73], "055493": 79, "055680": 102, "056": 77, "056499": 66, "056745": 59, "056764": 59, "056915": 68, "057095": 76, "0576": [51, 72], "057762": 76, "057792": 59, "057962": 61, "058042": 102, "058276": 73, "058375": 56, "058463": 76, "058508": 80, "05853715": 88, "058595": 59, "05891": 87, "0590": 49, "059128": 59, "059384": 76, "059627": 73, "059630": 63, "059685": 76, "06": [21, 22, 23, 56, 59, 60, 61, 72, 73, 76, 85, 86, 88], "060016": 56, "06008533": 87, "060201": 76, "060212": [72, 73], "060417": 59, "060581": 69, "060845": 102, "060933": 59, "061": 74, "0611": 49, "06111111": 52, "0615": 49, "06161": 87, "062": [77, 87], "062414": 73, "062507": 76, "0628": 49, "062964": 102, "062988": 59, "063017": 56, "0632": 49, "063234e": 60, "063327": 68, "0635": 49, "063593": 60, "0636": 49, "063700": 59, "0638": 49, "063881": 87, "0640": 49, "064161": 73, "064213": 60, "06428": 72, "064280": 72, "0645": 49, "0646222": 51, "0647": 49, "0649": 49, "065": 79, "0653": 49, "065356": [65, 66], "065368": 70, "0654": 49, "065451": 73, "0655": 49, "065725": 61, "0659": 49, "065969": 87, "065976": 68, "0662": 49, "066295": 68, "066464": 78, "066889": 76, "0669": 49, "06692492": 88, "067046e": 59, "0671": 49, "067212": 68, "067240": 76, "06724028": 76, "0673": 49, "0675": 49, "067528": 79, "067721": 102, "068073": 60, "06827": 78, "06834315": 62, "068377": 73, "068514": 59, "068700": 85, "068934": 56, "06895837": 50, "069443": 56, "0695854": 50, "069589": 68, "069600": 73, "069882e": 59, "07": [59, 60, 73, 76, 77, 79, 88], "070020": 76, "070196": 61, "0701961897676835": 61, "0702127": 50, "0704": 49, "070433": 68, "070497": 79, "070534": 18, "070552": 59, "070574e": 73, "07065401": 88, "0707": 49, "070751": 59, "07085301": 87, "070884": 76, "0711": 49, "071285": 102, "07136": [50, 71], "071362": 59, "071488e": 61, "0716": 49, "07168291": 50, "071777": 86, "071782": [17, 85], "0719": 49, "07202564": [65, 66], "07222222": 52, "072293": 75, "072516": 68, "072605": 56, "0727": 87, "073": 77, "073013": 76, "073207": 71, "073275": 59, "073384": 68, "07347676": 50, "07350015": [29, 32, 50, 71], "073520": 61, "0736": 49, "07366": [52, 86], "073694": 60, "0739130271918385": 70, "073929": 70, "0743": 49, "074304": 102, "07436521": 88, "074426": 76, "07456127": 50, "074617": 60, "07479278": 78, "074927": 56, "075261": 63, "075384": 76, "07538443": 76, "075403": 74, "07544271e": 88, "07561": 114, "07564554e": 88, "0758": 79, "075809": 56, "075869": 86, "075942": 70, "076019": 72, "076156": 102, "076179312": 102, "0761793120": 102, "076322": 76, "076347": 61, "0765": 52, "076596": 59, "076684": 114, "07685043": 88, "07689": 52, "07691847": 88, "076953": [65, 66], "076971": 54, "077144e": 60, "077161": 73, "07727773e": 88, "077319": 76, "077502": [103, 108], "077555": 59, "077592": 68, "077702": 56, "0777777777777778": 86, "07777778": [52, 86], "077840": 73, "077883": 76, "077923e": 59, "07796": 87, "078": 74, "078017": 59, "078096": 102, "078207": 54, "07828372": 102, "078426": 85, "078474": 102, "078709": 60, "078810": 76, "079085": 54, "07915": 52, "07919896": 88, "07942v3": 115, "079458e": 72, "079500e": 59, "07961": 78, "07978296": 88, "08": [61, 73, 76, 79, 87], "08005229": 88, "08031571": 88, "080854": 73, "08091581": 88, "080947": 54, "081": 52, "081100": 76, "081230": [59, 60], "081396": 66, "081488": 71, "08154161": 88, "08181827e": 88, "082": 74, "0820": 49, "082197": 68, "082263": 14, "082297": 88, "082400e": 59, "082574": 12, "082804": 63, "082858": 60, "082934": 73, "082973": 71, "083258": 102, "083318": 102, "08333333": 52, "08333617": 88, "0835771416": 50, "0836": 68, "08364": 68, "083706": 79, "083750": 73, "083949": 79, "084": 50, "084156": 60, "084184": 61, "0841842065698133": 61, "084212": 67, "084269": 73, "084323": 59, "084337": 87, "084633": 65, "085": 47, "0853505": 50, "085395": 59, "085566": 61, "085592": 68, "085671": 59, "085965": 73, "086004": 68, "08602774e": 88, "0862": 112, "086264": 61, "08664208": 88, "086679": 86, "086889": 65, "0872": 49, "087222": 60, "087561": 59, "087634": 59, "087745": 60, "087947": 76, "088048": 76, "088282": 66, "088357": 76, "08848": 86, "088482": [17, 85], "088504e": 14, "08888889": 52, "0894": 49, "08968939": 50, "089964": 70, "08e": 51, "09": [59, 60, 61, 72, 73, 76, 85], "09000000000000001": 86, "090025": 73, "09015": 49, "090255": 76, "090436": 60, "091179e": 59, "091263": 70, "091391": 102, "091406": 103, "091535": 59, "0916": 49, "091824": 60, "091992": 75, "092229": 79, "092247": 76, "092263": 87, "092365": 102, "092919": 105, "092935": 59, "093043": 76, "09310496": 102, "093153": 76, "093474": 76, "09347419": 76, "09351167": 87, "093746": 102, "093950": 71, "094026": 71, "094118": 76, "094378e": 59, "094381": 71, "09444444": 52, "094581e": 60, "09477686": 88, "094829": 87, "094999": 76, "095104": 56, "09512139": 88, "095654": 59, "095781": 6, "095785": 56, "09603": 112, "096337": 71, "096418": 56, "096550": 65, "096616": 85, "096688": 60, "096741": 62, "09682314": 87, "096915": 79, "097": 77, "097009": 68, "097157": 79, "097468": 61, "09779675": 102, "097796750": 102, "09792453": 88, "098": 51, "098256": 76, "09830758": 78, "098308": 78, "098317": 73, "098319": 76, "0986": 49, "098712": 76, "09879814e": 88, "098901": 68, "099": 77, "099001": 60, "099307": 60, "099647": 75, "099670": 73, "099731": [59, 60], "09980311": 102, "09988": 115, "0_": 28, "0ff823b17d45": 52, "0x1747bdd4520": 54, "0x1747bdd6b90": 54, "0x2920d7b7150": 75, "0x7f98e2fe8560": 79, "0x7fd59892a7e0": 117, "0x7fd598aea8a0": 87, "0x7fd598f12930": 86, "0x7fd598f67860": 86, "0x7fd598ff1010": 108, "0x7fd599496810": 102, "0x7fd59980a270": 86, "0x7fd59a029880": 102, "0x7fd59a154320": 102, "0x7fd59a156d20": 102, "0x7fd59a22bad0": 103, "0x7fd59a8b77a0": 87, "0x7fd59ae3cfe0": 87, "0x7fd59b220320": 87, "1": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 36, 37, 38, 41, 42, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116], "10": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 27, 29, 30, 32, 47, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 102, 103, 108, 112, 114, 115, 117], "100": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 29, 31, 32, 50, 52, 53, 59, 60, 62, 64, 67, 68, 69, 71, 74, 79, 80, 81, 83, 85, 86, 87, 88, 89, 102, 103, 108, 114, 116], "1000": [11, 13, 48, 57, 58, 62, 63, 65, 66, 67, 69, 70, 72, 73, 77, 78, 79, 82, 87], "10000": [47, 59, 60, 63, 72, 73, 76], "100000e": 73, "100044": 66, "100154": 70, "100208": 85, "100356": 61, "10038": 78, "100385": 70, "10039862": [80, 87], "100517": 102, "100715": 59, "10079785": 87, "100807": [59, 60], "100858": 78, "10089588": 76, "100896": 76, "10092": 73, "100923": 76, "100_000": 74, "101": [21, 22, 23, 49, 77, 85, 87, 115, 116], "10126": 73, "10127930": 102, "101279300": 102, "1015": [51, 72], "1016": [21, 22, 23, 49], "1016010": 51, "1018": 73, "101998": 56, "102": [83, 85, 87, 114, 116], "1021111": 88, "10235": 73, "10258": 73, "102616": 61, "102775": 61, "10299": 72, "103": [59, 71, 77, 80, 85, 87, 116], "10307": 102, "1031": 73, "103189": 73, "10348": 72, "103497": 76, "1038": 73, "103806": 61, "103951906910721": 61, "103952": 61, "10396": 72, "104": [51, 72, 80, 85, 87, 116], "10406": 73, "104087": 59, "1041": 49, "10414": 73, "104492": 68, "1045303": 50, "104787": 71, "104849": 59, "105": [28, 50, 68, 71, 85, 87, 116], "105318": 76, "1054": 52, "105461": 85, "1055": 49, "106": [52, 74, 85, 87, 116], "10607": [54, 83, 114], "10618": 73, "10637173e": 88, "106391": 102, "1065": [64, 69, 70], "106595": 87, "106746": 76, "106952": 67, "107": [52, 79, 85, 87, 116], "107073": 61, "107156": 68, "107295": 102, "1073": 73, "107413": 59, "10747": [54, 83, 114], "10799": 73, "108": [85, 87, 112, 115, 116], "1080": [29, 32, 49, 50, 71], "10824": [54, 83, 114], "108257e": 73, "108259": 56, "10831": [54, 83, 114], "10878571": 76, "108786": 76, "109": [59, 85, 87], "109005": 76, "10903": 72, "109069": 102, "109079e": 76, "109273": 71, "109277": 68, "10928": 73, "1093": 64, "109454": 73, "109470": 60, "1096": 49, "10967": 72, "109861": 114, "1099472942084532": 57, "10e": [61, 76], "11": [15, 47, 50, 51, 52, 53, 54, 56, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 76, 77, 78, 79, 81, 83, 85, 86, 87, 88, 89, 102, 103, 108, 114, 117], "110": [85, 87, 116], "110081": 68, "1101": 73, "11019365749799062": 79, "110194": 79, "110359": 71, "110365": 79, "110681": 78, "1107": 73, "11071087": [80, 87], "110717": 102, "1109": 73, "110902": 61, "110902411746278": 61, "111": [60, 85, 87, 116], "1111": [19, 20, 30, 48, 50, 58, 64, 71, 79, 82, 87, 103, 108, 112], "111164": 75, "11120": 73, "1113227": 88, "1117": [64, 69, 70], "1118": 51, "11199615e": 88, "112": [52, 85, 87, 116], "1120": 72, "112078": 85, "11208236": [81, 89], "1122": 73, "112216": 61, "1129": 73, "113": [19, 85, 87, 116], "113022": 68, "11311": 72, "113149": 68, "113207": 76, "1132618": 88, "113270": 61, "113415": 73, "11375": 73, "113780": 71, "113952": 68, "11399": 72, "114": [85, 87, 116], "1142764": 88, "1144500": 50, "11447": 78, "114530": 65, "1145370": 50, "114570": 60, "11458": 73, "114647": 61, "1147": 49, "1148": 73, "114834": 73, "11488": 73, "11495": 73, "114989": 56, "115": [85, 87, 116], "11500": [72, 117], "115060e": 76, "1151610541568202": 70, "115296e": 73, "115297e": 72, "1155142425200442": 70, "11552911": 78, "1155727137": 88, "11559": 73, "115636": 60, "11570": 72, "115792e": 73, "115901": 56, "115972": 59, "116": [85, 87, 116], "116027": 61, "11617": 73, "116274": 61, "1164683": 88, "116569": 73, "1166": [72, 115], "1167": 72, "11673": 73, "11675": 73, "117": [59, 85, 87], "1170": 78, "11700": 117, "117072": 65, "117112": 60, "117242": 76, "11724226": 76, "117366": 76, "1174": 88, "11743": 117, "11750": 73, "1176": 49, "1177": 49, "117710": 61, "11792": 51, "11796": 73, "118": [85, 87], "1180023": 88, "11802": 73, "1182": 51, "11823404": 79, "118255": 76, "1186": 51, "118601": 71, "11861": 51, "1187339840850312": 71, "11879": 73, "118799": 73, "118826": 88, "118938e": 87, "118952": 71, "119": [79, 85, 87, 116], "11932": 73, "11935": 78, "1195391": 88, "119766": 76, "1198": [50, 71], "12": [18, 47, 48, 49, 50, 51, 52, 53, 54, 56, 58, 59, 60, 61, 62, 65, 66, 67, 68, 70, 71, 72, 73, 74, 76, 77, 78, 79, 81, 83, 85, 86, 87, 88, 89, 102, 103, 108, 112, 114, 115, 116, 117], "120": [53, 62, 70, 74, 80, 85, 87, 116], "12002": 72, "1202": 115, "120468": 76, "12046836": 76, "120567": [65, 66], "120721": 71, "1208": 79, "12097": [19, 20, 30, 50, 64, 71, 82, 112], "121": [73, 85, 87, 116], "1210": 73, "12101": 73, "12105472": 102, "121054720": 102, "1211": 73, "1213405": 50, "121399": 73, "1214": 102, "121584e": 76, "121711": 73, "121774": 67, "121824": 60, "12196389e": 88, "122": [21, 22, 23, 49, 77, 83, 85, 87, 115, 116], "12214": 51, "12223182e": 88, "1223604": 88, "122408": 61, "122421": 68, "122777": 102, "123": [35, 51, 52, 72, 79, 85, 87, 116, 117], "1230": 73, "1230966": 88, "123192": 79, "12323": 73, "1234": [47, 48, 49, 54, 57, 58, 77, 82, 86, 88, 102], "12348": 79, "1238": 73, "123917": 73, "124": [85, 87], "12410": 73, "124306": 70, "124480": 70, "124805": 72, "1248124": 88, "124825": 60, "125": [85, 116], "12500": 72, "1250312": 88, "125065": 102, "12539340": 102, "1255": 73, "12579": 73, "1258": 50, "126": [85, 116], "12606": 73, "12612": 73, "126777": 102, "126802": 73, "12689": 73, "127": [21, 85, 116], "127006": 73, "12705095": [89, 102], "12707800": 50, "12715704": 88, "1272404618426184": 70, "1272584": 88, "127337": 68, "12752825": 102, "127563": 78, "1277": 74, "127778": 73, "127889": 85, "128": [51, 85, 116], "12802": 51, "12814": 73, "128229": 68, "128300e": 60, "128312": 76, "128408": 71, "1285": 49, "12861": 73, "128651": 60, "129": [71, 74, 85, 116], "12945": 115, "1295": [49, 73], "129514": 73, "12955": 72, "129606": 59, "129798": 59, "1298": 73, "12980769e": 88, "12983057": 87, "13": [22, 23, 25, 27, 48, 50, 51, 52, 53, 54, 56, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 70, 71, 72, 73, 76, 77, 78, 79, 81, 83, 85, 86, 87, 88, 89, 102, 103, 108, 114, 117], "130": [52, 65, 71, 85, 116], "130122": 78, "13034980e": 88, "130370": 61, "1306": 78, "130829": 76, "13084": 87, "13091": 73, "1309844442144665": 70, "131": [85, 116], "13102231": 87, "131024": 70, "13111617182232455658606164757678839091": 88, "13119": 78, "1312": 117, "131211": 73, "1313": [51, 117], "13137893e": 88, "131483": 68, "1318": 49, "131842": 68, "132": [52, 59, 71, 85, 116], "13208": 117, "1321": [72, 117], "132248": 85, "1323": 88, "1324": [51, 72], "132454": 63, "132481": 85, "1325": 51, "13257": 72, "132671": 61, "132903": 73, "132982": 59, "133": [52, 83, 85, 115, 116], "13300": 73, "133202": 73, "133421": 73, "13356": 73, "133596": 76, "133839": 68, "13398": 79, "133f5a": 74, "134": [71, 80, 85, 116], "1340371": 49, "1341": 51, "134146": 73, "1342": 73, "134211": 76, "1343": 72, "134542": 59, "134567": 73, "1346035": 51, "134687": 73, "13474": 73, "134765": 73, "134784": 87, "134784e": 59, "1348": 72, "1349": 78, "13490": 73, "135": [52, 85, 87, 116], "13505272": 50, "135142": 60, "135344": 56, "135352": 8, "135379": 102, "135665": 60, "135707": 86, "135856": 76, "13585644": 76, "135871": 71, "136": [54, 71, 79, 85, 116], "1360": 51, "13602": 79, "136089": 71, "1361": 73, "136102": 59, "1362430723104844": 70, "13642": 73, "136442": 71, "1366": 74, "136836": 71, "137": [21, 52, 54, 85, 116], "1371": 73, "137165": 87, "137213": 60, "137396": 76, "1378": [73, 88], "137999": 87, "138": [85, 116], "1380": 72, "138068": 65, "13809": 73, "138264": 79, "138378": 61, "1384": 72, "1386": 49, "13868238": 102, "138682380": 102, "138698": 102, "1387": 49, "138851": 65, "13893": 73, "139": [79, 85, 114], "139117e": 59, "139491": 102, "139508": 68, "13956": 78, "1398": 73, "1399": 49, "14": [48, 50, 51, 52, 53, 54, 56, 58, 59, 60, 61, 62, 65, 66, 67, 68, 70, 71, 72, 73, 76, 77, 78, 79, 81, 83, 85, 86, 87, 88, 89, 102, 103, 108, 114, 115, 117], "140": [53, 62, 68, 73, 80, 85, 116], "1400": 73, "14000073": 88, "140073": 59, "140081": 70, "1401": 49, "14018": 87, "140770": [59, 60], "140833": 61, "140861": 50, "140926": 76, "141": [73, 85, 116], "141002": 60, "141098e": 73, "14114": 78, "1412233": 88, "14141": 73, "141460": 56, "141546": 102, "141820": 61, "14192930383941444649505366687273798594100": 88, "1419293038394144464950536668727379859410013111617182232455658606164757678839091812132023242628313642435254555765718996410152133344748516263677477808284878892": 88, "141929303839414446495053666872737985941002567925273537405969708186939597989913111617182232455658606164757678839091410152133344748516263677477808284878892": 88, "141929303839414446495053666872737985941002567925273537405969708186939597989913111617182232455658606164757678839091812132023242628313642435254555765718996": 88, "1419293038394144464950536668727379859410025679252735374059697081869395979899812132023242628313642435254555765718996410152133344748516263677477808284878892": 88, "142": [85, 116], "14200098": 102, "142119": 59, "142270": 63, "142382": 59, "1424": 86, "14268": 87, "14281403493938022": 86, "14289": 73, "143": [83, 85, 116], "143342": 60, "143495": 85, "1435": 73, "143534": 59, "14368145": 102, "144": [85, 116], "14400": 72, "14405": 73, "14406": 73, "144084": 61, "1441": 49, "144137": 62, "144241": 65, "1443": 73, "144500e": 73, "144669": 76, "1447": 73, "144800": 61, "144908": 75, "144971": 72, "145": [85, 116], "145027": 56, "145245": 76, "14532650": 102, "145625": 76, "145748": 102, "14587": 73, "146": [85, 116], "146037": 76, "146087": 114, "146142808990006": 61, "146143": 61, "14625": 73, "1463386": 88, "146435": 60, "1465": 51, "146641": 102, "14667": 73, "1468115": 50, "146973": 61, "1469734445741286": 61, "147": [85, 116], "147015e": 73, "14702": 54, "147121": 76, "14744": 73, "1475952": 88, "14772": 73, "1479": 73, "14790924": 102, "147909240": 102, "147927": 54, "14798": 73, "148": [85, 116], "148005": 56, "14803": 73, "148134": [59, 60], "148161": 76, "14845": 54, "1485": 73, "148750e": [72, 73], "148790": 73, "148802": 73, "149": [85, 116], "1492": 47, "149215e": 60, "149228": 79, "149285": 76, "149472": 79, "149714": 71, "14984": 73, "149858": [16, 85], "149898": 76, "15": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 20, 48, 50, 51, 52, 56, 58, 59, 60, 61, 62, 65, 67, 68, 69, 70, 71, 72, 73, 76, 77, 78, 79, 83, 85, 86, 87, 88, 89, 102, 103, 108, 114, 117], "150": [28, 52, 79, 85, 116], "15000": [51, 72], "150000": 51, "15000000000000002": [61, 73, 76, 86], "150000e": 73, "1502": 50, "150200": 71, "150334": 73, "150408": 50, "150614": 54, "150719e": 72, "151": [85, 116], "151047e": 65, "151063": 59, "15113": 73, "151636": 61, "151819": 76, "15194": 72, "152": [74, 85, 116], "152034": 73, "152148": [59, 60], "152353": 60, "152706": 87, "15285": 73, "152896": 70, "152926": 63, "153": [79, 85, 116], "1530959776797396": 61, "153096": 61, "153119": 61, "153314": 60, "15347": 73, "15354": 78, "153587": 71, "153633": 54, "153639": 88, "153935": 60, "154": 85, "1540316": 88, "15430": 117, "154421": 102, "1545": 73, "154557": 76, "154758": 102, "154828": 61, "154890": 70, "155": [85, 116], "155000": 72, "155025": 76, "155120": 76, "155160": 56, "155174": 56, "155423": 56, "155516": 75, "15556": 73, "1557093": 50, "156": [85, 116], "1560": 73, "156021": 76, "156169": 60, "156202": [59, 60], "156317": [59, 60], "1564": 102, "156545": 102, "156684": 60, "1569": 73, "156969": 61, "157": [60, 85, 116], "157091": 102, "157154": 59, "1576": 73, "157733": 70, "1577657": 50, "157e": 87, "158": [85, 116], "158007": 76, "15815035": 51, "158178": 61, "1582": 73, "1586": 73, "158697": 102, "158726": 85, "1589": 73, "15891559": 76, "158916": 76, "159": 116, "15916": 49, "159386": 78, "1596": 52, "159633e": 60, "159841": 59, "159959": 73, "16": [6, 47, 48, 50, 51, 52, 53, 56, 59, 60, 61, 66, 67, 68, 71, 72, 73, 76, 77, 78, 79, 83, 85, 86, 87, 88, 89, 102, 103, 108, 114, 117], "160": [53, 62, 80, 116], "1604": 51, "160780710": 88, "160836": 68, "160932": 61, "161": [52, 115, 116], "161049": 60, "161141": 71, "161198": 75, "161236": 76, "161243": 76, "161269": 59, "161288": [60, 70], "161543": 73, "1619": 51, "162": 116, "16201": 73, "16211": 72, "162153": 76, "1622": 73, "16241": 73, "162436": 79, "162593": 70, "1626685": 50, "162683": 79, "162710": 61, "1628": 72, "16285489": 88, "162930": 73, "163": [73, 116], "163194": 76, "163566": 73, "163895": 61, "164": [56, 77, 116], "164034": 102, "1643265": 88, "164467": 72, "164608": 76, "164698": 67, "1648": 49, "164801": 76, "164805": 61, "164864": 71, "165": 116, "16500": 72, "165178": 76, "16536299": 102, "165362990": 102, "16539906e": 88, "1654": 73, "165419": 76, "16553": 72, "165549": 114, "165707": 56, "16590": 73, "16597": 73, "166": 116, "1661": 72, "166238": 70, "166375": 85, "166517": 68, "1666094": 88, "1667778": 88, "167": [51, 72, 116], "16725": 73, "167547": 76, "167581e": 59, "1676": 73, "167765": 73, "167993": 102, "168": 116, "16803512": 102, "168089": 70, "168092": 102, "1681": [49, 72], "168195": 78, "168614": 76, "168931": 76, "169": [52, 116], "1691": [49, 73], "16910": 73, "169117": 79, "169196": 76, "169230e": 61, "16951": 73, "16984": 73, "17": [48, 50, 51, 52, 56, 59, 60, 67, 68, 71, 72, 73, 76, 77, 78, 79, 83, 85, 86, 87, 88, 89, 102, 103, 108, 114, 117], "170": 116, "1704": 73, "170705": 85, "170709e": 60, "17083": 73, "171": 116, "1712": 115, "1714": 51, "171575": 76, "171696": 85, "171815": 86, "171833": 60, "171848e": 59, "171878": 74, "171942": 73, "172": [77, 116], "172022": 102, "172083": 60, "172628": 70, "172793": 76, "173": 116, "173504": 66, "17372": 73, "1738": 73, "17385178": 86, "173969": 102, "173e": 77, "174": 116, "174106": 78, "174185": 76, "174499": 102, "174516e": 76, "17453": 73, "1746": 73, "174835": 60, "174968": 72, "17499": 73, "175": 116, "1751": 72, "175176": 76, "17522": 73, "175254": 72, "175284": 61, "175369": 60, "175635027": 50, "17576": 73, "175894": 79, "175931": [85, 86, 87], "17605247": 88, "176495": 76, "17655394": 76, "176554": 76, "176929": 102, "177": [115, 116], "177007": 76, "17700723": 76, "177043": [59, 60], "1773": 73, "1774": 49, "177463": 75, "177496": 76, "177611": 76, "177740": 59, "177751": 76, "17778": 73, "177830": 60, "17799": 73, "177995": 76, "178": 116, "178169": 65, "178218": 60, "17823": 52, "178704": 102, "178763": 76, "178934": 102, "179": [65, 116], "179026": 60, "179101": 85, "1795850": 50, "179588e": 76, "179777": 60, "1798913180930109556": 74, "18": [48, 50, 51, 52, 54, 59, 60, 67, 68, 69, 71, 72, 73, 76, 77, 78, 79, 83, 85, 86, 87, 88, 102, 114, 117], "180": [53, 62, 80, 116], "180143": 56, "18015": 73, "180176e": 73, "180262": 60, "180271": 68, "1803": 49, "18030": 73, "180575": [65, 66], "1807": [49, 73], "1809": 115, "180951": 76, "181": 116, "1810286110": 88, "181195": 74, "1812": 73, "1814": 49, "18141": 73, "181446": 102, "182": 116, "1820": 49, "182427": 60, "182633": 76, "182849": 76, "183": [52, 72, 87, 116], "183339": 59, "183373": 87, "183526": 61, "183553": 77, "18356413": 87, "18368": 73, "183855": 86, "183888": 71, "184": [52, 115, 116], "184224": 56, "184247": 59, "184347": 60, "185": [51, 52], "18500": 73, "185130": 77, "1855": 73, "185984": 59, "186": [73, 116], "18604": 73, "1862": 49, "186237": 60, "18631": 73, "18637": 112, "186589": 56, "18666": 73, "186689": 87, "186735": 76, "18678094e": 88, "186836": 76, "187": 116, "187153": 102, "187664": 59, "187690": 76, "18789": 73, "188": [74, 116], "188175": 76, "1881752": 76, "188223": 76, "188400": 60, "1887": [85, 86, 87], "188760": 68, "18888149e": 88, "188882": 60, "188991": 102, "189": [52, 77, 116], "189195": 73, "189248": 59, "189293": 73, "1895815": [29, 50, 71], "189737": 76, "189739": 56, "189927": 73, "189998": 76, "19": [48, 50, 51, 52, 59, 60, 68, 70, 71, 72, 73, 76, 77, 78, 79, 85, 86, 87, 88, 102, 114, 117], "190": [52, 116], "19000": 73, "190096": 102, "19031969": 76, "190320": 76, "19033538": 50, "190648": 11, "19073905e": 88, "190809": 76, "190892": 79, "1909": [29, 50, 71], "190915": 61, "190921": 66, "190976": 77, "190982": 76, "191": [52, 115, 116], "191192": 59, "1912": 115, "191223": 60, "1912705": 82, "191294": 60, "191534": 72, "191716": 73, "1918": 49, "192": 116, "1922": 73, "192240": 102, "192505": 75, "192526": 78, "19252647": 78, "192539": [17, 85], "192587": 76, "192952": 56, "193": 116, "193060": 76, "193253": 59, "193285": 59, "193308": [17, 85], "193341": 60, "193375": 68, "19374710e": 88, "19382": 73, "193849": 77, "19385": 73, "193f0d909729": 52, "194": [69, 73, 116], "194092": 59, "1941": 51, "19413": [72, 73], "194303": 60, "194601": 62, "195": 116, "19508": 79, "19508031003642462": 79, "19509680e": 88, "1953701": 88, "195377": 76, "195396": 76, "195547": 73, "195564": 71, "19559": [51, 72], "195761": 76, "195781": 59, "1959": 115, "195963": 68, "196": 116, "196189": 76, "196437": 73, "196478e": 60, "196655": 68, "19680840": 102, "196e": 35, "197": 116, "1970": 73, "197000e": 73, "19705": 73, "197225": [54, 83, 114], "1972250000001000100001": [52, 83, 114], "1974": 73, "197424": 86, "197484": 102, "19756": 73, "19758": 73, "197600": 63, "197711": 73, "197920": 59, "19793": 73, "19794": 73, "198": 116, "198218": 71, "19824": 73, "198351": 76, "198493": 68, "198503": 77, "198549": 54, "198687": 51, "1988": [48, 58, 82, 87], "199": 116, "1990": [51, 72, 73], "1991": [51, 72, 73, 117], "199281e": 76, "199282e": 73, "199412": 60, "199458": 102, "1995": [50, 71], "1998": 74, "19983954": 80, "199893": 65, "1999": [74, 80], "1_": [61, 76], "1e": [4, 6, 8, 9, 11, 12, 13, 16, 17, 18, 68, 73], "1f77b4": 63, "1x_4x_3": 63, "2": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 35, 36, 38, 40, 42, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 102, 103, 104, 105, 108, 109, 110, 111, 113, 114, 115, 116], "20": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 25, 26, 27, 29, 30, 31, 48, 50, 51, 52, 53, 59, 60, 61, 62, 65, 67, 68, 69, 71, 72, 73, 76, 78, 79, 80, 81, 82, 85, 86, 87, 88, 89, 102, 103, 108, 114, 117], "200": [24, 27, 28, 49, 53, 61, 62, 64, 69, 75, 76, 80, 82, 86, 116], "2000": [18, 20, 51, 53, 56, 59, 60, 61, 68, 72, 73, 76, 80, 85, 87], "20000": [51, 72], "20000000000000004": [61, 73, 76], "200000e": 73, "200049": 59, "20008492": 88, "20010": 73, "200110": 73, "2003": [19, 115], "200303": 114, "2005": 62, "20055": 73, "2006": 73, "20073763": 69, "20074": 73, "201": [52, 73, 116], "2010": [50, 71], "2011": [50, 71, 112, 114], "2013": [64, 102, 115], "2014": [102, 115], "2015": [28, 115], "201528": [59, 60], "20158": 73, "2016": 74, "2017": [26, 115], "201768": 71, "201788e": 73, "2018": [19, 20, 30, 31, 48, 50, 51, 58, 62, 64, 69, 71, 72, 73, 78, 82, 88, 89, 95, 102, 112, 115, 116], "2019": [24, 52, 59, 60, 61, 65, 66, 73, 76, 78, 86, 89, 91, 96, 101, 112, 114, 115], "201e": 77, "202": 116, "2020": [8, 9, 21, 22, 23, 25, 27, 49, 52, 62, 79, 86, 87, 103, 105, 115], "2020435": 50, "2021": [29, 49, 50, 52, 59, 60, 71, 115, 116], "20219609": 50, "2022": [78, 79, 87, 103, 105, 111, 112, 115], "2023": [32, 53, 80, 87, 89, 97, 98, 115], "2024": [47, 57, 64, 69, 70, 74, 77, 79, 87, 112, 115], "202603": 60, "202650e": 61, "20269": 73, "20274": 73, "202846": 60, "203": [51, 59, 72, 116], "203284": 61, "20329": 73, "2036": 73, "203828": 73, "204": 116, "204007": 76, "20400735": 76, "204362": 79, "204455": 60, "204482": 76, "204794": 76, "204893": 56, "205": [77, 78, 116], "205187": 61, "205224": 78, "205333e": 72, "205938": 71, "206": 116, "2061": 73, "206253": [72, 73], "206256": 68, "2064": 73, "206614": 76, "207": [47, 77, 87, 116], "2075": 49, "207834": 60, "20783816": 50, "207840": 66, "207885": 72, "207912": 102, "208": [56, 74, 116], "208034e": 73, "2080787": 50, "20823898": 50, "2086": 73, "209": 56, "209014": 76, "209219e": 78, "209257": 8, "209546e": 73, "209894": 76, "21": [19, 20, 30, 48, 50, 51, 52, 59, 60, 64, 71, 72, 73, 76, 78, 79, 82, 85, 86, 87, 88, 102, 112, 114, 115, 117], "210": [22, 23, 27, 56], "2103": [73, 112], "2103034": 50, "210319": [59, 60], "210323": 76, "2104": 116, "21055516": 88, "2107": 115, "21078": 73, "211": [56, 77, 116], "21105": [52, 86, 112, 114], "2112": 79, "21142": 73, "211534": 61, "21155656": 76, "211557": 76, "212": [56, 116], "2122": 73, "21257396e": 88, "212811": 56, "212844": 71, "212863": 59, "213": [56, 77, 115, 116], "213026": 73, "213070": 60, "213135": 60, "213199": 87, "21361": 73, "2136254": 88, "213743e": 60, "2139": 25, "214": 74, "214458": 70, "214764": 78, "214769": 68, "215": 56, "215069": 76, "215342": 76, "2155": 73, "21550": 73, "21562": 73, "21573": 73, "215967": 102, "216": 56, "216207": 86, "21624417": 50, "2163": 73, "216344": 76, "21669513e": 88, "216761": 75, "216943": 85, "217": [56, 77, 115], "21716": 73, "2171802": [50, 71], "217244": 13, "217684": 68, "218": 56, "21804": [51, 72], "218383": 56, "218767": 73, "2189": 73, "218938": 73, "219": [21, 22, 23, 49, 56, 115], "2191274": 50, "219585": 56, "2197237644227434": 70, "22": [48, 50, 51, 52, 59, 60, 70, 71, 72, 76, 77, 78, 79, 85, 86, 87, 88, 102, 114, 117], "220": [35, 56, 116], "220088": 73, "220398": 59, "220407": 70, "220772": 76, "221": [56, 116], "2213": 71, "2214": 71, "221419": 73, "2215": 71, "2216": 71, "2217": [50, 71], "2217227": 88, "222": 116, "2222": [48, 50, 58, 87], "22222": 73, "222261": 85, "22272803e": 88, "222843": 76, "222882": [60, 70], "223": [74, 77, 116], "2231391": 88, "223158": 70, "22336235": 50, "223485956098176": [65, 66], "223617": 70, "22375856": 50, "22390": 72, "223928": 70, "224": [77, 116], "224897": [59, 60], "225": [49, 80, 116], "225034": 62, "22505965": 50, "22507006e": 88, "225175": 76, "225222": 76, "22522221": 76, "22528": 73, "225350": 60, "225427": 56, "225459760731946": 61, "225460": 61, "225574": 71, "2256": 73, "22562": 73, "225670": 70, "225776": 79, "226": 116, "2264": 49, "226479": 68, "226524": 76, "226598": 71, "226938": 66, "226969": 56, "227": [73, 116], "2271071": 32, "2276": 49, "2279": 73, "227932e": 72, "228035": 73, "2281": 73, "228404": 70, "228597e": 60, "228630": 60, "228648": 51, "229": [51, 116], "22925": 73, "22937": 73, "229443": 76, "229452": [85, 86, 87], "229472": 72, "2295": 73, "229759": 86, "2298": 49, "229961": [59, 60], "229994": [59, 60], "23": [9, 38, 42, 50, 51, 52, 59, 60, 62, 69, 71, 72, 73, 76, 78, 79, 83, 85, 86, 87, 88, 102, 112, 114, 115, 117], "230": 49, "230009": [65, 66], "2307": [50, 71, 82], "2308": 78, "230842": 59, "230956": 63, "231": [19, 116], "23113": 87, "231153": 60, "231310": 76, "231430": 102, "231467": 87, "231734": 87, "231798": 87, "231986": 76, "231e": 77, "232134": [59, 60], "232157": 60, "2328": 73, "232868e": 60, "232959": [65, 66], "232e": 87, "233": 26, "233029": 59, "233154": 117, "2335": 49, "233705": 60, "234": 115, "234137": 79, "234153": 79, "234205": 73, "2342459191": 88, "234431": 70, "234534": 61, "234605": 54, "2346449": 88, "234798": 73, "234910": 71, "235": 116, "235291": 59, "2359": 117, "23590": 73, "236008": 61, "236015e": 59, "236309": 73, "236884": 70, "23690345e": 88, "237": 52, "237115": 60, "237200e": 59, "237252": 73, "237341": 59, "237461": 78, "23748": 73, "23751359e": 88, "237896": 76, "23789633": 76, "238": [50, 71, 116], "238101": 76, "238225": 102, "238251": 61, "238529": 12, "23856": 73, "238619": 56, "238794": 76, "239": 116, "239019": 68, "239243": 59, "239267": 70, "239313": 60, "23965": 73, "23e": 51, "24": [50, 51, 52, 59, 60, 69, 70, 71, 72, 73, 76, 78, 79, 80, 85, 86, 87, 88, 102, 114, 115, 116, 117], "240127": [59, 60], "240146": 60, "240295": 78, "240532": [59, 60], "2407": 49, "24080030a4d": 52, "240813": 67, "241049": 76, "241064": 60, "241503": 85, "2416": 49, "241609": 73, "241645": 60, "241678": 59, "241827": 60, "241962": 79, "24199": 73, "241e": 77, "242": 115, "242000": 73, "242124": [72, 73], "242139": 102, "242158": [72, 73], "2424": 68, "242427": 68, "2424596822": 66, "242815": 102, "242902": 76, "2430561": 49, "243246": 76, "2438": 73, "2439": 73, "243e": 77, "244": [35, 73], "244090": 73, "244455": 76, "244622": 102, "24469564": 114, "245": [115, 116], "245062": 76, "2451": 49, "24510393": 51, "245370": 71, "245512": 76, "245531": 59, "245720": 63, "2458525": 88, "246": 116, "246624": 85, "2467506": 50, "246753": 76, "246879": 76, "247": [77, 116], "247020": 61, "247057e": 76, "2471": 73, "2472": 73, "247617": 85, "247717": 73, "24774": [72, 73], "247826": 71, "247977": 59, "248171": 76, "248441": 85, "248638": 61, "249": [50, 71, 74, 116], "2491": 73, "24917": 73, "249986": 70, "25": [17, 18, 21, 22, 23, 27, 28, 29, 30, 50, 51, 52, 59, 60, 61, 63, 64, 69, 70, 71, 72, 73, 76, 79, 80, 85, 86, 87, 88, 102, 114, 117], "250": [74, 116], "2500": 73, "25000000000000006": [61, 73, 76], "250073": 73, "250210": 61, "2503": 73, "250354": 76, "250425": 61, "251": [72, 73, 78], "251412": 60, "251480": 60, "251953": 73, "252": 47, "252133": 73, "252253": 78, "25240463": 87, "252524": 76, "252601": 102, "253026": [59, 60], "2532": 73, "253437": 75, "253675": 72, "253724": 76, "25374": 73, "254": [73, 116], "25401679": 50, "254035": 70, "254038": 66, "254083": 60, "2543": 73, "254324": 61, "254400": 102, "255": [73, 116], "25501609": 88, "255034e": 60, "255995": 59, "256": [73, 86], "256082": 85, "256416": 76, "256567": 71, "25672": 73, "25679252735374059697081869395979899": 88, "2567925273537405969708186939597989913111617182232455658606164757678839091812132023242628313642435254555765718996410152133344748516263677477808284878892": 88, "256944": 76, "256992": 73, "257019": 60, "257207": 50, "257377": 63, "257523": 59, "258083": 60, "258158": [59, 60], "2583": 73, "258522": 59, "258541e": 18, "25883418": 88, "258951": 76, "259164": 60, "259367": 85, "259395": 67, "2594": [51, 72], "259828": [59, 60], "259875": 60, "25x_3": 63, "26": [50, 51, 52, 54, 59, 60, 62, 69, 71, 72, 73, 80, 83, 85, 86, 87, 88, 102, 114], "26016": 73, "260161": [16, 85], "260211": [59, 60], "260356": 72, "260360": 76, "260762": 56, "261": 77, "2610": 73, "2613": 73, "261624": [72, 73], "261685": 73, "26175": 73, "261777": 73, "261903": 71, "2619317": 50, "262423e": 73, "262621": 71, "262829": 88, "263": [19, 73, 116], "2633": 73, "263942e": 60, "263974e": 76, "264": [115, 116], "264086": 63, "264274e": 73, "264884": 73, "265": 116, "2651": 87, "265119": 75, "2652": [52, 72, 73], "265547": 73, "265744": 70, "2657788": 88, "2658": 66, "265929": 77, "266": 116, "266147": 77, "266686": 59, "266922": 102, "267": 74, "2670691": 50, "267500": 71, "267581": 73, "267950": 76, "268": 116, "268055": 73, "268343": 70, "268628e": 60, "268942": 76, "268943": 59, "268998": 51, "269043": 76, "269112": 87, "269977": 73, "26bd56a6": 52, "26e": 51, "27": [22, 23, 27, 48, 50, 51, 52, 53, 54, 59, 60, 62, 69, 71, 72, 73, 80, 83, 85, 86, 87, 88, 102, 114, 115], "270": 116, "2700": 52, "270451267": 88, "270644": [59, 60], "271": 116, "271004": [72, 73], "271083": 73, "27109880": 88, "271183": 72, "272": 74, "272296": 73, "272332e": 59, "272408": 60, "272662": 73, "273": 52, "273299": 60, "273356": 61, "27358573": 88, "27371": [51, 72], "27372": [51, 72], "274": [52, 73], "2740991": 49, "274251e": 72, "274267": 71, "27429763": 87, "27456168": 88, "274793": 76, "274825": [17, 85], "27487": 73, "2754": 49, "275596": 102, "276": [52, 116], "276148": 76, "276189e": 71, "2764": 73, "2766091": 51, "276975045": 88, "27713": 73, "277299": 54, "27751": 73, "277512": 60, "277561e": 71, "277968": 76, "278": [78, 116], "2780": 50, "278000": 71, "278035": 56, "278391": 73, "278434": 65, "278454": 68, "2786": 102, "278804": 60, "279": 116, "27951256e": 88, "279595": 56, "27986": 73, "279933e": 60, "28": [50, 51, 52, 59, 60, 64, 69, 71, 72, 80, 85, 86, 87, 88, 102, 114, 116], "280196": 66, "280454dd": 52, "280514": 102, "280963": 75, "281": [77, 116], "281024": 76, "28111364": 51, "2815": 73, "2818": 49, "2819": 102, "282": [77, 115, 116], "282200": 66, "2825": [112, 114], "28251": 73, "282870": 73, "2830": [112, 114], "283041": 59, "283207": 59, "28326": 73, "2833423": 88, "283386": 59, "2836": 49, "2836059": 50, "28382": 73, "283974": 76, "283992": 59, "283994": 76, "283e": 77, "284": 116, "28425026": 78, "284271": 67, "284397": 117, "28452": [51, 72], "2849": 73, "284987": 73, "285": [77, 87, 116], "285001": 56, "285483": 68, "285e": 77, "286": 116, "286203": 59, "286371": 59, "2865": [49, 73], "286507": 61, "286563e": 73, "286593": 73, "287041": 76, "287123": 85, "287196": 59, "287815": 78, "287926": 76, "288": 74, "288976": 73, "289": 115, "289062": 72, "289357": 60, "289440": [59, 60], "289555": 68, "29": [50, 51, 52, 59, 60, 69, 71, 72, 78, 80, 85, 86, 87, 88, 102, 114], "290": 87, "290565": 60, "290736e": 60, "290901": 56, "290987": 72, "291": [73, 77], "2910": 73, "291008": 59, "291011": 87, "291071": 76, "29107127": 76, "291405": 76, "291406": 76, "291434": 60, "291500e": [72, 73], "291517": [59, 60], "291963": 76, "292": 75, "292028": 61, "292047": 102, "292105": 76, "292302995303554": 61, "292303": 61, "2925": 52, "2927": 73, "292997": 76, "29299726": 76, "293218": 76, "293617e": 73, "294067": [59, 60], "294123": 85, "294449": 59, "295": 115, "295307": 59, "295481": 76, "29548121": 76, "295837": [54, 83, 114], "2958370000000100000100": [52, 83, 114], "2958370001000010011100": [52, 83, 114], "2958371000000010010100": [52, 83, 114], "296099": 56, "296228": 73, "296729": 71, "29678199": [81, 89], "296901": 59, "297": 47, "297287": [59, 60], "2973": 73, "297349": [65, 66], "297682": 76, "297687": 73, "297749": 73, "29784405": 78, "298": [26, 52], "298076": 59, "298120": 61, "298228e": 73, "299": [52, 77], "299537": 66, "299712": 65, "29985387": 88, "2999": 56, "2_": [32, 53, 80, 103, 105, 111], "2_x": [32, 53, 80], "2d": [89, 96], "2dx_5": [61, 76], "2e": [47, 49, 50, 51, 52, 53, 86, 87, 89, 102, 114], "2f": 67, "2m": [103, 108, 111], "2n_t": 63, "2x": 76, "2x_0": [24, 59, 60, 65, 66], "2x_4": 63, "3": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 25, 26, 27, 30, 35, 36, 37, 38, 41, 42, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 102, 103, 108, 112, 113, 114, 115, 116], "30": [24, 47, 48, 50, 52, 53, 56, 57, 58, 59, 60, 61, 62, 69, 71, 72, 73, 76, 77, 80, 85, 86, 87, 88, 102, 114], "300": [48, 58, 61, 73, 76, 82, 115], "3000": 56, "30000000000000004": [61, 73, 76], "300031": 59, "30031116e": 88, "300892e": 60, "30093956": 78, "301": 52, "301366": 102, "301371": 76, "3016": 72, "301737": 59, "30189": 73, "302149": 59, "302357": 76, "302382": 70, "302648": 71, "303007": 59, "303324": 71, "303489": 76, "303613": 76, "30361321": 76, "30383": 73, "303835": 71, "303f00f0bd62": 52, "304130": 76, "304159": 76, "304201": 63, "30527": 73, "305341": 76, "305612": 71, "305775": 76, "305b": 52, "306297": 59, "30645": 73, "30672815": 50, "306915": 71, "306963": 76, "307407": 76, "308": 73, "308568": 60, "308774": 59, "30917769": [65, 66], "309539": 70, "309772": 71, "309823e": 73, "30982972": 76, "309830": 76, "31": [50, 51, 52, 53, 59, 60, 69, 71, 72, 73, 80, 85, 86, 87, 88, 102, 114, 117], "310000e": 73, "310145": 70, "310761": 75, "311": 77, "311253": 73, "311321": 60, "311667": 60, "311712": 65, "311869": 85, "3120": 73, "312652": 77, "313": 87, "313056": 102, "313209": 61, "313324": 73, "31337878": 73, "313535": 76, "31378": 52, "313870": 68, "314": 88, "3141": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 50, 52, 53, 54, 71, 81, 83, 85, 86, 87, 89, 102, 114], "314247": 79, "314341": 59, "3146": 18, "314625": 60, "314651": 65, "31476": [72, 73], "315": 88, "315031": 79, "315036": 59, "3151": 73, "315155": 60, "315290": [65, 66], "315310": 59, "315769e": 59, "316": 52, "316193": 76, "31632": 73, "316407": 87, "3164095": 88, "316540": 71, "316717": [59, 60], "316826": 59, "316863": 60, "317394": 63, "317487": 76, "317607": 76, "318": 52, "318000e": 73, "318438": 73, "318552": 73, "318584": 102, "318753": [65, 66], "319": 52, "319100": [65, 66], "31921453": 88, "319420": 68, "319759": 76, "319850": 76, "32": [15, 50, 51, 52, 59, 60, 69, 70, 71, 72, 73, 80, 85, 86, 87, 88, 89, 102, 114], "320": 73, "320314": 72, "320633": 61, "321686": 102, "322": 74, "322186e": 59, "32236455588136": 62, "322404": 78, "322751": 60, "3234": 73, "323636": 72, "323679": 71, "324": [51, 73], "324518": 75, "32458367": 50, "3245837": 76, "325056": 76, "325090": 73, "325486": 59, "325599": 59, "3259": 88, "326": 77, "326148": 60, "32651493": 88, "326721": 60, "326740": 76, "326871": 79, "3268714482135234": 79, "327257": 59, "327803": 85, "327958": 56, "328471": 59, "329339": 62, "32950022e": 88, "33": [50, 51, 52, 59, 60, 65, 69, 70, 71, 72, 73, 80, 85, 86, 87, 88, 102, 114, 115], "3300": [51, 72], "330068": 59, "330100": 59, "330143": 76, "33014346": 76, "330163": 60, "330285": [59, 60], "3304269": 50, "330615": 76, "330731": [17, 85], "331": 117, "331365": 65, "331521": 76, "331602": 73, "33175566": 76, "331756": 76, "332502": 60, "332782": [17, 85], "3329": 73, "332996": 71, "3333": [48, 50, 58, 85, 86, 87], "3333333": 52, "33335939e": 88, "3335": 73, "333581": 72, "333655": 59, "333704": 60, "333955": 60, "334": 51, "334750": 61, "33500": 73, "335176": 73, "3353147738": 88, "335446": 56, "335609e": 76, "335846": 76, "335853": 73, "336382": 60, "336461": 73, "336612": 63, "337": 74, "337380": 76, "3376": 49, "337619": 62, "3378": 88, "338": [47, 78], "33849": 73, "3386": 88, "338603": 59, "338775": 61, "338908": 61, "3392": 88, "339269": 78, "33928": 73, "339443": 60, "339570": 76, "339875": [65, 66], "34": [48, 49, 50, 51, 52, 53, 59, 60, 66, 69, 71, 72, 73, 74, 78, 80, 85, 86, 87, 88, 102, 117], "340": [51, 73], "340029": 60, "340142": 87, "340235": 68, "340274": 78, "341336": 13, "341472": 70, "341755e": 59, "3420": 73, "342117": 68, "342362": 56, "342632": 70, "342675": 50, "34287815": 78, "342989": 73, "342992": 71, "343": [73, 77], "343639": 85, "343685": 59, "34375": 72, "343828": 59, "344212": 117, "344440": 85, "344505": [72, 73], "344640": 76, "344787": [59, 60], "344834": 63, "345065e": 73, "345381": 61, "3453813031813522": 61, "3454": 73, "345852": 60, "345903": 76, "345989": 59, "346107": 72, "346206": 76, "346238": 78, "346269": 60, "346678": 75, "346964": 59, "347310": [17, 85], "347696": 61, "34769649731686": 61, "3478754": 88, "347929": 73, "348": 77, "348319": 60, "34858240261807": 62, "348617": 76, "348622": 77, "348700": 60, "348980e": 60, "3492131": 49, "349383": 71, "34943627": 69, "349638": 60, "34967621": 50, "349772": 66, "35": [51, 52, 59, 60, 61, 71, 72, 73, 76, 85, 86, 87, 88, 102, 103, 108, 117], "3500000000000001": [61, 73, 76], "350165": 86, "350208": 59, "350518": 76, "350712": [65, 66], "35077502": [103, 108], "351220": 60, "351629": 73, "351766": 75, "352": [51, 71], "352039": 74, "352250e": 72, "352259e": 73, "3522697": 50, "35292": 73, "352990": 73, "352998": 73, "353105": 18, "353412": 76, "35341202": 76, "35365143": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18], "353748e": 76, "3538": 49, "354": 73, "354188": 63, "354371": 76, "354688": 14, "355065": 56, "355209": 76, "355627": 85, "355651": 60, "356136e": 73, "356167": 66, "356183": 73, "35620768e": 88, "3564": 73, "3565": 73, "3568": 87, "357": 73, "357170": 59, "35731523": 87, "35758929": 88, "358158": [72, 117], "358289": 71, "358395": 78, "358799": 102, "359": [77, 117], "359100": 73, "359161e": 59, "359229": 56, "3593": 78, "359307": 60, "35th": 115, "36": [51, 52, 59, 60, 71, 72, 85, 86, 87, 88, 102], "360004": 76, "360065": 102, "360249": 67, "360475": [59, 60], "360572": 60, "360655": 73, "360683": 61, "360801": 61, "361": 77, "3612840": 88, "361518": 61, "361518457569366": 61, "361521": 14, "361623": 68, "3619201": 25, "362157": 59, "36231307e": 88, "363276": 50, "364221": 59, "3643": 102, "36432401": 88, "364595": 50, "3647": 52, "364800": 76, "36501": 73, "365551": 60, "36557195e": 88, "36566025e": 88, "366": 73, "36616": 73, "366310": 59, "3663433": 88, "366529": 75, "366538": 74, "366718627": 50, "366950": 59, "367": [35, 77], "367181": 59, "367323": 76, "367366": 68, "367398": 70, "367571": 61, "367625": 76, "368152": 71, "3682": [51, 72, 73], "3682266": 88, "368324": 71, "368499": 61, "3684990272106954": 61, "368577": 85, "369556": 61, "3696": 78, "369796": 76, "369869": 72, "369981": 71, "37": [51, 59, 60, 68, 71, 72, 73, 85, 86, 87, 88, 102], "370254e": 72, "3702770": 50, "370736": 71, "3707775": 50, "370908": 59, "3710": 73, "371357": [72, 73], "371429": 61, "371850e": 60, "372": 115, "37200": [72, 73], "372097": 61, "3722": 73, "37231324": 80, "3724": 73, "372427": 60, "3727679": 50, "373218e": 70, "373451": 85, "3738573": 50, "374364": 76, "37436439": 76, "3745": 73, "374821e": 73, "374862": 59, "374917e": 59, "375081": 73, "375274": 59, "375465": 76, "375618": 74, "375621": 70, "375844": 70, "3766": 68, "376617": 68, "376760": 60, "376806": 60, "377060": 73, "377147": 85, "377311": 76, "377669": 60, "378351": 11, "378588": 59, "378596": 71, "378688": 76, "378727": 59, "378834": 76, "3788859": 50, "379": 115, "379038": 76, "37939": 73, "379614": 76, "379626": 59, "379981e": 60, "38": [52, 59, 60, 72, 74, 85, 86, 87, 88, 102], "3800694": 50, "380837": [72, 73], "381": 77, "381072": 76, "381603": 59, "381685e": [72, 73], "381689": 76, "3817": 73, "382286": 73, "382582e": 6, "382872": 61, "383": 117, "383297": 76, "383531": 70, "384": 73, "384443": 60, "384677": 56, "384777": 73, "384865": 60, "384928": 59, "3851": 73, "385160": 60, "385240": 102, "385917": 71, "386": [52, 73], "386102": 61, "386502": 73, "386831": 56, "386988": 62, "387": 52, "3871": 49, "387426": 76, "387780": 76, "388026": 59, "388071": 76, "388185": 56, "38818693": 88, "388216e": 86, "388663e": 74, "388668": 76, "38866808": 76, "388871": 73, "389": 52, "389126": 87, "389164": 70, "389566": 75, "38973512e": 88, "389755": 59, "38990574": 88, "39": [47, 49, 50, 51, 52, 53, 54, 56, 59, 60, 62, 66, 67, 69, 71, 72, 73, 78, 79, 80, 85, 86, 87, 88, 102], "39010121e": 88, "390379": 76, "391377": 79, "3918686": 88, "392128": 60, "392242": 67, "39236801": 69, "392400": 73, "392623": 60, "392752": 62, "392833": 78, "392864e": [72, 73], "392917": 59, "393604": 61, "393654": 56, "394226": 60, "39425708": 50, "395076e": 73, "395136": 71, "395268": 85, "395569": 59, "395603": 59, "3958": 87, "395889": 73, "396": [35, 87], "39611477": 51, "396173": 65, "39621961e": 88, "396300": 65, "3964": 73, "396531": 73, "396985": 71, "396992": [59, 60], "397140": 61, "397155": 60, "39727": 73, "397313": 49, "397536": 70, "397578": 67, "397811": 78, "398": [83, 114], "398166": 56, "3985": 73, "398770": 76, "398999": 85, "399": 51, "399056": 76, "399223": 63, "399343e": 59, "399355": 63, "399679": 87, "399692": 76, "399858": 79, "3cd0": 52, "3dx_1": [61, 76], "3e1c": 52, "3ec2": 52, "3f5d93": 74, "3x_": 76, "3x_4": [61, 76], "4": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 22, 23, 27, 32, 35, 36, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 102, 103, 108, 114, 116], "40": [50, 53, 59, 60, 61, 62, 70, 72, 73, 76, 80, 83, 85, 86, 87, 88, 102, 103, 108], "400": 71, "4000000000000001": 86, "40000000000000013": [61, 73, 76], "400113": 68, "40029364": [103, 108], "400823": 76, "400855956463958": 61, "400856": 61, "400905": 56, "401": [19, 117], "401247": [89, 102], "40127723e": 88, "401690e": 60, "401931": [65, 66], "402077": 73, "402113": 102, "402301e": 86, "402619": 15, "402902": 73, "403": 77, "403071": 74, "403425": 76, "403626490670169": 81, "4036264906701690": 81, "403626491": 81, "403715": 6, "403771948": 89, "4039": 49, "404267": 59, "404300": 56, "404318": 49, "404411": 59, "40452": 73, "404550": 75, "4049607": 88, "405050": 60, "405203": 63, "405374": 73, "40583": 49, "405890": [17, 85], "406285": 76, "406446": 61, "4065173": 88, "40676": 49, "407": 77, "40732": 68, "407558": 59, "407565": 59, "408133": 74, "408476": [103, 108], "40847623": [103, 108], "408479": 71, "408509": 60, "408539": 76, "408565": 76, "409154": 49, "4093": 78, "409328": 73, "409395": 76, "409746": 61, "409848": [59, 60], "41": [47, 59, 60, 72, 73, 85, 86, 87, 88, 102], "410": 117, "410100": 59, "410152133344748516263677477808284878892": 88, "410393": 61, "410667": 85, "410681": 63, "410682": 59, "410795": 71, "41093655": 88, "411146e": 60, "411190": [59, 60], "411291": 75, "411295": 76, "411304": [59, 60], "411447": 73, "411582": 76, "411768": 60, "412004": 65, "412127": 76, "412304": 79, "412477": 63, "412653": 71, "412714": 61, "412726": 60, "412941e": 60, "413247e": 59, "41336": 86, "413376": 87, "41341040": 50, "413608": 76, "414": 77, "414073": 12, "414533": 60, "41525168e": 88, "415375": 59, "415556": 85, "41566": 87, "415812": 117, "415988": 73, "416052": 56, "416132": 60, "4166": 73, "4166667": 52, "416757": 76, "416899": 59, "416919": 60, "416e": 77, "417640": 59, "417727": 72, "417736": 70, "417767": [65, 66], "417834": 56, "41798768e": 88, "418": 35, "418056": 76, "41805621": 76, "418400": 68, "418741": 56, "418806e": 61, "418969": 85, "41918406e": 88, "419371": 76, "419871": 56, "41989983e": 88, "4199952": 50, "41e5": 52, "42": [8, 9, 27, 47, 53, 56, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 73, 75, 76, 78, 79, 80, 85, 86, 87, 88, 102, 115, 117], "4200": 73, "420316e": 73, "420608e": 78, "42073312": 50, "420967": 61, "421083": 49, "4211349413": 50, "421163": 59, "421200": 79, "421297e": 60, "421357": [65, 66], "421576e": 73, "421793": 78, "421919": 73, "422007": 78, "422266": 73, "422293e": 85, "422325": 61, "422591": 60, "42338": 73, "4235839": [65, 66], "42388745": 80, "423921e": 85, "423951": 49, "424108": 61, "424127": 88, "42412729": 50, "424292": 59, "424328": 76, "424651": 87, "424717": 61, "424748": 79, "425": 71, "425103": 49, "425208": 73, "425325": 68, "425493": 49, "42550": 73, "425636": 68, "426055": 49, "426540": 71, "426540301": 50, "426736": 73, "427": 73, "427486": [59, 60], "42755087": 78, "427551": 78, "427573": 71, "427654": 68, "427725": 76, "428": [102, 117], "428046": 75, "42811700": 117, "428255": 76, "428411": [72, 73], "428467": 76, "4284675": 76, "428771": [17, 85], "4290": 49, "429057": 60, "429230": 59, "429705": 59, "42ba": 52, "43": [51, 56, 59, 60, 85, 86, 87, 88, 102], "430298e": [72, 73], "430595": 60, "430608": 59, "431061e": 59, "4311947070055128": 86, "431253": 70, "431306": 76, "431701914": 112, "431998": 56, "432130e": 72, "432300e": 76, "43231359e": 88, "432707": 77, "43294": 52, "432f": 52, "433": [52, 77], "433221": 61, "4336": 73, "43374433": 80, "433750": 59, "433753": 70, "4339": 49, "434054e": 68, "434121": 70, "434535": 76, "43453524": 76, "435": 52, "43503345": 87, "43511": 73, "435401": 71, "4357": 73, "435927": 73, "435967": 71, "43597565": 76, "435976": 76, "436": [52, 73], "436016": 70, "43627032": 62, "436327": 73, "436394": 70, "436764": 77, "436806": 73, "436817": 70, "437667": 72, "437924": 73, "438": 71, "438219": 76, "438289": 73, "438569": 73, "438578e": 73, "43883": 66, "438834": 60, "4389": 73, "438960": 71, "439401e": 60, "439541": [72, 73], "439675": 77, "439699": 56, "43989": 85, "439958": 70, "43f0": 52, "44": [56, 59, 60, 62, 85, 86, 87, 88, 102], "440320": 73, "440605": 86, "440747": 59, "440a": 52, "441153": 76, "441209": 76, "441219": 65, "44124313": 87, "441282": 59, "4413943055": 88, "4416552": 50, "441849": 59, "443": 47, "443016": 61, "443032": 72, "44312177": 51, "443686": 76, "4437": 73, "443e": 77, "444046": 73, "4444": [48, 50, 58, 87], "444500": [72, 73], "44455946": 88, "444850": 73, "4449272": 73, "445": 77, "445476": 59, "44563945e": 88, "4461928741399595": 61, "446193": 61, "4462": 52, "44647451": 78, "44713577e": 88, "447492": 73, "447624": [59, 60], "447706": 61, "447849": 62, "448": 73, "448252": 60, "448456e": 60, "448569": 59, "448587": 61, "448745": 76, "448842": 60, "4489": 73, "44890536": 88, "448923": 67, "449107": 9, "449150": [17, 85], "44939536": 88, "44950": 73, "449677": 68, "44fa97767be8": 52, "45": [59, 60, 61, 65, 67, 70, 73, 76, 85, 86, 87, 88, 102], "4500": 72, "45000000000000007": [61, 73, 76, 86], "450031": 70, "450152": 71, "450812e": 60, "450870601": 50, "451312e": 59, "452": 52, "452091": 73, "452114": 85, "452488701": 50, "452489": 71, "452623": 60, "453": 52, "453279": 59, "4535": 73, "4539": 52, "454081": 73, "454397": 76, "454406": 56, "45467447": 88, "455": 52, "45500": 73, "455078": 61, "455091": 60, "455107": 61, "455120": 76, "4552": 52, "455293": 61, "4552b8af": 52, "455448": 78, "455672": 73, "455981": 103, "456370": 71, "456458e": 59, "4566031": 102, "45660310": 102, "4567": 78, "456892": 61, "457088": 76, "457609227": 88, "457667": 73, "458114": 73, "458307": 105, "458420": 73, "4584447": 50, "458784": 59, "458814": 68, "458855": 51, "4592": 50, "459200": 71, "459383": 61, "459418": 60, "459436": 70, "45957837": 88, "459760": 73, "459812": 61, "46": [59, 60, 67, 69, 85, 86, 87, 88, 102], "460": 73, "46003824": 88, "4601": 73, "460207": [59, 60], "460218": 61, "460289": 76, "460744": 72, "4610": 117, "461227e": 59, "461412": 85, "461629": 79, "462": 47, "462451": 61, "462567": 60, "462979": 59, "463325": 76, "4634": 73, "463418": 79, "463668": 73, "463766": 66, "463857": 73, "463903": 60, "463b": 52, "464": [87, 88], "464076": 61, "464284": 71, "46448227": 88, "464668": 13, "465": 56, "46507214": 78, "465424": 70, "465649": 79, "465730": 79, "4659651": 81, "465965114589023": 81, "4659651145890230": 81, "466047": 76, "46618738": 88, "466440": 61, "466756": 76, "467": 73, "46709481": 88, "46722576e": 88, "467613": 71, "467613401": 50, "467681": [59, 60], "467770": 61, "468072": 60, "468075": 76, "46807543": 76, "46811985": 76, "468120": 76, "468406": 73, "468907": 56, "468919": 73, "468d": 52, "469": 52, "469474": 79, "469676": 56, "469825": 61, "469895": 60, "469905": 60, "47": [51, 56, 59, 60, 62, 72, 78, 85, 86, 87, 88, 102, 116], "470055": 60, "47026570": 88, "47038188": 88, "470904": 59, "471": 56, "4710182": 88, "471435": 77, "471622": 59, "472": 73, "47222159": 80, "472255": 73, "472699": 60, "472891": 76, "472e": 52, "473099": 61, "47319": 87, "47419634": 114, "474214": [65, 66], "474731": 85, "475304": 73, "475517": 70, "475569": 59, "475e": 77, "476856": 61, "477130": [59, 60], "477150": 76, "477247": 60, "4773426601": 88, "477357": 77, "477474": 71, "47759584": 88, "47761563": 62, "478032": 73, "478059": 60, "478064": 60, "4781": 73, "47857478": 88, "479655": 70, "47966100e": 88, "479722": 60, "479860": 73, "479876": [65, 66], "479882": 60, "47990288": 88, "479928": 76, "47be": 52, "48": [52, 56, 59, 60, 66, 72, 73, 74, 85, 86, 87, 88, 102], "480": [47, 56], "480133e": 76, "480199": 68, "48029755": 78, "480579": 60, "48069071": [81, 89, 102], "480691": [89, 102], "480800e": 76, "481172": 76, "481218": 73, "481399": [72, 73], "481705": 87, "481713": 68, "481761e": 73, "482": [52, 56], "482012": 65, "482038": 61, "48208358": 76, "482084": 76, "482179": 59, "482251": 15, "482461": [103, 108], "48246134": [103, 108], "482483": 76, "482616": 70, "482790": 63, "482898e": 60, "48296": 78, "483": [77, 87], "48315": 78, "483186": 63, "483192": [72, 73], "48331": 78, "4835": 73, "483711": 76, "483717": 61, "48390784": 87, "484": 74, "484005": 88, "48404": 50, "484303": 60, "4845": 73, "484640": 76, "4849": 52, "485": [52, 73], "485197": 59, "48550": 79, "485617": [72, 73], "485812e": 73, "48583": [72, 73], "485871": 66, "486": [28, 73], "486178e": 59, "486202": 61, "486466": 74, "486532": 76, "48661": 73, "487": [56, 73], "487467": 73, "487524": 68, "487641e": 76, "487793": 60, "487872": 57, "488394": 59, "488460": 73, "488485": 73, "48873663": 62, "488811": 76, "488909": [72, 73], "488982e": 61, "4895498": 76, "489550": 76, "489699": 61, "489951": 60, "49": [52, 56, 59, 60, 85, 86, 87, 88, 102], "490000e": 73, "490070931": 50, "490488e": 72, "490504e": 73, "490700": 76, "490896": 56, "490941": 73, "491034": 59, "49119854": 88, "491245": 71, "49135": 18, "4915707": 87, "492": 73, "4923156": 81, "49231564722955": 81, "492315647229550": 81, "492417e": 87, "492637": 67, "492656": 60, "49270769e": 88, "493": [77, 87, 115], "493102e": 70, "493144": 79, "493195": 68, "493219": 76, "493313": 73, "493325": 8, "493426": 77, "494": 77, "494089": 60, "494129": 76, "494324": 71, "494324401": 50, "495": 75, "495108": 70, "49530782": 50, "495657": 61, "495752": 76, "49596416e": 88, "496": 75, "49650883": 78, "496551": 76, "496714": 79, "496777": 117, "49693": 86, "497": [47, 75], "497168": 70, "497422": 60, "497655": 9, "497674": 62, "497716": 74, "497964": 85, "498": 75, "498122": 68, "498286": 70, "498921": 76, "498979": 73, "498992": 59, "498f": 52, "499": [73, 75, 83, 114], "499000e": [72, 73], "499036": 74, "499776": 73, "49d4": 52, "4a53": 52, "4b8f": 52, "4dba": 52, "4dd2": 52, "4e": [50, 51], "4ecd": 52, "4fee": 52, "4x": 76, "4x_0": [24, 59, 60, 65, 66], "4x_1": [24, 59, 60], "5": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 36, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 95, 102, 103, 108, 113, 114, 116], "50": [17, 50, 52, 53, 56, 61, 63, 66, 69, 70, 72, 73, 74, 76, 85, 86, 87, 88, 102], "500": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 25, 26, 30, 39, 48, 52, 54, 58, 59, 60, 65, 66, 69, 72, 75, 77, 78, 82, 83, 85, 86, 87, 89, 102, 103, 108, 114, 117], "5000": [36, 59, 60, 61, 76], "50000": 71, "500000": [72, 73], "5000000000000001": [61, 73, 76], "500084": 76, "500267": 67, "5003517412": 50, "500517": 76, "50093148e": 88, "500964": 74, "501021": 73, "501047e": 59, "501983": 76, "502005": 85, "502016": 70, "502084": 87, "502205": 59, "502284": 74, "502494": 61, "5025850": 50, "502595": 60, "502612": 76, "502901": 60, "502995": 76, "503": 35, "503374": 70, "503504": 86, "503511": 73, "503700": 56, "50398782e": 88, "504286": 71, "5042861": 50, "504548e": 60, "5050973": 50, "505264": 59, "505353": 60, "506050": 59, "506644": 59, "506659": 73, "506687": 73, "50672034": 50, "506900e": 76, "506903": 61, "507": 77, "507285": 68, "5073225": 88, "50768b": 74, "508153": 75, "508433": 59, "508459": 71, "5085": 73, "508947": 87, "509059": 73, "509196": 76, "509461": 76, "5095912": 88, "50967": 79, "5097": 79, "5098": [54, 83, 114], "509853": 76, "5099": [52, 54, 83, 114], "509951": 61, "509958": 71, "51": [49, 51, 52, 56, 65, 70, 85, 86, 87, 88, 102, 116], "510000e": [72, 73], "510121": 59, "510385": 71, "510555": 56, "51079110": 50, "511257": 70, "511515": 73, "511540": 73, "5115547": 81, "5115547181877": 81, "51155471818770": 81, "511665": 59, "511668": 79, "5116683753999614": 79, "511862": 76, "5119": 88, "512": 71, "512108": 76, "512149": 76, "51214922": 76, "51243406e": 88, "512519": 71, "512572": 76, "512672": [87, 103, 108], "5131": 72, "513222": 68, "513624": 85, "513992": 76, "514": 52, "514173": 60, "514545": 73, "515031": 59, "515338e": 59, "515358": 61, "5154": 73, "5154789948092002": 71, "5155": 52, "515672": 60, "516": 52, "516125": 61, "516222": 76, "516242": 60, "516255": 76, "516256": 76, "516528": 76, "516797": 60, "517": [52, 71], "517279": 60, "5175": 73, "517753": 59, "517798": 56, "518089": 74, "518175": 71, "518375": 60, "5184081": 88, "518446": 73, "518478": 56, "518610": 87, "518782": 73, "518846": 71, "518854": 56, "5195933": 88, "51966955": 50, "519710": 76, "52": [49, 52, 67, 70, 77, 85, 86, 87, 88, 102], "520": 73, "520415": 59, "520641": 78, "5207785": 88, "520930": 61, "521002": 61, "521233": 56, "521611": 60, "521632": 59, "521788": 59, "522": 47, "522753": 14, "522835": 63, "523030": 79, "523163": 61, "5232": 69, "52343523e": 88, "523794e": 76, "523807": 75, "523977545": 50, "52424539": 50, "524657": 76, "524934": [59, 60], "5250": 73, "525064": 56, "52510803": 51, "5251546891842586": 79, "5255": 52, "525722": 59, "52590": [51, 72], "526": 71, "526532": 73, "526582": 68, "526769": [59, 60], "526984": 60, "527226": 59, "52732": 86, "527452": 60, "527540": 59, "528381e": 80, "528580": 76, "528763": 60, "528937": [65, 66], "528996901": 50, "528997": 71, "529": 71, "529405": 49, "529468": 85, "529782": 49, "53": [49, 52, 56, 67, 83, 85, 86, 87, 88, 102, 112, 115], "530659": 67, "530793": 59, "530940": 76, "53094017": 76, "531": 52, "531223": 61, "531594": 73, "53209683": 87, "532266": 61, "53257": 86, "532738": 76, "53273833": 76, "532751": 65, "5329": 73, "533": 77, "533283": 87, "533489": 63, "533900": 76, "534139": 56, "5346": 52, "535179": 76, "535318": 76, "535609": 73, "535718e": 73, "53606675": 76, "536067": 76, "536143": 73, "5365695": 88, "536746": 76, "536778e": 60, "536798e": [72, 73], "537240": 76, "53724023": 76, "5378": 88, "53791422": 87, "538": 52, "538013": 73, "538105": 60, "5382": 78, "538937": [72, 73], "539455": 76, "539475": 76, "53947541": 76, "539491": [65, 66], "539767": 61, "54": [49, 51, 52, 62, 77, 82, 85, 86, 87, 88, 102, 116], "540240": 73, "540375": 68, "5405": 68, "540549": 68, "540720": 74, "5408": 49, "541": 77, "541060": 68, "541159": 76, "541216": 88, "54163": 78, "5416844": 81, "541684435562712": 81, "541821": 73, "541990": 73, "542": 77, "542136": 59, "542159": 60, "542170": 60, "542333": 73, "542446": 66, "542451": 76, "542560": 79, "542584": 61, "5425843074324594": 61, "542647": 76, "542648": 85, "542671": 71, "542883": [103, 108], "5428834": [103, 108], "542919": 85, "542989": 76, "543": [71, 73], "543052": 56, "543075": 61, "543136": 61, "543380": 71, "5434231": 81, "543423145188043": 81, "5436005": 50, "543691": 60, "543764": 66, "54378": 78, "543832": 76, "544097": 76, "544383": 79, "544555": 71, "544669": 65, "54483": [89, 102], "5448331": [89, 102], "54517706e": 88, "545492": 56, "54550506": 77, "545605e": 76, "545919": 73, "545930": 85, "546294": 73, "5467606094959261": 61, "546761": 61, "546953": 60, "547039": 59, "54716": 78, "547324": 60, "547431": 75, "5476": 73, "5479": 73, "547909": 73, "549109e": 73, "55": [51, 52, 61, 72, 73, 76, 85, 86, 87, 88, 102], "5500000000000002": [61, 73, 76], "550242": 70, "551317": 60, "551586928482123": 61, "551587": 61, "551686": 61, "55176": 86, "5518": 73, "552": 73, "552058": 78, "552508": 73, "552694e": 59, "552727": 71, "552776": 76, "553004": 56, "55307": 86, "553522": 60, "553754": 85, "553878": [16, 85], "553916": 73, "554076": 61, "554793e": 88, "555": 71, "555137": 60, "555150": 73, "555445": 75, "555498": 76, "5555": [48, 58], "555536": 59, "555949e": 73, "555954": 73, "556191": [59, 60], "556792": 76, "55697255": 88, "5574dcd4": 52, "557595": 71, "557731": 75, "557999": 71, "558134": [59, 60], "5584": 71, "5585": 71, "55863386": 87, "558655": 61, "5589": 71, "559": 117, "5590": 71, "559144": 61, "559186": 61, "5592": 71, "559394": 76, "559522": 76, "559592e": 59, "559680": 73, "55dc37e31fb1": 52, "55e": 51, "56": [52, 82, 85, 86, 87, 88, 102, 112, 115], "560135": [89, 102], "56018481": 76, "560185": 76, "5602727": 62, "560530": 60, "56058531": 88, "560689": 49, "560723": 67, "561348": 60, "5616": 72, "561711": 73, "561785": 87, "561883": 15, "5619573": 88, "562013": 76, "56223": 78, "562288": 79, "562390": 85, "562452": 70, "562518": 73, "5625561": 49, "562712": [59, 60], "563067": 77, "563374e": 61, "563503": 76, "563528": 73, "563563": 68, "563673": 73, "5637822": 88, "56387280e": 88, "56390147e": 88, "564045": 76, "564073": 73, "5641": 73, "564142": 61, "564232": [59, 60], "564451": 60, "564577": 73, "564798": 74, "565": 87, "565066": 61, "565373": 59, "566": 79, "566024": 76, "566091": 73, "566388": 59, "567004": 78, "567215": 70, "567343": 73, "567364": 60, "567529": 76, "567695": 59, "567945": [65, 66], "568287": 68, "569315e": 60, "569449": 85, "569540": 60, "569590": 70, "56965663": 76, "569657": 76, "569911": 50, "5699994715": 50, "57": [52, 77, 85, 86, 87, 88, 102, 117], "570038": 61, "5700384030890744": 61, "570111": 75, "5702": 73, "57027633": 88, "570486": 49, "570562": 49, "570722": 114, "570936": 59, "5717491": 88, "571778": 49, "5718": 73, "5722": 72, "572408e": 60, "57245066": 76, "572451": 76, "572991": 60, "573700": 63, "574": 52, "574160": 68, "5748": 86, "57496671": 50, "575": 20, "575381": 72, "57572422": 78, "575810": 59, "57585824": 78, "57592948e": 88, "57599221": 78, "575e": 77, "576": 52, "5763996": 50, "57643609": 78, "577": 52, "5770": 72, "57715074": 50, "577271": 71, "577273": 59, "5776971": 78, "57775704": 78, "577807": [59, 60], "577813": 59, "577e": 77, "578081": 73, "578307": 76, "578523": 71, "578557": 60, "578846e": 61, "579125": 72, "57914935": 51, "579197": 68, "579213": 79, "579238": 61, "579322e": 72, "579875e": 59, "57e": 51, "58": [21, 51, 72, 79, 85, 86, 87, 88, 102, 116], "5800": 73, "58000": 72, "5804": 52, "580414": 79, "580751": 72, "580853": 59, "580922": 65, "581655": 73, "581827": 70, "581849": 60, "582031": 72, "582146": 60, "58241568": 88, "582761": 61, "582991": 72, "583034": 65, "583195": [59, 60], "583201": 60, "5833333": 52, "583534": 76, "583692": 70, "584012": 73, "584057e": 59, "584742": 66, "584849": 61, "584928": 59, "584942e": 71, "5852": 73, "585426": 88, "585479": 68, "585793": 61, "586362": 76, "5864": 49, "5866": 73, "586719": 61, "586719493648897": 61, "586794": 59, "5868472": 50, "586921": 70, "587135": 60, "587292": 73, "588": 73, "58812": 86, "5882": 72, "588233": 59, "588364": 85, "588854": 59, "589147e": 70, "589248": 78, "589440": 61, "589958": 60, "59": [47, 60, 85, 86, 87, 88, 102], "590320": 63, "5905": 72, "590736": 76, "590813": 76, "590904": 60, "590911": 61, "590991": 61, "591080": 63, "591411": 65, "591441": 6, "591652": 72, "591678": 72, "591782": 76, "59199423e": 88, "592186": 60, "592681e": 61, "59307502e": 88, "593648": 86, "593981": 85, "594": 20, "594316e": 76, "595353": 61, "596": 73, "596069e": 73, "596270": [65, 66], "5964": 69, "596460": 60, "596758": 59, "597": [47, 51], "597098": 73, "597923": 73, "598178": 73, "59854797": 87, "5985730": 51, "59861": 73, "598761e": 60, "599297": [85, 86, 87], "5cb31a99b9cc": 52, "5d": [61, 76], "5x_2": 63, "5x_3": 63, "5z_i": 76, "6": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 27, 28, 48, 49, 50, 51, 52, 53, 54, 56, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 85, 86, 87, 88, 89, 102, 103, 108, 112, 114, 115, 116], "60": [50, 53, 61, 62, 73, 74, 76, 80, 85, 86, 87, 88, 102, 115], "600": 71, "6000": 73, "6000000000000002": [61, 73, 76], "600000e": 73, "600195": 59, "600254": 75, "600694": 87, "600776": 56, "601": 51, "601061": 61, "601598": 71, "601783e": 60, "601984": 60, "602079": 66, "602168": 61, "602322": 85, "602386e": 59, "602492": 59, "602587": 76, "602628": 61, "6029": 73, "604016": 73, "604111": 73, "604603": 9, "604825": 73, "604841": [72, 73], "605": 73, "605195": 66, "606034": 76, "606129": 76, "606342": 61, "606759": 73, "6068": 50, "606800": 71, "606954": 61, "607264": 59, "6075": 117, "607600": 73, "607900e": 60, "608": [64, 77], "608392": 76, "60857": 49, "608818": 78, "609": 77, "609522": 70, "609575": 87, "61": [56, 85, 86, 87, 88, 102, 116], "610318": 56, "611": 117, "6110": 73, "611269": 71, "61170069": 77, "611859": 66, "612": 77, "612246": 68, "612792": 73, "613244": 60, "6133": 51, "613314": 61, "613408": 76, "613498": 73, "613574": 67, "613622": 60, "613691": 87, "614": 77, "61404894": 87, "614188": 71, "614678": 73, "615": [47, 56], "615498": 6, "615863": [65, 66], "616372": 72, "616617": 60, "61669761": [103, 108], "616698": [103, 108], "616828": 73, "617": 71, "61728": 85, "617283": 73, "6173": 52, "61771229": 77, "617877": 76, "618069": 72, "61810738": 51, "618574": 60, "618776": 62, "618881": 60, "619": 77, "619128": 60, "619294": 56, "619351": [59, 60], "619390": [59, 60], "619454": 63, "619613": 72, "619903": 60, "61e": [51, 117], "62": [6, 56, 66, 67, 85, 86, 87, 88, 102], "620156": 76, "620874e": 87, "620995": 80, "621094": [72, 73], "621318": 76, "62131806": 76, "621359": 85, "621490": 76, "6215": 72, "622": [73, 77], "622153": 73, "622272": 56, "6224": 50, "623024": 61, "623173": 59, "624": 71, "6240": 78, "62403053": 62, "6243811": 50, "624535": 86, "624764": 60, "624798": 72, "624818": 60, "624919": 73, "624988": 73, "625": [50, 71], "625159": 67, "625477": 76, "625766": 65, "625767": 59, "625891": [65, 66], "626433": 76, "6266": 73, "626633": 60, "627": 74, "627505": [65, 66], "627560": 76, "627564": 61, "627588e": 73, "628": 77, "628069": 71, "629346": 73, "629549": 60, "629595": 18, "629740": 59, "629e": 77, "63": [50, 56, 71, 85, 86, 87, 88, 102, 115, 116], "630150e": 76, "630880": 77, "630914": 67, "631083": 60, "63117637": 87, "631333": 76, "6318": [72, 117], "632058": 71, "63245862e": 88, "632747e": 76, "632958": 75, "6330631": 102, "633433": 71, "634": [47, 77], "63407762": 117, "634078": [72, 117], "63432064": 88, "63432651": 88, "634577": 102, "63499": 73, "635": 35, "635000e": [72, 73], "635199": [72, 73], "635768": 59, "63593298": 87, "636048": 87, "636453": 13, "636575": 61, "637326": 76, "6379": 72, "638": 74, "638264": 76, "638461": 70, "638488": 67, "639": 72, "639135": 71, "63916605": 51, "639345": 73, "639580": 60, "639603": 60, "64": [56, 66, 72, 73, 74, 77, 85, 86, 87, 88, 102, 114], "640": 73, "640900": 73, "641528": 76, "641547": 76, "64154727": 76, "64197957": 76, "641980": 76, "642": 77, "6420": 73, "642016": 76, "642329": 56, "64269": 78, "643133": 73, "64340": 78, "643512": 61, "643752": 76, "643939": 56, "644113": 85, "644371": 60, "644665": 61, "64476745e": 88, "644799": 63, "644985": 59, "645": 73, "645583": 56, "64579": 49, "6458": 50, "645800": 71, "646": 74, "646117": 60, "646937": 63, "647002": 73, "647004": 87, "647010": 73, "647196": 63, "64723": 78, "647254e": 59, "647689": 79, "647864": 85, "647873": 76, "64797": 78, "648": 72, "648355": 59, "648690": 60, "648769": 60, "649": 115, "649158": 76, "649514": 59, "649738": 59, "65": [56, 61, 67, 73, 76, 77, 85, 86, 87, 88, 102], "650": [64, 87], "6500000000000001": [61, 73, 76], "650000e": 73, "650234": 56, "650810": 73, "650867": 61, "651127": 60, "652071": 73, "6522": 115, "652312": 65, "652324": 56, "652349": 76, "652350": 71, "652450e": [72, 73], "6527": 64, "652778": 71, "6528": 73, "6530": 73, "653008e": 72, "653829": 68, "653846": 61, "653901": [59, 60], "654070e": 87, "654755": 63, "655284": 76, "6553": 117, "6554": 115, "655422": 73, "655547": 59, "65557405e": 88, "655959": 68, "657": [47, 52], "658": 71, "65824669": 88, "658267": 76, "658592": 60, "6586": 49, "658702": 60, "659": 52, "659245": [59, 60], "659339": 60, "6593871": 49, "659423": [59, 60], "659473": 79, "659636": 61, "659735": 59, "659755": 77, "6598": 69, "659835": 60, "65e": 88, "66": [56, 68, 69, 74, 85, 86, 87, 88, 102, 114, 116], "660": [52, 87], "660073": 60, "660320": 66, "660479": 87, "6607402": 77, "660776": 76, "66133": 87, "661369": 75, "661388": 59, "6625": 73, "662975": 70, "663081975281988": 61, "663082": 61, "663177": 56, "663182": 61, "6634357241067617": 79, "663529": 76, "663533": 73, "663672": 68, "663765": 60, "664103e": 73, "664147": 73, "664409": 60, "664797": 59, "664824": 73, "664850": 71, "665264": 76, "66601815": 87, "666104": 76, "666307": 63, "6666667": 52, "666742": 85, "666959e": 68, "667": 71, "667274": 67, "667492e": 73, "667536": 76, "667614": 61, "667614205604159": 61, "667981": 59, "667985": 67, "668337": 73, "668452": 67, "668584": 63, "668981": 65, "669579": 60, "66989604": 62, "67": [47, 52, 72, 79, 85, 86, 87, 88, 102, 114], "670867": [17, 85], "671224": 60, "671271": [59, 60], "67136": 73, "6716717587835648": 61, "671672": 61, "671690": 59, "6722": 52, "672234": [59, 60], "672368": 61, "6723684718264447": 61, "672384": [59, 60], "67245350": 50, "672511": 59, "673092": [59, 60], "673302": 71, "673330": 60, "67410934": 50, "6745349414": 50, "674552": 73, "67456": 79, "674609": 61, "674747": 70, "674858": 74, "674949e": 78, "675233": 60, "675293": 75, "6753439": 88, "675625": 85, "675775": 70, "676": 47, "676405": 61, "6765": [51, 72], "676534": 102, "676641": 59, "676756": 76, "676807": 72, "677123": 59, "677614": 76, "677980": 61, "678": 77, "678117": 73, "678826": 61, "67936506": 87, "679539": 71, "679789e": 59, "67ad635a": 52, "68": [52, 56, 78, 85, 86, 87, 88, 102], "680": 73, "6810775": 78, "681176": 71, "681246": 60, "681448": 73, "681521": 59, "681562": 73, "681817dcfcda": 52, "682": 87, "682122": 70, "682269": 73, "682293509": 88, "6826": 72, "682875": 61, "683487": 60, "683581": 87, "683637e": 68, "683687": 60, "683942": 76, "683984": 14, "684": 117, "68410364": 51, "68411700": [51, 117], "684128": 60, "684142": 59, "684502": 76, "685104": 8, "685107": 76, "68554404e": 88, "68562150e": 88, "685807": 76, "685989": 87, "686270": 60, "686627": 59, "687345": 76, "687612": 60, "687647": 76, "687854": 63, "687871": 71, "6878711": 50, "688": 115, "6883896796": 88, "688540": 85, "688641": 70, "688747": 73, "688886": 85, "688918": 73, "688956": 59, "689088": [59, 60], "689188": 63, "689392": 76, "689600": 70, "689932": 59, "69": [67, 85, 86, 87, 88, 102, 116], "690334": 61, "6903344145051182": 61, "691097": 59, "691157": 62, "69140475e": 88, "691423": 59, "691511": 72, "691848e": 60, "691911": 85, "692297": 60, "692460": 70, "692579": 60, "692725": 76, "692907": 73, "692959": 59, "693": 117, "693316": 73, "693497e": 73, "693690": 73, "693796": 71, "694154": 61, "694561": 77, "694845e": 73, "694919": 71, "6950": 73, "695045": 59, "69508862": 87, "695581": 67, "6956016": 88, "69562150e": 88, "695711": 70, "695928": 59, "696011": [16, 85], "696224": 85, "696289": [65, 66], "69684828": 87, "696966": 60, "697": 71, "697000": 61, "697420": [65, 66], "697545": 76, "697616": 60, "697693": 59, "698223": 63, "698244": 63, "6983517": 88, "69840389e": 88, "698509": 59, "698694": 71, "698751": 70, "699035": 76, "699082": 61, "69921": 52, "699259e": 76, "699333": 61, "699616": 70, "699697": 60, "6_design_1a": 64, "6_r2d_0": 64, "6_r2y_0": 64, "6b": 102, "6cea": 52, "7": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 22, 23, 27, 30, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 102, 103, 108, 114, 115, 116], "70": [51, 53, 61, 65, 72, 73, 74, 76, 85, 86, 87, 88, 102, 116], "700": [59, 60, 64, 71], "7000000000000002": [61, 73, 76], "700015": 76, "700102": 76, "700314": 56, "700458": 59, "701078": 76, "701088": 72, "701106": 67, "701265": 65, "701413": 73, "701672e": 61, "701841e": 66, "701866": 76, "7018663": 76, "701966": 73, "702489": 73, "703049": 59, "703108e": 15, "703325": 68, "703772": 73, "7040": 73, "704482": 68, "7045": 68, "704558": 68, "704814": 59, "704896": 68, "705090": 60, "705354": 59, "705581": 73, "7055958": 81, "705595810371231": 81, "7055958103712310": 81, "705794": 60, "70583": 78, "706056": 73, "706077": 60, "706122": 60, "706430": 59, "706645": 61, "706657": 61, "706862": 9, "707125": 60, "707441": 60, "707738": 60, "707868": 76, "707963e": 72, "708190": 71, "708235": 59, "708459": 76, "708472": 60, "708821": 56, "708837": 56, "709026": 63, "709596": 59, "709606": [17, 85], "71": [85, 86, 87, 88, 102, 116], "710059": 56, "710319": 60, "710515": 59, "710586e": 71, "711024": 73, "7111941": 88, "711328": 73, "711383e": 59, "711518": 73, "711638": 87, "712064": 59, "712082": 73, "712095": 56, "712157": 75, "712268": 59, "712372e": 60, "712503": 78, "712592": 72, "712774": 65, "712846": 85, "712960": 61, "713": 73, "713407": 73, "713457": 59, "713963": 74, "713986": 73, "713993": 60, "714": 88, "714240": 71, "714250": 60, "714321": 72, "714534e": 60, "714651": 76, "71465114": 76, "715013": 73, "715180e": 73, "7154": 73, "715407": 61, "7155": 73, "7158581": 50, "716013e": 59, "716098": 56, "7161": 73, "716387": 59, "716427e": 60, "716456": 76, "716595e": 73, "716615": 56, "716762": 61, "716793": 61, "716799": 71, "7167991": 50, "716801": 70, "717": 73, "717130": 73, "717185": 76, "7184698": 88, "718686": 79, "719552": 60, "72": [53, 68, 85, 86, 87, 88, 102, 116], "720559": 59, "720571": 76, "720573": 59, "720589": 77, "720664": 71, "721018": 59, "721071": 76, "721245": 60, "7215093d9089": 52, "72155839e": 88, "721609": 73, "722316": 76, "722634": 76, "722848": 61, "722881": 76, "7229": 73, "723": 52, "723314": 76, "723342": 85, "723345e": 76, "723657": 59, "723846": 56, "7239": 73, "7241399": 50, "724338": 76, "724767": [65, 66], "724918": 79, "725": 52, "725010": 56, "725061": 59, "725087": 73, "725166": 76, "72538046": 88, "725565": 59, "725802": 9, "725820": 70, "725919": 59, "726": [52, 77], "7263442": 88, "726658": 70, "7268131": 50, "727159e": 60, "727543": 63, "727631": 74, "727693": 73, "727704": 73, "727976": 61, "7282094": 87, "728294": 75, "728710": 76, "72875815e": 88, "728852": 73, "728e": 77, "729668": 85, "729867": 59, "73": [51, 56, 85, 86, 87, 88, 102], "730023": 73, "7308": 49, "730809": 59, "731174": 59, "731317": 61, "732": 77, "732067": 59, "732137": 59, "732150": 60, "7326": 73, "732638": 76, "73285": 13, "732918": 65, "733": 73, "733047": 60, "733644": 59, "734635": 59, "734770": 60, "734948": 76, "735369e": 85, "7357": 73, "735848": 85, "735941": 12, "735964": 63, "736082": [59, 60], "736084": 76, "73608412": 76, "73632712": 88, "736823": 60, "737052": 73, "7375615": 51, "73764317e": 88, "737951": [59, 60], "738": 72, "738065": 60, "738223": 73, "738315": 73, "738659e": 73, "738876": 60, "739": [73, 117], "739063": 59, "7395359436844482": 61, "739536": 61, "739595": 77, "739720": 73, "739817": 67, "74": [21, 51, 60, 72, 85, 86, 87, 88, 102, 116], "740": 71, "740180e": 76, "740367": 59, "740417": 72, "740505": 56, "740785": 59, "740869": 61, "741104": 61, "741380": 77, "741523": 56, "741702": 76, "7418": 49, "74189": 52, "742128": 76, "742375": 59, "742407": 75, "742411": 59, "742907": 76, "7432": 49, "743247": 73, "743341": 60, "743609": 59, "7437": 73, "74402577": 76, "744026": 76, "744236": 78, "74461783e": 88, "745": 73, "745022": 56, "745444": 59, "745638": 72, "745881": 59, "746361": 76, "746843": 66, "7470": 73, "747646": 73, "747945": 50, "747961": 73, "748084": 60, "748377": 72, "748513": 73, "748880": 73, "74893119": 88, "74938952": 87, "749443": 73, "749854893": 89, "75": [17, 21, 23, 52, 56, 61, 63, 68, 72, 73, 76, 85, 86, 87, 88, 102, 116], "75000": 79, "7500000000000002": [61, 73, 76], "750000e": 73, "750597": 60, "750701": 56, "751013": 73, "751261": 73, "751633": 73, "75171": 72, "751710": [61, 72], "751712655588833": 81, "7517126555888330": 81, "751712656": 81, "752015": 11, "752283": 73, "752696": 56, "752909": 68, "753": 47, "7533": 72, "753323": 59, "753393": 59, "753493": 74, "753523": 76, "753866": 60, "754469": 59, "754499": 60, "754678": 70, "754692": 85, "7548": 79, "754870": 71, "755688": 59, "755701e": 59, "755885": 85, "755910": 73, "7559417564883749": 61, "755942": 61, "7560824": 50, "756200": 56, "756805": 71, "756867e": 73, "756905": 9, "756969": 61, "757": [77, 115], "757151": [59, 60], "757183": 61, "757411": 76, "757559": 68, "757819": 71, "757917e": 76, "758391": 73, "758831": 60, "75887": 52, "759006": 62, "759054": 60, "759833": 60, "76": [85, 86, 87, 88, 102, 115, 116], "760104": 76, "7603": 49, "760386": 87, "760778": 71, "760915": 63, "761": [50, 71], "761224": 56, "761429": 60, "761714": 61, "762284": 76, "76228406": 76, "762748": 73, "763691": 73, "764093": [59, 60], "76419024e": 88, "764315": 76, "76444177e": 88, "764478": 75, "7646": 73, "764798": 76, "764953": 72, "765202": 73, "765363": [59, 60], "765500e": [72, 73], "765710e": 80, "765792": 76, "765864": 78, "76591188": 50, "765960": 59, "7660": 49, "7663": 73, "766499": 76, "766940": 56, "76702611e": 88, "767188": [65, 66], "767435": 79, "767549": 60, "768071": 76, "768273": [65, 66], "768763": 60, "768798": 56, "769361": 76, "769805": 76, "77": [77, 85, 86, 87, 88, 102], "770556": 73, "770944": [65, 66], "7710": 78, "771157": 102, "771390e": 73, "7714": 74, "7716982": 51, "771741": 73, "771965": 73, "772104": 59, "772157": 68, "77227783e": 88, "772396": 60, "772444": 85, "772791": 73, "77289874e": 88, "773": 52, "773177": 61, "773339": 68, "773488": 76, "77348822": 76, "773769": 70, "77401500e": 88, "774271e": 73, "775": [52, 73], "775191": [59, 60], "775285": 59, "775969": 78, "776254e": 59, "7763": 72, "776728e": 71, "776887": 72, "7776071": 50, "777718": 70, "777728": 85, "777867": 68, "777e": 77, "778400": 59, "7786": 49, "779": 77, "779068": 68, "779108": 59, "779167": 6, "779355": 74, "779517": [59, 60], "779682": 61, "7799": 69, "779912": 73, "78": [77, 85, 86, 87, 88, 102, 116], "780": 52, "780068": 70, "780338": 59, "780458": 76, "780856": 72, "781": 73, "781233": 73, "781530": 76, "781681": 76, "782": 52, "782050": 76, "782555": 73, "783": 52, "783276": 87, "7833": 49, "7838": 49, "784": 102, "784238": 71, "784405": 78, "784483": 71, "784624": 61, "784792": 70, "784872": 56, "785": [47, 52], "785038": 60, "785153": 60, "785815": 56, "785911": 76, "785e": 35, "786": 52, "786090": 70, "786237": 59, "786563": 70, "786744": 61, "786986": 56, "78711285e": 88, "78777": 78, "788": 115, "78818": 52, "788868": 60, "789032": 59, "789039": 60, "789330": 60, "789671": 61, "789671060840732": 61, "79": [56, 85, 86, 87, 88, 116], "790039e": 59, "790115": 73, "790261": 85, "790723": [65, 66], "791": 74, "791097": 72, "791241": 76, "791297": [17, 85], "792396": 56, "792939": 61, "792972": 85, "793315": 85, "79338596e": 88, "793570": 76, "793598": 60, "793735": 76, "793818": [59, 60], "794": 87, "794366": 73, "79458848e": 88, "794805": 65, "795": 77, "795647": 76, "7957": 73, "795932": 86, "796014": 60, "796384": 60, "796444": 73, "796596e": 59, "796e": 77, "797086": 59, "797280": 76, "797454": 87, "79771727": 88, "797737": 102, "797868": 60, "79792890e": 88, "797965": 102, "798071": 8, "798308": 72, "798783": [65, 66], "799403": 76, "7999": 80, "7b428990": 52, "7x": 76, "8": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 40, 49, 50, 51, 52, 53, 54, 56, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 85, 86, 87, 88, 89, 102, 103, 108, 114, 116, 117], "80": [53, 61, 62, 73, 76, 80, 85, 86, 87, 88, 116], "800": 71, "8000": [32, 53, 80], "8000000000000002": [61, 73, 76], "800143": 59, "800326e": 59, "800351": 59, "801623": 73, "802": 77, "802289": 73, "802738": 85, "803112": 70, "803300": 59, "803492e": 76, "803563": 73, "803902e": 73, "804": 73, "804219": 76, "804284": 78, "804316": 76, "804484": 76, "8048": 51, "804828": 76, "804889": 73, "805007": 71, "805153e": [72, 73], "805293": 60, "8055563": 50, "805774": 59, "8059": 72, "806218e": 73, "806531": 73, "806554": 59, "806732": 67, "80696592e": 88, "806973": 74, "80714504e": 88, "807879": 76, "808": [51, 102], "808246": 72, "808284": 73, "808640": 73, "809125": 59, "8095": 74, "809913": [59, 60], "80a8": 52, "81": [50, 59, 64, 67, 69, 85, 86, 87, 88, 116], "810044": 72, "810134": 76, "8102": [49, 72], "810306": 56, "810322": 59, "810363": 73, "810382": [72, 73], "810419": 60, "810707": 73, "810895": 60, "811011": 60, "811155": 67, "811458": 72, "811513": 60, "8116912": 102, "811696": 59, "811825": 71, "811901": 76, "81190107": 76, "812": 77, "812132023242628313642435254555765718996": 88, "8132463": 50, "813293": 76, "813342": 102, "813682": 73, "814136": 61, "814246e": 60, "814351": 61, "814913": 71, "815": 74, "8152": 73, "815213e": 60, "815224": 102, "815226": 87, "81568484": 76, "815685": 76, "815993": 76, "816176": 79, "816318": 71, "816373": 59, "816752": 73, "816982": 59, "817": 47, "817119": 59, "817291": 73, "8173602": 69, "817967": 87, "81827267": 76, "818273": 76, "818289": 76, "81828926": 76, "81835208": 88, "818380": [59, 60], "81856": 52, "819223": 85, "819507": 70, "82": [79, 85, 86, 87, 88, 116], "8202": 51, "820366": 71, "8209": 51, "820963": 56, "821": 115, "8210": 51, "821021": 61, "821457": 73, "821566": 76, "821855": 85, "821970": 70, "821995": 60, "8221": 49, "822289": [72, 117], "82228913": 117, "822482": 61, "8227": 73, "822822": 61, "823247": 76, "823273": [59, 60], "824350": [59, 60], "824657": 70, "824701": 61, "824750": 61, "824889": 61, "824961e": 73, "8250": 49, "825587": 60, "825617": 71, "825862": 76, "825980": 61, "8259803249536914": 61, "8260": 72, "826065": [59, 60], "826426": 87, "826467e": 59, "826492": 76, "826519": [17, 85], "82666866e": 88, "82684324": 78, "827375": 62, "827381": 76, "827735": 76, "827938162750831": [65, 66], "828058": 73, "828157": 56, "828618": 68, "828778e": 59, "828912": 59, "828915": [65, 66], "829543": 61, "829730e": 60, "829764": 85, "82985": 67, "83": [85, 86, 87, 88, 116], "830263": 70, "830273": 56, "830301": 75, "830442": 59, "830467": 59, "830496": 74, "831": 77, "831019": 61, "831190": 60, "831278": 59, "831741": 59, "832086": 76, "8326928": 78, "832693": 78, "832875": 76, "83287529": 76, "833024": 71, "833065": 56, "833227e": 86, "833464": 73, "833907": 71, "834133": 68, "835": 77, "8350": 73, "835035": 70, "835344": 56, "835596": 73, "835750": 68, "835935": 60, "836234": 87, "838114": 76, "838235": 74, "838457": 73, "83905": 8, "84": [52, 67, 77, 85, 86, 87, 88, 116], "840041": 73, "840303": 76, "84030318": 76, "840673": 59, "840718": 87, "840836": 76, "840995e": 72, "841": [50, 71], "841132": 72, "8415": 51, "841847": 73, "842132": 87, "842405": 61, "842625": 71, "842746": 76, "8428": 72, "842853": 76, "843018": 85, "843730": 71, "843796": 59, "844": 47, "8440": 73, "844107e": 60, "844308": 76, "844549": [65, 66], "844663": 56, "844667": 102, "844707": 76, "844889": 71, "845241": 77, "845534": 70, "846388": 61, "847029": 60, "847555": 59, "847595": [16, 85], "847948": 61, "847962": 59, "847966": 73, "848176017": 88, "848688e": 70, "848757e": 72, "848868": 61, "84930915e": 88, "849427": 85, "849747": 78, "8497f641": 52, "8499": 73, "85": [26, 61, 67, 73, 76, 80, 85, 86, 87, 88], "8500000000000002": [61, 73, 76], "850038": 56, "850321": 71, "850439": 60, "850575": [59, 60], "850656": 68, "850794": 76, "851": 115, "851198": 73, "8513": 52, "851366": 71, "852": 73, "85265193": 69, "85280376": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18], "85397773": 87, "854020": 74, "8548731": 88, "855035": 59, "855780": 76, "855862": 60, "856404": 66, "856758": 85, "8571": 49, "857161": 76, "857515": 85, "857544": 71, "857765": 73, "858212e": 60, "858952": 56, "859": 73, "85911521e": 88, "85912862": 102, "859129": [89, 102], "8597": 72, "85974356": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18], "85c5": 52, "85e": 51, "86": [85, 86, 87, 88, 116], "860663": 102, "860804": 76, "860992": 73, "861019": 56, "861519": 59, "862043": [65, 66], "862359": 61, "863772": 72, "863982270": 89, "864": 77, "86415573": 51, "86424193e": 88, "8644": 52, "864664": 56, "864741e": 73, "865": 74, "865313": 73, "865562": [59, 60], "865854": 73, "865860": [72, 73], "865914": 60, "866102": [59, 60], "866179899731091": 81, "866179900": 81, "866579": 73, "866798": 73, "867": 74, "867201": 70, "867565": 76, "8679": 73, "8679322": 88, "868": 52, "8685788": 76, "868579": 76, "869": [52, 77], "869020": 61, "869195": 60, "869398": 60, "869477": 59, "869585": 15, "869586": 67, "87": [51, 59, 67, 71, 85, 86, 87, 88, 116], "8700": 51, "870099": [65, 66], "870260": 76, "870332": 76, "870444": 72, "870857": 76, "871": 52, "871545e": 59, "871923": 60, "871972": 68, "872": 77, "872132": 60, "872222": 73, "872727": 59, "872768": 76, "872852": 76, "87290240e": 88, "872994": 73, "873198": 73, "873677": [65, 66], "87384812361": 49, "87384812362": 49, "87430335": 102, "874303353": 102, "874702": [65, 66], "8750": 73, "8759": 73, "876": 77, "876083": 73, "87623301": 49, "876431e": 61, "876549": 73, "87674597e": 88, "8768": 49, "8771": 73, "877153": 73, "877455": 75, "877833": [59, 60], "877903": 56, "878281": 76, "878289": 73, "878402": 59, "878746": 56, "878847e": 73, "878895": 56, "878968e": 59, "879": 87, "879049": 73, "879058": 70, "879103": 61, "879509": 59, "87e": 51, "88": [51, 67, 77, 85, 87, 88], "880106": 71, "880579": 76, "880591": 75, "880808e": 73, "880880e": 73, "880886": 72, "8810": 72, "881201": 73, "88125046e": 88, "881465": 63, "881581": 12, "88173062": 50, "881937": 56, "882475": 61, "882641": 72, "882928": 56, "883485": 60, "883622": 76, "883914": 61, "883953": 70, "884": 47, "884132": 76, "8843": 78, "88436088": 88, "8845": 49, "884821": 85, "884996": 61, "8850": 51, "885065": 76, "885832": 77, "885956": 59, "885978": [65, 66], "886041": 60, "886086": [59, 60], "886266": 73, "88629": 49, "886314": 60, "88664": 52, "887345": 73, "887556": 61, "887648": 60, "887680": 59, "888146": 71, "8881461": 50, "888352": 56, "888445": 60, "888775": 66, "888804": 73, "889293": 76, "889326": 60, "889638": 56, "889733": 76, "889792": 60, "88988263e": 88, "889913": [59, 60], "889963": 76, "88ad": 52, "89": [51, 60, 85, 87, 88, 115, 116], "890": [50, 71], "89027368": 102, "890273683": 102, "890318": 59, "89035917": 67, "890372": [54, 83, 114], "8903720000100010000010": [52, 83, 114], "8904": 47, "890454": 86, "890665": 68, "890855": 56, "8909": [50, 72, 117], "891606": 72, "891752": 60, "891997": 59, "892": 52, "892648": 76, "89273": 87, "892796": [59, 60], "892828": 68, "893": 52, "8932105": 50, "893461": 56, "893649": [59, 60], "893851": 76, "894": 52, "894307e": 73, "894448": 60, "8946549": 53, "895106": [59, 60], "895308": 73, "895333": 76, "895442": 72, "895690": [59, 60], "895768e": 61, "896023": 76, "896182e": 68, "896263": 68, "897220": 76, "897240": 73, "8974": 72, "897451": 59, "897495e": 60, "898183": 56, "898722": 76, "899021": 85, "899250": 56, "899460": 76, "899654": 56, "899662e": 59, "899716": 60, "8bdee1a1d83d": 52, "8da924c": 52, "8e3aa840": 52, "9": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 49, 50, 51, 52, 53, 54, 56, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 83, 85, 86, 87, 88, 89, 102, 103, 108, 113, 114, 116, 117], "90": [28, 51, 53, 61, 62, 73, 76, 80, 85, 87, 88, 116], "9000000000000002": [61, 73, 76], "900000e": 73, "900021": 86, "900829": 68, "901013": 60, "901148": 76, "90136": 72, "901360": 72, "901526": 67, "901683": 73, "901705": 59, "902": 102, "902573": 61, "902920": 85, "903056e": 76, "903339": 61, "903351e": 61, "903418": 71, "903674": 59, "903681": 76, "903767": [59, 60], "904156": 61, "9041560442482157": 61, "904315": 59, "904396": 59, "905042": 60, "905494": 61, "905858": 79, "905951": 78, "906072": 85, "9061": 73, "906716732639898": [65, 66], "906757": 57, "907115": 76, "907176": 76, "9073": 73, "907491": 61, "907801": 71, "907879": 56, "90794478": 102, "907944783": 102, "907961": 73, "908024": 79, "908663": 60, "908767": 70, "909304": [59, 60], "909571": 56, "90963122e": 88, "909942e": 85, "909975": 73, "909997": [72, 117], "91": [85, 87, 88, 116], "910000e": 73, "910895": 60, "9109": 52, "91102953": 76, "911030": 76, "9112": 72, "911277": 60, "911662": 65, "912230": [59, 60], "9126": [51, 117], "9127": [51, 117], "912903": 59, "913": 52, "91315015": 50, "913280": 79, "913371": 60, "913415e": 59, "913485": 73, "913585": 68, "913774": 61, "9142": 73, "91438767e": 88, "9145": 49, "914598": 56, "915": [51, 52, 72, 73], "915000e": [72, 73], "915260e": 59, "915488": [65, 66], "9158080176561963": 70, "916": 74, "916236": 49, "916359": 56, "916528": 65, "9166667": 52, "916914": 76, "916930": 59, "917": 52, "917000": 60, "917066": 73, "917248": 76, "91724807": 76, "917436": 76, "918": 77, "918227": 61, "919432": 76, "9197": 73, "919969": 59, "91e": 51, "92": [85, 86, 87, 88, 116], "920052": 60, "920335": 73, "920337": 66, "920645": 73, "9209": 49, "9210": 73, "921061": 79, "921198": 68, "921256e": 60, "921372": 61, "921778": 68, "921913": 71, "921956": [59, 60], "921e4f0d": 52, "922160": 73, "922251": 59, "9223": 73, "922668": 68, "922996": 71, "923074e": 61, "923517": 80, "923607": 76, "92369755": 50, "923804": 61, "923943": 117, "923977": 73, "924002": 76, "9243": 73, "924396": [65, 66], "924443": 56, "924634": 63, "9248": 52, "924821": 61, "924843": 71, "924921": 85, "925": 62, "925248": [65, 66], "925660": 59, "925736": 61, "925957": 65, "925994": 72, "925995": 60, "926": 117, "926227": 60, "926621": 61, "926901": 70, "927": 48, "927074": 76, "927232": 73, "9274": 73, "927950": 73, "92827999": 87, "92881435e": 88, "928947": 71, "92905": 50, "929643": 59, "92972925e": 102, "929729e": [89, 102], "93": [51, 68, 85, 86, 87, 88, 116], "9304028": 50, "931": 82, "931479": 76, "931978": 114, "932027": 61, "932404e": 73, "9325": 49, "9327": 49, "932973": 76, "93300": 87, "933322": 60, "933671": 60, "933857": 60, "933996": 61, "934058": 59, "934243": 60, "934433": [59, 60], "9345": 52, "934500": 60, "934511": 102, "934549": 73, "93458": 78, "934963": 60, "934992": 61, "935": [35, 69, 87], "935591": 76, "935730": 76, "935764": 60, "935989": 71, "9359891": 50, "93648": 80, "936494": 59, "936739": 76, "937116": 71, "937586": 73, "938": 102, "939068": [65, 66], "9392": 73, "939250": 59, "939458": 59, "9395": 73, "93958082416": 117, "94": [62, 69, 85, 87, 88, 116, 117], "940354721701296": 61, "940355": 61, "940373": 73, "941440": 59, "941724": 73, "941788": 65, "942139": 66, "942312": 76, "942460e": 76, "942489": 73, "9425": 49, "942550": 73, "942661": 71, "942823": 73, "94293793": 88, "94309994e": 88, "943548": 68, "943938": 76, "943949e": 76, "944253e": 76, "944266": [65, 66], "944280": 73, "94441007e": 88, "94473": 53, "945881": 59, "946180": 68, "94629": 80, "946297": 61, "946406": 66, "946433": 76, "946533": 59, "946658": 73, "946968": 61, "947440": 75, "947466": 86, "947613": 60, "947855": 56, "9480": 73, "948112": 77, "948154e": 65, "948785e": 59, "948868": 73, "948975": 67, "94906344": 50, "949241": 102, "949456": 76, "9496717": 88, "949866": 60, "95": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 39, 49, 51, 53, 56, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 72, 73, 76, 77, 78, 79, 80, 85, 87, 88, 102, 103, 108, 116, 117], "9500": 73, "950158": 59, "950545": 57, "95062986e": 88, "951502": 76, "951532": 71, "951920": 75, "952": [51, 77, 117], "9523": 49, "952839": 76, "95305": 53, "9534": 73, "953683": 71, "953704": 59, "95372559e": 88, "953884": 87, "954": 102, "95401167e": 88, "955005e": 73, "9551": 73, "9552": 49, "955541": [17, 85], "95559917": 86, "955701": 59, "955e": 87, "956047": 50, "9561": 49, "956574": 73, "956588": 87, "956724": 61, "9567242535070148": 61, "956877": 60, "956892": 73, "957": 74, "957229": 66, "957375": 71, "957745": 61, "9579": 51, "957996": 61, "958": 102, "9580": 51, "958105": 85, "958541": 73, "959132": 60, "959384": 60, "95958808": 88, "959613": 85, "95e": 51, "96": [51, 59, 60, 74, 85, 87, 88, 116], "960236": 87, "9605": 73, "960808": 61, "960834": 60, "9609": 49, "961539": 73, "961962": 61, "962364": 56, "962373": 60, "962954": 60, "963055": 73, "963427e": 60, "964025e": 76, "964065e": 60, "964261e": 71, "964318": 73, "9647": 49, "965341": 60, "965531": 87, "965696": 59, "965774": 73, "96582": 86, "966015": 76, "966097": 18, "966659": 61, "9666592590622916": 61, "967": 47, "967092": 60, "967467": 78, "968": 74, "968127": 56, "968134e": 76, "968258e": 59, "968577": 62, "969141": [85, 86, 87], "9699": 72, "969925e": 60, "97": [6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 54, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 85, 86, 87, 88, 89, 102, 114, 116, 117], "970065": 76, "970150": 60, "971058": [65, 66], "972509": 60, "97270930": 88, "972745": 59, "972748": 61, "97276281": 76, "972763": 76, "97314470": 50, "973156": 85, "973229": 60, "973241": 76, "973331": 73, "973741": 60, "973890": 60, "974202": 61, "974213": 60, "97441062": [65, 66], "974414": 61, "974487": 59, "97470872": 78, "9748910611": 50, "975": [59, 60, 65, 66, 68, 69, 74], "975289": 56, "9753": 52, "975447": 62, "975450": 60, "975461": 71, "975592": 56, "976088": 76, "976548e": 60, "976562": 76, "977202": 60, "977280": [59, 60], "977295": 73, "977507": 60, "977820": 59, "978303": 70, "9787": 73, "978977": 76, "979": 77, "979384": 68, "979475": 59, "979702": 59, "979857": 59, "979966": 68, "979971e": 59, "98": [59, 60, 73, 85, 87, 88, 116], "980026": 73, "9802393": 50, "980440": 60, "980643e": 61, "981104": 75, "981403": 60, "981438": 59, "981672": 61, "981715": 59, "982": 74, "982019e": 60, "982353e": 73, "982417": 61, "982720": 59, "982797": 75, "983": 74, "983192": 76, "983253": 59, "983759": 117, "98393441": 78, "984": 74, "984024": 75, "984083": [65, 66], "984551": 11, "984562": 76, "984866": 102, "984872": [59, 60], "984937": 61, "985": 74, "98505871e": 88, "985207": [59, 60], "985654": 60, "986249": 72, "986383": 73, "986417": 59, "98673": 67, "9870004": 52, "987220": 73, "987307": 70, "9875": 49, "987726": 60, "9880384": 52, "988421": [59, 60], "988463": 76, "988541": 59, "988709": 73, "988780": 73, "989723": 74, "99": [51, 56, 59, 60, 74, 85, 87, 88, 116], "990210": 73, "990462": 74, "990903": 59, "991": 52, "9914": [72, 73, 78], "991444e": 65, "9915": [51, 72, 73, 78], "991512": 51, "991963": [59, 60], "991977": 73, "991988": 59, "992": 77, "99232145": 78, "992582": [59, 60], "993201": 60, "993416": 68, "993575": 73, "994": 77, "994168239": 50, "994208": 56, "994214": 73, "994332": 57, "994377": 59, "9944": [69, 87], "9948104": 53, "994851": 73, "994937": 66, "995015": 73, "9951": 49, "995248": 76, "99549118e": 88, "99570898": 88, "99571372e": 88, "9961": 72, "9961392": 50, "996313": 59, "996892": 70, "996934": 71, "9970": 73, "997034": 80, "997494": 80, "997571": 71, "997621": 61, "997934": [65, 66], "998063": 57, "99864670889": 117, "998766": 73, "999": [47, 62, 63, 67, 74, 78, 117], "999207": 76, "9995": [59, 60, 63], "9996": [59, 60, 63], "9996553": 51, "9997": [59, 60, 63], "9998": [59, 60, 63], "9999": [59, 60, 63], "99c8": 52, "A": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 26, 27, 31, 35, 37, 38, 39, 41, 42, 47, 48, 49, 51, 52, 57, 58, 64, 66, 69, 70, 74, 75, 77, 78, 79, 82, 83, 85, 86, 87, 102, 103, 104, 105, 109, 110, 111, 112, 114, 115, 117], "ATE": [12, 18, 21, 51, 54, 56, 68, 72, 78, 79, 85, 87, 89, 95, 103, 109], "ATEs": [56, 74], "And": [53, 74, 80, 103, 106], "As": [48, 49, 50, 51, 52, 53, 56, 58, 59, 60, 61, 62, 64, 67, 68, 69, 70, 71, 72, 73, 74, 76, 79, 80, 81, 86, 87, 88, 89, 91, 102, 103, 105, 111, 117], "At": [21, 22, 23, 50, 56, 62, 63, 67, 69, 71, 73, 76, 117], "Being": 117, "But": [68, 69], "By": [49, 50, 71, 77, 79, 86, 87, 103, 108], "For": [4, 8, 9, 11, 12, 15, 23, 33, 34, 38, 42, 47, 49, 50, 52, 56, 57, 62, 67, 68, 69, 70, 71, 73, 75, 77, 78, 79, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 105, 106, 107, 109, 111, 113, 114, 117], "ITE": [27, 56], "ITEs": 56, "If": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 28, 29, 30, 31, 32, 35, 36, 37, 38, 39, 41, 42, 48, 50, 58, 59, 60, 62, 68, 69, 71, 73, 77, 82, 83, 85, 86, 87, 89, 90, 92, 93, 95, 102, 103, 105, 106, 107, 108, 110, 111, 112, 117], "In": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 37, 41, 47, 48, 49, 50, 51, 52, 53, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117], "It": [49, 50, 51, 59, 60, 64, 65, 66, 71, 72, 73, 77, 79, 86, 88, 112, 116], "No": [25, 47, 49, 51, 52, 53, 54, 56, 62, 67, 72, 73, 77, 78, 80, 83, 86, 87, 89, 102, 114, 115], "Of": [69, 102, 117], "On": [48, 58, 70, 74, 82, 115], "One": [51, 72, 73, 79, 85, 102], "Or": 35, "Such": [79, 86], "That": [35, 117], "The": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 47, 48, 49, 50, 52, 53, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 75, 76, 77, 78, 79, 80, 81, 84, 85, 86, 87, 88, 89, 90, 95, 98, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 115, 116, 117], "Then": [23, 61, 76, 87, 102, 103, 111, 112, 113], "There": [51, 72, 79, 87, 113, 117], "These": [51, 52, 55, 70, 72, 75, 77, 78, 85, 87, 117], "To": [34, 47, 48, 50, 51, 52, 53, 56, 57, 58, 59, 60, 61, 62, 65, 66, 68, 69, 71, 72, 73, 75, 76, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 102, 103, 105, 108, 111, 113, 114, 117], "With": [26, 59, 60, 86, 115], "_": [48, 50, 58, 59, 60, 61, 63, 64, 65, 66, 71, 72, 73, 75, 76, 77, 81, 82, 85, 87, 88, 89, 102, 103, 105, 108], "_0": [48, 50, 58, 64, 71, 81, 82, 88, 89, 97, 98, 102, 103, 111], "_1": [21, 22, 23, 27, 53, 74, 80, 89, 97, 98], "_2": [21, 22, 23, 27, 74], "_3": [21, 22, 23, 27], "_4": [21, 22, 23, 27], "_5": [21, 27], "__": [41, 42], "__init__": 70, "__version__": 113, "_all_coef": 88, "_all_s": 88, "_compute_scor": 34, "_compute_score_deriv": 34, "_coordinate_desc": 71, "_d": [77, 87], "_est_causal_pars_and_s": 116, "_estimator_typ": 70, "_h": [77, 87], "_i": [48, 53, 58, 76, 80, 82], "_id": 88, "_j": [21, 22, 23, 27, 29, 50, 71, 102], "_l": 86, "_m": [86, 88], "_n": [89, 92, 93, 95, 102, 103, 108, 110], "_n_folds_per_clust": 71, "_offset": 86, "_pred": 86, "_rmse": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18], "_x": 36, "_y": [77, 87], "a0": 70, "a09a": 52, "a09b": 52, "a1": 70, "a3d9": 52, "a4a147": 74, "a5e6": 52, "a5e7": 52, "a6ba": 52, "a79359d2da46": 52, "a840": 52, "a_": [53, 80], "a_0": 30, "a_1": 30, "a_j": 87, "ab": [49, 112], "ab71": 52, "abadi": [19, 62], "abb0fd28": 52, "abdt": [54, 83, 114], "abl": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 48, 58, 69, 73, 74, 86, 103, 105, 111], "about": [51, 69, 72, 87, 112, 114, 117], "abov": [48, 51, 56, 58, 59, 60, 65, 66, 69, 70, 72, 74, 75, 76, 77, 79, 82, 85, 86, 87, 113], "absolut": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 69, 86], "abstract": [34, 49, 50, 71, 89, 112, 116], "acc": [5, 49], "accept": [85, 86], "access": [37, 38, 49, 51, 65, 66, 67, 69, 78, 86, 103, 108, 117], "accord": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 48, 51, 53, 56, 58, 61, 62, 72, 76, 77, 79, 80, 86, 87, 102, 103, 104, 106, 107, 109, 117], "accordingli": [53, 62, 69, 70, 72, 77, 80], "account": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 50, 51, 71, 72, 73, 78, 79, 103, 108, 111, 117], "accumul": [51, 72, 73, 78], "accuraci": [37, 41, 49, 87], "acemoglu": 115, "achiev": [50, 68, 71, 75, 79, 87, 102], "acic_2024_post": 74, "acknowledg": [51, 52, 72], "acm": 115, "acov": 115, "across": [51, 72, 74, 117], "action": 116, "activ": [7, 10, 113, 116], "actual": [35, 67, 79], "acycl": [53, 80, 117], "ad": [7, 10, 19, 20, 34, 37, 38, 41, 42, 67, 83, 86, 87, 102, 103, 105, 116], "adapt": [11, 72, 116], "add": [49, 50, 53, 54, 56, 62, 63, 65, 66, 67, 74, 76, 77, 78, 79, 80, 86, 87, 115, 116], "add_trac": 79, "addit": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 23, 27, 29, 30, 31, 36, 39, 64, 79, 86, 87, 89, 96, 103, 104, 109, 111, 115, 116], "addition": [21, 22, 56, 61, 73, 78, 86, 87, 88, 102, 103, 108, 114], "address": 79, "adel": 115, "adj": [77, 79], "adj_coef_bench": 79, "adj_est": 79, "adj_vanderweelearah": 79, "adjust": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 40, 50, 63, 68, 71, 73, 78, 79, 85, 87, 102, 103, 108, 115, 116, 117], "adopt": [62, 87], "advanc": [70, 84, 88, 115], "advantag": [48, 49, 51, 56, 58, 72, 73, 82, 113], "advers": [103, 105], "adversari": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 78, 103, 108, 111], "ae": [48, 50, 51, 53], "ae56": 52, "ae89": 52, "aesthet": 48, "aeturrel": 31, "afd9e4": 74, "affect": [56, 64, 87, 116, 117], "after": [49, 51, 52, 53, 62, 64, 72, 73, 79, 80, 85, 86, 103, 106, 108, 113, 117], "after_stat": 48, "ag": [51, 72, 73, 75, 78, 117], "again": [48, 49, 50, 51, 53, 56, 58, 62, 67, 70, 71, 72, 77, 78, 79, 80, 82, 103, 106], "against": [62, 67, 69, 75, 86], "agebra": 85, "agegt54": [52, 54, 83, 114], "agelt35": [52, 54, 83, 114], "agg": 49, "aggreg": [49, 81, 88, 116], "aggregate_over_split": 35, "aggt": 49, "aim": 77, "aipw": 74, "aipw_est_1": 74, "aipw_est_2": 74, "aipw_obj_1": 74, "aipw_obj_2": 74, "air": [50, 71], "al": [19, 20, 24, 26, 29, 30, 48, 50, 51, 52, 58, 59, 60, 61, 62, 64, 65, 66, 69, 71, 72, 73, 76, 78, 82, 87, 88, 89, 91, 95, 96, 101, 102, 103, 105, 111, 112, 114, 116], "alexandr": [64, 115], "algebra": 87, "algorithm": [47, 49, 50, 52, 53, 56, 58, 61, 62, 68, 69, 71, 73, 76, 78, 80, 84, 86, 87, 88, 89, 102, 116, 117], "alia": [37, 38, 41, 42], "align": [48, 50, 53, 58, 61, 63, 69, 71, 72, 74, 75, 76, 80, 116], "all": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 23, 38, 41, 42, 43, 48, 49, 50, 51, 53, 56, 58, 62, 67, 68, 69, 70, 71, 72, 73, 75, 77, 79, 80, 82, 83, 85, 86, 87, 88, 102, 103, 111, 112, 113, 116], "all_coef": 88, "all_dml1_coef": 81, "all_s": 88, "all_smpl": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 68], "all_smpls_clust": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18], "all_z_col": [50, 71], "allow": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 34, 37, 38, 41, 42, 51, 56, 72, 73, 77, 85, 86, 87, 88, 89, 102, 112, 116, 117], "almqvist": 115, "along": 86, "alpha": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 28, 30, 48, 50, 51, 53, 54, 56, 58, 59, 60, 61, 64, 68, 69, 70, 71, 72, 73, 76, 81, 82, 85, 86, 87, 88, 89, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111], "alpha_": [29, 50, 71, 86], "alpha_0": [103, 111], "alpha_ml_l": 64, "alpha_ml_m": 64, "alpha_x": [11, 25, 87], "alreadi": [23, 53, 62, 80, 86, 87], "also": [4, 8, 9, 11, 12, 15, 47, 48, 49, 50, 51, 52, 56, 57, 58, 59, 60, 62, 65, 66, 67, 69, 70, 71, 72, 73, 75, 77, 78, 79, 82, 85, 86, 87, 88, 89, 102, 103, 105, 113, 114, 116, 117], "alter": [50, 71], "altern": [49, 51, 52, 72, 75, 84, 86, 102, 112, 114], "although": 79, "alwai": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 38, 42, 49, 77, 116], "always_tak": [11, 51, 72], "amamb": 71, "american": [28, 74], "amgrem": 71, "amhorn": 71, "amit": [79, 115], "amjavl": 71, "ammata": 71, "among": [51, 64, 72, 73, 78, 79], "amount": [51, 70, 72, 73, 117], "amp": [47, 50, 52, 53, 62, 71, 73, 78, 80], "an": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 27, 37, 38, 41, 42, 48, 49, 50, 51, 52, 53, 56, 58, 59, 60, 64, 67, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 102, 103, 105, 108, 112, 114, 115, 116, 117], "analog": [33, 34, 50, 71, 73, 78, 85, 87, 89, 92, 93, 102, 103, 108], "analys": 117, "analysi": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 36, 48, 50, 51, 58, 71, 72, 73, 82, 84, 85, 105, 108, 111, 112, 116], "analyt": [74, 76], "analyz": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 51, 72, 73, 78, 117], "ancillari": 79, "andrea": 115, "angl": 51, "angrist": 74, "ani": [47, 48, 49, 52, 53, 57, 58, 62, 79, 80, 82, 87, 113, 117], "anna": [8, 9, 21, 22, 23, 27, 49, 62, 87, 115], "annal": [102, 115], "anneal": 86, "annot": 48, "annual": 115, "anoth": [48, 49, 50, 51, 58, 69, 70, 71, 82, 86, 87], "anticip": 49, "anymor": [50, 71], "aos1161": 102, "aos1230": 102, "aos1671": 102, "ap": [51, 72], "ape_e401_uncond": 51, "ape_p401_uncond": 51, "api": [83, 112, 116], "apo": [4, 5, 90, 104], "apoorva": 116, "apoorva__l": 74, "apoorval": 74, "app": 116, "appeal": 79, "append": [58, 69, 82], "appendix": [26, 32, 53, 78, 80, 103, 105], "appli": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 20, 47, 48, 50, 51, 52, 53, 58, 62, 63, 68, 69, 71, 72, 73, 77, 79, 80, 82, 87, 88, 89, 102, 112, 114, 116, 117], "applic": [48, 58, 62, 74, 79, 82, 85, 88, 115, 117], "apply_along_axi": 75, "apply_cross_fit": [48, 88], "apply_crossfit": 116, "appreci": 112, "approach": [4, 6, 8, 9, 11, 12, 13, 16, 17, 18, 49, 50, 56, 71, 77, 78, 79, 84, 86, 88, 102, 103, 105, 113, 115, 117], "appropri": [51, 64, 72, 87, 88, 117], "approx": 85, "approxim": [48, 58, 59, 60, 61, 69, 76, 79, 82, 85, 87, 102, 116, 117], "apt": 113, "ar": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 30, 31, 33, 34, 35, 37, 38, 39, 40, 41, 42, 48, 49, 50, 51, 52, 53, 55, 56, 58, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 99, 100, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117], "arang": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 58, 61, 63, 73, 75, 76, 78, 79, 86], "arbitrarili": [38, 42], "architectur": [89, 115], "arellano": 115, "arg": [70, 77, 85, 87], "argmin": 69, "argu": [48, 51, 58, 72, 73, 78, 82, 117], "argument": [4, 12, 15, 23, 29, 30, 31, 35, 36, 39, 51, 59, 60, 62, 67, 69, 72, 73, 81, 85, 86, 87, 116, 117], "aris": [48, 49, 50, 58, 71, 79, 82, 117], "aronow": 74, "around": [49, 51, 72, 73, 77, 87, 89], "arr": 75, "arrai": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 23, 25, 26, 28, 29, 30, 31, 32, 37, 38, 39, 40, 41, 42, 53, 56, 58, 59, 60, 61, 62, 69, 71, 74, 75, 78, 79, 80, 81, 82, 84, 85, 86, 88, 102, 103, 108, 114, 116, 117], "arrang": 50, "array_lik": 17, "articl": [31, 112], "arxiv": [29, 49, 50, 71, 79, 112, 115, 116], "as_learn": [52, 86], "asarrai": [59, 60], "aspect": [51, 72, 73], "assert": 86, "assess": 49, "asset": [73, 78, 117], "assign": [7, 10, 51, 66, 72, 77, 85, 86, 87, 117], "assmput": 87, "associ": [51, 64, 72, 87, 102, 115], "assum": [47, 50, 57, 62, 71, 74, 75, 79, 87, 89, 92, 93, 102, 103, 111, 117], "assumpt": [49, 50, 51, 53, 62, 63, 69, 71, 72, 74, 77, 80, 87, 102, 117], "assur": 116, "astyp": [57, 77, 79], "asymptot": [33, 34, 48, 50, 58, 71, 82, 88, 102, 115], "ate": 56, "ate_estim": [53, 80], "ates": 56, "athei": 115, "att": [12, 21, 49, 63, 67, 68, 75, 79, 85, 87, 89, 95, 103, 109, 116], "att_gt": 49, "attach": 49, "atte_estim": 62, "attempt": [37, 38], "attenu": [51, 72], "attr": 51, "attribut": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 35, 39, 40, 41, 42, 69, 70, 81, 86, 88, 89, 102], "attributeerror": [37, 38], "attrict": 87, "attrit": [18, 53, 80, 87], "au": [52, 86, 112, 114], "auc": 49, "author": [49, 79, 112], "auto_ml": 70, "autodoubleml": 70, "autom": 70, "automat": [48, 58, 67, 82, 85, 103, 108], "automl": 116, "automl_l": 70, "automl_l_lesstim": 70, "automl_m": 70, "automl_m_lesstim": 70, "automobil": [50, 71], "autos": 64, "autosklearn": 70, "auxiliari": [48, 58, 82], "avail": [25, 49, 51, 52, 56, 62, 64, 69, 72, 73, 74, 75, 77, 79, 82, 85, 86, 87, 103, 111, 112, 113, 116, 117], "avaiv": 40, "aver": 56, "averag": [4, 5, 11, 12, 15, 21, 22, 23, 47, 49, 52, 53, 57, 62, 63, 67, 73, 74, 75, 77, 78, 79, 80, 84, 90, 95, 102, 104, 109, 115, 116, 117], "average_it": 56, "avoid": [48, 49, 58, 77, 87, 88, 113, 116], "awai": 78, "ax": [56, 58, 59, 60, 61, 63, 65, 66, 69, 70, 71, 72, 73, 74, 76, 77], "ax1": [56, 61, 68, 73, 76], "ax2": [56, 61, 68, 73, 76], "axhlin": [63, 70, 77], "axi": [50, 51, 56, 64, 68, 69, 71, 72, 74, 75, 77], "axvlin": [56, 58], "b": [8, 9, 31, 48, 50, 52, 58, 59, 60, 71, 74, 76, 77, 79, 82, 85, 86, 102, 103, 111, 112, 114, 115], "b208": 52, "b371": 52, "b5d34a6f42b": 52, "b5d7": 52, "b_": 87, "b_0": 30, "b_1": 30, "b_j": 31, "bach": [64, 69, 70, 79, 112, 115, 116], "backbon": 69, "backend": [7, 10, 49, 73, 78, 79, 84, 116], "backward": 116, "bad": 74, "balanc": [51, 72, 73], "band": [49, 84, 117], "bandwidth": [13, 16, 17, 35, 77, 87], "bar": [67, 70, 72, 85, 87, 89, 90, 95, 103, 104], "base": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 24, 27, 36, 40, 48, 49, 50, 51, 53, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 102, 103, 105, 108, 112, 114, 115, 116, 117], "base_estim": [41, 42, 77], "baselin": [27, 51, 70, 72], "basi": [4, 12, 15, 39, 59, 60, 68, 85], "basic": [49, 50, 51, 62, 71, 72, 73, 74, 77, 78, 79, 84, 86], "basis_df": 68, "basis_matrix": 68, "batch": 52, "battocchi": 115, "bay": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 102], "bb2913dc": 52, "bbotk": [52, 86, 116], "bbox_inch": 58, "bbox_to_anchor": 58, "bcallaway11": 49, "bd929a9e": 52, "bde4": 52, "becam": [51, 72, 73], "becaus": [38, 42, 47, 48, 49, 50, 57, 58, 66, 67, 71, 74, 79, 82, 117], "becker": [52, 86], "becom": [50, 66, 70, 71, 85, 88], "bee": 63, "been": [50, 51, 70, 71, 72, 73, 78, 79, 85, 86, 116], "befor": [49, 51, 56, 63, 67, 72, 76, 79, 87, 117], "begin": [25, 28, 29, 48, 50, 51, 52, 53, 58, 61, 63, 69, 71, 72, 74, 75, 76, 80, 81, 83, 86, 88, 102, 114, 117], "behav": 66, "behavior": [51, 74, 86], "behaviour": 66, "behind": 87, "being": [27, 32, 33, 34, 36, 41, 42, 50, 71, 77, 79, 87, 88, 89, 91, 102, 103, 108, 112], "belloni": [26, 64, 102, 115], "below": [47, 51, 57, 72, 74, 87, 113, 114], "bench_x1": 79, "bench_x2": 79, "benchmark": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 43, 56, 67, 105, 116], "benchmark_dict": [43, 78], "benchmark_inc": 78, "benchmark_pira": 78, "benchmark_result": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18], "benchmark_twoearn": 78, "benchmarking_set": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 56, 67, 78, 79, 103, 105], "benchmarking_vari": 67, "benefit": [48, 51, 58, 72, 82], "bernoulli": 25, "berri": [50, 71], "besid": 114, "best": [4, 12, 15, 38, 39, 42, 59, 60, 65, 66, 70, 113], "best_loss": 70, "beta": [18, 25, 26, 28, 32, 51, 53, 72, 75, 77, 80, 87], "beta_": [53, 80], "beta_0": [24, 53, 75, 80, 85], "beta_a": [21, 22, 79], "beta_j": [25, 26, 28, 32], "better": [49, 56, 69, 79], "between": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 47, 53, 56, 57, 61, 63, 64, 70, 74, 76, 78, 79, 80, 87, 89, 92, 93, 95, 99, 100, 102, 103, 111, 114, 116], "betwen": [47, 57], "beyond": 115, "bia": [32, 47, 53, 57, 64, 77, 79, 80, 84, 87, 88, 89, 97, 98, 103, 111, 115, 116], "bias": [47, 51, 57, 72, 73, 78, 117], "bias_bench": 79, "bibtex": 112, "big": [64, 81, 88, 89, 93, 96, 102, 103, 106, 107, 109, 110, 111], "bigg": [50, 71, 89, 94, 95, 103, 109], "bilia": 20, "bin": [48, 56, 58, 113], "binari": [4, 6, 8, 9, 11, 12, 13, 15, 16, 18, 24, 36, 47, 49, 51, 52, 57, 62, 67, 68, 69, 72, 74, 75, 79, 85, 86, 103, 104, 109, 116, 117], "binary_outcom": 36, "binary_treat": [24, 59, 65, 67], "bind": 116, "binder": [52, 86, 112, 114, 116], "binomi": [57, 74, 75, 76], "bischl": [52, 86, 112, 114], "black": [48, 52, 54, 83, 114], "blob": 49, "blog": 31, "blondel": [112, 114], "blp": [39, 50, 71], "blp_data": [50, 71], "blp_model": [65, 66], "blue": [48, 50, 53, 71], "bodori": 115, "bond": [51, 72, 73], "bonferroni": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 102], "bonu": [20, 52, 83, 114], "book": [52, 79, 86], "bool": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 24, 27, 35, 36, 37, 38, 39, 41, 42, 67, 77], "boolean": [32, 65, 66, 83, 88], "boost": [47, 51, 57, 62, 69, 72], "boost_class": [51, 72], "boost_summari": 72, "boostrap": [61, 116], "bootstrap": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 39, 56, 59, 60, 61, 65, 66, 73, 76, 84, 85, 88, 89, 112, 114, 116, 117], "both": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 22, 24, 49, 51, 52, 62, 63, 68, 69, 70, 72, 73, 75, 77, 78, 79, 83, 86, 87, 102, 103, 105, 108, 110, 111, 116, 117], "bottom": [50, 51, 69, 71, 72, 73], "bound": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 27, 51, 56, 67, 68, 72, 78, 79, 103, 105, 108, 111, 117], "branch": 52, "brantli": 49, "break": [48, 116], "breviti": 117, "brew": 113, "brewer": 50, "bridg": 79, "brief": 82, "bring": [47, 57], "brucher": [112, 114], "bsd": 116, "bst": 72, "budget": [70, 86], "bug": [112, 116], "build": [50, 69, 71, 75], "build_design_matric": [59, 60], "build_sim_dataset": 49, "built": [40, 70, 86, 112], "bureau": [79, 88, 115], "busi": [29, 32, 50, 71, 79, 115], "b\u00fchlmann": 115, "c": [19, 20, 22, 23, 26, 28, 30, 47, 48, 49, 50, 51, 52, 54, 57, 58, 63, 64, 65, 66, 71, 72, 74, 77, 82, 83, 86, 87, 112, 113, 114, 115, 117], "c1": [19, 20, 30, 50, 64, 71, 82, 112, 115], "c68": [19, 20, 30, 50, 64, 71, 82, 112, 115], "c895": 52, "c_": 102, "c_d": [26, 103, 109, 110, 111], "c_y": [26, 103, 111], "ca1af7be64b2": 52, "caac5a95": 52, "calcualt": 75, "calcul": [4, 12, 15, 49, 51, 56, 59, 60, 61, 65, 66, 69, 70, 72, 76, 78, 103, 108, 111], "calibr": [69, 70, 79], "call": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 38, 41, 42, 47, 49, 50, 51, 52, 53, 57, 59, 60, 61, 62, 65, 66, 71, 72, 73, 75, 76, 77, 78, 79, 80, 83, 86, 88, 89, 102, 103, 108, 111, 114, 116, 117], "callabl": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 48, 58, 59, 60, 69, 84, 86, 112], "callawai": 49, "camera": 64, "cameron": [50, 71], "can": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 24, 35, 38, 40, 42, 47, 48, 49, 50, 51, 52, 53, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 92, 93, 95, 99, 100, 102, 103, 104, 105, 108, 109, 110, 111, 112, 113, 114, 116, 117], "candid": 79, "cannot": [69, 77, 79, 87, 117], "capabl": [7, 10, 47, 57], "capo": [4, 68], "capo0": 68, "capo1": 68, "capsiz": [56, 70, 74, 77], "capthick": [56, 77], "cardin": [50, 71], "care": 86, "carlo": [21, 22, 24, 27, 59, 60, 65, 66, 79, 115], "casalicchio": [52, 86, 112, 114], "case": [4, 7, 10, 11, 12, 20, 24, 35, 47, 50, 51, 57, 59, 60, 61, 64, 66, 67, 68, 70, 71, 75, 76, 77, 78, 79, 83, 85, 86, 87, 88, 89, 102, 103, 108, 114, 116, 117], "cat": [48, 116], "catboost": 69, "cate": [12, 15, 39, 68, 84, 116], "cate_obj": 85, "cattaneo": [87, 115], "caus": [48, 58, 77, 82], "causal": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 25, 26, 28, 29, 30, 31, 32, 35, 47, 48, 50, 51, 52, 53, 57, 58, 68, 69, 70, 71, 72, 74, 78, 80, 81, 82, 83, 84, 87, 88, 102, 103, 108, 115], "causal_contrast": [5, 56, 68, 87], "causal_contrast_att": 68, "causal_contrast_c": 68, "causal_contrast_model": [56, 87], "causaldml": 115, "causalweight": 115, "caution": 102, "caveat": [66, 79], "cbind": 50, "cc": 72, "ccp_alpha": [12, 40, 72], "cd": 113, "cd_fast": 71, "cda85647": 52, "cdf": 85, "cdid": [50, 71], "cdot": [21, 22, 23, 27, 36, 50, 61, 63, 67, 71, 74, 76, 77, 79, 85, 87, 89, 90, 95, 96, 97, 98, 102, 103, 104], "cdot1": 67, "cell": 70, "center": 64, "central": [88, 116], "certain": [66, 87], "cexcol": 50, "cexrow": 50, "cf_d": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 21, 22, 43, 56, 67, 68, 78, 79, 103, 104, 105, 108, 109, 110, 111, 117], "cf_y": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 21, 22, 43, 56, 67, 68, 78, 79, 103, 104, 105, 108, 109, 110, 111, 117], "chad": 79, "chain": 66, "chainedassignmenterror": 66, "challeng": [50, 71, 103, 105], "chang": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 37, 38, 41, 42, 51, 53, 62, 66, 73, 78, 79, 80, 87, 89, 95, 102, 103, 104, 105, 106, 107, 108, 109, 113, 115, 116], "channel": 117, "chapter": [33, 34, 52, 86, 103, 111], "charact": [51, 52, 86, 116], "characterist": [78, 117], "chart": 70, "check": [37, 38, 41, 42, 48, 51, 58, 69, 70, 72, 73, 81, 82, 112, 113, 116], "check_data": 116, "check_scor": 116, "checkmat": 116, "chernozhukov": [19, 20, 26, 28, 30, 48, 50, 51, 58, 64, 69, 70, 71, 72, 73, 78, 82, 88, 89, 95, 102, 103, 105, 111, 112, 115, 116], "chetverikov": [19, 20, 30, 50, 64, 71, 82, 102, 112, 115], "chiang": [29, 50, 71, 115], "chieh": 115, "choic": [4, 6, 7, 8, 9, 10, 11, 12, 13, 16, 17, 18, 51, 64, 72, 75, 85, 86, 103, 105, 108, 111, 116], "choos": [47, 51, 57, 58, 64, 69, 72, 73, 81, 88, 89, 92, 93, 95, 99, 100, 102, 114, 117], "chosen": [4, 22, 27, 69, 86, 87], "chou": 74, "chr": 51, "christian": [64, 115], "christoph": 115, "chunk": 86, "ci": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 35, 56, 59, 60, 61, 62, 63, 65, 66, 67, 68, 70, 72, 73, 76, 77, 78, 79, 85, 87, 103, 108, 116, 117], "ci_at": 56, "ci_cvar": [61, 73], "ci_cvar_0": 61, "ci_cvar_1": 61, "ci_joint": 56, "ci_joint_cvar": 61, "ci_joint_lqt": 76, "ci_joint_qt": 76, "ci_length": 62, "ci_low": 56, "ci_lpq_0": 76, "ci_lpq_1": 76, "ci_lqt": [73, 76], "ci_pointwis": 56, "ci_pq_0": [73, 76], "ci_pq_1": [73, 76], "ci_qt": [73, 76], "ci_upp": 56, "cinelli": [79, 103, 105, 115], "circumv": 117, "citat": 116, "claim": 52, "clash": 49, "class": [0, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 33, 34, 35, 37, 38, 39, 40, 41, 42, 51, 52, 53, 54, 56, 62, 67, 68, 70, 72, 73, 78, 80, 81, 83, 85, 86, 88, 89, 102, 112, 114, 116], "class_estim": 77, "class_learn": 73, "class_learner_1": 69, "class_learner_2": 69, "classes_": 70, "classic": [49, 50, 71, 117], "classif": [12, 37, 41, 47, 49, 51, 52, 53, 69, 70, 75, 78, 85, 86, 87, 117], "classifavg": 52, "classifi": [4, 6, 8, 9, 11, 12, 13, 15, 16, 17, 18, 35, 37, 41, 52, 56, 70, 77, 86, 116], "classmethod": [7, 10], "claudia": [115, 116], "claus": 116, "clean": 116, "cleaner": 69, "cleanup": 116, "clear": [50, 71], "clearli": 77, "clever": 69, "clone": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 47, 48, 52, 58, 69, 71, 73, 81, 86, 87, 88, 89, 102, 103, 108, 113, 114], "close": [49, 51, 72, 79, 103, 105], "cluster": [4, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 29, 115, 116], "cluster_col": [7, 50, 71], "cluster_var": [7, 29], "cluster_var_i": [7, 50, 71], "cluster_var_j": [7, 50, 71], "cmap": 71, "cmd": 116, "co": [31, 63], "codaci": 116, "code": [4, 12, 15, 31, 47, 49, 50, 51, 52, 53, 57, 64, 72, 82, 85, 86, 87, 88, 89, 102, 113, 114, 116, 117], "codecov": 116, "coef": [6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 48, 49, 50, 51, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 85, 86, 87, 88, 89, 102, 114, 117], "coef_": 79, "coef_df": 50, "coef_valu": 70, "coeffici": [21, 22, 24, 38, 39, 42, 51, 53, 65, 66, 69, 72, 74, 75, 77, 79, 80, 85, 102, 103, 108, 117], "coefs_t": 75, "coefs_w": 75, "coffici": [103, 108], "cofid": 39, "coincid": [63, 68, 73], "col": [48, 50, 66, 72], "collect": [52, 53, 62, 71, 80], "colnam": [50, 69], "color": [51, 53, 56, 58, 59, 60, 61, 63, 70, 71, 72, 73, 74, 76, 77, 79], "color_palett": [56, 58, 71, 72, 73], "colorbar": 71, "colorblind": 56, "colorramppalett": 50, "colorscal": [59, 60], "colour": [48, 50], "column": [7, 10, 54, 56, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 77, 78, 79, 80, 83, 85, 86, 87, 88, 114, 116, 117], "column_stack": [56, 63, 65, 66, 77, 78, 79, 87], "colv": 50, "com": [31, 49, 51, 52, 64, 74, 79, 86, 113], "comb": 64, "combin": [49, 50, 52, 56, 62, 68, 69, 70, 71, 79, 86, 88, 103, 108, 116], "combind": 73, "combined_loss": 64, "come": [81, 86, 89, 103, 105, 112, 117], "command": [113, 116], "comment": 83, "common": [69, 78, 79, 85, 87, 115], "companion": 115, "compar": [48, 50, 58, 59, 60, 61, 63, 65, 66, 68, 71, 74, 76, 77, 79, 82, 86, 87, 103, 105], "comparevers": 51, "comparison": [56, 69, 74], "compat": [47, 49, 57, 116], "complement": 79, "complet": [70, 82, 103, 108, 113], "complex": [12, 49, 70], "compli": [77, 87], "complianc": [76, 77, 87, 89, 96], "complic": [52, 117], "complier": [51, 72, 73, 76, 77, 85, 87], "compon": [41, 42, 49, 51, 64, 69, 70, 72, 75, 85, 86, 88, 89, 90, 92, 93, 94, 95, 96, 99, 100, 116], "compont": 49, "composit": 115, "compris": 102, "comput": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 39, 43, 48, 49, 51, 52, 58, 72, 73, 78, 79, 88, 89, 103, 104, 105, 106, 107, 108, 109, 112, 115, 116, 117], "computation": [103, 105], "concat": [70, 71, 72, 75, 102], "concaten": [63, 72, 102], "concentr": 102, "concern": 79, "conclud": [77, 79, 117], "cond": 87, "conda": [71, 115, 116], "condit": [4, 6, 12, 15, 21, 22, 24, 33, 34, 48, 50, 51, 53, 56, 58, 62, 63, 67, 68, 71, 72, 75, 77, 79, 80, 82, 84, 87, 102, 103, 104, 109, 111, 114, 115, 116, 117], "conduct": [85, 87, 117], "conf": [49, 76], "confer": 115, "confid": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 39, 49, 50, 51, 53, 56, 59, 60, 61, 62, 65, 66, 68, 71, 73, 76, 77, 78, 80, 84, 85, 88, 89, 103, 108, 114, 115, 117], "confidenceband": 61, "confidenti": 79, "config": 74, "configur": [52, 70], "confint": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 39, 51, 53, 56, 59, 60, 61, 62, 63, 65, 66, 68, 69, 73, 75, 76, 77, 78, 80, 85, 88, 102, 112, 114, 117], "conflict": 113, "confound": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 21, 22, 24, 43, 47, 51, 57, 67, 72, 76, 78, 79, 83, 87, 102, 103, 105, 108, 110, 111, 114, 115, 116, 117], "congress": 115, "connect": [51, 72, 73], "consequ": [21, 22, 50, 67, 71, 78, 85, 87, 103, 104, 105, 109, 111], "conserv": [78, 79, 103, 111], "consid": [6, 11, 12, 13, 16, 36, 48, 50, 51, 53, 58, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 85, 86, 87, 88, 89, 102, 103, 105, 112, 117], "consider": [79, 87], "consist": [14, 15, 38, 42, 51, 62, 70, 72, 73, 74, 79, 82, 83, 87, 114, 116], "consol": [48, 116], "constant": [26, 38, 42, 64, 75, 85, 87, 102], "constrained_layout": 58, "construct": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 39, 52, 59, 60, 61, 63, 68, 73, 78, 81, 85, 89, 91, 98, 102, 116, 117], "construct_framework": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18], "construct_iv": 71, "constructiv": 50, "constructor": 52, "consum": [50, 71], "cont": 27, "cont_d": 56, "contain": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 27, 36, 37, 38, 41, 42, 48, 50, 51, 56, 58, 59, 60, 65, 66, 69, 71, 72, 82, 85, 86, 102, 103, 105, 108, 116], "context": [79, 87, 117], "contin": [27, 70], "continu": [27, 47, 52, 56, 57, 64, 74, 77, 87, 103, 111, 116, 117], "contour": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 64, 67, 78, 79, 103, 108], "contour_plot": 79, "contours_z": [59, 60], "contrast": [5, 61, 62, 68, 87], "contribut": [113, 116], "contributor": 116, "control": [28, 36, 49, 64, 68, 73, 75, 77, 79, 117], "convent": [35, 51, 72, 73, 77, 87], "converg": [48, 58, 69, 71, 82], "convergencewarn": 71, "convers": 71, "convert": [61, 71, 76], "convex": 74, "cooper": 116, "coor": [52, 86, 112, 114], "coordin": 79, "copi": [66, 70, 72, 75, 79], "cor": [103, 111], "core": [54, 56, 61, 62, 67, 71, 72, 73, 76, 78, 80, 83, 86, 114, 116], "cores_us": [61, 73, 76], "correct": [67, 68, 79, 85, 102, 116], "correctli": [37, 41, 62, 74, 78, 103, 111], "correl": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 50, 53, 64, 71, 78, 80, 87, 103, 105, 111], "correpond": 87, "correspond": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 27, 33, 34, 48, 50, 51, 52, 56, 58, 59, 60, 62, 63, 64, 68, 69, 71, 72, 73, 75, 76, 78, 79, 82, 85, 86, 87, 88, 102, 103, 105, 108, 109, 111, 116, 117], "cosh": 31, "coul": 50, "could": [47, 52, 57, 59, 60, 70, 79, 116, 117], "counfound": [21, 22, 76, 78, 85, 103, 111], "count": [56, 72, 73], "countour": [103, 108], "coupl": [51, 72, 73], "cournapeau": [112, 114], "cours": [51, 69, 72, 79, 102, 117], "cov": [18, 21, 36, 77], "cov_nam": [77, 87], "cov_typ": [4, 12, 15, 39, 116], "covari": [7, 8, 9, 10, 12, 14, 15, 21, 22, 24, 25, 26, 28, 29, 30, 31, 32, 35, 36, 39, 40, 47, 48, 49, 50, 51, 52, 53, 54, 56, 58, 59, 60, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 78, 79, 80, 82, 83, 85, 86, 87, 89, 92, 93, 102, 103, 105, 114, 115, 116], "cover": [49, 64, 78], "coverag": [69, 77, 85, 116], "cp": [51, 52, 86], "cpu": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18], "cpu_count": [61, 73, 76], "cran": [52, 115, 116], "creat": [24, 47, 50, 52, 56, 57, 58, 59, 60, 61, 65, 66, 71, 73, 75, 76, 79, 86, 103, 105, 108, 111, 113, 116], "create_synthetic_group_data": 75, "critic": [79, 117], "cross": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 23, 47, 48, 49, 51, 52, 53, 58, 69, 70, 72, 73, 77, 79, 82, 84, 86, 92, 93, 98, 102, 106, 108, 116, 117], "cross_sectional_data": [9, 23, 62, 87], "crossfit": [69, 87], "crosstab": 74, "crucial": [64, 87, 117], "csail": [112, 114], "csv": 64, "cumul": 87, "current": [40, 49, 66, 68, 89, 103, 111, 112, 113, 117], "custom": [48, 49, 58, 79, 86], "custom_measur": 49, "cut": 75, "cutoff": [35, 36, 77, 87], "cv": [52, 72, 86, 88], "cv_glmnet": [50, 51, 52, 53, 86, 87, 102, 114], "cvar": [6, 17, 84, 91, 116], "cvar_0": 61, "cvar_1": 61, "d": [4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 35, 36, 39, 40, 47, 48, 49, 50, 51, 52, 53, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 117], "d0": [61, 76, 102], "d0_true": 76, "d0cdb0ea4795": 52, "d1": [61, 74, 76, 102], "d10": 102, "d1_true": 76, "d2": [74, 102], "d21ee5775b5f": 52, "d3": 102, "d4": 102, "d5": 102, "d5a0c70f1d98": 52, "d6": 102, "d7": 102, "d8": 102, "d9": 102, "d_": [27, 29, 50, 56, 63, 71, 87, 102], "d_0": 87, "d_1": [74, 102], "d_2": 74, "d_col": [7, 10, 47, 48, 50, 51, 52, 53, 57, 59, 60, 65, 66, 68, 71, 72, 73, 75, 77, 78, 81, 82, 83, 86, 87, 88, 89, 114, 116, 117], "d_i": [24, 25, 26, 28, 30, 31, 32, 48, 53, 56, 58, 61, 62, 74, 76, 77, 80, 82, 87], "d_j": [56, 87, 102], "d_k": [87, 102], "d_l": 87, "d_w": 75, "da1440": 74, "dag": [53, 79, 80, 117], "dark": [48, 58], "darkblu": 50, "darkr": 50, "dash": [53, 56], "dat": 83, "data": [0, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 36, 37, 39, 41, 42, 49, 63, 64, 69, 74, 81, 84, 85, 86, 88, 102, 107, 108, 115, 116], "data_apo": 56, "data_cvar": 73, "data_dict": [35, 59, 60, 65, 66, 67, 77, 87], "data_dml": 78, "data_dml_bas": [51, 59, 60, 65, 66, 72, 73, 75], "data_dml_base_iv": [51, 72, 73], "data_dml_flex": [51, 72], "data_dml_flex_iv": 51, "data_dml_iv_flex": 72, "data_dml_new": 75, "data_fram": 117, "data_lqt": 73, "data_pq": 73, "data_qt": 73, "data_transf": [50, 71, 72], "datafram": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 23, 25, 26, 28, 29, 30, 31, 32, 35, 39, 40, 50, 53, 54, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 84, 85, 86, 87, 89, 102, 103, 105, 108, 114, 117], "dataset": [0, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 35, 48, 49, 53, 56, 58, 59, 60, 62, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 102, 103, 108, 114, 117], "datatyp": 116, "date": 86, "db": [51, 72, 73, 78, 117], "dbl": [49, 50, 51, 52, 83, 102, 114, 117], "dc13a11076b3": 52, "ddc9": 52, "de": [47, 57, 115], "deal": [47, 57], "debias": [19, 20, 29, 30, 50, 64, 71, 84, 86, 88, 112, 115, 116], "debt": [51, 72, 73], "decai": [53, 80], "decid": [51, 72], "decis": [12, 47, 51, 57, 72, 73, 85, 87, 115, 117], "decision_effect": 47, "decision_impact": [47, 57], "decisiontreeclassifi": [12, 40, 72], "decisiontreeregressor": 72, "declar": 117, "decreas": 77, "deep": [37, 38, 41, 42, 70], "deeper": 12, "def": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 58, 61, 69, 70, 71, 74, 75, 76, 79, 86, 89], "default": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 27, 29, 30, 31, 35, 36, 37, 38, 39, 40, 41, 42, 49, 50, 53, 62, 65, 66, 69, 71, 75, 77, 78, 79, 80, 81, 85, 86, 87, 88, 102, 103, 104, 108, 114, 117], "default_convert": 71, "defier": [77, 87], "defin": [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 36, 38, 42, 48, 51, 52, 56, 58, 59, 60, 61, 62, 64, 65, 66, 68, 69, 70, 72, 73, 75, 76, 77, 78, 79, 85, 86, 87, 89, 92, 93, 103, 105, 108, 111], "definit": [31, 65, 66, 68, 103, 104, 109], "defint": 103, "degre": [36, 51, 59, 60, 68, 71, 72, 77, 85, 103, 105], "dekel": 115, "delete_origin": 52, "deliber": 74, "delta": [28, 49, 62, 79, 87], "delta_bench": 79, "delta_i": 49, "delta_j": 28, "delta_theta": [43, 56, 67, 78, 79, 103, 105], "delta_v": 79, "demand": [50, 71, 103, 105], "demir": [19, 20, 30, 50, 64, 71, 82, 88, 112, 115], "demo": 79, "demonstr": [48, 49, 50, 58, 71, 77, 79, 83, 87, 102, 112, 114], "deni": 115, "denomin": [103, 104, 105, 109], "denot": [14, 50, 51, 53, 62, 63, 71, 72, 77, 79, 80, 85, 87, 89, 103, 105, 108, 109, 111], "dens_net_tfa": 51, "densiti": [13, 16, 17, 48, 53, 56, 58], "dep": 54, "dep1": [52, 54, 83, 114], "dep2": [52, 54, 83, 114], "depend": [4, 6, 12, 13, 17, 24, 52, 59, 60, 62, 65, 66, 67, 69, 70, 75, 77, 81, 85, 86, 87, 89, 96, 101, 103, 104, 105, 111, 114, 115], "deprec": [81, 88], "depreci": 116, "depth": [12, 40, 51, 52, 75, 81, 85, 86, 87, 88, 89, 102, 114, 117], "deriv": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 34, 87, 102], "describ": [49, 50, 71, 72, 73, 79, 86, 88, 113, 116], "descript": [51, 54, 78, 86, 88, 103, 105], "deserv": 87, "design": [35, 36, 56, 70, 84, 115, 116], "design_info": [59, 60], "design_matrix": [59, 60, 85], "desir": [22, 52, 75, 87, 113], "detail": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 33, 34, 48, 51, 52, 56, 58, 62, 63, 64, 70, 73, 77, 78, 79, 82, 83, 85, 86, 89, 91, 95, 96, 97, 98, 101, 102, 103, 105, 111, 112, 113, 114, 116, 117], "determin": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 38, 42, 51, 61, 72, 73, 76, 77, 78, 87, 102, 103, 111], "determinist": [75, 77, 85, 87], "deutsch": 112, "dev": [113, 116], "develop": [49, 50, 52, 71, 79, 87, 116], "deviat": [69, 87, 103, 111], "dezeur": 115, "df": [7, 10, 47, 48, 50, 53, 56, 57, 59, 60, 61, 63, 66, 68, 71, 74, 76, 77, 78, 79, 80, 82, 85, 87], "df_agg": 64, "df_apo": 56, "df_apo_ci": 56, "df_apos_ci": 56, "df_ate": 56, "df_bench": 79, "df_binari": 79, "df_bonu": [52, 83, 114], "df_capo0": 68, "df_capo1": 68, "df_cate": [59, 60, 68], "df_causal_contrast_c": 68, "df_ci": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 39], "df_coef": 69, "df_cvar": 73, "df_fuzzi": 77, "df_lqte": 73, "df_ml_g0": 69, "df_ml_g1": 69, "df_ml_m": 69, "df_pa": [62, 80], "df_plot": 50, "df_pq": 73, "df_qte": 73, "df_result": 64, "df_sharp": 77, "df_sort": 56, "df_summari": 72, "df_wide": 71, "dfg": 112, "dgp": [23, 50, 53, 61, 63, 64, 71, 74, 75, 76, 79, 80], "dgp1": 23, "dgp2": 23, "dgp3": 23, "dgp4": 23, "dgp5": 23, "dgp6": 23, "dgp_dict": 79, "dgp_tpye": 62, "dgp_type": [23, 62], "diagon": 79, "diagram": [47, 57, 87], "dichotom": [47, 57], "dict": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 37, 38, 39, 40, 41, 42, 43, 59, 60, 64, 70, 79, 86], "dict_kei": [103, 108], "dictionari": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 21, 22, 24, 27, 36, 43, 59, 60, 65, 66, 78, 85, 86, 103, 108], "dictonari": [51, 72], "did": [7, 10, 48, 62, 63, 71, 84, 116, 117], "diff": 72, "differ": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 21, 22, 23, 24, 47, 48, 50, 51, 52, 53, 56, 57, 58, 61, 65, 66, 67, 68, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 84, 85, 86, 88, 92, 93, 113, 114, 115, 116, 117], "differenti": 87, "difficult": 79, "dillon": 115, "dim": [36, 51], "dim_x": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 25, 26, 28, 29, 30, 31, 32, 36, 48, 50, 52, 58, 68, 69, 70, 71, 82, 85, 86, 87, 103, 108], "dim_z": [14, 28, 87], "dimens": [24, 29, 50, 71, 75, 88], "dimension": [14, 15, 24, 26, 64, 85, 87, 88, 102, 103, 108, 114, 115], "direct": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 48, 53, 58, 63, 68, 80, 82, 87, 117], "directli": [35, 48, 49, 51, 56, 58, 69, 78, 82, 103, 108, 114, 117], "discontinu": [35, 36, 84, 115, 116], "discret": [5, 27, 56, 71, 87, 116], "discretis": 73, "discuss": [25, 50, 51, 71, 72, 115, 116, 117], "disjoint": [50, 65, 66, 71], "displai": [50, 56, 71, 79, 85, 86, 103, 108], "displot": 72, "disproportion": [51, 72], "disregard": [38, 42], "dist": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18], "distr": 86, "distribut": [36, 48, 56, 58, 62, 69, 79, 82, 87, 103, 109, 113, 115, 116], "diverg": [35, 48, 58, 82], "divid": 87, "dmatrix": [59, 60, 85], "dml": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 33, 34, 35, 47, 48, 52, 53, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 89, 102, 103, 108, 113], "dml1": [84, 114, 116, 117], "dml2": [47, 50, 52, 53, 54, 62, 71, 73, 84, 87, 89, 102, 114, 116, 117], "dml_apo": 68, "dml_apo_obj": 87, "dml_apos_att": 68, "dml_apos_obj": 87, "dml_base": 71, "dml_combin": 102, "dml_cv_predict": 116, "dml_cvar": [61, 73], "dml_cvar_0": 61, "dml_cvar_1": 61, "dml_cvar_obj": [6, 85], "dml_data": [49, 50, 53, 54, 56, 62, 63, 67, 68, 69, 71, 74, 78, 79, 80, 85, 86, 87, 102, 117], "dml_data_bench": 79, "dml_data_bonu": [52, 114], "dml_data_df": 117, "dml_data_fuzzi": 77, "dml_data_lasso": 54, "dml_data_sharp": 77, "dml_data_sim": [52, 114], "dml_df": [50, 71], "dml_did": [62, 63], "dml_did_obj": [8, 9, 87], "dml_iivm_boost": [51, 72], "dml_iivm_forest": [51, 72], "dml_iivm_lasso": [51, 72], "dml_iivm_obj": [11, 57, 87], "dml_iivm_tre": [51, 72], "dml_irm": [59, 65, 68, 69, 75], "dml_irm_at": 67, "dml_irm_att": 68, "dml_irm_boost": [51, 72], "dml_irm_forest": [51, 72], "dml_irm_gat": 67, "dml_irm_gatet": 67, "dml_irm_lasso": [51, 54, 72], "dml_irm_new": 75, "dml_irm_obj": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 78, 85, 86, 87], "dml_irm_obj_ext": 86, "dml_irm_rf": 54, "dml_irm_tre": [51, 72], "dml_irm_weighted_att": 68, "dml_kwarg": 68, "dml_long": 43, "dml_lpq_0": 76, "dml_lpq_1": 76, "dml_lpq_obj": [13, 85], "dml_lqte": [73, 76], "dml_obj": [49, 56, 78, 79], "dml_obj_bench": 79, "dml_pliv": [50, 71], "dml_pliv_obj": [14, 50, 71, 87], "dml_plr": [60, 66, 102], "dml_plr_1": 102, "dml_plr_2": 102, "dml_plr_boost": [51, 72], "dml_plr_forest": [51, 72, 117], "dml_plr_lasso": [51, 54, 72], "dml_plr_no_split": 88, "dml_plr_obj": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 78, 81, 85, 86, 87, 88, 89, 102, 103, 105, 108], "dml_plr_obj_extern": 88, "dml_plr_obj_intern": 88, "dml_plr_obj_onfold": 70, "dml_plr_obj_untun": 70, "dml_plr_rf": 54, "dml_plr_tree": [51, 72, 117], "dml_pq_0": [73, 76], "dml_pq_1": [73, 76], "dml_pq_obj": [16, 85], "dml_procedur": [54, 81, 114, 116, 117], "dml_qte": [73, 76], "dml_qte_obj": [17, 85], "dml_short": 43, "dml_ssm": [53, 80, 87], "dml_tune": 116, "dmldummyclassifi": 86, "dmldummyregressor": 86, "dmlmt": 115, "dnorm": 48, "do": [49, 50, 51, 52, 68, 69, 71, 72, 73, 74, 79, 85, 86, 103, 111, 114, 117], "doabl": 89, "doc": [47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 112, 116], "doccument": 116, "docu": 116, "document": [55, 59, 60, 63, 65, 66, 68, 70, 79, 112, 116], "doe": [5, 17, 49, 50, 51, 56, 68, 71, 72, 74, 78, 79, 103, 111, 117], "doesn": [47, 57], "doi": [19, 20, 21, 22, 23, 25, 29, 30, 32, 49, 50, 52, 64, 71, 79, 82, 86, 88, 102, 112, 114, 116], "domain": 75, "don": [49, 70], "done": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 70, 73, 86, 88, 103, 105], "dosag": 56, "dot": [18, 63, 75, 83, 85, 86, 87, 102, 114], "doubl": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 29, 30, 31, 32, 35, 51, 64, 69, 70, 72, 74, 84, 86, 88, 89, 102, 103, 105, 116], "double_ml_bonus_data": 54, "double_ml_data_from_data_fram": [48, 82, 83, 117], "double_ml_data_from_matrix": [49, 52, 83, 86, 102, 114], "double_ml_irm": [54, 75], "double_ml_score_mixin": 0, "doubleiivm": 112, "doubleml": [0, 48, 50, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 73, 74, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 103, 108, 114, 115, 116], "doubleml2022python": 112, "doubleml2024r": 112, "doubleml_did_eval_linear": 49, "doubleml_did_eval_rf": 49, "doubleml_did_linear": 49, "doubleml_did_rf": 49, "doubleml_framework": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18], "doublemlapo": [0, 56, 68, 87, 89, 90, 116], "doublemlblp": [4, 12, 15, 59, 60, 68, 85, 116], "doublemlclusterdata": [0, 29], "doublemlcvar": [0, 61, 85, 89, 91, 116], "doublemldata": [0, 4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 23, 25, 26, 28, 30, 31, 32, 35, 47, 50, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 102, 103, 108, 116, 117], "doublemldid": [0, 62, 63, 87, 89, 92, 116], "doublemldidc": [0, 62, 87, 89, 93, 116], "doublemlframework": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 88, 102, 116], "doublemlframwork": 5, "doublemlidid": 87, "doublemlididc": 87, "doublemliivm": [0, 47, 51, 57, 72, 86, 87, 88, 89, 94, 116], "doublemlirm": [0, 4, 6, 8, 9, 11, 13, 14, 15, 16, 18, 49, 51, 54, 56, 59, 65, 67, 68, 69, 72, 74, 75, 78, 79, 85, 86, 87, 88, 89, 95, 112, 116], "doublemllpq": [0, 76, 85, 89, 96, 116], "doublemlpliv": [0, 86, 87, 88, 89, 99, 112, 116], "doublemlplr": [0, 4, 5, 6, 8, 9, 11, 12, 13, 14, 16, 17, 18, 48, 51, 52, 54, 58, 60, 66, 70, 72, 74, 78, 81, 82, 85, 86, 87, 88, 89, 100, 102, 103, 108, 112, 114, 116, 117], "doublemlpolicytre": [12, 85], "doublemlpq": [0, 73, 76, 85, 89, 101, 116], "doublemlqt": [0, 61, 73, 76, 85, 102, 116], "doublemlresampl": [68, 69], "doublemlsmm": 116, "doublemlssm": [0, 53, 80, 87, 89, 97, 98], "doubli": [21, 22, 23, 49, 115], "down": 79, "download": [47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 83, 113, 114], "downward": 79, "dpg_dict": 78, "dpi": [48, 58, 74], "dramat": 49, "draw": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 79, 88, 116], "draw_sample_split": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 68, 69, 88], "drawn": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 24, 36, 51, 72, 73, 75, 88], "drive": [48, 58, 82], "driven": [79, 117], "drop": [49, 70, 71, 74, 83, 86, 89, 92, 93, 102], "dt": [89, 93, 103, 106], "dt_bonu": 83, "dta": 49, "dtrain": 72, "dtype": [54, 56, 62, 65, 66, 67, 69, 71, 72, 73, 78, 80, 83, 85, 114], "dualiti": 71, "dubourg": [112, 114], "duchesnai": [112, 114], "due": [48, 49, 58, 59, 60, 67, 78, 79, 82, 87, 103, 105, 116, 117], "duflo": [19, 20, 30, 50, 64, 71, 82, 88, 112, 115], "dummi": [4, 12, 15, 37, 38, 39, 70, 79, 85, 86, 87, 116], "dummyclassifi": 37, "dummyregressor": 38, "duplic": 116, "durabl": [52, 54, 83, 114], "durat": 20, "dure": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 49, 50, 51, 52, 53, 70, 71, 72, 86, 88, 114, 116, 117], "dx": 25, "dynam": [49, 115], "e": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 27, 30, 32, 33, 34, 35, 37, 38, 41, 42, 48, 49, 50, 51, 53, 56, 58, 59, 60, 62, 64, 67, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 82, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 117], "e20ea26": 52, "e401": [51, 72, 73, 78, 117], "e4016553": 117, "e45228": 74, "e57c": 52, "each": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 27, 37, 38, 41, 42, 50, 52, 56, 63, 65, 66, 69, 70, 71, 73, 74, 75, 78, 79, 81, 83, 86, 87, 88, 102, 103, 108, 117], "earlier": 117, "earn": [51, 72, 73], "earner": [51, 72, 78], "easi": [52, 89], "easier": 70, "easili": [52, 69, 70, 73, 116], "ec973f": 74, "ecolor": [56, 63, 72, 74], "econ": 115, "econml": 115, "econom": [28, 29, 31, 32, 50, 64, 71, 74, 79, 88, 115], "econometr": [19, 20, 21, 22, 23, 30, 31, 49, 50, 64, 71, 82, 112, 115], "econometrica": [26, 50, 71, 74, 82, 115], "ecosystem": [112, 117], "ectj": [19, 20, 30, 50, 64, 71, 82, 112], "ed": 115, "edge_color": 58, "edgecolor": 58, "edit": [113, 115], "edu": [112, 114], "educ": [51, 72, 73, 78, 117], "ee97bda7": 52, "effect": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 27, 36, 37, 38, 41, 42, 47, 48, 49, 50, 52, 53, 56, 57, 58, 62, 63, 64, 67, 71, 75, 77, 80, 82, 84, 86, 87, 88, 89, 95, 102, 103, 105, 114, 115, 116, 117], "effici": [87, 115], "effort": 89, "eight": [50, 71], "either": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 24, 52, 63, 64, 75, 77, 85, 86, 87, 117], "eleanor": 115, "element": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 50, 53, 59, 60, 61, 62, 69, 71, 73, 76, 78, 80, 89, 90, 92, 93, 103, 108, 110, 111, 116], "element_text": [50, 51], "elementari": 115, "elif": [65, 66, 75], "elig": [73, 78, 117], "eligibl": [51, 72, 78], "ell": [48, 50, 58, 64, 71, 82, 89, 99, 100, 114], "ell_0": [11, 14, 15, 48, 58, 64, 70, 82, 87], "ell_2": 69, "els": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 39, 49, 50, 51, 63, 65, 66, 71, 75, 79], "em": 115, "emphas": [50, 71], "empir": [33, 34, 48, 50, 58, 71, 74, 79, 82, 87, 88, 89, 102], "emploi": [50, 64, 71, 79, 89, 94], "employ": [51, 72, 73], "employe": 117, "empti": 71, "emul": [103, 105], "enabl": [56, 75, 78, 85, 103, 105, 116], "enable_metadata_rout": [37, 38, 41, 42], "encapsul": [37, 38, 41, 42], "encod": 74, "end": [25, 28, 29, 48, 49, 50, 51, 53, 58, 61, 63, 64, 69, 71, 72, 74, 75, 76, 80, 81, 83, 86, 88, 102, 114, 117], "endogen": [51, 72, 73, 117], "enet_coordinate_descent_gram": 71, "engin": [52, 115], "enrol": [51, 72, 73], "ensembl": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 54, 56, 58, 59, 60, 65, 66, 67, 69, 72, 75, 78, 79, 81, 85, 86, 87, 88, 89, 102, 103, 108, 114, 117], "ensemble_learner_pipelin": 86, "ensemble_pipe_classif": 52, "ensemble_pipe_regr": 52, "ensur": [41, 42, 50, 66, 68, 70, 71, 75], "entir": [48, 51, 58, 72, 82, 103, 105], "entri": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 36, 48, 50, 54, 56, 58, 62, 67, 71, 72, 73, 78, 80, 82, 83, 86, 112, 114, 116], "enumer": [56, 61, 63, 65, 66, 69, 71, 72, 73, 76, 81, 86, 88], "env": [71, 113], "environ": 113, "ep": 74, "epanechnikov": 35, "epsilon": [51, 61, 62, 63, 72, 76, 85, 87], "epsilon_": [50, 63, 71], "epsilon_0": 36, "epsilon_1": 36, "epsilon_i": [24, 61, 74, 75, 76], "epsilon_sampl": 75, "epsilon_tru": [61, 76], "eqnarrai": 51, "equal": [4, 12, 50, 53, 71, 74, 80, 85, 86, 87, 103, 109], "equat": [36, 50, 51, 71, 72, 79, 81, 102, 117], "equilibrium": [50, 71], "equival": [64, 68, 88], "err": [6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 54, 56, 57, 59, 60, 61, 62, 65, 66, 67, 68, 70, 71, 72, 73, 74, 76, 78, 79, 80, 85, 86, 87, 88, 89, 102, 114, 117], "error": [4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 37, 38, 41, 42, 47, 48, 49, 51, 52, 53, 58, 63, 64, 65, 66, 69, 70, 72, 77, 79, 82, 86, 87, 88, 89, 102, 103, 108, 114, 116, 117], "errorbar": [56, 63, 65, 66, 70, 72, 74, 77], "erstellt": [50, 51, 52], "esim": 77, "especi": [69, 70], "essenti": 79, "est": 77, "est_method": 49, "esther": [88, 115], "estim": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 33, 34, 35, 37, 38, 39, 40, 41, 42, 48, 49, 50, 52, 56, 58, 59, 60, 61, 63, 65, 66, 68, 69, 71, 75, 77, 81, 82, 84, 85, 86, 87, 91, 92, 93, 96, 98, 101, 103, 105, 108, 112, 115, 116], "estimatior": [7, 10], "estimator_list": 70, "et": [19, 20, 24, 26, 29, 30, 48, 50, 51, 52, 58, 59, 60, 61, 62, 64, 65, 66, 69, 71, 72, 73, 76, 78, 82, 87, 88, 89, 91, 95, 96, 101, 102, 103, 105, 111, 112, 114, 116], "eta": [33, 34, 48, 50, 51, 63, 70, 71, 72, 76, 77, 81, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 111, 114, 117], "eta1": 74, "eta2": 74, "eta_": [102, 103, 111], "eta_0": [35, 81, 87, 89, 102], "eta_d": [77, 87], "eta_i": [24, 63, 75, 76, 77, 87], "eta_sampl": 75, "eta_tru": 76, "etc": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 50, 69, 70, 71, 116], "ev": [48, 58, 82], "eval": [52, 86], "eval_metr": [51, 72, 117], "eval_pr": 49, "eval_predict": 49, "evalu": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 26, 34, 49, 52, 59, 60, 61, 63, 67, 68, 73, 76, 78, 81, 115, 116], "evaluate_learn": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 69, 70, 86, 116], "evalut": 86, "even": [51, 52, 72, 74, 77, 86, 87, 117], "eventu": [50, 71], "everi": [50, 71], "everyth": 112, "evid": [67, 70], "exact": [68, 79], "exactli": [77, 79, 87], "exampl": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 24, 35, 47, 48, 51, 52, 53, 54, 56, 57, 58, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 80, 81, 82, 85, 86, 87, 88, 89, 102, 103, 108, 112, 114, 116, 117], "example_attgt": 49, "example_attgt_dml_eval_linear": 49, "example_attgt_dml_eval_rf": 49, "example_attgt_dml_linear": 49, "example_attgt_dml_rf": 49, "except": [38, 42, 64, 79, 116], "excess": 69, "exclud": 43, "exclus": [4, 12, 15, 65, 66, 85], "execut": [52, 117], "exemplarili": 114, "exemplatori": 75, "exhaust": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 86], "exhibit": [50, 71], "exist": [37, 38, 41, 42, 68, 87, 103, 111], "exogen": [51, 72, 73, 87, 117], "exp": [21, 22, 23, 24, 26, 27, 30, 48, 58, 59, 60, 63, 65, 66, 74, 75, 82], "expect": [21, 22, 38, 42, 49, 53, 56, 62, 67, 69, 70, 77, 79, 80, 85, 87, 88, 102, 103, 104, 114], "experi": [20, 25, 26, 48, 51, 58, 72, 79, 82, 83, 88, 114, 115], "experiment": [8, 9, 23, 89, 92, 93, 103, 106, 107], "expertis": 79, "explain": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 22, 78, 103, 105, 110, 111], "explan": [50, 62, 71, 78, 103, 110, 112, 117], "explanatori": [79, 102], "explicit": 79, "explicitli": [67, 117], "exploit": [48, 58, 82, 87, 117], "explor": 70, "exponenti": 102, "export": [70, 116], "exposur": 63, "express": [50, 64, 77, 103, 111], "extend": [79, 86, 112, 116], "extendend": [103, 111], "extens": [86, 89, 112, 115, 116], "extent": 64, "extern": [48, 58, 68, 70, 84, 103, 105, 116], "external_predict": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 58, 86], "externalptr": 51, "extra": 52, "extract": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 70], "extralearn": 52, "extrem": [51, 72], "ey": 64, "f": [51, 52, 56, 58, 61, 62, 63, 64, 69, 71, 72, 73, 75, 76, 78, 79, 80, 86, 103, 111, 112, 114], "f00584a57972": 52, "f1718fdeb9b0": 52, "f2e7": 52, "f3d24993": 52, "f6ebc": 74, "f_": [21, 23, 63, 85], "f_loc": [61, 76], "f_p": 63, "f_scale": [61, 76], "f_x": 87, "face_color": 58, "facet_wrap": 51, "facilit": 70, "fact": [51, 72, 73], "factor": [36, 48, 49, 50, 51, 52, 58, 69, 82, 86, 117], "faculti": 115, "fail": 116, "fair": 69, "fake": [47, 57], "fals": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 27, 35, 36, 37, 38, 39, 41, 42, 48, 51, 52, 53, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 79, 80, 83, 86, 87, 88, 89, 92, 93, 102, 103, 106, 107, 117], "famili": [51, 72, 86], "fanci": 49, "far": [51, 72], "farbmach": 25, "fast": [69, 75, 86], "faster": 64, "fb5c25fa": 52, "fc9e": 52, "fd8a": 52, "featur": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 19, 20, 38, 40, 42, 49, 54, 67, 68, 69, 72, 75, 85, 86, 87], "featureless": [52, 86], "features_bas": [51, 72, 73, 78], "features_flex": 51, "featureunion": 52, "februari": 79, "femal": [52, 54, 83, 114], "fern\u00e1ndez": [26, 88, 115], "fetch": [51, 71, 72, 73, 83], "fetch_401k": [51, 72, 73, 78, 117], "fetch_bonu": [52, 54, 83, 114], "few": [51, 72, 73], "ff7f0e": 63, "field": [50, 71, 86, 117], "fifteenth": 115, "fifth": 50, "fig": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 56, 59, 60, 61, 63, 64, 68, 69, 70, 73, 74, 76, 77, 79], "fig_al": 58, "fig_dml": 58, "fig_non_orth": 58, "fig_orth_nosplit": 58, "fig_po_al": 58, "fig_po_dml": 58, "fig_po_nosplit": 58, "figsiz": [54, 56, 59, 60, 61, 63, 65, 66, 68, 69, 70, 71, 72, 73, 74, 76, 77], "figur": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 30, 48, 50, 54, 56, 58, 59, 60, 61, 63, 64, 65, 66, 68, 70, 71, 72, 73, 76, 79, 82], "figure_format": 74, "file": [19, 20, 64, 74, 115, 116], "filenam": 48, "fill": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 48, 50, 51, 53, 62, 69, 72, 80], "fill_between": [59, 60, 61, 68, 73, 76], "fill_valu": 69, "filter": 52, "filterwarn": 58, "final": [48, 52, 53, 56, 58, 59, 60, 61, 63, 65, 66, 67, 73, 76, 77, 80, 82, 87, 117], "final_estim": 77, "financi": [19, 78, 117], "find": [51, 63, 72, 79, 85, 86, 117], "finish": 52, "finit": [48, 51], "firm": [50, 71, 78], "firmid": 71, "first": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 29, 35, 47, 48, 49, 50, 51, 52, 53, 56, 57, 58, 59, 60, 62, 63, 65, 66, 67, 68, 69, 71, 72, 73, 75, 76, 77, 79, 80, 82, 85, 87, 88, 102, 103, 108, 113, 114, 116, 117], "fit": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 37, 38, 39, 40, 41, 42, 47, 48, 49, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 89, 92, 93, 98, 102, 103, 105, 108, 112, 116, 117], "fit_arg": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18], "fit_transform": [68, 71, 72], "five": 71, "fix": [63, 69, 116], "flag": [23, 88, 113], "flake8": 116, "flamlclassifierdoubleml": 70, "flamlregressordoubleml": 70, "flatten": [70, 74], "flexibl": [35, 47, 49, 51, 52, 57, 62, 72, 87, 112, 115, 116, 117], "flexibli": [51, 72, 78], "float": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 35, 36, 37, 38, 39, 41, 42], "float32": [72, 73, 78], "float64": [54, 56, 62, 66, 67, 71, 72, 78, 80, 83, 86, 114], "floor": 52, "floor_divid": 71, "flt": 52, "flush": 48, "fmt": [56, 63, 65, 66, 70, 72, 74, 77], "fobj": 72, "focu": [50, 51, 68, 71, 72, 73, 79, 85, 87, 117], "focus": [73, 78, 79, 117], "fold": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 47, 50, 51, 52, 53, 62, 69, 71, 72, 73, 78, 80, 81, 84, 86, 87, 89, 92, 93, 102, 114, 117], "follow": [21, 22, 23, 24, 27, 48, 50, 51, 53, 58, 59, 60, 61, 62, 63, 65, 66, 70, 71, 72, 73, 76, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 117], "font_scal": [71, 72, 73], "fontsiz": [61, 73, 76], "force_all_x_finit": [7, 10], "forest": [25, 47, 48, 49, 51, 52, 57, 58, 62, 67, 69, 72, 78, 82, 86, 114, 117], "forest_summari": 72, "forg": [113, 115, 116], "form": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 39, 41, 42, 51, 53, 59, 60, 61, 62, 63, 65, 66, 67, 69, 72, 76, 77, 78, 80, 85, 87, 89, 90, 95, 103, 104, 105, 108, 109, 110, 111, 113, 114], "format": [58, 67, 103, 108], "formula": [50, 51, 71, 72, 77, 79, 116], "formula_flex": 51, "forschungsgemeinschaft": 112, "forthcom": [79, 115], "forum": 116, "forward": [12, 40], "found": [59, 60, 64, 65, 66, 70, 82, 83, 86, 87, 114], "foundat": [112, 115], "four": [51, 69, 72, 116], "fourth": [50, 71], "frac": [11, 21, 22, 23, 25, 26, 28, 30, 31, 32, 34, 38, 42, 48, 50, 52, 58, 63, 64, 67, 71, 74, 77, 81, 82, 85, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111], "fraction": 52, "frame": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 39, 47, 48, 50, 51, 53, 54, 56, 59, 60, 62, 65, 66, 67, 71, 72, 73, 74, 75, 78, 80, 82, 83, 114, 117], "framework": [34, 48, 50, 52, 58, 69, 70, 71, 74, 79, 82, 86, 102, 112, 114, 116, 117], "freez": 113, "fribourg": 115, "friendli": 56, "from": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 36, 38, 39, 42, 47, 48, 50, 51, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 102, 103, 108, 114, 116, 117], "from_arrai": [7, 10, 18, 35, 58, 61, 62, 63, 76, 82, 83, 86, 102, 114], "from_product": 71, "front": 56, "fr\u00e9chet": [103, 111], "fs_kernel": [35, 87], "fs_specif": [35, 87], "fsize": [51, 72, 73, 78, 117], "full": [56, 58, 61, 62, 63, 65, 66, 69, 72, 73, 76, 77, 80, 82, 87], "fulli": [12, 51, 55, 70, 72, 87], "fun": 48, "func": 49, "function": [0, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 30, 31, 33, 34, 35, 47, 48, 51, 52, 53, 57, 58, 59, 60, 61, 62, 68, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 106, 107, 111, 112, 115, 116, 117], "fund": [51, 72, 73, 112], "further": [21, 22, 23, 24, 27, 29, 50, 52, 53, 56, 59, 60, 61, 62, 63, 67, 68, 69, 71, 73, 75, 76, 77, 78, 79, 80, 86, 87, 89, 91, 96, 97, 98, 101, 102, 103, 105, 108, 110, 111, 112, 114, 116, 117], "furthermor": [58, 89, 90, 95], "futurewarn": 66, "fuzzi": [35, 36], "g": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 35, 37, 38, 41, 42, 48, 49, 52, 54, 58, 59, 60, 62, 63, 64, 67, 69, 73, 74, 75, 78, 80, 82, 85, 86, 87, 89, 90, 92, 93, 94, 95, 97, 98, 99, 100, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 117], "g_": [36, 56, 89, 91, 92, 93, 96, 101, 102], "g_0": [4, 8, 9, 11, 12, 14, 15, 16, 30, 31, 35, 36, 48, 50, 51, 58, 69, 71, 72, 82, 85, 86, 87, 89, 90, 97, 98, 103, 104, 109, 111, 114, 117], "g_1": [36, 69], "g_all": [48, 51], "g_all_po": 48, "g_ci": 51, "g_d": [89, 91, 101], "g_dml": 48, "g_dml_po": 48, "g_hat": [14, 15, 48, 58, 89], "g_hat0": [11, 12], "g_hat1": [11, 12], "g_k": 85, "g_nonorth": 48, "g_nosplit": 48, "g_nosplit_po": 48, "g_x": 63, "gain": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 22, 43, 69, 103, 105, 109, 116], "gain_statist": 116, "galleri": [82, 85, 86, 87, 112, 116], "gama": 70, "gamma": [28, 31, 32, 50, 71, 74, 75, 77, 79, 87, 89, 91, 96], "gamma_0": [24, 53, 75, 80, 89, 91, 96], "gamma_a": [21, 22, 79], "gamma_bench": 79, "gamma_v": 79, "gap": [71, 79], "gapo": 4, "gate": [4, 12, 15, 39, 74, 75, 84, 116], "gate_obj": 85, "gatet": 85, "gaussian": [13, 16, 17, 48, 58, 82, 85, 86, 102, 115], "ge": [21, 23, 24, 67, 75, 85], "geer": 115, "gelbach": [50, 71], "gener": [0, 4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 36, 38, 42, 47, 49, 50, 51, 52, 53, 54, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 95, 102, 104, 105, 106, 107, 109, 111, 115, 116, 117], "generate_treat": 76, "geom_bar": 51, "geom_dens": [51, 53], "geom_errorbar": 51, "geom_funct": 48, "geom_histogram": 48, "geom_hlin": 51, "geom_point": 51, "geom_til": 50, "geom_vlin": [48, 53], "geq": [77, 87], "german": 112, "get": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 37, 38, 41, 42, 52, 56, 69, 74, 78, 79, 103, 105, 112, 113], "get_dummi": 74, "get_feature_names_out": [68, 71, 72], "get_legend_handles_label": 56, "get_level_valu": 70, "get_logg": [48, 49, 50, 51, 52, 53, 81, 86, 87, 88, 89, 102, 114], "get_metadata_rout": [37, 38, 41, 42], "get_param": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 37, 38, 41, 42, 70, 86], "get_ylim": 68, "ggdid": 49, "ggplot": [48, 50, 51, 53], "ggplot2": [48, 50, 51, 53], "ggsave": 48, "ggtitl": 51, "gh": 116, "git": 113, "github": [49, 51, 64, 70, 74, 112, 115, 116], "githubusercont": 64, "give": [51, 68, 72], "given": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 21, 22, 26, 30, 31, 34, 35, 36, 37, 38, 41, 42, 48, 50, 53, 56, 58, 63, 65, 66, 71, 73, 74, 77, 79, 80, 82, 85, 89, 90, 102, 103, 104, 108, 109, 110, 111, 114, 116], "glmnet": [51, 52, 86, 116], "global": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 41, 42, 86, 87], "globalclassifi": 77, "globallearn": 77, "globalregressor": 77, "glrn": 52, "glrn_lasso": 52, "gname": 49, "go": [59, 60, 64, 68, 70, 77, 79], "goal": [56, 65, 66, 87], "goe": 87, "goldman": 115, "good": [64, 103, 105, 117], "gradient": [51, 72], "gradientboostingclassifi": 69, "gradientboostingregressor": 69, "gradual": 79, "gramfort": [112, 114], "graph": [52, 53, 80, 117], "graph_ensemble_classif": 52, "graph_ensemble_regr": 52, "graph_obj": 77, "graph_object": [59, 60, 64, 79], "graphlearn": [52, 86], "grasp": [56, 103, 105], "great": [63, 117], "greater": 117, "green": [48, 59, 60, 61, 76], "greg": 115, "grei": [51, 56], "grenand": 115, "grey50": 50, "grid": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 50, 52, 56, 59, 60, 61, 64, 68, 73, 74, 76, 79, 86, 103, 108], "grid_arrai": [59, 60], "grid_basi": 68, "grid_bound": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 79], "grid_search": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 52, 86], "grid_siz": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 59, 60], "gridextra": 50, "gridsearchcv": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 86], "grisel": [112, 114], "grob": 50, "group": [4, 12, 15, 47, 49, 56, 57, 67, 68, 73, 74, 75, 79, 84], "group_0": 85, "group_1": [65, 66, 85], "group_2": [65, 66, 85], "group_3": [65, 66], "group_effect": 75, "group_ind": 67, "group_treat": 67, "groupbi": [64, 72], "gruber": 25, "gt": [47, 49, 50, 51, 52, 53, 54, 56, 57, 59, 60, 61, 62, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 83, 114], "guarante": [50, 71], "guber": 25, "guess": [78, 103, 105], "guid": [33, 34, 37, 38, 41, 42, 48, 49, 50, 52, 56, 58, 63, 67, 68, 71, 77, 78, 86, 112, 114, 116], "guidelin": 116, "gunion": [52, 86], "gxidclusterperiodytreat": 49, "h": [21, 22, 23, 25, 29, 49, 50, 71, 77, 87, 115], "h20": 70, "h_0": [56, 67, 68, 78, 79, 103, 108, 117], "h_f": [35, 87], "ha": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 23, 37, 38, 39, 40, 41, 42, 48, 49, 50, 51, 58, 64, 69, 70, 71, 72, 73, 74, 77, 78, 79, 85, 86, 87, 103, 104, 105, 108, 109, 110, 111, 117], "half": [48, 58, 74, 82, 88], "hand": [35, 69, 70, 74, 117], "handbook": 74, "handl": [49, 56, 69, 86, 116], "hansen": [19, 20, 26, 28, 30, 50, 64, 71, 82, 112, 115], "happend": 69, "hard": [78, 103, 105], "harold": 115, "harsh": [37, 41], "hat": [48, 50, 58, 64, 67, 71, 74, 77, 81, 82, 85, 87, 88, 89, 102, 103, 105, 108, 110], "have": [4, 5, 12, 15, 17, 24, 27, 39, 40, 41, 42, 47, 48, 49, 50, 51, 52, 53, 56, 57, 59, 60, 62, 63, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 83, 85, 86, 87, 102, 103, 104, 105, 111, 113, 114, 116, 117], "hazlett": [79, 103, 105], "hc": [49, 115], "hc0": [39, 116], "hdm": [50, 71], "he": [53, 80], "head": [49, 50, 52, 54, 59, 60, 65, 66, 68, 70, 71, 72, 74, 77, 79, 83, 85, 114], "heat": [50, 71], "heatmap": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 50, 71, 79], "heavili": 69, "hei": 115, "height": [48, 50, 64, 70], "help": [49, 51, 61, 69, 73, 75, 79, 88, 117], "helper": 116, "henc": [49, 51, 52, 72, 79, 86, 89, 117], "here": [13, 16, 17, 49, 50, 51, 52, 53, 59, 60, 61, 62, 63, 65, 66, 67, 69, 71, 72, 73, 75, 76, 77, 79, 80, 83, 86, 87, 113], "heterogen": [12, 24, 51, 67, 72, 73, 75, 84, 87, 88, 115, 116, 117], "heteroskedast": [65, 66], "heurist": [48, 58, 82], "high": [14, 15, 26, 51, 63, 64, 72, 73, 81, 87, 102, 112, 114, 115], "higher": [49, 51, 64, 72, 73, 74, 77, 116, 117], "highli": [51, 72, 112], "highlight": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 62, 68, 70, 79, 116], "highlightcolor": [59, 60], "hint": 70, "hispan": 54, "hist": 56, "hist_e401": 51, "hist_p401": 51, "histogram": 56, "histplot": 58, "hjust": 51, "hline": [83, 102, 114, 117], "hold": [32, 50, 51, 53, 70, 71, 72, 80, 85, 86, 87], "holdout": [86, 88], "holm": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18], "home": [51, 72, 79], "homogen": 87, "hopefulli": 73, "horizont": [50, 63, 71], "hostedtoolcach": 72, "hot": 74, "hotstart_backward": [52, 86], "hotstart_forward": [52, 86], "household": [51, 72, 73, 78], "how": [37, 38, 41, 42, 47, 49, 50, 51, 53, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 75, 76, 77, 78, 79, 80, 82, 86, 87, 112, 113], "howev": [48, 51, 53, 58, 70, 72, 77, 79, 80, 82, 87, 117], "hown": [51, 72, 73, 78, 117], "hpwt": [50, 71], "hpwt0": 50, "hpwtairmpdspac": 50, "href": 112, "hspace": 69, "hstack": [18, 63], "html": [52, 66, 112, 114, 116], "http": [25, 31, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 86, 112, 113, 114, 116], "huber": [32, 53, 80, 87, 89, 97, 98, 115], "hue": 72, "huge": 69, "hugo": 115, "husd": [52, 54, 83, 114], "hyperparamet": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 51, 52, 54, 64, 69, 70, 72, 84, 114], "hypothes": [102, 115], "hypothesi": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 51, 72, 78, 103, 108, 115], "hypothet": 79, "i": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 47, 48, 49, 50, 51, 52, 53, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 98, 101, 102, 103, 104, 105, 108, 109, 111, 112, 113, 114, 116, 117], "i0": [62, 63, 87], "i03": 112, "i1": [62, 87], "i_": [28, 71, 75], "i_1": [50, 71], "i_2": [50, 71], "i_3": [50, 71], "i_4": 63, "i_est": 58, "i_fold": 50, "i_k": [50, 71, 81, 88, 102], "i_learn": 69, "i_level": 56, "i_rep": [48, 53, 58, 62, 69, 80, 82], "i_split": 71, "i_train": 58, "icp": 115, "id": [49, 50, 52, 71], "id_var": 71, "idea": [51, 52, 72, 73, 79, 86, 87, 103, 105, 117], "ident": [21, 22, 23, 24, 27, 28, 40, 52, 56, 68, 70, 77, 86, 87, 89, 95, 103, 108], "identfi": 79, "identif": [77, 87, 117], "identifi": [50, 51, 62, 67, 71, 72, 73, 77, 79, 85, 87, 103, 111, 116], "identifii": 85, "idnam": 49, "idx_tau": [61, 73, 76], "idx_treat": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 56, 103, 108], "ieee": 115, "ifels": 49, "ignor": [37, 38, 41, 42, 58, 77], "ii": [50, 71], "iid": 87, "iivm": [11, 25, 33, 34, 73, 81, 85, 94, 112, 116], "iivm_summari": 72, "iivmglmnet": 51, "iivmrang": 51, "iivmrpart": 51, "iivmxgboost11861": 51, "ij": [29, 50, 53, 56, 71, 80], "ilia": 115, "illustr": [48, 50, 51, 52, 53, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 72, 73, 75, 76, 78, 79, 80, 82, 86, 117], "iloc": [56, 62, 63, 69, 71, 74], "immedi": 113, "immun": [88, 115], "impact": [47, 57, 69, 74, 78], "implement": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 33, 34, 35, 41, 42, 48, 49, 50, 51, 52, 53, 58, 62, 64, 68, 69, 71, 72, 74, 77, 78, 79, 80, 82, 84, 85, 86, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 112, 114, 115, 116, 117], "impli": [21, 22, 50, 51, 71, 72, 73, 77, 85, 87, 103, 104, 106, 107, 109], "implment": 63, "import": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 35, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 102, 103, 108, 113, 114, 116, 117], "importlib": 64, "impos": 79, "improv": [62, 69, 75, 87, 116], "in_sample_norm": [8, 9, 62, 89, 92, 93, 103, 106, 107], "inbuild": 69, "inbuilt": 69, "inc": [51, 72, 73, 78, 117], "includ": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 43, 49, 51, 56, 63, 65, 66, 68, 72, 77, 78, 79, 85, 87, 102, 103, 104, 108, 109, 111, 113, 116, 117], "include_bia": [68, 71, 72], "include_scenario": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 79], "incom": [51, 72, 73, 75, 78, 117], "incorpor": [52, 78, 103, 108], "increas": [67, 69, 71, 79, 117], "increment": 116, "ind": 72, "independ": [8, 9, 21, 22, 23, 24, 36, 50, 52, 63, 67, 71, 75, 87, 89, 92, 93, 116], "index": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 48, 50, 54, 58, 63, 64, 65, 66, 70, 71, 72, 74, 75, 82, 83, 88, 89, 92, 93, 114], "index_col": 64, "india": [88, 115], "indic": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 23, 24, 27, 32, 35, 39, 50, 51, 53, 63, 67, 71, 72, 73, 77, 79, 80, 81, 83, 85, 87, 88], "individu": [4, 12, 41, 42, 49, 51, 56, 63, 65, 66, 67, 70, 72, 73, 77, 78, 85, 87, 117], "individual_df": 63, "induc": [84, 88], "industri": [50, 71], "inf": [7, 10, 49], "inf_model": 89, "infer": [26, 28, 47, 48, 50, 57, 58, 64, 69, 70, 71, 82, 84, 87, 88, 112, 114, 115, 116], "inferenti": 117, "infinit": [7, 10, 116], "influenc": [38, 42, 87], "info": [47, 52, 54, 56, 62, 67, 70, 71, 72, 73, 78, 80, 83, 114, 116, 117], "inform": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 37, 38, 39, 41, 42, 47, 52, 57, 59, 60, 69, 77, 78, 79, 87, 103, 105, 115], "infti": [48, 58, 82], "inher": 79, "inherit": [74, 116], "initi": [4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 35, 51, 52, 53, 61, 62, 72, 73, 76, 77, 78, 79, 80, 83, 85, 86, 87, 88, 114, 116, 117], "inlin": [54, 74], "inlinebackend": 74, "inner": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 86], "innermost": 86, "input": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 38, 42, 52, 78, 81, 102, 103, 105, 108], "insensit": 87, "insid": [37, 38, 41, 42], "insight": [64, 79], "insignific": 78, "inspect": 114, "inspir": [21, 25, 26, 32, 79], "instal": [51, 70, 77, 87, 116], "install_github": 113, "instanc": [41, 42, 51, 52, 72, 86], "instanti": [50, 51, 71, 72, 86, 88], "instead": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 37, 38, 41, 42, 47, 49, 51, 56, 57, 66, 67, 70, 72, 73, 85, 86, 87, 103, 106, 107, 109, 110, 116], "instruct": [113, 116], "instrument": [7, 10, 11, 14, 19, 25, 28, 50, 51, 52, 53, 54, 56, 62, 67, 71, 72, 73, 76, 78, 80, 83, 86, 87, 89, 96, 102, 114, 117], "instrument_effect": 47, "instrument_impact": 57, "insuffienct": 70, "int": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 27, 35, 36, 39, 40, 49, 50, 51, 57, 61, 62, 75, 76, 79, 80], "int64": [54, 69, 71, 83, 114], "int8": [72, 73, 78], "integ": [23, 52, 86], "integr": [70, 79, 103, 111, 116], "intend": [35, 52, 79, 117], "intent": [87, 117], "inter": 86, "interact": [4, 5, 11, 12, 21, 25, 26, 27, 35, 36, 56, 79, 84, 86, 104, 109, 112, 116, 117], "interchang": 102, "interest": [11, 12, 14, 15, 21, 22, 48, 51, 53, 58, 62, 64, 72, 73, 77, 80, 82, 85, 87, 89, 102, 114, 117], "interfac": [49, 51, 52, 83, 86, 88, 114], "intermedi": [66, 79], "intern": [49, 51, 52, 56, 70, 73, 86, 115], "internet": [51, 72, 73], "interpret": [65, 66, 79, 85, 103, 104, 105, 109, 110, 111, 113, 117], "intersect": [79, 103, 108, 116], "interv": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 39, 49, 50, 51, 53, 56, 59, 60, 61, 62, 65, 66, 68, 71, 73, 76, 77, 78, 80, 84, 85, 88, 89, 103, 108, 114, 115, 117], "intial": 77, "introduc": [48, 58, 82, 83, 102, 116, 117], "introduct": [48, 50, 52, 58, 71, 73, 78, 86, 87, 103, 105], "introductori": [49, 79], "intrument": [53, 80], "intspecifi": 35, "intuit": 79, "inuidur1": [52, 54, 83, 114], "inuidur1femaleblackothracedep1dep2q2q3q4q5q6agelt35agegt54durablelusdhusdtg": [52, 83, 114], "inuidur2": [54, 83, 114], "inv_sigmoid": 74, "invalid": [48, 58, 82], "invari": 87, "invers": [4, 6, 11, 12, 13, 16, 17, 18, 53, 80, 103, 104, 109], "invert_yaxi": 71, "investig": [64, 70, 79], "involv": [85, 86, 89, 117], "io": [74, 116], "ipw_norm": 116, "ipykernel_13509": 66, "ipynb": [47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80], "ira": [51, 72, 73], "irm": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 26, 27, 33, 34, 39, 40, 69, 79, 81, 84, 86, 95, 104, 109, 112, 116, 117], "irm_summari": 72, "irmglmnet": 51, "irmrang": 51, "irmrpart": 51, "irmxgboost8047": 51, "irrespect": 79, "is_classifi": [4, 8, 9, 11, 12, 15], "is_gat": [4, 12, 15, 39], "isnan": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 69, 86], "isoton": 79, "isotonicregress": 79, "issn": 64, "issu": [79, 112, 115, 116], "ite": [56, 65, 66, 67], "item": [11, 72, 81, 86, 88], "iter": [35, 47, 53, 62, 71, 72, 77, 80, 86, 102, 117], "itertool": 64, "its": [37, 38, 79, 81, 85, 86, 87, 88, 89, 102], "iv": [11, 14, 15, 25, 28, 29, 48, 50, 58, 71, 82, 83, 99, 100, 103, 110, 112, 116, 117], "iv_2": 47, "iv_var": [50, 71], "iv\u00e1n": [88, 115], "j": [19, 20, 21, 22, 23, 25, 26, 28, 29, 30, 31, 32, 48, 49, 50, 52, 53, 56, 58, 64, 71, 74, 80, 82, 86, 87, 102, 112, 114], "j_": [50, 71], "j_0": 102, "j_1": [50, 71], "j_2": [50, 71], "j_3": [50, 71], "j_k": [50, 71], "jame": 115, "janni": [51, 72], "jasenakova": 116, "javanmard": 115, "jbe": [50, 71], "jeconom": [21, 22, 23, 49], "jerzi": 115, "jia": 79, "jk": 87, "jmlr": [52, 112, 114, 116], "job": [51, 72, 73], "joint": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 39, 56, 59, 60, 61, 65, 66, 68, 73, 76, 87, 102, 116, 117], "jointli": [76, 85], "joss": [52, 86, 112, 114], "journal": [19, 20, 21, 22, 23, 29, 30, 32, 49, 50, 52, 64, 71, 74, 79, 82, 86, 112, 114, 115, 116], "jss": 112, "jump": [75, 77, 87], "jun": [49, 115], "jupyt": [47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80], "juraj": 115, "just": [49, 52, 56, 61, 62, 63, 65, 66, 67, 68, 75, 76, 87, 89, 92, 93, 103, 105], "justif": [88, 103, 105], "k": [19, 22, 23, 25, 26, 28, 29, 30, 32, 48, 50, 52, 58, 69, 70, 71, 77, 81, 82, 84, 85, 87, 102, 117], "k_h": [77, 87], "kaggl": [51, 72], "kallu": [61, 73, 76, 78, 89, 91, 96, 101, 115], "kappa": 87, "kato": [29, 50, 71, 102, 115], "kb": [56, 62, 67, 71, 72, 73, 78, 83, 114], "kde": [13, 16, 17, 72], "kdeplot": [62, 69, 80], "kdeunivari": [13, 16, 17], "kecsk\u00e9sov\u00e1": 116, "keep": [38, 42, 49, 66, 68, 79, 117], "kei": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 35, 40, 50, 51, 59, 60, 65, 66, 70, 71, 72, 73, 77, 79, 86, 87, 89, 103, 108, 116], "keith": 115, "kengo": 115, "kernel": [13, 16, 17, 35, 38, 42, 77, 87], "kernel_regress": 77, "kernelreg": 77, "keyword": [4, 12, 15, 23, 29, 30, 31, 36, 39], "kf": 88, "kfold": [71, 88], "kind": [47, 57, 72], "kj": [22, 23, 25, 26, 28, 29, 30, 32, 48, 50, 58, 71, 82], "klaassen": [25, 64, 69, 70, 79, 112, 115], "klaa\u00dfen": 25, "knau": 115, "know": [62, 75], "knowledg": [47, 57, 69, 74, 75], "known": [67, 69, 77, 79, 86, 87], "kohei": 115, "kotthof": 52, "kotthoff": [52, 86, 112, 114], "krueger": 74, "kueck": [51, 72], "kurz": [112, 115, 116], "kwarg": [4, 12, 15, 21, 22, 23, 27, 29, 30, 31, 35, 36, 37, 39, 70, 87], "l": [50, 52, 53, 54, 59, 60, 71, 79, 80, 86, 103, 110, 112, 114], "l1": [72, 80, 87], "l_hat": [14, 15, 48, 58, 89], "lab": 53, "label": [37, 41, 56, 58, 59, 60, 61, 63, 65, 66, 68, 70, 73, 74, 76, 77], "labor": 74, "laffer": 115, "laff\u00e9r": [32, 53, 80, 87, 89, 97, 98], "lal": [74, 116], "lambda": [50, 51, 52, 53, 72, 74, 75, 86, 87, 89, 93, 102, 114], "lambda_": 64, "lambda_0": [89, 93], "lambda_t": 23, "land": 75, "lang": [52, 86, 112, 114], "langl": [24, 75], "lappli": 88, "larg": [48, 58, 67, 69, 70, 74, 79, 87], "larger": [12, 49, 77, 79, 103, 108], "largest": 69, "largli": 69, "lasso": [50, 51, 52, 53, 72, 80, 86, 114, 115], "lasso_class": [51, 72], "lasso_pip": [52, 86], "lasso_summari": 72, "lassocv": [18, 64, 71, 72, 80, 86, 87, 102, 114], "last": [23, 52, 113], "late": [11, 47, 51, 72, 87, 89, 94], "latent": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 22, 78, 103, 110, 111], "later": [51, 52, 77, 79, 86, 117], "latter": [41, 42, 87], "layout": 64, "lbrace": [11, 12, 25, 26, 32, 50, 71, 81, 87, 88, 89, 90, 102, 103, 104], "ldot": [14, 15, 50, 53, 71, 80, 81, 87, 88, 102, 114], "le": [23, 62, 75, 85, 87, 89, 96, 101], "lead": [49, 79, 87], "leadsto": 102, "lear": [52, 86, 112, 114], "learn": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 29, 30, 31, 32, 35, 47, 51, 52, 54, 56, 57, 61, 64, 68, 69, 70, 72, 73, 74, 76, 77, 79, 83, 84, 86, 88, 89, 102, 103, 105, 116, 117], "learner": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 47, 48, 49, 50, 51, 53, 58, 59, 60, 62, 64, 71, 72, 73, 78, 79, 80, 81, 82, 84, 87, 88, 89, 102, 103, 108, 116, 117], "learner_class": [18, 116], "learner_cv": 52, "learner_forest_classif": 52, "learner_forest_regr": 52, "learner_l": 78, "learner_lasso": 52, "learner_list": 69, "learner_m": 78, "learner_nam": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 86], "learner_param_v": 52, "learner_rf": 102, "learnerclassif": 52, "learnerregr": 52, "learnerregrcvglmnet": 52, "learnerregrrang": [52, 86], "learning_r": [58, 61, 73, 76, 77, 79, 82], "least": [47, 51, 57, 72, 73, 78, 87, 88], "leav": [53, 79, 80], "left": [25, 26, 28, 29, 32, 48, 50, 56, 58, 69, 71, 72, 73, 74, 76, 77, 82, 87, 89, 92, 93, 102, 103, 104, 106, 107, 109], "legend": [51, 56, 58, 59, 60, 61, 63, 65, 66, 68, 69, 73, 74, 76], "len": [56, 61, 69, 70, 71, 73, 76], "length": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 48, 52, 62, 86], "leq": [50, 71], "less": [49, 51, 72, 73, 77, 79], "lester": 115, "let": [21, 22, 23, 27, 48, 49, 51, 52, 53, 56, 58, 61, 62, 65, 66, 68, 69, 72, 73, 76, 79, 80, 81, 82, 86, 87, 103, 105, 111, 117], "level": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 27, 35, 39, 50, 51, 53, 56, 59, 60, 61, 62, 63, 65, 66, 67, 68, 71, 72, 73, 76, 78, 79, 80, 86, 89, 90, 103, 104, 108, 117], "level_0": [52, 71], "level_1": 71, "level_bound": 56, "levinsohn": [50, 71], "lewi": 115, "lgbmclassifi": [61, 62, 63, 69, 73, 76, 77, 79], "lgbmregressor": [58, 61, 62, 63, 69, 73, 77, 79, 82], "lgr": [48, 49, 50, 51, 52, 53, 81, 86, 87, 88, 89, 102, 114], "lib": [71, 72], "liblinear": [72, 80, 87], "librari": [47, 48, 49, 50, 51, 52, 53, 81, 82, 83, 86, 87, 88, 89, 102, 113, 114, 117], "licens": 116, "lie": 115, "lightgbm": [58, 61, 62, 63, 69, 73, 76, 77, 79], "like": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 37, 38, 41, 42, 49, 51, 52, 64, 66, 72, 73, 79, 86, 88, 114, 117], "lim": 74, "lim_": [77, 87], "limegreen": [59, 60], "limit": [74, 87, 115], "limits_": 85, "lin": [77, 79, 87], "line": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 56, 63, 79], "linear": [4, 12, 14, 15, 21, 22, 27, 28, 29, 30, 31, 33, 34, 39, 47, 48, 49, 50, 52, 56, 57, 58, 59, 60, 62, 63, 64, 65, 68, 69, 70, 71, 78, 79, 81, 82, 84, 85, 86, 88, 90, 92, 93, 94, 95, 99, 100, 102, 108, 109, 110, 111, 112, 114, 115, 116, 117], "linear_model": [4, 12, 15, 18, 39, 54, 56, 57, 64, 68, 69, 71, 72, 77, 79, 80, 86, 87, 102, 114], "linearli": [77, 87], "linearregress": [47, 56, 57, 68, 69, 77, 79], "linearscoremixin": [0, 89], "lineplot": 56, "linestyl": [56, 63, 70, 77], "linetyp": 53, "linewidth": 63, "link": [79, 116], "linspac": [59, 60, 68, 79], "lint": 116, "linux": 113, "list": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 38, 42, 48, 49, 50, 51, 52, 58, 59, 60, 71, 73, 75, 82, 86, 88, 89, 113, 116], "listedcolormap": 71, "literatur": [79, 87], "littl": 67, "ll": [52, 102, 117], "lllllllllllllllll": [83, 114], "lm": [47, 49, 79], "ln_alpha_ml_l": 64, "ln_alpha_ml_m": 64, "load": [47, 49, 51, 52, 64, 72, 73, 83, 113, 114], "loader": 0, "loc": [56, 58, 61, 63, 64, 66, 71, 74, 76, 78, 79], "local": [11, 13, 85, 87, 115, 116], "localconvert": 71, "locat": [61, 76, 87], "log": [50, 64, 69, 71, 74, 78, 86, 87], "log_odd": 75, "log_p": [50, 71], "log_reg": [47, 49], "logarithm": 64, "logic": [11, 52, 86], "logical_not": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 69, 86], "logist": [21, 36, 47, 49, 51, 53, 56, 57, 72, 79, 80, 117], "logisticregress": [47, 54, 56, 57, 68, 77, 79], "logisticregressioncv": [18, 69, 72, 80, 87], "logit": [69, 74], "loglik": 52, "logloss": [51, 72, 117], "logo": 116, "logspac": 72, "long": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 21, 22, 48, 58, 69, 78, 79, 103, 105, 111, 115], "look": [49, 51, 52, 61, 62, 63, 69, 72, 73, 76, 77, 78], "loop": 56, "loss": [69, 70, 77, 78, 86, 87], "loss_ml_g0": 69, "loss_ml_g1": 69, "loss_ml_m": 69, "low": [63, 67, 85, 115], "lower": [51, 52, 56, 61, 63, 64, 67, 68, 73, 74, 76, 77, 78, 79, 86, 103, 108, 111, 117], "lower_bound": [59, 60], "lpq": [13, 17, 73, 85, 96, 116], "lpq_0": 76, "lpq_1": 76, "lqte": 85, "lr": 77, "lrn": [47, 48, 49, 50, 51, 52, 53, 81, 86, 87, 88, 89, 102, 114, 117], "lrn_0": 52, "lt": [47, 49, 50, 51, 52, 53, 54, 56, 62, 67, 71, 72, 73, 75, 78, 79, 80, 83, 114], "lucien": 116, "luka": 115, "luk\u00e1\u0161": 32, "lusd": [52, 54, 83, 114], "lvert": 64, "m": [18, 19, 20, 21, 28, 29, 30, 48, 50, 52, 54, 58, 64, 67, 69, 70, 71, 74, 82, 84, 85, 86, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116], "m_": [56, 87, 89, 90, 96, 102], "m_0": [4, 6, 8, 9, 11, 12, 14, 15, 16, 30, 31, 35, 48, 50, 51, 58, 64, 67, 70, 71, 72, 82, 85, 86, 87, 89, 91, 92, 93, 96, 97, 98, 101, 114, 117], "m_hat": [11, 12, 14, 15, 48, 58, 68, 89], "m_i": [77, 87], "ma": [29, 50, 71, 115], "mac": 113, "machin": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 29, 30, 31, 32, 35, 47, 51, 52, 53, 54, 56, 57, 61, 62, 64, 68, 69, 70, 72, 73, 74, 76, 77, 78, 79, 80, 84, 86, 87, 88, 89, 102, 103, 105, 116, 117], "machineri": [64, 115], "mackei": 115, "maco": 113, "made": [87, 117], "mae": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 69, 86], "maggi": 115, "magnitud": [103, 105], "mai": [38, 42, 53, 62, 80], "main": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 21, 22, 64, 73, 79, 87, 102, 103, 105, 115, 117], "mainli": 79, "maintain": [49, 112, 116], "mainten": 116, "major": [52, 79, 116], "make": [47, 56, 57, 69, 70, 79, 85, 86, 116, 117], "make_confounded_irm_data": [79, 116], "make_confounded_plr_data": 78, "make_did_sz2020": [8, 9, 62, 87], "make_heterogeneous_data": [59, 60, 65, 66, 67], "make_iivm_data": [11, 13, 85, 87], "make_irm_data": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 68, 69, 85, 86, 87], "make_irm_data_discrete_treat": 56, "make_pipelin": 72, "make_pliv_chs2015": [14, 87], "make_pliv_multiway_cluster_ckms2021": [7, 50, 71], "make_plr_ccddhnr2018": [4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 48, 58, 70, 81, 82, 85, 86, 87, 88, 89, 102, 103, 108], "make_simple_rdd_data": [35, 77, 87], "make_spd_matrix": 31, "make_ssm_data": [53, 80, 87], "malt": [112, 115], "maltekurz": 112, "man": [47, 57], "manag": [86, 113], "mani": [28, 33, 34, 48, 49, 50, 52, 58, 62, 70, 71, 82, 89, 102, 117], "manili": 39, "manipul": [51, 52, 77, 87], "manual": [51, 68, 70, 78, 117], "mao": 115, "map": [11, 37, 38, 41, 42, 49, 50, 71, 85, 87], "mapsto": [81, 85], "mar": [32, 87], "march": [64, 69, 70], "margin": [59, 60, 79], "marit": [51, 72], "marker": [56, 79], "markers": 74, "market": 74, "markettwo": 50, "markov": [31, 115], "marr": [51, 72, 73, 78, 117], "marshal": 86, "martin": [32, 79, 112, 115, 116], "masatoshi": 115, "master": 49, "mat": 50, "match": [86, 103, 110], "math": 18, "mathbb": [11, 12, 14, 15, 21, 22, 23, 27, 33, 34, 50, 53, 56, 62, 63, 67, 69, 70, 71, 74, 77, 80, 85, 87, 89, 90, 91, 92, 93, 95, 96, 97, 98, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 114, 117], "mathcal": [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 36, 48, 50, 53, 58, 61, 63, 71, 75, 76, 80, 82], "mathop": 85, "mathrm": [21, 22, 77, 87], "matia": 115, "matplotlib": [54, 56, 58, 59, 60, 61, 62, 63, 65, 66, 68, 69, 70, 71, 72, 73, 74, 76, 77, 79, 80], "matric": [75, 84, 116], "matrix": [21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 38, 42, 48, 50, 51, 52, 53, 58, 71, 80, 82, 83, 86, 102, 114, 116, 117], "matt": 115, "matter": [69, 74], "max": [51, 52, 68, 72, 73, 81, 85, 86, 87, 88, 89, 91, 102, 114, 117], "max_depth": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 54, 72, 78, 81, 85, 86, 87, 88, 89, 102, 103, 108, 114, 117], "max_featur": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 54, 72, 78, 81, 85, 86, 87, 88, 89, 102, 103, 108, 114, 117], "max_it": [71, 72, 79], "maxim": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 75, 85, 87], "maxima": 102, "maximum": [85, 86], "mb": [54, 80, 83, 114], "mb706": 116, "mea": 25, "mean": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 37, 38, 41, 42, 47, 48, 50, 51, 56, 57, 58, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 76, 78, 79, 82, 86, 87, 102, 117], "mean_absolute_error": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 69, 86], "meant": [85, 116], "measir": 78, "measur": [49, 52, 64, 70, 78, 79, 86, 87, 103, 104, 105, 109, 110, 111], "measure_col": 64, "measure_func": 49, "measure_pr": 49, "measures_r": 49, "mechan": [37, 38, 41, 42, 79], "median": [79, 88], "melt": 50, "membership": 79, "memori": [54, 56, 62, 67, 71, 72, 73, 78, 80, 83, 114], "mention": [67, 85], "merg": [51, 72], "mert": [88, 115], "meshgrid": [59, 60, 79], "messag": [48, 49, 50, 51, 52, 53, 114, 116], "meta": [37, 38, 41, 42, 86, 114], "metadata": [37, 38, 41, 42], "metadata_rout": [37, 38, 41, 42], "metadatarequest": [37, 38, 41, 42], "method": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 33, 34, 35, 37, 38, 39, 40, 41, 42, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 65, 66, 68, 69, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 85, 86, 87, 88, 89, 102, 103, 105, 108, 112, 114, 116], "methodolog": 115, "methodologi": 79, "metric": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 37, 41, 86], "michael": 115, "michaela": 116, "michel": [112, 114], "michela": [32, 115], "mid": [51, 72, 74, 77, 87], "mid_point": 56, "might": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 50, 61, 68, 69, 71, 75, 77, 78, 79, 86, 87], "mild": [48, 58, 82], "militari": 74, "miller": [50, 71], "mimic": 79, "min": [50, 51, 52, 53, 61, 68, 71, 72, 73, 76, 77, 81, 86, 87, 88, 89, 102, 114, 117], "min_": 85, "min_samples_leaf": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 40, 67, 72, 78, 81, 85, 86, 87, 88, 89, 102, 103, 108, 117], "min_samples_split": 72, "minim": [12, 40, 51, 69, 72, 77, 87], "minor": [48, 58, 82, 89, 116], "minsplit": 51, "minut": 70, "miruna": 115, "mislead": 116, "miss": [7, 10, 18, 52, 86, 87, 89, 97, 116], "missing": [32, 53, 80], "misspecif": 62, "misspecifi": 62, "mit": [112, 114], "mixin": [0, 33, 34, 89], "ml": [31, 50, 51, 52, 64, 70, 71, 72, 77, 81, 84, 86, 87, 88, 112, 115, 116], "ml_g": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 47, 48, 49, 51, 53, 54, 56, 57, 58, 59, 61, 62, 63, 65, 67, 68, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 85, 86, 87, 116], "ml_g0": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 49, 51, 54, 62, 69, 72, 78, 86, 87], "ml_g1": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 49, 51, 54, 62, 69, 72, 78, 86, 87], "ml_g_d0": [80, 87], "ml_g_d0_t0": [62, 87], "ml_g_d0_t1": [62, 87], "ml_g_d1": [80, 87], "ml_g_d1_t0": [62, 87], "ml_g_d1_t1": [62, 87], "ml_g_d_lvl0": 87, "ml_g_d_lvl1": 87, "ml_g_sim": 18, "ml_l": [14, 15, 48, 50, 51, 52, 54, 58, 60, 66, 70, 71, 72, 74, 78, 81, 82, 86, 87, 88, 89, 102, 103, 108, 114, 116, 117], "ml_l_bonu": 114, "ml_l_forest": 52, "ml_l_forest_pip": 52, "ml_l_lasso": 52, "ml_l_lasso_pip": 52, "ml_l_rf": 117, "ml_l_sim": 114, "ml_l_tune": 86, "ml_l_xgb": 117, "ml_m": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 85, 86, 87, 88, 89, 102, 103, 108, 114, 116, 117], "ml_m_bench_control": 79, "ml_m_bench_treat": 79, "ml_m_bonu": 114, "ml_m_forest": 52, "ml_m_forest_pip": 52, "ml_m_lasso": 52, "ml_m_lasso_pip": 52, "ml_m_rf": 117, "ml_m_sim": [18, 114], "ml_m_tune": 86, "ml_m_xgb": 117, "ml_pi": [18, 53, 80, 87], "ml_pi_sim": 18, "ml_r": [11, 14, 47, 50, 51, 57, 71, 72, 87, 116], "ml_r0": 87, "ml_r1": [51, 72, 87], "mlr": [52, 86], "mlr3": [47, 48, 49, 50, 51, 53, 81, 86, 87, 88, 89, 102, 112, 114, 116, 117], "mlr3book": [52, 86], "mlr3extralearn": [51, 86], "mlr3filter": 52, "mlr3learner": [47, 48, 49, 50, 51, 81, 86, 87, 88, 89, 102, 114, 117], "mlr3measur": 49, "mlr3pipelin": [86, 116], "mlr3tune": [52, 86, 116], "mlr3vers": 51, "mlrmeasur": 49, "mode": [79, 113], "model": [0, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 42, 43, 47, 48, 49, 50, 52, 57, 58, 61, 62, 63, 64, 67, 69, 71, 73, 76, 78, 81, 82, 83, 84, 86, 90, 92, 93, 94, 95, 99, 100, 104, 105, 108, 109, 110, 111, 112, 115, 116], "model_data": [51, 72], "model_label": 70, "model_select": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 58, 71, 86, 88], "modellist": 68, "modelmlestimatelowerupp": 51, "modern": [52, 86, 112, 114], "modul": [77, 87, 113], "moment": [33, 34, 50, 71, 89, 102, 103, 105, 111, 114], "monoton": 87, "mont": [21, 22, 24, 27, 59, 60, 65, 66], "montanari": 115, "more": [12, 39, 47, 49, 51, 56, 57, 59, 60, 64, 68, 69, 70, 72, 73, 77, 78, 79, 81, 85, 86, 87, 89, 95, 102, 103, 105, 108, 111, 114, 117], "moreov": [51, 52, 64, 86, 102, 117], "mortgag": [51, 72, 73], "most": [51, 61, 69, 72, 73, 76, 79, 85, 86, 87, 103, 108, 113], "motiv": [79, 82], "motivation_example_bch": 64, "mp": 49, "mpd": [50, 71], "mpg": 71, "mse": [52, 64, 86], "mserd": 77, "msr": [52, 86], "mtry": [51, 52, 81, 86, 87, 88, 89, 102, 117], "mu": 63, "mu_": 63, "mu_0": 87, "mu_mean": 63, "much": [51, 52, 72, 77, 79, 117], "muld": [54, 83, 114], "multi": [37, 41, 49, 50, 59, 60, 71], "multiclass": [52, 70], "multiindex": 71, "multioutput": [38, 42], "multioutputregressor": [38, 42], "multipl": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 27, 49, 50, 51, 53, 62, 68, 71, 72, 78, 79, 80, 83, 86, 88, 102, 103, 105, 116, 117], "multipletest": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18], "multipli": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 58, 84, 85, 89, 117], "multiprocess": [61, 73, 76], "multitest": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18], "multivariate_norm": 18, "multiwai": [29, 50, 71, 115], "music": 115, "must": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 86, 87], "mutat": 52, "mutual": [4, 12, 15, 51, 65, 66, 72, 73, 85], "my_sampl": 88, "my_task": 88, "n": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 36, 47, 48, 50, 52, 53, 56, 57, 58, 61, 63, 64, 67, 71, 74, 75, 76, 77, 80, 81, 82, 85, 86, 87, 88, 102, 112, 113], "n_": [27, 63], "n_coef": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 103, 108], "n_complier": 76, "n_core": [61, 73, 76], "n_estim": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 54, 58, 59, 60, 61, 62, 63, 65, 66, 67, 72, 73, 75, 76, 77, 78, 79, 81, 82, 85, 86, 87, 88, 89, 102, 103, 108, 114, 117], "n_eval": [52, 86], "n_featur": [37, 38, 41, 42], "n_fold": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 48, 49, 50, 51, 54, 58, 59, 60, 61, 62, 65, 66, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 82, 86, 88, 114, 117], "n_folds_per_clust": [50, 71], "n_folds_tun": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18], "n_iter": [35, 77, 87], "n_iter_randomized_search": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 86], "n_job": 72, "n_jobs_cv": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 69], "n_jobs_model": [5, 17, 61, 73, 76], "n_level": [27, 56], "n_ob": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 36, 39, 40, 48, 52, 53, 56, 58, 59, 60, 62, 63, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 102, 103, 108, 114], "n_output": [37, 38, 41, 42], "n_rep": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 48, 49, 50, 53, 54, 56, 58, 62, 67, 68, 69, 71, 77, 78, 79, 80, 82, 86, 88, 103, 108, 114, 117], "n_rep_boot": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 39, 56, 59, 60, 61, 65, 66, 68, 73, 76, 102], "n_sampl": [37, 38, 41, 42, 75], "n_samples_fit": [38, 42], "n_split": 88, "n_t": 63, "n_target": [41, 42], "n_time_period": 63, "n_true": [61, 76], "n_var": [48, 52, 58, 82, 83, 86, 102, 114], "n_w": 75, "n_x": [24, 59, 60, 65, 66, 67], "na": [7, 10, 48, 50, 53, 82, 116], "na_real_": [50, 116], "naiv": [48, 58, 82], "name": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 37, 38, 41, 42, 48, 49, 50, 65, 66, 67, 70, 71, 77, 78, 79, 86, 113, 116], "namespac": 49, "nan": [4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 56, 58, 61, 62, 63, 65, 66, 69, 70, 72, 73, 76, 80, 82, 86], "nanmean": 58, "narita": 115, "nathan": 115, "nation": [79, 88, 115], "nativ": 49, "natt": 75, "natur": 79, "ncol": [50, 51, 52, 77, 83, 86, 102, 114], "ncoverag": 69, "ndarrai": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 23, 25, 26, 28, 29, 30, 31, 32, 83], "nearli": 69, "necess": [50, 71], "necessari": [49, 50, 70, 71, 77, 87, 113], "need": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 34, 47, 48, 49, 51, 53, 57, 58, 70, 73, 80, 86, 88, 103, 111, 116, 117], "neg": [38, 42], "neighborhood": [77, 102], "neither": [7, 10, 50, 71, 83], "neng": 115, "neq": [77, 87], "nest": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 41, 42, 86, 89, 98, 103, 108], "net": [73, 78, 117], "net_tfa": [51, 72, 73, 78, 117], "never": [11, 49, 50, 66, 71, 116], "never_tak": [11, 51, 72], "nevertheless": 68, "new": [47, 48, 49, 50, 51, 52, 53, 59, 60, 70, 72, 75, 81, 82, 83, 85, 86, 87, 88, 89, 102, 112, 114, 115, 116, 117], "new_data": [59, 60, 75], "newei": [19, 20, 30, 50, 64, 71, 79, 82, 112, 115], "newest": 116, "next": [49, 51, 52, 59, 60, 61, 67, 69, 72, 73, 75, 76, 79, 116], "neyman": [50, 71, 81, 84, 103, 111, 112, 115], "nfold": [50, 51, 53, 87], "nh": 87, "nice": 49, "nifa": [72, 73, 78], "nil": 79, "nine": [50, 71], "nn": 77, "noack": [77, 87, 115, 116], "node": [51, 52, 81, 87, 88, 89, 102, 114, 117], "nois": [36, 74, 75], "non": [23, 29, 30, 31, 35, 47, 48, 51, 57, 58, 63, 72, 73, 75, 77, 86, 88, 89, 102], "non_orth_scor": [48, 58, 89], "nondur": 54, "none": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 27, 35, 37, 38, 39, 41, 42, 50, 51, 54, 56, 57, 62, 67, 72, 73, 78, 79, 80, 83, 86, 87, 89, 102, 113, 114], "nonignor": [18, 98], "nonlinear": [34, 51, 72, 77, 87, 89, 96, 101, 116], "nonlinearscoremixin": [0, 89], "nonparametr": [13, 16, 17, 77, 79, 103, 104, 105, 109, 110, 111, 115], "nop": 52, "nor": [7, 10, 50, 71, 83], "norm": 58, "normal": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 23, 48, 53, 57, 58, 61, 62, 63, 67, 73, 74, 75, 76, 77, 80, 82, 83, 86, 87, 89, 92, 93, 102, 114], "normalize_ipw": [4, 5, 6, 11, 12, 13, 16, 17, 18, 53, 68, 73, 80], "notat": [50, 53, 62, 71, 80, 87], "note": [7, 10, 11, 12, 14, 15, 18, 33, 34, 37, 38, 40, 41, 42, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 88, 89, 112, 114], "notebook": [47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 85, 86, 87, 117], "notic": [47, 57], "now": [49, 50, 51, 53, 59, 60, 69, 71, 72, 75, 79, 80, 114, 116], "np": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 23, 25, 26, 28, 29, 30, 31, 32, 35, 54, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 102, 103, 108, 114, 117], "nround": [48, 51, 117], "nrow": [49, 50, 52, 77, 83, 86, 102, 114], "nu": [11, 23, 31, 53, 80, 87, 103, 105, 108, 110, 111], "nu2": [103, 108], "nu_0": [103, 111], "nu_i": [53, 80], "nuis_g0": 47, "nuis_g1": 47, "nuis_l": 117, "nuis_m": [47, 117], "nuis_r0": 47, "nuis_r1": 47, "nuis_rmse_ml_l": 64, "nuis_rmse_ml_m": 64, "nuisanc": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 30, 31, 35, 48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 64, 67, 69, 71, 72, 73, 76, 78, 79, 80, 81, 82, 86, 87, 88, 89, 90, 92, 93, 96, 102, 103, 111, 112, 116, 117], "nuisance_el": [103, 104, 106, 107, 109, 110], "nuisance_loss": [69, 86, 116], "nuisance_target": 69, "null": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 49, 78, 86, 103, 108, 116], "null_hypothesi": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 78, 103, 108], "num": [51, 52, 81, 86, 87, 88, 89, 102, 114], "num_leav": [61, 63, 73, 76], "number": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 36, 38, 39, 40, 42, 48, 50, 58, 59, 60, 61, 63, 64, 65, 66, 69, 71, 73, 75, 76, 77, 79, 88, 102, 112, 114, 117], "numer": [34, 47, 52, 68, 74, 86, 89, 103, 104, 109, 116], "numeric_onli": 64, "numpi": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 35, 39, 40, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, 102, 103, 108, 114], "ny": 115, "o": [56, 63, 64, 65, 66, 69, 70, 72, 74, 77, 102, 112, 114], "ob": [49, 51, 63, 77], "obei": 89, "obj": 72, "obj_dml_data": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 47, 48, 50, 57, 58, 61, 68, 70, 71, 76, 81, 82, 85, 86, 87, 88, 89, 102, 103, 108, 116], "obj_dml_data_bonu": 83, "obj_dml_data_bonus_df": 83, "obj_dml_data_from_arrai": [7, 10], "obj_dml_data_from_df": [7, 10], "obj_dml_data_sim": 83, "obj_dml_plr": [48, 58, 82], "obj_dml_plr_bonu": [52, 114], "obj_dml_plr_bonus_pip": 52, "obj_dml_plr_bonus_pipe2": 52, "obj_dml_plr_bonus_pipe3": 52, "obj_dml_plr_bonus_pipe_ensembl": 52, "obj_dml_plr_fullsampl": 70, "obj_dml_plr_lesstim": 70, "obj_dml_plr_nonorth": [48, 58], "obj_dml_plr_orth_nosplit": [48, 58], "obj_dml_plr_sim": [52, 114], "obj_dml_plr_sim_pip": 52, "obj_dml_plr_sim_pipe_ensembl": 52, "obj_dml_plr_sim_pipe_tun": 52, "obj_dml_sim": 18, "object": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 23, 25, 26, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 41, 42, 47, 51, 52, 53, 54, 56, 59, 60, 61, 62, 66, 67, 68, 70, 72, 73, 76, 77, 80, 83, 85, 86, 87, 88, 89, 102, 112, 114, 115, 116, 117], "obs_confound": [47, 57], "observ": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 36, 39, 40, 43, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 64, 67, 69, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 92, 93, 102, 103, 105, 106, 107, 114, 115, 117], "obtain": [22, 47, 48, 49, 50, 53, 57, 58, 59, 60, 61, 62, 64, 69, 71, 76, 79, 80, 81, 82, 85, 86, 88, 89, 102, 103, 105, 108, 113, 114], "occur": [70, 116], "off": [75, 115], "offer": [49, 51, 72, 73, 79, 117], "offici": 113, "offset": 86, "often": 76, "oka": 115, "ol": [4, 12, 15, 39], "olma": [77, 87, 115, 116], "omega": [67, 85, 89, 90, 95, 103, 104, 109], "omega_": [29, 50, 71], "omega_1": [29, 50, 71], "omega_2": [29, 50, 71], "omega_epsilon": [50, 71], "omega_v": [29, 50, 71], "omega_x": [29, 50, 71], "omit": [78, 79, 103, 105, 111, 115, 116, 117], "ommit": 79, "onc": [49, 70, 79, 87, 117], "one": [14, 43, 47, 48, 49, 50, 51, 52, 56, 57, 58, 59, 60, 69, 71, 73, 74, 77, 78, 79, 82, 85, 86, 87, 88, 89, 92, 93, 95, 99, 100, 102, 103, 104, 105, 108, 109, 110, 114, 116], "ones": [52, 61, 63, 70, 76, 78, 85], "ones_lik": [56, 76], "onli": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 37, 38, 39, 41, 42, 49, 50, 51, 59, 60, 65, 66, 67, 69, 70, 71, 72, 73, 77, 81, 85, 86, 87, 89, 91, 96, 101, 102, 103, 104, 105, 109, 111, 116], "onlin": 117, "onto": 69, "oo": 70, "oob_error": [52, 86], "oop": 116, "opac": [59, 60], "open": [52, 86, 112, 114], "oper": 52, "opposit": [75, 77, 87], "oprescu": [24, 59, 60, 65, 66, 115], "opt": 72, "optim": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 35, 52, 59, 60, 70, 75, 85, 86, 115], "option": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 37, 38, 41, 42, 47, 48, 50, 51, 53, 56, 59, 60, 65, 66, 67, 69, 71, 72, 73, 80, 86, 87, 88, 89, 91, 96, 101, 102, 116], "oracl": [27, 36, 56], "oracle_valu": [21, 22, 27, 36, 56], "orang": 48, "orcal": [21, 22], "order": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 41, 49, 50, 51, 52, 68, 71, 72, 77, 86, 87, 88, 89], "org": [25, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 86, 112, 113, 116], "orient": [52, 86, 89, 112, 114, 115, 116], "origin": [37, 38, 40, 41, 42, 49, 52, 66, 75, 78, 79, 85, 89, 95], "orign": [51, 72], "orth_sign": [39, 40, 68], "orthogon": [39, 40, 50, 51, 71, 72, 81, 84, 87, 102, 103, 111, 112, 115], "orthongon": [103, 111], "osx": 113, "other": [0, 7, 10, 14, 15, 37, 38, 41, 42, 48, 50, 51, 52, 53, 56, 58, 62, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, 95, 102, 103, 111, 112, 113, 114, 115, 116, 117], "other_ind": 71, "otherwis": [4, 8, 9, 11, 12, 15, 37, 38, 41, 42, 51, 72, 73, 75, 87], "othrac": [52, 54, 83, 114], "our": [48, 49, 51, 52, 58, 59, 60, 61, 62, 69, 70, 72, 73, 76, 77, 78, 79, 82, 87, 112, 114, 116, 117], "ourselv": 69, "out": [14, 15, 50, 52, 54, 62, 64, 69, 70, 71, 73, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 99, 100, 102, 103, 105, 108, 110, 112, 114, 116, 117], "outcom": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 21, 22, 23, 27, 36, 47, 49, 50, 51, 52, 54, 57, 63, 64, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 83, 86, 90, 102, 104, 105, 108, 110, 111, 114, 116, 117], "outcome_0": 57, "outcome_1": 57, "outer": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 86], "output": [49, 69, 81, 102, 117], "outshr": 71, "outsid": 48, "over": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 48, 56, 58, 64, 69, 82, 84, 86, 103, 108], "overal": [75, 79], "overcom": [84, 89], "overfit": [70, 84, 88], "overlap": [62, 79, 87], "overrid": [86, 116], "overridden": 87, "overst": [51, 72, 73], "overview": [69, 102, 103, 108, 115], "overwrit": 116, "ownership": [51, 72], "p": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 26, 35, 36, 48, 49, 50, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 85, 86, 87, 88, 89, 90, 91, 92, 93, 96, 97, 98, 101, 102, 103, 104, 109, 112, 113, 114, 116], "p401": [51, 72, 73], "p_0": [89, 92, 93], "p_1": 102, "p_adjust": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 88, 102, 112, 114], "p_dbl": [52, 86], "p_hat": 68, "p_int": 86, "p_n": 28, "p_val": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18], "p_x": [29, 50, 71], "p_x0": 74, "p_x1": 74, "packag": [47, 48, 50, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 70, 71, 73, 75, 76, 77, 78, 79, 80, 81, 82, 85, 86, 87, 88, 89, 102, 103, 105, 112, 114, 115, 116, 117], "packagedata": 71, "packagevers": 51, "page": [79, 112, 115], "pair": [47, 57], "pake": [50, 71], "paket": [50, 51, 52], "pal": 50, "palett": 56, "panda": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 39, 40, 53, 54, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 83, 85, 87, 103, 105, 114], "pandas2ri": 71, "panel": [8, 23, 107, 115, 116], "paper": [25, 28, 52, 70, 74, 77, 78, 79, 103, 111, 112, 114, 115, 116], "par": 54, "par_grid": [52, 86], "paradox": [52, 86, 116], "parallel": [49, 56, 61, 62, 63, 69, 76, 87], "param": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 37, 38, 41, 42, 70, 86], "param_grid": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 86], "param_nam": 49, "param_set": [52, 86], "param_v": 52, "paramet": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 48, 49, 50, 51, 53, 56, 58, 59, 60, 61, 62, 64, 67, 68, 69, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 96, 101, 102, 103, 105, 108, 109, 111, 112, 114, 115, 116, 117], "parametr": [49, 79, 82, 86, 117], "params_exact": 86, "params_nam": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 49], "parenttoc": 112, "part": [31, 48, 50, 51, 52, 58, 69, 70, 71, 72, 82, 86, 88, 103, 111, 116, 117], "parti": 31, "partial": [14, 15, 22, 28, 29, 30, 31, 34, 50, 52, 54, 64, 70, 71, 78, 81, 84, 86, 88, 99, 100, 102, 104, 108, 109, 110, 111, 112, 114, 116, 117], "partial_": [89, 102], "partiallli": 78, "particip": [19, 73, 78, 117], "particular": [87, 112], "particularli": 70, "partion": [50, 71], "partit": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 50, 71, 81, 84], "partli": 117, "pass": [4, 12, 15, 37, 38, 39, 41, 42, 49, 52, 70, 86, 117], "passo": [112, 114], "past": 50, "paste0": [50, 53], "pastel": 58, "path": [86, 87], "path_to_r": 64, "patsi": [59, 60, 85], "pattern": 79, "paul": 115, "pd": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 23, 25, 26, 28, 29, 30, 31, 32, 35, 39, 56, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 85, 87], "pdf": [58, 74], "pedregosa": [112, 114], "pedregosa11a": [112, 114], "pedro": [49, 115], "penal": [53, 80], "penalti": [51, 52, 57, 72, 79, 80, 86, 87], "pennsylvania": [20, 83, 114], "pension": [51, 72, 73, 117], "peopl": [51, 72, 73], "pep8": 116, "per": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 50, 71], "percent": 86, "percentag": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 22], "perf_count": 69, "perfectli": [77, 87], "perform": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 40, 48, 50, 52, 58, 62, 64, 66, 67, 69, 70, 71, 73, 78, 79, 80, 82, 86, 87, 88, 89, 102, 112, 114, 115, 117], "perfrom": 67, "perhap": 117, "period": [8, 49, 62, 63, 87], "perp": 87, "perrot": [112, 114], "person": 117, "pessimist": 79, "peter": 115, "petra": 116, "petronelaj": 116, "pfister": [52, 86, 112, 114], "phi": [50, 71, 85, 102], "philipp": [79, 112, 115], "philippbach": [112, 116], "pi": [18, 26, 28, 31, 85, 87, 89, 97, 98], "pi_": [29, 50, 71], "pi_0": [89, 97, 98], "pi_i": [53, 80, 87], "pick": [77, 117], "pip": [77, 87], "pip3": 113, "pipe": 52, "pipe_forest_classif": 52, "pipe_forest_regr": 52, "pipe_lasso": 52, "pipelin": [37, 38, 41, 42, 52, 72, 116], "pipeop": 52, "pira": [51, 72, 73, 78, 117], "pivot": [64, 71, 115], "plai": [70, 117], "plan": [19, 51, 72, 73, 117], "plausibl": 79, "pleas": [37, 38, 41, 42, 49, 56, 70, 79, 88, 113], "plim": 74, "pliv": [14, 33, 34, 50, 71, 81, 85, 99, 112, 116], "plm": [84, 86, 102, 103, 108, 117], "plot": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 40, 48, 49, 51, 52, 53, 56, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 72, 73, 74, 76, 77, 78, 79, 80, 85, 103, 108], "plot_tre": [40, 75, 85], "plotli": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 59, 60, 64, 77, 79], "plr": [15, 33, 34, 52, 70, 74, 78, 81, 86, 88, 100, 102, 108, 109, 110, 111, 112, 114, 116, 117], "plr_est": 74, "plr_est1": 74, "plr_est2": 74, "plr_obj": 74, "plr_obj_1": 74, "plr_obj_2": 74, "plr_summari": 72, "plrglmnet": 51, "plrranger": 51, "plrrpart": 51, "plrxgboost8700": 51, "plt": [54, 56, 58, 59, 60, 61, 62, 63, 65, 66, 68, 69, 70, 71, 72, 73, 74, 76, 77, 79, 80], "plt_smpl": [50, 71], "plt_smpls_cluster": [50, 71], "plug": [67, 103, 104, 106, 107, 108, 109], "pm": [35, 50, 71, 102, 103, 108, 111], "pmatrix": [53, 80], "pmlr": [64, 69, 70], "po": [52, 86], "point": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 49, 50, 65, 66, 71, 79, 85, 87, 117], "pointwis": [39, 61, 65, 66, 76], "poli": [51, 68, 71, 72], "polici": [12, 14, 15, 40, 84, 87, 114, 115, 116], "policy_tre": [12, 75, 85], "policy_tree_2": 75, "policy_tree_obj": 85, "policytre": 75, "polit": 74, "poly_dict": 72, "polynomi": [19, 20, 36, 51, 54, 68, 72, 77], "polynomial_featur": [19, 20, 51, 54], "polynomialfeatur": [68, 71, 72], "popul": [79, 89], "popular": [69, 87, 103, 105], "porport": 78, "posit": [31, 51, 74, 79, 117], "posixct": [52, 86], "possibl": [7, 10, 38, 41, 42, 49, 52, 59, 60, 65, 66, 67, 68, 69, 70, 75, 77, 78, 79, 86, 87, 102, 103, 105, 116, 117], "possibli": [103, 105], "post": [28, 31, 87, 102, 115], "postdoubl": 115, "poster": 74, "potenti": [4, 5, 6, 13, 16, 18, 21, 27, 36, 53, 62, 68, 74, 77, 80, 90, 91, 102, 104, 113, 116, 117], "potential_level": 56, "power": [52, 70, 79, 86, 115], "pp": [49, 64, 69, 70], "pq": [13, 16, 17, 73, 101, 116], "pq_0": [73, 76], "pq_1": [73, 76], "pr": [18, 47, 50, 51, 52, 53, 86, 87, 88, 89, 102, 114, 117], "practic": [69, 79, 115], "pre": [49, 53, 62, 80, 86, 87], "precis": [49, 87, 103, 109, 117], "precomput": [38, 42], "pred": [49, 70], "pred_df": 75, "pred_dict": 86, "pred_treat": 75, "predict": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 35, 37, 38, 39, 40, 41, 42, 48, 50, 51, 52, 58, 61, 64, 68, 69, 70, 71, 72, 75, 79, 82, 85, 88, 103, 105, 108, 109, 116, 117], "predict_proba": [4, 6, 8, 9, 11, 12, 13, 15, 16, 17, 18, 35, 37, 41, 70, 86], "predictor": [4, 12, 15, 39, 40, 59, 60, 65, 66, 79, 81], "prefer": [51, 72, 73, 117], "preliminari": [6, 48, 58, 77, 89, 91, 96, 98, 101], "prepar": [49, 50, 71, 116], "preprint": 115, "preprocess": [51, 68, 71, 72, 73, 86], "presenc": [51, 72, 73], "present": [49, 79, 86, 89, 95, 117], "prespecifi": 78, "pretest": 49, "pretreat": [8, 9, 49, 62], "prettenhof": [112, 114], "preval": 79, "prevent": [88, 116], "previou": [63, 67, 68, 74, 113, 117], "previous": [86, 117], "price": [50, 71], "priliminari": [13, 17], "primari": 56, "principl": [103, 105], "print": [35, 48, 49, 50, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 83, 85, 86, 87, 88, 89, 102, 103, 108, 113, 114, 116, 117], "print_detail": 49, "prior": [69, 87], "privat": 116, "prob": 52, "probabilit": 67, "probabl": [4, 6, 11, 12, 13, 16, 17, 18, 23, 27, 41, 48, 49, 53, 56, 58, 62, 67, 74, 76, 77, 79, 80, 82, 87, 89, 92, 93, 96, 115], "problem": [51, 72, 73, 85, 86], "procedur": [48, 50, 51, 58, 69, 71, 72, 78, 79, 86, 102, 113, 116], "proceed": [28, 115], "process": [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 36, 49, 53, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 75, 76, 79, 80, 84, 102, 103, 105, 115, 116], "produc": 74, "product": [59, 60, 64, 69, 79, 103, 111], "producton": 50, "program": [26, 51, 72, 73, 115, 117], "progress": 55, "project": [52, 59, 60, 85, 112, 116], "project_z": [59, 60], "prone": 89, "pronounc": 77, "propens": [13, 17, 21, 22, 51, 53, 62, 67, 68, 69, 72, 73, 79, 80, 85, 87, 103, 104], "properli": [70, 117], "properti": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 51, 52, 69, 72, 73, 74, 78, 86, 87, 103, 108, 114, 116], "proport": [78, 103, 105, 110, 111], "propos": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 50, 52, 71, 77, 103, 105, 115, 116], "provid": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 35, 37, 38, 41, 42, 49, 50, 51, 52, 59, 60, 65, 66, 68, 70, 71, 72, 77, 79, 81, 82, 83, 84, 86, 102, 112, 114, 116, 117], "prune": [12, 40], "ps911c": 71, "ps944": 71, "pscore1": 74, "pscore2": 74, "psi": [33, 34, 48, 49, 50, 71, 81, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 111, 114], "psi_": [102, 103, 108, 110, 111], "psi_a": [11, 12, 14, 15, 33, 48, 50, 58, 71, 88, 89, 90, 92, 93, 94, 95, 99, 100, 102], "psi_b": [11, 12, 14, 15, 33, 48, 58, 85, 88, 89, 90, 92, 93, 94, 95, 99, 100], "psi_el": [88, 89], "psi_j": 102, "psi_nu2": [103, 108], "psi_sigma2": [103, 108], "public": [47, 57, 116], "publish": [79, 116], "pull": [51, 116], "purchas": 79, "pure": 79, "purp": [59, 60], "purpos": [48, 58, 67, 78, 79, 103, 105, 114], "pval": 102, "px": [64, 77], "py": [66, 71, 72, 79, 112, 113, 116], "py3": 113, "py_al": 58, "py_dml": 58, "py_dml_nosplit": 58, "py_dml_po": 58, "py_dml_po_nosplit": 58, "py_double_ml_apo": 56, "py_double_ml_bas": 58, "py_double_ml_basic_iv": 57, "py_double_ml_c": 59, "py_double_ml_cate_plr": 60, "py_double_ml_cvar": 61, "py_double_ml_did": 62, "py_double_ml_did_pretest": 63, "py_double_ml_firststag": 64, "py_double_ml_g": 65, "py_double_ml_gate_plr": 66, "py_double_ml_gate_sensit": 67, "py_double_ml_irm_vs_apo": 68, "py_double_ml_learn": 69, "py_double_ml_meets_flaml": 70, "py_double_ml_multiway_clust": 71, "py_double_ml_pens": 72, "py_double_ml_pension_qt": 73, "py_double_ml_plm_irm_hetfx": 74, "py_double_ml_policy_tre": 75, "py_double_ml_pq": 76, "py_double_ml_rdflex": 77, "py_double_ml_sensit": 78, "py_double_ml_sensitivity_book": 79, "py_double_ml_ssm": 80, "py_non_orthogon": 58, "py_po_al": 58, "pydata": 66, "pypi": [115, 116], "pyplot": [54, 56, 58, 59, 60, 61, 62, 63, 65, 66, 68, 69, 70, 71, 72, 73, 74, 76, 77, 79, 80], "pyproject": 116, "python": [31, 49, 70, 79, 81, 82, 83, 84, 85, 87, 88, 89, 102, 103, 105, 108, 112, 114, 115, 116, 117], "python3": [72, 113], "q": [52, 61, 76, 77, 86, 112, 114], "q2": [52, 54, 83, 114], "q3": [52, 54, 83, 114], "q4": [52, 54, 83, 114], "q5": [52, 54, 83, 114], "q6": [52, 54, 83, 114], "q_i": [77, 87], "qquad": 26, "qte": [61, 73, 116], "quad": [23, 51, 53, 62, 72, 75, 77, 80, 85, 87, 89, 96, 102, 103, 106], "quadrat": [53, 80], "qualiti": [78, 81, 116], "quanitl": 73, "quant": 61, "quantifi": 79, "quantil": [5, 6, 13, 16, 17, 27, 56, 61, 68, 78, 84, 86, 91, 96, 101, 115, 116], "quantiti": [47, 57, 79], "queri": 72, "question": [79, 117], "quick": 73, "quit": [69, 75, 78, 103, 105], "r": [11, 25, 37, 38, 41, 42, 58, 59, 60, 63, 64, 71, 74, 77, 79, 81, 82, 83, 84, 87, 88, 89, 94, 99, 102, 103, 104, 105, 109, 110, 111, 112, 114, 115, 116, 117], "r2_d": [26, 69], "r2_score": [38, 42], "r2_y": [26, 69], "r6": [52, 116], "r_0": [11, 14, 51, 72, 87], "r_all": 48, "r_d": 26, "r_df": 71, "r_dml": 48, "r_dml_nosplit": 48, "r_dml_po": 48, "r_dml_po_nosplit": 48, "r_double_ml_bas": 48, "r_double_ml_basic_iv": 47, "r_double_ml_did": 49, "r_double_ml_multiway_clust": 50, "r_double_ml_pens": 51, "r_double_ml_pipelin": 52, "r_double_ml_ssm": 53, "r_hat": 14, "r_hat0": 11, "r_hat1": 11, "r_non_orthogon": 48, "r_po_al": 48, "r_y": 26, "rais": [7, 10, 37, 38, 41, 42, 86], "randint": 74, "randn": 18, "random": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 23, 24, 27, 31, 32, 35, 36, 47, 48, 49, 51, 52, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 85, 86, 88, 97, 102, 103, 108, 111, 114, 115, 117], "random_search": 86, "random_st": [27, 58, 67, 68, 75], "randomforest": [51, 69, 72], "randomforest_class": [51, 59, 72, 75], "randomforest_reg": [59, 75], "randomforestclassifi": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 54, 56, 59, 60, 65, 66, 67, 69, 72, 75, 77, 78, 79, 85, 86, 87, 117], "randomforestregressor": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 54, 56, 58, 59, 60, 65, 66, 67, 69, 72, 75, 77, 78, 79, 81, 85, 86, 87, 88, 89, 102, 103, 108, 114, 117], "randomized_search": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 86], "randomizedsearchcv": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 86], "randomli": [48, 50, 58, 71, 82, 88, 117], "rang": [48, 56, 58, 61, 62, 63, 65, 66, 69, 70, 71, 73, 75, 76, 77, 79, 80, 82, 86, 87], "rangeindex": [54, 56, 62, 67, 71, 72, 73, 78, 80, 83, 114], "ranger": [49, 51, 52, 81, 86, 87, 88, 89, 102, 114, 117], "rangl": [24, 75], "rank": 116, "rate": [64, 69, 87], "rather": [77, 79, 87], "ratio": [86, 88, 103, 105], "ravel": [59, 60], "raw": [51, 64, 72], "raw_res_manual_lasso_r_100_n_100_p_200_rho_0": 64, "rbind": 51, "rbindlist": 51, "rbinom": 47, "rbrace": [11, 12, 25, 26, 32, 50, 71, 81, 87, 88, 89, 90, 102, 103, 104], "rcolorbrew": 50, "rcparam": [54, 59, 60, 61, 63, 65, 66, 68, 71, 72, 73, 76], "rd": [87, 116], "rdbu": 50, "rdbu_r": 71, "rdbwselect": 87, "rdd": [0, 7, 10, 84, 113], "rdflex": [0, 77, 87, 116], "rdflex_fuzzi": 77, "rdflex_fuzzy_stack": 77, "rdflex_obj": [35, 87], "rdflex_sharp": 77, "rdflex_sharp_stack": 77, "rdrobust": [35, 77, 87, 113, 116], "rdrobust_fuzzi": 77, "rdrobust_fuzzy_noadj": 77, "rdrobust_sharp": 77, "rdrobust_sharp_noadj": 77, "rdt044": 64, "re": [71, 79, 113], "read": 113, "read_csv": 64, "readabl": 116, "readili": 112, "real": [51, 72, 73, 78, 103, 105], "realat": 87, "realiz": [77, 87], "reason": [7, 10, 47, 57, 64, 69, 70, 78, 79, 103, 105, 117], "recal": [54, 103, 111], "receiv": [56, 77, 87], "recent": [70, 87], "recogn": [51, 72, 73], "recommend": [52, 69, 77, 79, 81, 88, 113, 115, 116], "recov": [47, 49, 57, 74], "recsi": 115, "red": [50, 53, 65, 66, 70, 71], "reduc": [51, 67, 70, 72, 77, 78, 79, 87, 116], "redund": 116, "reemploy": [20, 83, 114], "refactor": 116, "refer": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 25, 26, 28, 29, 30, 31, 32, 51, 56, 63, 67, 72, 73, 77, 78, 83, 84, 85, 87, 103, 105, 108, 115, 116], "reference_level": [5, 56, 68, 87], "refin": 116, "refit": [103, 105], "reflect": [75, 79, 85], "reg": [23, 51, 72, 117], "reg_estim": 77, "reg_learn": 73, "reg_learner_1": 69, "reg_learner_2": 69, "regard": [79, 112], "regener": 116, "region": [50, 61, 71, 102, 115], "regr": [47, 48, 49, 50, 51, 52, 53, 81, 86, 87, 88, 89, 102, 114, 117], "regravg": [52, 86], "regress": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 21, 22, 25, 26, 27, 28, 29, 30, 31, 35, 36, 39, 47, 49, 50, 52, 53, 56, 57, 64, 70, 71, 74, 78, 79, 80, 81, 82, 84, 85, 86, 88, 102, 104, 105, 108, 109, 110, 111, 112, 114, 115, 116, 117], "regressor": [38, 42, 48, 51, 56, 58, 61, 69, 70, 72, 82], "regular": [28, 84, 86, 89, 102, 115], "reich": [52, 86], "reinforc": 115, "reject": [51, 72], "rel": [51, 72, 103, 104, 105, 109], "relat": [68, 79, 117], "relationship": [47, 57, 64, 79, 102], "relev": [7, 8, 9, 10, 24, 37, 38, 39, 41, 42, 61, 75, 76, 87, 103, 117], "reli": [59, 60, 62, 63, 67, 68, 85, 86, 87, 103, 105, 117], "reload": 51, "remain": [49, 102, 117], "remark": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 48, 56, 58, 59, 60, 61, 63, 65, 66, 67, 68, 69, 73, 78, 85, 86, 87, 89, 92, 93, 96, 101, 102, 103, 109], "remot": 113, "remov": [51, 68, 79, 84, 88, 116], "renam": [72, 116], "render": [78, 79], "reorgan": 116, "rep": [48, 53, 82, 86, 102], "repeat": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 47, 48, 50, 51, 52, 53, 58, 67, 71, 72, 73, 74, 77, 78, 80, 82, 84, 86, 102, 106, 114, 116, 117], "repeatedkfold": 71, "repet": 78, "repetit": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 39, 59, 60, 64, 65, 66, 67, 69, 84, 86, 102, 114, 117], "repetiton": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35], "replac": [75, 79, 116], "replic": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 48, 51, 58, 64, 79], "repo": 116, "report": [51, 70, 72, 112, 116], "repositori": [64, 77, 116], "repr": [48, 50], "repres": [74, 79, 87], "represent": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 22, 78, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 114, 116], "reproduc": 27, "request": [37, 38, 41, 42, 116], "requir": [14, 15, 37, 41, 47, 51, 52, 56, 67, 72, 73, 78, 87, 102, 103, 105, 108, 113, 116, 117], "requirenamespac": 49, "res_df": 71, "res_dict": [21, 22, 24, 27, 36], "resampl": [47, 50, 52, 53, 62, 71, 73, 78, 80, 86, 87, 88, 89, 102, 112, 114, 117], "research": [50, 52, 71, 74, 79, 88, 112, 114, 115, 117], "resembl": [53, 80], "reset": 49, "reset_index": [64, 71, 72], "reshap": [58, 59, 60, 63, 68], "reshape2": 50, "residu": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 22, 38, 42, 78, 103, 105, 110, 111], "resolut": [52, 86], "resourc": 69, "resourcewis": 69, "respect": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 51, 56, 72, 73, 77, 85, 87, 88, 103, 111, 117], "respons": [19, 52, 86], "rest": 87, "restart": 113, "restrict": 69, "restructur": 116, "restud": 64, "result": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 47, 48, 49, 52, 53, 56, 58, 59, 60, 62, 63, 64, 67, 68, 69, 75, 77, 78, 79, 80, 82, 86, 88, 89, 92, 93, 103, 105, 108, 114, 116], "result_iivm": 51, "result_irm": 51, "result_plr": 51, "retain": [37, 38, 41, 42], "retina": 74, "retir": [51, 72, 73, 78], "return": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 48, 49, 50, 52, 53, 58, 61, 66, 69, 70, 71, 74, 75, 76, 78, 79, 80, 81, 86, 89, 103, 105, 116], "return_count": [56, 69], "return_tune_r": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18], "return_typ": [4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 23, 25, 26, 28, 29, 30, 31, 32, 48, 51, 52, 53, 58, 62, 68, 69, 70, 72, 73, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 102, 103, 108, 114, 117], "rev": 50, "reveal": 67, "review": [28, 64, 115], "revist": [50, 71], "rf": 77, "rho": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 43, 56, 67, 68, 77, 78, 79, 103, 105, 108, 111, 117], "rho_val": 79, "richter": [52, 86, 112, 114], "riesz": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 22, 78, 103, 105, 106, 107, 108, 110, 111], "riesz_rep": [103, 108], "right": [25, 26, 28, 29, 32, 48, 50, 58, 69, 71, 72, 73, 74, 76, 77, 79, 82, 87, 89, 92, 93, 102, 103, 104, 106, 107, 109], "rightarrow_": [48, 58, 82], "risk": [6, 84, 116], "ritov": 115, "rival": 71, "rival_ind": 71, "rmd": 49, "rmse": [49, 62, 69, 70, 73, 78, 80, 86, 87, 89, 102, 114, 116], "rmse_dml_ml_l_fullsampl": 70, "rmse_dml_ml_l_lesstim": 70, "rmse_dml_ml_l_onfold": 70, "rmse_dml_ml_l_untun": 70, "rmse_dml_ml_m_fullsampl": 70, "rmse_dml_ml_m_lesstim": 70, "rmse_dml_ml_m_onfold": 70, "rmse_dml_ml_m_untun": 70, "rmse_oos_ml_l": 70, "rmse_oos_ml_m": 70, "rmse_oos_onfolds_ml_l": 70, "rmse_oos_onfolds_ml_m": 70, "rnorm": [47, 52, 83, 86, 102, 114], "robin": [19, 20, 30, 50, 64, 71, 82, 112, 115], "robinson": [48, 58, 82], "robject": 71, "robu": [65, 66], "robust": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 21, 22, 23, 29, 35, 49, 56, 67, 68, 77, 78, 79, 87, 103, 108, 115, 117], "roc\u00edo": 115, "role": [7, 10, 48, 58, 70, 82, 117], "romano": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 102], "root": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 34, 64, 82, 86, 89, 115], "rotat": [70, 77], "roth": [77, 87, 115], "rough": [79, 117], "roughli": 79, "round": [51, 56, 68, 69, 74, 79], "rout": [37, 38, 41, 42], "row": [48, 51, 54, 59, 60, 63, 70, 71, 75, 83, 88, 114, 117], "row_index": 66, "rownam": 50, "rowv": 50, "roxygen2": 116, "royal": [79, 115], "rpart": [51, 52, 86], "rpart_cv": 52, "rprocess": 69, "rpy2": 71, "rpy2pi": 71, "rskf": 68, "rsmp": [52, 86, 88], "rsmp_tune": [52, 86], "rssb": 79, "rtype": 5, "ruben": 115, "ruiz": [47, 57], "rule": [49, 85], "run": [49, 77, 87, 113, 116], "runif": 47, "runner": 79, "runtime_learn": 52, "rv": [56, 67, 68, 78, 79, 103, 108, 117], "rva": [56, 67, 68, 78, 79, 103, 108, 117], "rvert": 64, "rvert_": 64, "s1": 70, "s2": 70, "s_": [29, 50, 71, 87], "s_1": 30, "s_2": 30, "s_col": [7, 10, 53, 77, 80, 87], "s_i": [32, 53, 77, 80, 87], "s_x": [29, 50, 71], "safeguard": [62, 86], "sake": [51, 72, 79, 117], "same": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 27, 39, 48, 50, 53, 58, 59, 60, 67, 68, 69, 71, 73, 75, 77, 78, 79, 80, 86, 89, 92, 93, 102, 103, 109, 116], "samii": 74, "sampl": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 29, 32, 35, 37, 38, 41, 42, 47, 49, 50, 52, 57, 62, 65, 66, 68, 69, 71, 73, 75, 78, 84, 86, 102, 114, 115, 116], "sample_weight": [35, 37, 38, 41, 42, 77], "sant": [8, 9, 21, 22, 23, 27, 49, 62, 87, 115], "sara": 115, "sasaki": [29, 50, 71, 115], "satisfi": [53, 80, 86, 89, 102], "save": [48, 51, 58, 65, 66, 69, 70, 72, 73, 86, 103, 108, 117], "savefig": 58, "saveguard": 69, "saver": [51, 72, 73], "scalar": 87, "scale": [48, 50, 61, 63, 68, 74, 76, 79, 102, 103, 111], "scale_color_manu": 48, "scale_fill_manu": [48, 50], "scaled_psi": 68, "scatter": [56, 63, 65, 66, 74, 77, 79], "scatterplot": 56, "scenario": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 56, 67, 68, 78, 79, 87, 103, 108, 117], "scene": [59, 60, 64], "scene_camera": 64, "schacht": [64, 69, 70], "schaefer": 74, "schedul": 116, "scheme": [50, 71, 86, 88, 112], "schneider": 52, "schratz": [52, 86, 112, 114], "scienc": [31, 47, 57, 74, 115], "scikit": [69, 72, 86, 112, 114, 116, 117], "scipi": 58, "score": [0, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 33, 34, 35, 36, 37, 38, 41, 42, 47, 49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 64, 67, 68, 69, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 106, 107, 108, 109, 110, 111, 112, 116, 117], "scoring_method": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18], "script": 113, "sd": 47, "se": [48, 50, 58, 78, 82, 86, 88, 102, 103, 108, 115, 117], "se_df": 50, "se_dml": [48, 58, 82], "se_dml_po": [48, 58, 82], "se_nonorth": [48, 58], "se_orth_nosplit": [48, 58], "se_orth_po_nosplit": [48, 58], "seaborn": [54, 56, 58, 62, 69, 71, 72, 73, 79, 80], "search": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 34, 86, 89], "search_mod": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 86], "searchabl": 51, "second": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 29, 48, 50, 52, 58, 69, 70, 71, 81, 82, 88, 102, 103, 105, 111, 114], "secondari": 56, "section": [9, 23, 49, 50, 51, 52, 67, 70, 71, 73, 79, 95, 106, 116], "secur": 74, "see": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 19, 20, 26, 32, 33, 34, 37, 38, 39, 41, 42, 47, 49, 50, 51, 52, 56, 57, 59, 60, 62, 66, 68, 70, 71, 73, 74, 75, 77, 78, 79, 86, 87, 88, 89, 91, 95, 96, 97, 98, 101, 103, 105, 108, 111, 113, 114, 116], "seed": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 27, 35, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 102, 103, 108, 114, 117], "seek": 74, "seem": [49, 51, 67, 72, 73, 117], "seen": [65, 66, 68], "sel_cols_chiang": 71, "select": [7, 10, 18, 27, 28, 32, 47, 64, 69, 77, 79, 81, 83, 84, 86, 102, 114, 115, 116, 117], "selected_coef": 69, "selected_featur": [52, 86], "selected_learn": 69, "self": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 37, 38, 39, 40, 41, 42, 69, 70, 117], "selfref": 51, "semenova": [59, 60, 115], "semi": 82, "semiparametr": 19, "sens": [78, 79], "sensemakr": [103, 105], "sensit": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 43, 84, 85, 105, 108, 111, 116], "sensitivity_analysi": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 56, 67, 68, 78, 79, 103, 108, 117], "sensitivity_benchmark": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 56, 67, 78, 79, 103, 105], "sensitivity_el": [103, 108], "sensitivity_param": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 78, 79, 103, 105, 108], "sensitivity_plot": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 56, 67, 78, 79, 103, 108], "sensitivity_summari": [56, 67, 68, 78, 79, 103, 108, 117], "sensitv": 68, "sensitvity_benchmark": 56, "sensiv": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18], "senstiv": [103, 110], "sep": 48, "separ": [74, 78, 86, 87, 116], "seper": [70, 77, 78, 88, 102, 103, 105], "seq_len": [48, 53, 82], "sequenti": 20, "seri": [66, 79, 115], "serv": [83, 114, 116], "serverless": [115, 116], "servic": 74, "set": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 29, 30, 31, 37, 40, 41, 42, 47, 48, 49, 50, 51, 53, 54, 56, 57, 58, 59, 60, 62, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 85, 87, 88, 89, 90, 92, 93, 95, 102, 103, 104, 105, 109, 110, 113, 114, 116, 117], "set_as_param": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18], "set_config": [37, 38, 41, 42], "set_fit_request": [41, 42], "set_fold_specif": 86, "set_index": 72, "set_ml_nuisance_param": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 51, 54, 72, 86, 116], "set_param": [37, 38, 41, 42, 70, 86], "set_sample_split": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 68, 69, 88, 116], "set_score_request": [37, 38, 41, 42], "set_styl": [72, 73], "set_text": 69, "set_threshold": [48, 49, 50, 51, 52, 53, 81, 86, 87, 88, 89, 102, 114], "set_tick": 71, "set_ticklabel": 71, "set_titl": [56, 68, 70, 71, 77], "set_x_d": [7, 10], "set_xlabel": [56, 58, 68, 70, 71, 77], "set_xlim": 58, "set_xtick": 74, "set_xticklabel": 74, "set_ylabel": [56, 68, 70, 71, 74, 77], "set_ylim": [61, 68, 70, 71, 76], "setdiff": 116, "setdiff1d": 71, "setminu": [50, 71, 102], "settings_l": 70, "settings_m": 70, "setup": [113, 116], "seven": [50, 71], "sever": [43, 51, 52, 69, 70, 72, 73, 78, 79, 82, 86, 117], "shape": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 37, 38, 39, 40, 41, 42, 56, 59, 60, 63, 65, 66, 69, 71, 72, 75, 77, 78, 79, 86, 87], "share": [50, 51, 71, 72], "sharma": [79, 115], "sharp": 35, "shock": [50, 71], "short": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 21, 22, 78, 79, 103, 105, 115, 116, 117], "shortcut": 51, "shortli": [50, 52, 71, 86], "shota": 115, "should": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 34, 37, 38, 41, 42, 51, 53, 56, 65, 66, 69, 72, 77, 78, 80, 83, 85, 86, 87, 102, 103, 105, 112], "show": [47, 48, 50, 53, 54, 56, 57, 58, 59, 60, 62, 64, 67, 68, 69, 70, 71, 74, 77, 79, 80, 82, 103, 110, 113], "showcas": 75, "showlabel": 79, "showlegend": 79, "shown": [47, 57, 74, 114], "showscal": [59, 60, 64], "shrink": 77, "shuffl": 88, "side": [77, 87, 103, 108], "sigma": [18, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 48, 50, 53, 58, 71, 80, 82, 85, 88, 102, 103, 105, 108, 110, 111], "sigma2": [103, 108], "sigma_": [22, 23, 25, 26, 28, 29, 30, 32, 48, 50, 58, 71, 82], "sigma_0": [103, 111], "sigma_j": 102, "sigmoid": 74, "sign": 79, "signal": [39, 40], "signatur": [11, 12, 13, 14, 15, 16, 17, 89], "signif": [47, 49, 50, 51, 52, 53, 86, 87, 88, 89, 102, 114, 117], "signific": [47, 50, 51, 52, 53, 56, 67, 68, 72, 75, 77, 78, 79, 86, 87, 88, 89, 102, 103, 108, 114, 117], "silverman": [13, 16, 17], "sim": [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 36, 48, 49, 50, 53, 58, 61, 63, 71, 75, 76, 80, 82, 87], "similar": [22, 27, 49, 52, 59, 60, 67, 70, 73, 77, 78, 79, 87], "similarli": 70, "simpl": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 24, 41, 42, 49, 52, 59, 60, 65, 66, 67, 68, 75, 79, 84, 87, 103, 105], "simplest": 85, "simpli": [52, 62, 117], "simplic": [51, 69, 72, 75, 79], "simplif": [103, 106], "simplifi": [68, 74, 79, 85, 103, 110], "simul": [21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 48, 52, 53, 58, 59, 60, 61, 64, 65, 66, 69, 70, 76, 77, 79, 80, 82, 86, 102, 114], "simul_data": 18, "simulaten": 87, "simulation_run": 64, "simult": 49, "simultan": [84, 117], "sin": [24, 27, 31, 59, 60, 63, 65, 66], "sinc": [21, 22, 37, 41, 51, 53, 56, 62, 63, 65, 66, 67, 69, 70, 72, 74, 80, 86, 87, 103, 108, 109, 113, 116], "singl": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 62, 65, 66, 73, 74, 86, 102], "single_learner_pipelin": 86, "singleton": 88, "sinh": 31, "sipp": [51, 72, 73], "site": [71, 72], "situat": [50, 71], "six": 50, "sixth": 71, "size": [18, 48, 50, 51, 52, 58, 61, 63, 64, 67, 69, 70, 72, 74, 75, 76, 79, 81, 83, 86, 87, 88, 89, 102, 114, 117], "sizeabl": 79, "skill": 115, "sklearn": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 31, 35, 37, 38, 40, 41, 42, 54, 56, 57, 58, 59, 60, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 77, 78, 79, 80, 81, 85, 86, 87, 88, 89, 102, 103, 108, 114, 117], "skotara": 79, "slide": 74, "slightli": [63, 65, 66, 67, 69, 85, 89, 92, 93, 103, 105], "sligthli": [8, 9], "slow": [48, 58, 82], "slower": [48, 58, 82], "small": [24, 53, 62, 63, 68, 75, 80, 87, 103, 105, 109], "smaller": [51, 62, 65, 66, 67, 70, 72, 77, 79, 87, 117], "smallest": 69, "smpl": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 48, 50, 58, 69, 71, 88, 89], "smpls_cluster": [50, 71], "sn": [54, 56, 58, 62, 69, 71, 72, 73, 79, 80], "so": [41, 42, 47, 51, 52, 53, 57, 62, 70, 72, 74, 79, 80, 86, 102, 117], "social": [74, 115], "societi": [50, 71, 79, 115], "softwar": [52, 86, 112, 114, 115, 116], "solari": 116, "sole": 79, "solut": [81, 85, 89], "solv": [33, 50, 71, 85, 86, 102], "solver": [72, 80, 87], "some": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 37, 38, 41, 42, 51, 52, 53, 54, 62, 63, 69, 70, 72, 73, 77, 78, 80, 85, 86, 87, 113, 116], "sometim": 69, "sonabend": [52, 86], "sophist": 86, "sort": [72, 87], "sort_valu": 56, "sourc": [52, 86, 114, 116], "sourcefileload": 64, "sp": 49, "space": [50, 71, 86], "spars": [64, 86, 102, 114, 115], "sparsiti": 115, "spec": 115, "special": [50, 71, 87], "specif": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 34, 35, 50, 51, 56, 68, 69, 71, 72, 79, 83, 84, 85, 86, 87, 88, 89, 95, 102, 108, 111, 112, 114], "specifi": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 35, 47, 50, 51, 52, 53, 56, 57, 59, 60, 61, 62, 65, 66, 68, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 87, 90, 95, 113, 114, 116, 117], "specifii": 73, "speed": [5, 17, 69], "speedup": 69, "spefici": 11, "spindler": [28, 64, 69, 70, 79, 112, 115, 116], "spine": [72, 73], "spline": [59, 60, 85], "spline_basi": [59, 60, 85], "spline_grid": [59, 60], "split": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 47, 50, 52, 53, 62, 68, 69, 71, 73, 75, 78, 80, 84, 85, 86, 87, 89, 102, 114, 116], "split_sampl": [68, 69], "sponsor": [51, 72, 73], "sprintf": 48, "sq_error": 64, "sqrt": [21, 22, 23, 26, 27, 48, 50, 52, 54, 58, 61, 71, 76, 82, 88, 102, 103, 105, 114], "squar": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 38, 42, 51, 64, 72, 86, 87, 103, 111, 115], "squarederror": [51, 72, 117], "squeez": [61, 62, 76, 80], "src": 72, "ssm": [7, 10, 32, 84], "ssrn": 25, "stabil": 67, "stabl": [47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 112], "stack": [52, 86], "stackingclassifi": 77, "stackingregressor": 77, "stacklrn": 52, "stackrel": 87, "stage": [35, 59, 60, 65, 66, 75, 77, 86, 87, 116, 117], "standard": [23, 49, 52, 61, 65, 66, 77, 87, 88, 89, 102, 103, 108, 111, 116, 117], "standard_norm": [83, 86, 102, 114], "standardscal": 72, "star": 87, "start": [49, 51, 52, 59, 60, 64, 67, 69, 70, 71, 72, 76, 79, 87, 112, 117], "stat": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 35, 58, 77, 83, 86, 87, 102, 112, 115], "stat_bin": 48, "stat_dens": 51, "state": 117, "stationar": 62, "stationari": 87, "statist": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 29, 32, 43, 50, 71, 78, 79, 102, 103, 108, 112, 114, 115, 116, 117], "statsmodel": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 39, 77], "statu": [49, 51, 53, 62, 72, 74, 77, 80], "std": [6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 47, 49, 50, 51, 52, 53, 54, 56, 57, 59, 60, 61, 62, 65, 66, 67, 68, 70, 71, 72, 73, 74, 76, 78, 79, 80, 85, 86, 87, 88, 89, 102, 114, 117], "stefan": 115, "step": [48, 51, 52, 58, 65, 66, 67, 72, 75, 82, 86, 87, 102, 112, 117], "stepdown": 102, "stick": [51, 72], "still": [53, 59, 60, 62, 65, 66, 67, 73, 77, 78, 80, 86], "stochast": [14, 15, 87, 114], "stock": [51, 72, 73], "store": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 81, 86, 88, 89, 102, 103, 108, 116], "store_model": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 70], "store_predict": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 49, 72, 75], "stori": [79, 115], "str": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 35, 37, 38, 39, 41, 42, 51, 56, 65, 66, 76, 77, 85, 87, 116], "straightforward": [65, 66, 69, 85], "strategi": [74, 79, 87, 117], "stratifi": [68, 69], "stratum": 74, "strength": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 21, 22, 78, 79, 103, 105, 108, 110], "strictli": 87, "string": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 85, 102, 103, 108, 114, 116], "string_label": 74, "strong": [53, 80, 103, 105], "stronger": [102, 117], "structur": [19, 20, 30, 50, 51, 53, 64, 71, 72, 80, 82, 86, 112, 115, 117], "student": 115, "studi": [32, 50, 51, 64, 69, 70, 71, 72, 73, 78, 114, 117], "style": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 70, 116], "styler": 116, "styliz": 79, "sub": [37, 38, 41, 42, 50, 71], "subclass": 116, "subfold": 86, "subgroup": [11, 51, 72, 116], "subject": [50, 71], "submiss": 116, "subobject": [37, 38, 41, 42], "subplot": [50, 56, 58, 59, 60, 61, 63, 65, 66, 68, 69, 70, 71, 72, 73, 74, 76, 77], "subplots_adjust": 69, "subpopul": 87, "subsampl": [52, 69], "subscript": [103, 105], "subsequ": [50, 71], "subset": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 37, 41, 50, 69, 71, 75, 81, 85, 86, 103, 105], "subseteq": 85, "substanti": [51, 72, 74], "substract": 102, "subtract": 102, "sudo": 113, "suffic": 79, "suffici": [69, 70, 79], "suggest": [50, 51, 71, 72, 79, 116], "suitabl": [53, 59, 60, 80], "sum": [38, 42, 50, 51, 71, 72, 73, 76, 77, 85, 102], "sum_": [36, 48, 50, 58, 71, 77, 81, 82, 85, 87, 102], "sum_i": 74, "sum_oth": 71, "sum_riv": 71, "summar": [49, 56, 74, 79, 81, 103, 108], "summari": [6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 47, 49, 50, 52, 53, 54, 56, 57, 59, 60, 61, 62, 65, 66, 67, 68, 71, 73, 76, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, 102, 103, 114, 116, 117], "summary_result": 51, "suppli": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 59, 60, 65, 66, 67, 75, 85, 103, 104, 105, 108], "support": [11, 24, 35, 49, 50, 69, 71, 75, 77, 86, 87, 117], "support_s": [24, 59, 60, 65, 66, 75], "support_t": 75, "support_w": 75, "suppos": 79, "suppress": [49, 51, 52, 53], "suppresswarn": 48, "suprema": 102, "suptitl": [61, 69, 70, 73, 76], "supxlabel": [61, 73, 76], "supylabel": [61, 73, 76], "sure": [56, 86, 116], "surfac": [59, 60, 64], "surpress": [50, 114], "survei": [51, 72, 73, 117], "susan": 115, "sven": [79, 112, 115], "svenk": 71, "svenklaassen": [112, 116], "svg": [48, 58], "switch": [48, 58, 79, 82], "symbol": 79, "symmetr": 31, "syntax": [77, 87], "synthet": [24, 36, 47, 57, 59, 60, 61, 65, 66, 70, 75, 76], "syrgkani": [79, 115], "system": 115, "szita": 115, "t": [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 27, 35, 37, 38, 41, 42, 47, 49, 50, 51, 52, 53, 54, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 93, 102, 103, 106, 114, 117], "t_1_start": 69, "t_1_stop": 69, "t_2_start": 69, "t_2_stop": 69, "t_3_start": 69, "t_3_stop": 69, "t_col": [7, 9, 10, 87], "t_df": 75, "t_diff": 63, "t_dml": 48, "t_i": [62, 75, 77, 87], "t_idx": 63, "t_nonorth": 48, "t_orth_nosplit": 48, "t_sigmoid": 75, "t_stat": 102, "tabl": [48, 50, 51, 52, 53, 56, 81, 83, 86, 87, 88, 89, 102, 114, 117], "tabular": [69, 83, 102, 114, 117], "taddi": 115, "take": [11, 12, 14, 15, 21, 22, 24, 53, 59, 60, 61, 62, 63, 64, 65, 66, 69, 73, 76, 77, 78, 80, 81, 85, 86, 87, 89, 90, 95, 103, 104, 109, 110, 114], "taken": [51, 72, 73, 117], "taker": [11, 116], "talk": 117, "target": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 33, 34, 41, 42, 47, 50, 51, 52, 53, 59, 60, 69, 71, 85, 86, 87, 88, 89, 96, 101, 102, 103, 109, 111, 112, 114, 116, 117], "task": [47, 70, 83, 88, 117], "task_typ": 116, "tau": [36, 61, 63, 73, 74, 76, 77, 85, 87, 89, 91, 96, 101], "tau_": [74, 77, 87], "tau_0": [77, 87], "tau_1": 74, "tau_2": 74, "tau_vec": [61, 73, 76], "tax": [51, 72, 73], "te": [49, 59, 60, 75], "techniqu": [48, 58, 82, 88, 117], "templat": 116, "ten": 70, "tend": [51, 72, 73, 87], "tensor": [59, 60], "tenth": 115, "term": [48, 50, 51, 52, 58, 63, 64, 71, 72, 74, 79, 82, 87, 112, 117], "termin": [52, 86], "terminatorev": 52, "test": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 20, 25, 37, 38, 41, 42, 47, 48, 49, 50, 51, 52, 53, 58, 67, 71, 79, 82, 86, 87, 88, 89, 102, 114, 115, 116, 117], "test_id": [50, 88], "test_ind": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18], "test_set": 88, "test_siz": 58, "text": [21, 22, 23, 25, 27, 35, 36, 50, 51, 61, 64, 74, 75, 76, 77, 79, 85, 87, 88], "textbf": [81, 86, 117], "textposit": 79, "textrm": [103, 104, 105, 109, 110, 111], "tg": [52, 54, 83, 114], "th": [50, 71], "than": [12, 48, 49, 51, 58, 64, 68, 69, 72, 73, 74, 77, 78, 79, 82, 87, 103, 108, 117], "thank": [49, 51, 52, 72, 116], "thatw": 63, "thei": [49, 51, 63, 65, 66, 72, 74, 87, 103, 111], "them": [51, 52, 59, 60, 61, 67, 70, 72, 76, 87], "theme": [50, 51], "theme_minim": [48, 51, 53], "theorem": [103, 111], "theoret": [69, 79, 88, 115], "theori": [85, 115], "therebi": [50, 52, 71, 117], "therefor": [56, 74, 77, 78, 88, 89, 103, 110], "theta": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 25, 26, 27, 29, 31, 32, 33, 34, 48, 50, 52, 53, 56, 58, 62, 63, 64, 67, 68, 69, 71, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 105, 108, 110, 111, 114, 117], "theta_": [56, 77, 79, 85, 87, 102, 103, 111], "theta_0": [11, 12, 14, 15, 24, 48, 50, 51, 53, 56, 58, 59, 60, 64, 65, 66, 71, 72, 79, 80, 82, 85, 87, 89, 96, 101, 102, 103, 104, 109, 111, 114], "theta_dml": [48, 58, 82], "theta_dml_po": [48, 58, 82], "theta_initi": 58, "theta_nonorth": [48, 58], "theta_orth_nosplit": [48, 58], "theta_orth_po_nosplit": [48, 58], "theta_resc": 48, "theta_t": 63, "thi": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 34, 37, 38, 40, 41, 42, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 85, 86, 87, 88, 89, 91, 92, 93, 95, 96, 101, 102, 103, 104, 105, 108, 109, 112, 113, 114, 115, 116, 117], "think": 52, "third": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 48, 58, 71, 82, 88], "thirion": [112, 114], "this_df": [64, 72], "this_split_ind": 71, "those": [49, 51, 72, 73], "though": [47, 57, 74], "thread": [74, 86], "three": [50, 52, 65, 66, 113, 116], "threshold": [4, 6, 8, 9, 11, 12, 13, 16, 17, 18, 77, 79, 87], "through": [49, 61, 65, 66, 76, 77, 86, 87], "throughout": 67, "thu": [70, 77, 85, 87], "tibbl": 49, "tick_param": 77, "tight": 58, "tight_layout": [70, 71, 77], "tighter": 77, "tild": [21, 22, 23, 27, 50, 71, 74, 81, 85, 88, 89, 96, 97, 98, 101, 102, 103, 110, 111], "time": [7, 8, 10, 28, 29, 48, 49, 50, 51, 53, 58, 62, 63, 64, 65, 66, 71, 72, 73, 77, 78, 79, 80, 87, 116, 117], "time_budget": 70, "time_df": 63, "time_period": 63, "titiunik": [87, 115], "titl": [50, 51, 53, 56, 59, 60, 61, 64, 65, 66, 69, 70, 71, 72, 73, 74, 76, 77, 79, 112], "tmp": 66, "tname": 49, "tnr": [52, 86], "to_fram": 75, "to_numpi": [61, 67, 73, 76], "todo": [50, 54], "toeplitz": 64, "togeth": [65, 66, 102], "toler": 71, "tomasz": [115, 116], "toml": 116, "too": 69, "tool": [49, 52, 78, 117], "top": [50, 69, 71, 72, 73, 77, 79, 87, 112], "total": [38, 42, 51, 70, 72], "tpot": 70, "tracker": 112, "tradit": 102, "train": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 41, 42, 48, 50, 52, 58, 59, 60, 61, 65, 66, 68, 69, 71, 72, 75, 76, 81, 82, 88], "train_id": [50, 88], "train_ind": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18], "train_set": 88, "train_test_split": 58, "transact": 115, "transform": [21, 22, 36, 68, 74, 79, 117], "translat": 64, "transpos": 63, "treament": 75, "treat": [12, 23, 49, 56, 62, 63, 67, 75, 77, 79, 85, 87, 102, 117], "treat1_param": 74, "treat2_param": 74, "treat_var": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 86], "treatment": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 27, 30, 35, 47, 49, 50, 52, 53, 54, 56, 57, 62, 63, 64, 67, 69, 70, 71, 75, 77, 78, 79, 80, 82, 83, 84, 86, 88, 89, 90, 91, 92, 93, 95, 96, 101, 102, 103, 104, 108, 110, 112, 114, 115, 116, 117], "treatment_df": 63, "treatment_effect": [24, 59, 60], "treatment_level": [4, 5, 56, 68, 87], "treatment_var": [7, 10], "tree": [12, 40, 51, 52, 62, 63, 69, 72, 81, 84, 86, 87, 88, 89, 102, 114, 116], "tree_param": [12, 40], "tree_summari": 72, "trees_class": [51, 72], "trend": [49, 62, 63, 71, 87, 115], "tri": [64, 103, 105], "triangular": [35, 77, 87], "trim": [4, 6, 8, 9, 11, 12, 13, 16, 17, 18, 51, 72, 73, 79], "trimming_rul": [4, 5, 6, 8, 9, 11, 12, 13, 16, 17, 18, 73], "trimming_threshold": [4, 5, 6, 8, 9, 11, 12, 13, 16, 17, 18, 51, 59, 68, 72, 73, 75, 76, 79], "trm": [52, 86], "true": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 27, 32, 35, 36, 37, 38, 41, 42, 47, 48, 49, 50, 51, 52, 53, 54, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 86, 87, 88, 89, 90, 91, 96, 97, 98, 101, 102, 103, 106, 107, 111, 114, 117], "true_effect": [59, 60, 63, 65, 66], "true_gatet_effect": 67, "true_group_effect": 67, "true_tau": 77, "truncat": [4, 5, 6, 8, 9, 11, 12, 13, 16, 17, 18, 73], "try": [69, 78], "tune": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 64, 69, 77, 84, 87, 112, 114, 116], "tune_on_fold": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 86], "tune_r": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18], "tune_set": [52, 86], "tuned_model": 70, "tuner": 86, "tunergridsearch": 52, "tupl": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18], "turn": 79, "turrel": 31, "tutori": 51, "tw": [72, 73], "twice": 87, "twinx": 56, "two": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 23, 24, 47, 48, 51, 52, 57, 58, 61, 62, 68, 69, 70, 72, 73, 74, 75, 76, 78, 79, 81, 82, 85, 86, 87, 88, 89, 96, 102, 117], "twoclass": 52, "twoearn": [51, 72, 73, 78, 117], "type": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 27, 35, 36, 37, 38, 39, 40, 41, 42, 43, 48, 49, 50, 51, 52, 58, 69, 70, 71, 77, 79, 82, 86, 87, 89, 99, 100, 102, 103, 110, 116, 117], "typic": [66, 87, 112], "u": [11, 12, 13, 16, 17, 21, 22, 23, 24, 26, 32, 38, 42, 48, 49, 50, 51, 56, 58, 61, 62, 63, 65, 66, 68, 69, 71, 72, 73, 75, 76, 78, 79, 82, 87, 103, 105, 113, 117], "u_hat": [48, 58, 89], "u_i": [25, 28, 31, 32], "u_t": 23, "uehara": 115, "uhash": 52, "ulf": 115, "unambigu": 79, "uncertainti": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 65, 66, 68, 77, 78, 103, 108, 117], "unchang": [37, 38, 41, 42], "uncondit": [51, 72, 117], "unconfounded": [79, 115], "under": [18, 48, 51, 58, 62, 72, 75, 77, 79, 82, 87, 102, 115], "underbrac": [48, 58, 63, 82, 85], "underfit": 70, "underli": [21, 27, 51, 52, 56, 65, 66, 74, 75, 87, 103, 105, 117], "underlin": [50, 71], "underset": [77, 87], "understand": 79, "undesir": 86, "unevenli": 88, "uniform": [23, 35, 36, 57, 59, 60, 61, 63, 75, 76, 102], "uniform_averag": [38, 42], "uniformli": [61, 73, 102], "uniqu": [47, 56, 57, 69, 77, 89, 103, 111], "unique_label": 70, "unit": [48, 49, 53, 62, 63, 67, 77, 80, 87, 89, 92, 93, 116], "univari": [24, 59, 60], "univers": 115, "unknown": 87, "unlik": [51, 72, 73, 79], "unobserv": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 21, 47, 51, 57, 72, 73, 78, 79, 87, 103, 105, 111, 117], "unpen": 49, "unstabl": [103, 105], "unter": [50, 51, 52], "untest": 79, "until": [87, 116], "untreat": [79, 87], "up": [5, 17, 51, 64, 69, 70, 72, 73, 78, 79, 86, 87, 88, 103, 105, 113, 116, 117], "upcom": 116, "updat": [37, 38, 41, 42, 50, 66, 71, 72, 115, 116], "update_layout": [59, 60, 64, 77, 79], "update_trac": [59, 60], "upload": 116, "upon": [89, 116], "upper": [51, 52, 56, 58, 61, 63, 67, 68, 73, 76, 77, 78, 79, 86, 103, 108, 111, 117], "upper_bound": [59, 60], "upsilon": [53, 80], "upsilon_i": [53, 80], "upward": [51, 72, 73, 79], "upweight": 74, "url": [64, 112, 115], "us": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 37, 38, 39, 41, 42, 48, 50, 51, 53, 56, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 85, 87, 88, 89, 92, 93, 102, 103, 105, 108, 109, 110, 111, 112, 113, 114, 116, 117], "usa": 115, "usabl": 69, "usag": [49, 54, 56, 62, 67, 71, 72, 73, 78, 80, 83, 114, 116], "use_label_encod": [72, 117], "use_other_treat_as_covari": [7, 10, 83], "use_pred_offset": 86, "usecolormap": [59, 60], "user": [33, 34, 37, 38, 41, 42, 48, 49, 50, 51, 52, 56, 58, 67, 68, 69, 71, 72, 77, 78, 85, 86, 87, 89, 102, 112, 113, 114, 116, 117], "user_guid": 66, "userwarn": [72, 79], "usual": [50, 59, 60, 62, 69, 71, 77, 78, 79, 85, 86, 88, 103, 111], "util": [0, 34, 68, 69, 70, 74, 77, 86, 87, 116], "v": [11, 12, 14, 15, 19, 20, 26, 28, 29, 30, 32, 38, 42, 48, 50, 51, 56, 58, 67, 68, 69, 70, 71, 72, 74, 77, 81, 82, 85, 87, 102, 112, 114, 115, 116, 117], "v108": 112, "v12": [112, 114], "v22": 52, "v23": 112, "v_": [29, 50, 71, 87], "v_i": [25, 26, 30, 31, 32, 48, 58, 82, 87], "v_j": 102, "val": [26, 88, 115], "val_list": 64, "valid": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 25, 48, 49, 50, 51, 58, 61, 62, 69, 70, 71, 72, 73, 76, 82, 84, 85, 86, 88, 89, 91, 96, 101, 103, 105, 115, 117], "valu": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 36, 37, 38, 41, 42, 43, 47, 48, 49, 50, 51, 52, 53, 56, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 77, 78, 79, 81, 84, 86, 87, 88, 91, 96, 97, 98, 101, 102, 103, 105, 108, 111, 114, 116, 117], "value_count": 72, "van": 115, "vanderpla": [112, 114], "vanish": [48, 58, 82], "var": [21, 22, 23, 27, 50, 71, 74, 77, 103, 104, 105, 109, 110, 111], "var_ep": 79, "varepsilon": [11, 21, 22, 29, 50, 53, 71, 80, 85, 87], "varepsilon_": [29, 50, 71], "varepsilon_0": 23, "varepsilon_1": 23, "varepsilon_d": [22, 27], "varepsilon_i": [27, 28, 53, 61, 76, 80], "vari": [51, 63, 69, 72, 74, 79], "variabl": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 35, 50, 51, 52, 53, 54, 56, 62, 64, 67, 70, 71, 72, 73, 77, 78, 79, 80, 83, 85, 86, 87, 88, 89, 102, 103, 105, 108, 111, 114, 115, 116, 117], "varianc": [33, 34, 50, 52, 71, 77, 78, 79, 84, 87, 88, 103, 105, 108, 109, 110, 111, 114], "variant": [49, 68], "variat": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 22, 68, 78, 103, 105, 111], "variou": [49, 70, 79, 86, 117], "varoquaux": [112, 114], "vasili": [79, 115], "vector": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 24, 25, 26, 28, 29, 31, 32, 47, 50, 51, 53, 57, 62, 65, 66, 67, 71, 72, 75, 80, 87, 102, 114, 116], "venv": 113, "verbos": [51, 58, 63, 69, 70, 77, 79], "veri": [49, 50, 52, 67, 69, 71, 79, 89, 112], "verifi": 74, "versa": [69, 74, 103, 108], "version": [21, 37, 38, 41, 42, 50, 51, 52, 79, 81, 85, 102, 103, 104, 106, 107, 109, 116], "versoin": 79, "versu": 66, "vertic": [50, 56, 71], "via": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 21, 22, 23, 34, 49, 53, 61, 62, 63, 64, 65, 66, 67, 68, 69, 77, 78, 80, 81, 83, 84, 85, 86, 87, 88, 91, 98, 102, 103, 105, 108, 111, 112, 113, 114, 115, 116, 117], "viabl": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18], "vice": [69, 74, 103, 108], "victor": [64, 79, 88, 112, 115], "view": 66, "vignett": [49, 116], "villa": [47, 57], "violet": [61, 73, 76], "vira": 115, "virtual": 113, "virtualenv": 113, "visibl": [73, 77, 79], "visit": [112, 117], "visual": [50, 67, 68, 70, 71, 77], "vol": 49, "volum": [79, 112], "voluntari": 74, "vv740": 71, "vv760g": 71, "w": [19, 20, 21, 22, 23, 30, 33, 34, 37, 38, 41, 42, 50, 64, 71, 74, 75, 81, 82, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 114], "w24678": 88, "w30302": 115, "w_": [23, 50, 71, 75, 87], "w_1": [23, 75], "w_2": [23, 75], "w_3": 23, "w_4": 23, "w_df": 75, "w_i": [32, 62, 75, 77, 81, 85, 87, 88, 89, 102], "wa": [50, 63, 70, 71, 79, 116], "wager": 115, "wai": [51, 69, 70, 72, 79, 86, 89, 113], "wander": 31, "wang": 115, "want": [47, 50, 51, 52, 57, 61, 62, 69, 71, 76, 77, 86, 87, 112, 113, 115], "warn": [47, 48, 49, 50, 51, 52, 53, 58, 72, 79, 81, 86, 87, 88, 89, 102, 114, 116], "wayon": 50, "we": [12, 40, 47, 48, 49, 50, 51, 52, 53, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 94, 102, 103, 105, 111, 113, 114, 116, 117], "weak": [103, 105, 115], "wealth": [19, 78], "websit": [51, 52, 86, 112], "wedg": [50, 71], "week": 116, "wei": 102, "weight": [4, 5, 6, 11, 12, 13, 16, 17, 18, 37, 38, 41, 42, 50, 51, 52, 53, 56, 67, 68, 71, 72, 77, 80, 84, 86, 87, 89, 90, 95, 102, 103, 104, 109, 116], "weights_bar": [4, 12, 68], "weights_dict": 68, "weiss": [112, 114], "well": [7, 10, 41, 42, 48, 50, 58, 64, 69, 70, 71, 81, 82, 83, 88, 113, 114], "were": [51, 53, 72, 73, 80, 117], "what": [49, 64, 69], "when": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 37, 38, 41, 42, 51, 62, 66, 68, 72, 74, 87, 89, 102, 112, 113, 114, 116], "whenev": [51, 72], "whera": [103, 109], "where": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 38, 39, 40, 42, 47, 48, 50, 51, 53, 56, 57, 58, 61, 62, 63, 67, 71, 72, 74, 75, 76, 77, 79, 80, 81, 82, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 108, 109, 111, 113, 114, 116, 117], "wherea": [24, 53, 56, 62, 79, 80, 89, 95, 103, 104, 117], "whether": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 23, 24, 27, 32, 35, 39, 51, 63, 69, 72, 73, 77, 79, 83, 86, 87, 103, 105, 116], "which": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 24, 34, 37, 41, 47, 48, 49, 51, 52, 53, 55, 56, 57, 58, 62, 64, 66, 67, 69, 70, 72, 73, 75, 77, 78, 79, 80, 82, 83, 85, 86, 87, 89, 102, 103, 104, 105, 108, 109, 111, 113, 116, 117], "while": [47, 57, 87], "white": [50, 65, 66, 71, 79], "whitegrid": [72, 73], "whitnei": [79, 115], "who": [49, 51, 72, 79], "whole": [48, 58, 62, 77, 82, 86, 103, 105], "whom": 87, "widehat": 87, "width": [48, 50, 59, 60, 64], "wiki": 116, "wiksel": 115, "wild": [4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 102], "window": 113, "wise": [65, 66], "wish": 113, "within": [35, 50, 65, 66, 71, 75, 77], "without": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 27, 35, 47, 48, 57, 58, 69, 70, 79, 82, 84, 86, 87, 103, 105, 113, 116], "wolf": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 102], "won": 79, "word": [35, 77, 87, 116, 117], "work": [37, 38, 41, 42, 55, 56, 66, 67, 69, 74, 78, 79, 86, 87, 102, 113, 115], "workflow": [112, 116], "workspac": 72, "world": 115, "worri": 79, "wors": [38, 42], "would": [38, 42, 49, 51, 52, 59, 60, 64, 69, 72, 73, 77, 78, 79, 85, 86, 103, 111, 117], "wrapper": [49, 77, 86], "write": [48, 49, 53, 58, 62, 66, 80, 82, 103, 111], "written": [87, 89, 103, 104, 109], "wrong": [69, 74], "wspace": 69, "wurd": [50, 51, 52], "www": [112, 113], "x": [4, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 36, 37, 38, 41, 42, 48, 49, 50, 51, 52, 53, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 109, 110, 111, 114, 117], "x0": [56, 74, 77], "x1": [50, 52, 53, 56, 62, 68, 70, 71, 74, 77, 78, 79, 80, 83, 85, 86, 87, 89, 102, 103, 105, 114], "x10": [50, 52, 53, 68, 70, 71, 80, 83, 86, 87, 89, 102, 114], "x100": [50, 52, 53, 71, 80, 83, 87, 114], "x11": [50, 52, 53, 71, 80, 83, 86, 87, 89, 102, 114], "x12": [50, 52, 53, 71, 80, 83, 86, 87, 89, 102, 114], "x13": [50, 52, 53, 71, 80, 83, 86, 87, 89, 102, 114], "x14": [50, 52, 53, 71, 80, 83, 86, 87, 89, 102, 114], "x15": [50, 52, 53, 71, 80, 83, 86, 87, 89, 102, 114], "x16": [50, 52, 53, 71, 80, 83, 86, 87, 89, 102, 114], "x17": [50, 52, 53, 71, 80, 83, 86, 87, 89, 102, 114], "x18": [50, 52, 53, 71, 80, 83, 86, 87, 89, 102, 114], "x19": [50, 52, 53, 71, 80, 83, 86, 87, 89, 102, 114], "x1x2x3x4x5x6x7x8x9x10": 50, "x2": [50, 52, 53, 56, 62, 68, 70, 71, 77, 78, 79, 80, 83, 85, 86, 87, 89, 102, 114], "x20": [50, 52, 53, 71, 80, 83, 86, 87, 89, 102, 114], "x21": [50, 52, 53, 71, 80, 83, 87, 114], "x22": [50, 52, 53, 71, 80, 83, 87, 114], "x23": [50, 52, 53, 71, 80, 83, 87, 114], "x24": [50, 52, 53, 71, 80, 83, 87, 114], "x25": [50, 52, 53, 71, 80, 83, 87, 114], "x26": [50, 52, 53, 71, 80, 83, 87, 114], "x27": [50, 52, 53, 71, 80, 83, 87, 114], "x28": [50, 52, 53, 71, 80, 83, 87, 114], "x29": [50, 52, 53, 71, 80, 83, 87, 114], "x2_dummi": 79, "x2_preds_control": 79, "x2_preds_treat": 79, "x3": [50, 52, 53, 56, 62, 68, 70, 71, 78, 79, 80, 83, 85, 86, 87, 89, 102, 114], "x30": [50, 52, 53, 71, 80, 83, 87, 114], "x31": [50, 52, 53, 71, 80, 83, 87, 114], "x32": [50, 52, 53, 71, 80, 83, 87, 114], "x33": [50, 52, 53, 71, 80, 83, 87, 114], "x34": [50, 52, 53, 71, 80, 83, 87, 114], "x35": [50, 52, 53, 71, 80, 83, 87, 114], "x36": [50, 52, 53, 71, 80, 83, 87, 114], "x37": [50, 52, 53, 71, 80, 83, 87, 114], "x38": [50, 52, 53, 71, 80, 83, 87, 114], "x39": [50, 52, 53, 71, 80, 83, 87, 114], "x4": [50, 52, 53, 56, 62, 68, 70, 71, 78, 79, 80, 83, 86, 87, 89, 102, 114], "x40": [50, 52, 53, 71, 80, 83, 87, 114], "x41": [50, 52, 53, 71, 80, 83, 87, 114], "x42": [50, 52, 53, 71, 80, 83, 87, 114], "x43": [50, 52, 53, 70, 71, 80, 83, 87, 114], "x44": [50, 52, 53, 70, 71, 80, 83, 87, 114], "x45": [50, 52, 53, 70, 71, 80, 83, 87, 114], "x46": [50, 52, 53, 70, 71, 80, 83, 87, 114], "x47": [50, 52, 53, 70, 71, 80, 83, 87, 114], "x48": [50, 52, 53, 70, 71, 80, 83, 87, 114], "x49": [50, 52, 53, 70, 71, 80, 83, 87, 114], "x5": [50, 52, 53, 68, 70, 71, 79, 80, 83, 86, 87, 89, 102, 114], "x50": [50, 52, 53, 70, 71, 80, 83, 87, 114], "x51": [50, 52, 53, 71, 80, 83, 87, 114], "x52": [50, 52, 53, 71, 80, 83, 87, 114], "x53": [50, 52, 53, 71, 80, 83, 87, 114], "x54": [50, 52, 53, 71, 80, 83, 87, 114], "x55": [50, 52, 53, 71, 80, 83, 87, 114], "x56": [50, 52, 53, 71, 80, 83, 87, 114], "x57": [50, 52, 53, 71, 80, 83, 87, 114], "x58": [50, 52, 53, 71, 80, 83, 87, 114], "x59": [50, 52, 53, 71, 80, 83, 87, 114], "x6": [50, 52, 53, 68, 70, 71, 80, 83, 86, 87, 89, 102, 114], "x60": [50, 52, 53, 71, 80, 83, 87, 114], "x61": [50, 52, 53, 71, 80, 83, 87, 114], "x62": [50, 52, 53, 71, 80, 83, 87, 114], "x63": [50, 52, 53, 71, 80, 83, 87, 114], "x64": [50, 52, 53, 71, 72, 80, 83, 87, 114], "x65": [50, 52, 53, 71, 80, 83, 87, 114], "x66": [50, 52, 53, 71, 80, 83, 87, 114], "x67": [50, 52, 53, 71, 80, 83, 87, 114], "x68": [50, 52, 53, 71, 80, 83, 87, 114], "x69": [50, 52, 53, 71, 80, 83, 87, 114], "x7": [50, 52, 53, 68, 70, 71, 80, 83, 86, 87, 89, 102, 114], "x70": [50, 52, 53, 71, 80, 83, 87, 114], "x71": [50, 52, 53, 71, 80, 83, 87, 114], "x72": [50, 52, 53, 71, 80, 83, 87, 114], "x73": [50, 52, 53, 71, 80, 83, 87, 114], "x74": [50, 52, 53, 71, 80, 83, 87, 114], "x75": [50, 52, 53, 71, 80, 83, 87, 114], "x76": [50, 52, 53, 71, 80, 83, 87, 114], "x77": [50, 52, 53, 71, 80, 83, 87, 114], "x78": [50, 52, 53, 71, 80, 83, 87, 114], "x79": [50, 52, 53, 71, 80, 83, 87, 114], "x8": [50, 52, 53, 68, 70, 71, 80, 83, 86, 87, 89, 102, 114], "x80": [50, 52, 53, 71, 80, 83, 87, 114], "x81": [50, 52, 53, 71, 80, 83, 87, 114], "x82": [50, 52, 53, 71, 80, 83, 87, 114], "x83": [50, 52, 53, 71, 80, 83, 87, 114], "x84": [50, 52, 53, 71, 80, 83, 87, 114], "x85": [50, 52, 53, 71, 80, 83, 87, 114], "x86": [50, 52, 53, 71, 80, 83, 87, 114], "x87": [50, 52, 53, 71, 80, 83, 87, 114], "x88": [50, 52, 53, 71, 80, 83, 87, 114], "x89": [50, 52, 53, 71, 80, 83, 87, 114], "x9": [50, 52, 53, 68, 70, 71, 80, 83, 86, 87, 89, 102, 114], "x90": [50, 52, 53, 71, 80, 83, 87, 114], "x91": [50, 52, 53, 71, 80, 83, 87, 114], "x92": [50, 52, 53, 71, 80, 83, 87, 114], "x93": [50, 52, 53, 71, 80, 83, 87, 114], "x94": [50, 52, 53, 71, 80, 83, 87, 114], "x95": [50, 52, 53, 71, 80, 83, 87, 114], "x96": [50, 52, 53, 71, 80, 83, 87, 114], "x96x97x98x99x100ydcluster_var_icluster_var_jz": 50, "x97": [50, 52, 53, 71, 80, 83, 87, 114], "x98": [50, 52, 53, 71, 80, 83, 87, 114], "x99": [50, 52, 53, 71, 80, 83, 87, 114], "x_": [29, 30, 48, 50, 58, 63, 71, 79, 82], "x_0": [59, 60, 63, 65, 66, 67], "x_1": [14, 15, 21, 22, 23, 27, 59, 60, 61, 63, 65, 66, 67, 76, 79, 87, 103, 105, 114], "x_1x_3": [61, 76], "x_2": [21, 22, 23, 27, 59, 60, 61, 63, 65, 66, 67, 76, 79, 103, 105], "x_3": [21, 22, 23, 27, 59, 60, 63, 65, 66, 67, 103, 105], "x_4": [21, 22, 23, 27, 59, 60, 61, 65, 66, 67, 76], "x_5": [21, 22, 27, 59, 60, 65, 66], "x_6": [59, 60, 65, 66], "x_7": [59, 60, 65, 66], "x_8": [59, 60, 65, 66], "x_9": [59, 60, 65, 66], "x_binary_control": 79, "x_binary_tr": 79, "x_col": [7, 10, 47, 50, 51, 52, 57, 64, 71, 72, 73, 75, 77, 78, 79, 83, 86, 87, 114, 116, 117], "x_cols_bench": 79, "x_cols_binari": 79, "x_cols_poli": 71, "x_conf": 76, "x_conf_tru": 76, "x_df": 63, "x_domain": 52, "x_i": [24, 25, 26, 28, 30, 31, 32, 36, 48, 53, 58, 61, 62, 65, 66, 74, 76, 77, 80, 82, 85, 87], "x_p": [14, 15, 87, 114], "x_train": 70, "x_true": [61, 76], "x_var": 52, "xaxis_titl": [59, 60, 64, 77, 79], "xformla": 49, "xgb": 70, "xgb_untuned_l": 70, "xgb_untuned_m": 70, "xgbclassifi": [69, 72, 74, 117], "xgboost": [48, 51, 69, 72, 74, 117], "xgbregressor": [69, 70, 72, 74, 117], "xi": [23, 27, 87], "xi_": 102, "xi_0": [29, 50, 71], "xi_i": [53, 80], "xiaoji": 115, "xintercept": [48, 53], "xlab": [48, 50, 51], "xlabel": [56, 59, 60, 61, 63, 65, 66, 70, 72, 73, 76], "xlim": [48, 51], "xtick": [56, 70], "xval": [52, 86], "xx": 58, "y": [4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 35, 36, 37, 38, 41, 42, 47, 48, 49, 50, 51, 52, 53, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 108, 109, 110, 111, 114, 117], "y0": [49, 56, 61, 76], "y0_cvar": 61, "y0_quant": [61, 76], "y1": [49, 61, 76], "y1_cvar": 61, "y1_quant": [61, 76], "y_": [29, 50, 53, 62, 63, 71, 80, 87], "y_0": [8, 23, 36, 89, 92], "y_1": [8, 23, 36, 89, 92], "y_col": [7, 10, 47, 48, 50, 51, 52, 53, 57, 59, 60, 64, 65, 66, 68, 71, 72, 73, 75, 77, 78, 81, 82, 83, 86, 87, 88, 89, 114, 116, 117], "y_df": [63, 75], "y_diff": 63, "y_i": [24, 25, 26, 28, 30, 31, 32, 48, 53, 58, 61, 62, 74, 75, 76, 77, 80, 82, 87], "y_pred": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 38, 42, 69, 86], "y_train": 70, "y_true": [4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 18, 38, 42, 69, 86], "ya": 115, "yasui": 115, "yata": 115, "yaxis_titl": [59, 60, 64, 77, 79], "year": 112, "yerr": [56, 63, 65, 66, 70, 72, 74, 77], "yet": [50, 55], "yggvpl": 71, "yield": 87, "yintercept": 51, "ylab": [48, 50, 51], "ylabel": [56, 59, 60, 61, 63, 65, 66, 70, 72, 73, 76], "ylim": 72, "ymax": 51, "ymin": 51, "yname": 49, "york": 115, "you": [37, 38, 41, 42, 47, 48, 57, 63, 66, 71, 78, 87, 112, 113, 117], "your": [69, 113], "ython": 112, "yukun": 115, "yusuk": 115, "yuya": 115, "yy": 58, "z": [7, 10, 11, 13, 14, 18, 21, 22, 23, 25, 27, 28, 29, 32, 47, 50, 51, 53, 57, 59, 60, 64, 71, 72, 76, 79, 80, 85, 87, 89, 94, 96, 98, 99, 102, 116], "z1": [14, 87], "z2": 87, "z3": 87, "z4": 87, "z_": [29, 50, 71], "z_1": [21, 22, 27], "z_2": [21, 22, 27], "z_3": [21, 22, 27], "z_4": [21, 22, 27], "z_5": 21, "z_col": [7, 10, 11, 13, 14, 47, 50, 51, 53, 57, 71, 72, 73, 80, 83, 85, 87, 116], "z_i": [28, 32, 53, 76, 80, 87], "z_j": [21, 22, 23, 27], "z_true": 76, "zadik": 115, "zaxis_titl": [59, 60, 64], "zero": [23, 36, 61, 62, 63, 68, 69, 75, 76, 78, 79, 87, 102], "zeros_lik": 76, "zeta": [11, 14, 15, 51, 72, 85, 87, 114], "zeta_": [29, 50, 71], "zeta_0": [29, 50, 71], "zeta_i": [26, 28, 30, 48, 58, 82], "zeta_j": 102, "zhang": 115, "zhao": [8, 9, 21, 22, 23, 27, 49, 62, 87, 115], "zimmert": [62, 115], "zip": [59, 60], "zorder": 56, "\u03c4_x0": 74, "\u03c4_x1": 74, "\u2139": 48}, "titles": ["API Reference", "<span class=\"section-number\">1. </span>DoubleML Data Class", "<span class=\"section-number\">4. </span>Datasets", "<span class=\"section-number\">2. </span>DoubleML Models", "<span class=\"section-number\">2.4. </span>doubleml.DoubleMLAPO", "<span class=\"section-number\">2.5. </span>doubleml.DoubleMLAPOS", "<span class=\"section-number\">2.12. </span>doubleml.DoubleMLCVAR", "<span class=\"section-number\">1.2. </span>doubleml.DoubleMLClusterData", "<span class=\"section-number\">2.7. </span>doubleml.DoubleMLDID", "<span class=\"section-number\">2.8. </span>doubleml.DoubleMLDIDCS", "<span class=\"section-number\">1.1. </span>doubleml.DoubleMLData", "<span class=\"section-number\">2.6. </span>doubleml.DoubleMLIIVM", "<span class=\"section-number\">2.3. </span>doubleml.DoubleMLIRM", "<span class=\"section-number\">2.11. </span>doubleml.DoubleMLLPQ", "<span class=\"section-number\">2.2. </span>doubleml.DoubleMLPLIV", "<span class=\"section-number\">2.1. </span>doubleml.DoubleMLPLR", "<span class=\"section-number\">2.10. </span>doubleml.DoubleMLPQ", "<span class=\"section-number\">2.13. </span>doubleml.DoubleMLQTE", "<span class=\"section-number\">2.9. </span>doubleml.DoubleMLSSM", "<span class=\"section-number\">4.1.1. </span>doubleml.datasets.fetch_401K", "<span class=\"section-number\">4.1.2. </span>doubleml.datasets.fetch_bonus", "<span class=\"section-number\">4.2.10. </span>doubleml.datasets.make_confounded_irm_data", "<span class=\"section-number\">4.2.9. </span>doubleml.datasets.make_confounded_plr_data", "<span class=\"section-number\">4.2.7. </span>doubleml.datasets.make_did_SZ2020", "<span class=\"section-number\">4.2.11. </span>doubleml.datasets.make_heterogeneous_data", "<span class=\"section-number\">4.2.4. </span>doubleml.datasets.make_iivm_data", "<span class=\"section-number\">4.2.3. </span>doubleml.datasets.make_irm_data", "<span class=\"section-number\">4.2.12. </span>doubleml.datasets.make_irm_data_discrete_treatments", "<span class=\"section-number\">4.2.2. </span>doubleml.datasets.make_pliv_CHS2015", "<span class=\"section-number\">4.2.6. </span>doubleml.datasets.make_pliv_multiway_cluster_CKMS2021", "<span class=\"section-number\">4.2.1. </span>doubleml.datasets.make_plr_CCDDHNR2018", "<span class=\"section-number\">4.2.5. </span>doubleml.datasets.make_plr_turrell2018", "<span class=\"section-number\">4.2.8. </span>doubleml.datasets.make_ssm_data", "<span class=\"section-number\">6.1. </span>doubleml.double_ml_score_mixins.LinearScoreMixin", "<span class=\"section-number\">6.2. </span>doubleml.double_ml_score_mixins.NonLinearScoreMixin", "<span class=\"section-number\">3.1. </span>doubleml.rdd.RDFlex", "<span class=\"section-number\">4.2.13. </span>doubleml.rdd.datasets.make_simple_rdd_data", "<span class=\"section-number\">5.1.2. </span>doubleml.utils.DMLDummyClassifier", "<span class=\"section-number\">5.1.1. </span>doubleml.utils.DMLDummyRegressor", "<span class=\"section-number\">5.1.3. </span>doubleml.utils.DoubleMLBLP", "<span class=\"section-number\">5.1.4. </span>doubleml.utils.DoubleMLPolicyTree", "<span class=\"section-number\">5.1.6. </span>doubleml.utils.GlobalClassifier", "<span class=\"section-number\">5.1.5. </span>doubleml.utils.GlobalRegressor", "<span class=\"section-number\">5.2.1. </span>doubleml.utils.gain_statistics", "<span class=\"section-number\">6. </span>Score Mixin Classes for DoubleML Models", "<span class=\"section-number\">3. </span>Other models", "<span class=\"section-number\">5. </span>Utility Classes and Functions", "R: Basic Instrumental Variables Calculation", "R: Basics of Double Machine Learning", "R: DoubleML for Difference-in-Differences", "R: Cluster Robust Double Machine Learning", "R: Impact of 401(k) on Financial Wealth", "R: Ensemble Learners and More with <code class=\"docutils literal notranslate\"><span class=\"pre\">mlr3pipelines</span></code>", "R: Sample Selection Models", "DML: Bonus Data", "Examples", "Python: Average Potential Outcome (APO) Models", "Python: Basic Instrumental Variables calculation", "Python: Basics of Double Machine Learning", "Python: Conditional Average Treatment Effects (CATEs) for IRM models", "Python: Conditional Average Treatment Effects (CATEs) for PLR models", "Python: Conditional Value at Risk of potential outcomes", "Python: Difference-in-Differences", "Python: Difference-in-Differences Pre-Testing", "Python: First Stage and Causal Estimation", "Python: Group Average Treatment Effects (GATEs) for IRM models", "Python: Group Average Treatment Effects (GATEs) for PLR models", "Python: GATE Sensitivity Analysis", "Python: IRM and APO Model Comparison", "Python: Choice of learners", "DoubleML meets FLAML - How to tune learners automatically within <code class=\"docutils literal notranslate\"><span class=\"pre\">DoubleML</span></code>", "Python: Cluster Robust Double Machine Learning", "Python: Impact of 401(k) on Financial Wealth", "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)", "Python: PLM and IRM for Multiple Treatments", "Python: Policy Learning with Trees", "Python: Potential Quantiles and Quantile Treatment Effects", "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)", "Python: Sensitivity Analysis", "Example: Sensitivity Analysis for Causal ML", "Python: Sample Selection Models", "<span class=\"section-number\">6. </span>Double machine learning algorithms", "<span class=\"section-number\">1. </span>The basics of double/debiased machine learning", "<span class=\"section-number\">2. </span>The data-backend DoubleMLData", "User Guide", "<span class=\"section-number\">4. </span>Heterogeneous treatment effects", "<span class=\"section-number\">7. </span>Learners, hyperparameters and hyperparameter tuning", "<span class=\"section-number\">3. </span>Models", "<span class=\"section-number\">9. </span>Sample-splitting, cross-fitting and repeated cross-fitting", "<span class=\"section-number\">5. </span>Score functions", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">8. </span>Variance estimation and confidence intervals", "<span class=\"section-number\">10. </span>Sensitivity analysis", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "DoubleML", "Installing DoubleML", "Getting Started", "Double Machine Learning Literature", "Release Notes", "DoubleML Workflow"], "titleterms": {"": 70, "0": 117, "1": [70, 79, 117], "2": [70, 79, 117], "2011": 79, "2023": 79, "3": [70, 79, 117], "4": [79, 117], "401": [51, 72, 73, 78], "5": [79, 117], "6": 117, "7": 117, "95": 70, "A": [50, 71], "ATE": [53, 67, 74, 80], "No": [50, 71], "One": [50, 59, 60, 71], "The": [51, 72, 74, 82, 83, 114], "acknowledg": [49, 112], "acycl": [47, 57], "addit": 74, "adjust": 77, "advanc": [77, 86, 102], "al": 79, "algorithm": [81, 103, 112, 114], "altern": 89, "analysi": [56, 67, 68, 78, 79, 103, 117], "api": [0, 70], "apo": [56, 68, 87, 89, 103], "applic": [50, 71, 78], "approach": [48, 58, 69, 82], "arah": 79, "arbitrari": 74, "arrai": 83, "asset": [51, 72], "assumpt": 79, "att": 62, "augment": 74, "automat": 70, "automl": 70, "averag": [51, 56, 59, 60, 65, 66, 68, 72, 85, 87, 89, 103], "backend": [50, 51, 71, 72, 83, 114, 117], "band": 102, "base": 52, "basic": [47, 48, 57, 58, 82], "benchmark": [78, 79, 103], "bia": [48, 58, 82], "binari": [87, 89], "bonu": 54, "bootstrap": 102, "build": 113, "calcul": [47, 57], "call": 70, "callabl": 89, "case": 55, "cate": [59, 60, 74, 85], "causal": [54, 56, 64, 79, 89, 114, 117], "chernozhukov": 79, "choic": 69, "citat": 112, "class": [1, 44, 46, 50, 71], "cluster": [50, 71], "code": 112, "coeffici": 70, "combin": 64, "compar": [69, 70], "comparison": [49, 68, 70], "comput": [69, 70], "conclus": [70, 79], "conda": 113, "condit": [59, 60, 61, 73, 85, 89], "confid": [70, 102], "construct": 86, "contrast": 56, "covari": 77, "coverag": [62, 64], "cran": 113, "creat": 70, "cross": [50, 62, 71, 87, 88, 89, 103, 114], "custom": [69, 70], "cvar": [61, 73, 85, 89], "dag": [47, 57], "data": [1, 47, 48, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 65, 66, 67, 68, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 82, 83, 87, 89, 103, 114, 117], "datafram": 83, "dataset": [2, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 36, 54], "debias": [48, 58, 82, 114], "default": 70, "defin": [50, 71], "demo": 49, "depend": 113, "design": [77, 87], "detail": [49, 87], "develop": 113, "dgp": [48, 56, 58], "did": [49, 87], "differ": [49, 62, 63, 69, 87, 89, 102, 103], "dimension": [59, 60], "direct": [47, 57], "disclaim": 79, "discontinu": [77, 87], "distribut": [53, 80], "dml": [50, 54, 71, 88, 114, 117], "dml1": 81, "dml2": 81, "dmldummyclassifi": 37, "dmldummyregressor": 38, "doubl": [48, 50, 58, 71, 81, 82, 112, 114, 115], "double_ml_score_mixin": [33, 34], "doubleml": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 47, 49, 51, 52, 57, 70, 72, 78, 79, 102, 112, 113, 117], "doublemlapo": [4, 5], "doublemlblp": 39, "doublemlclusterdata": [7, 50, 71], "doublemlcvar": 6, "doublemldata": [10, 51, 72, 83, 114], "doublemldid": 8, "doublemldidc": 9, "doublemliivm": 11, "doublemlirm": 12, "doublemllpq": 13, "doublemlpliv": [14, 50, 71], "doublemlplr": 15, "doublemlpolicytre": 40, "doublemlpq": 16, "doublemlqt": 17, "doublemlssm": 18, "effect": [51, 55, 59, 60, 61, 65, 66, 68, 72, 73, 74, 76, 78, 79, 85], "elig": [51, 72], "empir": 64, "ensembl": [52, 77], "error": [50, 71], "estim": [47, 51, 53, 54, 57, 62, 64, 67, 70, 72, 73, 74, 76, 78, 79, 80, 88, 89, 102, 114, 117], "et": 79, "evalu": [69, 70, 86], "exampl": [49, 50, 55, 59, 60, 71, 78, 79], "exploit": [49, 52], "extern": [86, 88], "featur": [52, 112], "fetch_401k": 19, "fetch_bonu": 20, "figur": 74, "file": 113, "final": 49, "financi": [51, 72, 73], "first": 64, "fit": [50, 70, 71, 88, 114], "flaml": 70, "flexibl": 77, "fold": [70, 88], "forest": 54, "formul": [79, 117], "from": [49, 52, 83, 113], "full": 70, "function": [46, 49, 50, 71, 89, 114], "fuzzi": [77, 87], "gain_statist": 43, "gate": [65, 66, 67, 85], "gatet": 67, "gener": [2, 48, 55, 56, 58, 70, 77, 82, 103], "get": 114, "github": 113, "global": 77, "globalclassifi": 41, "globalregressor": 42, "graph": [47, 57], "group": [65, 66, 85], "guid": 84, "helper": [50, 71], "heterogen": [55, 68, 74, 85], "how": [52, 70], "hyperparamet": [68, 86], "identif": 79, "iivm": [51, 72, 87, 89], "impact": [51, 72, 73], "implement": [81, 87, 89, 103], "induc": [48, 58, 82], "infer": [102, 117], "initi": [50, 70, 71], "instal": 113, "instrument": [47, 57], "integr": 49, "interact": [51, 65, 72, 75, 87, 89, 103], "interv": [70, 102], "invers": 74, "irm": [51, 54, 59, 65, 68, 72, 74, 75, 78, 85, 87, 89, 103], "iv": [47, 51, 57, 72, 87, 89], "k": [51, 72, 73, 78, 88], "lambda": 64, "lasso": [54, 64], "latest": 113, "lear": [50, 71], "learn": [48, 50, 58, 71, 75, 81, 82, 85, 112, 114, 115], "learner": [52, 54, 68, 69, 70, 77, 86, 114], "less": 70, "level": 87, "linear": [51, 66, 72, 74, 77, 87, 89, 103], "linearscoremixin": 33, "literatur": 115, "load": [50, 54, 71, 79], "loader": 2, "local": [51, 72, 73, 76, 77, 89], "loss": 64, "lpq": [76, 89], "lqte": [73, 76], "m": 88, "machin": [48, 50, 58, 71, 81, 82, 112, 114, 115], "main": 112, "mainten": 112, "make_confounded_irm_data": 21, "make_confounded_plr_data": 22, "make_did_sz2020": 23, "make_heterogeneous_data": 24, "make_iivm_data": 25, "make_irm_data": 26, "make_irm_data_discrete_treat": 27, "make_pliv_chs2015": 28, "make_pliv_multiway_cluster_ckms2021": 29, "make_plr_ccddhnr2018": 30, "make_plr_turrell2018": 31, "make_simple_rdd_data": 36, "make_ssm_data": 32, "mar": [53, 80], "market": [50, 71], "matric": 83, "meet": 70, "method": [70, 117], "metric": [69, 70], "minimum": 86, "miss": [53, 80], "missing": [87, 89], "mixin": 44, "ml": [48, 49, 58, 79, 82, 117], "mlr3": 52, "mlr3extralearn": 52, "mlr3learner": 52, "mlr3pipelin": 52, "model": [3, 44, 45, 51, 53, 54, 56, 59, 60, 65, 66, 68, 70, 72, 74, 75, 79, 80, 85, 87, 88, 89, 102, 103, 114, 117], "modul": 54, "more": 52, "motiv": [50, 71], "multipl": [56, 74, 87], "multipli": 102, "naiv": [47, 57], "net": [51, 72], "neyman": [89, 114], "nonignor": [53, 80, 87, 89], "nonlinearscoremixin": 34, "nonrespons": [53, 80, 87, 89], "note": 116, "nuisanc": [70, 114], "object": [50, 71, 78], "option": 113, "orthogon": [48, 58, 82, 89, 114], "other": 45, "out": [48, 58, 82], "outcom": [53, 56, 61, 62, 80, 85, 87, 89, 103], "over": 102, "overcom": [48, 58, 82], "overfit": [48, 58, 82], "overlap": 74, "packag": [49, 51, 72, 113], "panel": [62, 87, 89, 103], "paramet": [52, 54, 70, 89], "partial": [48, 51, 58, 66, 72, 74, 82, 87, 89, 103], "particip": [51, 72], "partit": 88, "penalti": 64, "perform": [49, 74], "pip": 113, "pipelin": 86, "pliv": [87, 89], "plm": [74, 87, 89], "plot": [50, 70, 71], "plr": [51, 54, 60, 66, 72, 85, 87, 89, 103], "polici": [75, 85], "potenti": [56, 61, 73, 76, 85, 87, 89, 103], "pq": [76, 85, 89], "pre": 63, "predict": [49, 86], "preprocess": 52, "problem": 117, "process": [48, 50, 56, 58, 71, 82], "product": [50, 71], "propens": 74, "provid": 88, "python": [55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 78, 80, 86, 113], "qte": [76, 85], "qualiti": 64, "quantil": [73, 76, 85, 89], "r": [47, 48, 49, 50, 51, 52, 53, 55, 86, 113], "random": [53, 54, 80, 87, 89], "rank": 74, "rdd": [35, 36, 77, 87], "rdflex": 35, "real": [50, 71], "refer": [0, 47, 49, 50, 52, 57, 64, 69, 70, 71, 74, 79, 82, 86, 88, 102, 112, 114], "regress": [51, 65, 66, 72, 75, 77, 87, 89, 103], "regular": [48, 58, 82], "releas": [113, 116], "remark": 49, "remov": [48, 58, 82], "repeat": [62, 87, 88, 89, 103], "repetit": 88, "requir": 86, "respect": [50, 71], "result": [50, 51, 71, 72, 74], "risk": [61, 73, 85, 89], "robust": [50, 71], "sampl": [48, 53, 58, 70, 80, 82, 87, 88, 89], "sandbox": 55, "score": [44, 48, 58, 74, 82, 89, 114], "section": [62, 87, 89, 103], "select": [53, 80, 87, 89], "sensit": [56, 67, 68, 78, 79, 103, 117], "set": [52, 86], "sharp": [77, 87], "simpl": [48, 58, 82], "simul": [47, 50, 57, 62, 71, 78], "simultan": 102, "singl": 56, "sourc": [112, 113], "specif": [103, 117], "specifi": [54, 86, 89], "split": [48, 58, 82, 88], "ssm": 87, "stack": 77, "stage": 64, "standard": [50, 69, 71], "start": 114, "step": 70, "studi": 55, "summari": [51, 70, 72, 74], "test": 63, "theori": 103, "time": [69, 70], "train": 70, "treat": 68, "treatment": [51, 59, 60, 61, 65, 66, 68, 72, 73, 74, 76, 85, 87], "tree": [75, 85], "tune": [52, 70, 86], "two": [50, 59, 60, 71], "under": [53, 74, 80], "untun": 70, "up": 52, "us": [47, 49, 52, 54, 57, 70, 86], "user": 84, "util": [37, 38, 39, 40, 41, 42, 43, 46], "v": 64, "valid": 102, "valu": [61, 73, 85, 89], "vanderweel": 79, "variabl": [47, 57], "varianc": 102, "version": 113, "via": 89, "wai": [50, 71], "wealth": [51, 72, 73], "weight": [74, 85], "when": 70, "whl": 113, "within": 70, "without": [77, 88], "workflow": 117, "xgboost": 70, "zero": [50, 71]}})