Search.setIndex({"alltitles": {"(One-Way) Cluster Robust Double Machine Learing": [[65, "(One-Way)-Cluster-Robust-Double-Machine-Learing"], [93, "(One-Way)-Cluster-Robust-Double-Machine-Learing"]], "0. Problem Formulation": [[178, "problem-formulation"]], "1. Data Backend": [[178, "data-backend"]], "1. Formulation of Causal Model & Identification Assumptions": [[103, "1.-Formulation-of-Causal-Model-&-Identification-Assumptions"]], "2. Causal Model": [[178, "causal-model"]], "2. Estimation of Causal Effect": [[103, "2.-Estimation-of-Causal-Effect"]], "3. ML Methods": [[178, "ml-methods"]], "3. Sensitivity Analysis": [[103, "3.-Sensitivity-Analysis"]], "4. Benchmarking Analysis": [[103, "4.-Benchmarking-Analysis"]], "4. DML Specifications": [[178, "dml-specifications"]], "5. Conclusion": [[103, "5.-Conclusion"]], "5. Estimation": [[178, "estimation"]], "6. Inference": [[178, "inference"]], "7. Sensitivity Analysis": [[178, "sensitivity-analysis"]], "A Motivating Example: Two-Way Cluster Robust DML": [[65, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"], [93, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"]], "API Reference": [[0, null]], "ATE Estimation and Sensitivity": [[89, "ATE-Estimation-and-Sensitivity"]], "ATE estimates distribution": [[68, "ATE-estimates-distribution"], [68, "id3"], [104, "ATE-estimates-distribution"], [104, "id3"]], "ATT Estimation": [[71, "ATT-Estimation"], [71, "id1"], [73, "ATT-Estimation"], [74, "ATT-Estimation"], [74, "id1"]], "ATT Estimation: Conditional Parallel Trends": [[72, "ATT-Estimation:-Conditional-Parallel-Trends"]], "ATT Estimation: Unconditional Parallel Trends": [[72, "ATT-Estimation:-Unconditional-Parallel-Trends"]], "ATTE Estimation": [[69, "ATTE-Estimation"], [69, "id2"]], "Acknowledgements": [[173, "acknowledgements"]], "Acknowledgements and Final Remarks": [[64, "Acknowledgements-and-Final-Remarks"]], "Additional Results: CATE estimates": [[96, "Additional-Results:-CATE-estimates"]], "Advanced: External Predictions": [[115, "advanced-external-predictions"]], "Advanced: Global and Local Learners, Stacked Ensembles": [[100, "Advanced:-Global-and-Local-Learners,-Stacked-Ensembles"]], "Aggregated Effects": [[71, "Aggregated-Effects"], [74, "Aggregated-Effects"]], "Aggregation Details": [[71, "Aggregation-Details"], [72, "Aggregation-Details"], [73, "Aggregation-Details"], [74, "Aggregation-Details"]], "Algorithm DML1": [[105, "algorithm-dml1"]], "Algorithm DML2": [[105, "algorithm-dml2"]], "All Combinations": [[71, "All-Combinations"]], "All combinations": [[74, "All-combinations"]], "Anticipation": [[71, "Anticipation"], [74, "Anticipation"]], "Application Results": [[65, "Application-Results"], [93, "Application-Results"]], "Application: 401(k)": [[102, "Application:-401(k)"]], "Assumptions": [[97, "Assumptions"]], "AutoML with less Computation time": [[92, "AutoML-with-less-Computation-time"]], "Average Potential Outcome (APOs)": [[80, "Average-Potential-Outcome-(APOs)"]], "Average Potential Outcomes (APOs)": [[127, "average-potential-outcomes-apos"], [144, "average-potential-outcomes-apos"], [162, "average-potential-outcomes-apos"]], "Average Potential Outcomes (APOs) for Multiple Treatment Levels": [[127, "average-potential-outcomes-apos-for-multiple-treatment-levels"]], "Average Treatment Effect": [[90, "Average-Treatment-Effect"]], "Average Treatment Effect on the Treated": [[90, "Average-Treatment-Effect-on-the-Treated"]], "Basic Tuning Example": [[78, "Basic-Tuning-Example"]], "Basics": [[71, "Basics"], [74, "Basics"]], "Benchmarking": [[162, "benchmarking"]], "Benchmarking Analysis": [[102, "Benchmarking-Analysis"]], "Binary Interactive Regression Model (IRM)": [[127, "binary-interactive-regression-model-irm"], [144, "binary-interactive-regression-model-irm"]], "CATEs for IRM models": [[114, "cates-for-irm-models"]], "CATEs for PLR models": [[114, "cates-for-plr-models"]], "CVaR Treatment Effects": [[85, "CVaR-Treatment-Effects"]], "CVaR of potential outcomes": [[114, "cvar-of-potential-outcomes"]], "CVaR treatment effects": [[114, "cvar-treatment-effects"]], "Causal Analysis with DoubleML": [[103, "Causal-Analysis-with-DoubleML"]], "Causal Contrasts": [[80, "Causal-Contrasts"]], "Causal Research Question": [[72, "Causal-Research-Question"]], "Causal estimation vs. lasso penalty \\lambda": [[86, "Causal-estimation-vs.-lasso-penalty-\\lambda"]], "Chernozhukov et al. (2023): Benchmarking": [[103, "Chernozhukov-et-al.-(2023):-Benchmarking"]], "Citation": [[173, "citation"]], "Cluster Robust Cross Fitting": [[65, "Cluster-Robust-Cross-Fitting"], [93, "Cluster-Robust-Cross-Fitting"]], "Cluster Robust Standard Errors": [[65, "Cluster-Robust-Standard-Errors"], [93, "Cluster-Robust-Standard-Errors"]], "Clustering and double machine learning": [[65, "Clustering-and-double-machine-learning"], [93, "Clustering-and-double-machine-learning"]], "Combined loss vs. lasso penalty \\lambda": [[86, "Combined-loss-vs.-lasso-penalty-\\lambda"]], "Compare Metrics for Nuisance Estimation": [[92, "Compare-Metrics-for-Nuisance-Estimation"]], "Comparing different learners": [[77, "Comparing-different-learners"]], "Comparison": [[78, "Comparison"], [78, "id2"]], "Comparison and summary": [[92, "Comparison-and-summary"]], "Comparison to AutoML with less Computation time and Untuned XGBoost Learners": [[92, "Comparison-to-AutoML-with-less-Computation-time-and-Untuned-XGBoost-Learners"]], "Comparison to did package": [[64, "Comparison-to-did-package"]], "Computation time": [[77, "Computation-time"]], "Conclusion": [[92, "Conclusion"]], "Conditional Value at Risk (CVaR)": [[85, "Conditional-Value-at-Risk-(CVaR)"]], "Conditional average treatment effects (CATEs)": [[114, "conditional-average-treatment-effects-cates"]], "Conditional value at risk (CVaR)": [[114, "conditional-value-at-risk-cvar"], [144, "conditional-value-at-risk-cvar"]], "Confidence bands and multiplier bootstrap for valid simultaneous inference": [[161, "confidence-bands-and-multiplier-bootstrap-for-valid-simultaneous-inference"]], "Control Groups": [[71, "Control-Groups"], [74, "Control-Groups"]], "Correlated Random Effects": [[97, "Correlated-Random-Effects"]], "Coverage Simulation": [[69, "Coverage-Simulation"], [69, "id3"]], "Creating the DoubleMLData Object": [[79, "Creating-the-DoubleMLData-Object"]], "Cross-fitting with K folds": [[143, "cross-fitting-with-k-folds"]], "Cross-fitting, DML algorithms and Neyman-orthogonal score functions": [[175, "cross-fitting-dml-algorithms-and-neyman-orthogonal-score-functions"]], "Custom evaluation metrics": [[77, "Custom-evaluation-metrics"]], "DML: Bonus Data": [[75, null]], "Data": [[66, "Data"], [68, "Data"], [68, "id1"], [69, "Data"], [69, "id1"], [71, "Data"], [72, "Data"], [73, "Data"], [74, "Data"], [83, "Data"], [84, "Data"], [85, "Data"], [87, "Data"], [88, "Data"], [89, "Data"], [90, "Data"], [91, "Data"], [94, "Data"], [95, "Data"], [97, "Data"], [98, "Data"], [99, "Data"], [99, "id1"], [102, "Data"], [104, "Data"], [104, "id1"], [175, "data"]], "Data Backend": [[112, null]], "Data Description": [[71, "Data-Description"], [74, "Data-Description"]], "Data Details": [[71, "Data-Details"], [74, "Data-Details"]], "Data Generating Process (DGP)": [[63, "Data-Generating-Process-(DGP)"], [78, "Data-Generating-Process-(DGP)"], [79, "Data-Generating-Process-(DGP)"], [80, "Data-Generating-Process-(DGP)"], [82, "Data-Generating-Process-(DGP)"]], "Data Generation": [[92, "Data-Generation"]], "Data Simulation": [[62, "Data-Simulation"], [81, "Data-Simulation"]], "Data and Effect Estimation": [[102, "Data-and-Effect-Estimation"]], "Data generating process": [[106, "data-generating-process"]], "Data preprocessing": [[67, "Data-preprocessing"]], "Data with Anticipation": [[71, "Data-with-Anticipation"], [74, "Data-with-Anticipation"]], "Data-Backend for Cluster Data": [[65, "Data-Backend-for-Cluster-Data"], [93, "Data-Backend-for-Cluster-Data"]], "Dataset Generators": [[2, "dataset-generators"]], "Dataset Loaders": [[2, "dataset-loaders"]], "Datasets": [[2, null]], "Define Helper Functions for Plotting": [[65, "Define-Helper-Functions-for-Plotting"], [93, "Define-Helper-Functions-for-Plotting"]], "Define Nuisance Learners": [[78, "Define-Nuisance-Learners"], [78, "id1"]], "Demo Example from did": [[64, "Demo-Example-from-did"]], "Detailed Hyperparameter Tuning Guide": [[78, "Detailed-Hyperparameter-Tuning-Guide"]], "Detailed Tuning Result Analysis": [[78, "Detailed-Tuning-Result-Analysis"]], "Details on Predictive Performance": [[64, "Details-on-Predictive-Performance"]], "Difference-in-Differences": [[76, "difference-in-differences"]], "Difference-in-Differences Models": [[144, "difference-in-differences-models"], [162, "difference-in-differences-models"]], "Difference-in-Differences Models (DID)": [[127, "difference-in-differences-models-did"]], "Difference-in-Differences for Panel Data": [[162, "difference-in-differences-for-panel-data"]], "Difference-in-Differences for repeated cross-sections": [[162, "difference-in-differences-for-repeated-cross-sections"]], "Disclaimer": [[103, "Disclaimer"]], "Double Machine Learning Algorithm": [[173, "double-machine-learning-algorithm"]], "Double Machine Learning Literature": [[176, null]], "Double machine learning algorithms": [[105, null]], "Double/debiased machine learning": [[63, "Double/debiased-machine-learning"], [82, "Double/debiased-machine-learning"], [106, "double-debiased-machine-learning"]], "DoubleML": [[173, null]], "DoubleML Data Class": [[1, null]], "DoubleML Models": [[3, null]], "DoubleML Object": [[102, "DoubleML-Object"]], "DoubleML Workflow": [[178, null]], "DoubleML meets FLAML - How to tune learners automatically within DoubleML": [[92, null]], "DoubleML with TabPFN": [[79, "DoubleML-with-TabPFN"]], "DoubleMLAPOS Tuning Example": [[78, "DoubleMLAPOS-Tuning-Example"]], "DoubleMLDIDData": [[112, "doublemldiddata"]], "DoubleMLData": [[112, "doublemldata"]], "DoubleMLData from arrays and matrices": [[107, "doublemldata-from-arrays-and-matrices"], [112, "doublemldata-from-arrays-and-matrices"]], "DoubleMLData from dataframes": [[107, null], [112, "doublemldata-from-dataframes"]], "DoubleMLPanelData": [[71, "DoubleMLPanelData"], [74, "DoubleMLPanelData"], [112, "doublemlpaneldata"]], "DoubleMLRDDData": [[112, "doublemlrdddata"]], "DoubleMLSSMData": [[112, "doublemlssmdata"]], "Effect Aggregation": [[72, "Effect-Aggregation"], [73, "Effect-Aggregation"], [127, "effect-aggregation"]], "Effect Heterogeneity": [[76, "effect-heterogeneity"], [90, "Effect-Heterogeneity"]], "Empirical coverage vs. lasso penalty \\lambda": [[86, "Empirical-coverage-vs.-lasso-penalty-\\lambda"]], "Estimate DML models without sample-splitting": [[143, "estimate-dml-models-without-sample-splitting"]], "Estimate double/debiased machine learning models": [[175, "estimate-double-debiased-machine-learning-models"]], "Estimating Potential Quantiles and Quantile Treatment Effects": [[95, "Estimating-Potential-Quantiles-and-Quantile-Treatment-Effects"]], "Estimating local quantile treatment effects (LQTEs)": [[95, "Estimating-local-quantile-treatment-effects-(LQTEs)"]], "Estimating the Average Treatment Effect of 401(k) Eligibility on Net Financial Assets": [[66, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"], [94, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"]], "Estimating the treatment effect on the Conditional Value a Risk (CVaR)": [[95, "Estimating-the-treatment-effect-on-the-Conditional-Value-a-Risk-(CVaR)"]], "Estimation": [[68, "Estimation"], [68, "id2"], [104, "Estimation"], [104, "id2"]], "Estimation Approaches": [[97, "Estimation-Approaches"]], "Estimation of Average Potential Outcomes": [[79, "Estimation-of-Average-Potential-Outcomes"]], "Estimation quality vs. \\lambda": [[86, "Estimation-quality-vs.-\\lambda"]], "Evaluate learners": [[115, "evaluate-learners"]], "Event Study Aggregation": [[71, "Event-Study-Aggregation"], [72, "Event-Study-Aggregation"], [73, "Event-Study-Aggregation"], [74, "Event-Study-Aggregation"]], "Example usage": [[108, "example-usage"], [109, "example-usage"], [110, "example-usage"], [111, "example-usage"], [112, "example-usage"], [112, "id6"], [112, "id8"], [112, "id10"]], "Examples": [[76, null]], "Exploiting the Functionalities of did": [[64, "Exploiting-the-Functionalities-of-did"]], "Externally provide a sample splitting / partition": [[143, "externally-provide-a-sample-splitting-partition"]], "Feature preprocessing pipelines": [[97, "Feature-preprocessing-pipelines"]], "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)": [[100, null]], "Fuzzy RDD": [[100, "Fuzzy-RDD"]], "Fuzzy RDD Without Adjustment": [[100, "Fuzzy-RDD-Without-Adjustment"]], "Fuzzy RDD with Flexible Adjustment": [[100, "Fuzzy-RDD-with-Flexible-Adjustment"]], "Fuzzy RDD with Linear Adjustment": [[100, "Fuzzy-RDD-with-Linear-Adjustment"]], "Fuzzy Regression Discontinuity Design": [[127, "fuzzy-regression-discontinuity-design"]], "GATE Estimation and Sensitivity": [[89, "GATE-Estimation-and-Sensitivity"]], "GATET Estimation and Sensitivity": [[89, "GATET-Estimation-and-Sensitivity"]], "GATEs for IRM models": [[114, "gates-for-irm-models"]], "GATEs for PLR models": [[114, "gates-for-plr-models"]], "General Examples": [[76, "general-examples"]], "General algorithm": [[162, "general-algorithm"]], "Generate Fuzzy Data": [[100, "Generate-Fuzzy-Data"]], "Generate Sharp Data": [[100, "Generate-Sharp-Data"]], "Getting Started": [[175, null]], "Group Aggregation": [[71, "Group-Aggregation"], [73, "Group-Aggregation"], [74, "Group-Aggregation"]], "Group Average Treatment Effects (GATEs)": [[87, "Group-Average-Treatment-Effects-(GATEs)"], [88, "Group-Average-Treatment-Effects-(GATEs)"]], "Group average treatment effects (GATEs)": [[114, "group-average-treatment-effects-gates"]], "Group-Time Combinations": [[71, "Group-Time-Combinations"], [74, "Group-Time-Combinations"]], "Heterogeneous treatment effects": [[114, null]], "How to exploit more features of mlr3pipelines in DoubleML": [[67, "How-to-exploit-more-features-of-mlr3pipelines-in-DoubleML"]], "Hyperparameter Tuning": [[78, "Hyperparameter-Tuning"], [78, "id4"]], "Hyperparameter Tuning with Pipelines": [[78, "Hyperparameter-Tuning-with-Pipelines"]], "Hyperparameter tuning": [[97, "Hyperparameter-tuning"], [115, "hyperparameter-tuning"], [115, "r-tune-params"]], "Hyperparameter tuning (Grid Search)": [[115, "hyperparameter-tuning-grid-search"]], "Hyperparameter tuning with pipelines": [[115, "hyperparameter-tuning-with-pipelines"]], "Implementation": [[162, "implementation"]], "Implementation Details": [[127, "implementation-details"]], "Implementation of the double machine learning algorithms": [[105, "implementation-of-the-double-machine-learning-algorithms"]], "Implementation of the score function and the estimate of the causal parameter": [[144, "implementation-of-the-score-function-and-the-estimate-of-the-causal-parameter"]], "Implemented Neyman orthogonal score functions": [[144, "implemented-neyman-orthogonal-score-functions"]], "Imports and Setup": [[79, "Imports-and-Setup"]], "Initialize DoubleMLClusterData object": [[65, "Initialize-DoubleMLClusterData-object"]], "Initialize DoubleMLData object with clusters": [[93, "Initialize-DoubleMLData-object-with-clusters"]], "Initialize the objects of class DoubleMLPLIV": [[65, "Initialize-the-objects-of-class-DoubleMLPLIV"], [93, "Initialize-the-objects-of-class-DoubleMLPLIV"]], "Installing DoubleML": [[174, null]], "Instrumental Variables Directed Acyclic Graph (IV - DAG)": [[62, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"], [81, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"]], "Interactive IV Model (IIVM)": [[66, "Interactive-IV-Model-(IIVM)"], [94, "Interactive-IV-Model-(IIVM)"]], "Interactive IV model (IIVM)": [[127, "interactive-iv-model-iivm"], [144, "interactive-iv-model-iivm"]], "Interactive Regression Model (IRM)": [[66, "Interactive-Regression-Model-(IRM)"], [87, "Interactive-Regression-Model-(IRM)"], [94, "Interactive-Regression-Model-(IRM)"], [98, "Interactive-Regression-Model-(IRM)"]], "Interactive regression model (IRM)": [[162, "interactive-regression-model-irm"]], "Interactive regression models (IRM)": [[127, "interactive-regression-models-irm"], [144, "interactive-regression-models-irm"], [162, "interactive-regression-models-irm"]], "Key Takeaways": [[79, "Key-Takeaways"]], "Key arguments": [[108, null], [109, null], [110, null], [111, null], [112, "key-arguments"], [112, "id5"], [112, "id7"], [112, "id9"]], "Learners and Hyperparameters": [[90, "Learners-and-Hyperparameters"]], "Learners to estimate the nuisance models": [[175, "learners-to-estimate-the-nuisance-models"]], "Learners, hyperparameters and hyperparameter tuning": [[115, null]], "Linear Covariate Adjustment": [[71, "Linear-Covariate-Adjustment"], [74, "Linear-Covariate-Adjustment"]], "Load Data": [[103, "Load-Data"]], "Load and Process Data": [[65, "Load-and-Process-Data"], [93, "Load-and-Process-Data"]], "Load bonus data using the dml datasets module": [[75, "Load-bonus-data-using-the-dml-datasets-module"]], "Local Average Treatment Effects of 401(k) Participation on Net Financial Assets": [[66, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"], [94, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"]], "Local Potential Quantile Estimation": [[99, "Local-Potential-Quantile-Estimation"]], "Local Potential Quantiles (LPQs)": [[99, "Local-Potential-Quantiles-(LPQs)"]], "Local Quantile Treatment Effects (LQTEs)": [[99, "Local-Quantile-Treatment-Effects-(LQTEs)"]], "Local potential quantiles (LPQs)": [[144, "local-potential-quantiles-lpqs"]], "Logistic partial linear regression (LPLR)": [[144, "logistic-partial-linear-regression-lplr"]], "Logistic partially linear regression model (LPLR)": [[127, "logistic-partially-linear-regression-model-lplr"]], "Machine Learning Methods Comparison": [[79, "Machine-Learning-Methods-Comparison"]], "Main Features": [[173, "main-features"]], "Minimum requirements for learners": [[115, "minimum-requirements-for-learners"], [115, "r-learner-req"]], "Missingness at Random": [[127, "missingness-at-random"], [144, "missingness-at-random"]], "Model": [[91, "Model"], [97, "Model"]], "Model Performance Evaluation": [[79, "Model-Performance-Evaluation"]], "Model-specific implementations": [[162, "model-specific-implementations"]], "Models": [[127, null]], "Motivation": [[65, "Motivation"], [93, "Motivation"]], "Multiple Average Potential Outcome Models (APOS)": [[80, "Multiple-Average-Potential-Outcome-Models-(APOS)"]], "Naive estimation": [[62, "Naive-estimation"], [81, "Naive-estimation"]], "No Clustering / Zero-Way Clustering": [[65, "No-Clustering-/-Zero-Way-Clustering"], [93, "No-Clustering-/-Zero-Way-Clustering"]], "Nonignorable Nonresponse": [[127, "nonignorable-nonresponse"], [144, "nonignorable-nonresponse"]], "One-Way Clustering with Respect to the Market": [[65, "One-Way-Clustering-with-Respect-to-the-Market"], [93, "One-Way-Clustering-with-Respect-to-the-Market"]], "One-Way Clustering with Respect to the Product": [[65, "One-Way-Clustering-with-Respect-to-the-Product"], [93, "One-Way-Clustering-with-Respect-to-the-Product"]], "One-dimensional Example": [[83, "One-dimensional-Example"], [84, "One-dimensional-Example"]], "Outcome missing at random (MAR)": [[68, "Outcome-missing-at-random-(MAR)"], [104, "Outcome-missing-at-random-(MAR)"]], "Outcome missing under nonignorable nonresponse": [[68, "Outcome-missing-under-nonignorable-nonresponse"], [104, "Outcome-missing-under-nonignorable-nonresponse"]], "Overcoming regularization bias by orthogonalization": [[63, "Overcoming-regularization-bias-by-orthogonalization"], [82, "Overcoming-regularization-bias-by-orthogonalization"], [106, "overcoming-regularization-bias-by-orthogonalization"]], "Panel Data": [[127, "id15"], [129, null], [144, "panel-data"], [144, "id3"], [162, "panel-data"]], "Panel Data (Repeated Outcomes)": [[69, "Panel-Data-(Repeated-Outcomes)"]], "Panel data": [[127, "panel-data"]], "Parameter tuning": [[67, "Parameter-tuning"]], "Parameters & Implementation": [[127, "parameters-implementation"]], "Partialling out score": [[63, "Partialling-out-score"], [82, "Partialling-out-score"], [106, "partialling-out-score"]], "Partially Linear Regression Model (PLR)": [[66, "Partially-Linear-Regression-Model-(PLR)"], [88, "Partially-Linear-Regression-Model-(PLR)"], [94, "Partially-Linear-Regression-Model-(PLR)"]], "Partially linear IV regression model (PLIV)": [[127, "partially-linear-iv-regression-model-pliv"], [144, "partially-linear-iv-regression-model-pliv"]], "Partially linear models (PLM)": [[127, "partially-linear-models-plm"], [144, "partially-linear-models-plm"], [162, "partially-linear-models-plm"]], "Partially linear panel regression (PLPR)": [[144, "partially-linear-panel-regression-plpr"]], "Partially linear panel regression model (PLPR)": [[127, "partially-linear-panel-regression-model-plpr"]], "Partially linear regression model (PLR)": [[127, "partially-linear-regression-model-plr"], [144, "partially-linear-regression-model-plr"], [162, "partially-linear-regression-model-plr"]], "Performance Summary and Insights": [[79, "Performance-Summary-and-Insights"]], "Plot Coefficients and 95% Confidence Intervals": [[92, "Plot-Coefficients-and-95%-Confidence-Intervals"]], "Policy Learning with Trees": [[98, "Policy-Learning-with-Trees"], [114, "policy-learning-with-trees"]], "Potential Quantile Estimation": [[99, "Potential-Quantile-Estimation"]], "Potential Quantiles (PQs)": [[99, "Potential-Quantiles-(PQs)"]], "Potential quantiles (PQs)": [[114, "potential-quantiles-pqs"], [144, "potential-quantiles-pqs"]], "Python: Average Potential Outcome (APO) Models": [[80, null]], "Python: Basic Instrumental Variables calculation": [[81, null]], "Python: Basics of Double Machine Learning": [[82, null]], "Python: Building the package from source": [[174, "python-building-the-package-from-source"]], "Python: Case studies": [[76, "python-case-studies"]], "Python: Causal Machine Learning with TabPFN": [[79, null]], "Python: Choice of learners": [[77, null]], "Python: Cluster Robust Double Machine Learning": [[93, null]], "Python: Conditional Average Treatment Effects (CATEs) for IRM models": [[83, null]], "Python: Conditional Average Treatment Effects (CATEs) for PLR models": [[84, null]], "Python: Conditional Value at Risk of potential outcomes": [[85, null]], "Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments": [[101, null]], "Python: Difference-in-Differences": [[69, null]], "Python: Difference-in-Differences Pre-Testing": [[70, null]], "Python: First Stage and Causal Estimation": [[86, null]], "Python: GATE Sensitivity Analysis": [[89, null]], "Python: Group Average Treatment Effects (GATEs) for IRM models": [[87, null]], "Python: Group Average Treatment Effects (GATEs) for PLR models": [[88, null]], "Python: Hyperparametertuning with Optuna": [[78, null]], "Python: IRM and APO Model Comparison": [[90, null]], "Python: Impact of 401(k) on Financial Wealth": [[94, null]], "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)": [[95, null]], "Python: Installing DoubleML": [[174, "python-installing-doubleml"]], "Python: Installing a released version from a .whl file": [[174, "python-installing-a-released-version-from-a-whl-file"]], "Python: Installing the latest release from pip or conda": [[174, "python-installing-the-latest-release-from-pip-or-conda"]], "Python: Learners and hyperparameters": [[115, "python-learners-and-hyperparameters"]], "Python: Log-Odds Effects for Logistic PLR models": [[91, null]], "Python: Optional Dependencies": [[174, "python-optional-dependencies"]], "Python: PLM and IRM for Multiple Treatments": [[96, null]], "Python: Panel Data Introduction": [[73, null]], "Python: Panel Data with Multiple Time Periods": [[71, null]], "Python: Policy Learning with Trees": [[98, null]], "Python: Potential Quantiles and Quantile Treatment Effects": [[99, null]], "Python: Real-Data Example for Multi-Period Difference-in-Differences": [[72, null]], "Python: Repeated Cross-Sectional Data with Multiple Time Periods": [[74, null]], "Python: Sample Selection Models": [[104, null]], "Python: Sensitivity Analysis": [[102, null]], "Python: Sensitivity Analysis for Causal ML": [[103, null]], "Python: Static Panel Models with Fixed Effects": [[97, null]], "Quantile Treatment Effects (QTEs)": [[99, "Quantile-Treatment-Effects-(QTEs)"]], "Quantile treatment effects (QTEs)": [[114, "quantile-treatment-effects-qtes"]], "Quantiles": [[114, "quantiles"]], "R: Basic Instrumental Variables Calculation": [[62, null]], "R: Basics of Double Machine Learning": [[63, null]], "R: Case studies": [[76, "r-case-studies"]], "R: Cluster Robust Double Machine Learning": [[65, null]], "R: DoubleML for Difference-in-Differences": [[64, null]], "R: Ensemble Learners and More with mlr3pipelines": [[67, null]], "R: Impact of 401(k) on Financial Wealth": [[66, null]], "R: Installing DoubleML": [[174, "r-installing-doubleml"]], "R: Installing the development version from GitHub": [[174, "r-installing-the-development-version-from-github"]], "R: Installing the latest release from CRAN": [[174, "r-installing-the-latest-release-from-cran"]], "R: Learners and hyperparameters": [[115, "r-learners-and-hyperparameters"]], "R: Sample Selection Models": [[68, null]], "Ranking Treatment Effects under Treatment Propensity and Treatment Effect Heterogeneity": [[96, "Ranking-Treatment-Effects-under-Treatment-Propensity-and-Treatment-Effect-Heterogeneity"]], "Real-Data Application": [[65, "Real-Data-Application"], [93, "Real-Data-Application"]], "References": [[62, "References"], [64, "References"], [65, "References"], [67, "References"], [77, "References"], [81, "References"], [86, "References"], [92, "References"], [93, "References"], [96, "References"], [101, "References"], [103, "References"], [106, "references"], [115, "references"], [125, null], [143, "references"], [161, "references"], [173, "references"], [175, "references"]], "Regression Discontinuity Designs (RDD)": [[127, "regression-discontinuity-designs-rdd"]], "Regularization Bias in Simple ML-Approaches": [[63, "Regularization-Bias-in-Simple-ML-Approaches"], [82, "Regularization-Bias-in-Simple-ML-Approaches"]], "Regularization bias in simple ML-approaches": [[106, "regularization-bias-in-simple-ml-approaches"]], "Release Notes": [[177, null]], "Repeated Cross-Sectional Data": [[69, "Repeated-Cross-Sectional-Data"], [144, "repeated-cross-sectional-data"], [144, "id4"], [162, "repeated-cross-sectional-data"]], "Repeated cross-fitting with K folds and M repetitions": [[143, "repeated-cross-fitting-with-k-folds-and-m-repetitions"]], "Repeated cross-sections": [[127, "repeated-cross-sections"], [127, "id16"], [129, "repeated-cross-sections"]], "Running a small simulation": [[101, "Running-a-small-simulation"]], "Sample Selection Models": [[144, "sample-selection-models"]], "Sample Selection Models (SSM)": [[127, "sample-selection-models-ssm"]], "Sample splitting to remove bias induced by overfitting": [[63, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [82, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [106, "sample-splitting-to-remove-bias-induced-by-overfitting"]], "Sample-splitting without cross-fitting": [[143, "sample-splitting-without-cross-fitting"]], "Sample-splitting, cross-fitting and repeated cross-fitting": [[143, null]], "Sandbox/Archive": [[76, "sandbox-archive"]], "Score Mixin Classes for DoubleML Models": [[60, null]], "Score functions": [[144, null]], "Selected Combinations": [[71, "Selected-Combinations"], [74, "Selected-Combinations"]], "Sensitivity Analysis": [[71, "Sensitivity-Analysis"], [74, "Sensitivity-Analysis"], [80, "Sensitivity-Analysis"], [90, "Sensitivity-Analysis"], [102, "Sensitivity-Analysis"], [102, "id1"]], "Sensitivity Analysis with IRM": [[102, "Sensitivity-Analysis-with-IRM"]], "Sensitivity analysis": [[162, null]], "Set up learners based on mlr3pipelines": [[67, "Set-up-learners-based-on-mlr3pipelines"]], "Sharp RDD": [[100, "Sharp-RDD"]], "Sharp RDD Without Adjustment": [[100, "Sharp-RDD-Without-Adjustment"]], "Sharp RDD with Flexible Adjustment": [[100, "Sharp-RDD-with-Flexible-Adjustment"]], "Sharp RDD with Linear Adjustment": [[100, "Sharp-RDD-with-Linear-Adjustment"]], "Sharp Regression Discontinuity Design": [[127, "sharp-regression-discontinuity-design"]], "Simulate two-way cluster data": [[65, "Simulate-two-way-cluster-data"], [93, "Simulate-two-way-cluster-data"]], "Simulation Example": [[102, "Simulation-Example"]], "Simultaneous inference over different DoubleML models (advanced)": [[161, "simultaneous-inference-over-different-doubleml-models-advanced"]], "Single Average Potential Outcome Models (APO)": [[80, "Single-Average-Potential-Outcome-Models-(APO)"]], "Source code and maintenance": [[173, "source-code-and-maintenance"]], "Special Data Types": [[112, "special-data-types"]], "Specify learner and estimate causal parameter: IRM model with Lasso as learner": [[75, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: IRM model with random forest as learner": [[75, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-random-forest-as-learner"]], "Specify learner and estimate causal parameter: PLR model with Lasso as learner": [[75, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: PLR model with random forest as learner": [[75, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-random-forest-as-learner"]], "Specifying alternative score functions via callables": [[144, "specifying-alternative-score-functions-via-callables"]], "Specifying learners and set hyperparameters": [[115, "specifying-learners-and-set-hyperparameters"], [115, "r-set-params"]], "Standard approach": [[77, "Standard-approach"]], "Step 1: Custom API for FLAML Models within DoubleML": [[92, "Step-1:-Custom-API-for-FLAML-Models-within-DoubleML"]], "Step 1: Initialize and Train the AutoML Models:": [[92, "Step-1:-Initialize-and-Train-the-AutoML-Models:"]], "Step 2: Evaluate the Tuned Models": [[92, "Step-2:-Evaluate-the-Tuned-Models"]], "Step 2: Using the API when calling DoubleML\u2019s .fit() Method": [[92, "Step-2:-Using-the-API-when-calling-DoubleML's-.fit()-Method"]], "Step 3: Create and Fit DoubleML Model": [[92, "Step-3:-Create-and-Fit-DoubleML-Model"]], "Summary Figure": [[96, "Summary-Figure"]], "Summary of Results": [[66, "Summary-of-Results"], [94, "Summary-of-Results"]], "The Augmented Inverse Propensity Weighting Model estimates the ATE under arbitrary effect and propensity score heterogeneity": [[96, "The-Augmented-Inverse-Propensity-Weighting-Model-estimates-the-ATE-under-arbitrary-effect-and-propensity-score-heterogeneity"]], "The Data Backend: DoubleMLData": [[66, "The-Data-Backend:-DoubleMLData"], [94, "The-Data-Backend:-DoubleMLData"]], "The DoubleML package": [[66, "The-DoubleML-package"], [94, "The-DoubleML-package"]], "The Partially Linear Model performs overlap weighting": [[96, "The-Partially-Linear-Model-performs-overlap-weighting"]], "The basics of double/debiased machine learning": [[106, null]], "The causal model": [[175, "the-causal-model"]], "The data-backend DoubleMLData": [[175, "the-data-backend-doublemldata"]], "Theory": [[162, "theory"]], "Time Aggregation": [[71, "Time-Aggregation"], [73, "Time-Aggregation"], [74, "Time-Aggregation"]], "Transformation Approaches": [[97, "Transformation-Approaches"]], "Tuning on the Folds": [[92, "Tuning-on-the-Folds"]], "Tuning on the full Sample": [[92, "Tuning-on-the-full-Sample"]], "Two treatment periods": [[127, "two-treatment-periods"], [144, "two-treatment-periods"], [162, "two-treatment-periods"]], "Two-Dimensional Example": [[83, "Two-Dimensional-Example"], [84, "Two-Dimensional-Example"]], "Two-Way Clustering with Respect to Product and Market": [[65, "Two-Way-Clustering-with-Respect-to-Product-and-Market"], [93, "Two-Way-Clustering-with-Respect-to-Product-and-Market"]], "Universal Base Period": [[71, "Universal-Base-Period"], [74, "Universal-Base-Period"]], "Untuned (default parameter) XGBoost": [[92, "Untuned-(default-parameter)-XGBoost"]], "Untuned Model": [[78, "Untuned-Model"], [78, "id3"]], "Untuned Model with Pipeline": [[78, "Untuned-Model-with-Pipeline"]], "Use ensemble learners based on mlr3pipelines": [[67, "Use-ensemble-learners-based-on-mlr3pipelines"]], "User Guide": [[113, null]], "Using DoubleML": [[62, "Using-DoubleML"], [81, "Using-DoubleML"]], "Using ML for DiD: Integrating DoubleML in did": [[64, "Using-ML-for-DiD:-Integrating-DoubleML-in-did"]], "Using learners from mlr3, mlr3learners and mlr3extralearners": [[67, "Using-learners-from-mlr3,-mlr3learners-and-mlr3extralearners"]], "Using pipelines to construct learners": [[115, "using-pipelines-to-construct-learners"]], "Utility Classes": [[61, "utility-classes"]], "Utility Classes and Functions": [[61, null]], "Utility Functions": [[61, "utility-functions"]], "VanderWeele and Arah (2011): Benchmarking": [[103, "VanderWeele-and-Arah-(2011):-Benchmarking"]], "Variance estimation": [[161, "variance-estimation"]], "Variance estimation and confidence intervals": [[161, null]], "Visualizations": [[91, "Visualizations"]], "Visualizing Average Potential Outcomes": [[79, "Visualizing-Average-Potential-Outcomes"]], "Visualizing Average Treatment Effects": [[79, "Visualizing-Average-Treatment-Effects"]], "Visualizing the Treatment Effect Structure": [[79, "Visualizing-the-Treatment-Effect-Structure"]], "Weighted Average Treatment Effects": [[114, "weighted-average-treatment-effects"]], "cre_general": [[97, "cre_general"]], "cre_normal": [[97, "cre_normal"]], "doubleml.data.DoubleMLClusterData": [[4, null]], "doubleml.data.DoubleMLDIDData": [[5, null]], "doubleml.data.DoubleMLData": [[6, null]], "doubleml.data.DoubleMLPanelData": [[7, null]], "doubleml.data.DoubleMLRDDData": [[8, null]], "doubleml.data.DoubleMLSSMData": [[9, null]], "doubleml.datasets.fetch_401K": [[10, null]], "doubleml.datasets.fetch_bonus": [[11, null]], "doubleml.did": [[3, "doubleml-did"]], "doubleml.did.DoubleMLDID": [[12, null]], "doubleml.did.DoubleMLDIDAggregation": [[13, null]], "doubleml.did.DoubleMLDIDBinary": [[14, null]], "doubleml.did.DoubleMLDIDCS": [[15, null]], "doubleml.did.DoubleMLDIDMulti": [[16, null]], "doubleml.did.datasets.make_did_CS2021": [[17, null]], "doubleml.did.datasets.make_did_SZ2020": [[18, null]], "doubleml.did.datasets.make_did_cs_CS2021": [[19, null]], "doubleml.double_ml_score_mixins.LinearScoreMixin": [[20, null]], "doubleml.double_ml_score_mixins.NonLinearScoreMixin": [[21, null]], "doubleml.irm": [[3, "doubleml-irm"]], "doubleml.irm.DoubleMLAPO": [[22, null]], "doubleml.irm.DoubleMLAPOS": [[23, null]], "doubleml.irm.DoubleMLCVAR": [[24, null]], "doubleml.irm.DoubleMLIIVM": [[25, null]], "doubleml.irm.DoubleMLIRM": [[26, null]], "doubleml.irm.DoubleMLLPQ": [[27, null]], "doubleml.irm.DoubleMLPQ": [[28, null]], "doubleml.irm.DoubleMLQTE": [[29, null]], "doubleml.irm.DoubleMLSSM": [[30, null]], "doubleml.irm.datasets.make_confounded_irm_data": [[31, null]], "doubleml.irm.datasets.make_heterogeneous_data": [[32, null]], "doubleml.irm.datasets.make_iivm_data": [[33, null]], "doubleml.irm.datasets.make_irm_data": [[34, null]], "doubleml.irm.datasets.make_irm_data_discrete_treatments": [[35, null]], "doubleml.irm.datasets.make_ssm_data": [[36, null]], "doubleml.plm": [[3, "doubleml-plm"]], "doubleml.plm.DoubleMLLPLR": [[37, null]], "doubleml.plm.DoubleMLPLIV": [[38, null]], "doubleml.plm.DoubleMLPLPR": [[39, null]], "doubleml.plm.DoubleMLPLR": [[40, null]], "doubleml.plm.datasets.make_confounded_plr_data": [[41, null]], "doubleml.plm.datasets.make_lplr_LZZ2020": [[42, null]], "doubleml.plm.datasets.make_pliv_CHS2015": [[43, null]], "doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021": [[44, null]], "doubleml.plm.datasets.make_plpr_CP2025": [[45, null]], "doubleml.plm.datasets.make_plr_CCDDHNR2018": [[46, null]], "doubleml.plm.datasets.make_plr_turrell2018": [[47, null]], "doubleml.rdd": [[3, "doubleml-rdd"]], "doubleml.rdd.RDFlex": [[48, null]], "doubleml.rdd.datasets.make_simple_rdd_data": [[49, null]], "doubleml.utils.DMLDummyClassifier": [[50, null]], "doubleml.utils.DMLDummyRegressor": [[51, null]], "doubleml.utils.DMLOptunaResult": [[52, null]], "doubleml.utils.DoubleMLBLP": [[53, null]], "doubleml.utils.DoubleMLPolicyTree": [[54, null]], "doubleml.utils.GlobalClassifier": [[55, null]], "doubleml.utils.GlobalRegressor": [[56, null]], "doubleml.utils.PSProcessor": [[57, null]], "doubleml.utils.PSProcessorConfig": [[58, null]], "doubleml.utils.gain_statistics": [[59, null]], "fd_exact": [[97, "fd_exact"]], "wg_approx": [[97, "wg_approx"]]}, "docnames": ["api/api", "api/data_class", "api/datasets", "api/dml_models", "api/generated/doubleml.data.DoubleMLClusterData", "api/generated/doubleml.data.DoubleMLDIDData", "api/generated/doubleml.data.DoubleMLData", "api/generated/doubleml.data.DoubleMLPanelData", "api/generated/doubleml.data.DoubleMLRDDData", "api/generated/doubleml.data.DoubleMLSSMData", "api/generated/doubleml.datasets.fetch_401K", "api/generated/doubleml.datasets.fetch_bonus", "api/generated/doubleml.did.DoubleMLDID", "api/generated/doubleml.did.DoubleMLDIDAggregation", "api/generated/doubleml.did.DoubleMLDIDBinary", "api/generated/doubleml.did.DoubleMLDIDCS", "api/generated/doubleml.did.DoubleMLDIDMulti", "api/generated/doubleml.did.datasets.make_did_CS2021", "api/generated/doubleml.did.datasets.make_did_SZ2020", "api/generated/doubleml.did.datasets.make_did_cs_CS2021", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin", "api/generated/doubleml.irm.DoubleMLAPO", "api/generated/doubleml.irm.DoubleMLAPOS", "api/generated/doubleml.irm.DoubleMLCVAR", "api/generated/doubleml.irm.DoubleMLIIVM", "api/generated/doubleml.irm.DoubleMLIRM", "api/generated/doubleml.irm.DoubleMLLPQ", "api/generated/doubleml.irm.DoubleMLPQ", "api/generated/doubleml.irm.DoubleMLQTE", "api/generated/doubleml.irm.DoubleMLSSM", "api/generated/doubleml.irm.datasets.make_confounded_irm_data", "api/generated/doubleml.irm.datasets.make_heterogeneous_data", "api/generated/doubleml.irm.datasets.make_iivm_data", "api/generated/doubleml.irm.datasets.make_irm_data", "api/generated/doubleml.irm.datasets.make_irm_data_discrete_treatments", "api/generated/doubleml.irm.datasets.make_ssm_data", "api/generated/doubleml.plm.DoubleMLLPLR", "api/generated/doubleml.plm.DoubleMLPLIV", "api/generated/doubleml.plm.DoubleMLPLPR", "api/generated/doubleml.plm.DoubleMLPLR", "api/generated/doubleml.plm.datasets.make_confounded_plr_data", "api/generated/doubleml.plm.datasets.make_lplr_LZZ2020", "api/generated/doubleml.plm.datasets.make_pliv_CHS2015", "api/generated/doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021", "api/generated/doubleml.plm.datasets.make_plpr_CP2025", "api/generated/doubleml.plm.datasets.make_plr_CCDDHNR2018", "api/generated/doubleml.plm.datasets.make_plr_turrell2018", "api/generated/doubleml.rdd.RDFlex", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data", "api/generated/doubleml.utils.DMLDummyClassifier", "api/generated/doubleml.utils.DMLDummyRegressor", "api/generated/doubleml.utils.DMLOptunaResult", "api/generated/doubleml.utils.DoubleMLBLP", "api/generated/doubleml.utils.DoubleMLPolicyTree", "api/generated/doubleml.utils.GlobalClassifier", "api/generated/doubleml.utils.GlobalRegressor", "api/generated/doubleml.utils.PSProcessor", "api/generated/doubleml.utils.PSProcessorConfig", "api/generated/doubleml.utils.gain_statistics", "api/mixins", "api/utility", "examples/R_double_ml_basic_iv", "examples/R_double_ml_basics", "examples/R_double_ml_did", "examples/R_double_ml_multiway_cluster", "examples/R_double_ml_pension", "examples/R_double_ml_pipeline", "examples/R_double_ml_ssm", "examples/did/py_did", "examples/did/py_did_pretest", "examples/did/py_panel", "examples/did/py_panel_data_example", "examples/did/py_panel_simple", "examples/did/py_rep_cs", "examples/double_ml_bonus_data", "examples/index", "examples/learners/py_learner", "examples/learners/py_optuna", "examples/learners/py_tabpfn", "examples/py_double_ml_apo", "examples/py_double_ml_basic_iv", "examples/py_double_ml_basics", "examples/py_double_ml_cate", "examples/py_double_ml_cate_plr", "examples/py_double_ml_cvar", "examples/py_double_ml_firststage", "examples/py_double_ml_gate", "examples/py_double_ml_gate_plr", "examples/py_double_ml_gate_sensitivity", "examples/py_double_ml_irm_vs_apo", "examples/py_double_ml_lplr", "examples/py_double_ml_meets_flaml", "examples/py_double_ml_multiway_cluster", "examples/py_double_ml_pension", "examples/py_double_ml_pension_qte", "examples/py_double_ml_plm_irm_hetfx", "examples/py_double_ml_plpr", "examples/py_double_ml_policy_tree", "examples/py_double_ml_pq", "examples/py_double_ml_rdflex", "examples/py_double_ml_robust_iv", "examples/py_double_ml_sensitivity", "examples/py_double_ml_sensitivity_booking", "examples/py_double_ml_ssm", "guide/algorithms", "guide/basics", "guide/data/base_data", "guide/data/did_data", "guide/data/panel_data", "guide/data/rdd_data", "guide/data/ssm_data", "guide/data_backend", "guide/guide", "guide/heterogeneity", "guide/learners", "guide/learners/python/evaluate_learners", "guide/learners/python/external_preds", "guide/learners/python/minimum_req", "guide/learners/python/set_hyperparams", "guide/learners/python/tune_hyperparams", "guide/learners/python/tune_hyperparams_old", "guide/learners/r/minimum_req", "guide/learners/r/pipelines", "guide/learners/r/set_hyperparams", "guide/learners/r/tune_and_pipelines", "guide/learners/r/tune_hyperparams", "guide/models", "guide/models/did/did_aggregation", "guide/models/did/did_binary", "guide/models/did/did_cs", "guide/models/did/did_implementation", "guide/models/did/did_pa", "guide/models/did/did_setup", "guide/models/irm/apo", "guide/models/irm/apos", "guide/models/irm/iivm", "guide/models/irm/irm", "guide/models/plm/lplr", "guide/models/plm/pliv", "guide/models/plm/plpr", "guide/models/plm/plr", "guide/models/ssm/ssm", "guide/resampling", "guide/scores", "guide/scores/did/did_cs_binary_score", "guide/scores/did/did_cs_score", "guide/scores/did/did_pa_binary_score", "guide/scores/did/did_pa_score", "guide/scores/irm/apo_score", "guide/scores/irm/cvar_score", "guide/scores/irm/iivm_score", "guide/scores/irm/irm_score", "guide/scores/irm/lpq_score", "guide/scores/irm/pq_score", "guide/scores/plm/lplr_score", "guide/scores/plm/pliv_score", "guide/scores/plm/plpr_score", "guide/scores/plm/plr_score", "guide/scores/ssm/mar_score", "guide/scores/ssm/nr_score", "guide/se_confint", "guide/sensitivity", "guide/sensitivity/benchmarking", "guide/sensitivity/did/did_cs_binary_sensitivity", "guide/sensitivity/did/did_cs_sensitivity", "guide/sensitivity/did/did_pa_binary_sensitivity", "guide/sensitivity/did/did_pa_sensitivity", "guide/sensitivity/implementation", "guide/sensitivity/irm/apo_sensitivity", "guide/sensitivity/irm/irm_sensitivity", "guide/sensitivity/plm/plr_sensitivity", "guide/sensitivity/theory", "index", "intro/install", "intro/intro", "literature/literature", "release/release", "workflow/workflow"], "envversion": {"nbsphinx": 4, "sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1}, "filenames": ["api/api.rst", "api/data_class.rst", "api/datasets.rst", "api/dml_models.rst", "api/generated/doubleml.data.DoubleMLClusterData.rst", "api/generated/doubleml.data.DoubleMLDIDData.rst", "api/generated/doubleml.data.DoubleMLData.rst", "api/generated/doubleml.data.DoubleMLPanelData.rst", "api/generated/doubleml.data.DoubleMLRDDData.rst", "api/generated/doubleml.data.DoubleMLSSMData.rst", "api/generated/doubleml.datasets.fetch_401K.rst", "api/generated/doubleml.datasets.fetch_bonus.rst", "api/generated/doubleml.did.DoubleMLDID.rst", "api/generated/doubleml.did.DoubleMLDIDAggregation.rst", "api/generated/doubleml.did.DoubleMLDIDBinary.rst", "api/generated/doubleml.did.DoubleMLDIDCS.rst", "api/generated/doubleml.did.DoubleMLDIDMulti.rst", "api/generated/doubleml.did.datasets.make_did_CS2021.rst", "api/generated/doubleml.did.datasets.make_did_SZ2020.rst", "api/generated/doubleml.did.datasets.make_did_cs_CS2021.rst", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin.rst", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin.rst", "api/generated/doubleml.irm.DoubleMLAPO.rst", "api/generated/doubleml.irm.DoubleMLAPOS.rst", "api/generated/doubleml.irm.DoubleMLCVAR.rst", "api/generated/doubleml.irm.DoubleMLIIVM.rst", "api/generated/doubleml.irm.DoubleMLIRM.rst", "api/generated/doubleml.irm.DoubleMLLPQ.rst", "api/generated/doubleml.irm.DoubleMLPQ.rst", "api/generated/doubleml.irm.DoubleMLQTE.rst", "api/generated/doubleml.irm.DoubleMLSSM.rst", "api/generated/doubleml.irm.datasets.make_confounded_irm_data.rst", "api/generated/doubleml.irm.datasets.make_heterogeneous_data.rst", "api/generated/doubleml.irm.datasets.make_iivm_data.rst", "api/generated/doubleml.irm.datasets.make_irm_data.rst", "api/generated/doubleml.irm.datasets.make_irm_data_discrete_treatments.rst", "api/generated/doubleml.irm.datasets.make_ssm_data.rst", "api/generated/doubleml.plm.DoubleMLLPLR.rst", "api/generated/doubleml.plm.DoubleMLPLIV.rst", "api/generated/doubleml.plm.DoubleMLPLPR.rst", "api/generated/doubleml.plm.DoubleMLPLR.rst", "api/generated/doubleml.plm.datasets.make_confounded_plr_data.rst", "api/generated/doubleml.plm.datasets.make_lplr_LZZ2020.rst", "api/generated/doubleml.plm.datasets.make_pliv_CHS2015.rst", "api/generated/doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021.rst", "api/generated/doubleml.plm.datasets.make_plpr_CP2025.rst", "api/generated/doubleml.plm.datasets.make_plr_CCDDHNR2018.rst", "api/generated/doubleml.plm.datasets.make_plr_turrell2018.rst", "api/generated/doubleml.rdd.RDFlex.rst", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data.rst", "api/generated/doubleml.utils.DMLDummyClassifier.rst", "api/generated/doubleml.utils.DMLDummyRegressor.rst", "api/generated/doubleml.utils.DMLOptunaResult.rst", "api/generated/doubleml.utils.DoubleMLBLP.rst", "api/generated/doubleml.utils.DoubleMLPolicyTree.rst", "api/generated/doubleml.utils.GlobalClassifier.rst", "api/generated/doubleml.utils.GlobalRegressor.rst", "api/generated/doubleml.utils.PSProcessor.rst", "api/generated/doubleml.utils.PSProcessorConfig.rst", "api/generated/doubleml.utils.gain_statistics.rst", "api/mixins.rst", "api/utility.rst", "examples/R_double_ml_basic_iv.ipynb", "examples/R_double_ml_basics.ipynb", "examples/R_double_ml_did.ipynb", "examples/R_double_ml_multiway_cluster.ipynb", "examples/R_double_ml_pension.ipynb", "examples/R_double_ml_pipeline.ipynb", "examples/R_double_ml_ssm.ipynb", "examples/did/py_did.ipynb", "examples/did/py_did_pretest.ipynb", "examples/did/py_panel.ipynb", "examples/did/py_panel_data_example.ipynb", "examples/did/py_panel_simple.ipynb", "examples/did/py_rep_cs.ipynb", "examples/double_ml_bonus_data.ipynb", "examples/index.rst", "examples/learners/py_learner.ipynb", "examples/learners/py_optuna.ipynb", "examples/learners/py_tabpfn.ipynb", "examples/py_double_ml_apo.ipynb", "examples/py_double_ml_basic_iv.ipynb", "examples/py_double_ml_basics.ipynb", "examples/py_double_ml_cate.ipynb", "examples/py_double_ml_cate_plr.ipynb", "examples/py_double_ml_cvar.ipynb", "examples/py_double_ml_firststage.ipynb", "examples/py_double_ml_gate.ipynb", "examples/py_double_ml_gate_plr.ipynb", "examples/py_double_ml_gate_sensitivity.ipynb", "examples/py_double_ml_irm_vs_apo.ipynb", "examples/py_double_ml_lplr.ipynb", "examples/py_double_ml_meets_flaml.ipynb", "examples/py_double_ml_multiway_cluster.ipynb", "examples/py_double_ml_pension.ipynb", "examples/py_double_ml_pension_qte.ipynb", "examples/py_double_ml_plm_irm_hetfx.ipynb", "examples/py_double_ml_plpr.ipynb", "examples/py_double_ml_policy_tree.ipynb", "examples/py_double_ml_pq.ipynb", "examples/py_double_ml_rdflex.ipynb", "examples/py_double_ml_robust_iv.ipynb", "examples/py_double_ml_sensitivity.ipynb", "examples/py_double_ml_sensitivity_booking.ipynb", "examples/py_double_ml_ssm.ipynb", "guide/algorithms.rst", "guide/basics.rst", "guide/data/base_data.rst", "guide/data/did_data.rst", "guide/data/panel_data.rst", "guide/data/rdd_data.rst", "guide/data/ssm_data.rst", "guide/data_backend.rst", "guide/guide.rst", "guide/heterogeneity.rst", "guide/learners.rst", "guide/learners/python/evaluate_learners.rst", "guide/learners/python/external_preds.rst", "guide/learners/python/minimum_req.rst", "guide/learners/python/set_hyperparams.rst", "guide/learners/python/tune_hyperparams.rst", "guide/learners/python/tune_hyperparams_old.rst", "guide/learners/r/minimum_req.rst", "guide/learners/r/pipelines.rst", "guide/learners/r/set_hyperparams.rst", "guide/learners/r/tune_and_pipelines.rst", "guide/learners/r/tune_hyperparams.rst", "guide/models.rst", "guide/models/did/did_aggregation.rst", "guide/models/did/did_binary.rst", "guide/models/did/did_cs.rst", "guide/models/did/did_implementation.rst", "guide/models/did/did_pa.rst", "guide/models/did/did_setup.rst", "guide/models/irm/apo.rst", "guide/models/irm/apos.rst", "guide/models/irm/iivm.rst", "guide/models/irm/irm.rst", "guide/models/plm/lplr.rst", "guide/models/plm/pliv.rst", "guide/models/plm/plpr.rst", "guide/models/plm/plr.rst", "guide/models/ssm/ssm.rst", "guide/resampling.rst", "guide/scores.rst", "guide/scores/did/did_cs_binary_score.rst", "guide/scores/did/did_cs_score.rst", "guide/scores/did/did_pa_binary_score.rst", "guide/scores/did/did_pa_score.rst", "guide/scores/irm/apo_score.rst", "guide/scores/irm/cvar_score.rst", "guide/scores/irm/iivm_score.rst", "guide/scores/irm/irm_score.rst", "guide/scores/irm/lpq_score.rst", "guide/scores/irm/pq_score.rst", "guide/scores/plm/lplr_score.rst", "guide/scores/plm/pliv_score.rst", "guide/scores/plm/plpr_score.rst", "guide/scores/plm/plr_score.rst", "guide/scores/ssm/mar_score.rst", "guide/scores/ssm/nr_score.rst", "guide/se_confint.rst", "guide/sensitivity.rst", "guide/sensitivity/benchmarking.rst", "guide/sensitivity/did/did_cs_binary_sensitivity.rst", "guide/sensitivity/did/did_cs_sensitivity.rst", "guide/sensitivity/did/did_pa_binary_sensitivity.rst", "guide/sensitivity/did/did_pa_sensitivity.rst", "guide/sensitivity/implementation.rst", "guide/sensitivity/irm/apo_sensitivity.rst", "guide/sensitivity/irm/irm_sensitivity.rst", "guide/sensitivity/plm/plr_sensitivity.rst", "guide/sensitivity/theory.rst", "index.rst", "intro/install.rst", "intro/intro.rst", "literature/literature.rst", "release/release.rst", "workflow/workflow.rst"], "indexentries": {"adjust_ps() (doubleml.utils.psprocessor method)": [[57, "doubleml.utils.PSProcessor.adjust_ps", false]], "aggregate() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.aggregate", false]], "aggregate_over_splits() (doubleml.rdd.rdflex method)": [[48, "doubleml.rdd.RDFlex.aggregate_over_splits", false]], "bootstrap() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.bootstrap", false]], "bootstrap() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.bootstrap", false]], "bootstrap() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlplpr method)": [[39, "doubleml.plm.DoubleMLPLPR.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlplr method)": [[40, "doubleml.plm.DoubleMLPLR.bootstrap", false]], "capo() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.capo", false]], "cate() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.cate", false]], "cate() (doubleml.plm.doublemlplr method)": [[40, "doubleml.plm.DoubleMLPLR.cate", false]], "causal_contrast() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.causal_contrast", false]], "confint() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.confint", false]], "confint() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.confint", false]], "confint() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.confint", false]], "confint() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.confint", false]], "confint() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.confint", false]], "confint() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.confint", false]], "confint() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.confint", false]], "confint() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.confint", false]], "confint() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.confint", false]], "confint() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.confint", false]], "confint() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.confint", false]], "confint() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.confint", false]], "confint() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.confint", false]], "confint() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.confint", false]], "confint() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.confint", false]], "confint() (doubleml.plm.doublemlplpr method)": [[39, "doubleml.plm.DoubleMLPLPR.confint", false]], "confint() (doubleml.plm.doublemlplr method)": [[40, "doubleml.plm.DoubleMLPLR.confint", false]], "confint() (doubleml.rdd.rdflex method)": [[48, "doubleml.rdd.RDFlex.confint", false]], "confint() (doubleml.utils.doublemlblp method)": [[53, "doubleml.utils.DoubleMLBLP.confint", false]], "construct_framework() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.construct_framework", false]], "construct_framework() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.construct_framework", false]], "construct_framework() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlplpr method)": [[39, "doubleml.plm.DoubleMLPLPR.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlplr method)": [[40, "doubleml.plm.DoubleMLPLR.construct_framework", false]], "dmldummyclassifier (class in doubleml.utils)": [[50, "doubleml.utils.DMLDummyClassifier", false]], "dmldummyregressor (class in doubleml.utils)": [[51, "doubleml.utils.DMLDummyRegressor", false]], "dmloptunaresult (class in doubleml.utils)": [[52, "doubleml.utils.DMLOptunaResult", false]], "doublemlapo (class in doubleml.irm)": [[22, "doubleml.irm.DoubleMLAPO", false]], "doublemlapos (class in doubleml.irm)": [[23, "doubleml.irm.DoubleMLAPOS", false]], "doublemlblp (class in doubleml.utils)": [[53, "doubleml.utils.DoubleMLBLP", false]], "doublemlclusterdata (class in doubleml.data)": [[4, "doubleml.data.DoubleMLClusterData", false]], "doublemlcvar (class in doubleml.irm)": [[24, "doubleml.irm.DoubleMLCVAR", false]], "doublemldata (class in doubleml.data)": [[6, "doubleml.data.DoubleMLData", false]], "doublemldid (class in doubleml.did)": [[12, "doubleml.did.DoubleMLDID", false]], "doublemldidaggregation (class in doubleml.did)": [[13, "doubleml.did.DoubleMLDIDAggregation", false]], "doublemldidbinary (class in doubleml.did)": [[14, "doubleml.did.DoubleMLDIDBinary", false]], "doublemldidcs (class in doubleml.did)": [[15, "doubleml.did.DoubleMLDIDCS", false]], "doublemldiddata (class in doubleml.data)": [[5, "doubleml.data.DoubleMLDIDData", false]], "doublemldidmulti (class in doubleml.did)": [[16, "doubleml.did.DoubleMLDIDMulti", false]], "doublemliivm (class in doubleml.irm)": [[25, "doubleml.irm.DoubleMLIIVM", false]], "doublemlirm (class in doubleml.irm)": [[26, "doubleml.irm.DoubleMLIRM", false]], "doublemllplr (class in doubleml.plm)": [[37, "doubleml.plm.DoubleMLLPLR", false]], "doublemllpq (class in doubleml.irm)": [[27, "doubleml.irm.DoubleMLLPQ", false]], "doublemlpaneldata (class in doubleml.data)": [[7, "doubleml.data.DoubleMLPanelData", false]], "doublemlpliv (class in doubleml.plm)": [[38, "doubleml.plm.DoubleMLPLIV", false]], "doublemlplpr (class in doubleml.plm)": [[39, "doubleml.plm.DoubleMLPLPR", false]], "doublemlplr (class in doubleml.plm)": [[40, "doubleml.plm.DoubleMLPLR", false]], "doublemlpolicytree (class in doubleml.utils)": [[54, "doubleml.utils.DoubleMLPolicyTree", false]], "doublemlpq (class in doubleml.irm)": [[28, "doubleml.irm.DoubleMLPQ", false]], "doublemlqte (class in doubleml.irm)": [[29, "doubleml.irm.DoubleMLQTE", false]], "doublemlrdddata (class in doubleml.data)": [[8, "doubleml.data.DoubleMLRDDData", false]], "doublemlssm (class in doubleml.irm)": [[30, "doubleml.irm.DoubleMLSSM", false]], "doublemlssmdata (class in doubleml.data)": [[9, "doubleml.data.DoubleMLSSMData", false]], "draw_sample_splitting() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlplpr method)": [[39, "doubleml.plm.DoubleMLPLPR.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlplr method)": [[40, "doubleml.plm.DoubleMLPLR.draw_sample_splitting", false]], "evaluate_learners() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlplpr method)": [[39, "doubleml.plm.DoubleMLPLPR.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlplr method)": [[40, "doubleml.plm.DoubleMLPLR.evaluate_learners", false]], "fetch_401k() (in module doubleml.datasets)": [[10, "doubleml.datasets.fetch_401K", false]], "fetch_bonus() (in module doubleml.datasets)": [[11, "doubleml.datasets.fetch_bonus", false]], "fit() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.fit", false]], "fit() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.fit", false]], "fit() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.fit", false]], "fit() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.fit", false]], "fit() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.fit", false]], "fit() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.fit", false]], "fit() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.fit", false]], "fit() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.fit", false]], "fit() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.fit", false]], "fit() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.fit", false]], "fit() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.fit", false]], "fit() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.fit", false]], "fit() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.fit", false]], "fit() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.fit", false]], "fit() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.fit", false]], "fit() (doubleml.plm.doublemlplpr method)": [[39, "doubleml.plm.DoubleMLPLPR.fit", false]], "fit() (doubleml.plm.doublemlplr method)": [[40, "doubleml.plm.DoubleMLPLR.fit", false]], "fit() (doubleml.rdd.rdflex method)": [[48, "doubleml.rdd.RDFlex.fit", false]], "fit() (doubleml.utils.dmldummyclassifier method)": [[50, "doubleml.utils.DMLDummyClassifier.fit", false]], "fit() (doubleml.utils.dmldummyregressor method)": [[51, "doubleml.utils.DMLDummyRegressor.fit", false]], "fit() (doubleml.utils.doublemlblp method)": [[53, "doubleml.utils.DoubleMLBLP.fit", false]], "fit() (doubleml.utils.doublemlpolicytree method)": [[54, "doubleml.utils.DoubleMLPolicyTree.fit", false]], "fit() (doubleml.utils.globalclassifier method)": [[55, "doubleml.utils.GlobalClassifier.fit", false]], "fit() (doubleml.utils.globalregressor method)": [[56, "doubleml.utils.GlobalRegressor.fit", false]], "from_arrays() (doubleml.data.doublemlclusterdata class method)": [[4, "doubleml.data.DoubleMLClusterData.from_arrays", false]], "from_arrays() (doubleml.data.doublemldata class method)": [[6, "doubleml.data.DoubleMLData.from_arrays", false]], "from_arrays() (doubleml.data.doublemldiddata class method)": [[5, "doubleml.data.DoubleMLDIDData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlpaneldata class method)": [[7, "doubleml.data.DoubleMLPanelData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlrdddata class method)": [[8, "doubleml.data.DoubleMLRDDData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlssmdata class method)": [[9, "doubleml.data.DoubleMLSSMData.from_arrays", false]], "from_config() (doubleml.utils.psprocessor class method)": [[57, "doubleml.utils.PSProcessor.from_config", false]], "gain_statistics() (in module doubleml.utils)": [[59, "doubleml.utils.gain_statistics", false]], "gapo() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.gapo", false]], "gate() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.gate", false]], "gate() (doubleml.plm.doublemlplr method)": [[40, "doubleml.plm.DoubleMLPLR.gate", false]], "get_metadata_routing() (doubleml.utils.dmldummyclassifier method)": [[50, "doubleml.utils.DMLDummyClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.dmldummyregressor method)": [[51, "doubleml.utils.DMLDummyRegressor.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalclassifier method)": [[55, "doubleml.utils.GlobalClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalregressor method)": [[56, "doubleml.utils.GlobalRegressor.get_metadata_routing", false]], "get_params() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.get_params", false]], "get_params() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.get_params", false]], "get_params() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.get_params", false]], "get_params() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.get_params", false]], "get_params() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.get_params", false]], "get_params() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.get_params", false]], "get_params() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.get_params", false]], "get_params() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.get_params", false]], "get_params() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.get_params", false]], "get_params() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.get_params", false]], "get_params() (doubleml.plm.doublemlplpr method)": [[39, "doubleml.plm.DoubleMLPLPR.get_params", false]], "get_params() (doubleml.plm.doublemlplr method)": [[40, "doubleml.plm.DoubleMLPLR.get_params", false]], "get_params() (doubleml.utils.dmldummyclassifier method)": [[50, "doubleml.utils.DMLDummyClassifier.get_params", false]], "get_params() (doubleml.utils.dmldummyregressor method)": [[51, "doubleml.utils.DMLDummyRegressor.get_params", false]], "get_params() (doubleml.utils.globalclassifier method)": [[55, "doubleml.utils.GlobalClassifier.get_params", false]], "get_params() (doubleml.utils.globalregressor method)": [[56, "doubleml.utils.GlobalRegressor.get_params", false]], "globalclassifier (class in doubleml.utils)": [[55, "doubleml.utils.GlobalClassifier", false]], "globalregressor (class in doubleml.utils)": [[56, "doubleml.utils.GlobalRegressor", false]], "linearscoremixin (class in doubleml.double_ml_score_mixins)": [[20, "doubleml.double_ml_score_mixins.LinearScoreMixin", false]], "make_confounded_irm_data() (in module doubleml.irm.datasets)": [[31, "doubleml.irm.datasets.make_confounded_irm_data", false]], "make_confounded_plr_data() (in module doubleml.plm.datasets)": [[41, "doubleml.plm.datasets.make_confounded_plr_data", false]], "make_did_cs2021() (in module doubleml.did.datasets)": [[17, "doubleml.did.datasets.make_did_CS2021", false]], "make_did_cs_cs2021() (in module doubleml.did.datasets)": [[19, "doubleml.did.datasets.make_did_cs_CS2021", false]], "make_did_sz2020() (in module doubleml.did.datasets)": [[18, "doubleml.did.datasets.make_did_SZ2020", false]], "make_heterogeneous_data() (in module doubleml.irm.datasets)": [[32, "doubleml.irm.datasets.make_heterogeneous_data", false]], "make_iivm_data() (in module doubleml.irm.datasets)": [[33, "doubleml.irm.datasets.make_iivm_data", false]], "make_irm_data() (in module doubleml.irm.datasets)": [[34, "doubleml.irm.datasets.make_irm_data", false]], "make_irm_data_discrete_treatments() (in module doubleml.irm.datasets)": [[35, "doubleml.irm.datasets.make_irm_data_discrete_treatments", false]], "make_lplr_lzz2020() (in module doubleml.plm.datasets)": [[42, "doubleml.plm.datasets.make_lplr_LZZ2020", false]], "make_pliv_chs2015() (in module doubleml.plm.datasets)": [[43, "doubleml.plm.datasets.make_pliv_CHS2015", false]], "make_pliv_multiway_cluster_ckms2021() (in module doubleml.plm.datasets)": [[44, "doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021", false]], "make_plpr_cp2025() (in module doubleml.plm.datasets)": [[45, "doubleml.plm.datasets.make_plpr_CP2025", false]], "make_plr_ccddhnr2018() (in module doubleml.plm.datasets)": [[46, "doubleml.plm.datasets.make_plr_CCDDHNR2018", false]], "make_plr_turrell2018() (in module doubleml.plm.datasets)": [[47, "doubleml.plm.datasets.make_plr_turrell2018", false]], "make_simple_rdd_data() (in module doubleml.rdd.datasets)": [[49, "doubleml.rdd.datasets.make_simple_rdd_data", false]], "make_ssm_data() (in module doubleml.irm.datasets)": [[36, "doubleml.irm.datasets.make_ssm_data", false]], "nonlinearscoremixin (class in doubleml.double_ml_score_mixins)": [[21, "doubleml.double_ml_score_mixins.NonLinearScoreMixin", false]], "p_adjust() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.p_adjust", false]], "p_adjust() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.p_adjust", false]], "p_adjust() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlplpr method)": [[39, "doubleml.plm.DoubleMLPLPR.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlplr method)": [[40, "doubleml.plm.DoubleMLPLR.p_adjust", false]], "plot_effects() (doubleml.did.doublemldidaggregation method)": [[13, "doubleml.did.DoubleMLDIDAggregation.plot_effects", false]], "plot_effects() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.plot_effects", false]], "plot_tree() (doubleml.utils.doublemlpolicytree method)": [[54, "doubleml.utils.DoubleMLPolicyTree.plot_tree", false]], "policy_tree() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.policy_tree", false]], "predict() (doubleml.utils.dmldummyclassifier method)": [[50, "doubleml.utils.DMLDummyClassifier.predict", false]], "predict() (doubleml.utils.dmldummyregressor method)": [[51, "doubleml.utils.DMLDummyRegressor.predict", false]], "predict() (doubleml.utils.doublemlpolicytree method)": [[54, "doubleml.utils.DoubleMLPolicyTree.predict", false]], "predict() (doubleml.utils.globalclassifier method)": [[55, "doubleml.utils.GlobalClassifier.predict", false]], "predict() (doubleml.utils.globalregressor method)": [[56, "doubleml.utils.GlobalRegressor.predict", false]], "predict_proba() (doubleml.utils.dmldummyclassifier method)": [[50, "doubleml.utils.DMLDummyClassifier.predict_proba", false]], "predict_proba() (doubleml.utils.globalclassifier method)": [[55, "doubleml.utils.GlobalClassifier.predict_proba", false]], "psprocessor (class in doubleml.utils)": [[57, "doubleml.utils.PSProcessor", false]], "psprocessorconfig (class in doubleml.utils)": [[58, "doubleml.utils.PSProcessorConfig", false]], "rdflex (class in doubleml.rdd)": [[48, "doubleml.rdd.RDFlex", false]], "robust_confset() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.robust_confset", false]], "score() (doubleml.utils.dmldummyclassifier method)": [[50, "doubleml.utils.DMLDummyClassifier.score", false]], "score() (doubleml.utils.dmldummyregressor method)": [[51, "doubleml.utils.DMLDummyRegressor.score", false]], "score() (doubleml.utils.globalclassifier method)": [[55, "doubleml.utils.GlobalClassifier.score", false]], "score() (doubleml.utils.globalregressor method)": [[56, "doubleml.utils.GlobalRegressor.score", false]], "sensitivity_analysis() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlplpr method)": [[39, "doubleml.plm.DoubleMLPLPR.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlplr method)": [[40, "doubleml.plm.DoubleMLPLR.sensitivity_analysis", false]], "sensitivity_benchmark() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlplpr method)": [[39, "doubleml.plm.DoubleMLPLPR.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlplr method)": [[40, "doubleml.plm.DoubleMLPLR.sensitivity_benchmark", false]], "sensitivity_plot() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlplpr method)": [[39, "doubleml.plm.DoubleMLPLPR.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlplr method)": [[40, "doubleml.plm.DoubleMLPLR.sensitivity_plot", false]], "set_fit_request() (doubleml.utils.globalclassifier method)": [[55, "doubleml.utils.GlobalClassifier.set_fit_request", false]], "set_fit_request() (doubleml.utils.globalregressor method)": [[56, "doubleml.utils.GlobalRegressor.set_fit_request", false]], "set_ml_nuisance_params() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlplpr method)": [[39, "doubleml.plm.DoubleMLPLPR.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlplr method)": [[40, "doubleml.plm.DoubleMLPLR.set_ml_nuisance_params", false]], "set_params() (doubleml.utils.dmldummyclassifier method)": [[50, "doubleml.utils.DMLDummyClassifier.set_params", false]], "set_params() (doubleml.utils.dmldummyregressor method)": [[51, "doubleml.utils.DMLDummyRegressor.set_params", false]], "set_params() (doubleml.utils.globalclassifier method)": [[55, "doubleml.utils.GlobalClassifier.set_params", false]], "set_params() (doubleml.utils.globalregressor method)": [[56, "doubleml.utils.GlobalRegressor.set_params", false]], "set_sample_splitting() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlplpr method)": [[39, "doubleml.plm.DoubleMLPLPR.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlplr method)": [[40, "doubleml.plm.DoubleMLPLR.set_sample_splitting", false]], "set_score_request() (doubleml.utils.dmldummyclassifier method)": [[50, "doubleml.utils.DMLDummyClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.dmldummyregressor method)": [[51, "doubleml.utils.DMLDummyRegressor.set_score_request", false]], "set_score_request() (doubleml.utils.globalclassifier method)": [[55, "doubleml.utils.GlobalClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.globalregressor method)": [[56, "doubleml.utils.GlobalRegressor.set_score_request", false]], "set_x_d() (doubleml.data.doublemlclusterdata method)": [[4, "doubleml.data.DoubleMLClusterData.set_x_d", false]], "set_x_d() (doubleml.data.doublemldata method)": [[6, "doubleml.data.DoubleMLData.set_x_d", false]], "set_x_d() (doubleml.data.doublemldiddata method)": [[5, "doubleml.data.DoubleMLDIDData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlpaneldata method)": [[7, "doubleml.data.DoubleMLPanelData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlrdddata method)": [[8, "doubleml.data.DoubleMLRDDData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlssmdata method)": [[9, "doubleml.data.DoubleMLSSMData.set_x_d", false]], "tune() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.tune", false]], "tune() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.tune", false]], "tune() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.tune", false]], "tune() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.tune", false]], "tune() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.tune", false]], "tune() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.tune", false]], "tune() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.tune", false]], "tune() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.tune", false]], "tune() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.tune", false]], "tune() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.tune", false]], "tune() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.tune", false]], "tune() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.tune", false]], "tune() (doubleml.plm.doublemlplpr method)": [[39, "doubleml.plm.DoubleMLPLPR.tune", false]], "tune() (doubleml.plm.doublemlplr method)": [[40, "doubleml.plm.DoubleMLPLR.tune", false]], "tune_ml_models() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.tune_ml_models", false]], "tune_ml_models() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.tune_ml_models", false]], "tune_ml_models() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.tune_ml_models", false]], "tune_ml_models() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.tune_ml_models", false]], "tune_ml_models() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.tune_ml_models", false]], "tune_ml_models() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.tune_ml_models", false]], "tune_ml_models() (doubleml.plm.doublemlplpr method)": [[39, "doubleml.plm.DoubleMLPLPR.tune_ml_models", false]], "tune_ml_models() (doubleml.plm.doublemlplr method)": [[40, "doubleml.plm.DoubleMLPLR.tune_ml_models", false]]}, "objects": {"doubleml.data": [[4, 0, 1, "", "DoubleMLClusterData"], [5, 0, 1, "", "DoubleMLDIDData"], [6, 0, 1, "", "DoubleMLData"], [7, 0, 1, "", "DoubleMLPanelData"], [8, 0, 1, "", "DoubleMLRDDData"], [9, 0, 1, "", "DoubleMLSSMData"]], "doubleml.data.DoubleMLClusterData": [[4, 1, 1, "", "from_arrays"], [4, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLDIDData": [[5, 1, 1, "", "from_arrays"], [5, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLData": [[6, 1, 1, "", "from_arrays"], [6, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLPanelData": [[7, 1, 1, "", "from_arrays"], [7, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLRDDData": [[8, 1, 1, "", "from_arrays"], [8, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLSSMData": [[9, 1, 1, "", "from_arrays"], [9, 1, 1, "", "set_x_d"]], "doubleml.datasets": [[10, 2, 1, "", "fetch_401K"], [11, 2, 1, "", "fetch_bonus"]], "doubleml.did": [[12, 0, 1, "", "DoubleMLDID"], [13, 0, 1, "", "DoubleMLDIDAggregation"], [14, 0, 1, "", "DoubleMLDIDBinary"], [15, 0, 1, "", "DoubleMLDIDCS"], [16, 0, 1, "", "DoubleMLDIDMulti"]], "doubleml.did.DoubleMLDID": [[12, 1, 1, "", "bootstrap"], [12, 1, 1, "", "confint"], [12, 1, 1, "", "construct_framework"], [12, 1, 1, "", "draw_sample_splitting"], [12, 1, 1, "", "evaluate_learners"], [12, 1, 1, "", "fit"], [12, 1, 1, "", "get_params"], [12, 1, 1, "", "p_adjust"], [12, 1, 1, "", "sensitivity_analysis"], [12, 1, 1, "", "sensitivity_benchmark"], [12, 1, 1, "", "sensitivity_plot"], [12, 1, 1, "", "set_ml_nuisance_params"], [12, 1, 1, "", "set_sample_splitting"], [12, 1, 1, "", "tune"], [12, 1, 1, "", "tune_ml_models"]], "doubleml.did.DoubleMLDIDAggregation": [[13, 1, 1, "", "plot_effects"]], "doubleml.did.DoubleMLDIDBinary": [[14, 1, 1, "", "bootstrap"], [14, 1, 1, "", "confint"], [14, 1, 1, "", "construct_framework"], [14, 1, 1, "", "draw_sample_splitting"], [14, 1, 1, "", "evaluate_learners"], [14, 1, 1, "", "fit"], [14, 1, 1, "", "get_params"], [14, 1, 1, "", "p_adjust"], [14, 1, 1, "", "sensitivity_analysis"], [14, 1, 1, "", "sensitivity_benchmark"], [14, 1, 1, "", "sensitivity_plot"], [14, 1, 1, "", "set_ml_nuisance_params"], [14, 1, 1, "", "set_sample_splitting"], [14, 1, 1, "", "tune"], [14, 1, 1, "", "tune_ml_models"]], "doubleml.did.DoubleMLDIDCS": [[15, 1, 1, "", "bootstrap"], [15, 1, 1, "", "confint"], [15, 1, 1, "", "construct_framework"], [15, 1, 1, "", "draw_sample_splitting"], [15, 1, 1, "", "evaluate_learners"], [15, 1, 1, "", "fit"], [15, 1, 1, "", "get_params"], [15, 1, 1, "", "p_adjust"], [15, 1, 1, "", "sensitivity_analysis"], [15, 1, 1, "", "sensitivity_benchmark"], [15, 1, 1, "", "sensitivity_plot"], [15, 1, 1, "", "set_ml_nuisance_params"], [15, 1, 1, "", "set_sample_splitting"], [15, 1, 1, "", "tune"], [15, 1, 1, "", "tune_ml_models"]], "doubleml.did.DoubleMLDIDMulti": [[16, 1, 1, "", "aggregate"], [16, 1, 1, "", "bootstrap"], [16, 1, 1, "", "confint"], [16, 1, 1, "", "fit"], [16, 1, 1, "", "p_adjust"], [16, 1, 1, "", "plot_effects"], [16, 1, 1, "", "sensitivity_analysis"], [16, 1, 1, "", "sensitivity_benchmark"], [16, 1, 1, "", "sensitivity_plot"], [16, 1, 1, "", "tune_ml_models"]], "doubleml.did.datasets": [[17, 2, 1, "", "make_did_CS2021"], [18, 2, 1, "", "make_did_SZ2020"], [19, 2, 1, "", "make_did_cs_CS2021"]], "doubleml.double_ml_score_mixins": [[20, 0, 1, "", "LinearScoreMixin"], [21, 0, 1, "", "NonLinearScoreMixin"]], "doubleml.irm": [[22, 0, 1, "", "DoubleMLAPO"], [23, 0, 1, "", "DoubleMLAPOS"], [24, 0, 1, "", "DoubleMLCVAR"], [25, 0, 1, "", "DoubleMLIIVM"], [26, 0, 1, "", "DoubleMLIRM"], [27, 0, 1, "", "DoubleMLLPQ"], [28, 0, 1, "", "DoubleMLPQ"], [29, 0, 1, "", "DoubleMLQTE"], [30, 0, 1, "", "DoubleMLSSM"]], "doubleml.irm.DoubleMLAPO": [[22, 1, 1, "", "bootstrap"], [22, 1, 1, "", "capo"], [22, 1, 1, "", "confint"], [22, 1, 1, "", "construct_framework"], [22, 1, 1, "", "draw_sample_splitting"], [22, 1, 1, "", "evaluate_learners"], [22, 1, 1, "", "fit"], [22, 1, 1, "", "gapo"], [22, 1, 1, "", "get_params"], [22, 1, 1, "", "p_adjust"], [22, 1, 1, "", "sensitivity_analysis"], [22, 1, 1, "", "sensitivity_benchmark"], [22, 1, 1, "", "sensitivity_plot"], [22, 1, 1, "", "set_ml_nuisance_params"], [22, 1, 1, "", "set_sample_splitting"], [22, 1, 1, "", "tune"], [22, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLAPOS": [[23, 1, 1, "", "bootstrap"], [23, 1, 1, "", "causal_contrast"], [23, 1, 1, "", "confint"], [23, 1, 1, "", "draw_sample_splitting"], [23, 1, 1, "", "fit"], [23, 1, 1, "", "sensitivity_analysis"], [23, 1, 1, "", "sensitivity_benchmark"], [23, 1, 1, "", "sensitivity_plot"], [23, 1, 1, "", "set_sample_splitting"], [23, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLCVAR": [[24, 1, 1, "", "bootstrap"], [24, 1, 1, "", "confint"], [24, 1, 1, "", "construct_framework"], [24, 1, 1, "", "draw_sample_splitting"], [24, 1, 1, "", "evaluate_learners"], [24, 1, 1, "", "fit"], [24, 1, 1, "", "get_params"], [24, 1, 1, "", "p_adjust"], [24, 1, 1, "", "sensitivity_analysis"], [24, 1, 1, "", "sensitivity_benchmark"], [24, 1, 1, "", "sensitivity_plot"], [24, 1, 1, "", "set_ml_nuisance_params"], [24, 1, 1, "", "set_sample_splitting"], [24, 1, 1, "", "tune"], [24, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLIIVM": [[25, 1, 1, "", "bootstrap"], [25, 1, 1, "", "confint"], [25, 1, 1, "", "construct_framework"], [25, 1, 1, "", "draw_sample_splitting"], [25, 1, 1, "", "evaluate_learners"], [25, 1, 1, "", "fit"], [25, 1, 1, "", "get_params"], [25, 1, 1, "", "p_adjust"], [25, 1, 1, "", "robust_confset"], [25, 1, 1, "", "sensitivity_analysis"], [25, 1, 1, "", "sensitivity_benchmark"], [25, 1, 1, "", "sensitivity_plot"], [25, 1, 1, "", "set_ml_nuisance_params"], [25, 1, 1, "", "set_sample_splitting"], [25, 1, 1, "", "tune"], [25, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLIRM": [[26, 1, 1, "", "bootstrap"], [26, 1, 1, "", "cate"], [26, 1, 1, "", "confint"], [26, 1, 1, "", "construct_framework"], [26, 1, 1, "", "draw_sample_splitting"], [26, 1, 1, "", "evaluate_learners"], [26, 1, 1, "", "fit"], [26, 1, 1, "", "gate"], [26, 1, 1, "", "get_params"], [26, 1, 1, "", "p_adjust"], [26, 1, 1, "", "policy_tree"], [26, 1, 1, "", "sensitivity_analysis"], [26, 1, 1, "", "sensitivity_benchmark"], [26, 1, 1, "", "sensitivity_plot"], [26, 1, 1, "", "set_ml_nuisance_params"], [26, 1, 1, "", "set_sample_splitting"], [26, 1, 1, "", "tune"], [26, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLLPQ": [[27, 1, 1, "", "bootstrap"], [27, 1, 1, "", "confint"], [27, 1, 1, "", "construct_framework"], [27, 1, 1, "", "draw_sample_splitting"], [27, 1, 1, "", "evaluate_learners"], [27, 1, 1, "", "fit"], [27, 1, 1, "", "get_params"], [27, 1, 1, "", "p_adjust"], [27, 1, 1, "", "sensitivity_analysis"], [27, 1, 1, "", "sensitivity_benchmark"], [27, 1, 1, "", "sensitivity_plot"], [27, 1, 1, "", "set_ml_nuisance_params"], [27, 1, 1, "", "set_sample_splitting"], [27, 1, 1, "", "tune"], [27, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLPQ": [[28, 1, 1, "", "bootstrap"], [28, 1, 1, "", "confint"], [28, 1, 1, "", "construct_framework"], [28, 1, 1, "", "draw_sample_splitting"], [28, 1, 1, "", "evaluate_learners"], [28, 1, 1, "", "fit"], [28, 1, 1, "", "get_params"], [28, 1, 1, "", "p_adjust"], [28, 1, 1, "", "sensitivity_analysis"], [28, 1, 1, "", "sensitivity_benchmark"], [28, 1, 1, "", "sensitivity_plot"], [28, 1, 1, "", "set_ml_nuisance_params"], [28, 1, 1, "", "set_sample_splitting"], [28, 1, 1, "", "tune"], [28, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLQTE": [[29, 1, 1, "", "bootstrap"], [29, 1, 1, "", "confint"], [29, 1, 1, "", "draw_sample_splitting"], [29, 1, 1, "", "fit"], [29, 1, 1, "", "p_adjust"], [29, 1, 1, "", "set_sample_splitting"], [29, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLSSM": [[30, 1, 1, "", "bootstrap"], [30, 1, 1, "", "confint"], [30, 1, 1, "", "construct_framework"], [30, 1, 1, "", "draw_sample_splitting"], [30, 1, 1, "", "evaluate_learners"], [30, 1, 1, "", "fit"], [30, 1, 1, "", "get_params"], [30, 1, 1, "", "p_adjust"], [30, 1, 1, "", "sensitivity_analysis"], [30, 1, 1, "", "sensitivity_benchmark"], [30, 1, 1, "", "sensitivity_plot"], [30, 1, 1, "", "set_ml_nuisance_params"], [30, 1, 1, "", "set_sample_splitting"], [30, 1, 1, "", "tune"], [30, 1, 1, "", "tune_ml_models"]], "doubleml.irm.datasets": [[31, 2, 1, "", "make_confounded_irm_data"], [32, 2, 1, "", "make_heterogeneous_data"], [33, 2, 1, "", "make_iivm_data"], [34, 2, 1, "", "make_irm_data"], [35, 2, 1, "", "make_irm_data_discrete_treatments"], [36, 2, 1, "", "make_ssm_data"]], "doubleml.plm": [[37, 0, 1, "", "DoubleMLLPLR"], [38, 0, 1, "", "DoubleMLPLIV"], [39, 0, 1, "", "DoubleMLPLPR"], [40, 0, 1, "", "DoubleMLPLR"]], "doubleml.plm.DoubleMLLPLR": [[37, 1, 1, "", "bootstrap"], [37, 1, 1, "", "confint"], [37, 1, 1, "", "construct_framework"], [37, 1, 1, "", "draw_sample_splitting"], [37, 1, 1, "", "evaluate_learners"], [37, 1, 1, "", "fit"], [37, 1, 1, "", "get_params"], [37, 1, 1, "", "p_adjust"], [37, 1, 1, "", "sensitivity_analysis"], [37, 1, 1, "", "sensitivity_benchmark"], [37, 1, 1, "", "sensitivity_plot"], [37, 1, 1, "", "set_ml_nuisance_params"], [37, 1, 1, "", "set_sample_splitting"], [37, 1, 1, "", "tune"], [37, 1, 1, "", "tune_ml_models"]], "doubleml.plm.DoubleMLPLIV": [[38, 1, 1, "", "bootstrap"], [38, 1, 1, "", "confint"], [38, 1, 1, "", "construct_framework"], [38, 1, 1, "", "draw_sample_splitting"], [38, 1, 1, "", "evaluate_learners"], [38, 1, 1, "", "fit"], [38, 1, 1, "", "get_params"], [38, 1, 1, "", "p_adjust"], [38, 1, 1, "", "sensitivity_analysis"], [38, 1, 1, "", "sensitivity_benchmark"], [38, 1, 1, "", "sensitivity_plot"], [38, 1, 1, "", "set_ml_nuisance_params"], [38, 1, 1, "", "set_sample_splitting"], [38, 1, 1, "", "tune"], [38, 1, 1, "", "tune_ml_models"]], "doubleml.plm.DoubleMLPLPR": [[39, 1, 1, "", "bootstrap"], [39, 1, 1, "", "confint"], [39, 1, 1, "", "construct_framework"], [39, 1, 1, "", "draw_sample_splitting"], [39, 1, 1, "", "evaluate_learners"], [39, 1, 1, "", "fit"], [39, 1, 1, "", "get_params"], [39, 1, 1, "", "p_adjust"], [39, 1, 1, "", "sensitivity_analysis"], [39, 1, 1, "", "sensitivity_benchmark"], [39, 1, 1, "", "sensitivity_plot"], [39, 1, 1, "", "set_ml_nuisance_params"], [39, 1, 1, "", "set_sample_splitting"], [39, 1, 1, "", "tune"], [39, 1, 1, "", "tune_ml_models"]], "doubleml.plm.DoubleMLPLR": [[40, 1, 1, "", "bootstrap"], [40, 1, 1, "", "cate"], [40, 1, 1, "", "confint"], [40, 1, 1, "", "construct_framework"], [40, 1, 1, "", "draw_sample_splitting"], [40, 1, 1, "", "evaluate_learners"], [40, 1, 1, "", "fit"], [40, 1, 1, "", "gate"], [40, 1, 1, "", "get_params"], [40, 1, 1, "", "p_adjust"], [40, 1, 1, "", "sensitivity_analysis"], [40, 1, 1, "", "sensitivity_benchmark"], [40, 1, 1, "", "sensitivity_plot"], [40, 1, 1, "", "set_ml_nuisance_params"], [40, 1, 1, "", "set_sample_splitting"], [40, 1, 1, "", "tune"], [40, 1, 1, "", "tune_ml_models"]], "doubleml.plm.datasets": [[41, 2, 1, "", "make_confounded_plr_data"], [42, 2, 1, "", "make_lplr_LZZ2020"], [43, 2, 1, "", "make_pliv_CHS2015"], [44, 2, 1, "", "make_pliv_multiway_cluster_CKMS2021"], [45, 2, 1, "", "make_plpr_CP2025"], [46, 2, 1, "", "make_plr_CCDDHNR2018"], [47, 2, 1, "", "make_plr_turrell2018"]], "doubleml.rdd": [[48, 0, 1, "", "RDFlex"]], "doubleml.rdd.RDFlex": [[48, 1, 1, "", "aggregate_over_splits"], [48, 1, 1, "", "confint"], [48, 1, 1, "", "fit"]], "doubleml.rdd.datasets": [[49, 2, 1, "", "make_simple_rdd_data"]], "doubleml.utils": [[50, 0, 1, "", "DMLDummyClassifier"], [51, 0, 1, "", "DMLDummyRegressor"], [52, 0, 1, "", "DMLOptunaResult"], [53, 0, 1, "", "DoubleMLBLP"], [54, 0, 1, "", "DoubleMLPolicyTree"], [55, 0, 1, "", "GlobalClassifier"], [56, 0, 1, "", "GlobalRegressor"], [57, 0, 1, "", "PSProcessor"], [58, 0, 1, "", "PSProcessorConfig"], [59, 2, 1, "", "gain_statistics"]], "doubleml.utils.DMLDummyClassifier": [[50, 1, 1, "", "fit"], [50, 1, 1, "", "get_metadata_routing"], [50, 1, 1, "", "get_params"], [50, 1, 1, "", "predict"], [50, 1, 1, "", "predict_proba"], [50, 1, 1, "", "score"], [50, 1, 1, "", "set_params"], [50, 1, 1, "", "set_score_request"]], "doubleml.utils.DMLDummyRegressor": [[51, 1, 1, "", "fit"], [51, 1, 1, "", "get_metadata_routing"], [51, 1, 1, "", "get_params"], [51, 1, 1, "", "predict"], [51, 1, 1, "", "score"], [51, 1, 1, "", "set_params"], [51, 1, 1, "", "set_score_request"]], "doubleml.utils.DoubleMLBLP": [[53, 1, 1, "", "confint"], [53, 1, 1, "", "fit"]], "doubleml.utils.DoubleMLPolicyTree": [[54, 1, 1, "", "fit"], [54, 1, 1, "", "plot_tree"], [54, 1, 1, "", "predict"]], "doubleml.utils.GlobalClassifier": [[55, 1, 1, "", "fit"], [55, 1, 1, "", "get_metadata_routing"], [55, 1, 1, "", "get_params"], [55, 1, 1, "", "predict"], [55, 1, 1, "", "predict_proba"], [55, 1, 1, "", "score"], [55, 1, 1, "", "set_fit_request"], [55, 1, 1, "", "set_params"], [55, 1, 1, "", "set_score_request"]], "doubleml.utils.GlobalRegressor": [[56, 1, 1, "", "fit"], [56, 1, 1, "", "get_metadata_routing"], [56, 1, 1, "", "get_params"], [56, 1, 1, "", "predict"], [56, 1, 1, "", "score"], [56, 1, 1, "", "set_fit_request"], [56, 1, 1, "", "set_params"], [56, 1, 1, "", "set_score_request"]], "doubleml.utils.PSProcessor": [[57, 1, 1, "", "adjust_ps"], [57, 1, 1, "", "from_config"]]}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function"}, "terms": {"": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 40, 43, 44, 45, 46, 47, 48, 53, 55, 56, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 77, 78, 79, 80, 86, 87, 88, 89, 91, 93, 94, 95, 97, 100, 102, 103, 104, 105, 107, 109, 110, 111, 112, 115, 116, 123, 125, 126, 127, 129, 130, 132, 133, 142, 144, 159, 160, 161, 162, 163, 173, 175, 176, 177, 178], "0": [4, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 53, 54, 56, 57, 58, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 112, 114, 115, 116, 117, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170, 174, 175, 177], "00": [71, 74, 78, 87, 88, 90, 94, 95, 143], "000": [101, 161, 178], "00000": 90, "000000": [71, 73, 74, 75, 80, 90, 94, 95, 107, 112, 114, 175], "0000000": 161, "0000000000000010000100": [67, 107, 112, 175], "000000e": [71, 74, 87, 88, 90, 94, 95], "000005": 72, "000006": 80, "000017": 99, "000025": [72, 93], "000034": 94, "000039": 93, "000056": 114, "000064": 81, "000067": 93, "000076": 127, "000087": 72, "000091": 93, "0001": [72, 75, 94], "000105": 72, "000128": 91, "000129": 37, "000144": [72, 127], "000219": 28, "000222": 72, "000242": 29, "000320": 99, "00032016": 99, "000335": [71, 74], "000341": 93, "000362": 71, "000393": 71, "000413": 74, "000432": 71, "000436": 74, "000442": [74, 93], "000475": 71, "00047580260495": 62, "000488": 93, "000493": 74, "000494": 89, "0005": 75, "000503": 74, "000515": 72, "000522": 93, "000550": 74, "000556": 71, "000565": 84, "000574": 71, "0005a80b528f": 67, "000619": 71, "000662": 74, "000670": 93, "000728": 97, "000743": 102, "000915799": 161, "0009157990": 161, "000916": [108, 112], "000943": [83, 84], "000972": 72, "001": [17, 19, 57, 62, 64, 65, 66, 67, 68, 78, 82, 115, 123, 124, 125, 126, 127, 143, 144, 161, 175, 178], "0010": 72, "001002670": 143, "001006": 78, "001008": 78, "001035": 72, "001051": 93, "001073": 78, "001080": 78, "001097": 78, "001145": 95, "001146": 72, "0011563701553192595": 78, "001159": 78, "001181": 78, "001184": 78, "001197": 72, "001280": 71, "00133": 67, "001353": 78, "001360": 72, "00138944": [105, 144], "0013978426220758982": 78, "001403": 100, "001471": 90, "001494": [114, 115, 117, 127], "001524": [72, 74], "001576": 78, "0016": [66, 94], "001698": 90, "001738": 78, "001758": 83, "001770": 72, "001779": 78, "0018": [66, 72, 94], "0019": 75, "001907": 90, "002037900301454": 92, "00205": 143, "002149e": 84, "002169338": 161, "0021693380": 161, "0021693381": 161, "002225": 83, "002290": 70, "0023": 64, "002314": 72, "002392": 72, "002436": 89, "002566": 74, "0026": 75, "0027": 72, "002728e": 84, "002779": 102, "0028": [64, 66, 72, 94], "002821": 103, "00282133350419121": 103, "002873e": 83, "002983": 93, "003": [18, 31, 41, 96, 101], "003045": 90, "003051": 83, "003074": 127, "003087": 72, "003115": 71, "003134": 99, "003159": 72, "003220": 80, "003247e": 97, "003275": [127, 128], "0033": 72, "003325": 72, "003328": 99, "003384": 72, "0034": 86, "003404": 80, "003415": 80, "003427": 93, "003438": 71, "003504": 74, "003528e": 83, "003553326": 143, "003599": 74, "003646": 74, "003765": [127, 128], "003779": 89, "003836": 99, "003903": 72, "003924": 89, "0040": 72, "00409412": [105, 144], "004121": 72, "004157": 72, "004192": 83, "0042": [66, 94], "004244": 72, "004253": 80, "004269": 84, "004298": 97, "004392": 89, "004526": 80, "004542": 90, "0046": 72, "0047": [66, 94], "004831": 72, "004846": 103, "004868": 78, "004910": 72, "004956": 72, "005075": 72, "005177": 97, "005307": 71, "005316": [127, 128], "005339": [83, 84], "005485": 78, "005521": 72, "005734": 78, "005846": 71, "005857": 93, "005879415": 143, "005889558": 143, "005954541": 143, "005e": 127, "006055": 80, "0060715124549546": 92, "0065": 72, "006593": 114, "006693": 72, "006788721": 143, "006852": 71, "006922": 75, "006958": [83, 84], "006998": 74, "007272": 72, "00728": 175, "0073": 75, "007421": 114, "007451": 74, "007517": 71, "007596": 78, "007638e": 74, "00778625": 143, "007789e": 92, "008": 103, "008023": 95, "008025": 97, "008099": 71, "008122": 73, "008223": [83, 84], "008334887": 143, "008487": 75, "008674": 16, "008825": 85, "008825189994473": 85, "008883698": 144, "00888458890362062": 105, "008884589": 105, "008928": 97, "008941": 83, "008981413": 143, "008dbd": 96, "008e80": 96, "009": [96, 103], "009122": 99, "009329847": 144, "009428": 85, "00944171905420782": 103, "00950122695463054": 105, "009501226954630540": 105, "009501227": 105, "009645422": 65, "009656": 99, "00972": 75, "009763": [127, 130], "009790": 95, "0098": 72, "009986": 99, "01": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 54, 57, 58, 62, 65, 66, 67, 68, 71, 74, 78, 79, 83, 84, 90, 94, 95, 96, 98, 99, 100, 115, 121, 123, 124, 125, 126, 127, 130, 132, 143, 144, 161, 175, 178], "010": [62, 96], "010045": 92, "010213": 102, "010269": 93, "010308": 97, "010364": 114, "010401": 71, "010430": 73, "010450": 65, "010459": 72, "0105": 72, "010547": 97, "010553": 71, "010726": 78, "01077577": 143, "010865": 96, "010940": 93, "011016": 79, "011131": 99, "011134": 96, "011204": 90, "01128": 75, "011285": 83, "011598": 99, "011798": 72, "0118095": 65, "011823": 102, "011866": 72, "011914": 74, "011988e": 99, "012": 96, "012037": 96, "012080": 97, "01219": 67, "0124105481660435": 92, "012480": 97, "01274": 103, "012745": 71, "012831": 103, "01292612": 143, "012933": 74, "013034": 103, "013048": 74, "013088": 25, "01309197": 143, "013235": 96, "013291": 72, "013311": 71, "013350": 83, "013450": 90, "01351638": 65, "01355599": 74, "013593": 102, "013677": 100, "013848": 83, "013849": 78, "013870": 83, "013976": 84, "01398951": 65, "013990": 161, "014": 100, "014029": 72, "01403089": 65, "014064": 97, "014080": [83, 84], "014204": 97, "014206": 72, "014327": 72, "014357": 71, "0144": 64, "014431": 95, "014432": 70, "014511": 74, "014593": 78, "014637": 93, "014681": 102, "014721": 95, "014738": 83, "0149": 64, "014914": 78, "015": 67, "015038": 85, "015081": 72, "015090": 72, "0151": 72, "015200": 72, "015313": 71, "015404": 72, "015565": 99, "015645": 71, "015698": 99, "015702": 72, "01574297": 99, "015743": 99, "016": 96, "016154": 93, "016200": [83, 84], "016352": 72, "016386": 72, "016392": 71, "0164": 72, "01643": 176, "016518": 95, "016596": 97, "016598": 83, "0166": 72, "016640": 72, "016654": 74, "016672": 74, "016702": 87, "016715": 72, "016786": 78, "016814": 72, "017": 67, "017059": 72, "017185": 95, "017393e": 161, "017605": 83, "017660": 90, "0177": 72, "01772": 163, "01778407": 143, "017800092": 161, "0178000920": 161, "0179": 72, "017921": 72, "017939": 72, "018": 67, "018066": 97, "018092": 114, "018148": 99, "018164": 72, "018273": 72, "018353508": 143, "018386": 72, "01854269": 143, "0187512020118494": 92, "01899958": 143, "01900305": [127, 130], "01903": [67, 115, 125, 173, 175], "019091": 74, "01916030e": 143, "019249": 74, "019253": 72, "01925597": 65, "019352": 74, "019435": 72, "019439633": 161, "0194396330": 161, "0194396331": 161, "019533": 71, "019596": 85, "0196": 72, "019660": 29, "019729": 74, "019813": 72, "019901": 74, "01990373": 104, "019904": [127, 132], "01991": 72, "019940": 72, "019964": 72, "02": [16, 71, 74, 83, 84, 94, 95, 99, 127, 130, 132, 143], "02008393": [127, 132], "0201": 72, "02016117": 175, "020166": 99, "020271": 93, "020272": 90, "020348": 72, "020360838": 161, "0203608380": 161, "0203608381": 161, "02052929": [105, 144], "020599": 97, "020654": 71, "020747": 72, "02079162e": 143, "020837": [127, 132], "02092": 175, "021269": [87, 88], "02163217": 65, "021690": 88, "021728": 79, "021784": 72, "021866": 98, "021926": 85, "022": 96, "022131": 72, "022148": 73, "022258": 90, "022275": 72, "02247976": 65, "022511": 95, "022605": 71, "02262850": 143, "022629": 114, "022749": 114, "022768": 75, "022783": 102, "022799": 71, "0228": 72, "022820": 83, "022915": 93, "022950": 95, "023020e": [94, 95], "023182": 72, "023256": 99, "023319": 72, "023399": 71, "0234": 72, "023563": 161, "023728": 84, "024250": 83, "024266": 90, "02433298": 143, "024355": 70, "024364": 162, "024401": [87, 88], "024458": 72, "024604": 93, "024615": [39, 127], "02467": 101, "024703": 74, "024782": 99, "024926": 70, "024985": 79, "025": [83, 84, 87, 88, 90, 96], "025077": 161, "0253": 67, "025407": 92, "025443": 75, "025534": 72, "025703": 96, "025738": 71, "025813114": 161, "0258131140": 161, "02584": 67, "025841": [90, 97], "025889": 72, "025945": 72, "025964": 90, "025972": 72, "0261": 72, "026116": 71, "026161": 97, "026564": 72, "026723": 85, "026790": 72, "026966": 90, "02699695": 77, "02708816": [127, 130], "02710743": 143, "027168": 97, "027419e": 84, "027586": 71, "027880": 72, "02791": 75, "0281": 67, "028146": 92, "028328": 71, "028406": 97, "028417": 72, "0285": 72, "028520": [83, 84], "028539": 72, "028543": 72, "028626": 74, "028667": 74, "028681": 114, "028731": 114, "028868": 95, "028977": 72, "02900983": 99, "029010": 99, "029029": 72, "029050": 72, "029209": 178, "029232": 97, "029364": [162, 168], "0295": 72, "029657": 72, "029696": 84, "029831": 99, "029892": 74, "029910e": [94, 95], "02998611": 143, "02e": 66, "03": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 68, 71, 74, 80, 83, 84, 85, 89, 90, 94, 95, 99, 100, 102, 103, 127, 128, 130, 132, 143, 162, 168, 178], "03004459": 143, "030059": 114, "0301": [67, 72], "03018": 27, "030214": 72, "030242": 74, "030346": 175, "030354": 72, "03045": 68, "030497": 84, "030516": 72, "0306": 72, "030611": 72, "030668": 74, "0307": 67, "030735": 72, "030757": 73, "030817": 84, "030858": 78, "0309": 72, "030917": 72, "030934": 99, "030962": 99, "030964": 79, "031075": 92, "0311": 72, "03113": 104, "031134": [115, 121], "0312190390696285": 78, "031269": 75, "031323": 92, "031491": 92, "031639": 99, "031692": 92, "031712": 95, "03191": 176, "031942": 71, "032016": 74, "03220": 177, "03244552": [115, 116], "0325": 175, "032574": 72, "032664": 83, "032746": 97, "032953": 102, "033": 123, "033119": 115, "033136": 72, "033180": 74, "033208": 96, "033353": 26, "033435": 74, "0335": 72, "033508": 72, "033599": 120, "033661": 92, "033702": 71, "033779": 92, "033821": 72, "033946": [87, 88], "034": 96, "03411": 175, "034246": 73, "0343": 72, "034331": 72, "03438": 68, "034609": 84, "034617": 72, "034666": 84, "034690": 85, "03474476": 143, "0348": 72, "034812763": 161, "0348127630": 161, "0348127631": 161, "034846": 94, "03489": [44, 65, 93], "035017": 72, "035068": 73, "035119185": 161, "0351191850": 161, "0351191851": 161, "035203": 72, "035214": 71, "035265": 85, "0353": 72, "03536": 175, "03538": 67, "03539": 67, "035391": 75, "0354": 67, "035411": 175, "03545": 67, "0355": 72, "035545": 75, "035572": 75, "035579": 84, "035607": 72, "035613": 72, "035730": 99, "035735": 84, "03574": 75, "035762": 99, "0359": 67, "036029": 71, "0361": 72, "036101": 72, "036129015": 161, "0361290150": 161, "0361290151": 161, "036143": 99, "036147": 99, "036240": 80, "036288": 71, "0363": 64, "036309": 74, "036381": 83, "036577": 79, "036729": 93, "036749328": 143, "037": 96, "037008": [87, 88], "037114": 90, "037271": 83, "037301": 78, "037376": 71, "0374": 67, "037509": 104, "037529": 90, "037686": 72, "037747": [83, 84], "03783666": 99, "037837": 99, "037855e": 84, "038006": 84, "03803836": 143, "038085": 84, "038103": 90, "038206": 72, "038683": 73, "038718": 72, "038812": 98, "038831": 92, "038847": 83, "038966": 71, "039": 178, "03907122389107094": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "039141": 80, "039154": 95, "03917696": [144, 161], "0392": 72, "03920960e": 143, "039302e": 85, "039383": 96, "0394": 72, "039404": 72, "039516": 97, "039661": 90, "03968904": [127, 130], "039695": 71, "039895": 90, "039978": 72, "04": [16, 41, 66, 71, 74, 80, 83, 84, 94, 95, 99, 100, 102, 127, 128, 130, 132, 143, 178], "040": [62, 96], "040010": 90, "040067": 84, "040112": 161, "040118": 79, "040139": [83, 84], "040486": 72, "0405": 72, "040533": [40, 144, 161], "04053339": 161, "040556": [127, 130], "040568": 73, "040784": 80, "040813": 83, "040851": 72, "040912": 84, "041091": 83, "0411": 72, "041147": 85, "0412": 72, "041240": 72, "041262": 92, "041275e": 83, "041284": 85, "041381": 83, "041387": 85, "041491e": 85, "041557": 72, "041569": 96, "041632": 71, "04165": 127, "0418": 64, "041831": 85, "042034": 103, "042060": 92, "042108600": 143, "042265": 85, "042345": 73, "0425": [72, 115, 125, 126], "042501": 74, "042505": 72, "042517": 84, "042574": 71, "042583": 95, "04259053": 143, "042684": 72, "0428": [64, 104], "042804": 90, "042844e": 99, "042854": 83, "04300012336462904": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "043002": 84, "043049": 72, "043056": 74, "043087": 72, "0433": 64, "043477": 72, "0434e374": 67, "043534": 74, "043549": 71, "043580": 74, "043601": 72, "043652": 72, "043662": 72, "043820": 78, "04387": [115, 124], "043996": 84, "044": [96, 123], "044113": 85, "04415": 67, "044176": 90, "044239": 90, "04424": 67, "044399": 84, "044447": [108, 112], "04444978": 161, "044449780": 161, "0445": [79, 115, 124], "044507": 72, "044619": 71, "04465": 65, "04486": 175, "04487585": [162, 168], "04491": 127, "044929": 90, "04497975": [162, 168], "04501612": 161, "04502": [115, 124, 144, 161], "045062": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "045144": 93, "045172": 90, "045206": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "045258": 72, "045379": 175, "04552": 93, "045553": 85, "045624": 70, "04563": [115, 126], "045642": 73, "045754": 99, "04583": [115, 123], "04586": [115, 124], "045932": 99, "045984": 90, "045991": 83, "045993": [115, 116, 119], "046088": 99, "04608822": 99, "046238": 83, "04625": [115, 123], "046350": 74, "046451": 90, "046525": 114, "046527": 85, "046601": 73, "0466028": 65, "046654": 72, "046728": 102, "046788": 95, "04682310e": 143, "046844": 114, "046901": 72, "046922": [115, 119], "047028": 74, "047042855": 143, "047123": 84, "047134": 74, "047239": 90, "047288": 114, "0473": 83, "047390": 84, "047873": 90, "047954": 93, "048": 96, "048308": 88, "048476": 90, "0485": 72, "048699": 104, "048723": [115, 121], "048751": 72, "048783": 72, "049051": 71, "049729": 84, "05": [16, 17, 19, 58, 62, 64, 65, 66, 67, 68, 71, 74, 78, 83, 84, 85, 86, 93, 94, 95, 96, 99, 100, 103, 115, 120, 121, 123, 124, 125, 126, 127, 128, 130, 132, 143, 144, 161, 175, 178], "05039": 102, "050420": 97, "050701": 74, "050925": 72, "051": 67, "051186": 99, "051334": 73, "051336": 72, "051593": 73, "051651": 100, "051663": 74, "051712": 98, "051870e": 85, "052": 100, "052023": 90, "0520233166790431": 92, "052065": 71, "052169": 73, "05223162": 143, "052264": 72, "052301": 72, "0524": 72, "052488": 88, "052502": 99, "052745": 85, "052811": 95, "052973": 71, "053": [67, 127], "053188": 83, "0533": 64, "053331": 85, "053389": 161, "053468": 72, "053530": [127, 132], "053541": 99, "053558": 85, "053659": 69, "053898": [127, 132], "054": [67, 100], "054064": 92, "054068": 93, "054162": 93, "054279": 78, "054315": [127, 132], "054330": 84, "054339": 161, "054365": 72, "054370": 85, "054451": 74, "054529": 161, "054771e": 99, "054820": 71, "054993": 73, "055133": 71, "055165": 102, "055246": 72, "055280e": 83, "055410": 84, "055439": 95, "055493": 103, "055680": [72, 161], "056": 100, "056172": 79, "0563": 72, "056330": 73, "056499": 88, "056504": 71, "056915": 90, "057371": 95, "057535": 83, "0576": [66, 94], "057608": 83, "057704": 72, "057762": 99, "057810": 72, "057831": 97, "057847": 72, "057962": 85, "058036": 73, "058042": 161, "058175": 99, "058375": 80, "058383": 83, "058463": 99, "058467": 84, "058508": 104, "058538": 25, "058676e": 83, "0587": 72, "058743": 97, "058892": 83, "058903541934281406": 78, "05891": 127, "059": 96, "0590": 64, "059058": 84, "059338": 73, "059384": 99, "059396": 72, "059630": 70, "059685": 99, "059820": 71, "0599": 64, "059908": 71, "059936": 83, "059976e": 84, "06": [18, 31, 41, 71, 74, 80, 83, 84, 85, 94, 95, 99, 115, 123, 124], "060006": 71, "060016": 80, "06008533": 127, "060186": 72, "0602": 72, "060201": 99, "060212": [94, 95], "0603268864456956": 92, "060485": [71, 73], "060517": 72, "060672": [127, 130], "060706": 78, "060834": 72, "060845": 161, "06111111": 67, "0615": 64, "061705": 72, "062": [100, 127], "062167": [127, 130], "0622": 64, "062293": 84, "062414": 95, "062581": 71, "06269": 101, "06270135": 77, "062774": 83, "06278811": [127, 130], "0629": 64, "062964": 161, "063": 96, "063017": 80, "063067": 83, "063085": 73, "063098": 83, "0631": 64, "0633": 64, "063312": 95, "063327": 90, "06338236": 143, "0634": 64, "063428e": 95, "063449": 71, "0635": 64, "063590": 99, "063641": 84, "0638": 64, "06389": 127, "06397789": 99, "063978": 99, "063979": 83, "064036": 73, "064125": 72, "064129": 73, "064164": 95, "06428": 94, "064280": 94, "0643": 64, "064358": 72, "064528": 73, "0646": 64, "0646222": 66, "064673": 79, "065": 103, "0651": 64, "065139": 71, "0652": 64, "065277": 114, "065338": 72, "065356": [87, 88], "065453": 78, "065469": 73, "065497": 73, "065535": 30, "065659e": 74, "065725": 85, "0658": 64, "065969": 127, "065976": 90, "0660": 64, "066039": 84, "066070": 73, "0661": 72, "066295": 90, "0663": 64, "066464": 102, "066477": 72, "066478": 92, "0665": 72, "066568": 73, "0668": 73, "066855": 71, "06692492": 143, "0670": 64, "067212": 90, "067436": 92, "0675": 64, "067524": 71, "067528": 103, "067636": 73, "067639": 95, "067721": 161, "067778": 71, "067856": 74, "0680": 64, "068040": 71, "06811": 127, "068148": 99, "068221": 98, "06827": 102, "068340": 83, "068377": 95, "068669": 73, "068700": 114, "068817": 72, "068934": 80, "06895837": 65, "0690": 64, "069082": 73, "0691": 64, "069144": 92, "069443": 80, "0695854": 65, "069589": 90, "069634": 83, "069724": 72, "069761": 97, "0698": 64, "069985": 73, "07": [83, 84, 95, 99, 100, 103, 114, 127, 143], "070020": 99, "070085": 74, "0702": 64, "0702127": 65, "070393": 95, "0704": [64, 72], "070427": 83, "070433": 90, "070497": 103, "070797": 95, "07085301": 127, "070884": 99, "071279": 161, "0713": 64, "07136": [65, 93], "071543e": 85, "07168291": 65, "071731": 95, "071771e": 83, "071777": [115, 121], "071782": 29, "0718": 64, "07188595": 143, "07202564": [87, 88], "072052": 74, "072069": 26, "072153": 83, "07222222": 67, "072293": 98, "072377": 72, "072516": 90, "072583e": 84, "072605": 80, "0727": 127, "072756": 72, "072973": 72, "073": 100, "073013": 99, "073087": 72, "07318590": 143, "073207": 93, "0732109605604835": 78, "073293e": 83, "073300": 71, "073384": 90, "07347676": 65, "07350015": [36, 44, 65, 93], "073518": 85, "0735183279373635": 85, "073520": 85, "073654": [127, 132], "07366": [67, 115, 126], "07379901": 143, "073900": 84, "074303": 78, "074304": 161, "07436521": 143, "074426": 99, "07456127": 65, "074700e": 95, "07479278": 102, "074927": 80, "07525740": 143, "075261": 70, "075384": 99, "07538443": 99, "07544271e": 143, "07561": 175, "07564554e": 143, "0758": 103, "075809": 80, "075869": [115, 121], "076": 62, "076019": 94, "076119": 95, "076150": 161, "076179312": 161, "0761793120": 161, "076187e": 83, "076259": 72, "076279": 83, "076322": 99, "076347": 85, "0765": 67, "076684": 175, "076775": 120, "07685043": 143, "07689": 67, "07691847": 143, "076953": [87, 88], "076971": 75, "077073": 115, "077090": 92, "07723": 72, "07727773e": 143, "077319": 99, "077461": 97, "077502": [162, 168], "077504": 97, "077527e": 95, "077543": 83, "077592": 90, "077702": 80, "0777777777777778": [115, 126], "07777778": [67, 115, 126], "07781396": 143, "077883": 99, "077920": [108, 112], "07796": 127, "078": 123, "078090": 161, "07809234": 143, "078095": 16, "078138007883929": 78, "078151": 72, "078207": 75, "07828372": 161, "078382": 84, "078384e": 83, "078426": 114, "078474": 161, "078578": 72, "078810": 99, "078895": 97, "078987": 71, "079085": 75, "07915": 67, "07919896": 143, "07942v3": 176, "079458e": 94, "07961": 102, "079736": 73, "079761": [108, 112], "07978296": 143, "079932": 72, "08": [74, 85, 95, 99, 103, 143], "080041e": 83, "080121": 95, "08031571": 143, "080533": [127, 130], "0806": 72, "080633": 78, "080854": 95, "080890": 73, "08091581": 143, "080947": 75, "080987": 83, "081": [67, 96], "081100": 99, "081121": 72, "081230": [83, 84], "08131311": 143, "081396": 88, "081413": 72, "081488": 93, "08154161": 143, "081784": 97, "08181827e": 143, "082": 143, "0820": 64, "082142": 71, "082197": 90, "0822": 72, "082297": 143, "08230384": 143, "08240977": 143, "082436": 71, "082643": 16, "082804": 70, "082829": 97, "082973": 93, "083079": 95, "083153": 84, "083258": 161, "083312": 161, "08333333": 67, "08333617": 143, "0835771416": 65, "0836": 90, "08364": 90, "083687": 72, "083706": 103, "083949": 103, "084": 65, "084007": 89, "084269": 95, "084288": 83, "084660": 71, "084714": 84, "084807": 84, "084821e": 74, "084932": 73, "085": 96, "085120": 72, "085159": 97, "085279e": 87, "0853505": 65, "085372": 83, "085566": 85, "085592": 90, "085697": 78, "085730": 79, "08599499": 72, "086004": 90, "08602774e": 143, "0862": 173, "086264": 85, "086401570476133": 85, "086402": 85, "086559": 97, "08664208": 143, "086666": [127, 130], "086679": [115, 121], "086754": 71, "086801": 71, "086826": 87, "086873": 71, "087": 178, "087084": 71, "087151": 73, "087184": 38, "087370": [127, 130], "087566": 92, "087634": 83, "087826": 84, "08783333": 72, "0879": 64, "087947": 99, "088019": 83, "088048": 99, "088082": 92, "088282": 88, "088288": [108, 112], "088346e": 83, "088357": 99, "088401": 83, "08848": [115, 126], "088482": 29, "088533": 72, "088642": 74, "088792": 92, "088836": 92, "08888889": 67, "0889": [64, 79], "089117": 71, "089191": 84, "089229": 79, "089459": 127, "089647": 78, "08968939": 65, "089943": 73, "089991": 97, "08e": 66, "09": [38, 62, 83, 84, 85, 94, 95, 99], "09000000000000001": [115, 121], "09015": 64, "090255": 99, "090331": 84, "090363": 98, "090441": 71, "090491": 74, "090612": 72, "090728": 91, "090776314": 143, "091": 123, "091308": 114, "091391": 161, "091406": 162, "091432": 83, "091464": 74, "09174584": [127, 130], "091797": 72, "091992": 98, "091996": 96, "09218894": [127, 130], "0922": 64, "092229": 103, "092262": 127, "092365": 161, "092453": 99, "09245337": 99, "092646": 99, "092919": 163, "0929369228758206": 92, "093043": 99, "09310496": 161, "093153": 99, "093173": 83, "0932": 64, "0935": 127, "09351167": 127, "093607": 84, "093740": 161, "09377": 143, "093950": 93, "094026": 93, "094118": 99, "094129": 97, "094189": 71, "094381": 93, "094411": 71, "094420": 95, "09444444": 67, "094487": 74, "094530": 71, "094766": 98, "094799": 97, "094829": 127, "094883": 83, "094895": 71, "094968": 84, "094999": 99, "095104": 80, "09524859": 143, "095391": 74, "095566": 71, "095785": 80, "095835": [127, 132], "09603": 173, "096257": [127, 130], "096337": 93, "096418": 80, "096616": 24, "09682314": 127, "096890e": 114, "096915": 103, "096979": 71, "097": 100, "097009": 90, "097024": 72, "097140": 87, "097157": 103, "097208": 98, "097215": 74, "097290": 72, "097293": 71, "097468": 85, "09756": 101, "097754": 72, "09779675": 161, "097796750": 161, "098": 66, "09801463544687879": 78, "098256": 99, "09830758": 102, "098308": 102, "098319": 99, "098570": 71, "098645": 74, "098662": 71, "098712": 99, "098769": 71, "09879814e": 143, "098901": 90, "098924": 71, "09899716": 143, "099": 100, "09957868943595276": 78, "09961663": 143, "099642": 74, "099647": 98, "099670": 95, "099731": [83, 84], "09980311": 161, "09988": 176, "09e": 143, "0_": 43, "0ff823b17d45": 67, "0x": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "0x1747bdd4520": 75, "0x1747bdd6b90": 75, "0x7f028a1bb4a0": 78, "0x7f029668ba10": 78, "0x7f1af38e8c80": 98, "0x7f7d14a557c0": 178, "0x7f7d1510c710": 168, "0x7f7d152738c0": 162, "0x7f7d15389ca0": 161, "0x7f7d1538a1b0": 161, "0x7f7d15953530": 117, "0x7f7d15ac94c0": 161, "0x7f7d15acaea0": 161, "0x7f7d15b5fb90": 127, "0x7f7d15c051f0": 116, "0x7f7d15c06c30": 117, "0x7f7d1601bfe0": 120, "0x7f7d2537ef30": 115, "0x7f7d25407950": 128, "0x7f7d25692990": 127, "0x7f7d256930b0": 127, "0x7f7d259deb40": 128, "0x7f7d25c247a0": 127, "0x7f7d26a8eab0": 127, "0x7f7d2cd609b0": 115, "0x7f7d2d0b9a30": 127, "0x7f7d2d0e7b60": 127, "0x7f7d2d238950": 115, "0x7f7d2d6aa600": 115, "0x7f7f77616db0": 103, "1": [8, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 140, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177], "10": [10, 11, 12, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 105, 106, 107, 109, 112, 114, 115, 116, 117, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 132, 143, 144, 161, 162, 168, 173, 175, 176, 177, 178], "100": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 40, 44, 47, 65, 67, 68, 69, 77, 78, 83, 84, 86, 89, 90, 93, 96, 97, 101, 103, 104, 105, 107, 109, 112, 114, 115, 117, 120, 125, 126, 127, 129, 143, 144, 161, 162, 168, 175, 177], "1000": [17, 19, 25, 27, 63, 69, 70, 77, 78, 79, 81, 82, 87, 88, 89, 91, 92, 94, 95, 100, 102, 103, 106, 127], "10000": [62, 70, 74, 83, 84, 94, 99], "100000e": 95, "10003": 74, "100044": 88, "10006": 74, "10016": 74, "100208": 114, "100356": 85, "10038": 102, "10039862": [104, 127], "100510": 161, "100664": 83, "10074": 95, "100763": 71, "100796": 72, "10079785": 127, "1008": 72, "100807": [83, 84], "100858": 102, "10089588": 99, "100896": 99, "100923": 99, "100979": 74, "100_000": 96, "101": [18, 31, 41, 64, 100, 114, 176, 177], "10109": 114, "10112": 74, "10126": 95, "10127930": 161, "101279300": 161, "1014139": 143, "1015": [66, 94], "101569": 73, "1016": [17, 18, 19, 31, 41, 64], "1016010": 66, "1016338581630878": 92, "1018": 95, "101805": [127, 130], "10188955": 72, "101998": 80, "102": [107, 112, 114, 175, 177], "102282": 16, "10235": 95, "102567": 97, "102606": 71, "102616": 85, "102765": 71, "10277": 95, "102775": 85, "102863": 78, "10299": 94, "103": [83, 93, 100, 107, 112, 114, 177], "10307": 161, "1030891095588866": 92, "1031": 95, "103179163001313": 92, "1032116": 143, "103215": 16, "10348": 94, "103497": 99, "103806": 85, "1038994": 143, "10396": 94, "104": [66, 94, 114, 177], "1040": 95, "104016": 83, "10406": 95, "1041": 64, "10414": 95, "104353112": 143, "104383": 84, "10443848": 143, "104469": 96, "104475": 83, "10449": 101, "104492": 90, "1045303": 65, "104688": 71, "104787": 93, "104822": 71, "104913": [127, 130], "104917": [127, 130], "105": [43, 65, 90, 93, 114, 177], "1050": 97, "1051": 97, "1051036": 143, "105227e": 84, "105258": 71, "1053151": 143, "105318": 99, "1053478": 143, "1054": 67, "105461": 114, "1055": [64, 101], "1056": 64, "106": [62, 67, 114, 177], "10607": [75, 107, 112, 175], "106116": 95, "10627": 95, "1063": 79, "10635341": 72, "10637173e": 143, "106385": 161, "1065": [77, 86, 92, 127], "106595": [12, 127, 129], "106715": 89, "1067925": 143, "1068329": 143, "107": [67, 72, 103, 114, 177], "107073": 85, "107114e": 71, "107156": 90, "107290": 161, "1073": 95, "10735422": 72, "1074": 143, "107445": 72, "107467": [108, 112], "10747": [75, 107, 112, 175], "1074902": 143, "107743": 71, "107746": 99, "107809": 83, "10791": 95, "107935": 99, "108": [114, 173, 176, 177], "1080": [36, 44, 64, 65, 93], "108016": 74, "108206": 71, "10824": [75, 107, 112, 175], "108257e": 95, "1082586": 143, "108259": 80, "10829": 95, "10831": [75, 107, 112, 175], "108402": 73, "108451": 74, "108553830": 143, "108742": 16, "108778618": 143, "108783": 85, "108783087402629": 85, "10878571": 99, "108786": 99, "109": [78, 83, 114], "10903": 94, "109069": 161, "109079e": 99, "109145": 74, "109273": 93, "109277": 90, "10928": 95, "1093": [42, 45, 86], "109454": 95, "1095": 79, "1096": 101, "10967": 94, "109861": 175, "1099472942084532": 81, "109985": 71, "10e": [85, 99], "11": [40, 42, 62, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 93, 94, 95, 96, 97, 99, 100, 102, 103, 104, 105, 107, 109, 112, 114, 115, 116, 117, 119, 120, 121, 123, 124, 127, 128, 129, 130, 143, 144, 161, 162, 168, 175, 177, 178], "110": [78, 114, 177], "110081": 90, "1101": 95, "110179": 84, "11019365749799062": 103, "110194": 103, "110359": 93, "110365": 103, "110435": 74, "110557": 92, "1106": 64, "110681": 102, "11071087": [104, 127], "110717": 161, "110742": 92, "1109": 95, "110e": 127, "111": [84, 114, 177], "111043": 87, "1111": [10, 11, 46, 63, 65, 82, 86, 93, 103, 106, 127, 162, 168, 173], "11120": 95, "111352344760325": 92, "111526": 78, "111577": 95, "111604": 73, "111657": 73, "11165835": 72, "1117": [77, 86, 92], "111783": 95, "1118": 66, "11199615e": 143, "112": [67, 114, 177], "1120": 94, "112078": 114, "11208236": [105, 144], "112104": 74, "112135": 85, "1121351274811793": 85, "1122": [64, 95], "112216": 85, "112415": 78, "112611": 71, "112867e": 84, "11296435": 72, "113": [10, 114, 177], "113005": 95, "113022": 90, "11311": 94, "113149": 90, "113167": 71, "113192": 71, "113207": 99, "113270": 85, "11327273": 74, "113560": 97, "11376": 95, "113780": 93, "113952": 90, "11399": 94, "114": [96, 114, 177], "114026": 92, "114131": 71, "114258": 83, "114278": 74, "114332": 71, "114368": 74, "1144": 95, "1144500": 65, "11447": 102, "114530": 87, "1145370": 65, "114570": 84, "11458": 95, "114647": 85, "11470807": 143, "114834": 95, "114989": 80, "115": [114, 177], "11500": [94, 178], "115060e": 99, "1151": 95, "115297e": [94, 95], "11530": 95, "11552911": 102, "11570": 94, "115785": 16, "11588": 95, "115901": 80, "116": [114, 177], "11604254": 72, "116145": 71, "116253": 72, "116274": 85, "116492": 71, "1166": [94, 176], "1167": [72, 94], "11673": 95, "11676": 95, "11684973": [127, 130], "117": [83, 114], "11700": 178, "117194": 98, "11720": 95, "11724134": 143, "117242": 99, "11724226": 99, "117366": 99, "11743": 178, "117457": 84, "11750": 95, "117710": 85, "117832": 73, "11789998": 99, "117900": 99, "11792": 66, "11796": 95, "118": 114, "118018e": 127, "1182": 66, "11820": 95, "11823404": 103, "118255": 99, "118277": 73, "118286": 72, "118399": 71, "11850": 95, "118596": 85, "1186": 66, "118601": 93, "11861": 66, "118692": 74, "1187": 102, "118708e": 93, "1187339840850312": 93, "118799": 95, "1188": 64, "118952": 93, "119": [103, 114, 177], "11935": 102, "119370": 74, "119409": 92, "119486": 71, "1197": 143, "119766": 99, "1198": [65, 93], "119894": 71, "12": [4, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 57, 58, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 102, 103, 105, 107, 109, 112, 114, 115, 116, 117, 119, 120, 121, 124, 126, 127, 128, 129, 130, 132, 143, 144, 161, 162, 168, 173, 175, 176, 177, 178], "120": [68, 69, 92, 104, 114, 177], "12002": 94, "1200x600": [71, 72, 74], "1200x800": [71, 72, 74], "1202": 176, "120360": 71, "120567": [87, 88], "120721": 93, "120742": 71, "120814": 97, "12097": [10, 11, 46, 65, 86, 93, 95, 106, 173], "121": [95, 114, 177], "1210": 95, "12105472": 161, "121054720": 161, "1211": 95, "121297": 95, "1213405": 65, "1214": 161, "121584e": 99, "121750": 84, "121774": 89, "1219202": 143, "121938": 73, "12196389e": 143, "121970": 74, "121992": 71, "121995": 74, "122": [18, 31, 41, 64, 71, 100, 107, 112, 114, 176, 177], "12214": 66, "12223182e": 143, "122408": 85, "122421": 90, "122671e": 84, "122777": 161, "122802": 74, "123": [48, 66, 67, 71, 74, 94, 97, 103, 114, 177, 178], "1230": 95, "123041": 74, "123192": 103, "1232": 101, "12323": 95, "1234": [62, 63, 64, 75, 81, 82, 100, 101, 106, 115, 116, 119, 121, 125, 126, 143, 161], "12348": 103, "123501e": 95, "12373": 97, "124": 114, "1240": 101, "12410": 95, "1247297280098136": 78, "124744": 71, "124805": 94, "124837": 97, "125": [114, 177], "12500": 94, "125059": 161, "125193e": 74, "12539340": 161, "125649": 114, "12566840": 143, "12572": 95, "1258": [65, 95], "12596978": 143, "126": [114, 177], "126023": 78, "12612": 95, "12616": 95, "12616109": 72, "126354": 97, "126467": 73, "126494": 16, "126771": 161, "126802": 95, "12689": 95, "127": [31, 114, 177], "12705095": [144, 161], "12707800": 65, "1273": 64, "127337": 90, "127482": 74, "1275": 95, "12752825": 161, "127563": 102, "1277": 96, "12799943": 72, "128": [66, 114, 177], "12802": 66, "128061": 78, "12814": 95, "128229": 90, "128312": 99, "128372": 84, "128393": 95, "128408": 93, "128412": [108, 112], "128453": 72, "128532": 71, "128556": 71, "12861": 95, "128970": 73, "129": [93, 114, 177], "129085": 84, "129412": 71, "129446": 84, "12945": 176, "12945848305380006": 115, "129460": 74, "1295": [64, 95], "12955": 94, "1297": 95, "129722": 74, "129731": 74, "129803": 71, "12980769e": 143, "12983057": 127, "129881": 78, "13": [12, 14, 15, 17, 18, 19, 22, 24, 25, 26, 27, 28, 30, 33, 35, 37, 38, 39, 40, 41, 42, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 78, 79, 80, 83, 84, 85, 87, 88, 89, 90, 92, 93, 94, 95, 97, 99, 100, 101, 102, 103, 105, 107, 109, 112, 114, 115, 116, 117, 119, 120, 121, 124, 127, 129, 130, 143, 144, 161, 162, 168, 175, 178], "130": [67, 93, 114, 177], "130122": 102, "130255": 71, "13034980e": 143, "130370": 85, "130572": 71, "1306": 102, "130609": 84, "1307026549407077": 120, "130802": 83, "130829": 99, "13084": 127, "13091": 95, "130971": 87, "131": [114, 177], "1310": 72, "131017163": 143, "13102231": 127, "131071": 78, "13119": 102, "1312": [64, 178], "1313": [66, 178], "13137893e": 143, "131483": 90, "131842": 90, "131849": 71, "131928": [108, 112], "132": [67, 83, 93, 114, 177], "13208": 178, "1321": [94, 178], "13217265": 72, "132248": 114, "132255": 71, "132377": 74, "1324": [66, 94], "132454": 70, "132481": 114, "1325": 66, "13257": 94, "132579": 97, "132671": 85, "1328": 102, "132941": 95, "133": [67, 107, 112, 114, 176, 177], "133202": 95, "133249": 71, "133421": 95, "133422": 74, "133509": 83, "13355896": [127, 130], "13357": 95, "133596": 99, "133676": 84, "133839": 90, "13398": 103, "133f5a": 96, "134": [93, 104, 114, 177], "134037": 73, "1340371": 64, "1341": 66, "134211": 99, "13423359": 72, "1343": [94, 95], "134567": 95, "1346035": 66, "134664e": 83, "134686": 71, "134687": 95, "13474": 95, "134765": 95, "134784": 127, "1348": 94, "1349": 64, "13490": 95, "135": [67, 114, 127, 177], "13505272": 65, "135309": 83, "135372": 72, "135375": 80, "135378": 161, "135398": 84, "135508": 71, "135596": 95, "135707": [115, 121], "135856": 99, "13585644": 99, "135871": 93, "1359": 95, "136": [62, 75, 93, 103, 114, 177], "1360": [66, 79], "136010": 83, "13602": 103, "136089": 93, "136223": 71, "13642": 95, "136442": 93, "1365": 97, "136500": 97, "1366": 96, "13669227": 72, "136707": 71, "136836": 93, "136885": 95, "136898": 83, "137": [31, 67, 75, 114, 177], "1371": 95, "137165": 127, "137179": 97, "1372": 72, "137230": 92, "137234": 72, "137396": 99, "137448": 72, "137718": 73, "1378": 95, "137999": 127, "138": [114, 177], "1380": 94, "13809": 95, "138142": 87, "138264": 103, "138378": 85, "1384": 94, "13846074": 72, "13868238": 161, "138682380": 161, "138698": 161, "138715": 73, "138841": [127, 132], "1389": 64, "13893": 95, "139": [103, 114, 175], "1391": 64, "139491": 161, "139508": 90, "139527": 97, "13956": 102, "139622": 95, "139659": [127, 132], "139778": 74, "1398": 95, "139811": 78, "13993416": [127, 130], "14": [63, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 78, 79, 80, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 102, 103, 105, 107, 109, 112, 114, 115, 116, 117, 119, 120, 121, 127, 129, 130, 143, 144, 161, 162, 168, 175, 176, 178], "140": [68, 69, 90, 95, 104, 114, 177], "1400": 95, "14000073": 143, "1401": 64, "14018": 127, "140406": [127, 130], "140530": 83, "140770": [83, 84], "140833": 85, "140861": 65, "140941": 71, "141": [95, 114, 177], "14104495": 72, "141069": 84, "141101e": 95, "14114": 102, "141218": 97, "141247": 80, "14141": 95, "141484": 83, "14152194": 72, "141546": 161, "1418": 72, "141820": 85, "142": [114, 177], "14200098": 161, "142045e": 93, "142132": 83, "142142": 71, "142270": 70, "1424": [115, 125, 126], "142482": 99, "14268": 127, "142711": 73, "14281403493938022": [115, 121], "14289": 95, "143": [107, 112, 114, 177], "143088": 74, "1435": 95, "143593": 83, "14368145": 161, "14389651": 72, "143943": 78, "144": [114, 177], "14400": 94, "14405": 95, "14406": 95, "144084": 85, "144195": 84, "1443": 95, "144530": 71, "144669": 99, "144800": 85, "144813": 80, "144908": 98, "144971": 94, "145": [96, 114, 177], "145056": 71, "145082": [127, 130], "145189": 97, "145245": 99, "14530": 95, "14532650": 161, "145625": 99, "145699e": 83, "145737": 161, "14588": 95, "145998": 72, "146": [114, 177], "146037": 99, "146087": 175, "146214": [108, 112], "14625": 95, "146446": 71, "1465": 66, "146641": 161, "146646": 97, "14677029": 74, "146786": 74, "1468115": 65, "146861": 114, "14688": 74, "146883": 71, "147": [114, 177], "14702": 75, "147121": 99, "147147": 97, "147383": 84, "14744": 95, "147464": 114, "14772": 95, "1479": 95, "14790924": 161, "147909240": 161, "147927": 75, "14795": 95, "148": [62, 114, 177], "148005": 80, "14803": 95, "148134": [83, 84], "148161": 99, "14818": 74, "148210": 85, "1482102407485826": 85, "14845": 75, "148455": 85, "1484554868601506": 85, "1485": 95, "148750e": [94, 95], "148835": 95, "149": [114, 177], "1492": 62, "149228": 103, "14923198": 72, "149265": 84, "149285": 99, "149392e": 114, "149446427": 83, "149472": 103, "149538": 71, "149681": 83, "149714": 93, "149724": 74, "149779": 74, "149858": 28, "149897": 84, "149898": 99, "15": [11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 63, 65, 66, 67, 69, 71, 74, 77, 78, 79, 80, 83, 84, 85, 89, 90, 91, 92, 93, 94, 95, 97, 99, 100, 101, 102, 103, 107, 112, 114, 115, 116, 117, 119, 120, 121, 127, 129, 130, 143, 144, 161, 162, 168, 175, 178], "150": [43, 67, 97, 103, 114, 177], "1500": 97, "15000": [66, 94], "150000": 66, "15000000000000002": [85, 95, 99, 115, 121], "1502": 65, "150200": 93, "150234": 95, "150408": 65, "150435": 95, "150614": 75, "1507": 64, "150719e": 94, "150871": 71, "151": [96, 114, 177], "15108": 95, "15121745": 72, "151285": 74, "15142127": 72, "151447": 92, "151570": 71, "15157396": 72, "151618": 71, "151636": 85, "151819": 99, "15194": 94, "152": [114, 177], "1520042": 72, "152148": [83, 84], "152402": 97, "152414": 84, "152563": 97, "152706": 127, "15275": 74, "15287": 95, "152926": 70, "153": [103, 114, 177], "153119": 85, "15320478": 97, "153293": 95, "15332": 74, "15339": 95, "153398": 95, "15354": 102, "153587": 93, "153633": 75, "153639": 143, "153652": 71, "153971e": 83, "153983": 85, "1539831483813536": 85, "154": [79, 114], "15430": 178, "154415": 161, "1545": 95, "154557": 99, "154630": [127, 132], "154707": 80, "154752": 161, "154828": 85, "155": [79, 114, 177], "155000": 94, "155025": 99, "155120": 99, "155160": 80, "1554": 95, "155423": 80, "15549": 95, "15556": 95, "1557093": 65, "155995": 84, "156": [114, 177], "156021": 99, "1562": 64, "156202": [83, 84], "156205": 78, "156209": 84, "156239": 71, "156317": [83, 84], "156328": 92, "1564": 161, "156540": 161, "156554": 83, "156656": 74, "156704": 95, "1569": 95, "156969": 85, "157": [84, 114, 177], "157080": 161, "157470": 98, "157493": 72, "1576": 95, "1577657": 65, "158": [114, 177], "158007": 99, "158030": 73, "158076": 84, "15815035": 66, "158178": 85, "158198": 95, "1582": 95, "158288": 95, "1586": [64, 95], "158682": 92, "158697": 161, "158726": 114, "1589": 95, "159": 177, "15915": 73, "15916": [64, 73], "159190": 97, "15931726": 72, "159386": 102, "159466": 99, "15946647": 99, "159592": 84, "1596": 67, "159826": 95, "159857": 97, "159959": 95, "16": [24, 62, 63, 65, 66, 67, 68, 69, 71, 73, 74, 78, 79, 80, 83, 84, 85, 88, 89, 90, 93, 94, 95, 97, 99, 100, 102, 103, 107, 112, 114, 115, 116, 117, 119, 120, 121, 123, 124, 125, 126, 127, 129, 130, 143, 144, 161, 162, 168, 175, 178], "160": [68, 69, 104, 177], "160285e": 84, "160296": 74, "1604": 66, "16063597": 72, "16075658": 72, "160825": 87, "160836": 90, "160932": 85, "161": [67, 79, 176, 177], "161141": 93, "16118746": [127, 130], "161236": 99, "161243": 99, "161305": 83, "161441": 69, "16164827": 72, "1619": [66, 72], "162": [79, 177], "16201": 95, "162018": 73, "16211": 94, "162153": 99, "1622": 95, "162389": 83, "16241": 95, "162436": 103, "16249": 95, "16261656": 72, "1626685": 65, "162683": 103, "162710": 85, "162752": 92, "1628": 94, "16289493": 72, "162909": 93, "162930": 95, "162933": 97, "163": 177, "163013": 16, "163194": 99, "163312": 72, "163566": 95, "163587": 71, "163895": 85, "164": [80, 95, 100, 127, 177], "164034": 161, "16403495": 72, "164467": 94, "164608": 99, "1646101": 143, "164698": 89, "164701": 71, "164763": 97, "164801": 99, "164805": 85, "164864": 93, "164941": 16, "164943": 92, "165": [79, 103, 177], "16500": 94, "165012": 74, "165178": 99, "1653": 95, "16536299": 161, "165362990": 161, "16539906e": 143, "165419": 99, "16553": 94, "165549": 175, "165670e": 84, "16569492": 72, "165707": 80, "16590": 95, "165912": 74, "166": [79, 177], "166088": 79, "1661": 94, "16618": 95, "166322": [108, 112], "166517": 90, "166593": 71, "166713": 71, "166904": 95, "167": [66, 94, 177], "167035": 95, "16733637": 74, "16745936": 143, "16747866": [127, 130], "167547": 99, "1676": 95, "1676705013704926": 78, "167824": 97, "167930": 95, "167981": 161, "168": 177, "16803512": 161, "168084": 72, "168092": 161, "1681": [72, 94], "168195": 102, "168614": 99, "1687": 64, "168931": 99, "169": [67, 177], "1690": 64, "1691": 95, "16910": 95, "169117": 103, "169196": 99, "169220e": 85, "16951": 95, "169632": 97, "169677": 91, "16984": 95, "169858": 92, "169875": 74, "169901": 71, "169952": 97, "169981": 71, "17": [63, 65, 66, 67, 69, 71, 74, 78, 79, 80, 83, 84, 89, 90, 92, 93, 94, 95, 97, 99, 100, 102, 103, 107, 112, 114, 115, 117, 119, 120, 121, 123, 126, 127, 129, 130, 143, 144, 161, 162, 168, 175, 178], "170": 177, "1705": 95, "170652": 74, "170705": 114, "17083": 95, "171": [78, 79, 177], "1712": 176, "171230": 74, "171255": 114, "171290": 72, "1714": 66, "171550": 97, "171575": 99, "1715858": 72, "171696": 114, "171815": [115, 121], "171840": 73, "171849": 97, "171860": 84, "1719": [71, 72, 73, 74], "172": [100, 177], "17200365": 72, "172022": 161, "17203181": 72, "172083": 84, "17210698": 72, "17212231434661667175848586889193959697": 143, "1721223143466166717584858688919395969751015172324323441424549505355596568799891416181927293039404448565862647273749046111213202535365152546976788081839499": 143, "17219434": 72, "172672": 37, "172793": 99, "172820": 74, "173": [79, 177], "17304053": 72, "173367": 71, "173504": 88, "173542": 96, "1736": 64, "17372": 95, "173835": 71, "17385178": [115, 116], "173964": 161, "173e": 100, "174": 177, "174177": 99, "174185": 99, "174499": 161, "174516e": 99, "17453": 95, "1746": 95, "17469": 95, "174968": 94, "174e": 127, "175": [79, 177], "17500": 95, "1751": 94, "175254": 94, "175267": 74, "175284": 85, "17536": 95, "175564e": 84, "175635027": 65, "17576": 95, "175881": 71, "175894": 103, "175931": [114, 115, 117, 127], "176056e": 114, "176495": 99, "17655394": 99, "176554": 99, "176929": 161, "177": [176, 177], "177007": 99, "17700723": 99, "177015": 78, "177043": [83, 84], "1773": 95, "177397": 95, "177496": 99, "177611": 99, "177751": 99, "177995": 99, "178": [95, 177], "17800": 95, "17807": 95, "178169": 87, "17823": 67, "178273": 84, "178599": 97, "178661": 161, "178751": 79, "178763": 99, "178781": 74, "178934": 161, "179": [91, 95, 97, 108, 112, 177], "179101": 114, "179204": 84, "179250": 74, "179276": 84, "179548": [108, 112], "1795850": 65, "179588e": 99, "179810": 84, "1798913180930109556": 96, "179966": 97, "18": [63, 65, 66, 67, 69, 71, 74, 75, 77, 78, 79, 83, 84, 87, 89, 90, 92, 93, 94, 95, 97, 99, 100, 102, 103, 107, 112, 114, 115, 117, 120, 121, 123, 127, 129, 130, 143, 161, 175, 178], "180": [68, 69, 78, 96, 104, 177], "180059": 73, "180103": 74, "180271": 90, "180296": 84, "1803": 64, "18030": 95, "180307": 84, "180474": 84, "180575": [87, 88], "1807": [64, 95], "1809": 176, "180930": 97, "180951": 99, "181": [71, 177], "18106423": 74, "181101": 74, "181125018": 143, "1812": 95, "181330": 80, "181350": 83, "1814": 64, "18141": 95, "181432": 161, "18145027": 143, "182": 177, "182018": 84, "182151": 114, "18218759": 72, "182288": 84, "18234463": 72, "182354": 84, "182399": 16, "182633": 99, "182826": [127, 132], "182849": 99, "183": [67, 79, 127, 177], "1832": 95, "183340": 71, "183373": 127, "183459": 74, "183499": 71, "183526": 85, "183553": 100, "18356413": 127, "18368": 95, "183814": 95, "183855": [115, 121], "183888": 93, "183958": 74, "184": [67, 74, 176, 177], "184075": 120, "184178": 115, "184224": 80, "184238": 74, "184449": 83, "184690356": 143, "184806": 79, "185": [66, 67, 74, 78], "185035": 83, "185091": 97, "185130": 100, "185617": 84, "185774": 71, "185930": 114, "18595": 96, "185956e": 84, "186": 177, "18604": 95, "18610137": 143, "186136": 84, "18622": 95, "18637": 173, "186589": 80, "18666": 95, "186689": 127, "186735": 99, "18678094e": 143, "186836": 99, "186864": 92, "186868": 69, "187": 177, "187148": 161, "187617": 84, "187690": 99, "187709": 73, "18773": 95, "187732": 83, "187782": 83, "187878": 97, "1879": 79, "187977": 74, "188": [74, 177], "188175": 99, "1881752": 99, "188223": 99, "188360": 74, "188369": 72, "18841787": 143, "188525": 71, "188545": 84, "1887": [114, 115, 117, 127], "188760": 90, "1888": 95, "18888149e": 143, "188986e": 83, "189": [67, 71, 95, 100, 177], "189023": 161, "189195": 95, "1895815": [44, 65, 93], "18973327": 143, "189737": 99, "189739": 80, "18976": 95, "189890": 74, "189998": 99, "19": [15, 63, 65, 66, 67, 69, 71, 74, 78, 79, 83, 84, 90, 92, 93, 94, 95, 97, 99, 100, 102, 103, 107, 112, 114, 115, 120, 121, 123, 127, 129, 130, 143, 161, 175, 178], "190": [67, 71, 177], "190000e": 95, "190032": 71, "190090": 161, "190181": 71, "190231": 95, "19031969": 99, "190320": 99, "19033538": 65, "19073905e": 143, "190796": [127, 128], "190809": 99, "190892": 103, "1909": [44, 65, 93], "190915": 85, "190921": 88, "190955": 73, "190976": 100, "190982": 99, "191": [67, 176, 177], "191034": 74, "19115": 74, "1912": 176, "1912705": 106, "191501": 83, "191534": 94, "191578": 25, "192": [71, 78, 177], "192234": 161, "1923": 95, "192314": 71, "192526": 102, "19252647": 102, "192539": 29, "192952": 80, "193": [74, 177], "193018": 74, "193060": 99, "193308": 29, "193375": 90, "193494": 74, "193495": 71, "193581": 83, "193644": 95, "19374710e": 143, "19382": 95, "193833": 95, "19383495": 143, "193849": 100, "19385": 95, "193870": 83, "193f0d909729": 67, "194": [74, 77, 177], "19404": 71, "1941": 66, "19413": [94, 95], "194167": 83, "194588e": 83, "1946": 64, "194945": 114, "195": [71, 74, 78, 96, 109, 112, 177], "19508": 103, "19508031003642456": 103, "19509680e": 143, "195101": 83, "195132": 84, "195137": 74, "195377": 99, "195396": 99, "19550": 95, "195564": 93, "19559": [66, 94], "195816": 83, "1959": 176, "195920": 79, "195963": 90, "195982": 74, "196": [78, 177], "196099": 127, "196189": 99, "196303": 73, "196655": 90, "19680840": 161, "196824": 99, "196835": 92, "19695": 74, "197": [78, 108, 112, 177], "1970": 95, "19705": 95, "197225": [75, 107, 112, 175], "1972250000001000100001": [67, 107, 112, 175], "197356": 83, "197424": [115, 121], "197434": 72, "197462": 84, "197484": 161, "1975": 95, "19756": 95, "19758": 95, "197600": 70, "1978": [127, 140], "197910": 74, "19793": 95, "19798": 95, "1979957": 143, "198": [74, 78, 177], "198221": 95, "19824": 95, "1983609": 143, "198493": 90, "198503": 100, "198529e": 84, "198549": 75, "198617": 98, "198687": 66, "198719": 74, "1988": [63, 82, 106, 127], "199": [71, 78, 94, 177], "1990": [66, 94, 95], "1991": [66, 94, 95, 178], "199213": 84, "199281e": 99, "199445": 161, "1995": [65, 93], "19962187": 127, "1998": 96, "19983954": 104, "1999": 96, "1_": [85, 99], "1d": 13, "1e": [12, 15, 57, 58, 78, 90, 95, 97], "1f77b4": 70, "1m": [115, 123, 124], "1mnon": [115, 123], "1x_4x_3": 70, "2": [10, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 47, 48, 49, 51, 54, 56, 57, 58, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 119, 120, 121, 123, 124, 126, 127, 128, 129, 130, 131, 132, 133, 136, 140, 143, 144, 161, 162, 163, 165, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177], "20": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 37, 38, 39, 40, 41, 42, 44, 46, 47, 63, 65, 66, 67, 68, 69, 70, 71, 74, 77, 78, 79, 83, 84, 85, 89, 90, 91, 92, 93, 94, 95, 97, 99, 102, 103, 104, 105, 106, 107, 112, 114, 115, 120, 121, 123, 126, 127, 129, 130, 143, 144, 161, 162, 168, 175, 178], "200": [17, 19, 32, 35, 43, 64, 68, 69, 71, 74, 77, 78, 85, 86, 98, 99, 104, 106, 111, 112, 115, 120, 121, 125, 126, 176, 177], "2000": [11, 30, 66, 68, 80, 83, 84, 85, 90, 94, 95, 99, 101, 104, 114, 127], "20000": [66, 94], "20000000000000004": [85, 95, 99], "200000e": 95, "200042": 83, "2001": 72, "20010": 95, "200110": 95, "2002": 64, "200225": 79, "2003": [10, 72, 176], "200303": 175, "2004": [72, 127, 131], "2005": [69, 72], "20055": 95, "2006": [72, 95, 127, 131], "20069141": 72, "2007": [72, 127, 131], "20074": 95, "200982": 87, "201": [62, 67, 71, 177], "2010": [65, 93], "20102216": 72, "201053": 74, "2011": [65, 93, 173, 175], "20112577": 72, "2013": [86, 161, 176], "2014": [161, 176], "2015": [43, 176], "201528": [83, 84], "20158": 95, "20159577": 72, "2016": 96, "20167273": 77, "2017": [34, 176], "201768": 93, "2018": [10, 11, 46, 47, 63, 65, 66, 69, 77, 82, 86, 93, 94, 95, 101, 102, 106, 127, 133, 143, 144, 152, 161, 173, 176, 177], "2019": [32, 67, 83, 84, 85, 87, 88, 95, 99, 102, 115, 125, 144, 150, 153, 154, 173, 175, 176], "201e": 100, "202": [71, 74, 177], "2020": [12, 14, 15, 17, 18, 19, 31, 33, 35, 41, 64, 67, 69, 71, 74, 91, 103, 115, 125, 127, 129, 133, 162, 163, 176], "2020435": 65, "2021": [17, 19, 42, 44, 64, 65, 67, 71, 72, 73, 74, 83, 84, 93, 127, 128, 131, 133, 144, 155, 176, 177], "20219609": 65, "2022": [102, 103, 127, 129, 133, 162, 163, 172, 173, 176], "202298": 84, "2023": [36, 68, 101, 104, 127, 142, 144, 159, 160, 176], "202391": 71, "2024": [62, 77, 81, 86, 92, 96, 100, 101, 103, 127, 173, 176], "2025": [16, 17, 19, 39, 45, 71, 74, 78, 97, 101, 127, 128, 130, 132, 140, 176], "202650e": 85, "20269": 95, "2029554862": 70, "203": [66, 71, 74, 83, 94, 177], "203082": 114, "2032": 79, "203284": 85, "20329": 95, "20343894": 143, "203517": 71, "203828": 95, "203932": 71, "204": [71, 177], "204007": 99, "20400735": 99, "20416604": 143, "204362": 103, "204482": 99, "204495": 84, "204794": 99, "204893": 80, "205": [71, 74, 100, 102, 177], "205100": 74, "205187": 85, "205224": 102, "205245": 95, "205280": 71, "205333e": 94, "205355": 74, "20550957": 72, "20579722": 72, "20589430": 143, "205938": 93, "206": [71, 74, 78, 79, 177], "20603": 74, "206079": 72, "206253": [94, 95], "206256": 90, "206273": 71, "20656066": 143, "206561": 97, "206703": 71, "206752": 79, "206802": 26, "206949": 74, "206955": 71, "207": [71, 78, 100, 177], "207221": 71, "207242": 84, "2075": 64, "207607": 97, "207628": 71, "20783816": 65, "207840": 88, "207885": 94, "207912": 161, "207991": 72, "208": [71, 74, 78, 79, 80, 177], "2080": 72, "208051": 71, "2080787": 65, "20823898": 65, "208300": 99, "208485": 79, "2086": 95, "20877105": [127, 130], "208908": 74, "209": [71, 74, 78, 79, 80, 96], "209219e": 102, "20956": 95, "209623": 71, "209894": 99, "209963": 71, "21": [10, 11, 46, 63, 65, 66, 67, 69, 71, 74, 78, 79, 83, 84, 86, 92, 93, 94, 95, 97, 99, 101, 102, 103, 106, 107, 112, 114, 115, 120, 121, 123, 125, 126, 127, 129, 143, 161, 173, 175, 176, 178], "210": [17, 18, 19, 35, 41, 71, 74, 78, 79, 80, 95], "210215": 71, "2103": 173, "2103034": 65, "210319": [83, 84], "210323": 99, "2104": 177, "2107": 176, "21082": 95, "211": [74, 78, 79, 80, 100, 177], "21105": [67, 115, 125, 173, 175], "211127": 71, "2112": 103, "2114026": 99, "211403": 99, "21142": 95, "211534": 85, "211682": 84, "211720": 74, "212": [71, 74, 80, 96, 177], "2121": 95, "212317": 99, "212491": 99, "21257396e": 143, "212811": 80, "212844": 93, "213": [78, 79, 80, 100, 176, 177], "21317068": 74, "213199": 127, "21342": 95, "21351": 95, "2138": 79, "213846": 72, "2139": 33, "214": [71, 74, 78, 79, 177], "2147": 64, "214732": 83, "214764": 102, "214769": 90, "215": [71, 78, 80, 177], "215103": 84, "215159": 95, "215342": 99, "2155": 95, "21550": 95, "21562": 95, "215641": 74, "21577625": 74, "21595548": 74, "215958": 161, "216": [70, 79, 80, 177], "216113": 99, "216207": [115, 121], "216215e": 83, "216224": 84, "21624417": 65, "2163": 95, "216333": 71, "216344": 99, "21669513e": 143, "2167": 95, "216943": 114, "217": [71, 78, 79, 80, 100, 176, 177], "21716": 95, "2171802": [65, 93], "217244": 27, "2175": 95, "217550": 72, "217684": 90, "217755": 74, "218": [71, 78, 79, 80, 177], "21804": [66, 94], "218173": 71, "218223": 92, "218383": 80, "218546": 99, "218550": 71, "218618": 97, "2189": 95, "218924": 95, "219": [18, 31, 41, 64, 71, 74, 78, 79, 80, 97, 108, 112, 176, 177], "2191274": 65, "219176": 74, "219287": [127, 130], "219350": 74, "219382": 71, "219585": 80, "219834": 71, "22": [63, 65, 66, 67, 69, 71, 74, 78, 79, 83, 84, 92, 93, 94, 97, 99, 100, 102, 103, 114, 115, 120, 121, 123, 127, 129, 143, 161, 175, 178], "220": [71, 74, 78, 79, 80, 177], "220211": 95, "220381": 74, "220446e": 74, "220587": 16, "220666": 97, "220772": 99, "220780": 74, "220834": 97, "221": [71, 74, 79, 80, 96, 177], "221127": 83, "2213": 93, "221307": 74, "2214": 93, "2215": [64, 93], "2216": 93, "2217": [65, 93], "2218": 176, "222": [71, 74, 79, 177], "2222": [63, 65, 82, 127], "222261": 114, "222264e": 84, "222306": 92, "222430": 84, "22272803e": 143, "222797": 97, "222843": 99, "223": [71, 74, 78, 100, 177], "223095": 69, "223161": 74, "22336235": 65, "223405": 73, "223485956098176": [87, 88], "223625": 16, "22375856": 65, "22390": 94, "224": [74, 100, 177], "224073": 79, "2244": 176, "224822": 84, "224897": [83, 84], "225": [17, 19, 64, 71, 74, 104, 176, 177], "2250": 127, "22505965": 65, "22507006e": 143, "225084": 97, "225175": 99, "22528": 95, "225427": 80, "225574": 93, "2256": 95, "22562": 95, "225635": 99, "22563538": 99, "225764": 92, "225776": 103, "225917": 92, "226": 177, "226262": [127, 130], "226479": 90, "226524": 99, "226581": 74, "226598": 93, "226919": 84, "226938": 88, "227": [71, 74, 78, 108, 112, 177], "227018": 85, "227018245501943": 85, "227086": 92, "2271071": 36, "227184e": 84, "227494": 97, "227567": 114, "227621e": 84, "2279": 95, "227932e": 94, "228": [71, 74, 78, 177], "228035": 95, "2281": 95, "228648": [66, 127, 130], "228654": 74, "228725": 95, "229": [66, 71, 74, 177], "22913609": 72, "22913612": 72, "22914349": 72, "22914466": 72, "22925": 95, "229387": 71, "229443": 99, "229452": [114, 115, 117, 127], "229472": 94, "2295": [74, 95], "22959": 95, "229726": 95, "229759": [115, 121], "229961": [83, 84], "229994": [83, 84], "22m": [115, 123, 124], "23": [51, 56, 65, 66, 67, 69, 71, 74, 78, 83, 84, 91, 92, 93, 94, 97, 99, 102, 103, 107, 112, 114, 115, 120, 121, 123, 127, 143, 161, 173, 175, 176, 178], "230": [17, 19, 64, 74, 176, 177], "230009": [87, 88], "230115": 97, "2302": 101, "230368": 83, "23055555": 72, "2307": [65, 93, 101, 106], "23072381": 72, "2308": 102, "230821": 95, "230951": [127, 130], "230956": 70, "231": [10, 108, 112, 177], "231165": 127, "23122993": 72, "23124913": [127, 130], "231310": 99, "23134916": 72, "231430": 161, "231467": 127, "231647": 74, "231734": 127, "231798": 127, "231831": 74, "231835": 97, "231879": 79, "231986": 99, "231e": 100, "232": 177, "232134": [83, 84], "232277": 84, "232411": 71, "232485": 84, "232497e": 83, "232659": 78, "232687": 97, "232740": 74, "232774": 83, "232872e": 96, "232959": [87, 88], "232e": 127, "233": [34, 71, 177], "2331": 95, "233126": 97, "233154": 178, "233415": 95, "233573": 78, "23368": 95, "233734": [127, 128], "23391813": 72, "233931": 78, "234": [74, 176, 177], "2341": 64, "234137": 103, "234153": 103, "234534": 85, "234605": 75, "234910": 93, "235": [176, 177], "2352": 64, "2353": 64, "235419": 78, "235642": 96, "2359": 178, "236": [78, 177], "236008": 85, "23611": 95, "236159": 71, "236292": 74, "236411": 99, "236549": 72, "236719": 73, "23689448": [127, 130], "23690345e": 143, "237": [67, 74, 177], "237158": 84, "237164": 84, "237461": 102, "23751359e": 143, "237756": 79, "237976": 71, "238": [65, 93, 96, 123, 177], "238012": 99, "23801203": 99, "238101": 99, "238213": 161, "238251": 85, "23826283337384757606367707782878992100": 143, "238262833373847576063677077828789921001721223143466166717584858688919395969751015172324323441424549505355596568799846111213202535365152546976788081839499": 143, "2382628333738475760636770778287899210017212231434661667175848586889193959697510151723243234414245495053555965687998914161819272930394044485658626472737490": 143, "238262833373847576063677077828789921001721223143466166717584858688919395969791416181927293039404448565862647273749046111213202535365152546976788081839499": 143, "2382628333738475760636770778287899210051015172324323441424549505355596568799891416181927293039404448565862647273749046111213202535365152546976788081839499": 143, "238287": 83, "238416": 83, "238449": 84, "23856": 95, "238582e": 71, "238619": 80, "238718": 84, "238826": 73, "238854": 72, "239": 177, "239019": 90, "239155": 74, "239352": 95, "239503": 83, "23968": 95, "239845": 95, "23e": 66, "24": [42, 65, 66, 67, 71, 74, 78, 83, 84, 92, 93, 94, 97, 99, 102, 103, 104, 114, 115, 121, 123, 127, 143, 161, 175, 176, 177, 178], "240": 92, "240053": 84, "240127": [83, 84], "24018": 97, "240194": 83, "240295": 102, "2403": 101, "240321": 74, "240495": 114, "240532": [83, 84], "240693": 114, "24080030a4d": 67, "240813": 89, "240853": 74, "240883": 74, "241": [78, 123, 177], "241049": 99, "241064": 84, "241260": 74, "241474e": 84, "241503": 114, "241841": 98, "241962": 103, "24197136": [127, 130], "241973": 95, "241e": 100, "242": [176, 177], "242124": [94, 95], "242139": 161, "242158": [94, 95], "2424": 90, "242427": 90, "242433099": 143, "242559": 69, "242604": 78, "242815": 161, "242864": 95, "242902": 99, "242907": 73, "243": [95, 177], "243056": 73, "2430561": 64, "243180": 78, "243246": 99, "243457": 98, "243743": 71, "2438": 95, "24385009": 143, "243e": 100, "244": [95, 177], "244256": 71, "244455": 99, "2445": 95, "244622": 161, "244627": 78, "244633": 97, "24469564": 175, "245": [176, 177], "245036": 74, "245062": 99, "2451": 64, "24510393": 66, "245370": 93, "245512": 99, "245552": 84, "245610": 84, "245720": 70, "2458": 64, "245963": 71, "246": [74, 177], "2467506": 65, "246753": 99, "2468": 95, "246879": 99, "246981": 97, "247": [62, 74, 100, 177], "247002": 74, "247020": 85, "247056": 74, "247057e": 99, "2472": 95, "247617": 114, "24767182": 143, "24774": [94, 95], "247823": 74, "247826": 93, "247998": 74, "248": [95, 177], "248171": 99, "248178": 83, "24835916": [127, 130], "248441": 114, "248487": 92, "248638": 85, "248725": [127, 130], "2489": 79, "248907": 74, "248919": 71, "249": [65, 93, 96, 177], "249067": 84, "2491": 95, "249100": 93, "24917": 95, "249306": 92, "249494": 83, "249637e": 74, "2499": [72, 109, 112], "249970": 78, "25": [17, 18, 19, 29, 30, 31, 35, 41, 42, 43, 44, 45, 46, 65, 66, 67, 70, 71, 74, 78, 83, 84, 85, 86, 92, 93, 94, 95, 97, 99, 103, 114, 115, 121, 127, 143, 161, 175, 178], "250": [39, 45, 96, 127, 177], "2500": [72, 95, 109, 112, 127, 132], "25000000000000006": [85, 95, 99], "250083": 95, "2501": 95, "250176": 71, "250210": 85, "250425": 85, "250529": 92, "2506": 101, "250838": 92, "251": [94, 95, 102, 177], "251152": 95, "252": [95, 177], "25208393": 74, "252133": 95, "252253": 102, "252524": 99, "252601": 161, "252849": 84, "252858": 97, "253": 177, "253026": [83, 84], "253610": 16, "253675": 94, "253724": 99, "25374": 95, "253828": 79, "254": [95, 177], "25401679": 65, "254038": 88, "254043": 71, "2543": 95, "254324": 85, "254400": 161, "254858": 79, "254983": 71, "255": [95, 177], "255022": 72, "255151e": 99, "255527": 71, "255982": 74, "256": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 95, 115, 121, 177], "256082": 114, "256325": 71, "256352": 84, "256413": 71, "256416": 99, "25643": 95, "256567": 93, "25672": 95, "256944": 99, "257": 177, "257037": 95, "257109": 73, "257207": 65, "2573": 72, "25734822": 72, "257377": 70, "2575": [127, 130], "257551": 71, "257605": 74, "257762": 99, "257833": 71, "258": 177, "258087": 74, "258158": [83, 84], "25821083": 78, "2584": 95, "25855": 69, "25888685": 72, "259": 177, "259184": 79, "259230": 84, "259395": 89, "2594": [66, 94], "259828": [83, 84], "25991853": 72, "25x_3": 70, "26": [65, 66, 67, 69, 71, 74, 75, 78, 79, 83, 84, 92, 93, 94, 97, 107, 112, 114, 115, 121, 127, 143, 161, 175], "260": 177, "260084": 97, "26016": 95, "260161": 28, "260211": [83, 84], "260328": 83, "260356": 94, "260360": 99, "26070131": 72, "260762": 80, "261": [100, 177], "2610": 95, "261031": [127, 132], "261041": 74, "261140": 84, "261366": 83, "261624": [94, 95], "261635": 114, "261685": 95, "261686": 92, "261903": 93, "2619317": 65, "262": 177, "2620": 95, "262083": 92, "262204": 95, "262261": 83, "262364": 97, "262829": 143, "263": [10, 95, 177], "263190": 74, "2633": 95, "263466": 71, "264": [176, 177], "264056e": 74, "264086": 70, "264255": 16, "264426": 92, "264827": 74, "264958": 71, "265": 177, "265028": 74, "2651": 127, "265119": 98, "2652": [67, 94, 95], "265547": 95, "265562": 83, "2658": 88, "265888": 79, "265929": 100, "266": 177, "266147": 100, "2663": 79, "266495": 74, "266566": 74, "266633": 83, "266724": 95, "266909": 161, "267": 96, "267055e": 83, "2670691": 65, "267164": 92, "267214": 74, "26741094": 74, "267444": 78, "267500": 93, "267581": 95, "267950": 99, "268": 177, "268052": 97, "268406": 78, "268850e": 84, "26891880": 143, "268942": 99, "268977": 95, "268998": 66, "269043": 99, "269112": 127, "269425e": 83, "269979": 71, "26bd56a6": 67, "26e": 66, "27": [17, 18, 19, 35, 41, 63, 65, 66, 67, 68, 69, 71, 74, 75, 78, 83, 84, 92, 93, 94, 97, 107, 112, 114, 115, 121, 127, 143, 161, 175, 176], "270": 177, "2700": 67, "270000e": 95, "270197": 73, "270311": 16, "270644": [83, 84], "27066": 95, "270784e": 74, "270873": 74, "271": 177, "271004": [94, 95], "271183": 94, "2714838731": 93, "271504": 71, "271860": 71, "271867": 93, "272384": 84, "272505": 92, "272608": 97, "272643": 95, "273": 67, "273025": 71, "273208": 95, "27334903": 74, "273356": 85, "273433": 78, "27371": [66, 94], "27372": [66, 94], "2739": [70, 71, 74, 100, 103], "274": [67, 78, 95], "2740991": 64, "27422755": 74, "274251e": 94, "27429763": [127, 129], "27431071": [127, 130], "2747": 64, "27472": 95, "274793": 99, "274825": 29, "275268": 74, "275344": 97, "275454": 84, "275538": [108, 112], "275596": 161, "275831": 84, "276": [67, 177], "2760": 95, "276002": 79, "276148": 99, "276189e": 93, "2762": 95, "276311": 95, "2766091": 66, "276666": 97, "276774": 78, "2769": 72, "27695": 95, "277299": 75, "2773": 72, "277626": 114, "277808": 83, "277987": 92, "278": [87, 102, 177], "2780": 65, "278000": 93, "278035": 80, "278391": 95, "278434": 87, "278454": 90, "278543": 83, "2786": 161, "278840": 74, "278907": 99, "279": 177, "279140": 79, "27927": 95, "279304": 97, "27951256e": 143, "279524": 99, "279595": 80, "279693": 74, "27991": 95, "279926": 84, "28": [65, 66, 67, 71, 73, 74, 78, 83, 84, 86, 92, 93, 94, 97, 114, 115, 116, 119, 121, 127, 143, 161, 175, 177], "280122": 83, "280196": 88, "280282": 71, "280454dd": 67, "2805": 173, "280501": 161, "280594": 71, "280766": 84, "280963": 98, "281": [100, 177, 178], "281024": 99, "28111364": 66, "281360": 16, "2815": 95, "2819": 161, "281908": 83, "281980": [127, 132], "282": [100, 176, 177], "282200": 88, "282308": 78, "2825": [173, 175], "282521": 71, "282645": 97, "2830": [64, 173, 175], "283168e": 83, "28326": 95, "2836059": 65, "28367": 95, "283e": 100, "284": 177, "284072": 74, "284073": 99, "28425026": 102, "284271": 89, "284397": 178, "28448202": [127, 130], "28452": [66, 94], "284866": 74, "2849": 95, "284987": 95, "285": [100, 123, 127, 177], "285001": 80, "28505986": 74, "285160": 93, "285224": 71, "285275": 71, "285276": 99, "28537589": 74, "285483": 90, "285981": 74, "285e": 100, "286": 177, "2861": 79, "286203": 83, "2865": [64, 95], "286507": 85, "286563e": 95, "287": 177, "287041": 99, "287096": 74, "287123": 114, "287648": 71, "287813": 73, "287815": 102, "287926": 99, "288": [96, 177], "288105e": 83, "288311": 71, "288499": 71, "288788e": 84, "288833": 78, "289": [176, 177], "2890": 64, "289006": 79, "289062": [94, 95], "289081": 99, "289170": 93, "289198": 84, "289315": 97, "289440": [83, 84], "289536": 83, "289549": 79, "289555": 90, "289706": 83, "289983": 92, "29": [65, 66, 67, 71, 74, 78, 79, 83, 84, 92, 93, 94, 97, 102, 114, 115, 121, 127, 143, 161, 175], "290": 127, "29001936": [127, 132], "290112e": 84, "290479": 97, "290507": 84, "290518": 71, "290711": 71, "290901": 80, "290987": 94, "291": [100, 177], "2910": 95, "291011": [12, 127, 129], "291071": 99, "29107127": 99, "291342": 84, "291406": 99, "291500e": [94, 95], "291517": [83, 84], "291963": 99, "292": [98, 177], "2920": 95, "292028": 85, "292046": 161, "2921": 79, "292105": 99, "292179": 114, "2925": 67, "292652e": 84, "292747": 97, "292843": [127, 132], "292997": 99, "29299726": 99, "293067": 74, "29320382": 143, "293218": 99, "293352": 74, "293611": [127, 132], "293669": 95, "293896": 74, "294": 177, "294067": [83, 84], "294123": 114, "2945": 79, "295": [62, 176, 177], "295324": 71, "295590": 83, "295837": [75, 107, 112, 175], "2958370000000100000100": [67, 107, 112, 175], "2958370001000010011100": [67, 107, 112, 175], "2958371000000010010100": [67, 107, 112, 175], "295855642811191": 85, "295856": 85, "295868": 95, "296077": 78, "296099": 80, "296104": 74, "296171": 83, "296174": 71, "296523": 83, "29653721": 74, "296729": 93, "29675887": 99, "296759": 99, "29678199": [105, 144], "296887": 71, "297": 177, "297102": 83, "297204": 83, "297287": [83, 84], "297349": [87, 88], "297682": 99, "2977": 95, "29784405": 102, "298": [34, 67], "298069": 83, "298088": 84, "298120": 85, "298132": 83, "298150": 92, "298235e": 95, "298327": 80, "298404": 74, "298461": 72, "298629": 114, "298995": 114, "299": [67, 100], "299372": 127, "299468e": 97, "299537": 88, "299755": 92, "2999": 80, "29999": 71, "2_": [36, 68, 104, 162, 163, 172], "2_x": [36, 68, 104], "2d": [13, 144, 153], "2dx_5": [85, 99], "2e": [62, 64, 65, 66, 67, 68, 115, 123, 124, 125, 126, 127, 144, 161, 175], "2f": [79, 89], "2m": [162, 168, 172], "2n_t": 70, "2x": 99, "2x_0": [32, 83, 84, 87, 88], "2x_4": 70, "3": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 45, 46, 49, 50, 51, 55, 56, 57, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 132, 133, 143, 144, 161, 162, 168, 173, 174, 175, 176, 177], "30": [32, 39, 45, 62, 63, 65, 67, 68, 69, 71, 73, 74, 78, 79, 80, 81, 82, 83, 84, 85, 91, 92, 93, 94, 95, 97, 99, 100, 114, 115, 119, 121, 127, 143, 161, 175], "300": [63, 82, 85, 95, 99, 106, 176], "3000": 80, "30000": 71, "30000000000000004": [85, 95, 99], "300011": 71, "3002": 79, "300261": 72, "30031116e": 143, "300566": 78, "30060692": 74, "30093956": 102, "301": 67, "301290": 74, "301366": 161, "301371": 99, "3016": 94, "301629": 83, "30175": 95, "30186": 95, "302062": 83, "302163": 71, "30229388": 99, "302294": 99, "302494": 74, "302552": 83, "302648": 93, "303324": 93, "30365474": 74, "303697": 71, "30379054": 74, "303835": 93, "303f00f0bd62": 67, "304130": 99, "304159": 99, "304201": 70, "304256": 71, "3044": 79, "304917": 73, "30492191": 143, "305142": 92, "305272": 69, "30529": 95, "305341": 99, "305612": 93, "305687": 99, "305775": 99, "305811": 114, "305b": 67, "30623029": 74, "306282e": 84, "30628774": 99, "306288": 99, "306416": 127, "30645": 95, "30672815": 65, "306915": 93, "306963": 99, "307098": 84, "3071196": 74, "307407": 99, "307444": 92, "30749578": 74, "307540": 74, "307670": 72, "307821": 74, "307923": 16, "308": 95, "308239": 95, "308307": 74, "308415": 16, "3087": 79, "308837": 84, "30917769": [87, 88], "309464": 95, "309772": 93, "309952": 79, "31": [65, 66, 67, 68, 71, 73, 74, 78, 79, 83, 84, 92, 93, 94, 97, 114, 115, 121, 127, 143, 161, 175, 178], "310097": 83, "31041695": 74, "310899": 78, "311": 100, "311081": 74, "311375": 84, "311393": 74, "311594": 83, "311740": 92, "311807": 83, "311869": 114, "312030": 95, "312172": 87, "312187": 71, "3125": 95, "312652": 100, "312769e": 84, "312873": 83, "313044": 161, "313087": 71, "313209": 85, "313289": 71, "313535": 99, "31378": 67, "313796": 84, "313870": 90, "314": [78, 97, 143], "3141": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 65, 67, 68, 75, 93, 105, 107, 112, 114, 115, 120, 121, 123, 124, 125, 126, 127, 144, 161, 175], "3142": [39, 127], "314247": 103, "314309": 83, "314325": 74, "314410": 97, "3146": 30, "314625": 84, "31476": [94, 95], "314781": 87, "315": 177, "315031": 103, "315155": 84, "315290": [87, 88], "315468": 71, "316": [67, 177], "316193": 99, "316197": 79, "31633": 95, "316717": [83, 84], "316777": 97, "317394": 70, "317487": 99, "317607": 99, "318": [67, 177], "318052": 71, "318373": 71, "318571": 161, "318753": [87, 88], "3188": 143, "319": [67, 177], "319100": [87, 88], "319242316": 143, "319291": 78, "319420": 90, "319559": 79, "319634": 95, "319735": 97, "319750": 16, "319759": 99, "319771": [127, 132], "319850": 99, "319903": 69, "319965": 83, "32": [40, 65, 66, 67, 71, 73, 74, 78, 83, 84, 92, 93, 94, 95, 97, 104, 114, 115, 121, 127, 129, 143, 144, 161, 175], "320": [95, 96], "320000e": 95, "320131": 97, "320314": 94, "320633": 85, "321": 177, "321498": 71, "321673": 161, "322": 96, "322024": 97, "322041": [127, 130], "322404": 102, "322681": 74, "322736": 37, "322764": 84, "322991": 83, "323061": 71, "3235": 95, "323636": 94, "323679": 93, "324": [66, 177], "324423": 71, "32458367": 65, "3245837": 99, "324910": 79, "325043": 97, "325046": 95, "325056": 99, "3252698": 74, "325657": 83, "32575555": 71, "325819": 95, "325825": 78, "326": 100, "32613259": 74, "326146": 71, "326186": 74, "326740": 99, "32674263": 73, "326871": 103, "3268714482135149": 103, "327": [95, 177], "327030": 74, "327121": 16, "327578": 84, "32757929": 74, "32789": 95, "327958": 80, "327997": 83, "328141": 71, "328198": 74, "328367": 95, "328478": 78, "328709": 74, "32875335": 73, "328797": 97, "328884": 83, "32896496": 71, "32950022e": 143, "329671": 83, "329967": 84, "33": [65, 66, 67, 71, 74, 78, 83, 84, 93, 94, 95, 97, 114, 115, 121, 124, 127, 143, 161, 175, 176], "330": 177, "3300": [66, 94], "330000e": 95, "330285": [83, 84], "330301": [127, 128], "3304269": 65, "33047861": 74, "330615": 99, "33065771": 99, "330658": 99, "33066045": 74, "330727": 97, "330731": 29, "331": [123, 177], "331161": 84, "331215": 92, "331488": 74, "331521": 99, "331538": 78, "3316": 64, "331602": 95, "33168943": 95, "33175566": 99, "331756": 99, "331913": 87, "332": 177, "332028": [127, 130], "332039": 78, "332535": [127, 130], "332782": 29, "3329": 95, "332996": 93, "333000": 83, "333115": 74, "333250": 95, "3333": [63, 65, 82, 114, 115, 117, 127], "3333333": 67, "33333333": [71, 73, 74], "33335939e": 143, "3334": [79, 95], "333581": 94, "333653": 73, "333703": 74, "333882": 83, "333947": 84, "333973": 84, "334": 66, "3341": 79, "334750": 85, "335125": 78, "335446": 80, "335496": 78, "335609e": 99, "335846": 99, "335869": 114, "3359": 79, "336": [96, 177], "33610636": 143, "336461": 95, "336498": 99, "336612": 70, "336716": 83, "336870": 72, "337": 177, "3371": 95, "337970": [127, 128, 132], "337986": 84, "338": [102, 177], "33849": 95, "338502": 97, "3386": 143, "338775": 85, "338855": 95, "338890": 84, "3389": 79, "338908": 85, "339177": 79, "339268": 99, "339269": 102, "339352": 97, "339444": 97, "339553": 84, "339570": 99, "339762e": 95, "339875": [87, 88], "34": [63, 64, 65, 66, 67, 68, 71, 73, 74, 78, 83, 84, 93, 94, 97, 101, 102, 114, 115, 121, 127, 143, 161, 178], "340": [66, 95], "340142": 127, "340217": 72, "340235": 90, "340274": 102, "340664": 74, "340712": 95, "340844": [38, 97], "340996": 73, "341106": 83, "341336": 27, "3419421": 74, "342117": 90, "3422": 95, "342214e": 84, "342362": 80, "342675": 65, "342693": 78, "34287815": 102, "342992": 93, "343": [100, 127, 177], "343117": 78, "343639": 114, "34375": 94, "34421179": 74, "344212": 178, "344219": 97, "34436006": 143, "344368": 83, "344440": 114, "344463": 84, "34450402": 73, "344505": [94, 95], "344640": 99, "344748": 98, "344787": [83, 84], "344834": 70, "344845": 84, "345": 177, "34527949": 71, "3454": 95, "34541423": 74, "345581": 74, "345903": 99, "346107": 94, "346206": 99, "346238": 102, "346274": 83, "346678": 98, "346683": 85, "3466832975109777": 85, "3467": 64, "346795": 97, "347027": 71, "347213": 93, "347310": 29, "347501": 91, "347770e": 84, "347793": 84, "34784951": 72, "348": 100, "348338": 95, "348617": 99, "348622": 100, "348697": 85, "3486970271639334": 85, "349213": 73, "3492131": 64, "34922643": 72, "349461": 84, "349609": 74, "34967621": 65, "34968595": 72, "349772": 88, "34980811": 77, "349867": 84, "349900e": 95, "34990576": 143, "34m": [115, 123, 124], "34mglmnet": [115, 123], "34mmlr3": [115, 123, 124], "34mmlr3learner": [115, 123, 124], "34mmlr3pipelin": [115, 123], "34mranger": [115, 123, 124], "34mrpart": [115, 123], "35": [66, 67, 71, 74, 78, 83, 84, 85, 87, 93, 94, 95, 97, 99, 109, 112, 114, 115, 121, 127, 143, 161, 162, 168, 178], "3500000000000001": [85, 95, 99], "35015342": 72, "350165": [115, 119], "350323": 95, "350491": 83, "350518": 99, "350533": 99, "35053317": 99, "350712": [87, 88], "35077502": [162, 168], "351303": 97, "35155": 95, "352": [66, 93, 177], "352047": [127, 130], "352246": 99, "352250e": [94, 95], "3522697": 65, "352561": 74, "352679": 74, "35292": 95, "353": 96, "353748e": 99, "353813": [127, 130], "353950": 71, "354": [95, 96, 177], "354055": 115, "354107": 71, "354188": 70, "354371": 99, "354529": 84, "354796574": 143, "355": 177, "355065": 80, "355564": 84, "355627": 114, "35595672": 72, "35595889": 72, "35596931": 72, "3559724": 72, "356": 177, "356032": 74, "356136e": 95, "356167": 88, "35620768e": 143, "356492": 71, "356501": 71, "35652709": 74, "3566": 95, "3568": 127, "356821": 114, "357": [95, 177], "35731523": [127, 129], "357450": 83, "357586986897548": 69, "357654": 95, "357881": 97, "358": 177, "358158": [94, 178], "358289": 93, "358329": 71, "358395": 102, "358690e": 95, "35871235": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40], "358787": 161, "359": [100, 177, 178], "359050": 84, "359229": 80, "3593": [77, 102], "359307": 84, "359622": 79, "3598": 95, "35th": 176, "36": [66, 67, 71, 73, 74, 78, 79, 83, 84, 93, 94, 97, 114, 115, 121, 127, 143, 161], "360": 177, "360004": 99, "360054": 161, "360071": 74, "360249": 89, "360403": 78, "360475": [83, 84], "360683": 85, "360801": 85, "360881": 71, "360965": [108, 112], "361": [100, 177], "361220": 114, "361579": 71, "361623": 90, "361810": 71, "3619201": 33, "36213": 95, "362251": 71, "36231307e": 143, "362398": 25, "36274369": [127, 132], "362753": 74, "362760": 95, "362770": 71, "363": 177, "363103": 85, "3631031251500065": 85, "363215": 83, "363276": 65, "363549": 73, "363576": 92, "363771": 83, "363819": 71, "363990": 83, "364099": 71, "364276": 92, "3643": 161, "364595": 65, "3647": 67, "364800": 99, "365": 177, "365150": [127, 132], "365367": 96, "365509e": 83, "365527": 95, "36557195e": 143, "36566025e": 143, "365702": 83, "366188": [108, 112], "366201": 74, "366514": 97, "366718627": 65, "366912": 83, "367": [100, 177], "367017": 92, "367225": 83, "367323": 99, "367366": 90, "367571": 85, "367625": 99, "367697": 84, "367957": 71, "368": [71, 73, 74, 95, 177], "368152": 93, "368191": [127, 128], "3682": [66, 94, 95], "368324": 93, "368577": 114, "369": 177, "369050": 96, "369190": 74, "369212e": 84, "369556": 85, "3696": 102, "369796": 99, "369869": 94, "369981": 93, "36m": [115, 123, 124], "37": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 66, 73, 77, 79, 83, 84, 90, 93, 94, 97, 114, 115, 125, 126, 127, 143, 161], "370": 177, "370000e": 95, "370036": 97, "370165": 95, "370184": 74, "370254e": 94, "3702770": 65, "370331": 74, "370423": 74, "370736": 93, "3707775": 65, "370889": 78, "370910": 74, "371": [96, 177], "3711": 79, "3711317415516624": 85, "371132": 85, "3712": 95, "371357": [94, 95], "371429": 85, "371659": 74, "371972": 26, "372": [176, 177], "37200": [94, 95], "372097": 85, "372150e": 74, "3722": 95, "3724": 95, "372555": 84, "3727679": 65, "37280292": 143, "373034e": 38, "373235": 95, "373451": 114, "37362103": 74, "373675": 71, "373783": 83, "3738573": 65, "373941e": 83, "374": 177, "374335": 99, "37433503": 99, "3745": 95, "374812e": 95, "374947": 74, "375081": 95, "375106": 78, "375151": 83, "375193": 71, "375403": 69, "375465": 99, "375776": 83, "375897": 78, "375995": 92, "3763": 64, "376472": 74, "3766": 90, "376617": 90, "376681": 84, "376925": 74, "376972": 84, "377": 177, "377147": 114, "377246": 95, "377311": 99, "377572": 84, "377955": 99, "378": 143, "378161": 72, "378286e": 71, "378367": 99, "378471": 83, "378596": 93, "378834": 99, "3788859": 65, "379": 176, "379614": 99, "379892": 83, "38": [67, 78, 79, 83, 84, 94, 97, 114, 127, 143, 161], "3800694": 65, "380442": 83, "380538": 79, "380695": 79, "380837": [94, 95], "381": 100, "381072": 99, "381247": 95, "381260": 71, "381278": 95, "381365": 71, "381684e": 94, "381685e": 95, "381689": 99, "381857": 74, "382024": [108, 112], "382030": 74, "382111": 74, "382285": 95, "382730": [127, 130], "382872": 85, "382919": 16, "383297": 99, "38348": 95, "383791": 71, "383921": [127, 130], "384": 95, "38402797": [127, 130], "384166": 97, "384189": 92, "384208": 92, "384232": 79, "384501": 97, "384677": 80, "384717": 83, "384755": 84, "3848": 95, "384883": 95, "385226": 161, "385290": 74, "385917": 93, "386": [67, 95], "386102": 85, "38642035": 73, "386428e": 84, "386502": 95, "386616": 84, "386831": 80, "387": 67, "387268": 78, "387304": 74, "387426": 99, "387673": 83, "387780": 99, "387879": 74, "388005": 95, "388071": 99, "388185": 80, "38818693": 143, "388216e": [115, 121], "38841329": 143, "38898864": 99, "388989": 99, "389": 67, "389184": 74, "389186": 127, "38937465": 143, "389440": 74, "389603": 83, "38973512e": 143, "38977615": 143, "38983785": 69, "38990574": 143, "39": [62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 77, 78, 79, 80, 83, 84, 89, 91, 93, 94, 95, 96, 97, 102, 103, 104, 114, 115, 120, 127, 143, 161], "39007": 30, "39010121e": 143, "390155": 92, "390312": 84, "390379": 99, "390495": 84, "390897068": 143, "391120": 71, "3912767": 74, "391314": 74, "391377": 103, "391847": 74, "39186467": 77, "392031": 74, "392242": 89, "392307": 71, "392472": 95, "392623": 84, "392801": 83, "392833": 102, "392864e": [94, 95], "393441": 84, "39356231": 74, "393604": 85, "393654": 80, "3937": 79, "393971": 71, "39425708": 65, "39454997": [127, 130], "394554": [127, 128, 132], "39456988": 73, "394757": 92, "395076e": 95, "395195": 93, "395268": 114, "395428": 71, "395558": 83, "39573129": 73, "3958": 127, "395868": 71, "396": 127, "3961": 95, "39611477": 66, "39621961e": 143, "396272": 92, "396531": 95, "396671": 83, "396835e": 84, "396985": 93, "396992": [83, 84], "397140": 85, "39727": 95, "397313": 64, "397578": 89, "3977": 79, "397811": 102, "3979": 73, "398": [107, 112, 175], "398000e": 95, "398055": 74, "398166": 80, "398367": 79, "3985": 95, "398770": 99, "398999": 24, "399": 66, "399056": 99, "399223": 70, "399238": 71, "399355": 70, "39966157": 74, "399679": 127, "399692": 99, "399858": 103, "39m": [115, 123, 124], "3cd0": 67, "3dx_1": [85, 99], "3e1c": 67, "3ec2": 67, "3f5d93": 96, "3x_": 99, "3x_4": [85, 99], "4": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 36, 37, 38, 39, 40, 41, 42, 49, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 119, 120, 121, 126, 127, 128, 129, 130, 132, 143, 144, 161, 162, 168, 175, 177], "40": [65, 68, 69, 78, 83, 84, 85, 94, 95, 97, 99, 104, 107, 112, 114, 115, 121, 127, 143, 161, 162, 168, 178], "400": 93, "4000": [71, 74, 97], "4000000000000001": [115, 121], "40000000000000013": [85, 95, 99], "400113": 90, "40029364": [162, 168], "400297": 84, "4005743": 127, "400587": [92, 97], "4006": 64, "400698": 97, "400823": 99, "400905": 80, "401": [10, 178], "401009": 92, "401035": 78, "401037": 78, "401247": [40, 144, 161], "40127723e": 143, "40172708": 74, "401829": 74, "401931": [87, 88], "402": [107, 112], "4020": 79, "402077": 95, "402100": 161, "402301e": [115, 119], "402381": 74, "402519": 97, "402924": 78, "403": 100, "403113": 85, "403113188429014": 85, "403425": 99, "40359107": 69, "403626490670169": 105, "4036264906701690": 105, "403626491": 105, "403771948": 144, "403823": 74, "404": 123, "404300": 80, "404318": 64, "40436742": 73, "404489": 71, "40452": 95, "404550": 98, "404734": 74, "4047353": 74, "404824": 95, "404834": 127, "405000": 95, "405203": 70, "405313": 83, "40538347": 73, "40583": 64, "405890": 29, "405892": 83, "40608": [127, 128], "406085": 114, "406285": 99, "406446": 85, "4065173": 143, "406565": 83, "406756": 84, "40676": 64, "407": 100, "407303": 74, "40732": 90, "407596": 97, "407758": 16, "407786e": 83, "408000": 84, "408135": 83, "408476": [162, 168], "40847623": [162, 168], "408539": 99, "408565": 99, "4086243": 73, "408682": 83, "40913821": 73, "409154": [64, 71], "4093": 102, "409390": 95, "409395": 99, "409532": 83, "409538": 74, "409551": 84, "409746": 85, "409848": [83, 84], "41": [78, 83, 84, 94, 95, 97, 114, 115, 126, 127, 143, 161], "410095e": 84, "4101895": 78, "41025222": 74, "4103": 79, "410387": 97, "410393": 85, "41058596": 73, "410681": 70, "410795": 93, "410901": 74, "41093655": 143, "410981": 74, "411190": [83, 84], "411295": 99, "411304": [83, 84], "411582": 99, "412": 95, "412127": 99, "412304": 103, "412406": 95, "412477": 70, "412620": 78, "412653": 93, "412714": 85, "412864": 87, "412919": 84, "412947": 83, "412979": 97, "413": 95, "41336": [115, 116, 119], "413376": 127, "41341040": 65, "413606": 73, "413784": 95, "414": 100, "41404477": 73, "414225": 71, "415040": 84, "41525168e": 143, "415294": 99, "415556": 114, "41566": 127, "415812": 178, "416052": 80, "4166": 95, "4166667": 67, "416709": 84, "416737": 92, "416899": 83, "416e": 100, "41749373": 73, "417519": 71, "417619": 84, "417669": 95, "417727": 94, "417767": [87, 88], "417834": 80, "417859": 83, "41798768e": 143, "417988": 99, "418052": 83, "418056": 99, "41805621": 99, "41830279": 74, "418400": 90, "418591": 95, "418713": 74, "418741": 80, "418806e": 85, "418922": 71, "418969": 114, "41914743": 74, "41918406e": 143, "4192": 64, "419371": 99, "419589": 74, "419871": 80, "41989983e": 143, "4199952": 65, "41e5": 67, "42": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 40, 68, 69, 70, 71, 73, 77, 78, 79, 80, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 95, 97, 98, 99, 102, 103, 104, 109, 112, 114, 115, 121, 127, 128, 129, 130, 132, 143, 161, 176, 178], "42005186": 73, "420308e": 95, "420401": 71, "42073312": 65, "420967": 85, "421045": 97, "421083": 64, "42112047": 74, "4211349413": 65, "421200": 103, "421357": [87, 88], "42143": 95, "421464": 73, "421576e": 95, "421793": 102, "4218": 79, "421919": 95, "422004969": 143, "422007": 102, "422116": 95, "42211633": 74, "422119": 95, "422244": 78, "422251": 84, "422325": 85, "422525": 74, "422646e": 84, "422933": 78, "422984": 16, "423": 62, "4232": 79, "4234": 79, "4235": 79, "423521": 71, "4235839": [87, 88], "42365741": [127, 132], "423951": 64, "424108": 85, "424127": 143, "42412729": 65, "4242": 91, "424328": 99, "424573": 92, "424651": 127, "424682": 83, "4247": 79, "424717": 85, "424748": 103, "424879": 95, "425": 93, "425000e": 95, "42510109": 73, "425103": 64, "425325": 90, "425414": [108, 112], "425493": 64, "42550": 95, "425617": 97, "425636": 90, "425923": 97, "426055": 64, "426133": [127, 130], "42632728": 73, "426466": 74, "42651596": 73, "426540": 93, "426540301": 65, "426734": 92, "426736": 95, "427": 95, "42709109": 73, "427147": 74, "42717352": 74, "427486": [83, 84], "42755087": 102, "427551": 102, "427573": 93, "4276": 79, "427654": 90, "427725": 99, "42778788": 73, "427907": 74, "428": [161, 178], "428046": 98, "42811700": 178, "428191": 83, "428255": 99, "428411": [94, 95], "428467": 99, "4284675": 99, "4286": 79, "428612": [127, 132], "428766": 83, "428771": 29, "428779": 84, "429133": 92, "429141e": 83, "429298": 92, "429309": 95, "42934105": 104, "42ba": 67, "43": [62, 66, 73, 78, 80, 83, 84, 94, 95, 97, 114, 127, 143, 161, 178], "4302": [64, 79], "430298e": [94, 95], "43032016": 143, "430345": 92, "430558": 71, "430700": 97, "430872": 78, "430878": 78, "4311947070055128": [115, 121], "431250": 74, "431306": 99, "431437": 102, "431538": 97, "431575": 96, "431605": 83, "431639": [127, 132], "431701914": 173, "431747": 84, "431929": 95, "431998": 80, "432130e": 94, "432300e": 99, "43231359e": 143, "432707": 100, "43294": 67, "432f": 67, "433": [67, 100], "4330": 95, "433221": 85, "433473": 95, "433509": 79, "433614": [127, 132], "43361991": 99, "433620": 99, "433684e": 83, "434054e": 90, "434116": 83, "434393": [127, 132], "4344": 79, "434535": 99, "43453524": 99, "434698e": 83, "4348": 79, "43482": 95, "435": 67, "43503345": 127, "435192": [127, 130], "435306": 78, "435401": 93, "43545644": 143, "4357": 95, "435927": 95, "435967": 93, "436": [67, 95], "43607231": 73, "436327": 95, "436363": 74, "436764": 100, "436806": 95, "436923": 71, "437134": 95, "437242": 71, "437601": 84, "437667": 94, "437924": 95, "438": 93, "438219": 99, "438326": 74, "438569": 95, "43883": 88, "438899": 97, "438960": 93, "4391": 79, "439190": 74, "439257": [127, 132], "439343": 97, "439541": [94, 95], "439675": 100, "439699": 80, "43989": 24, "43f0": 67, "44": [62, 73, 78, 80, 83, 84, 94, 97, 114, 127, 129, 143, 161, 178], "4401": 79, "440170": 83, "440320": 95, "440524": 83, "440605": [115, 119], "440790": 79, "440867": 78, "440a": 67, "441004": 95, "441064": 71, "441153": 99, "441209": 99, "44124313": 127, "441311": 87, "441528": 83, "441556": 97, "441567": [127, 130], "4416552": 65, "4417": 79, "441882": 73, "4420": 79, "442976": 78, "443016": 85, "443032": 94, "44312177": 66, "44349955": 78, "443686": 99, "4438": 95, "443e": 100, "444": 123, "44404522": [127, 130], "444046": 95, "44412537": 127, "4444": [63, 65, 82, 127], "444500": [94, 95], "444675": 97, "444805": 92, "445": [62, 100], "4450": 79, "445163": 71, "445238": [127, 132], "4455": 79, "44563945e": 143, "445816": 83, "4460": 79, "446023": 98, "4462": 67, "44647451": 102, "446981": [127, 130], "44713577e": 143, "447375": 78, "447525": 73, "447624": [83, 84], "447641": 97, "447706": 85, "447803": [127, 130], "447863": 85, "447863428811719": 85, "447897": 83, "448": 95, "448070": 84, "4481": 79, "448587": 85, "4487": 95, "448745": 99, "44890536": 143, "448923": 89, "449150": 29, "449172": 78, "449345": 84, "449566": 71, "449677": 90, "44969661646483505": 115, "449715": 78, "4498": 72, "44fa97767be8": 67, "45": [62, 69, 83, 84, 85, 89, 92, 95, 97, 99, 114, 115, 123, 127, 143, 161, 178], "4500": 94, "45000000000000007": [85, 95, 99, 115, 121], "450000e": 95, "450097": 84, "450152": 93, "4506": 79, "450870601": 65, "451137": [127, 128, 132], "451158": 78, "4512273953561296": 120, "4513": 79, "451671": [127, 132], "452": 67, "452091": 95, "452114": 114, "452488701": 65, "452489": 93, "45256": 97, "452622": 83, "452854": 71, "453": 67, "453189": 78, "4535": 95, "4536": 95, "4539": 67, "454129": 16, "454406": 80, "45451": 95, "454513": [127, 132], "45467447": 143, "454991": 79, "455": 67, "455078": 85, "455107": 85, "4552": 67, "455262": 84, "455293": 85, "4552b8af": 67, "455349": 74, "455371": 74, "455448": 102, "455564": 16, "455672": 95, "455981": 162, "45613995": [127, 130], "456225": 99, "456370": 93, "456453": 16, "4566031": 161, "45660310": 161, "456617": 99, "4567": 102, "4568": 79, "456892": 85, "456939": 71, "457088": 99, "458042": 74, "45804658": 73, "458091": 74, "458114": 95, "458138": 74, "458307": 163, "4584": 95, "458420": 95, "4584447": 65, "458814": 90, "458855": 66, "459098": 83, "4592": 65, "459200": 93, "459383": 85, "45957837": 143, "459617": 16, "459760": 95, "459812": 85, "459816": 97, "46": [73, 79, 83, 84, 89, 97, 114, 127, 143, 161, 178], "4601": 95, "460207": [83, 84], "460218": 85, "460289": 99, "460385": 83, "460744": 94, "4610": 178, "4611": 79, "46111213202535365152546976788081839499": 143, "461162e": 84, "461396": 92, "461412": 114, "461469": 72, "461493": 92, "461629": 103, "461759": 97, "462022": 73, "46218924": 74, "462319": 95, "462451": 85, "462473": 84, "462884": 97, "463046": 84, "463208": 95, "463325": 99, "463381": [39, 127], "4634": 95, "463418": 103, "463494": 74, "4637": 79, "463766": 88, "463803": 84, "463992": 71, "463b": 67, "464076": 85, "464106": 74, "464282": 93, "464424": 97, "46448227": 143, "464537e": 83, "464668": 27, "464705": 74, "4649": 79, "465": [62, 80, 96, 127], "46507214": 102, "4652": 79, "465508": 74, "465649": 103, "465730": 103, "4659651": 105, "465965114589023": 105, "4659651145890230": 105, "465981": 99, "46598119": 99, "466047": 99, "46618738": 143, "466233": 74, "466269": [127, 132], "4663": 79, "466440": 85, "466452": 78, "466684": 95, "466756": 99, "46709481": 143, "46722576e": 143, "467464": 74, "46746487": 74, "46755529": 143, "467613": 93, "467613401": 65, "467681": [83, 84], "467752": 73, "467770": 85, "468001": 95, "468016": 78, "468035e": 83, "468072": 84, "468075": 99, "46807543": 99, "468215e": 97, "468406": 95, "468519": 74, "468869": 84, "468907": 80, "468d": 67, "469": 67, "469037": 73, "469170": 95, "469201": 74, "46925504": 74, "469379": 95, "469474": 103, "469676": 80, "469825": 85, "47": [66, 79, 80, 83, 84, 87, 94, 97, 102, 114, 127, 143, 161, 177], "470023": 84, "470365": 95, "470737": 71, "470801": 83, "470828": 83, "470907": 71, "471": 80, "471025": 71, "471435": 100, "471454": [108, 112], "471666": [108, 112], "471829": 74, "472255": 95, "472863": 97, "472891": 99, "472952": 84, "472e": 67, "473036": 114, "473066": 71, "473099": 85, "47319": 127, "473466": 73, "473907": 97, "47419634": 175, "474214": [87, 88], "474699": 84, "474709": 127, "475395": 95, "475551": 83, "475942": 97, "475e": 100, "476074": 97, "4766": 79, "476856": 85, "47699623": [127, 132], "4770": 79, "477130": [83, 84], "477150": 99, "477151e": 71, "4772": 79, "477354": 95, "477357": 100, "477395": 95, "477474": 93, "47759584": 143, "47782": 95, "477834": [127, 132], "47805578": [127, 132], "4782": 79, "478399": 95, "478447": 97, "47857478": 143, "478668": 84, "478794e": 84, "478827": 71, "478896": 95, "479119": 84, "479288": 16, "479527": 84, "47966100e": 143, "479722": 84, "479831": 71, "479876": [87, 88], "479882": 84, "479928": 99, "47be": 67, "48": [67, 80, 83, 84, 88, 94, 95, 96, 97, 114, 127, 143, 161], "480": [80, 123], "480000e": 95, "480133e": 99, "480199": 90, "48029755": 102, "480579": 84, "48069071": [105, 144, 161], "480691": [40, 144, 161], "480717": 97, "480800e": 99, "481034": 97, "481172": 99, "481344": [127, 128, 132], "481399": [94, 95], "4814": 77, "481713": 90, "481761e": 95, "481962": 97, "482": [67, 80], "482038": 85, "482045": 97, "482339": 73, "482349": 87, "482461": [162, 168], "48246134": [162, 168], "482483": 99, "482508": 92, "482790": 70, "48296": 102, "483": [100, 127], "48315": 102, "483165": 83, "483186": 70, "483192": [94, 95], "48331": 102, "483357": 84, "483531": 99, "48353114": 99, "483642": 95, "483711": 99, "483717": 85, "483855": 95, "484": 95, "48404": 65, "484074": 84, "484111e": 83, "484123": 71, "4842": 95, "484272e": 84, "484545": [127, 130], "484583": 71, "484640": 99, "484741": 97, "4849": 67, "485": [67, 78, 95], "485000": 78, "485368": 84, "485468": [108, 112], "48550": 103, "485617": [94, 95], "485712": 74, "485759": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "48583": [94, 95], "485871": 88, "485957": 74, "486": [43, 62, 95], "486054": 71, "486173": 74, "486202": 85, "486302": 74, "486342": 95, "486476": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "486532": 99, "486547": 74, "486692": 78, "48683": 97, "486830": 97, "486996": 71, "487": [80, 95], "487352": 72, "487467": 95, "487524": 90, "487641e": 99, "487856": 97, "487872": 81, "487934": 16, "488455": 92, "488551": 78, "488584": 73, "488759": 83, "488811": 99, "488866": 83, "488909": [94, 95], "488982e": 85, "489054": 98, "489292": 78, "489511": 78, "4895498": 99, "489550": 99, "489567": 101, "489612": 73, "489642": 97, "489699": 85, "49": [67, 78, 80, 83, 84, 96, 97, 114, 127, 143, 161], "490": 123, "490070931": 65, "490173": 97, "4904": 78, "490488e": [94, 95], "490689": 92, "490882": [127, 128], "490888": 78, "490896": 80, "490898": 95, "490941": 95, "49098": 102, "491245": 93, "491249": 92, "491479": [127, 132], "4915707": [127, 129], "491837": 97, "492": 95, "492074": [127, 132], "492075": 83, "4923156": 105, "49231564722955": 105, "492315647229550": 105, "492410": 99, "492417e": 127, "492626": 89, "49270769e": 143, "493": [100, 127, 176], "4931": 95, "493144": 103, "493195": 90, "493219": 99, "493426": 100, "493792": 92, "494": 100, "494253e": 83, "494324": 93, "494324401": 65, "495": 98, "495028": 84, "49530782": 65, "495323": 97, "495396": [127, 130], "495405": 99, "495657": 85, "495665e": 71, "495752": 99, "495852": 96, "49596416e": 143, "496": 98, "496074": 83, "496300e": 95, "496328": 74, "4965": 79, "49650883": 102, "496551": 99, "496714": 103, "496777": 178, "49693": [115, 124], "497": 98, "497687": 71, "497915": 83, "497964": 114, "498": [95, 98], "498048": 79, "498049": 96, "498122": 90, "498575": 78, "498921": 99, "498979": 95, "498f": 67, "499": [78, 98, 107, 109, 112, 175], "499000e": [94, 95], "49934441": 78, "499416": 84, "4996": 79, "49d4": 67, "4a53": 67, "4b8f": 67, "4dba": 67, "4dd2": 67, "4e": [65, 66], "4ecd": 67, "4f": 78, "4fee": 67, "4x": 99, "4x_0": [32, 83, 84, 87, 88], "4x_1": [32, 83, 84], "5": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 57, 58, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 132, 143, 144, 152, 161, 162, 168, 174, 175, 177], "50": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 65, 67, 68, 70, 74, 77, 78, 80, 85, 88, 92, 94, 95, 96, 97, 99, 114, 127, 143, 161], "500": [7, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 37, 38, 39, 40, 41, 42, 46, 53, 63, 67, 71, 72, 73, 74, 75, 77, 78, 79, 82, 83, 84, 87, 88, 94, 97, 98, 100, 102, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 127, 128, 129, 130, 132, 144, 161, 162, 168, 175, 178], "5000": [49, 71, 72, 73, 74, 83, 84, 85, 99, 101], "50000": 93, "500000": [94, 95], "5000000000000001": [85, 95, 99], "500000e": 95, "500084": 99, "500267": 89, "5003517412": 65, "500371": 97, "500635": 95, "50093148e": 143, "501021": 95, "5011": 79, "501203": 92, "501403": 99, "501579": 97, "501951": 96, "501988": [127, 132], "502005": 114, "502018e": 71, "502084": 127, "502268": 95, "50243496": 143, "502494": 85, "50256": 97, "502560": 97, "5025850": 65, "502612": 99, "502995": 99, "503089": 99, "50313595": 143, "503226": 84, "50327379": 143, "503504": [115, 116, 119], "503562": 71, "503700": 80, "503768": 84, "50398782e": 143, "504076": 84, "504148": 96, "504286": 93, "5042861": 65, "504452": 97, "5050973": 65, "505528": 97, "505712": 78, "505807": 97, "505855": 71, "505972": 97, "506080": 79, "506464": 74, "506657": 74, "50672034": 65, "506736": 114, "506833": 83, "506900e": 99, "506903": 85, "507": 100, "507285": 90, "507683": 84, "50768b": 96, "508068": 83, "508153": 98, "50828321": 78, "508369": 74, "508406": 98, "508433": 83, "508459": 93, "508756": [108, 112], "508880": 71, "5089": 95, "508947": [15, 127, 129], "508988": 83, "509005": 83, "509059": 95, "509102": [95, 97], "509196": 99, "509451": 74, "509461": 99, "509651": 92, "50967": 103, "5097": 103, "5098": [75, 107, 112, 175], "509853": 99, "509894": 78, "5099": [67, 75, 107, 112, 175], "509951": 85, "509958": 93, "51": [64, 66, 67, 73, 80, 87, 97, 114, 127, 143, 161, 177], "510000e": [94, 95], "510151723243234414245495053555965687998": 143, "510257e": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "510385": 93, "51043241": 143, "510555": 80, "510586": 99, "51079110": 65, "510971": 92, "511500": 97, "5115547": 105, "5115547181877": 105, "51155471818770": 105, "511626": [39, 127], "511668": 103, "5116683753999616": 103, "511722": 38, "511822": 97, "512": [62, 93], "512108": 99, "512149": 99, "51214922": 99, "51233332": [127, 130], "51243406e": 143, "512519": 93, "512566": 84, "512572": 99, "512657e": 95, "512672": [127, 162, 168], "512885": 78, "512911": 120, "5131": 94, "513222": 90, "513658": 95, "513992": 99, "514": 67, "514024": 83, "514086": 97, "514179": 83, "514199": 79, "514692": 71, "514957": 84, "515208": 84, "515326": 72, "515358": 85, "5154": 95, "5154782580830215": 93, "5155": 67, "516": 67, "516125": 85, "516145": 95, "516214": 74, "516222": 99, "516255": 99, "516256": 99, "516443": 97, "516528": 99, "517": [67, 93], "517401": 72, "5175": 95, "517510812139451": 69, "517798": 80, "518175": 93, "518289": 97, "518478": 80, "518517": 30, "518610": 127, "518616": 84, "518846": 93, "518854": 80, "518978": 92, "519197": 74, "519637": 83, "51966955": 65, "519710": 99, "519774": 74, "519828": 84, "519898": 83, "52": [64, 67, 89, 92, 97, 100, 114, 127, 143, 161], "520": 95, "520012": 83, "520096": 92, "520406": 83, "520641": 102, "520918": [127, 132], "520930": 85, "521002": 85, "5210215656500876": 78, "521044": 78, "521104": 95, "521118": 78, "521233": 80, "521357": 84, "521584": 73, "521608e": 83, "521644": 78, "521990": 78, "521994": 78, "52220861": 74, "522398": 92, "522605": 95, "522835": 70, "523024": 97, "523030": 103, "523140": 84, "523163": 85, "523400": 71, "52343523e": 143, "523453": 78, "523794e": 99, "5238": 79, "523837": 71, "523977545": 65, "524215": 95, "52424539": 65, "52465": [115, 123], "524657": 99, "524817": 92, "524934": [83, 84], "525": 97, "525041": 96, "525064": 80, "52510803": 66, "5251546891842586": 103, "525316": 78, "525324": 91, "52544332": 127, "5255": 67, "52590": [66, 94], "526": [93, 123], "526125": 83, "526532": 95, "526582": 90, "526769": [83, 84], "527141": 83, "52714188": [127, 130], "52732": [115, 124], "527644e": 99, "528000e": 102, "528075": 78, "528145": 84, "5282": 97, "528200": 97, "52821": 97, "528255": [127, 130], "528272e": 84, "528381e": 104, "528553": 78, "528725": 99, "528937": [87, 88], "528991": 78, "528996901": 65, "528997": 93, "529": 93, "529178": 83, "529405": 64, "529468": 114, "529660": 74, "529692": 84, "529782": 64, "53": [64, 67, 78, 80, 89, 93, 97, 107, 112, 114, 127, 129, 143, 161, 173, 176], "5301": 95, "5302": 64, "530659": 89, "530830": 95, "530859036": 173, "530940": 99, "53094017": 99, "531": 67, "531030": 84, "531153": 74, "531223": 85, "53148135": [127, 130], "531515": 114, "531594": 95, "531999": 95, "532202e": 30, "532266": 85, "532421": 87, "53257": [115, 119], "532738": 99, "53273833": 99, "5328": 95, "53291002": [127, 130], "533": 100, "533283": 127, "533316": 95, "533489": 70, "5337": 79, "533871": 95, "533900": 99, "534139": 80, "5346": 67, "534759": 83, "535179": 99, "535202": 71, "535318": 99, "53606675": 99, "536067": 99, "536510e": 97, "536658": [127, 130], "536792": 95, "536798e": [94, 95], "537681": 95, "53791422": [104, 127], "538": [67, 123], "5382": 102, "538227": 84, "538264e": 74, "538282": 95, "53849791": 99, "538498": 99, "538513": [108, 112], "538937": [94, 95], "539455": 99, "539491": [87, 88], "539641": 97, "539767": 85, "53e": 143, "54": [64, 66, 67, 78, 79, 92, 97, 100, 106, 114, 143, 161, 177], "540101": 99, "54010127": 99, "540240": 95, "540299": 78, "540375": 90, "540401": 71, "540470e": 74, "5405": 90, "540549": 90, "540578": 92, "540833": 74, "541": 100, "541010": 99, "541060": 90, "541159": 99, "54119805": 69, "541447": 97, "54163": 102, "5416844": 105, "541684435562712": 105, "541863": 78, "542": [100, 123], "542446": 88, "542451": 99, "542560": 103, "542576e": 71, "542647": 99, "542648": 114, "542671": 93, "542754": 84, "542766": 71, "542769": 99, "542856": 74, "542883": [162, 168], "5428834": [162, 168], "542919": 114, "542989": 99, "542993": 74, "543": [93, 95], "543014": 95, "543052": 80, "543075": 85, "543136": 85, "543287": 78, "543380": 93, "5434231": 105, "543423145188043": 105, "543571": 97, "543572": 71, "5436005": 65, "5436876323961168": 85, "543688": 85, "543734": 95, "543764": 88, "54378": 102, "543789e": 74, "543934": 71, "544058": 78, "544097": 99, "544276": 84, "544383": 103, "54443876": 143, "544555": 93, "544657": 78, "544680": 84, "54483": [144, 161], "5448331": [144, 161], "545118": 74, "54517706e": 143, "545492": 80, "54550506": 100, "545605e": 99, "545672": 84, "545698": 97, "545930": 114, "545972": 97, "546223": 83, "546260": 92, "546438": 95, "546967": 83, "54716": 102, "547257": 84, "547306": 85, "5473064979425033": 85, "5475": 95, "54757459": 143, "547595": 74, "547794": 16, "547846": 97, "5479": 95, "547909": 95, "548138": 73, "549127": 74, "549192": 78, "5494": 95, "55": [66, 67, 78, 85, 92, 94, 95, 97, 99, 114, 143, 161], "5500000000000002": [85, 95, 99], "550060": 74, "550176": 92, "550196": 78, "55021688": [127, 132], "550472": 84, "550562": 74, "551010e": 95, "551051": 97, "551071": 73, "551317": 84, "5515587": 143, "551686": 85, "55176": [115, 123], "551852": 97, "552081": 92, "552727": 93, "552776": 99, "553": 96, "553004": 80, "55307": [115, 126], "553097": [127, 130], "553154": [127, 132], "55346908": [127, 132], "553616": 97, "553672": 95, "553754": 114, "553878": 28, "55390846": [127, 130], "554076": 85, "554793e": 143, "5548": 79, "554986": 85, "554986206618521": 85, "555": 93, "555445": 98, "555498": 99, "5555": [63, 82], "555544": 83, "555679": 97, "555705": 73, "555954": 95, "556191": [83, 84], "556333": [127, 130], "556533e": 95, "55662098": [127, 130], "556792": 99, "5572": 79, "557388e": 74, "5574dcd4": 67, "557595": 93, "557741": 92, "557999": 93, "558134": [83, 84], "55828259": 104, "558387": 84, "5584": 93, "5585": 93, "55855122": 69, "55863386": 127, "558655": 85, "5589": 93, "558996": 95, "559": [42, 176, 178], "5590": 93, "559055": 97, "559144": 85, "559186": 85, "5592": 93, "559394": 99, "559522": 99, "559680": 95, "559712": [108, 112], "559844": 84, "559871": [39, 127], "55dc37e31fb1": 67, "55e": 66, "56": [67, 79, 97, 106, 114, 143, 161, 173, 176], "560135": [40, 144, 161], "560167": 97, "56018481": 99, "560185": 99, "560250": 84, "560263": 72, "560269": 71, "560616": 78, "560689": 64, "560723": 89, "5607573": [127, 130], "561309": 95, "5616": 94, "561652": 84, "561785": [15, 127, 129], "562007": 72, "562013": 99, "56223": 102, "562288": 103, "562291": 97, "562317e": 83, "562320e": 83, "562390": 114, "562556": 73, "5625561": 64, "562712": [83, 84], "562866": 95, "563067": 100, "563196": 97, "563374e": 85, "563456": 114, "563503": 99, "563563": 90, "563673": 95, "563690": 95, "563760": 95, "5638": 95, "56387280e": 143, "56390147e": 143, "564": 127, "564045": 99, "564073": 95, "564232": [83, 84], "56436922": 74, "564451": 84, "564465": 74, "564577": 95, "564610": 83, "564800e": 95, "564806": 71, "564829": 85, "565066": 85, "56521754": [127, 130], "565915": 95, "566": 103, "566024": 99, "56633984": [127, 130], "566600": 99, "56670073": 99, "566701": 99, "566964": 83, "567004": 102, "567491": 84, "567695": 83, "567945": [87, 88], "568071": 95, "568105": 71, "568143e": 83, "56824247": 74, "568287": 90, "5686": 72, "56894697": 143, "56915009": [127, 130], "569449": 114, "569761": 97, "569911": 65, "5699994715": 65, "57": [67, 78, 97, 100, 114, 143, 161, 178], "57002385": [127, 130], "570278": 83, "570351": 79, "570486": 64, "570562": 64, "570722": 175, "5708": 95, "570885": 74, "571040": 84, "571429": 85, "5714294154804167": 85, "571778": 64, "5718": 95, "5722": 94, "57245066": 99, "572451": 99, "572717": 101, "573349e": 84, "573679": 16, "573700": 70, "574": 67, "574050": 83, "574160": 90, "57436": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "574538": 74, "574796": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "5748": [115, 124], "57487552": 74, "57496671": 65, "575": 11, "57500243": 74, "575381": 94, "575423": 16, "57572422": 102, "57585824": 102, "57592948e": 143, "575959": 71, "57599221": 102, "575e": 100, "576": 67, "576002": 97, "5763996": 65, "57643609": 102, "576879": 93, "577": 67, "5770": 94, "577071": 83, "577136": 74, "57715074": 65, "577210e": 84, "577271": 93, "5775": 143, "57751232": 95, "577634": 83, "5776971": 102, "57775704": 102, "577807": [83, 84], "577884": 95, "577e": 100, "5780788": [127, 130], "578081": 95, "578307": 99, "578437": 71, "578557": 84, "578847e": 85, "579080": 78, "579125": 94, "57914935": 66, "579197": 90, "579213": 103, "579238": 85, "579322e": 94, "579521": 98, "57e": 66, "58": [31, 66, 78, 94, 97, 103, 114, 143, 161, 177], "5800": 95, "58000": 94, "5804": [67, 95], "580414": 103, "580713": [127, 128], "580751": 94, "580831": 84, "580922": 87, "581048": 71, "5816666": 127, "581675": 78, "581880": 92, "582031": 94, "582290": 84, "582331": 95, "58241568": 143, "582528": 74, "582705e": 83, "582754": 101, "582761": 85, "582803": 83, "582991": 94, "583076": 95, "583140": 84, "583195": [83, 84], "58327432": 74, "5833333": 67, "583534": 99, "583870": 73, "583969": 83, "584007": [127, 130], "584742": 88, "584849": 85, "5852": 95, "585402": 79, "585412": 83, "585426": 143, "585479": 90, "585513": 84, "585793": 85, "585826": 97, "585995": 97, "58631757": [127, 130], "586362": 99, "586423": 83, "5867": 95, "5868472": 65, "586878": [127, 132], "586962e": 84, "587135": 84, "5875": 64, "587594": 95, "587670": 74, "587796": [127, 130], "588": 176, "58812": [115, 124], "5882": 94, "588364": 24, "588396": 85, "5883964619856044": 85, "58859711": [127, 130], "588824": 83, "589184": 78, "589667": 97, "59": [71, 78, 84, 97, 114, 127, 143, 161], "590": 95, "590098": 85, "590320": 70, "590402": 96, "590467244372398": 78, "5905": 94, "590670": [127, 132], "590736": 99, "590738": 99, "590813": 99, "590880": 79, "590911": 85, "590991": 85, "591080": 70, "591652": 94, "591678": 94, "591679": 84, "59172611": [127, 130], "59199423e": 143, "592093": 87, "592124": [127, 128], "592681e": 85, "5930036": 143, "59307502e": 143, "593588": 71, "593637": 84, "593648": [115, 116, 119], "593833": 71, "593981": 114, "594": [11, 62], "594244": 71, "594316e": 99, "59476031": 143, "595353": 85, "595676": 71, "595987": 84, "596": 95, "596076e": 95, "596228": 97, "596270": [87, 88], "596367": 83, "596588e": 84, "597": 66, "597051": 114, "59769369": [127, 130], "597923": 95, "59798": 95, "598371": 95, "598401": 97, "59854797": 127, "5985730": 66, "598753": 83, "598909": 74, "599297": [114, 115, 117, 127], "599586e": 92, "5cb31a99b9cc": 67, "5d": [85, 99], "5x_2": 70, "5x_3": 70, "5z_i": 99, "6": [11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 41, 42, 43, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 105, 107, 109, 111, 112, 114, 115, 116, 117, 119, 120, 121, 127, 128, 129, 130, 132, 143, 144, 161, 162, 168, 173, 175, 176, 177], "60": [65, 68, 69, 79, 85, 95, 96, 99, 104, 114, 143, 161, 176], "600": 93, "6000": 95, "6000000000000002": [85, 95, 99], "600000e": 95, "60013741": [127, 132], "600254": 98, "600354": 84, "600445": 78, "600694": 127, "600776": 80, "600895099": 143, "601": 66, "601061": 85, "601109": 79, "60112": 74, "60113": 74, "601158": 78, "601598": 93, "601757": 95, "602079": 88, "602168": 85, "602322": 114, "602587": 99, "602628": 85, "60271089": 143, "602870": 83, "603040": 74, "60337339": [127, 130], "604": 123, "604110": 95, "6043": 79, "604532": 95, "604841": [94, 95], "605": 95, "605195": 88, "60570778": 72, "606034": 99, "606129": 99, "6063077": 143, "606342": 85, "60650817": 72, "606515": 96, "606727": 83, "606733": 95, "60678471": 72, "6068": 65, "606800": 93, "606954": 85, "60712561": [127, 130], "6075": 178, "607618": 99, "60772111": 72, "608": [86, 100], "608177": 79, "608357": 74, "60857": 64, "608818": 102, "60883043": [127, 130], "60884259": 72, "60885835": 72, "60891278": 72, "609": 100, "60914855": 74, "609575": [12, 127, 129], "609947": 79, "61": [24, 71, 80, 97, 114, 143, 161, 177], "610053e": 84, "610093e": 83, "610318": 80, "610394": 97, "610400": 74, "611": 178, "611134": 97, "611245": 71, "611269": 93, "61170069": 100, "611859": 88, "611995": 95, "612": 100, "612073": 84, "612246": 90, "612310": 71, "612328e": 83, "61250258": [127, 130], "612792": 95, "61302716": 72, "613243": 74, "6133": 66, "613391": 84, "613408": 99, "613498": 95, "61357": 127, "613574": 89, "613688": 74, "613691": [12, 127, 129], "613950": 85, "614": 100, "614188": 93, "61448202": [127, 130], "61458483": 127, "614716": 84, "615": [62, 80], "6150747": 143, "615180": 78, "615863": [87, 88], "616013": 74, "616086": 95, "616116": 95, "616243": 96, "616346": 84, "616372": 94, "61669761": [162, 168], "616698": [162, 168], "616828": 95, "617121": 74, "617215": 97, "617217": 71, "617277": 114, "6173": 67, "617481": 95, "61771229": 100, "617780": 114, "617927": 97, "6180": 79, "618069": 94, "61810738": 66, "618625": 83, "618722": 95, "618753": 95, "619": 100, "619259": 74, "619294": 80, "619351": [83, 84], "619390": [83, 84], "619454": 70, "619613": 94, "61e": [66, 178], "62": [80, 88, 89, 97, 114, 143, 161], "620021": 84, "620026": 99, "620135": 84, "620156": 99, "620267": 84, "620517": 97, "6205846": 143, "620874e": 127, "620995": 104, "621060": 79, "621094": [94, 95], "621097e": 84, "621275": 98, "621359": 114, "621490": 99, "6215": 94, "621953": 99, "62195343": 99, "622": [95, 100], "622153": 95, "622272": 80, "6224": 65, "622502": 92, "623024": 85, "623107": 83, "6232": 79, "6236967": [127, 130], "623702": 74, "624": 93, "6240": 102, "6241": 95, "624206": 87, "624320": 16, "6243811": 65, "624535": [115, 119], "624798": 94, "624894": 114, "624919": 95, "625": [65, 93], "625002": 95, "625159": 89, "625165": 98, "625266": 97, "625436": 97, "625477": 99, "625888": 79, "625891": [87, 88], "6261064": 143, "626192": [127, 128], "626433": 99, "626530e": 83, "626765": 95, "626866": 84, "627319": 92, "627505": [87, 88], "627560": 99, "627564": 85, "62760904": 74, "627874": 84, "628": 100, "628069": 93, "62813191": 74, "628157": 74, "628213": 16, "62874477": 74, "628834": 95, "62896401": 74, "629055": 97, "62916": 78, "629319": 95, "629835": 95, "629e": 100, "63": [65, 80, 93, 96, 114, 143, 161, 176, 177], "630124": 78, "630150e": 99, "630162": 97, "630290": [127, 130], "630880": 100, "630914": 89, "631113": 84, "63117311": 127, "631179": 84, "631333": 99, "631762": 84, "6318": [94, 178], "631976": 97, "632058": 93, "632075": 71, "632333": 74, "63245862e": 143, "63273799": 74, "632747e": 99, "6330631": 161, "6333475": 143, "633433": 93, "633997": 71, "634": 100, "63407762": 178, "634078": [94, 178], "634587": 161, "6349": 79, "63499": 95, "635000e": [94, 95], "635199": [94, 95], "635349": 74, "63555591": 143, "635757": 95, "635900": 83, "63593298": [104, 127], "636": 62, "636048": 127, "636453": 27, "636639": 78, "637": 178, "637049": 74, "637108": 85, "637240": 95, "637326": 99, "6379": 94, "63817859": 99, "638179": 99, "638264": 99, "638488": 89, "638854": 74, "639": [94, 96], "639135": 93, "63916605": 66, "639585": 84, "64": [80, 88, 94, 95, 97, 100, 114, 143, 161, 175], "640": 95, "640314": 84, "640432": 83, "6405886": 143, "640906": 73, "641109": [127, 128], "64117907": [127, 130], "641355": 78, "641373": 74, "641528": 99, "642": 100, "642016": 99, "642096": 98, "642329": 80, "64269": 102, "6427": 95, "642768": 84, "642847": 97, "64340": 102, "643433": 71, "643512": 85, "643623": 99, "64362337": 99, "643749": 97, "643752": 99, "643939": 80, "643988": 83, "644111": 71, "644113": 114, "644202": 71, "644665": 85, "64476745e": 143, "644799": 70, "644803": 74, "644963": 84, "645": 95, "645583": 80, "64579": 64, "6458": 65, "645800": 93, "646318": 78, "646419": 84, "646937": 70, "646963": 30, "647": 127, "647196": 70, "64723": 102, "647357": 72, "64761459": 74, "647679": 84, "647689": 103, "647715": 71, "647750": 95, "647864": 114, "647873": 99, "64797": 102, "647986": [127, 132], "648": 94, "648000e": 95, "648094": 83, "648122": 74, "648472": [127, 132], "648751": 73, "648820e": 114, "649": [127, 176], "649158": 99, "649177": 79, "649335": 83, "649969": 95, "65": [77, 80, 85, 89, 95, 99, 100, 114, 143, 161], "650": 86, "6500000000000001": [85, 95, 99], "650046": 127, "6502": 95, "650567": 72, "65066863": 74, "650867": 85, "650893": 84, "65144127": 74, "651662": 95, "65167758": 74, "651919": 99, "65213584": 74, "6522": 176, "652324": 80, "652350": 93, "652450e": [94, 95], "652526": 87, "6527": 86, "652778": 93, "652816": 74, "652891": 71, "6529": 72, "652940": 84, "653008e": 94, "653056": 74, "653094": 83, "653424": 87, "653829": 90, "653846": 85, "653900": 114, "653901": [83, 84], "65394": 143, "654070e": 127, "654191": 79, "654755": 70, "654964": 92, "655284": 99, "6553": 178, "6554": 176, "655547": 83, "65557405e": 143, "655959": 90, "656296": 71, "656939": 84, "657": [67, 127], "657016": 74, "657283": 102, "657431": 78, "658138": 72, "658267": 99, "658345": 84, "658564": 71, "6586": 64, "658612": [108, 112], "658769": 83, "659": 67, "659245": [83, 84], "659387": 73, "6593871": 64, "659423": [83, 84], "659473": 103, "659755": 100, "659799": [108, 112], "659944": 74, "66": [80, 90, 96, 114, 143, 161, 175, 177], "660": [67, 127], "660128": 85, "660320": 88, "660360": 84, "660479": [15, 127, 129], "6607402": 100, "660776": 99, "661145": 95, "661166": 37, "66133": 127, "661338e": 95, "661521": 83, "662": 62, "66202941": 73, "66213425": 73, "662347": 84, "66235106": 73, "6625": 95, "662961": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "663115": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "663177": 80, "663182": 85, "663357": 84, "6634357241067574": 103, "663529": 99, "663533": 95, "663672": 90, "663859": 83, "664108": [127, 132], "664406": 71, "664846": 93, "665264": 99, "665467": 97, "66557278": 74, "665602": 92, "665868e": 95, "665974": 92, "66601815": [127, 129], "666104": 99, "666200": 97, "666220": 83, "666307": 70, "666530": 83, "6666667": 67, "666742": 114, "666750": 95, "66680088": 74, "666959e": 90, "667": 93, "6672603": 143, "667285944503316": 85, "667286": 85, "667536": 99, "66758915": 74, "667682": 69, "667985": 89, "668446": 89, "668584": 70, "668662": 74, "669130": 92, "66933951": 74, "669562": 87, "669733": 84, "669919": 84, "67": [62, 67, 79, 94, 103, 114, 143, 161, 175], "670051e": 83, "67045331": 143, "670511": 97, "670842": 74, "670867": 29, "671168": 114, "671271": [83, 84], "671382": 83, "671420": 92, "671633": 95, "671717": 84, "67199": 95, "6722": [67, 79], "672234": [83, 84], "672266": 83, "672384": [83, 84], "67245350": 65, "673092": [83, 84], "6732597": 143, "673302": 93, "6734628878523097": 85, "673463": 85, "673581815999206": 85, "673582": 85, "67410934": 65, "674181": 95, "674507e": 83, "6745349414": 65, "67456": 103, "674609": 85, "675011": 85, "675011328023803": 85, "675451": 78, "675489": 83, "675528": 84, "676093e": 87, "67618978": 69, "676242": 84, "676405": 85, "6765": [66, 94], "676536": 161, "676714": 83, "67674909": 69, "676756": 99, "676807": 94, "677227": 74, "677614": 99, "677980": 85, "678": 100, "678104": 79, "678117": 95, "678540": 79, "678826": 85, "678953": 79, "67913613": [127, 132], "67931778": 73, "67936506": [127, 129], "679486": 79, "679539": 93, "67954547": 73, "67969571": 73, "67ad635a": 67, "68": [67, 80, 101, 102, 114, 143, 161], "680": 95, "680366": 95, "6807": 79, "681": 93, "6810775": 102, "681176": 93, "681261": 83, "681817dcfcda": 67, "68183347": [127, 130], "682": 62, "682247": 71, "682315": 95, "6826": [38, 94], "682830": 84, "6833263": 143, "683331": 85, "683487": 84, "683496": 71, "683582": 127, "683637e": 90, "683785": 84, "683942": 99, "684": 178, "68410364": 66, "68411700": [66, 178], "684502": 99, "684657": 84, "685066": 83, "685107": 99, "685143": 92, "68554404e": 143, "68562150e": 143, "685807": 99, "685816": 83, "685970": 83, "685989": 127, "685995": 72, "686684": 84, "686694": 84, "686698": 83, "686892": 71, "687227e": 84, "687345": 99, "687647": 99, "687854": 70, "687871": 93, "6878711": 65, "68799": 96, "688": 176, "688537": 114, "688540": 114, "688561": 71, "688886": 114, "688918": 95, "689088": [83, 84], "689188": 70, "689392": 99, "689456842": 143, "689542": [108, 112], "689967": 84, "69": [89, 96, 114, 143, 161, 177], "690536": 95, "69057693": 73, "690723": 74, "69088788": 71, "690968": 83, "690986": 97, "69110442": 73, "69140475e": 143, "691511": 94, "691761": 95, "69187724": 73, "6921": 79, "692180": 72, "69232607": 71, "692538": 84, "692725": 99, "692768": 83, "692812": 74, "692907": 95, "6930": 79, "693345": 95, "693359": 95, "6934117220290754": 85, "693412": 85, "693495e": 95, "693796": 93, "693837": 79, "693874": 83, "693985": 74, "69404895": [127, 132], "694154": 85, "694274": 73, "69452316": 71, "694561": 100, "694755": 95, "694919": 93, "69505403": 127, "69520523": 69, "6955": 95, "695581": 89, "695582": 92, "69562150e": 143, "695860": 83, "696011": 28, "696289": [87, 88], "696526": 74, "69667705": 143, "69680706": 71, "696826e": 84, "69684828": 127, "697": 93, "697000": 85, "697118": 99, "697230": 97, "697420": [87, 88], "697591": 83, "697900": 84, "698": 178, "698223": 70, "698244": 70, "698302": 83, "69840389e": 143, "698450": 16, "69850969": [127, 130], "698694": 93, "699035": 99, "699082": 85, "699086": 71, "699097": 79, "69921": 67, "699259e": 99, "699333": 85, "699689": 74, "6_design_1a": 86, "6_r2d_0": 86, "6_r2y_0": 86, "6b": 161, "6cea": 67, "7": [12, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 40, 41, 42, 46, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 105, 106, 107, 109, 111, 112, 114, 115, 116, 117, 119, 120, 121, 127, 128, 129, 130, 132, 143, 144, 161, 162, 168, 175, 176, 177], "70": [66, 68, 85, 94, 95, 99, 114, 143, 161, 177], "700": [83, 84, 86, 93], "7000000000000002": [85, 95, 99], "700015": 99, "70007159": 99, "700072": 99, "700102": 99, "700314": 80, "700596": 99, "701088": 94, "701106": 89, "701413": 95, "701491": 87, "70162423": 71, "701672e": 85, "701841e": 88, "702119e": 84, "702500": 95, "702632": 84, "703049": 83, "703189": 72, "703325": 90, "703597": 71, "7040": 95, "704039": 84, "704482": 90, "7045": 90, "704558": 90, "704613": [127, 130], "704881": 84, "704896": 90, "705170": 83, "705519": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "705581": 95, "7055958": 105, "705595810371231": 105, "7055958103712310": 105, "7056177": 71, "705620": 83, "705682": 83, "705798": 84, "705801": 84, "70583": 102, "705981": 84, "706173": 84, "706208": 95, "706385": 83, "706645": 85, "707101": 85, "707868": 99, "70793174": [127, 130], "707963e": 94, "708003": [127, 132], "708062": 84, "708154": 84, "708190": 93, "708459": 99, "708532": 83, "708582": 97, "708762": 84, "708821": 80, "708837": 80, "708934": 83, "709026": 70, "7093568": 143, "70937397": 143, "709593": 83, "709606": 29, "71": [114, 143, 161, 177], "710059": 80, "710715": 97, "711051": 83, "711328": 95, "711367": 71, "711518": 95, "711748": 72, "71177947": 71, "712065": 87, "712072": 98, "712095": 80, "712130": [127, 130], "712209e": 84, "712503": 102, "712507": 71, "71258113": 71, "712592": 94, "712631": 83, "712846": 114, "712921": 95, "712960": 85, "713": 95, "713137": 84, "713236": 97, "713582": 92, "713719": 83, "713991": 71, "71407235": 71, "714240": 93, "714261": 84, "7142787": 99, "714279": 99, "714321": 94, "715013": 95, "715179e": 95, "715407": 85, "7155": 95, "715596": 83, "7157": 95, "715764": 16, "7158581": 65, "716": 95, "716098": 80, "7161": 95, "716100": 78, "716456": 99, "716615": 80, "716668": 97, "716740": 114, "716762": 85, "716793": 85, "716799": 93, "7167991": 65, "716917": 74, "717185": 99, "718253": 97, "718294": 69, "718686": 103, "7193": 95, "719401": 74, "719831": 84, "72": [68, 90, 114, 143, 161, 177], "72017804": [127, 132], "720208": 83, "720409": 83, "720447": 79, "720571": 99, "720589": 100, "72066211": 71, "720664": 93, "721071": 99, "721097": 16, "721118": 92, "721243": 83, "7215093d9089": 67, "72155839e": 143, "721604": 84, "721609": 95, "721719": 72, "722": 93, "722316": 99, "7224": 95, "722634": 99, "722848": 85, "723": 67, "72330647": [127, 132], "723314": 99, "723342": 114, "723345e": 99, "723414e": 84, "723689": 95, "72369688": [127, 132], "723846": 80, "7239": 95, "723958": 83, "7241399": 65, "724172": 97, "724338": 99, "724375e": 84, "724430e": 97, "724498": 79, "724748": 74, "724767": [87, 88], "72484982": 71, "724918": 103, "725": 67, "725010": 80, "725031": 99, "725087": 95, "725142": 83, "725166": 99, "725232": 114, "725477": 83, "725662": 78, "725992": 83, "726": [67, 100], "726078": 79, "7268131": 65, "7271188": [127, 132], "72718858": 71, "727501": 95, "727543": 70, "727550": 97, "727693": 95, "727759": 97, "727780": 95, "727976": 85, "7282094": [127, 129], "728478": 84, "72860010": 143, "728710": 99, "72875815e": 143, "728e": 100, "729571": 74, "729668": 114, "7297": 64, "72993015": [127, 132], "72995122": 71, "73": [66, 77, 80, 114, 143, 161], "730011e": 83, "730116": 97, "73051187": 71, "730629": 92, "730680": 71, "730773": 83, "7308": 71, "730823": 83, "731338": 74, "731754": 85, "731928": 92, "732": 100, "732040": 74, "732162": 84, "732307": 83, "7326": 95, "732626": 71, "732706": [127, 130], "732845": 83, "73285": 27, "732983": 83, "73299337": 71, "733404": 78, "733945e": 71, "734123": 97, "734145": 97, "734332": 83, "7344": 79, "734948": 99, "735218": 71, "735426": 83, "735587": 79, "735620": 71, "735694": 99, "73569431": 99, "735713": 84, "73579164": 78, "735848": 114, "735958": 71, "735964": 70, "73602094": [127, 132], "736082": [83, 84], "73649108": [127, 132], "736770e": 83, "7368": 64, "737077": 97, "737210": 84, "7375615": 66, "73764317e": 143, "73766741": [127, 132], "737796": 95, "737884": 25, "737915": 74, "737951": [83, 84], "7380": 71, "7381": 72, "738147": 95, "738315": 95, "738422": 84, "738936": 84, "739": 95, "739125e": 83, "739182": 83, "739261": 84, "7393": 79, "73949306": [127, 132], "739595": 100, "739660": 83, "739720": 95, "739817": 89, "74": [31, 66, 84, 94, 114, 143, 161, 177], "740": 93, "740180e": 99, "740200": 83, "740339": 79, "740417": 94, "7405": 95, "740505": 80, "7407627053044026": 85, "740763": 85, "740869": 85, "741043": 83, "741104": 85, "741255": [127, 128], "741380": 100, "741523": 80, "741702": 99, "74189": 67, "742128": 99, "742365": 98, "742407": 98, "742695": 83, "742907": 99, "7437": 95, "744024": 71, "74402577": 99, "744026": 99, "744125": 84, "744211": 95, "744236": 102, "74461783e": 143, "745": 143, "745353": 83, "745638": 94, "746361": 99, "746843": 88, "747008": 83, "747023": 78, "747057": 79, "747164": 99, "747381": 74, "747945": 65, "747977": 84, "748284": 92, "748296": 96, "748377": 94, "748513": 95, "7486": 64, "748945": 95, "749068": 71, "749204": 74, "74938952": [127, 129], "749854893": 144, "749982": 97, "75": [18, 29, 31, 67, 70, 80, 85, 90, 94, 95, 99, 114, 143, 161, 177], "75000": 103, "7500000000000002": [85, 95, 99], "750275": 95, "750601": 73, "750701": 80, "751013": 95, "751081": 95, "751194": 79, "751281": 84, "751289": 74, "751372": 83, "751541": 74, "751633": 95, "75171": 94, "751710": [85, 94], "751712655588833": 105, "7517126555888330": 105, "751712656": 105, "752522": 83, "752696": 80, "752909": 90, "753136": 83, "7533": 94, "753516": 84, "753523": 99, "753880": 84, "754503": 83, "75466617": [127, 132], "754692": 114, "7548": 103, "754870": 93, "755367": 71, "755721": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "755739": 74, "755831": 74, "755885": 114, "7560824": 65, "756120": 97, "756200": 80, "756337": 83, "756539": 79, "756585519864526": 85, "756586": 85, "7566": 71, "756805": 93, "756867e": 95, "756969": 85, "757": [100, 176], "757018929": 143, "757031": 71, "757136": 95, "757151": [83, 84], "757559": 90, "757596": 85, "757663": 87, "757690": 99, "757819": 93, "758027": 95, "758334": 72, "758340": 79, "758754": 84, "758852": 84, "75887": 67, "75899873": [127, 132], "759373": 84, "76": [114, 143, 161, 176, 177], "760104": 99, "760155": 95, "76023347": [127, 132], "760248": 71, "760386": [12, 127, 129], "760403": 84, "760660": 97, "760778": 93, "760868": 78, "760915": 70, "761": [65, 93], "761224": 80, "761317": 79, "761487": 71, "761714": 85, "762049": 79, "762284": 99, "76228406": 99, "763218": 74, "763978": 72, "764093": [83, 84], "76419024e": 143, "764315": 99, "76444177e": 143, "764782": 73, "764859": 69, "764953": 94, "765092": [127, 132], "765363": [83, 84], "765500e": [94, 95], "76551270": 143, "765557": 83, "7656": [64, 95], "765710e": 104, "765792": 99, "76591188": 65, "766005": 99, "76608187": 72, "766109": 97, "766336": 71, "766499": 99, "766585": 95, "7669": 95, "766940": 80, "767020": 73, "76702611e": 143, "767027": 79, "767188": [87, 88], "767435": 103, "767486": 84, "768273": [87, 88], "768439": 71, "768798": 80, "768846": 74, "768874": 84, "769063": 95, "769361": 99, "769402": 74, "769555e": 78, "769633": 71, "769805": 99, "77": [97, 100, 114, 143, 161], "770": 62, "770556": 95, "770944": [87, 88], "7710": 102, "7711": 64, "771167": 161, "7712": 79, "771247e": 71, "771275": 95, "771383e": 95, "7714": 96, "771463014326052": 78, "7715": 64, "771529": 92, "7716982": 66, "771876": 95, "771965": 95, "771986": 69, "772": 123, "772157": 90, "77227783e": 143, "772373": 83, "772444": 114, "772612": 99, "77261209": 99, "772791": 95, "77289874e": 143, "773": 67, "773177": 85, "7733": 64, "773305437": 143, "773339": 90, "7735388": 74, "773881": 97, "77400201": 74, "77401500e": 143, "774212": [127, 132], "774236": 96, "774365": 84, "7746": 71, "774683e": 93, "774915": 83, "775": [67, 95], "775191": [83, 84], "775969": 102, "7763": 94, "776601": 83, "776887": 94, "777297": 26, "7776071": 65, "777728": 24, "777867": 90, "777927": 97, "77794511": 74, "777e": 100, "778305": 83, "779": [100, 127], "779068": 90, "779349": 83, "779510": 97, "779517": [83, 84], "779682": 85, "779727e": 95, "78": [87, 97, 100, 114, 143, 161, 177], "780": 67, "780458": 99, "780856": 94, "780943": [84, 99], "781076": 95, "781352": 71, "7816": 72, "781681": 99, "782": 67, "782147": [127, 132], "782159": 95, "78240482": [127, 132], "782643": 99, "782997": 97, "783": [67, 95], "783183": 84, "783211": 73, "783276": 127, "784": 161, "784206": 97, "784238": 93, "784405": 102, "784841": 95, "784872": 80, "784910": 84, "784933": [39, 127], "785": 67, "7850": 64, "785107": 85, "785815": 80, "785911": 99, "785979": 92, "786": 67, "786191": 89, "786429": 83, "786744": 85, "786986": 80, "78711285e": 143, "78721": [127, 128], "787219": 97, "78777": 102, "787795": 16, "787838e": 83, "788": 176, "78818": 67, "788267": 92, "788337": 74, "788385": 114, "78864446": 143, "788896": 84, "7889255": [127, 130], "788972": 83, "789": 127, "789355e": 102, "78980657": [127, 130], "79": [80, 114, 143, 177], "790": 94, "790000e": 95, "790115": 95, "790142": 92, "79014812": 74, "790261": 114, "790723": [87, 88], "790885": 84, "7910908091075142": 85, "791091": 85, "791097": 94, "791241": 99, "791297": 29, "79228725": [127, 132], "792396": 80, "792889": 74, "792939": 85, "792972": 114, "793297e": 95, "793315": 114, "79338596e": 143, "793570": 99, "793680": 74, "793735": 99, "793818": [83, 84], "794": 127, "794119": 95, "794366": 95, "79458848e": 143, "794755": 95, "794805": 87, "795": 100, "795008": 84, "795647": 99, "795828": 84, "795932": [115, 121], "7963": 95, "796340": 95, "796605": 71, "796854": 97, "7969": 72, "796975": 97, "796e": 100, "797078": [127, 132], "797095": 73, "797280": 99, "797323": 93, "797379": 74, "797410": 83, "797454": 127, "797617": 16, "797743": 161, "79792890e": 143, "797966": 79, "797971": 161, "797984": 84, "798308": 94, "798388": 84, "798396": 96, "798685": 26, "798706": 72, "798783": [87, 88], "798862e": 83, "7992264": [127, 132], "799403": 99, "799698": 83, "799890": 83, "7b428990": 67, "7x": 99, "8": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 42, 54, 57, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 105, 107, 109, 112, 114, 115, 116, 117, 119, 120, 121, 126, 127, 128, 129, 130, 132, 143, 144, 161, 162, 168, 175, 177, 178], "80": [68, 69, 78, 79, 85, 95, 99, 104, 114, 143, 177], "800": 93, "8000": [36, 68, 104], "8000000000000002": [85, 95, 99], "800000e": 95, "8001": 72, "800175": 96, "800577": 73, "800668": 78, "801130": 95, "801269": 127, "801373": 72, "8014": 72, "8016702": 74, "802": 100, "802149": 84, "802738": 114, "803300": 83, "803492e": 99, "803889e": 95, "803908e": 84, "804219": 99, "804284": 102, "804316": 99, "804549": 95, "8048": 66, "804828": 99, "805007": 93, "805153e": [94, 95], "8055563": 65, "805714": 83, "805720": [71, 99], "805739": 71, "805789": 83, "8059": 94, "806073": 74, "806167": 95, "806220e": 95, "806356": 95, "806532e": 83, "806732": 89, "806736e": 83, "80696592e": 143, "80714504e": 143, "8074": 143, "807879": 99, "808": [66, 161], "808246": 94, "808347": 69, "808352": 73, "808640": 95, "808860": 84, "808901": 74, "809": 95, "809278": 69, "809392": 83, "8095": 96, "809913": [83, 84], "80a8": 67, "81": [65, 83, 86, 89, 97, 114, 143, 177], "810044": 94, "810118": 78, "810134": 99, "8102": 94, "810306": 80, "810366": 97, "810382": [94, 95], "810564": 83, "810650": 99, "810833": 84, "810916": 95, "810960": 78, "810975": 74, "811083": 74, "811155": 89, "811255": 84, "81128199": 99, "811282": 99, "811329": 78, "811458": 94, "811675": 74, "8116912": 161, "811763": 83, "811825": 93, "811901": 99, "81190107": 99, "812": 100, "812003": 83, "812658": 84, "812901": 71, "813006": 72, "8132463": 65, "813293": 99, "813342": 161, "813682": 95, "813701": 72, "814351": 85, "814717": 85, "814913": 93, "814935": 74, "815224": 161, "815226": [15, 127, 129], "815993": 99, "8160": 95, "816131": 83, "816176": 103, "816296e": 83, "816318": 93, "8164": 64, "816417": 83, "817": 62, "817967": 127, "817977": 79, "818029": 95, "81827267": 99, "818273": 99, "818289": 99, "81828926": 99, "818362": 74, "818380": [83, 84], "818443": 97, "81850929": [127, 132], "81856": 67, "818990e": 84, "819223": 114, "819519": [127, 130], "819805": 73, "82": [79, 97, 103, 114, 143, 177], "8202": 66, "820366": 93, "8208": 95, "82087071": 71, "8209": 66, "820963": 80, "820993": 95, "821": 176, "8210": 66, "821009": 95, "821021": 85, "821246": 97, "821422": 97, "8215": 64, "821566": 99, "821988": 96, "822289": [94, 178], "82228913": 178, "822482": 85, "8226": 79, "8227": 95, "822822": 85, "823134": 97, "823247": 99, "823273": [83, 84], "82329138": 77, "823666": 84, "82372418": [127, 130], "824350": [83, 84], "824701": 85, "824750": 85, "824889": 85, "824961e": 95, "825617": 93, "825749": 84, "8260": 94, "826013": 97, "826065": [83, 84], "826074e": 93, "826085346": 143, "826366": 78, "826426": [15, 127, 129], "826492": 99, "826519": 29, "82666866e": 143, "82684324": 102, "826897": 99, "827093": 83, "827381": 99, "827414": 79, "827735": 99, "827874854703913": 85, "827875": 85, "827938162750831": [87, 88], "828157": 80, "828362": 97, "828618": 90, "828915": [87, 88], "829038": 37, "829543": 85, "829764": 114, "829863": 97, "83": [78, 114, 143, 177], "830102": 83, "830120": 84, "830142": 91, "830273": 80, "830301": 98, "830850": 83, "830967": 83, "831": 100, "831019": 85, "831167e": 83, "831479": 83, "83156889": 143, "832086": 99, "832446": 73, "8325": 127, "8326928": 102, "832693": 102, "832875": 99, "83287529": 99, "832965": 95, "832991": 16, "833018": 95, "833024": 93, "833027": 97, "833065": 80, "833096": 95, "833227e": [115, 116, 119], "833510": [127, 128], "833551": 71, "833563": 92, "8336022": 97, "833907": 93, "834133": 90, "835": 100, "8350": [79, 95], "835344": 80, "835750": 90, "835812": 71, "835839": 95, "836234": 127, "837": 123, "837022": 78, "837366e": 84, "837699": 84, "838107": 74, "838114": 99, "838235": 96, "838322": 83, "83854057": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40], "838883": 79, "839572": 71, "8396": 71, "8397": 64, "84": [67, 89, 100, 114, 127, 143, 177], "840041": 95, "84014899": [127, 130], "840202": 97, "840303": 99, "84030318": 99, "840411": 74, "840718": [12, 127, 129], "840836": 99, "840991": 74, "840995e": 94, "841": [65, 93], "841132": 94, "8415": 66, "841712": 95, "841847": 95, "841920": 84, "842029": 74, "842205": 95, "842262": 127, "842405": 85, "842625": 93, "842746": 99, "8428": 94, "842853": 99, "8429": 79, "843": 95, "843018": 114, "843296": 95, "843399": 71, "843454": 83, "843730": 93, "844026": 114, "844308": 99, "8445": 77, "844549": [87, 88], "844663": 80, "844673": 161, "844707": 99, "844893": 93, "8452": 79, "845241": 100, "845525": 83, "845579": 96, "84609957": [127, 132], "8461": 79, "846602": 95, "846707": 95, "847": 62, "847136": 85, "847409": 71, "847595": 28, "847948": 85, "84813353": [127, 130], "848578": 95, "848734": 83, "848757e": 94, "848868": 85, "848997": 95, "84930915e": 143, "849350121": 143, "849427": 114, "849747": 102, "849766": 95, "8497f641": 67, "8498": 95, "85": [34, 77, 85, 89, 95, 99, 104, 114, 143], "8500000000000002": [85, 95, 99], "850038": 80, "850101": 84, "850242": [127, 130], "850321": 93, "850575": [83, 84], "850587": 72, "850646": 97, "850656": 90, "850684": 74, "850970": 74, "851": 176, "851012": 92, "851100": 80, "8513": 67, "851311": 72, "851366": 93, "852016": 99, "852592e": 92, "852916e": 84, "853538": 73, "853675": 97, "85397773": [127, 129], "85402594": 77, "855": 95, "855242": 83, "855780": 99, "856037": 84, "856124": 97, "856404": 88, "856552": 80, "856758": 114, "857161": 99, "857515": 114, "857544": 93, "85832446": 71, "858577": 98, "858595": 73, "858635": 92, "858713": 73, "858794": 71, "858952": 80, "859": 95, "85911521e": 143, "85912862": 161, "859129": [40, 144, 161], "8597": 94, "85c5": 67, "85e": 66, "86": [78, 114, 143, 177], "86024531": 71, "860378": 95, "860663": 161, "860804": 99, "861004": 97, "861019": 80, "861877": 74, "862043": [87, 88], "862274": 87, "862359": 85, "863337": 83, "86343": 78, "863430": 78, "863772": 94, "863982270": 144, "864": 100, "86415573": 66, "86424193e": 143, "8643": 64, "8644": 67, "864404": 95, "864664": 80, "8648": 79, "865101": 79, "865313": 95, "865442": 92, "865562": [83, 84], "865853": 74, "865860": [94, 95], "866102": [83, 84], "86610467": 99, "866105": 99, "866142": 84, "866179899731091": 105, "866179900": 105, "866579": 95, "866798": 95, "867187": 78, "867565": 99, "867775": 84, "8679": 95, "868": 67, "8681": 77, "868728": 72, "869": [67, 100], "869020": 85, "869301": 114, "869427": 38, "869586": 89, "869617": 114, "86975793": [127, 130], "869778": 98, "869787": [127, 132], "869912e": 84, "87": [66, 83, 89, 93, 114, 143, 177], "870": 73, "8700": 66, "870099": [87, 88], "870187": 84, "870260": 99, "870288": 83, "870332": 99, "870444": 94, "870943": 84, "871": 67, "871056": 71, "871080": 79, "8714": 72, "871972": 90, "872": 100, "872354": 99, "872435e": 97, "8725": 64, "872768": 99, "872852": 99, "87290240e": 143, "873039": 84, "873198": 95, "873289": 71, "873677": [87, 88], "873848": 73, "87384812361": 64, "87384812362": 64, "87430335": 161, "874303353": 161, "874702": [87, 88], "874721": 83, "87484243": [127, 130], "8751": 95, "875275": 74, "8759": 95, "875993": 74, "876": [100, 123], "876233": 73, "87623301": 64, "87640238": [127, 130], "876431e": 85, "876638": 74, "87674597e": 143, "877": 62, "8771": 95, "877153": 95, "877287": 16, "877446": 114, "877455": 98, "877833": [83, 84], "877903": 80, "878122": 92, "878281": 99, "878402": 83, "878746": 80, "878802": 95, "878851": 72, "878895": 80, "878914": 83, "879103": 85, "879470": 97, "879850": 73, "87e": 66, "88": [42, 66, 78, 89, 100, 114, 143], "880083": 74, "880106": 93, "880579": 99, "880591": 98, "880886": 94, "8810": 94, "881201": 95, "88125046e": 143, "881254": [127, 132], "881457": 73, "881465": 70, "881567": 84, "88173062": 65, "88173585": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40], "881937": 80, "882059": 115, "882470": 16, "882641": 94, "8828": 64, "882896": 99, "882928": 80, "883123": 120, "883224": 84, "883301": 85, "883622": 99, "883741": 97, "883914": 85, "88415932": [127, 130], "8843": 102, "884404": 96, "884821": 114, "884996": 85, "8850": 66, "885065": 99, "885513": 95, "885686e": 83, "885832": 100, "885978": [87, 88], "886086": [83, 84], "88629": 64, "886300": 83, "886402": 71, "886531": 97, "886611": 95, "88664": 67, "886764": 84, "8872": 72, "887212": 95, "887270": 84, "887556": 85, "887731": 69, "887778": 95, "888146": 93, "8881461": 65, "888217": 95, "88825455": 143, "888348": 84, "888352": 80, "888423": 95, "888757": 99, "888775": 88, "889261": 83, "889293": 99, "889566": 102, "889638": 80, "889733": 99, "88988263e": 143, "889913": [83, 84], "88ad": 67, "89": [66, 84, 97, 114, 143, 176, 177], "890": [65, 93], "89027368": 161, "890273683": 161, "890342": 83, "89035917": 89, "890372": [75, 107, 112, 175], "8903720000100010000010": [67, 107, 112, 175], "8904": 62, "890454": [115, 121], "890548": 78, "890665": 90, "890855": 80, "89086974": 71, "8909": [65, 94, 178], "891102": 84, "891299": 79, "891350": 97, "891472": 71, "891547": 78, "891593": 83, "891606": 94, "891649": 25, "891757": 84, "892": 67, "892648": 99, "892796": [83, 84], "89282006": 71, "892828": 90, "893": 67, "8932105": 65, "893461": 80, "893521": 84, "893649": [83, 84], "893851": 99, "893884": 95, "894": 67, "894297": 74, "894329e": 95, "89460366": [127, 132], "8946549": 68, "894701": 84, "895106": [83, 84], "895275": 16, "895308": 95, "895333": 99, "895442": 94, "895690": [83, 84], "895768e": 85, "895853e": 83, "896023": 99, "896182e": 90, "896263": 90, "896333": 84, "896761": 72, "896820e": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "897115": 83, "897208": [127, 128], "897220": 99, "897396e": 74, "8974": 94, "897590": 97, "8979": 79, "89814202": [127, 132], "898183": 80, "898722": 99, "899021": 114, "899054": 74, "899060": 71, "899081": 71, "899250": 80, "899460": 99, "899568": 83, "8996": 77, "899654": 80, "8bdee1a1d83d": 67, "8da924c": 67, "8e3aa840": 67, "9": [12, 14, 15, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 42, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 105, 107, 109, 112, 114, 115, 116, 117, 119, 120, 121, 127, 128, 129, 130, 132, 143, 144, 161, 162, 168, 174, 175, 177, 178], "90": [43, 66, 68, 69, 85, 95, 99, 104, 114, 143, 177], "9000000000000002": [85, 95, 99], "900021": [115, 121], "900829": 90, "901": 123, "901013": 84, "901096": 71, "901148": 99, "901207": 74, "90136": 94, "901360": 94, "901526": 89, "901563": 16, "901797": [127, 128], "901998": 92, "902": 161, "902219": 83, "902573": 85, "902920": 114, "903056e": 99, "903339": 85, "903366e": 85, "903418": 93, "903575": 74, "903681": 99, "903731": 92, "903767": [83, 84, 97], "90385035": 71, "903879": 84, "9045": 95, "904890": 71, "904900": 83, "904976": 83, "905494": 85, "905612": 79, "90568159": 71, "905848": 73, "905858": 103, "905951": 102, "906051": 85, "906051023766621": 85, "90653054": 71, "9067": 95, "906716732639898": [87, 88], "906757": 81, "90678968": [127, 130], "907115": 99, "907176": 99, "907491": 85, "907801": 93, "907879": 80, "90794478": 161, "907944783": 161, "908024": 103, "908307": 97, "90835303": 74, "908624": 73, "908912": 84, "909157": 71, "909304": [83, 84], "909571": 80, "90963122e": 143, "909942e": 24, "909997": [94, 178], "91": [78, 101, 114, 143, 177], "910288": 74, "9109": 67, "911": 62, "91102953": 99, "911030": 99, "9112": 94, "911226": 97, "911264": 74, "911794e": 84, "911818": 84, "912033": 30, "912230": [83, 84], "9126": [66, 178], "9127": [66, 178], "912801": 84, "912837": 83, "913": 67, "913072": 74, "91315015": 65, "913280": 103, "913585": 90, "913774": 85, "914": 96, "914161819272930394044485658626472737490": 143, "9142": 95, "9143065172164": 78, "914319": 74, "91438767e": 143, "9145": 64, "914598": 80, "915": [66, 67, 94, 95], "915000e": [94, 95], "915154": 71, "915165": 73, "915488": [87, 88], "915502": 84, "916171": 98, "9162": 79, "916236": 64, "916355": 84, "916359": 80, "9166667": 67, "916913": 99, "916914": 99, "916921": 95, "916984": 99, "917": 67, "917038": 79, "917116": 71, "917497": 87, "91771387": 99, "917714": 99, "9179": 95, "918": 100, "918044": 83, "918199e": 83, "918227": 85, "919826": 97, "919879": 95, "91e": 66, "92": [71, 101, 114, 115, 124, 143, 177], "920283": 95, "920337": 88, "920661": 78, "920695": 95, "920878": 74, "9209": [64, 79], "9210": 95, "921061": 103, "921198": 90, "921372": 85, "921778": 90, "921913": 93, "921956": [83, 84], "921e4f0d": 67, "922006": 71, "922232": 83, "92248736": [127, 132], "922668": 90, "922996": 93, "923033": 84, "923084e": 85, "923228": 74, "923517": 104, "92351798": 71, "923607": 99, "92369755": 65, "923943": 178, "924002": 99, "924061": 83, "924232e": 92, "9243": 95, "924396": [87, 88], "924415": 74, "924443": 80, "924540": 92, "924634": 70, "924636e": [39, 127], "924724": 85, "924732": 95, "9247983": 71, "9248": 67, "924821": 85, "9251": 64, "925189": 97, "925198": 97, "925231": 72, "925248": [87, 88], "92566": 127, "925736": 85, "925994": 94, "926085": 73, "926260": 87, "926363": 71, "926621": 85, "926626": [127, 130], "926685": 93, "926768": 74, "927": 63, "927074": 99, "9271": 64, "927232": 95, "9274": 95, "927501": 84, "927563": 84, "927848": 74, "927956": 78, "928233": 71, "928269": 95, "92827999": 127, "92881435e": 143, "9289008": [127, 130], "928991": 73, "929031": 83, "92904226": 71, "92905": 65, "929571": 71, "929607": 95, "929636": 78, "929699": 84, "92972925e": 161, "929729e": [40, 144, 161], "93": [66, 78, 90, 101, 114, 115, 123, 126, 143, 177], "930224": 73, "930260": 74, "930357": 78, "9304028": 65, "930497": 78, "930967": 73, "931": 106, "931479": 99, "931646": 83, "93196253": 71, "931978": 175, "932027": 85, "932233": 79, "932327": 83, "932404e": 95, "932651": 78, "932692": 73, "932906": 84, "932973": 99, "93300": 127, "933603": 84, "933996": 85, "934053": 74, "934122": 84, "934270": 93, "934428": 73, "934433": [83, 84], "9345": 67, "934511": 161, "93458": 102, "934634": 84, "934866": 97, "934992": 85, "935": 77, "935591": 99, "935730": 99, "935734": 71, "935983": 72, "935989": 93, "9359891": 65, "935993": 84, "9360": 72, "936094": 83, "936124": 78, "936238": 84, "93648": 104, "9365": 79, "936739": 99, "936748": 71, "937586": 95, "937681": 74, "937905": 71, "937967": 79, "938": 161, "93839805": 74, "938460": 83, "9386744462704798": 92, "9388": 95, "939068": [87, 88], "939390": 83, "9395": 95, "93958082416": 178, "93965493": [127, 132], "94": [77, 78, 114, 143, 177, 178], "940490": 74, "940693": [127, 128], "940739": 97, "940795": 74, "941047": 16, "941257": 74, "941364": 79, "941440": 83, "941709": 85, "9417090334740552": 85, "94212251": 71, "942139": 88, "942312": 99, "942460e": 99, "942550": 95, "942629": 95, "942661": 93, "942777": [127, 130], "94278854": 97, "94309994e": 143, "943426": 74, "943468": 74, "943548": 90, "94391245": 71, "944020": 97, "944045e": 99, "944253e": 99, "944266": [87, 88], "944280": 95, "944317": 95, "94441007e": 143, "944630": 99, "94473": 68, "945058": 83, "945289": 74, "946180": 90, "94629": 104, "946297": 85, "946406": 88, "946433": 99, "946968": 85, "947194": 83, "947290": 84, "947391": 78, "947440": 98, "947466": [115, 116, 119], "947855": 80, "9480": 95, "948112": 100, "94821111": 71, "9485": 64, "94863972": 71, "948644": 78, "948828": 83, "948947": 97, "948975": 89, "94906344": 65, "949241": 161, "94925824": 71, "949456": 99, "949459e": 83, "9497455": [127, 130], "949866": 84, "95": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 45, 48, 53, 64, 66, 68, 69, 70, 71, 72, 73, 74, 78, 79, 80, 83, 84, 85, 86, 87, 88, 89, 90, 94, 95, 99, 100, 102, 103, 104, 114, 127, 143, 161, 162, 168, 177, 178], "9500": 95, "950280": 99, "950545": 81, "95062986e": 143, "950864e": 74, "951532": 93, "951604": 84, "951920": 98, "952": [66, 100, 178], "952163": 74, "9523": 64, "952369": 73, "952392": 71, "952572": 79, "95261401": 71, "952839": 99, "95292821": 71, "95305": 68, "9532": 95, "953683": 93, "953706": 83, "95372559e": 143, "953870": 72, "953884": 127, "954": 161, "95401167e": 143, "954107": 97, "95431331": 71, "954738": 84, "9552": 64, "955370": 71, "955541": 29, "95559917": [115, 116], "955689": 83, "955787": 73, "956047": 65, "956091": 73, "9561": 64, "956272": 93, "956574": 95, "956588": 127, "95659459": 71, "956762": [127, 132], "957229": 88, "957339": 93, "957375": 93, "957417": 95, "957479": 85, "9574793755564219": 85, "957745": 85, "9579": 66, "957996": 85, "958": 161, "9580": 66, "958477": 71, "958636": 89, "959058": 72, "95912979": 74, "959441": 84, "959613": 114, "95e": 66, "96": [39, 66, 69, 83, 84, 96, 114, 127, 143, 177], "960049": 80, "960236": 127, "960808": 85, "960846": 95, "9609": 64, "961018": 84, "961538": 95, "961962": 85, "962022": 84, "962041": 84, "962228": 84, "962364": 80, "9625": 95, "962664": 74, "962918": 71, "962948": 79, "964025e": 99, "964261e": 93, "964317": 95, "9647": 64, "964914": 16, "964940": 84, "965": 69, "965371": 74, "965376": 83, "965531": 127, "96570886": 143, "96582": [115, 121], "966015": 99, "966254": 74, "966273": 97, "966566": 84, "967207": 74, "967467": 102, "967671e": 71, "967799": 74, "968127": 80, "968134e": 99, "968288": 95, "968305e": 83, "96838085": 71, "96860995": 71, "968703": 71, "96877491": 71, "96896106": [127, 132], "969": 123, "969115": 72, "969141": [114, 115, 117, 127], "969702563412915": 85, "969703": 85, "9699": 94, "969990": 74, "97": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 114, 115, 116, 117, 119, 120, 121, 127, 128, 129, 130, 132, 143, 144, 161, 175, 177, 178], "970": 62, "970065": 99, "970652": 84, "971058": [87, 88], "971088": 71, "971132": 95, "971153": 74, "971617": 78, "972": 96, "972051": 95, "972405": 83, "972793": 73, "97314470": 65, "973241": 99, "973293": 84, "973874": 85, "97391304": 71, "974": [96, 143], "974023": 83, "974066": 97, "974104": 84, "974202": 85, "974213": 84, "97436783": [127, 132], "97441062": [87, 88], "974414": 85, "974416": 84, "974491": 79, "97470872": 102, "9748910611": 65, "974925": 84, "97499195": 99, "974992": 99, "975": [77, 83, 84, 87, 88, 90, 96, 127, 130], "975289": 80, "9753": 67, "975561": 71, "97598781": 71, "976088": 99, "976398": 84, "976562": 99, "976791e": 84, "977113": 84, "977280": [83, 84], "977295": 95, "9773": 95, "977489": 95, "97755018": 71, "978000e": 95, "978052": 84, "97827524": 71, "978438e": 84, "978446": 83, "9787": 95, "978977": 99, "979": 100, "979135": 114, "979384": 90, "979409e": 84, "979966": 90, "98": [83, 84, 114, 143, 177], "980": 123, "980016": 74, "980086": 73, "9802393": 65, "980407": 84, "980574": 83, "980643e": 85, "980771": 83, "980838": 83, "981104": 98, "981339": 84, "981622": 83, "981672": 85, "981703": 83, "98175961": 71, "982353e": 95, "982392": [127, 130], "982417": 85, "982456": 78, "9826": 143, "982815": 95, "982948": 83, "983192": 99, "983350": 74, "983389": 83, "983482": 95, "983483": 80, "983759": 178, "98393441": 102, "984083": [87, 88], "984368": 79, "984562": 99, "984866": 161, "984872": [83, 84], "984937": 85, "98494283": 71, "98505871e": 143, "985185": 73, "985207": [83, 84], "985279": 95, "985427": 97, "985569": 95, "985971": 84, "986249": 94, "986683": 83, "98673": 89, "9870004": 67, "987175": 79, "9875": 64, "98750578": 69, "987849": 97, "987887e": 74, "9880384": 67, "988081": 83, "988421": [83, 84], "988463": 99, "988866": 96, "989135": 96, "989364": [127, 130], "99": [57, 66, 78, 80, 83, 84, 95, 96, 114, 143, 177], "990": 96, "990210": 95, "990567e": 95, "990838": 83, "990890": 74, "990983": 73, "991": [67, 96], "9914": [94, 95, 102], "9915": [66, 94, 95, 102], "991512": 66, "991824": 83, "991963": [83, 84], "991977": 95, "992": [96, 100], "992288": 83, "99232145": 102, "992359": 16, "992541": [127, 128], "992582": [83, 84], "992627": 74, "992759": 97, "993416": 90, "994": [96, 100], "994168239": 65, "994208": 80, "994304": 95, "994332": 81, "9944": [15, 127, 129], "994615": 79, "994711": 73, "9948104": 68, "994864": 95, "994937": 88, "995": 123, "9951": 64, "995248": 99, "99534683": 97, "99549118e": 143, "99571372e": 143, "9961": 94, "9961392": 65, "996413": 95, "99665933": 74, "996729": 83, "996934": 93, "9970": [74, 95], "997034": 104, "997038": 84, "997494": 104, "997571": 93, "997621": 85, "997934": [87, 88], "998050": 98, "998063": 81, "9981": 79, "99864670889": 178, "99883095": [127, 132], "998855": 95, "998861": 74, "999": [57, 70, 79, 89, 91, 96, 102, 178], "999120": 98, "999207": 99, "9995": [70, 83, 84], "999596": 37, "9996": [70, 83, 84], "999608": 71, "9996553": 66, "9997": [70, 83, 84], "9998": [70, 83, 84], "999823": 72, "9999": [70, 83, 84], "99c8": 67, "A": [10, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 34, 35, 37, 38, 39, 40, 41, 45, 47, 48, 50, 51, 53, 55, 56, 62, 63, 64, 66, 67, 71, 72, 73, 74, 77, 79, 81, 82, 86, 92, 96, 98, 100, 101, 102, 103, 106, 107, 112, 114, 115, 116, 119, 120, 122, 125, 127, 128, 130, 132, 142, 161, 162, 163, 169, 170, 171, 172, 173, 175, 176, 178], "ATE": [26, 30, 31, 66, 75, 79, 80, 90, 94, 102, 103, 114, 127, 137, 142, 144, 152, 162, 170], "ATEs": [78, 79, 80, 96], "And": [68, 96, 104, 162, 164, 165], "As": [63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 77, 78, 79, 80, 82, 83, 84, 85, 86, 89, 90, 92, 93, 94, 95, 96, 97, 99, 103, 104, 105, 115, 125, 126, 127, 130, 131, 132, 143, 144, 146, 148, 150, 161, 162, 163, 172, 178], "At": [18, 31, 41, 65, 69, 70, 72, 77, 78, 80, 89, 93, 95, 99, 178], "Being": 178, "But": [77, 90], "By": [64, 65, 93, 100, 103, 115, 126, 127, 130, 132, 162, 168], "For": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 51, 56, 62, 64, 65, 67, 69, 71, 72, 73, 74, 77, 78, 79, 80, 81, 89, 90, 92, 93, 95, 97, 98, 100, 102, 103, 105, 106, 107, 108, 112, 114, 115, 116, 123, 126, 127, 128, 129, 130, 131, 132, 133, 134, 140, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 170, 172, 173, 174, 175, 178], "ITE": [35, 79, 80], "ITEs": [78, 79, 80], "If": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 48, 49, 50, 51, 53, 55, 56, 57, 58, 63, 65, 69, 71, 72, 73, 74, 77, 78, 79, 82, 83, 84, 90, 93, 95, 100, 106, 107, 112, 114, 115, 126, 127, 129, 135, 144, 145, 146, 147, 148, 149, 152, 161, 162, 163, 164, 165, 166, 167, 168, 171, 172, 173, 178], "In": [7, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 50, 55, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 143, 144, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178], "It": [13, 64, 65, 66, 79, 83, 84, 86, 87, 88, 93, 94, 95, 97, 100, 101, 103, 108, 111, 112, 115, 122, 124, 127, 128, 143, 173, 177], "No": [33, 58, 62, 64, 66, 67, 68, 69, 71, 72, 73, 74, 75, 78, 79, 80, 89, 91, 94, 95, 97, 100, 102, 104, 107, 109, 110, 111, 112, 115, 116, 127, 129, 130, 132, 140, 144, 161, 175, 176], "Not": [127, 133], "Of": [77, 161, 178], "On": [63, 78, 82, 92, 96, 97, 101, 106, 176], "One": [42, 66, 71, 74, 94, 95, 103, 114, 161], "Or": 48, "Such": [103, 115], "That": [48, 178], "The": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 62, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 124, 125, 126, 127, 128, 129, 131, 133, 136, 138, 139, 140, 141, 142, 143, 144, 149, 152, 157, 158, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 176, 177, 178], "Then": [18, 85, 99, 101, 127, 161, 162, 172, 173, 174], "There": [66, 94, 103, 127, 133, 174, 178], "These": [25, 66, 67, 72, 76, 78, 92, 94, 98, 100, 102, 114, 127, 140, 178], "To": [12, 14, 15, 16, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 62, 63, 65, 66, 67, 68, 69, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 90, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 112, 114, 115, 116, 117, 119, 120, 121, 122, 124, 126, 127, 131, 140, 143, 161, 162, 163, 168, 172, 173, 174, 175, 178], "Will": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30], "With": [34, 83, 84, 115, 117, 176], "_": [9, 12, 14, 15, 17, 19, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 63, 65, 69, 70, 71, 73, 74, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 97, 98, 99, 100, 101, 105, 106, 111, 112, 114, 127, 130, 133, 140, 143, 144, 146, 157, 161, 162, 163, 168], "_0": [63, 65, 82, 86, 93, 105, 106, 143, 144, 157, 159, 160, 161, 162, 172], "_1": [17, 18, 19, 31, 35, 41, 68, 96, 97, 104, 127, 140, 144, 159, 160], "_2": [17, 18, 19, 31, 35, 41, 96], "_3": [17, 18, 19, 31, 35, 41], "_4": [17, 18, 19, 31, 35, 41], "_5": [31, 35], "__": [55, 56], "__init__": [92, 97, 101], "__version__": 174, "_a": [144, 146, 148], "_all_coef": 143, "_all_s": 143, "_b": [144, 146, 148], "_compute_scor": 21, "_compute_score_deriv": 21, "_coordinate_desc": 93, "_d": [79, 100, 127], "_est_causal_pars_and_s": 177, "_estimator_typ": 92, "_h": [100, 127], "_i": [63, 68, 82, 97, 99, 104, 106, 127, 133, 140, 144, 157], "_id": 143, "_j": [17, 18, 19, 31, 35, 41, 44, 65, 93, 161], "_l": [115, 124, 126], "_lower_quantil": [71, 74], "_m": [115, 124, 126, 143], "_mean": [71, 74], "_n": [144, 145, 146, 147, 148, 152, 161, 162, 168, 171], "_n_folds_per_clust": 93, "_offset": [115, 126], "_pred": [115, 126], "_rmse": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40], "_upper_quantil": [71, 74], "_x": 49, "_y": [100, 127], "a0": 92, "a09a": 67, "a09b": 67, "a1": 92, "a3d9": 67, "a4a147": 96, "a5e6": 67, "a5e7": 67, "a6ba": 67, "a79359d2da46": 67, "a840": 67, "a_": [68, 104], "a_0": [42, 46, 144, 155], "a_1": 46, "a_i": [45, 127, 140], "a_j": [127, 134], "ab": [64, 101, 173], "ab71": 67, "abadi": [10, 69], "abb0fd28": 67, "abdt": [75, 107, 112, 175], "abl": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 63, 77, 82, 95, 96, 115, 118, 162, 163, 172], "about": [14, 16, 66, 69, 70, 77, 79, 94, 101, 127, 173, 175, 178], "abov": [63, 66, 72, 77, 80, 82, 83, 84, 87, 88, 92, 94, 96, 97, 98, 99, 100, 103, 106, 114, 115, 125, 127, 131, 133, 140, 174], "absolut": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 77, 115, 116], "abstract": [21, 64, 65, 93, 144, 173, 177], "abus": [71, 74, 127, 130, 132, 133, 140], "acc": [23, 64], "acceler": 79, "accept": [17, 19, 108, 112, 114, 115, 119, 124], "access": [50, 51, 64, 66, 71, 72, 73, 74, 77, 78, 79, 87, 88, 89, 102, 110, 112, 115, 124, 162, 168, 178], "accompani": 173, "accord": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 63, 66, 68, 69, 72, 78, 80, 82, 85, 94, 99, 100, 103, 104, 115, 126, 127, 128, 133, 161, 162, 164, 165, 166, 167, 169, 170, 178], "accordingli": [68, 69, 77, 92, 94, 100, 104], "account": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 65, 66, 71, 74, 93, 94, 95, 102, 103, 127, 140, 162, 168, 172, 178], "accross": 91, "accumul": [66, 94, 95, 102], "accur": 79, "accuraci": [50, 55, 64, 127], "acemoglu": 176, "achiev": [52, 65, 79, 90, 93, 97, 98, 103, 127, 161], "acic_2024_post": 96, "acknowledg": [66, 67, 94], "acm": 176, "acov": 176, "across": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 66, 78, 79, 94, 96, 97, 127, 140, 178], "action": 177, "activ": [4, 5, 6, 7, 8, 9, 174, 177], "actual": [48, 71, 74, 78, 89, 103], "acycl": [68, 104, 178], "ad": [5, 6, 7, 8, 9, 10, 11, 21, 50, 51, 55, 56, 89, 97, 107, 112, 115, 117, 127, 161, 162, 163, 177], "adapt": [25, 91, 94, 177], "add": [64, 65, 68, 69, 70, 75, 78, 79, 80, 87, 88, 89, 96, 97, 99, 100, 102, 103, 104, 115, 118, 122, 127, 176, 177], "add_trac": 103, "addit": [12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 40, 44, 46, 47, 49, 53, 71, 72, 73, 74, 86, 91, 97, 103, 108, 109, 110, 111, 112, 115, 118, 127, 128, 140, 144, 153, 162, 169, 170, 172, 176, 177], "addition": [31, 41, 74, 78, 80, 85, 95, 97, 102, 115, 118, 127, 143, 161, 162, 168, 175], "additional_inform": 13, "additional_paramet": 13, "address": 103, "adel": 176, "adj": [100, 103], "adj_coef_bench": 103, "adj_est": 103, "adj_vanderweelearah": 103, "adjust": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 54, 57, 65, 70, 90, 91, 93, 95, 102, 103, 114, 127, 133, 140, 161, 162, 168, 176, 177, 178], "adjust_p": 57, "adopt": [7, 69, 109, 112, 127, 129, 133], "advanc": [92, 113, 143, 176], "advantag": [63, 64, 66, 80, 82, 94, 95, 106, 174], "advers": [162, 163], "adversari": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 102, 162, 168, 172], "ae": [63, 65, 66, 68], "ae56": 67, "ae89": 67, "aesthet": 63, "aeturrel": 47, "afd9e4": 96, "affect": [79, 80, 86, 127, 177, 178], "after": [57, 58, 64, 66, 67, 68, 69, 86, 94, 95, 97, 103, 104, 114, 115, 119, 122, 124, 127, 133, 162, 164, 168, 174, 178], "after_stat": 63, "afterward": 78, "ag": [66, 94, 95, 98, 102, 178], "again": [63, 64, 65, 66, 68, 69, 71, 74, 78, 80, 82, 89, 92, 93, 94, 97, 100, 101, 102, 103, 104, 106, 162, 164, 165], "against": [69, 77, 79, 89, 98, 115, 117], "agebra": 114, "agegt54": [67, 75, 107, 112, 175], "agelt35": [67, 75, 107, 112, 175], "agg": [64, 71, 74, 101], "agg_df": [71, 74], "agg_df_anticip": [71, 74], "agg_dict": [71, 74], "agg_dictionari": [71, 74], "agg_did_obj": [127, 128], "aggrag": [127, 128], "aggreg": [13, 16, 64, 105, 128, 143, 177], "aggregate_over_split": 48, "aggregated_eventstudi": [71, 72, 73, 74], "aggregated_framework": [71, 72, 73, 74, 127, 128], "aggregated_group": [71, 74], "aggregated_tim": [71, 73, 74], "aggregation_0": 13, "aggregation_1": 13, "aggregation_color_idx": 13, "aggregation_method_nam": 13, "aggregation_nam": 13, "aggregation_weight": [13, 71, 72, 73, 74], "aggt": 64, "ai": [73, 101, 176], "aim": 100, "aipw": 96, "aipw_est_1": 96, "aipw_est_2": 96, "aipw_obj_1": 96, "aipw_obj_2": 96, "air": [65, 93], "al": [10, 11, 32, 34, 42, 44, 46, 63, 65, 66, 67, 69, 77, 82, 83, 84, 85, 86, 87, 88, 91, 93, 94, 95, 99, 102, 106, 127, 129, 133, 143, 144, 150, 152, 153, 154, 155, 161, 162, 163, 172, 173, 175, 177], "albeit": 97, "alexandr": [86, 176], "algebra": 127, "algorithm": [62, 64, 65, 67, 68, 69, 71, 72, 73, 74, 77, 80, 82, 85, 90, 93, 95, 97, 99, 102, 104, 113, 115, 116, 125, 126, 127, 129, 130, 132, 143, 144, 161, 177, 178], "alia": [50, 51, 55, 56], "align": [42, 63, 65, 68, 70, 71, 74, 77, 82, 85, 91, 93, 94, 96, 97, 98, 99, 104, 127, 130, 132, 133, 140, 144, 146, 148, 177], "all": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 51, 55, 56, 59, 63, 64, 65, 66, 68, 69, 77, 78, 79, 80, 82, 89, 90, 91, 92, 93, 94, 95, 97, 98, 100, 103, 104, 106, 107, 109, 112, 114, 115, 116, 118, 124, 126, 127, 128, 129, 131, 133, 140, 143, 144, 146, 148, 161, 162, 172, 173, 174, 177], "all_coef": 143, "all_dml1_coef": 105, "all_s": 143, "all_smpl": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 90], "all_smpls_clust": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "all_z_col": [65, 93], "alloc": [97, 127, 140], "allow": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 50, 51, 55, 56, 66, 71, 74, 79, 80, 94, 95, 97, 100, 112, 114, 115, 116, 127, 140, 143, 144, 161, 173, 177, 178], "almqvist": 176, "along": [79, 97, 115, 121], "alongsid": [108, 110, 112], "alpha": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 42, 43, 46, 63, 65, 66, 68, 71, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 95, 97, 99, 105, 106, 114, 115, 116, 119, 120, 121, 123, 124, 126, 127, 140, 143, 144, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172], "alpha_": [44, 65, 93, 115, 121], "alpha_0": [162, 172], "alpha_i": [39, 45, 97, 127, 140], "alpha_ml_l": 86, "alpha_ml_m": 86, "alpha_x": [25, 33, 127], "alreadi": [18, 68, 69, 71, 74, 78, 101, 104, 115, 116, 127, 129], "also": [12, 14, 15, 16, 22, 23, 25, 26, 39, 40, 62, 63, 64, 65, 66, 67, 69, 71, 72, 74, 77, 78, 79, 80, 81, 82, 83, 84, 87, 88, 89, 92, 93, 94, 95, 97, 98, 100, 102, 103, 106, 114, 115, 116, 119, 121, 122, 123, 124, 125, 126, 127, 140, 143, 144, 161, 162, 163, 174, 175, 177, 178], "alter": [65, 93], "altern": [37, 64, 66, 67, 72, 94, 97, 98, 113, 115, 119, 124, 125, 126, 127, 140, 161, 173, 175], "although": 103, "alwai": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 51, 56, 64, 91, 100, 177], "always_tak": [25, 66, 94], "alyssa": 176, "amamb": 93, "american": [43, 96], "amgrem": 93, "amhorn": 93, "amit": [103, 176], "amjavl": 93, "ammata": 93, "among": [66, 86, 94, 95, 102, 103], "amount": [16, 66, 92, 94, 95, 178], "amp": [62, 65, 67, 68, 69, 71, 72, 73, 74, 93, 95, 97, 102, 104], "an": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 41, 42, 50, 51, 55, 56, 63, 64, 65, 66, 67, 68, 71, 72, 73, 74, 77, 78, 80, 82, 83, 84, 86, 89, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 111, 112, 114, 115, 117, 121, 122, 125, 126, 127, 128, 131, 133, 134, 143, 144, 161, 162, 163, 168, 173, 175, 176, 177, 178], "analog": [20, 21, 65, 71, 72, 74, 93, 95, 102, 114, 127, 129, 144, 145, 146, 147, 148, 161, 162, 168], "analys": [107, 112, 178], "analysi": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 49, 63, 65, 66, 82, 93, 94, 95, 101, 106, 113, 114, 127, 132, 163, 168, 172, 173, 177], "analyst": 101, "analyt": [96, 99], "analyz": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 66, 94, 95, 102, 178], "ancillari": 103, "andrea": 176, "angl": 66, "angrist": 96, "ani": [62, 63, 64, 67, 68, 69, 79, 81, 82, 97, 101, 103, 104, 106, 127, 174, 178], "anna": [12, 14, 15, 17, 18, 19, 31, 35, 41, 64, 69, 71, 72, 73, 74, 127, 128, 129, 131, 133, 176], "annal": [161, 176], "annalivia": 176, "anneal": [115, 126], "annot": 63, "annual": 176, "anoth": [63, 64, 65, 66, 77, 82, 92, 93, 97, 101, 106, 115, 126, 127, 135], "anticip": [14, 16, 17, 19, 64, 72, 73, 127, 128, 130, 131, 132, 133], "anticipation_period": [14, 16, 17, 19, 71, 74], "anymor": [65, 93], "aos1161": 161, "aos1230": 161, "aos1671": 161, "ap": [66, 94], "ape_e401_uncond": 66, "ape_p401_uncond": 66, "api": [79, 91, 107, 112, 173, 177], "apo": [22, 23, 78, 79, 134, 149, 169], "apo_result": 79, "apoorva": 177, "apoorva__l": 96, "apoorval": 96, "app": 177, "appeal": 103, "append": [77, 79, 82, 101, 106], "appendix": [34, 36, 68, 71, 74, 102, 104, 162, 163], "appli": [11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 57, 58, 62, 63, 65, 66, 67, 68, 70, 71, 73, 74, 77, 82, 90, 93, 94, 95, 97, 100, 101, 103, 104, 106, 123, 127, 143, 144, 161, 173, 175, 176, 177, 178], "applic": [42, 63, 69, 79, 82, 96, 103, 106, 108, 112, 114, 143, 176, 178], "applicatoin": 72, "apply_along_axi": 98, "apply_cross_fit": [63, 143], "apply_crossfit": 177, "appreci": 173, "approach": [7, 14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 39, 64, 65, 79, 80, 93, 100, 102, 103, 109, 112, 113, 115, 125, 127, 140, 143, 144, 157, 161, 162, 163, 174, 176, 178], "appropri": [66, 86, 94, 97, 127, 143, 178], "approx": [97, 114, 127, 130, 132, 140], "approxim": [39, 63, 74, 77, 82, 83, 84, 85, 97, 99, 103, 106, 114, 127, 140, 161, 177, 178], "april": 71, "apt": 174, "ar": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 37, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 80, 82, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 105, 106, 107, 108, 112, 114, 115, 116, 117, 119, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 151, 152, 156, 157, 158, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 177, 178], "arang": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 70, 82, 85, 91, 95, 98, 99, 102, 103, 107, 112, 115, 121], "arbitrarili": [51, 56], "architectur": [79, 144, 176], "arellano": 176, "arg": [92, 100, 114, 127], "argmax": 101, "argmin": 77, "argu": [63, 66, 82, 94, 95, 102, 106, 178], "argument": [12, 14, 15, 17, 18, 19, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 44, 46, 47, 48, 49, 53, 66, 69, 71, 72, 73, 74, 77, 78, 83, 84, 89, 93, 94, 95, 101, 105, 107, 114, 115, 116, 117, 120, 124, 126, 127, 128, 177, 178], "aris": [63, 64, 65, 72, 82, 93, 103, 106, 178], "aronow": 96, "around": [64, 66, 79, 94, 95, 100, 127, 144], "arr": 98, "arrai": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 50, 51, 53, 54, 55, 56, 57, 68, 69, 71, 72, 73, 74, 77, 78, 79, 80, 82, 83, 84, 85, 93, 96, 97, 98, 101, 102, 103, 104, 105, 106, 108, 110, 111, 114, 115, 116, 117, 120, 121, 143, 161, 162, 168, 175, 177, 178], "arrang": 65, "array_lik": 29, "articl": [47, 173], "arxiv": [44, 64, 65, 93, 101, 103, 173, 176, 177], "as_learn": [67, 115, 123, 125], "asarrai": [83, 84, 97], "aspect": [66, 94, 95, 97, 127, 140], "assert": [115, 124], "assess": [64, 97], "asset": [95, 102, 178], "assign": [4, 5, 6, 7, 8, 9, 17, 19, 57, 66, 71, 74, 78, 79, 91, 94, 100, 107, 112, 114, 115, 126, 127, 128, 142, 162, 178], "assmput": [127, 142], "associ": [66, 86, 94, 127, 161, 176], "assum": [39, 62, 65, 69, 81, 93, 96, 98, 101, 103, 127, 128, 129, 131, 133, 144, 145, 146, 147, 148, 161, 162, 172, 178], "assumpt": [64, 65, 66, 68, 69, 70, 71, 72, 74, 77, 93, 94, 96, 100, 104, 127, 128, 129, 131, 133, 140, 142, 161, 178], "assur": 177, "astyp": [71, 73, 81, 100, 103], "asymptot": [20, 21, 63, 65, 79, 82, 93, 106, 143, 161, 176], "ate": [79, 80], "ate_estim": [68, 104], "ates": [79, 80], "athei": 176, "att": [16, 17, 19, 26, 31, 64, 70, 89, 90, 98, 103, 114, 127, 128, 129, 130, 131, 132, 133, 137, 144, 152, 162, 170, 177], "att_": [127, 131], "att_gt": [64, 72, 73], "attach": 64, "atte_estim": 69, "attempt": [50, 51], "attenu": [66, 94], "attr": 66, "attribut": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 52, 53, 54, 55, 56, 57, 58, 77, 92, 105, 109, 112, 115, 116, 117, 143, 144, 161], "attributeerror": [50, 51], "attrict": 127, "attrit": [30, 68, 104, 127, 142], "au": [67, 115, 125, 173, 175], "auc": 64, "author": [64, 103, 173], "auto_ml": 92, "autodoubleml": 92, "autom": 92, "automat": [13, 16, 63, 71, 73, 74, 78, 82, 89, 106, 114, 162, 168], "automl": 177, "automl_l": 92, "automl_l_lesstim": 92, "automl_m": 92, "automl_m_lesstim": 92, "automobil": [65, 93], "autos": 86, "autosklearn": 92, "auxiliari": [37, 63, 82, 106], "avail": [33, 64, 66, 67, 69, 71, 72, 74, 77, 79, 80, 86, 94, 95, 96, 98, 100, 103, 106, 107, 112, 114, 115, 116, 123, 125, 127, 128, 129, 130, 132, 162, 172, 173, 174, 177, 178], "avaiv": 54, "aver": [78, 80], "averag": [17, 18, 19, 22, 23, 25, 26, 31, 40, 41, 52, 62, 64, 67, 68, 69, 70, 71, 73, 74, 78, 81, 89, 95, 96, 98, 100, 101, 102, 103, 104, 113, 128, 129, 133, 134, 135, 136, 137, 142, 149, 152, 161, 169, 170, 176, 177, 178], "average_it": [78, 79, 80], "avoid": [58, 63, 64, 82, 100, 127, 143, 174, 177], "awai": 102, "ax": [13, 16, 70, 71, 72, 73, 74, 77, 79, 80, 82, 83, 84, 85, 87, 88, 92, 93, 94, 95, 96, 99, 100], "ax1": [79, 80, 85, 90, 95, 99], "ax2": [79, 80, 85, 90, 95, 99], "axhlin": [70, 78, 79, 92, 97, 100], "axi": [13, 16, 65, 66, 77, 79, 80, 86, 90, 93, 94, 96, 98, 100], "axvlin": [79, 80, 82], "b": [12, 14, 15, 16, 17, 19, 45, 47, 63, 65, 67, 82, 83, 84, 93, 96, 99, 100, 103, 106, 114, 115, 125, 127, 133, 161, 162, 172, 173, 175, 176], "b208": 67, "b371": 67, "b5d34a6f42b": 67, "b5d7": 67, "b_": 127, "b_0": 46, "b_1": 46, "b_j": 47, "bach": [77, 86, 92, 103, 173, 176, 177], "back": 79, "backbon": 77, "backend": [5, 6, 7, 8, 9, 64, 95, 102, 103, 107, 109, 111, 113, 115, 120, 177], "backward": [4, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 107, 112, 177], "bad": 96, "balanc": [42, 66, 71, 72, 74, 91, 94, 95], "balanced_r0": 42, "band": [64, 113, 178], "bandwidth": [27, 28, 29, 48, 100, 127, 177], "bar": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 45, 89, 92, 94, 97, 114, 127, 140, 144, 149, 152, 157, 162, 169], "base": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 37, 38, 39, 40, 45, 49, 52, 54, 58, 63, 64, 65, 66, 68, 69, 70, 72, 73, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 110, 112, 114, 115, 118, 124, 125, 126, 127, 128, 131, 133, 140, 142, 143, 144, 161, 162, 163, 168, 173, 175, 176, 177, 178], "base_classifi": 78, "base_estim": [55, 56, 100], "base_regressor": 78, "baseestim": [97, 101], "baselin": [14, 35, 66, 92, 94], "basesampl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "basi": [22, 26, 40, 53, 74, 83, 84, 90, 114], "basic": [64, 65, 66, 69, 73, 93, 94, 95, 96, 100, 102, 103, 113, 115, 125], "basis_df": 90, "basis_matrix": 90, "batch": 67, "battocchi": 176, "batuhan": 177, "bay": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 161], "bayesian": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "bb2913dc": 67, "bbotk": [67, 115, 125, 126, 177], "bbox_inch": 82, "bbox_to_anchor": [82, 91], "bcallaway11": [64, 72], "bd929a9e": 67, "bde4": 67, "becam": [66, 94, 95], "becaus": [51, 56, 62, 63, 64, 65, 81, 82, 89, 93, 96, 97, 101, 103, 106, 178], "becker": [67, 115, 125], "becom": [65, 92, 93, 114, 143], "bee": 70, "been": [13, 16, 65, 66, 71, 73, 74, 92, 93, 94, 95, 102, 103, 114, 115, 122, 123, 127, 133, 173, 177], "befor": [17, 19, 57, 58, 64, 66, 70, 71, 73, 74, 78, 80, 89, 94, 97, 99, 103, 127, 129, 178], "begin": [33, 42, 43, 44, 63, 65, 66, 67, 68, 70, 71, 74, 77, 82, 85, 93, 94, 96, 97, 98, 99, 104, 105, 107, 112, 115, 124, 126, 127, 130, 132, 133, 140, 143, 144, 146, 148, 161, 175, 178], "behav": [71, 73, 74, 101], "behavior": [66, 96, 115, 117], "behind": [72, 127], "being": [17, 19, 20, 21, 35, 36, 49, 52, 55, 56, 65, 74, 93, 100, 103, 127, 132, 133, 143, 144, 150, 161, 162, 168, 173], "belloni": [34, 86, 161, 176], "below": [57, 58, 62, 66, 72, 81, 94, 96, 97, 101, 127, 174, 175], "bench_x1": 103, "bench_x2": 103, "benchmark": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 59, 79, 80, 89, 163, 177], "benchmark_dict": [59, 102], "benchmark_inc": 102, "benchmark_pira": 102, "benchmark_result": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40], "benchmark_twoearn": 102, "benchmarking_set": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 80, 89, 102, 103, 162, 163], "benchmarking_vari": 89, "benefit": [63, 66, 82, 94, 106], "bernoulli": [33, 42], "berri": [65, 93], "besid": 175, "best": [22, 26, 40, 51, 52, 53, 56, 71, 74, 78, 79, 83, 84, 87, 88, 92, 174], "best_estim": 52, "best_loss": 92, "best_param": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 52], "best_scor": 52, "beta": [30, 33, 34, 36, 43, 66, 68, 94, 98, 100, 104, 127, 144, 155], "beta_": [68, 104], "beta_0": [32, 68, 91, 98, 104, 114, 127, 138, 144, 155], "beta_a": [31, 41, 103], "beta_j": [33, 34, 36, 43], "better": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 64, 71, 74, 77, 79, 80, 103, 127, 133, 140], "between": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 58, 62, 68, 70, 71, 74, 79, 80, 81, 85, 86, 92, 96, 97, 99, 101, 102, 103, 104, 127, 135, 144, 145, 146, 147, 148, 152, 155, 156, 157, 158, 161, 162, 172, 175, 177], "betwen": [62, 81], "beyond": 176, "bia": [36, 62, 68, 81, 86, 100, 103, 104, 113, 127, 142, 143, 144, 159, 160, 162, 172, 176, 177], "bias": [62, 66, 71, 74, 81, 94, 95, 97, 102, 178], "bias_bench": 103, "bibtex": 173, "big": [86, 105, 143, 144, 145, 146, 153, 161, 162, 164, 165, 166, 167, 170, 171, 172], "bigg": [65, 93, 144, 151, 152, 162, 165, 170], "bilia": 11, "bilinski": 176, "bin": [63, 79, 80, 82, 174], "binari": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 32, 37, 39, 40, 42, 49, 62, 64, 66, 67, 69, 77, 81, 89, 90, 91, 94, 96, 98, 103, 114, 115, 122, 126, 129, 133, 136, 137, 138, 142, 162, 169, 170, 177, 178], "binary_outcom": 49, "binary_treat": [32, 83, 87, 89], "binary_unbalanc": 42, "bind": 177, "binder": [67, 115, 125, 173, 175, 177], "binomi": [81, 96, 98, 99, 101], "bischl": [67, 115, 125, 173, 175], "black": [63, 67, 75, 107, 112, 175], "blob": 64, "block": [97, 127, 140], "blog": 47, "blondel": [173, 175], "blp": [53, 65, 93], "blp_data": [65, 93], "blp_model": [87, 88], "blue": [63, 65, 68, 93], "blueprint": 78, "bodori": 176, "bond": [66, 94, 95], "bonferroni": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 161], "bonu": [11, 67, 107, 112, 175], "book": [67, 101, 103, 115, 122, 125, 126, 176], "bool": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 42, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 89, 100], "boolean": [36, 87, 88, 107, 112, 143], "boost": [62, 66, 69, 71, 74, 77, 78, 79, 81, 94], "boost_class": [66, 94], "boost_summari": 94, "boostrap": [85, 177], "bootstrap": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 53, 71, 72, 73, 74, 80, 83, 84, 85, 87, 88, 95, 99, 113, 114, 127, 128, 143, 144, 173, 175, 177, 178], "both": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 40, 41, 64, 66, 67, 69, 70, 77, 78, 79, 90, 91, 92, 94, 95, 97, 98, 100, 101, 102, 103, 107, 108, 112, 115, 117, 126, 127, 130, 131, 132, 140, 161, 162, 163, 168, 171, 172, 177, 178], "bottom": [65, 66, 77, 93, 94, 95], "bound": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 35, 37, 38, 39, 40, 57, 58, 66, 71, 74, 79, 80, 89, 90, 94, 102, 103, 162, 163, 168, 172, 177, 178], "branch": 67, "brantli": [64, 176], "break": [63, 177], "breve": [144, 155], "breviti": 178, "brew": 174, "brewer": 65, "bridg": 103, "brief": 106, "briefli": 72, "bring": [62, 81], "brucher": [173, 175], "bsd": [173, 177], "bst": 94, "budget": [92, 115, 126], "bug": [173, 177], "build": [65, 77, 93, 98], "build_design_matric": [83, 84], "build_sim_dataset": 64, "built": [54, 92, 115, 126, 173], "bureau": [103, 143, 176], "busi": [36, 44, 65, 93, 103, 176], "b\u00fchlmann": 176, "c": [10, 11, 17, 18, 19, 34, 41, 43, 46, 62, 63, 64, 65, 66, 67, 70, 75, 81, 82, 86, 87, 88, 93, 94, 96, 97, 100, 101, 106, 107, 112, 115, 125, 126, 127, 133, 144, 146, 148, 162, 165, 167, 173, 174, 175, 176, 178], "c1": [10, 11, 46, 65, 86, 93, 101, 106, 173, 176], "c68": [10, 11, 46, 65, 86, 93, 101, 106, 173, 176], "c895": 67, "c_": [16, 127, 130, 132, 133, 144, 146, 148, 161], "c_d": [34, 162, 170, 171, 172], "c_i": [45, 127, 133, 140, 144, 157], "c_y": [34, 162, 172], "ca1af7be64b2": 67, "caac5a95": 67, "calcualt": 98, "calcul": [22, 26, 40, 64, 66, 71, 74, 77, 79, 80, 83, 84, 85, 87, 88, 92, 94, 97, 99, 102, 127, 140, 162, 168, 172], "calendar": [71, 72, 73, 74], "calibr": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 57, 58, 77, 92, 103], "calibration_method": [57, 58], "call": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 51, 55, 56, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 78, 79, 81, 83, 84, 85, 87, 88, 94, 95, 98, 99, 100, 102, 103, 104, 107, 112, 115, 117, 126, 143, 144, 161, 162, 168, 172, 175, 177, 178], "callabl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 52, 63, 77, 78, 82, 83, 84, 113, 115, 116, 173], "callawai": [17, 19, 64, 71, 72, 73, 74, 127, 128, 131, 133, 176], "callback": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "camera": 86, "cameron": [65, 93], "can": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 40, 48, 51, 54, 56, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 112, 114, 115, 116, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 131, 133, 134, 140, 142, 143, 144, 145, 146, 147, 148, 149, 152, 155, 156, 157, 158, 161, 162, 163, 168, 169, 170, 171, 172, 173, 174, 175, 177, 178], "candid": 103, "cannot": [77, 100, 103, 127, 178], "capabl": [5, 6, 7, 8, 9, 62, 81], "capo": [22, 90], "capo0": 90, "capo1": 90, "capsiz": [78, 79, 80, 92, 96, 97, 100], "capthick": [78, 79, 80, 97, 100], "cardin": [65, 93], "care": [72, 101, 115, 117], "carlo": [31, 32, 35, 41, 83, 84, 87, 88, 103, 176], "casalicchio": [67, 115, 125, 173, 175], "case": [4, 5, 6, 7, 8, 9, 11, 14, 22, 23, 25, 26, 32, 39, 48, 62, 65, 66, 71, 72, 74, 81, 83, 84, 85, 86, 89, 90, 91, 92, 93, 97, 98, 99, 100, 101, 102, 103, 107, 112, 114, 115, 117, 118, 119, 121, 122, 124, 127, 129, 133, 140, 142, 143, 144, 146, 148, 161, 162, 168, 175, 177, 178], "cat": [63, 177], "catboost": 77, "cate": [26, 40, 53, 90, 113, 177], "cate_obj": 114, "categor": [17, 19], "categori": 79, "cattaneo": [127, 176], "caus": [63, 82, 100, 106], "causal": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 62, 63, 65, 66, 67, 68, 77, 81, 82, 90, 92, 93, 94, 96, 97, 101, 102, 104, 105, 106, 107, 110, 112, 113, 127, 133, 143, 161, 162, 168, 176], "causal_contrast": [23, 78, 79, 80, 90, 127], "causal_contrast_att": 90, "causal_contrast_c": 90, "causal_contrast_model": [79, 80, 127], "causal_contrast_result": 79, "causaldml": 176, "causalml": [101, 176], "causalweight": 176, "caution": 161, "caveat": 103, "cbind": 65, "cbook": [71, 72, 73, 74], "cc": 94, "ccp_alpha": [26, 54, 94], "cd": 174, "cd_fast": 93, "cda85647": 67, "cdf": 114, "cdid": [65, 93], "cdot": [17, 18, 19, 31, 35, 41, 42, 45, 49, 65, 70, 71, 74, 85, 89, 93, 96, 99, 100, 103, 114, 127, 128, 130, 132, 133, 143, 144, 146, 148, 149, 152, 153, 155, 159, 160, 161, 162, 165, 167, 169], "cdot1": 89, "cell": [78, 92, 97], "center": [71, 72, 74, 79, 86, 91], "central": [143, 177], "certain": [127, 144, 146, 148], "cexcol": 65, "cexrow": 65, "cf_d": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 41, 59, 71, 74, 80, 89, 90, 102, 103, 162, 163, 168, 169, 170, 171, 172, 178], "cf_y": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 41, 59, 71, 74, 80, 89, 90, 102, 103, 162, 163, 168, 169, 170, 171, 172, 178], "cff": 177, "chad": 103, "challeng": [65, 93, 162, 163], "chanc": 74, "chang": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 50, 51, 55, 56, 66, 68, 69, 73, 91, 95, 102, 103, 104, 127, 132, 133, 144, 148, 152, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 174, 176, 177], "channel": 178, "chapter": [20, 21, 67, 101, 115, 122, 123, 125, 126, 162, 172], "charact": [66, 67, 115, 123, 124, 126, 177], "characterist": [102, 178], "chart": 92, "check": [50, 51, 55, 56, 63, 66, 69, 70, 71, 74, 77, 82, 92, 94, 95, 101, 105, 106, 127, 128, 173, 174, 177], "check_data": 177, "check_scor": 177, "checkmat": 177, "chernozhukov": [10, 11, 34, 43, 46, 63, 65, 66, 77, 82, 86, 92, 93, 94, 95, 101, 102, 106, 143, 144, 152, 161, 162, 163, 172, 173, 176, 177], "chetverikov": [10, 11, 46, 65, 86, 93, 101, 106, 161, 173, 176], "chiang": [44, 65, 93, 176], "chieh": 176, "choic": [5, 6, 7, 8, 9, 14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 66, 71, 72, 73, 74, 78, 86, 94, 98, 114, 115, 116, 124, 126, 127, 131, 144, 146, 148, 162, 163, 168, 172, 177], "cholecyst": 101, "choos": [62, 66, 72, 77, 78, 81, 82, 86, 94, 95, 105, 127, 131, 143, 144, 145, 146, 147, 148, 152, 155, 156, 157, 158, 161, 175, 178], "chosen": [22, 35, 41, 77, 78, 115, 126, 127], "chou": 96, "chr": 66, "christian": [86, 176], "christoph": 176, "chunk": [115, 126], "ci": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 69, 70, 71, 72, 73, 74, 78, 79, 80, 83, 84, 85, 87, 88, 89, 90, 92, 94, 95, 99, 100, 102, 103, 114, 127, 162, 168, 177, 178], "ci_at": [79, 80], "ci_bound": 16, "ci_cre_gener": 97, "ci_cre_norm": 97, "ci_cvar": [85, 95], "ci_cvar_0": 85, "ci_cvar_1": 85, "ci_fd": 97, "ci_joint": [71, 72, 73, 74, 80], "ci_joint_cvar": 85, "ci_joint_lqt": 99, "ci_joint_qt": 99, "ci_length": 69, "ci_low": [78, 79, 80, 97], "ci_lpq_0": 99, "ci_lpq_1": 99, "ci_lqt": [95, 99], "ci_pointwis": [79, 80], "ci_pq_0": [95, 99], "ci_pq_1": [95, 99], "ci_qt": [95, 99], "ci_tun": 78, "ci_tuned_pipelin": 78, "ci_untun": 78, "ci_untuned_pipelin": 78, "ci_upp": [78, 79, 80, 97], "ci_wg": 97, "cinelli": [103, 162, 163, 176], "circumv": 178, "citat": 177, "cite": 173, "cl": 70, "claim": 67, "clarifi": [71, 72, 73, 74], "clark": [39, 45, 97, 127, 140, 176], "clash": 64, "class": [0, 4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 66, 67, 68, 69, 71, 72, 73, 74, 75, 78, 79, 80, 89, 90, 91, 92, 94, 95, 97, 101, 102, 104, 105, 107, 108, 109, 110, 111, 112, 114, 115, 117, 118, 119, 122, 124, 126, 127, 128, 143, 144, 161, 173, 175, 177], "class_estim": 100, "class_learn": 95, "class_learner_1": 77, "class_learner_2": 77, "classes_": [92, 101], "classic": [64, 65, 72, 93, 178], "classif": [26, 50, 55, 62, 64, 66, 67, 68, 69, 71, 72, 73, 74, 77, 92, 95, 98, 102, 104, 114, 115, 116, 122, 126, 127, 129, 130, 132, 178], "classifavg": 67, "classifi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 39, 40, 48, 50, 55, 67, 79, 80, 91, 92, 100, 101, 115, 118, 122, 177], "classifiermixin": 101, "classmethod": [4, 5, 6, 7, 8, 9, 57], "claudia": [176, 177], "claus": [173, 177], "clean": 177, "cleaner": 77, "cleanup": 177, "clear": [65, 79, 93], "clearli": [71, 74, 100], "clever": 77, "client": 79, "clip": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 57, 58], "clipping_threshold": [12, 15, 57, 58, 83], "clone": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 62, 63, 67, 77, 82, 93, 95, 97, 105, 115, 118, 124, 126, 127, 143, 144, 161, 162, 168, 174, 175], "close": [58, 64, 66, 94, 97, 101, 103, 162, 163], "cluster": [5, 6, 7, 8, 9, 12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 44, 73, 97, 107, 108, 110, 111, 112, 127, 140, 176, 177], "cluster_col": [4, 5, 6, 8, 9, 65, 93, 107, 108, 110, 111, 112], "cluster_var": [4, 5, 6, 7, 8, 9, 44, 107, 108, 112], "cluster_var_i": [65, 93], "cluster_var_j": [65, 93], "cmap": 93, "cmd": 177, "co": [42, 45, 47, 70], "codaci": 177, "code": [22, 26, 40, 47, 62, 64, 65, 66, 67, 68, 81, 86, 94, 106, 114, 115, 123, 124, 125, 126, 127, 143, 144, 161, 174, 175, 177, 178], "codecov": 177, "coef": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 102, 103, 104, 105, 106, 114, 115, 116, 117, 119, 120, 121, 127, 128, 129, 130, 132, 143, 144, 161, 175, 178], "coef_": 103, "coef_df": 65, "coef_valu": 92, "coeffici": [16, 31, 32, 41, 51, 53, 56, 66, 68, 77, 78, 87, 88, 91, 94, 96, 97, 98, 100, 103, 104, 114, 161, 162, 168, 178], "coefs_t": 98, "coefs_w": 98, "coffici": [162, 168], "cofid": 53, "coincid": [70, 90, 95], "col": [63, 65, 94], "col_nam": [71, 74], "collect": [67, 68, 69, 78, 79, 93, 104], "colnam": [65, 77], "color": [13, 16, 66, 68, 70, 74, 78, 79, 80, 82, 83, 84, 85, 92, 93, 94, 95, 96, 97, 99, 100, 103], "color_palett": [13, 16, 71, 74, 78, 79, 80, 82, 93, 94, 95, 97], "colorbar": 93, "colorblind": [13, 16, 71, 74, 78, 79, 80, 97], "colorramppalett": 65, "colorscal": [83, 84], "colour": [63, 65], "column": [5, 6, 7, 8, 9, 16, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 100, 102, 103, 104, 107, 108, 109, 110, 111, 112, 114, 115, 117, 127, 130, 132, 140, 143, 175, 177, 178], "column_stack": [8, 70, 78, 79, 80, 87, 88, 100, 102, 103, 127], "columntransform": 97, "columntransformercolumntransform": 97, "columntransformerifittedcolumntransform": 97, "colv": 65, "com": [47, 64, 66, 67, 72, 73, 86, 96, 103, 115, 125, 173, 174], "comb": 86, "combin": [14, 16, 64, 65, 67, 69, 72, 73, 77, 78, 79, 80, 90, 92, 93, 97, 103, 115, 123, 127, 131, 143, 162, 168, 177], "combind": 95, "combined_loss": 86, "come": [105, 115, 118, 144, 162, 163, 173, 178], "command": [174, 177], "comment": [107, 112], "commit": 177, "common": [77, 97, 102, 103, 114, 127, 176], "commonli": 79, "companion": 176, "compar": [63, 65, 70, 71, 74, 78, 79, 82, 83, 84, 85, 87, 88, 90, 91, 96, 97, 99, 100, 101, 103, 106, 115, 116, 127, 162, 163, 177], "comparevers": 66, "comparison": [71, 74, 77, 80, 96, 97], "comparison_data": [78, 97], "compat": [4, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 62, 64, 81, 107, 112, 177], "complement": 103, "complet": [52, 78, 92, 106, 162, 168, 174], "complex": [26, 64, 78, 92], "compli": [100, 127], "complianc": [99, 100, 127, 144, 153], "complic": [67, 78, 97, 178], "complier": [66, 94, 95, 99, 100, 114, 127], "compon": [17, 19, 55, 56, 64, 66, 72, 77, 79, 86, 92, 94, 98, 114, 115, 126, 143, 144, 145, 146, 147, 148, 149, 151, 152, 153, 156, 157, 158, 177], "compont": 64, "compos": 97, "composit": 176, "comprehens": 79, "compris": 161, "comput": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 53, 59, 63, 64, 66, 67, 71, 72, 73, 74, 79, 82, 94, 95, 101, 102, 103, 143, 144, 155, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 176, 177, 178], "computation": [162, 163], "concat": [79, 92, 93, 94, 98, 161], "concaten": [70, 78, 94, 97, 161], "concentr": 161, "concern": 103, "conclud": [100, 103, 178], "cond": [127, 129, 142], "conda": [176, 177], "condit": [20, 21, 22, 24, 26, 31, 32, 39, 40, 41, 63, 65, 66, 68, 69, 70, 71, 74, 80, 82, 89, 90, 93, 94, 97, 98, 100, 103, 104, 106, 113, 127, 130, 131, 132, 133, 140, 161, 162, 169, 170, 172, 175, 176, 177, 178], "conduct": [114, 127, 129, 130, 132, 178], "conf": [64, 99], "confer": 176, "confid": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 53, 64, 65, 66, 68, 69, 71, 72, 73, 74, 78, 79, 80, 83, 84, 85, 87, 88, 90, 93, 95, 99, 100, 102, 104, 113, 114, 127, 143, 144, 162, 168, 175, 176, 177, 178], "confidenceband": 85, "confidenti": 103, "config": [57, 58, 96], "configur": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 58, 67, 92], "confint": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 53, 66, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 83, 84, 85, 87, 88, 90, 95, 97, 98, 99, 100, 101, 102, 104, 114, 143, 161, 173, 175, 178], "conflict": 174, "confound": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 37, 38, 39, 40, 41, 45, 59, 62, 66, 71, 72, 74, 81, 89, 91, 94, 99, 102, 103, 107, 112, 127, 138, 139, 140, 141, 161, 162, 163, 168, 171, 172, 175, 176, 177, 178], "congress": 176, "connect": [66, 94, 95], "consequ": [31, 41, 65, 78, 89, 93, 102, 114, 127, 129, 162, 163, 169, 170, 172], "conserv": [102, 103, 162, 172], "consid": [24, 25, 26, 27, 28, 45, 49, 57, 58, 63, 65, 66, 68, 69, 70, 71, 72, 73, 74, 77, 82, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 105, 106, 114, 115, 119, 124, 127, 128, 133, 136, 137, 140, 143, 144, 161, 162, 163, 173, 178], "consider": [103, 127], "consist": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 51, 56, 66, 69, 91, 92, 94, 95, 96, 103, 106, 107, 112, 127, 133, 138, 139, 140, 141, 175, 177], "consol": [63, 177], "constant": [17, 19, 34, 51, 56, 71, 74, 79, 86, 91, 98, 114, 127, 161], "constrained_layout": 82, "construct": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 53, 67, 70, 71, 74, 78, 83, 84, 85, 90, 95, 101, 102, 105, 114, 122, 123, 144, 150, 160, 161, 177, 178], "construct_framework": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40], "construct_iv": 93, "constructiv": 65, "constructor": [67, 108, 112], "consum": [65, 93], "cont": 35, "cont_d": [78, 79, 80], "contain": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 40, 45, 49, 50, 51, 52, 55, 56, 63, 65, 66, 71, 72, 74, 77, 78, 80, 82, 83, 84, 87, 88, 93, 94, 101, 106, 108, 109, 111, 112, 114, 115, 126, 127, 128, 130, 132, 161, 162, 163, 168, 177], "context": [103, 127, 142, 178], "contin": [35, 92], "continu": [35, 37, 42, 62, 67, 78, 79, 80, 81, 86, 91, 96, 100, 127, 162, 172, 177, 178], "contour": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 86, 89, 102, 103, 162, 168], "contour_plot": 103, "contours_z": [83, 84], "contrast": [23, 69, 79, 85, 90, 127, 135], "contribut": [174, 177], "contributor": 177, "control": [14, 16, 17, 19, 43, 49, 57, 64, 72, 73, 78, 86, 90, 95, 98, 100, 101, 103, 107, 108, 112, 127, 128, 130, 131, 132, 133, 144, 146, 148, 162, 165, 167, 178], "control_group": [14, 16, 71, 72, 73, 74, 127, 128, 130, 132], "conveni": [107, 110, 112], "convent": [66, 72, 94, 95, 100, 127, 133, 140], "converg": [37, 63, 77, 82, 93, 106], "convergencewarn": 93, "convers": 93, "convert": [17, 19, 79, 85, 93, 99, 127, 140], "convex": 96, "cooper": 177, "coor": [67, 115, 125, 173, 175], "coordin": [79, 103], "copi": [74, 78, 92, 94, 98, 103], "cor": [162, 172], "core": [17, 19, 71, 72, 73, 74, 75, 78, 79, 80, 85, 89, 91, 93, 94, 95, 99, 102, 107, 109, 112, 115, 122, 175, 177], "cores_us": [85, 95, 99], "correct": [89, 90, 91, 103, 114, 161, 177], "correctli": [50, 55, 69, 78, 96, 102, 162, 172], "correl": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 65, 68, 86, 93, 101, 102, 104, 127, 140, 144, 157, 162, 163, 172], "correpond": [71, 74, 127], "correspond": [7, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 41, 63, 65, 66, 67, 69, 70, 71, 72, 73, 74, 77, 78, 80, 82, 83, 84, 86, 90, 93, 94, 95, 97, 98, 99, 102, 103, 106, 109, 112, 114, 115, 116, 117, 124, 126, 127, 129, 131, 133, 134, 140, 142, 143, 144, 146, 148, 161, 162, 163, 165, 167, 168, 170, 172, 177, 178], "correspondingli": [71, 74], "cosh": 47, "coul": 65, "could": [62, 67, 71, 72, 74, 78, 81, 83, 84, 92, 103, 177, 178], "counfound": [31, 41, 99, 102, 114, 162, 172], "count": [78, 79, 80, 94, 95], "counti": 72, "countour": [162, 168], "countyr": 72, "coupl": [66, 94, 95], "cournapeau": [173, 175], "cours": [66, 77, 94, 103, 161, 178], "cov": [30, 31, 49, 100], "cov_nam": [100, 127], "cov_typ": [22, 26, 40, 53, 177], "covari": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 19, 26, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 53, 54, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 75, 78, 79, 80, 82, 83, 84, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 127, 129, 130, 132, 138, 139, 140, 141, 142, 144, 145, 146, 147, 148, 161, 162, 163, 175, 176, 177], "cover": [64, 86, 102, 111, 112], "coverag": [71, 74, 77, 100, 101, 114, 177], "cp": [66, 67, 115, 123], "cpu": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 79], "cpu_count": [85, 95, 99], "cran": [67, 176, 177], "cre": [39, 97, 127, 140, 144, 157], "cre_gener": [39, 127, 140, 144, 157], "cre_norm": [39, 127, 140, 144, 157], "creat": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 40, 57, 62, 65, 67, 71, 74, 78, 80, 81, 82, 83, 84, 85, 87, 88, 91, 93, 95, 97, 98, 99, 103, 107, 112, 115, 123, 162, 163, 168, 172, 174, 177], "create_default_for_vers": 79, "create_synthetic_group_data": 98, "creation": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "crictial": 143, "critic": [103, 178], "cross": [12, 14, 15, 16, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 52, 57, 58, 62, 63, 64, 66, 67, 68, 72, 77, 79, 82, 92, 94, 95, 97, 100, 103, 106, 108, 112, 113, 115, 116, 117, 126, 130, 131, 133, 147, 160, 161, 164, 165, 168, 177, 178], "cross_sectional_data": [15, 18, 69, 127, 129], "crossfit": [77, 127], "crosstab": 96, "crucial": [86, 127, 178], "csail": [173, 175], "csdid": 73, "csv": [73, 86], "cuda": 79, "cumul": 127, "current": [54, 58, 69, 70, 71, 74, 90, 144, 162, 172, 173, 174, 178], "custom": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 63, 64, 72, 78, 82, 103, 115, 116], "custom_measur": 64, "cut": 98, "cutoff": [48, 49, 100, 127], "cv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 57, 67, 94, 97, 115, 125, 126, 143], "cv_calibr": [57, 58], "cv_glmnet": [65, 66, 67, 68, 115, 123, 126, 127, 161, 175], "cvar": [24, 29, 113, 150, 177], "cvar_0": 85, "cvar_1": 85, "d": [4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 53, 54, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 132, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 147, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 161, 162, 163, 164, 166, 168, 169, 170, 171, 172, 173, 175, 176, 178], "d0": [85, 99, 161], "d0_true": 99, "d0cdb0ea4795": 67, "d1": [85, 96, 99, 161], "d10": 161, "d1_true": 99, "d2": [96, 161], "d21ee5775b5f": 67, "d2cml": 73, "d3": 161, "d4": 161, "d5": 161, "d5a0c70f1d98": 67, "d6": 161, "d7": 161, "d8": 161, "d9": 161, "d_": [35, 39, 44, 45, 65, 70, 80, 93, 97, 127, 129, 133, 140, 144, 157, 161], "d_0": [127, 134], "d_1": [96, 161], "d_2": 96, "d_col": [4, 5, 6, 7, 8, 9, 16, 62, 63, 65, 66, 67, 68, 71, 72, 73, 74, 81, 83, 84, 87, 88, 90, 93, 94, 95, 97, 98, 100, 101, 102, 105, 106, 107, 108, 109, 111, 112, 115, 123, 124, 127, 128, 130, 132, 143, 144, 175, 177, 178], "d_deman": 97, "d_demean": 97, "d_diff": [39, 97, 127], "d_i": [32, 33, 34, 36, 42, 43, 46, 47, 63, 68, 69, 80, 82, 85, 96, 97, 99, 100, 104, 106, 127, 129, 140, 142], "d_it": [127, 140], "d_j": [80, 127, 134, 135, 161], "d_k": [127, 135, 161], "d_l": [127, 134], "d_mean": 97, "d_w": 98, "da1440": 96, "dag": [68, 103, 104, 178], "dai": [71, 78], "dark": [63, 82], "darkblu": 65, "darkr": 65, "dash": [68, 71, 74, 79, 80], "dat": [107, 112], "data": [0, 10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 53, 55, 56, 64, 70, 77, 86, 96, 101, 105, 107, 109, 110, 111, 113, 114, 115, 116, 117, 119, 120, 121, 123, 124, 125, 126, 128, 130, 131, 132, 133, 140, 143, 161, 165, 166, 167, 168, 176, 177], "data_apo": [78, 79, 80], "data_cvar": 95, "data_dgp3": 97, "data_dict": [48, 83, 84, 87, 88, 89, 100, 127], "data_dml": 102, "data_dml_bas": [66, 83, 84, 87, 88, 94, 95, 98], "data_dml_base_iv": [66, 94, 95], "data_dml_flex": [66, 94], "data_dml_flex_iv": 66, "data_dml_iv_flex": 94, "data_dml_new": 98, "data_fram": 178, "data_lqt": 95, "data_obj": 97, "data_origin": 97, "data_pq": 95, "data_qt": 95, "data_transf": [65, 93, 94], "data_transform": 97, "data_tun": 97, "dataclass": [52, 58], "datafram": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 53, 54, 65, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 108, 109, 110, 111, 114, 115, 116, 117, 119, 127, 129, 144, 161, 162, 163, 168, 175, 178], "dataset": [0, 5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 52, 63, 64, 68, 69, 71, 74, 77, 78, 79, 80, 82, 83, 84, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 119, 127, 128, 129, 130, 132, 143, 144, 161, 162, 168, 175, 177, 178], "datatyp": [73, 177], "date": [16, 17, 19, 115, 123], "date_format": 16, "datetim": [7, 16, 17, 19, 45, 71, 72, 74, 97, 109, 112], "datetime64": [71, 109, 112], "datetime_complet": 78, "datetime_start": 78, "datetime_unit": [7, 16, 71, 74, 109, 112, 127, 128, 130, 132], "david": 177, "db": [66, 94, 95, 102, 178], "dbl": [64, 65, 66, 67, 107, 112, 161, 175, 178], "dc13a11076b3": 67, "ddc9": 67, "de": [62, 81, 176], "deal": [62, 81], "debias": [10, 11, 42, 44, 46, 65, 86, 93, 101, 113, 115, 143, 173, 176, 177], "debt": [66, 94, 95], "decai": [68, 104], "decid": [66, 94, 101], "decis": [26, 62, 66, 81, 94, 95, 114, 127, 176, 178], "decision_effect": 62, "decision_impact": [62, 81], "decisiontreeclassifi": [26, 54, 94], "decisiontreeregressor": 94, "declar": 178, "decomposit": [127, 128], "decreas": 100, "dedic": [107, 111, 112], "deep": [50, 51, 55, 56, 92], "deeper": 26, "def": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 71, 74, 77, 78, 82, 85, 92, 93, 96, 97, 98, 99, 101, 103, 115, 116, 120, 144], "default": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 64, 65, 68, 69, 71, 72, 73, 74, 77, 78, 87, 88, 93, 98, 100, 102, 103, 104, 105, 108, 109, 112, 114, 115, 116, 119, 124, 126, 127, 143, 161, 162, 168, 169, 175, 178], "default_arg": [71, 74], "default_convert": 93, "default_jitt": 16, "defier": [100, 127], "defin": [7, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 56, 63, 66, 67, 69, 72, 77, 79, 80, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 94, 95, 97, 98, 99, 100, 101, 102, 103, 114, 115, 116, 120, 123, 125, 127, 129, 130, 132, 133, 140, 142, 144, 145, 147, 148, 162, 163, 168, 172, 177], "definit": [47, 71, 74, 78, 87, 88, 90, 127, 131, 162, 169, 170], "degre": [49, 66, 83, 84, 90, 93, 94, 97, 100, 114, 162, 163], "dekel": 176, "delete_origin": 67, "deliber": 96, "delta": [16, 43, 64, 69, 71, 74, 97, 103, 127, 129, 130, 131, 132, 133, 140, 144, 146, 148], "delta_bench": 103, "delta_i": 64, "delta_j": 43, "delta_t": [17, 19, 71, 74], "delta_theta": [59, 80, 89, 102, 103, 162, 163], "delta_v": 103, "demand": [65, 93, 162, 163], "demir": [10, 11, 46, 65, 86, 93, 101, 106, 143, 173, 176], "demo": [72, 103], "demonstr": [63, 64, 65, 72, 79, 82, 93, 100, 103, 107, 112, 127, 161, 173, 175], "deni": 176, "denomin": [162, 163, 169, 170], "denot": [19, 38, 65, 66, 68, 69, 70, 74, 93, 94, 100, 103, 104, 114, 127, 129, 130, 132, 133, 134, 139, 140, 144, 162, 163, 168, 170, 172], "dens_net_tfa": 66, "densiti": [27, 28, 29, 63, 68, 79, 80, 82], "dep": 75, "dep1": [67, 75, 107, 112, 175], "dep2": [67, 75, 107, 112, 175], "depend": [17, 19, 22, 23, 24, 26, 27, 29, 32, 42, 67, 69, 71, 74, 77, 83, 84, 87, 88, 89, 91, 92, 97, 98, 100, 105, 114, 115, 124, 127, 130, 131, 132, 144, 153, 154, 162, 163, 169, 172, 175, 176], "deprec": [4, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 69, 70, 71, 72, 73, 74, 83, 105, 107, 112, 115, 127, 143, 144, 162], "deprecationwarn": [70, 83], "depreci": 177, "depth": [26, 54, 66, 67, 98, 105, 114, 115, 126, 127, 143, 144, 161, 175, 178], "deriv": [12, 14, 15, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 127, 161], "describ": [13, 64, 65, 71, 74, 93, 94, 95, 97, 103, 115, 127, 140, 143, 144, 155, 174, 177], "descript": [19, 66, 73, 75, 78, 102, 115, 120, 124, 126, 143, 144, 146, 148, 162, 163, 165, 167], "deserv": 127, "design": [8, 17, 19, 42, 45, 48, 49, 78, 79, 80, 92, 110, 112, 113, 176, 177], "design_info": [83, 84], "design_matrix": [83, 84, 114], "desir": [41, 67, 98, 127, 174], "detail": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 63, 66, 67, 69, 70, 79, 80, 82, 86, 92, 95, 97, 100, 102, 103, 106, 107, 112, 114, 115, 116, 120, 123, 125, 128, 129, 130, 131, 132, 133, 140, 142, 144, 150, 152, 153, 154, 159, 160, 161, 162, 163, 165, 167, 172, 173, 174, 175, 177, 178], "determin": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 41, 42, 51, 56, 66, 74, 85, 94, 95, 99, 100, 102, 127, 161, 162, 172], "determinist": [98, 100, 114, 127], "deutsch": 173, "dev": [174, 177], "develop": [64, 65, 67, 93, 103, 127, 129, 133, 177], "deviat": [77, 127, 162, 172], "devic": [79, 127, 140], "dezeur": 176, "df": [5, 6, 7, 8, 9, 16, 62, 63, 65, 68, 70, 71, 74, 78, 79, 80, 81, 83, 84, 85, 90, 93, 96, 99, 100, 102, 103, 104, 106, 108, 109, 111, 112, 114, 127, 128, 130, 132], "df_agg": 86, "df_all_apo": 79, "df_all_at": 79, "df_anticip": [71, 74], "df_apo": [78, 79, 80], "df_apo_ci": 80, "df_apo_plot": 78, "df_apos_ci": 80, "df_ate": [79, 80], "df_bench": 103, "df_binari": 103, "df_bonu": [67, 107, 112, 175], "df_capo0": 90, "df_capo1": 90, "df_cate": [83, 84, 90], "df_causal_contrast_c": 90, "df_ci": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 53], "df_coef": 77, "df_comparison": [78, 97], "df_cvar": 95, "df_fuzzi": 100, "df_lqte": 95, "df_ml_g0": 77, "df_ml_g1": 77, "df_ml_m": 77, "df_pa": [69, 104], "df_perform": 79, "df_plot": 65, "df_post_treat": [71, 74], "df_pq": 95, "df_qte": 95, "df_result": 86, "df_sharp": 100, "df_sort": [79, 80], "df_summari": 94, "df_treat": 74, "df_wide": 93, "dfg": 173, "dgp": [17, 18, 19, 45, 65, 68, 70, 71, 74, 85, 86, 93, 96, 98, 99, 103, 104], "dgp1": [17, 18, 19, 39, 45, 97, 127], "dgp2": [17, 18, 19, 45], "dgp3": [17, 18, 19, 45, 97], "dgp4": [17, 18, 19], "dgp5": [17, 18, 19], "dgp6": [17, 18, 19], "dgp_confounded_irm_data": 103, "dgp_dict": 103, "dgp_tpye": 69, "dgp_type": [17, 18, 19, 39, 45, 69, 71, 74, 97, 127], "diagnost": 72, "diagon": 103, "diagram": [62, 81, 127], "dichotom": [62, 81], "dict": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 50, 51, 52, 53, 54, 55, 56, 59, 71, 74, 83, 84, 86, 92, 103, 115, 119], "dict_kei": [78, 162, 168], "dict_rdd": [110, 112], "dictionari": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 41, 49, 59, 71, 74, 78, 83, 84, 87, 88, 102, 114, 115, 117, 120, 127, 128, 162, 168], "dictonari": [66, 94], "did": [0, 5, 7, 63, 69, 70, 71, 72, 73, 74, 78, 79, 93, 108, 109, 112, 113, 128, 129, 130, 132, 133, 144, 146, 148, 177, 178], "did_aggreg": [71, 73, 74], "did_data": 70, "did_multi": [127, 128], "diff": 94, "differ": [5, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 37, 38, 39, 40, 41, 62, 63, 65, 66, 67, 68, 71, 73, 74, 78, 79, 80, 81, 82, 85, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 99, 100, 102, 103, 104, 105, 108, 112, 113, 114, 115, 116, 119, 123, 124, 128, 129, 130, 132, 133, 140, 143, 145, 146, 147, 148, 174, 175, 176, 177, 178], "differenti": 127, "difficult": 103, "dillon": 176, "dim": [49, 66], "dim_x": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 49, 63, 65, 67, 77, 82, 90, 91, 92, 93, 97, 106, 109, 112, 114, 115, 117, 127, 162, 168], "dim_z": [38, 43, 127], "dimens": [17, 19, 32, 44, 65, 93, 98, 143], "dimension": [13, 32, 34, 37, 38, 39, 40, 86, 101, 114, 127, 138, 139, 140, 141, 143, 161, 162, 168, 175, 176], "direct": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 63, 68, 70, 82, 90, 104, 106, 127, 178], "directli": [48, 63, 64, 66, 77, 78, 80, 82, 102, 106, 107, 112, 162, 168, 175, 178], "discontinu": [8, 45, 48, 49, 97, 110, 112, 113, 176, 177], "discret": [23, 35, 78, 79, 80, 93, 127, 134, 177], "discretis": 95, "discuss": [33, 65, 66, 93, 94, 127, 128, 133, 176, 177, 178], "disjoint": [65, 87, 88, 93], "displai": [13, 65, 71, 72, 73, 74, 78, 79, 80, 93, 103, 114, 115, 116, 162, 168], "displot": 94, "disproportion": [66, 94], "disregard": [51, 56], "dist": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40], "distinct": [107, 112], "distinguish": [16, 71, 74], "distr": [115, 123], "distribut": [17, 19, 39, 49, 63, 69, 77, 79, 80, 82, 97, 103, 106, 127, 129, 133, 140, 162, 170, 174, 176, 177], "diverg": [48, 63, 82, 106], "divid": [71, 74, 127], "dmatrix": [83, 84, 114], "dml": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 62, 63, 67, 68, 72, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 98, 99, 100, 101, 102, 103, 104, 105, 106, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 119, 120, 121, 127, 128, 129, 130, 132, 144, 161, 162, 168, 174], "dml1": [113, 175, 177, 178], "dml2": [62, 65, 67, 68, 75, 93, 113, 127, 144, 161, 175, 177, 178], "dml_apo": 90, "dml_apo_obj": 127, "dml_apos_att": 90, "dml_apos_obj": 127, "dml_apos_tun": 78, "dml_apos_untun": 78, "dml_combin": 161, "dml_cover": 101, "dml_cv_predict": 177, "dml_cvar": [85, 95], "dml_cvar_0": 85, "dml_cvar_1": 85, "dml_cvar_obj": [24, 114], "dml_data": [7, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 64, 65, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 89, 90, 93, 96, 101, 102, 103, 104, 108, 109, 110, 111, 112, 114, 115, 120, 121, 125, 126, 127, 128, 130, 132, 161, 178], "dml_data_anticip": [71, 74], "dml_data_arrai": [111, 112], "dml_data_bench": 103, "dml_data_bonu": [67, 175], "dml_data_df": 178, "dml_data_dgp3": 97, "dml_data_fuzzi": 100, "dml_data_lasso": 75, "dml_data_sharp": 100, "dml_data_sim": [67, 175], "dml_data_tun": 97, "dml_df": [65, 93], "dml_did": [69, 70], "dml_did_obj": [12, 15, 16, 127, 128, 129, 130, 132], "dml_iivm": 101, "dml_iivm_boost": [66, 94], "dml_iivm_forest": [66, 94], "dml_iivm_lasso": [66, 94], "dml_iivm_obj": [25, 81, 127], "dml_iivm_tre": [66, 94], "dml_irm": [77, 83, 87, 90, 98], "dml_irm_at": 89, "dml_irm_att": 90, "dml_irm_boost": [66, 94], "dml_irm_forest": [66, 94], "dml_irm_gat": 89, "dml_irm_gatet": 89, "dml_irm_lasso": [66, 75, 94], "dml_irm_new": 98, "dml_irm_obj": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 102, 114, 115, 117, 127], "dml_irm_obj_ext": [115, 117], "dml_irm_rf": 75, "dml_irm_tre": [66, 94], "dml_irm_weighted_att": 90, "dml_kwarg": 90, "dml_length": 101, "dml_long": 59, "dml_lplr": 91, "dml_lplr_obj": [37, 127], "dml_lpq_0": 99, "dml_lpq_1": 99, "dml_lpq_obj": [27, 114], "dml_lqte": [95, 99], "dml_obj": [64, 71, 72, 73, 74, 79, 80, 102, 103], "dml_obj_al": [71, 74], "dml_obj_anticip": [71, 74], "dml_obj_bench": 103, "dml_obj_lasso": 72, "dml_obj_linear": [71, 74], "dml_obj_linear_logist": 72, "dml_obj_nyt": [71, 74], "dml_obj_tun": 78, "dml_obj_tuned_pipelin": 78, "dml_obj_univers": [71, 74], "dml_obj_untun": 78, "dml_obj_untuned_pipelin": 78, "dml_pliv": [65, 93], "dml_pliv_obj": [38, 65, 93, 127], "dml_plpr_cre_gener": 97, "dml_plpr_cre_norm": 97, "dml_plpr_fd_exact": 97, "dml_plpr_obj": [39, 127], "dml_plpr_wg_approx": 97, "dml_plr": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 84, 88, 161], "dml_plr_1": 161, "dml_plr_2": 161, "dml_plr_boost": [66, 94], "dml_plr_forest": [66, 94, 178], "dml_plr_lasso": [66, 75, 94], "dml_plr_no_split": 143, "dml_plr_obj": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 102, 105, 114, 115, 116, 119, 120, 121, 123, 124, 125, 126, 127, 143, 144, 161, 162, 163, 168], "dml_plr_obj_extern": 143, "dml_plr_obj_intern": 143, "dml_plr_obj_onfold": 92, "dml_plr_obj_untun": 92, "dml_plr_rf": 75, "dml_plr_tree": [66, 94, 178], "dml_pq_0": [95, 99], "dml_pq_1": [95, 99], "dml_pq_obj": [28, 114], "dml_procedur": [75, 105, 175, 177, 178], "dml_qte": [95, 99], "dml_qte_obj": [29, 114], "dml_robust_confset": 101, "dml_robust_length": 101, "dml_short": 59, "dml_ssm": [68, 104, 127], "dml_standard_ci": 101, "dml_tune": 177, "dmldummyclassifi": [115, 117], "dmldummyregressor": [115, 117], "dmlmt": 176, "dmloptunaresult": 78, "dnorm": 63, "do": [64, 65, 66, 67, 71, 72, 74, 77, 90, 93, 94, 95, 96, 103, 114, 115, 117, 119, 143, 162, 172, 175, 178], "doabl": 144, "doc": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 173, 177], "docu": 177, "documen": 78, "document": [70, 71, 72, 73, 74, 76, 78, 79, 83, 84, 87, 88, 90, 91, 92, 97, 103, 127, 133, 140, 144, 162, 173, 177], "doe": [16, 23, 29, 64, 65, 66, 70, 71, 74, 78, 79, 80, 90, 93, 94, 96, 100, 102, 103, 127, 144, 148, 162, 172, 178], "doesn": [62, 81], "doi": [10, 11, 17, 18, 19, 31, 33, 36, 41, 42, 44, 45, 46, 64, 65, 67, 86, 93, 103, 106, 115, 125, 143, 161, 173, 175, 177], "domain": 98, "don": [64, 92], "done": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 72, 92, 95, 115, 120, 121, 126, 143, 162, 163], "dosag": [79, 80], "dot": [30, 39, 70, 71, 74, 97, 98, 107, 112, 114, 115, 120, 121, 127, 131, 133, 134, 140, 161, 175], "doubl": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 40, 42, 44, 45, 46, 47, 48, 66, 72, 77, 86, 92, 94, 96, 101, 113, 115, 127, 140, 143, 144, 161, 162, 163, 177], "double_ml": 101, "double_ml_bonus_data": 75, "double_ml_data_from_data_fram": [63, 106, 107, 112, 178], "double_ml_data_from_matrix": [64, 67, 107, 112, 115, 125, 126, 161, 175], "double_ml_framework": [127, 128], "double_ml_irm": 75, "double_ml_score_mixin": 0, "doubleiivm": 173, "doubleml": [0, 63, 65, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 95, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 132, 143, 144, 162, 168, 175, 176, 177], "doubleml2022python": 173, "doubleml2024r": 173, "doubleml_did_eval_linear": 64, "doubleml_did_eval_rf": 64, "doubleml_did_linear": 64, "doubleml_did_rf": 64, "doubleml_framework": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40], "doublemlapo": [79, 80, 90, 127, 144, 149, 177], "doublemlblp": [22, 26, 40, 83, 84, 90, 114, 177], "doublemlclusterdata": [0, 107, 112], "doublemlcvar": [85, 114, 144, 150, 177], "doublemldata": [0, 4, 7, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 62, 65, 67, 68, 69, 70, 75, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 95, 96, 98, 99, 100, 101, 102, 103, 104, 105, 106, 108, 109, 110, 111, 113, 114, 115, 116, 117, 119, 120, 121, 123, 124, 127, 129, 143, 144, 161, 162, 168, 177, 178], "doublemldid": [69, 70, 127, 129, 144, 147, 177], "doublemldidaggreg": [71, 72, 73, 74, 127, 128], "doublemldidbinari": 70, "doublemldidc": [69, 127, 129, 144, 145, 177], "doublemldiddata": [0, 12, 15, 18, 69, 70, 108, 127, 129], "doublemldidmulti": [71, 72, 73, 74, 78, 127, 128, 130, 131, 132, 144, 146, 148, 177], "doublemlframework": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 71, 72, 73, 74, 127, 128, 143, 161, 177], "doublemlframwork": 23, "doublemlidid": [127, 129], "doublemlididc": [127, 129], "doublemliivm": [62, 66, 81, 94, 101, 115, 118, 119, 122, 124, 127, 143, 144, 151, 177], "doublemlirm": [12, 14, 15, 22, 24, 25, 27, 28, 30, 37, 38, 39, 40, 64, 66, 75, 77, 78, 80, 83, 87, 89, 90, 94, 96, 98, 102, 103, 114, 115, 117, 118, 119, 122, 124, 127, 143, 144, 152, 173, 177], "doublemllplr": [91, 127, 144, 155, 177], "doublemllpq": [99, 114, 144, 153, 177], "doublemlpaneldata": [0, 14, 16, 39, 70, 72, 73, 97, 109, 127, 128, 130, 131, 132, 140, 177], "doublemlpliv": [115, 119, 124, 127, 143, 144, 156, 173, 177], "doublemlplpr": [97, 109, 112, 127, 140, 144, 157], "doublemlplr": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 63, 66, 67, 75, 82, 84, 88, 92, 94, 96, 102, 105, 106, 114, 115, 116, 119, 120, 121, 122, 123, 124, 125, 126, 127, 143, 144, 158, 161, 162, 168, 173, 175, 177, 178], "doublemlpolicytre": [26, 114], "doublemlpq": [95, 99, 114, 144, 154, 177], "doublemlqt": [85, 95, 99, 114, 161, 177], "doublemlrdddata": [0, 48, 100, 110, 127, 177], "doublemlresampl": [77, 90], "doublemlsmm": 177, "doublemlssm": [68, 104, 127, 144, 159, 160, 177], "doublemlssmdata": [0, 30, 36, 104, 111, 127, 177], "doubli": [18, 31, 41, 64, 72, 101, 176], "doudou": [42, 176], "down": [91, 103], "download": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 107, 112, 174, 175], "download_fil": 72, "downward": 103, "dpg_dict": 102, "dpi": [63, 82, 96], "dr": [127, 131], "dramat": 64, "draw": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 103, 143, 177], "draw_sample_split": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 77, 90, 143], "drawn": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 40, 49, 66, 71, 74, 94, 95, 98, 143], "drive": [63, 82, 106], "driven": [103, 178], "drop": [64, 92, 93, 96, 97, 107, 112, 115, 125, 126, 144, 145, 146, 147, 148, 161], "dropna": [71, 74], "dt": [71, 144, 145, 162, 164], "dt_bonu": [107, 112], "dta": [64, 73], "dtrain": 94, "dtype": [71, 72, 73, 74, 75, 78, 79, 80, 87, 88, 89, 91, 93, 94, 95, 97, 101, 102, 107, 109, 112, 114, 175], "dualiti": 93, "dubourg": [173, 175], "duchesnai": [173, 175], "due": [63, 64, 72, 82, 83, 84, 89, 91, 102, 103, 106, 127, 142, 162, 163, 177, 178], "duflo": [10, 11, 46, 65, 86, 93, 101, 106, 143, 173, 176], "dummi": [22, 26, 40, 50, 51, 53, 72, 92, 97, 103, 114, 115, 117, 127, 129, 177], "dummyclassifi": [50, 72], "dummyregressor": [51, 72], "duplic": 177, "durabl": [67, 75, 107, 112, 175], "durat": [11, 78], "dure": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 52, 64, 65, 66, 67, 68, 92, 93, 94, 115, 119, 124, 126, 143, 175, 177, 178], "dx": 33, "dynam": [64, 176], "e": [6, 7, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 36, 37, 38, 39, 40, 41, 46, 48, 50, 51, 52, 55, 56, 63, 64, 65, 66, 68, 69, 71, 72, 73, 74, 77, 78, 79, 80, 82, 83, 84, 86, 89, 91, 92, 93, 94, 95, 96, 97, 100, 101, 102, 103, 104, 106, 109, 112, 114, 115, 116, 117, 119, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178], "e20ea26": 67, "e401": [66, 94, 95, 102, 178], "e4016553": 178, "e45228": 96, "e57c": 67, "each": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 40, 50, 51, 55, 56, 65, 67, 70, 71, 72, 73, 74, 77, 78, 79, 80, 87, 88, 91, 92, 93, 95, 96, 97, 98, 101, 102, 103, 105, 107, 109, 112, 115, 116, 119, 124, 126, 127, 133, 140, 143, 161, 162, 168, 178], "earlier": [71, 74, 178], "earn": [66, 94, 95], "earner": [66, 94, 102], "easi": [67, 101, 144], "easier": 92, "easili": [67, 77, 92, 95, 177], "ec973f": 96, "ecolor": [70, 78, 79, 80, 94, 96, 97], "econ": 176, "econml": 176, "econom": [36, 43, 44, 47, 65, 86, 93, 96, 103, 143, 176], "econometr": [10, 11, 17, 18, 19, 31, 41, 42, 45, 46, 47, 64, 65, 86, 93, 101, 106, 173, 176], "econometrica": [34, 65, 93, 96, 101, 106, 176], "ecosystem": [173, 178], "ectj": [10, 11, 42, 45, 46, 65, 86, 93, 106, 173], "ed": 176, "edge_color": 82, "edgecolor": 82, "edit": [174, 176], "edu": [173, 175], "educ": [66, 94, 95, 102, 178], "ee97bda7": 67, "effect": [12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 41, 45, 49, 50, 51, 55, 56, 62, 63, 64, 65, 67, 68, 69, 70, 78, 80, 81, 82, 86, 89, 93, 98, 100, 101, 104, 106, 113, 115, 123, 124, 125, 126, 128, 129, 131, 133, 135, 136, 137, 140, 142, 143, 144, 152, 157, 161, 162, 163, 175, 176, 177, 178], "effici": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 79, 127, 176], "effort": 144, "eight": [65, 93], "either": [12, 14, 15, 16, 17, 19, 22, 24, 25, 26, 27, 28, 30, 32, 37, 38, 39, 40, 67, 70, 71, 74, 86, 98, 100, 114, 115, 116, 121, 122, 126, 127, 130, 133, 178], "elapsed_tim": 79, "eleanor": 176, "element": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 65, 68, 69, 71, 72, 73, 74, 77, 83, 84, 85, 93, 95, 99, 102, 104, 127, 130, 131, 132, 144, 145, 146, 147, 148, 149, 155, 162, 165, 167, 168, 171, 172, 177], "element_text": [65, 66], "elementari": 176, "elif": [87, 88, 98], "elig": [95, 102, 178], "eligibl": [66, 94, 102], "ell": [63, 65, 82, 86, 93, 106, 127, 140, 144, 156, 157, 158, 175], "ell_0": [25, 38, 39, 40, 63, 82, 86, 92, 106, 127, 136, 144, 157, 158], "ell_1": [72, 97, 127, 140], "ell_2": 77, "els": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 53, 64, 65, 66, 70, 78, 79, 87, 88, 93, 97, 98, 103], "em": 176, "emphas": [65, 93], "empir": [20, 21, 63, 65, 72, 82, 93, 96, 103, 106, 127, 130, 131, 132, 143, 144, 161], "emploi": [65, 78, 86, 93, 97, 103, 127, 140, 144, 151], "employ": [66, 72, 94, 95], "employe": 178, "empti": 93, "emul": [162, 163], "enabl": [13, 78, 79, 80, 98, 102, 114, 162, 163, 177], "enable_metadata_rout": [50, 51, 55, 56], "encapsul": [50, 51, 55, 56, 115, 123, 124], "encod": 96, "encount": [74, 79], "end": [33, 42, 43, 44, 63, 64, 65, 66, 68, 70, 71, 74, 77, 82, 85, 86, 93, 94, 96, 97, 98, 99, 104, 105, 107, 112, 115, 124, 126, 127, 130, 132, 133, 140, 143, 144, 146, 148, 161, 175, 178], "end_tim": 79, "endogen": [66, 94, 95, 178], "enet_coordinate_descent_gram": 93, "enforc": 72, "engin": [67, 176], "enrol": [66, 94, 95], "ensembl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 75, 77, 78, 79, 80, 82, 83, 84, 87, 88, 89, 91, 94, 98, 102, 103, 105, 114, 115, 116, 117, 118, 119, 123, 127, 128, 129, 130, 132, 143, 144, 161, 162, 168, 175, 178], "ensemble_learner_pipelin": [115, 123], "ensemble_pipe_classif": 67, "ensemble_pipe_regr": 67, "ensur": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 55, 56, 65, 79, 90, 92, 93, 98, 101, 110, 112], "enter": 127, "entir": [16, 63, 66, 82, 94, 97, 106, 127, 140, 162, 163], "entri": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 46, 47, 49, 63, 65, 71, 72, 73, 74, 75, 78, 79, 80, 82, 89, 91, 93, 94, 95, 102, 106, 107, 109, 112, 115, 124, 126, 173, 175, 177], "enumer": [70, 77, 78, 79, 80, 85, 87, 88, 93, 94, 95, 97, 99, 105, 115, 124, 126, 143], "env": 174, "environ": [78, 97, 174], "ep": 96, "epanechnikov": 48, "epsilon": [66, 69, 70, 85, 94, 99, 114, 127, 129, 133], "epsilon_": [65, 70, 71, 74, 93], "epsilon_0": 49, "epsilon_1": 49, "epsilon_i": [32, 85, 96, 98, 99], "epsilon_sampl": 98, "epsilon_tru": [85, 99], "eqnarrai": 66, "equal": [13, 17, 19, 22, 23, 26, 65, 68, 71, 74, 93, 96, 101, 104, 114, 115, 126, 127, 162, 170], "equat": [49, 65, 66, 93, 94, 103, 105, 161, 178], "equilibrium": [65, 93], "equiv": [127, 133, 144, 146, 148], "equival": [86, 90, 143], "err": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 69, 71, 72, 73, 74, 75, 78, 79, 80, 81, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 102, 103, 104, 114, 115, 116, 117, 119, 120, 121, 127, 128, 129, 130, 132, 143, 144, 161, 175, 178], "error": [5, 6, 7, 8, 9, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 50, 51, 55, 56, 62, 63, 64, 66, 67, 68, 70, 77, 78, 79, 82, 86, 87, 88, 92, 94, 97, 100, 103, 106, 115, 116, 117, 123, 124, 125, 126, 127, 139, 140, 141, 143, 144, 161, 162, 168, 175, 177, 178], "error_on_convergence_failur": 37, "errorbar": [70, 78, 79, 80, 87, 88, 92, 94, 96, 97, 100], "erstellt": [65, 66, 67], "es_linear_logist": 72, "es_rf": 72, "escap": 93, "esim": 100, "especi": [77, 92, 107, 112], "essenti": 103, "est": 100, "est_bound": 16, "est_method": 64, "esther": [143, 176], "estim": [10, 12, 13, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 41, 48, 50, 51, 52, 53, 54, 55, 56, 63, 64, 65, 67, 70, 77, 78, 80, 82, 83, 84, 85, 87, 88, 90, 91, 93, 98, 100, 101, 105, 106, 113, 114, 115, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 135, 137, 140, 145, 146, 147, 150, 153, 154, 155, 160, 162, 163, 168, 173, 176, 177], "estimand": 101, "estimator_list": 92, "et": [10, 11, 32, 34, 42, 44, 46, 63, 65, 66, 67, 69, 77, 82, 83, 84, 85, 86, 87, 88, 91, 93, 94, 95, 99, 102, 106, 127, 129, 133, 143, 144, 150, 152, 153, 154, 155, 161, 162, 163, 172, 173, 175, 177], "eta": [20, 21, 63, 65, 66, 70, 92, 93, 94, 99, 100, 105, 114, 127, 130, 131, 132, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 172, 175, 178], "eta1": 96, "eta2": 96, "eta_": [161, 162, 172], "eta_0": [48, 105, 127, 130, 131, 132, 144, 161], "eta_d": [100, 127], "eta_i": [17, 19, 32, 70, 71, 74, 98, 99, 100, 127], "eta_sampl": 98, "eta_tru": 99, "etc": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 65, 77, 92, 93, 177], "ev": [63, 82, 106], "eval": [16, 67, 71, 72, 73, 74, 115, 125, 126, 127, 130, 131, 132, 144, 146, 148], "eval_metr": [66, 94, 178], "eval_pr": 64, "eval_predict": 64, "evalu": [12, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 34, 37, 38, 39, 40, 64, 67, 70, 71, 72, 73, 74, 83, 84, 85, 89, 90, 95, 99, 102, 105, 116, 117, 126, 127, 131, 133, 176, 177], "evaluate_learn": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 77, 78, 92, 115, 116, 177], "evalut": [115, 116], "even": [66, 67, 71, 74, 78, 94, 96, 100, 115, 123, 127, 178], "event": [127, 128], "eventstudi": [16, 71, 72, 73, 74, 127, 128], "eventu": [65, 93], "everi": [65, 72, 93], "everyth": 173, "evid": [89, 92], "exact": [39, 90, 103], "exactli": [100, 103, 127], "examin": 79, "exampl": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 40, 48, 57, 58, 62, 63, 66, 67, 68, 69, 70, 71, 73, 74, 75, 77, 79, 80, 81, 82, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 114, 115, 116, 117, 118, 119, 120, 121, 123, 124, 125, 126, 127, 128, 130, 131, 132, 140, 142, 143, 144, 161, 162, 168, 173, 175, 177, 178], "example_attgt": 64, "example_attgt_dml_eval_linear": 64, "example_attgt_dml_eval_rf": 64, "example_attgt_dml_linear": 64, "example_attgt_dml_rf": 64, "except": [51, 56, 86, 103, 177], "excess": 77, "exclud": 59, "exclus": [22, 26, 40, 87, 88, 114], "execut": [67, 178], "exemplarili": 175, "exemplatori": 98, "exhaust": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 115, 121], "exhibit": [65, 93], "exist": [50, 51, 55, 56, 90, 127, 129, 133, 162, 172], "exogen": [66, 94, 95, 127, 178], "exp": [17, 18, 19, 31, 32, 34, 35, 41, 42, 45, 46, 63, 70, 82, 83, 84, 87, 88, 96, 98, 106], "expect": [31, 37, 41, 51, 56, 64, 68, 69, 71, 72, 74, 77, 80, 89, 91, 92, 100, 103, 104, 110, 111, 112, 114, 127, 143, 161, 162, 169, 175], "experi": [11, 33, 34, 63, 66, 82, 94, 101, 103, 106, 107, 112, 143, 175, 176], "experiment": [12, 14, 15, 16, 17, 18, 19, 144, 145, 146, 147, 148, 162, 164, 165, 166, 167], "expertis": 103, "expit": [37, 91, 127, 138, 144, 155], "explain": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 41, 78, 102, 162, 163, 171, 172], "explan": [65, 69, 93, 102, 162, 171, 173, 178], "explanatori": [103, 161], "explicit": 103, "explicitli": [71, 73, 74, 89, 178], "exploit": [63, 82, 106, 127, 178], "explor": [78, 92], "exponenti": 161, "export": [92, 177], "expos": [107, 108, 110, 111, 112, 127, 133], "exposit": 97, "exposur": [7, 17, 19, 70, 71, 72, 73, 74], "express": [65, 86, 100, 162, 172], "ext": [16, 17, 19], "extend": [91, 97, 103, 108, 111, 112, 115, 122, 173, 177], "extendend": [162, 172], "extens": [115, 117, 122, 144, 173, 176, 177], "extent": 86, "extern": [63, 82, 90, 92, 113, 117, 121, 124, 126, 162, 163, 177], "external_predict": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 82, 115, 117], "externalptr": 66, "extra": 67, "extra_degre": 97, "extra_nam": 97, "extract": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 79, 92], "extralearn": 67, "extrem": [57, 58, 66, 94], "extreme_threshold": [57, 58], "ey": 86, "ezequiel": 177, "f": [66, 67, 69, 70, 71, 74, 77, 78, 79, 80, 82, 85, 86, 93, 94, 95, 97, 98, 99, 101, 102, 103, 104, 115, 125, 162, 172, 173, 175], "f00584a57972": 67, "f1718fdeb9b0": 67, "f2e7": 67, "f3d24993": 67, "f6ebc": 96, "f_": [17, 18, 19, 31, 70, 114], "f_loc": [85, 99], "f_p": 70, "f_scale": [85, 99], "f_t": [71, 74], "f_x": 127, "face_color": 82, "facet_wrap": 66, "facilit": 92, "fact": [66, 94, 95], "factor": [49, 63, 64, 65, 66, 67, 77, 82, 106, 115, 123, 124, 162, 165, 167, 178], "faculti": 176, "fail": 177, "failur": 37, "fair": 77, "fake": [62, 81, 101], "fall": 79, "fallback": [115, 123, 124], "fals": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 42, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 63, 66, 67, 68, 69, 71, 72, 73, 74, 77, 78, 79, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 97, 98, 99, 100, 101, 103, 104, 107, 109, 112, 115, 124, 125, 126, 127, 130, 132, 143, 144, 145, 146, 147, 148, 161, 162, 164, 165, 166, 167, 178], "famili": [66, 94, 115, 123, 126], "familiar": 101, "fanci": 64, "far": [66, 94], "farbmach": 33, "fashion": 97, "fast": [77, 78, 98, 115, 122], "faster": 86, "fb5c25fa": 67, "fc9e": 67, "fd": [97, 127, 140], "fd8a": 67, "fd_exact": [39, 127, 140, 144, 157], "featur": [10, 11, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 51, 54, 56, 64, 70, 71, 74, 75, 77, 79, 89, 90, 94, 98, 100, 103, 110, 112, 114, 115, 117, 123, 124, 127], "featureless": [67, 115, 123], "features_bas": [66, 94, 95, 102], "features_flex": 66, "featureunion": 67, "februari": [71, 103], "feder": 72, "feedback": [97, 127, 140], "femal": [67, 75, 107, 112, 175], "fern\u00e1ndez": [34, 143, 176], "fetch": [66, 93, 94, 95, 107, 112], "fetch_401k": [66, 94, 95, 102, 178], "fetch_bonu": [67, 75, 107, 112, 175], "few": [66, 94, 95], "ff7f0e": 70, "field": [65, 93, 115, 124, 178], "fifteenth": 176, "fifth": [65, 71, 74], "fig": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 70, 71, 72, 73, 74, 77, 78, 79, 80, 83, 84, 85, 86, 90, 92, 95, 96, 99, 100, 103], "fig_al": 82, "fig_dml": 82, "fig_non_orth": 82, "fig_orth_nosplit": 82, "fig_po_al": 82, "fig_po_dml": 82, "fig_po_nosplit": 82, "figsiz": [13, 16, 70, 71, 74, 75, 77, 78, 79, 80, 83, 84, 85, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100], "figur": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 46, 63, 65, 70, 71, 72, 74, 75, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 97, 99, 103, 106], "figure_format": 96, "file": [10, 11, 72, 86, 96, 176, 177], "filenam": 63, "fill": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 63, 65, 66, 68, 69, 77, 94, 104], "fill_between": [83, 84, 85, 90, 95, 99], "fill_valu": 77, "fillna": 71, "filter": 67, "filterwarn": [77, 78, 79, 82, 97], "final": [63, 67, 68, 70, 71, 73, 74, 78, 80, 82, 83, 84, 85, 87, 88, 89, 95, 97, 99, 100, 104, 106, 127, 140, 142, 144, 146, 148, 178], "final_estim": [78, 100], "final_estimatorridg": 78, "financi": [10, 102, 178], "find": [66, 69, 70, 79, 94, 103, 114, 115, 125, 126, 178], "finish": 67, "finit": [63, 66], "firm": [65, 93, 102], "firmid": 93, "first": [7, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 41, 44, 48, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 80, 81, 82, 83, 84, 87, 88, 89, 90, 91, 93, 94, 95, 97, 98, 99, 100, 103, 104, 106, 114, 127, 128, 131, 133, 140, 143, 161, 162, 168, 174, 175, 177, 178], "fit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 50, 51, 52, 53, 54, 55, 56, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 113, 114, 115, 116, 117, 118, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 132, 133, 140, 144, 147, 160, 161, 162, 163, 168, 173, 177, 178], "fit_arg": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40], "fit_transform": [90, 93, 94], "fittedpipelin": [78, 97], "five": 93, "fix": [45, 70, 71, 74, 77, 78, 127, 140, 176, 177], "flag": [18, 107, 112, 143, 174], "flake8": 177, "flamlclassifierdoubleml": 92, "flamlregressordoubleml": 92, "flatten": [92, 96], "flexibl": [48, 62, 64, 66, 67, 69, 81, 94, 127, 173, 176, 177, 178], "flexibli": [66, 72, 94, 102], "float": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 41, 42, 45, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 71, 72, 73, 74, 97, 109, 112], "float32": [94, 95, 102], "float64": [71, 72, 73, 74, 75, 78, 79, 80, 85, 87, 88, 89, 91, 93, 94, 102, 107, 109, 112, 115, 121, 175], "floor": 67, "floor_divid": 93, "flt": 67, "flush": 63, "fmt": [70, 78, 79, 80, 87, 88, 92, 94, 96, 97, 100], "fobj": 94, "focu": [65, 66, 72, 78, 90, 93, 94, 95, 103, 114, 127, 129, 133, 142, 178], "focus": [78, 95, 102, 103, 178], "fold": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 62, 65, 66, 67, 68, 69, 71, 72, 73, 74, 77, 78, 93, 94, 95, 97, 102, 104, 105, 113, 115, 116, 119, 124, 125, 126, 127, 129, 130, 132, 140, 144, 147, 161, 175, 178], "follow": [17, 18, 19, 31, 32, 35, 41, 42, 63, 65, 66, 68, 69, 70, 71, 72, 73, 74, 78, 82, 83, 84, 85, 87, 88, 91, 92, 93, 94, 95, 99, 100, 101, 102, 103, 104, 106, 107, 112, 114, 115, 116, 117, 123, 124, 126, 127, 128, 130, 131, 132, 133, 140, 143, 144, 146, 148, 155, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 178], "font_scal": [93, 94, 95], "fontsiz": [71, 74, 85, 91, 95, 99], "force_all_d_finit": [5, 6, 7, 8, 9, 107, 108, 112], "force_all_x_finit": [4, 5, 6, 7, 8, 9, 107, 112], "forest": [33, 62, 63, 64, 66, 67, 69, 77, 79, 81, 82, 89, 94, 102, 106, 115, 123, 124, 126, 175, 178], "forest_summari": 94, "forg": [174, 176, 177], "form": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 40, 41, 45, 53, 55, 56, 66, 68, 69, 70, 77, 83, 84, 85, 87, 88, 89, 94, 97, 99, 100, 102, 104, 114, 127, 128, 129, 133, 136, 137, 138, 139, 140, 141, 144, 146, 148, 149, 152, 162, 163, 168, 169, 170, 171, 172, 174, 175], "format": [7, 16, 42, 72, 79, 82, 89, 162, 168, 177], "former": [72, 101], "formula": [65, 66, 93, 94, 100, 103, 177], "formula_flex": 66, "forschungsgemeinschaft": 173, "forthcom": [103, 176], "forum": 177, "forward": [26, 54], "found": [52, 73, 83, 84, 86, 87, 88, 91, 92, 97, 106, 107, 112, 115, 120, 122, 127, 142, 175], "foundat": [79, 173, 176], "four": [66, 77, 79, 94, 127, 130, 140, 177], "fourth": [65, 93], "frac": [17, 18, 19, 21, 25, 31, 33, 34, 36, 41, 42, 43, 45, 46, 47, 51, 56, 63, 65, 67, 70, 74, 82, 86, 89, 91, 93, 96, 100, 105, 106, 114, 127, 130, 131, 132, 136, 138, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 159, 160, 161, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172], "fraction": [19, 67], "frame": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 53, 62, 63, 65, 66, 68, 71, 72, 73, 74, 75, 78, 79, 80, 83, 84, 87, 88, 89, 91, 93, 94, 95, 96, 98, 102, 106, 107, 109, 112, 175, 178], "framealpha": [71, 74], "frameon": [71, 74, 91], "framework": [13, 16, 21, 63, 65, 67, 77, 79, 82, 92, 93, 96, 103, 106, 115, 125, 161, 173, 175, 177, 178], "freez": 174, "fribourg": 176, "friendli": [71, 74, 79, 80], "from": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 51, 53, 56, 57, 58, 62, 63, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 108, 109, 110, 111, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 125, 126, 127, 128, 129, 130, 131, 132, 133, 140, 143, 144, 161, 162, 168, 175, 177, 178], "from_arrai": [4, 5, 6, 7, 8, 9, 30, 48, 69, 70, 82, 85, 99, 106, 107, 108, 110, 111, 112, 115, 120, 121, 161, 175], "from_config": [57, 58], "from_product": 93, "front": 80, "fr\u00e9chet": [162, 172], "fs_kernel": [48, 127], "fs_specif": [48, 127], "fsize": [66, 94, 95, 102, 178], "full": [52, 69, 70, 77, 78, 79, 80, 82, 85, 87, 88, 94, 95, 99, 100, 101, 104, 106, 127], "fulli": [26, 66, 76, 92, 94, 101, 127, 137], "fun": 63, "func": 64, "function": [0, 4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 42, 45, 46, 47, 48, 62, 63, 66, 67, 68, 69, 71, 72, 73, 74, 77, 79, 81, 82, 83, 84, 85, 90, 91, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 113, 115, 116, 117, 118, 119, 120, 121, 122, 124, 126, 127, 128, 129, 130, 131, 132, 133, 136, 138, 140, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 172, 173, 176, 177, 178], "fund": [66, 94, 95, 173], "further": [13, 16, 17, 18, 19, 31, 32, 35, 41, 44, 65, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 80, 83, 84, 85, 89, 90, 91, 93, 95, 97, 98, 99, 100, 102, 103, 104, 115, 116, 117, 118, 119, 124, 127, 129, 132, 140, 142, 144, 150, 153, 154, 155, 159, 160, 161, 162, 163, 168, 171, 172, 173, 175, 177, 178], "furthermor": [82, 97, 109, 112, 127, 140, 144, 149, 152], "futur": [4, 14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 71, 72, 73, 74, 83, 115, 127, 144, 162], "futurewarn": [70, 71, 72, 73, 74], "fuzzi": [48, 49], "g": [6, 7, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 41, 48, 50, 51, 52, 55, 56, 63, 64, 67, 69, 70, 71, 72, 73, 74, 75, 77, 82, 83, 84, 86, 89, 95, 96, 98, 101, 102, 104, 106, 109, 112, 114, 115, 116, 117, 125, 127, 128, 129, 130, 131, 132, 133, 140, 144, 145, 146, 147, 148, 149, 151, 152, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 177, 178], "g0": 79, "g1": 79, "g_": [49, 80, 127, 132, 144, 145, 147, 148, 150, 153, 154, 161], "g_0": [12, 14, 15, 16, 22, 23, 25, 26, 28, 38, 39, 40, 45, 46, 47, 48, 49, 63, 65, 66, 77, 79, 82, 93, 94, 106, 114, 115, 119, 124, 127, 134, 135, 136, 137, 139, 141, 144, 146, 148, 149, 157, 158, 159, 160, 162, 169, 170, 172, 175, 178], "g_1": [49, 77, 97, 127, 140], "g_all": [63, 66], "g_all_po": 63, "g_ci": 66, "g_d": [144, 150, 154], "g_dml": 63, "g_dml_po": 63, "g_hat": [38, 40, 63, 82, 144], "g_hat0": [25, 26], "g_hat1": [25, 26], "g_i": [17, 19, 127, 130, 132, 133, 144, 146, 148], "g_k": 114, "g_nonorth": 63, "g_nosplit": 63, "g_nosplit_po": 63, "g_valu": 14, "g_x": 70, "gain": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 41, 59, 77, 162, 163, 170, 177], "gain_statist": 177, "galleri": [77, 78, 106, 114, 115, 116, 120, 123, 127, 128, 130, 132, 142, 173, 177], "gama": 92, "gamma": [36, 43, 47, 65, 93, 96, 97, 98, 100, 103, 127, 140, 144, 150, 153], "gamma_0": [32, 68, 98, 104, 144, 150, 153], "gamma_a": [31, 41, 103], "gamma_bench": 103, "gamma_i": [39, 97, 127, 140], "gamma_v": 103, "gap": [93, 103], "gapo": 22, "gate": [22, 26, 40, 53, 96, 98, 113, 177], "gate_obj": 114, "gatet": 114, "gaussian": [27, 28, 29, 63, 82, 106, 114, 115, 123, 126, 161, 176], "ge": [18, 31, 32, 89, 98, 114, 127, 131, 133], "geer": 176, "gelbach": [65, 93], "gener": [0, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 51, 56, 57, 62, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 77, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 107, 112, 113, 114, 115, 117, 120, 121, 123, 125, 126, 127, 130, 132, 133, 134, 140, 143, 144, 146, 148, 149, 152, 161, 163, 164, 165, 166, 167, 169, 170, 172, 176, 177, 178], "generate_treat": 99, "generate_weakiv_data": 101, "geom_bar": 66, "geom_dens": [66, 68], "geom_errorbar": 66, "geom_funct": 63, "geom_histogram": 63, "geom_hlin": 66, "geom_point": 66, "geom_til": 65, "geom_vlin": [63, 68], "geq": [17, 19, 100, 127], "german": 173, "get": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 50, 51, 55, 56, 67, 71, 73, 74, 77, 78, 79, 80, 96, 102, 103, 162, 163, 173, 174], "get_dummi": 96, "get_feature_names_out": [90, 93, 94, 97], "get_legend_handles_label": [79, 80], "get_level_valu": 92, "get_logg": [63, 64, 65, 66, 67, 68, 105, 115, 124, 125, 126, 127, 143, 144, 161, 175], "get_metadata_rout": [50, 51, 55, 56], "get_n_split": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "get_param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 50, 51, 55, 56, 92, 115, 118], "get_ylim": 90, "ggdid": 64, "ggplot": [63, 65, 66, 68], "ggplot2": [63, 65, 66, 68], "ggsave": 63, "ggtitl": 66, "gh": 177, "git": 174, "github": [64, 66, 72, 78, 86, 92, 96, 97, 173, 176, 177], "githubusercont": [73, 86], "give": [66, 90, 94, 127, 140], "given": [12, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 34, 37, 38, 39, 40, 41, 46, 47, 48, 49, 50, 51, 55, 56, 63, 65, 68, 70, 72, 73, 78, 80, 82, 87, 88, 93, 95, 96, 97, 100, 103, 104, 106, 114, 127, 130, 131, 132, 140, 144, 149, 161, 162, 168, 169, 170, 171, 172, 175, 177], "glmnet": [66, 67, 115, 125, 126, 177], "global": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 55, 56, 115, 124, 126, 127], "globalclassifi": 100, "globallearn": 100, "globalregressor": 100, "glrn": 67, "glrn_lasso": 67, "gmm": 101, "gname": 64, "go": [83, 84, 86, 90, 92, 100, 103], "goal": [78, 80, 87, 88, 127], "goe": 127, "goldman": 176, "good": [86, 91, 162, 163, 178], "gpu": 79, "gradient": [66, 94], "gradientboostingclassifi": 77, "gradientboostingregressor": 77, "gradual": 103, "gramfort": [173, 175], "grant": 173, "graph": [67, 68, 104, 178], "graph_ensemble_classif": 67, "graph_ensemble_regr": 67, "graph_obj": 100, "graph_object": [83, 84, 86, 103], "graphlearn": [67, 115, 123], "grasp": [80, 162, 163], "great": [70, 178], "greater": 178, "green": [63, 83, 84, 85, 99], "greg": 176, "grei": [66, 79, 80], "grenand": 176, "grey50": 65, "grid": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 65, 67, 71, 74, 78, 79, 80, 83, 84, 85, 86, 90, 91, 95, 96, 97, 99, 103, 125, 126, 162, 168], "grid_arrai": [83, 84], "grid_basi": 90, "grid_bound": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 103], "grid_search": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 67, 115, 121, 125, 126], "grid_siz": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 83, 84], "gridextra": 65, "gridsearchcv": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 115, 121], "grisel": [173, 175], "grk": 173, "grob": 65, "group": [7, 14, 16, 17, 19, 22, 26, 39, 40, 62, 64, 72, 78, 79, 80, 81, 89, 90, 95, 96, 97, 98, 103, 113, 127, 128, 130, 131, 132, 133, 140, 144, 146, 148, 162, 165, 167], "group_0": 114, "group_1": [87, 88, 114], "group_2": [87, 88, 114], "group_3": [87, 88], "group_effect": 98, "group_ind": 89, "group_treat": 89, "groupbi": [71, 74, 79, 86, 94, 101], "gruber": 33, "gt": [62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 78, 79, 80, 81, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 107, 112, 175], "gt_combin": [16, 71, 72, 74, 127, 128, 130, 131, 132], "gt_dict": [71, 74], "guarante": [65, 93], "guber": 33, "guess": [102, 162, 163], "guid": [20, 21, 50, 51, 55, 56, 63, 64, 65, 67, 70, 71, 72, 73, 74, 80, 82, 89, 90, 93, 100, 102, 115, 122, 173, 175, 177], "guidelin": 177, "gunion": [67, 115, 123], "gxidclusterperiodytreat": 64, "h": [17, 18, 19, 31, 33, 41, 44, 64, 65, 93, 100, 101, 109, 112, 127, 176], "h20": 92, "h_0": [71, 74, 80, 89, 90, 102, 103, 162, 168, 178], "h_f": [48, 127], "ha": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 50, 51, 53, 54, 55, 56, 63, 64, 65, 66, 72, 74, 77, 78, 82, 86, 92, 93, 94, 95, 96, 97, 100, 101, 102, 103, 107, 112, 114, 115, 116, 117, 127, 129, 133, 162, 163, 168, 169, 170, 171, 172, 173, 178], "had": 72, "half": [63, 82, 96, 106, 143], "hand": [48, 77, 92, 96, 101, 178], "handbook": 96, "handl": [64, 72, 77, 78, 79, 80, 108, 109, 112, 115, 116, 117, 177], "hansen": [10, 11, 34, 43, 46, 65, 86, 93, 101, 106, 173, 176], "happend": 77, "hard": [102, 162, 163], "harold": 176, "harsh": [50, 55], "hasn": [13, 16, 71, 73, 74], "hat": [63, 65, 82, 86, 89, 93, 96, 97, 100, 105, 106, 114, 127, 140, 143, 144, 161, 162, 163, 168, 171], "have": [16, 22, 23, 26, 29, 32, 35, 40, 53, 54, 55, 56, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 83, 84, 89, 90, 92, 93, 94, 95, 96, 98, 100, 101, 102, 103, 104, 107, 112, 114, 115, 117, 122, 123, 124, 127, 131, 144, 146, 148, 161, 162, 163, 169, 172, 174, 175, 177, 178], "hazlett": [103, 162, 163], "hc": [64, 176], "hc0": [53, 177], "hdm": [65, 93], "he": [68, 104], "head": [64, 65, 67, 71, 72, 73, 74, 75, 83, 84, 87, 88, 90, 92, 93, 94, 96, 97, 100, 103, 107, 108, 112, 114, 175], "heat": [65, 93], "heatmap": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 65, 93, 103], "heavili": 77, "hei": 176, "height": [13, 16, 63, 65, 86, 92], "help": [64, 66, 72, 77, 85, 95, 98, 103, 127, 128, 143, 178], "helper": [108, 112, 177], "henc": [64, 66, 67, 94, 103, 115, 126, 144, 178], "here": [27, 28, 29, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 83, 84, 85, 87, 88, 89, 91, 93, 94, 95, 97, 98, 99, 100, 103, 104, 107, 112, 115, 117, 120, 127, 130, 174], "herzig": 177, "heterogen": [17, 19, 26, 32, 39, 42, 66, 89, 94, 95, 97, 98, 113, 127, 137, 140, 143, 176, 177, 178], "heteroskedast": [87, 88], "heurist": [63, 82, 106], "high": [34, 37, 38, 39, 40, 66, 70, 86, 94, 95, 101, 105, 127, 138, 139, 140, 141, 161, 173, 175, 176], "higher": [64, 66, 86, 94, 95, 96, 100, 101, 177, 178], "highli": [66, 94, 173], "highlight": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 69, 77, 78, 90, 92, 103, 177], "highlightcolor": [83, 84], "hint": 92, "hispan": 75, "hist": [79, 80], "hist_e401": 66, "hist_p401": 66, "histogram": [79, 80], "histori": 52, "histplot": 82, "hjust": 66, "hline": [107, 112, 161, 175, 178], "hold": [36, 52, 58, 65, 66, 68, 91, 92, 93, 94, 104, 114, 115, 126, 127, 131], "holdout": [115, 126, 143], "holm": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "home": [66, 70, 71, 73, 74, 94, 100, 101, 103], "homogen": [97, 127, 140], "hook": 177, "hopefulli": 95, "horizont": [65, 70, 78, 79, 93], "hostedtoolcach": [70, 71, 72, 73, 74, 93, 94, 103], "hot": 96, "hotstart_backward": [67, 115, 123, 124], "hotstart_forward": [67, 115, 123], "household": [66, 94, 95, 102], "how": [13, 17, 19, 42, 50, 51, 55, 56, 62, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 106, 108, 112, 115, 122, 123, 124, 126, 127, 173, 174], "howev": [63, 66, 68, 82, 91, 92, 94, 97, 100, 103, 104, 106, 127, 178], "hown": [66, 94, 95, 102, 178], "hpwt": [65, 93], "hpwt0": 65, "hpwtairmpdspac": 65, "href": 173, "hspace": 77, "hstack": [30, 70, 97], "html": [67, 78, 97, 173, 175, 177], "http": [33, 47, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 115, 125, 173, 174, 175, 177], "huber": [36, 68, 104, 127, 142, 144, 159, 160, 176], "hue": [71, 74, 94], "huge": 77, "hugo": 176, "husd": [67, 75, 107, 112, 175], "hyperparamet": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 52, 66, 67, 75, 77, 79, 86, 92, 94, 113, 117, 119, 120, 121, 122, 123, 124, 126, 175, 177], "hyperparametertun": 97, "hypothes": [161, 176], "hypothesi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 66, 94, 102, 162, 168, 176], "hypothet": 103, "i": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 152, 153, 154, 155, 160, 161, 162, 163, 165, 167, 168, 169, 170, 172, 173, 174, 175, 177, 178], "i0": [69, 70, 127, 129], "i03": 173, "i1": [39, 69, 97, 127, 129, 140], "iT": [39, 97, 127, 140], "i_": [43, 93, 98], "i_1": [65, 93], "i_2": [65, 93], "i_3": [65, 93], "i_4": 70, "i_est": 82, "i_fold": 65, "i_k": [65, 93, 105, 143, 161], "i_learn": 77, "i_level": 80, "i_rep": [63, 68, 69, 77, 82, 104, 106], "i_split": 93, "i_train": 82, "icp": 176, "id": [7, 16, 39, 64, 65, 67, 71, 72, 73, 74, 93, 97, 109, 112, 127, 128, 130, 132, 162, 167], "id_col": [7, 16, 71, 72, 73, 74, 97, 109, 112, 127, 128, 130, 132, 140], "id_var": 93, "idea": [66, 67, 78, 94, 95, 103, 115, 125, 127, 162, 163, 178], "ideal": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "ident": [17, 18, 19, 31, 32, 35, 41, 43, 54, 67, 72, 78, 80, 90, 92, 100, 115, 117, 127, 133, 140, 144, 152, 162, 168], "identfi": 103, "identif": [71, 72, 73, 74, 100, 101, 127, 178], "identifi": [7, 65, 66, 69, 72, 79, 89, 93, 94, 95, 100, 103, 108, 109, 110, 111, 112, 114, 127, 129, 131, 133, 140, 142, 162, 172, 177], "identifii": 114, "idnam": 64, "idx_gt_att": 16, "idx_learn": 79, "idx_tau": [85, 95, 99], "idx_treat": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 79, 80, 162, 168], "ieee": 176, "ifels": 64, "ignor": [50, 51, 55, 56, 72, 77, 78, 79, 82, 97, 100], "ignore_index": 79, "ii": [65, 93], "iid": [71, 74, 127, 129, 140], "iivm": [20, 21, 25, 33, 95, 105, 114, 136, 151, 173, 177], "iivm_summari": 94, "iivmglmnet": 66, "iivmrang": 66, "iivmrpart": 66, "iivmxgboost11861": 66, "ij": [44, 65, 68, 80, 93, 104], "ilia": 176, "illustr": [63, 65, 66, 67, 68, 69, 70, 72, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 97, 98, 99, 102, 103, 104, 106, 115, 116, 117, 120, 121, 124, 126, 178], "iloc": [69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 93, 96, 97, 101], "immedi": 174, "immun": [143, 176], "impact": [62, 77, 81, 96, 102], "implement": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 55, 56, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 82, 86, 90, 93, 94, 96, 97, 100, 102, 103, 104, 106, 113, 114, 115, 118, 121, 122, 126, 128, 129, 131, 133, 140, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 175, 176, 177, 178], "impli": [31, 41, 65, 66, 71, 74, 93, 94, 95, 100, 114, 127, 133, 162, 164, 165, 166, 167, 169, 170], "implicitli": [127, 131], "implment": [70, 78, 127, 128], "import": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 57, 58, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 119, 120, 121, 123, 124, 127, 128, 129, 130, 132, 143, 144, 161, 162, 168, 174, 175, 177, 178], "importlib": 86, "impos": 103, "improv": [69, 71, 74, 77, 98, 127, 177], "in_sample_norm": [12, 14, 15, 16, 69, 144, 145, 146, 147, 148, 162, 164, 165, 166, 167], "inbuild": 77, "inbuilt": 77, "inc": [66, 94, 95, 102, 178], "includ": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 52, 59, 64, 66, 70, 71, 72, 73, 74, 78, 79, 80, 87, 88, 90, 94, 97, 100, 102, 103, 114, 127, 161, 162, 168, 169, 170, 172, 174, 177, 178], "include_bia": [90, 93, 94, 97], "include_never_tr": [17, 19, 74], "include_scenario": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 103], "incom": [66, 94, 95, 98, 102, 178], "incorpor": [67, 97, 102, 162, 168], "increas": [19, 71, 74, 77, 89, 91, 93, 103, 178], "increment": 177, "ind": 94, "independ": [12, 14, 15, 16, 18, 31, 32, 41, 49, 65, 67, 70, 89, 91, 93, 98, 127, 133, 140, 142, 144, 145, 146, 147, 148, 177], "index": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 63, 65, 70, 75, 78, 79, 82, 86, 87, 88, 92, 93, 94, 96, 97, 98, 106, 107, 112, 143, 144, 145, 146, 147, 148, 173, 175], "index_col": 86, "india": [143, 176], "indic": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 36, 37, 38, 39, 40, 48, 52, 53, 65, 66, 68, 70, 71, 72, 73, 74, 89, 93, 94, 95, 97, 100, 101, 103, 104, 105, 107, 108, 109, 111, 112, 114, 127, 129, 133, 134, 142, 143, 144, 157], "indices_x": 97, "indices_x_pr": 97, "indices_x_tr": 97, "individu": [17, 19, 22, 23, 26, 39, 55, 56, 64, 66, 70, 71, 74, 78, 79, 80, 87, 88, 89, 92, 94, 95, 97, 100, 102, 114, 127, 140, 178], "individual_df": 70, "induc": [113, 143], "industri": [65, 93], "inf": [5, 6, 7, 8, 9, 64, 71, 72, 73, 74, 101], "inf_model": 144, "infer": [34, 43, 62, 63, 65, 72, 77, 79, 81, 82, 86, 92, 93, 101, 106, 113, 127, 143, 173, 175, 176, 177], "inferenti": 178, "infinit": [5, 6, 7, 8, 9, 101, 107, 108, 112, 177], "influenc": [25, 51, 56, 79, 97, 127, 140], "info": [62, 67, 71, 72, 73, 74, 75, 78, 79, 80, 89, 91, 92, 93, 94, 95, 102, 107, 109, 112, 123, 175, 177, 178], "inform": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 50, 51, 53, 55, 56, 62, 67, 71, 72, 73, 74, 77, 81, 83, 84, 97, 100, 101, 102, 103, 127, 128, 162, 163, 176], "infti": [63, 82, 106, 127, 133], "inher": 103, "inherit": [96, 108, 109, 110, 111, 112, 177], "initi": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 66, 67, 68, 69, 71, 72, 73, 74, 78, 79, 85, 94, 95, 99, 100, 102, 103, 104, 107, 109, 112, 114, 115, 117, 119, 124, 126, 127, 143, 175, 177, 178], "inlcud": [97, 127, 140], "inlin": [75, 96], "inlinebackend": 96, "inner": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 115, 117, 119, 124], "innermost": [115, 124], "input": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 51, 56, 67, 72, 78, 102, 105, 115, 120, 123, 127, 131, 140, 161, 162, 163, 168], "input_featur": 97, "insensit": 127, "insid": [50, 51, 55, 56], "insight": [86, 103], "insignific": 102, "inspect": [78, 175], "inspir": [31, 33, 34, 36, 71, 74, 103], "instabl": 58, "instal": [66, 79, 92, 100, 127, 177], "install_github": 174, "instanc": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 52, 55, 56, 66, 67, 79, 94, 115, 123], "instanti": [65, 66, 93, 94, 115, 122, 125, 143], "instead": [4, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 50, 51, 55, 56, 62, 64, 66, 70, 71, 72, 73, 74, 78, 79, 80, 81, 83, 89, 92, 94, 95, 107, 112, 114, 115, 118, 127, 144, 148, 162, 164, 165, 166, 167, 170, 171, 177], "instruct": [174, 177], "instrument": [5, 6, 7, 8, 9, 10, 25, 33, 37, 38, 43, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 78, 79, 80, 89, 91, 93, 94, 95, 97, 99, 102, 104, 107, 109, 110, 111, 112, 115, 116, 127, 129, 130, 132, 136, 139, 144, 153, 155, 161, 175, 178], "instrument_effect": 62, "instrument_impact": 81, "instrument_strength": 101, "insuffienct": 92, "int": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 41, 42, 45, 48, 49, 53, 54, 57, 64, 65, 66, 69, 72, 78, 79, 81, 85, 97, 98, 99, 101, 103, 104, 109, 112], "int32": 72, "int64": [71, 73, 74, 75, 93, 107, 109, 112, 175], "int8": [94, 95, 102], "integ": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 67, 97, 115, 123, 124], "integr": [79, 92, 103, 162, 172, 177], "intend": [48, 67, 103, 178], "intent": [127, 178], "inter": [115, 117], "interact": [22, 23, 25, 26, 31, 33, 34, 35, 48, 49, 78, 80, 97, 103, 113, 115, 122, 136, 137, 169, 170, 173, 177, 178], "interaction_onli": 97, "interc": 97, "interchang": 161, "interest": [25, 26, 31, 37, 38, 39, 40, 41, 63, 66, 68, 69, 82, 86, 91, 94, 95, 100, 101, 104, 106, 114, 127, 129, 131, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 161, 175, 178], "interfac": [37, 64, 66, 67, 79, 107, 112, 115, 126, 143, 175], "intermedi": 103, "intern": [19, 37, 64, 66, 67, 80, 92, 95, 115, 117, 120, 126, 127, 128, 140, 176], "internet": [66, 94, 95], "interpret": [71, 72, 73, 74, 87, 88, 91, 101, 103, 114, 162, 163, 169, 170, 171, 172, 174, 178], "intersect": [103, 162, 168, 177], "interv": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 53, 64, 65, 66, 68, 69, 71, 72, 73, 74, 78, 79, 80, 83, 84, 85, 87, 88, 90, 93, 95, 99, 100, 102, 104, 113, 114, 143, 144, 162, 168, 175, 176, 177, 178], "intial": 100, "introduc": [63, 82, 97, 106, 107, 112, 161, 177, 178], "introduct": [63, 65, 67, 82, 93, 95, 102, 115, 122, 123, 126, 127, 129, 133, 162, 163], "introductori": [64, 103], "intrument": [68, 104], "intspecifi": 48, "intuit": 103, "inuidur1": [67, 75, 107, 112, 175], "inuidur1femaleblackothracedep1dep2q2q3q4q5q6agelt35agegt54durablelusdhusdtg": [67, 107, 112, 175], "inuidur2": [75, 107, 112, 175], "inv_sigmoid": 96, "invalid": [63, 74, 82, 93, 101, 106], "invari": [97, 127, 129, 133, 140], "invers": [22, 23, 24, 25, 26, 27, 28, 29, 30, 68, 104, 162, 169, 170], "invert": [25, 101], "invert_yaxi": 93, "investig": [86, 92, 103], "involv": [114, 115, 125, 144, 178], "io": [78, 96, 177], "ipw_norm": 177, "ipykernel_49369": 70, "ipykernel_52138": 83, "ipykernel_53763": 93, "ipynb": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104], "ira": [66, 94, 95], "irm": [0, 9, 12, 14, 15, 16, 20, 21, 37, 38, 39, 40, 53, 54, 77, 78, 79, 80, 84, 88, 89, 103, 104, 105, 111, 112, 113, 115, 116, 117, 130, 132, 137, 152, 169, 170, 173, 177, 178], "irm_summari": 94, "irmglmnet": 66, "irmrang": 66, "irmrpart": 66, "irmxgboost8047": 66, "irrespect": 103, "irrevers": [127, 133], "is_classifi": [12, 14, 15, 22, 23, 25, 26, 39, 40], "is_gat": [22, 26, 40, 53], "isfinit": [71, 72, 73, 74], "isnan": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 77, 115, 116], "isoton": [57, 58, 103], "isotonicregress": 103, "issn": 86, "issu": [13, 16, 103, 173, 176, 177], "ite": [71, 74, 78, 79, 80, 87, 88, 89], "ite_lower_quantil": [71, 74], "ite_mean": [71, 74], "ite_upper_quantil": [71, 74], "item": [25, 79, 94, 105, 115, 124, 126, 143], "iter": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 62, 68, 69, 93, 94, 100, 104, 115, 121, 123, 127, 140, 161, 178], "itertool": 86, "its": [50, 51, 79, 103, 105, 114, 115, 122, 127, 129, 130, 132, 143, 144, 161], "iv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 37, 38, 39, 40, 43, 44, 63, 65, 82, 93, 106, 107, 112, 136, 139, 156, 157, 158, 162, 171, 173, 177, 178], "iv_2": 62, "iv_var": [65, 93], "iv\u00e1n": [143, 176], "j": [10, 11, 17, 18, 19, 31, 33, 34, 36, 41, 42, 43, 44, 46, 47, 63, 64, 65, 67, 68, 80, 82, 86, 93, 96, 101, 104, 106, 115, 125, 127, 134, 161, 173, 175], "j_": [65, 93], "j_0": 161, "j_1": [65, 93], "j_2": [65, 93], "j_3": [65, 93], "j_k": [65, 93], "jame": 176, "jan": 177, "janari": [71, 74], "janni": [66, 94], "januari": 71, "jasenakova": 177, "javanmard": 176, "jbe": [65, 93], "jeconom": [17, 18, 19, 31, 41, 64], "jerzi": 176, "jia": 103, "jitter": [16, 78, 79], "jitter_strength": [78, 79], "jitter_valu": 16, "jk": [127, 135], "jmlr": [67, 173, 175, 177], "job": [66, 94, 95], "john": 176, "joint": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 53, 71, 72, 73, 74, 80, 83, 84, 85, 87, 88, 90, 95, 99, 101, 127, 129, 161, 177, 178], "joint_prob": 91, "jointli": [99, 114], "jonathan": 176, "joss": [67, 115, 125, 173, 175], "journal": [10, 11, 17, 18, 19, 31, 36, 41, 42, 44, 45, 46, 64, 65, 67, 86, 93, 96, 101, 103, 106, 115, 125, 173, 175, 176, 177], "jss": 173, "juliu": 177, "jump": [98, 100, 127], "jun": [64, 176], "jupyt": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104], "juraj": 176, "just": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 64, 67, 69, 70, 71, 74, 78, 80, 85, 87, 88, 89, 90, 98, 99, 127, 144, 145, 146, 147, 148, 162, 163], "justif": [143, 162, 163], "k": [10, 17, 18, 19, 33, 34, 36, 41, 42, 43, 44, 45, 46, 63, 65, 67, 77, 82, 92, 93, 97, 100, 101, 105, 106, 113, 114, 127, 140, 161, 178], "k_h": [100, 127], "kaggl": [66, 94], "kallu": [85, 95, 99, 101, 102, 144, 150, 153, 154, 176], "kappa": 127, "kato": [44, 65, 93, 161, 176], "kb": [72, 73, 78, 79, 80, 89, 91, 93, 94, 95, 102, 107, 109, 112, 175], "kde": [27, 28, 29, 94], "kdeplot": [69, 77, 104], "kdeunivari": [27, 28, 29], "kecsk\u00e9sov\u00e1": 177, "keel": 101, "keep": [51, 56, 64, 78, 90, 103, 111, 112, 178], "kei": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 54, 65, 66, 78, 83, 84, 87, 88, 92, 93, 94, 95, 100, 103, 115, 117, 126, 127, 144, 162, 168, 177], "keith": 176, "kelz": 101, "kengo": 176, "kennedi": 101, "kept": [110, 112], "kernel": [27, 28, 29, 48, 51, 56, 100, 127], "kernel_regress": 100, "kernelreg": 100, "keyword": [17, 18, 19, 22, 26, 40, 44, 46, 47, 49, 53], "kf": 143, "kfold": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 93, 143], "kind": [62, 81, 94], "kj": [17, 18, 19, 33, 34, 36, 41, 42, 43, 44, 46, 63, 65, 82, 93, 106], "klaassen": [33, 77, 86, 92, 103, 173, 176], "klaa\u00dfen": 33, "kluge": 177, "knau": 176, "know": [69, 98, 101], "knowledg": [62, 77, 81, 96, 98], "known": [77, 89, 100, 101, 103, 115, 116, 127, 133], "kohei": 176, "kotthof": 67, "kotthoff": [67, 115, 125, 173, 175], "krueger": 96, "kueck": [66, 94], "kurz": [173, 176, 177], "kwarg": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 41, 44, 46, 47, 48, 49, 50, 53, 92, 127], "l": [65, 67, 68, 75, 83, 84, 93, 101, 103, 104, 115, 125, 162, 171, 173, 175], "l1": [94, 104, 127], "l_": [97, 127, 140], "l_hat": [38, 40, 63, 82, 144], "lab": 68, "label": [13, 16, 50, 55, 70, 78, 79, 80, 82, 83, 84, 85, 87, 88, 90, 91, 92, 95, 96, 97, 99, 100], "labor": 96, "laffer": 176, "laff\u00e9r": [36, 68, 104, 127, 142, 144, 159, 160], "lag": [97, 127, 140], "lal": [96, 177], "lambda": [65, 66, 67, 68, 71, 74, 94, 96, 98, 115, 123, 125, 126, 127, 144, 145, 146, 161, 175], "lambda_": 86, "lambda_0": [144, 145, 146], "lambda_l1": [78, 79], "lambda_l2": [78, 79], "lambda_t": [18, 19, 74], "land": 98, "lang": [67, 115, 125, 173, 175], "langl": [32, 98], "lanni": 101, "lappli": 143, "larg": [63, 77, 82, 89, 92, 96, 103, 127], "larger": [26, 42, 64, 100, 103, 162, 168], "largest": 77, "largli": 77, "lasso": [65, 66, 67, 68, 72, 94, 97, 104, 115, 120, 121, 125, 126, 175, 176], "lasso_class": [66, 94], "lasso_pip": [67, 115, 125], "lasso_summari": 94, "lassocv": [30, 39, 72, 86, 93, 94, 97, 104, 115, 121, 127, 161, 175], "lassocvlassocv": 97, "last": [18, 67, 97, 174], "late": [25, 62, 66, 94, 101, 127, 136, 144, 151], "latent": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 41, 102, 162, 171, 172], "later": [66, 67, 78, 100, 103, 115, 126, 178], "latest": 173, "latter": [7, 55, 56, 101, 127, 140], "layout": 86, "lbrace": [25, 26, 33, 34, 36, 65, 93, 105, 127, 134, 136, 137, 143, 144, 149, 161, 162, 169], "ldot": [37, 38, 39, 40, 65, 68, 91, 93, 104, 105, 127, 138, 139, 140, 141, 143, 161, 175], "le": [18, 69, 98, 114, 127, 129, 144, 153, 154], "lead": [64, 97, 103, 127], "leadsto": 161, "lear": [67, 115, 125, 173, 175], "learn": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 40, 42, 44, 45, 46, 47, 48, 62, 66, 67, 72, 75, 77, 78, 80, 81, 85, 86, 90, 92, 94, 95, 96, 97, 99, 100, 101, 103, 107, 112, 113, 115, 118, 127, 140, 143, 144, 161, 162, 163, 177, 178], "learner": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 52, 57, 62, 63, 64, 65, 66, 68, 69, 71, 72, 73, 74, 79, 82, 83, 84, 86, 91, 93, 94, 95, 97, 101, 102, 103, 104, 105, 106, 113, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 132, 143, 144, 161, 162, 168, 177, 178], "learner_class": [30, 177], "learner_cv": 67, "learner_dict": 79, "learner_forest_classif": 67, "learner_forest_regr": 67, "learner_l": 102, "learner_lasso": 67, "learner_list": 77, "learner_m": 102, "learner_nam": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 52, 57, 78, 79, 115, 126], "learner_pair": 79, "learner_param_v": 67, "learner_rf": 161, "learnerclassif": 67, "learnerregr": 67, "learnerregrcvglmnet": 67, "learnerregrrang": [67, 115, 124], "learning_r": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 71, 74, 78, 79, 82, 85, 95, 97, 99, 100, 103, 106], "learnt": [97, 127, 140], "least": [62, 66, 81, 94, 95, 102, 127, 131, 143], "leav": [68, 103, 104], "left": [17, 19, 33, 34, 36, 43, 44, 45, 63, 65, 77, 79, 80, 82, 93, 94, 95, 96, 99, 100, 106, 127, 144, 145, 146, 147, 148, 161, 162, 164, 165, 166, 167, 169, 170], "legend": [66, 70, 71, 74, 77, 78, 79, 80, 82, 83, 84, 85, 87, 88, 90, 91, 95, 96, 97, 99], "lemp": 72, "len": [77, 78, 79, 80, 85, 92, 93, 95, 97, 99, 101], "length": [12, 13, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 63, 67, 69, 71, 74, 101, 115, 119, 124], "leq": [65, 93], "less": [64, 66, 94, 95, 100, 103], "lester": 176, "let": [17, 18, 19, 31, 35, 41, 63, 64, 66, 67, 68, 69, 71, 74, 77, 78, 79, 80, 82, 85, 87, 88, 90, 94, 95, 99, 103, 104, 105, 106, 115, 119, 124, 127, 129, 133, 140, 142, 162, 163, 172, 178], "level": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 40, 45, 48, 53, 65, 66, 68, 69, 70, 71, 72, 73, 74, 78, 79, 80, 83, 84, 85, 87, 88, 89, 90, 93, 94, 95, 99, 101, 102, 103, 104, 115, 117, 134, 135, 144, 149, 162, 168, 169, 178], "level_0": [67, 93], "level_1": 93, "level_bound": [78, 79, 80], "leverag": 79, "levi": 101, "levinsohn": [65, 93], "lewi": 176, "lgbm": [78, 79], "lgbm_argument": 79, "lgbmclassifi": [69, 70, 71, 74, 77, 78, 79, 85, 95, 99, 100, 103], "lgbmlgbmregressorlgbmregressor": 78, "lgbmregressor": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 69, 70, 71, 74, 77, 78, 79, 82, 85, 95, 97, 100, 103, 106], "lgr": [63, 64, 65, 66, 67, 68, 105, 115, 124, 125, 126, 127, 143, 144, 161, 175], "lib": [70, 71, 72, 73, 74, 93, 94, 100, 103], "liblinear": [94, 104, 127], "librari": [62, 63, 64, 65, 66, 67, 68, 79, 105, 106, 107, 112, 115, 123, 124, 125, 126, 127, 143, 144, 161, 174, 175, 178], "licens": [173, 177], "lie": 176, "lightgbm": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 69, 70, 71, 74, 77, 78, 79, 82, 85, 95, 97, 99, 100, 103], "like": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 50, 51, 55, 56, 64, 66, 67, 71, 72, 73, 74, 78, 86, 94, 95, 103, 115, 118, 119, 121, 127, 128, 143, 175, 178], "lim": 96, "lim_": [100, 127], "limegreen": [83, 84], "limit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 96, 127, 133, 176], "limits_": 114, "lin": [100, 103, 127], "line": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 70, 72, 78, 79, 80, 91, 97, 103], "line_width": [78, 79], "linear": [17, 19, 20, 21, 22, 26, 31, 35, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 53, 62, 63, 64, 65, 67, 69, 70, 72, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 90, 91, 92, 93, 97, 101, 102, 103, 105, 106, 113, 114, 115, 119, 120, 121, 124, 126, 130, 131, 132, 138, 139, 140, 141, 143, 145, 146, 147, 148, 149, 151, 152, 156, 157, 158, 161, 168, 170, 171, 172, 173, 175, 176, 177, 178], "linear_learn": [71, 74], "linear_model": [22, 26, 30, 39, 40, 53, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 86, 90, 93, 94, 97, 100, 101, 103, 104, 115, 120, 121, 127, 161, 175], "linear_regress": 78, "linear_regressionlinearregress": 78, "linearli": [100, 127], "linearregress": [62, 71, 72, 73, 74, 77, 78, 79, 80, 81, 90, 100, 101, 103], "linearregressionlinearregress": 78, "linearscoremixin": [0, 144], "lineplot": [71, 74, 79, 80], "linestyl": [70, 71, 74, 78, 79, 80, 91, 92, 97, 100], "linetyp": 68, "linewidth": [70, 71, 74, 78, 79, 97], "link": [91, 103, 127, 138, 140, 177], "linspac": [83, 84, 90, 103], "lint": 177, "linux": 174, "list": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 51, 56, 57, 63, 64, 65, 66, 67, 71, 72, 74, 78, 82, 83, 84, 93, 95, 98, 106, 115, 119, 122, 123, 124, 125, 126, 143, 144, 174, 177], "list_confset": 25, "listedcolormap": 93, "literatur": [103, 127, 129, 133], "littl": [89, 101], "liu": [42, 91, 144, 155, 176], "ll": [67, 161, 178], "lllllllllllllllll": [107, 112, 175], "lm": [62, 64, 103], "ln_alpha_ml_l": 86, "ln_alpha_ml_m": 86, "load": [62, 64, 66, 67, 78, 86, 94, 95, 97, 107, 112, 174, 175], "loader": 0, "loc": [70, 71, 72, 73, 74, 78, 79, 80, 82, 85, 86, 87, 88, 91, 93, 96, 97, 99, 102, 103], "local": [25, 27, 78, 101, 114, 127, 136, 176, 177], "localconvert": 93, "locat": [85, 99, 127], "log": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 65, 69, 71, 72, 73, 74, 77, 78, 79, 86, 93, 95, 96, 97, 102, 104, 115, 116, 120, 127, 129, 130, 132], "log_odd": 98, "log_p": [65, 93], "log_reg": [62, 64], "logarithm": [79, 86], "logic": [25, 67, 115, 123, 124], "logical_not": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 77, 115, 116], "logist": [31, 37, 42, 49, 62, 64, 66, 68, 72, 80, 81, 94, 101, 103, 104, 138, 176, 177, 178], "logistic_regress": 78, "logisticregress": [62, 71, 72, 73, 74, 75, 78, 79, 80, 81, 90, 100, 101, 103], "logisticregressioncv": [30, 72, 77, 94, 104, 127], "logit": [17, 19, 37, 77, 96, 144, 155], "loglik": 67, "logloss": [66, 79, 94, 178], "logloss_m": 79, "logo": 177, "logspac": 94, "long": [7, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 41, 63, 72, 77, 82, 102, 103, 162, 163, 172, 176], "longer": [71, 74, 107, 112, 127, 131], "look": [64, 66, 67, 69, 70, 71, 72, 73, 74, 77, 78, 85, 91, 94, 95, 97, 99, 100, 102], "loop": [80, 101], "loss": [69, 71, 72, 73, 74, 77, 78, 79, 92, 95, 100, 102, 104, 115, 116, 127, 129, 130, 132], "loss_ml_g0": 77, "loss_ml_g1": 77, "loss_ml_m": 77, "low": [70, 89, 101, 114, 176], "lower": [25, 66, 67, 70, 71, 74, 79, 80, 85, 86, 89, 90, 95, 96, 99, 100, 101, 102, 103, 115, 125, 126, 162, 168, 172, 178], "lower_bound": [83, 84], "lplr": [91, 138, 155], "lpop": 72, "lpq": [27, 29, 95, 114, 153, 177], "lpq_0": 99, "lpq_1": 99, "lqte": 114, "lr": 100, "lrn": [62, 63, 64, 65, 66, 67, 68, 105, 115, 122, 123, 124, 125, 126, 127, 143, 144, 161, 175, 178], "lrn_0": 67, "lt": [62, 64, 65, 66, 67, 68, 71, 72, 73, 74, 75, 78, 79, 80, 89, 91, 93, 94, 95, 98, 102, 103, 107, 112, 175], "lucien": 177, "luka": 176, "luk\u00e1\u0161": 36, "lusd": [67, 75, 107, 112, 175], "lvert": 86, "m": [7, 10, 11, 16, 30, 31, 37, 43, 44, 46, 63, 65, 67, 71, 74, 75, 77, 79, 82, 86, 89, 92, 93, 96, 97, 101, 106, 109, 112, 113, 114, 115, 125, 127, 128, 130, 132, 140, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 162, 164, 165, 166, 167, 169, 170, 171, 172, 173, 174, 175, 176, 177], "m_": [79, 80, 127, 130, 132, 134, 144, 146, 148, 149, 153, 161], "m_0": [12, 14, 15, 16, 22, 23, 24, 25, 26, 28, 37, 38, 39, 40, 45, 46, 47, 48, 63, 65, 66, 82, 86, 89, 92, 93, 94, 106, 114, 115, 119, 124, 127, 136, 137, 139, 141, 144, 145, 146, 147, 148, 150, 153, 154, 155, 157, 158, 159, 160, 175, 178], "m_1": [97, 127, 140], "m_hat": [25, 26, 38, 40, 63, 82, 90, 144], "m_i": [100, 127], "ma": [44, 65, 93, 101, 127, 128, 176], "mac": 174, "machin": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 40, 42, 44, 45, 46, 47, 48, 62, 66, 67, 68, 69, 71, 72, 73, 74, 75, 77, 80, 81, 85, 86, 90, 92, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 113, 115, 116, 125, 127, 129, 130, 132, 140, 143, 144, 161, 162, 163, 177, 178], "machineri": [86, 176], "mackei": 176, "maco": 174, "made": [127, 142, 178], "mae": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 77, 115, 116], "maggi": 176, "magnitud": [42, 162, 163], "mai": [51, 56, 68, 69, 97, 104], "main": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 41, 73, 78, 86, 95, 103, 127, 161, 162, 163, 176, 178], "mainli": [77, 78, 103], "maintain": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 64, 173, 177], "mainten": 177, "major": [67, 103, 177], "make": [62, 77, 79, 80, 81, 92, 103, 114, 115, 122, 177, 178], "make_confounded_irm_data": [103, 177], "make_confounded_plr_data": 102, "make_did_cs2021": [7, 16, 71, 109, 112, 127, 128, 132], "make_did_cs_cs2021": [74, 127, 130], "make_did_sz2020": [5, 12, 15, 69, 108, 112, 127, 129], "make_heterogeneous_data": [83, 84, 87, 88, 89], "make_iivm_data": [25, 27, 114, 127], "make_irm_data": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 77, 90, 114, 115, 117, 127], "make_irm_data_discrete_treat": [78, 79, 80], "make_lplr_lzz2020": [37, 91, 127], "make_pipelin": [94, 97], "make_pliv_chs2015": [38, 127], "make_pliv_multiway_cluster_ckms2021": [65, 93], "make_plpr_cp2025": [39, 97, 109, 112, 127], "make_plr_ccddhnr2018": [6, 7, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 63, 82, 92, 105, 106, 114, 115, 116, 119, 123, 124, 126, 127, 143, 144, 161, 162, 168, 177], "make_simple_rdd_data": [8, 48, 100, 110, 112, 127], "make_spd_matrix": 47, "make_ssm_data": [9, 68, 104, 111, 112, 127], "malt": [173, 176], "man": [62, 81], "manag": [115, 126, 174], "mandatori": [110, 112], "mani": [20, 21, 43, 63, 64, 65, 67, 69, 82, 92, 93, 106, 144, 161, 178], "manili": 53, "manipul": [66, 67, 100, 127], "manual": [66, 90, 92, 102, 178], "mao": 176, "map": [25, 50, 51, 55, 56, 64, 65, 93, 114, 127, 136], "mapsto": [105, 114], "mar": [36, 127], "march": [77, 86, 92], "margin": [83, 84, 103], "marit": [66, 94], "marker": [71, 74, 80, 103], "markers": [78, 96, 97], "market": 96, "markettwo": 65, "markov": [47, 176], "marr": [66, 94, 95, 102, 178], "marshal": [115, 123], "martin": [36, 103, 173, 176, 177], "masatoshi": 176, "masip": [101, 177], "mask": 16, "maskedarrai": [127, 128], "master": [64, 72], "mat": 65, "match": [115, 126, 162, 171], "math": [30, 71, 72, 73, 74], "mathbb": [17, 18, 19, 20, 21, 25, 26, 31, 35, 38, 39, 40, 41, 42, 65, 68, 69, 70, 71, 74, 77, 78, 79, 80, 89, 91, 92, 93, 96, 97, 100, 104, 114, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145, 146, 147, 148, 149, 150, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 168, 169, 170, 171, 172, 175, 178], "mathcal": [17, 18, 19, 31, 32, 33, 34, 35, 36, 41, 42, 43, 44, 45, 46, 47, 49, 63, 65, 68, 70, 82, 85, 93, 98, 99, 104, 106, 127, 128, 131, 133], "mathop": 114, "mathrm": [31, 41, 71, 72, 73, 74, 100, 127, 128, 130, 131, 132, 133, 144, 146, 148, 162, 165, 167], "matia": 176, "matplotlib": [13, 16, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 103, 104], "matric": [98, 177], "matrix": [17, 18, 19, 31, 33, 34, 35, 36, 41, 43, 44, 46, 47, 51, 56, 63, 65, 66, 67, 68, 82, 93, 104, 106, 107, 112, 115, 125, 126, 161, 175, 177, 178], "matt": 176, "matter": [77, 96], "max": [17, 19, 66, 67, 78, 79, 90, 94, 95, 101, 105, 114, 115, 126, 127, 143, 144, 146, 148, 150, 161, 162, 165, 167, 175, 178], "max_depth": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 75, 78, 79, 94, 97, 102, 105, 114, 115, 117, 127, 129, 143, 144, 161, 162, 168, 175, 178], "max_featur": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 75, 94, 102, 105, 114, 115, 117, 127, 143, 144, 161, 162, 168, 175, 178], "max_it": [78, 79, 93, 94, 103], "maxim": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 98, 114, 127], "maxima": 161, "maximum": [57, 58, 114, 115, 126], "mb": [71, 74, 75, 107, 112, 175], "mb706": 177, "mea": 33, "mean": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 42, 50, 51, 55, 56, 62, 63, 65, 66, 69, 71, 74, 77, 78, 79, 80, 81, 82, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 97, 99, 101, 102, 103, 106, 115, 116, 126, 127, 140, 161, 178], "mean_absolute_error": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 77, 115, 116], "meant": [72, 114, 177], "measir": 102, "measur": [64, 67, 72, 86, 92, 101, 102, 103, 115, 125, 126, 127, 128, 142, 162, 163, 169, 170, 171, 172], "measure_col": 86, "measure_func": 64, "measure_pr": 64, "measures_r": 64, "mechan": [50, 51, 55, 56, 103, 127, 133], "median": [101, 103, 143], "medium": 79, "melt": 65, "membership": 103, "memori": [71, 72, 73, 74, 75, 78, 79, 80, 89, 91, 93, 94, 95, 102, 107, 109, 112, 175], "mention": [89, 114], "merg": [66, 94], "mert": [143, 176], "meshgrid": [83, 84, 103], "messag": [63, 64, 65, 66, 67, 68, 77, 79, 175, 177], "meta": [50, 51, 55, 56, 115, 175], "metadata": [50, 51, 55, 56], "metadata_rout": [50, 51, 55, 56], "metadatarequest": [50, 51, 55, 56], "method": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 77, 78, 80, 81, 82, 83, 84, 85, 87, 88, 90, 93, 94, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 124, 126, 127, 128, 129, 130, 132, 143, 144, 161, 162, 163, 168, 173, 175, 177], "methodolog": 176, "methodologi": 103, "metric": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 50, 55, 79, 115, 116], "michael": 176, "michaela": 177, "michel": [173, 175], "michela": [36, 176], "mid": [66, 94, 96, 97, 100, 127, 144, 157, 158], "mid_point": [78, 79, 80], "might": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 65, 72, 77, 85, 90, 93, 97, 98, 100, 102, 103, 115, 117, 127], "mild": [63, 82, 106], "militari": 96, "miller": [65, 93], "mimic": 103, "min": [17, 19, 65, 66, 67, 68, 71, 72, 73, 74, 78, 79, 85, 90, 93, 94, 95, 99, 100, 105, 115, 123, 126, 127, 131, 143, 144, 161, 175, 178], "min_": 114, "min_child_sampl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 78, 97], "min_data_in_leaf": 79, "min_samples_leaf": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 54, 79, 89, 91, 94, 102, 105, 114, 115, 117, 127, 129, 143, 144, 161, 162, 168, 178], "min_samples_split": [94, 127, 128, 130, 132], "minim": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 54, 66, 77, 94, 100, 127], "minimum": [57, 58, 72, 118, 122, 126], "minor": [63, 72, 82, 106, 144, 177], "minsplit": 66, "minut": 92, "mirror": [107, 112], "miruna": 176, "mislead": 177, "miss": [5, 6, 7, 8, 9, 30, 67, 107, 108, 112, 115, 123, 124, 127, 144, 159, 177], "missing": [36, 68, 104], "misspecif": 69, "misspecifi": 69, "mit": [173, 175], "mix": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "mixin": [0, 20, 21, 144], "ml": [47, 65, 66, 67, 72, 86, 92, 93, 94, 97, 100, 101, 105, 113, 115, 124, 126, 127, 143, 173, 176, 177], "ml_a": 37, "ml_boost": 97, "ml_g": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 52, 62, 63, 64, 66, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 85, 87, 89, 90, 94, 95, 96, 98, 99, 100, 101, 102, 103, 104, 106, 114, 115, 117, 123, 125, 127, 128, 129, 130, 132, 177], "ml_g0": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 52, 64, 66, 69, 71, 72, 73, 75, 77, 94, 102, 115, 117, 127, 129, 132], "ml_g1": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 64, 66, 69, 71, 72, 73, 75, 77, 94, 102, 115, 117, 127, 129, 132], "ml_g_d0": [104, 127], "ml_g_d0_t0": [69, 74, 127, 129, 130], "ml_g_d0_t1": [69, 74, 127, 129, 130], "ml_g_d1": [104, 127], "ml_g_d1_t0": [69, 74, 127, 129, 130], "ml_g_d1_t1": [69, 74, 127, 129, 130], "ml_g_d_lvl0": [78, 79, 127], "ml_g_d_lvl1": [78, 79, 127], "ml_g_param": 78, "ml_g_params_pipelin": 78, "ml_g_pipelin": 78, "ml_g_sim": 30, "ml_l": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 63, 65, 66, 67, 75, 82, 84, 88, 92, 93, 94, 96, 97, 102, 105, 106, 115, 116, 119, 120, 121, 123, 124, 126, 127, 143, 144, 161, 162, 168, 175, 177, 178], "ml_l_bonu": 175, "ml_l_forest": 67, "ml_l_forest_pip": 67, "ml_l_lasso": 67, "ml_l_lasso_pip": 67, "ml_l_param": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 115, 120], "ml_l_rf": 178, "ml_l_sim": 175, "ml_l_tune": [115, 121], "ml_l_xgb": 178, "ml_lasso": 97, "ml_lasso_alt": 97, "ml_lasso_wg": 97, "ml_m": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 114, 115, 116, 117, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 132, 143, 144, 161, 162, 168, 175, 177, 178], "ml_m_bench_control": 103, "ml_m_bench_treat": 103, "ml_m_bonu": 175, "ml_m_forest": 67, "ml_m_forest_pip": 67, "ml_m_lasso": 67, "ml_m_lasso_pip": 67, "ml_m_param": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 78, 115, 120], "ml_m_params_pipelin": 78, "ml_m_pipelin": 78, "ml_m_rf": 178, "ml_m_sim": [30, 175], "ml_m_studi": 78, "ml_m_tune": [115, 121], "ml_m_xgb": 178, "ml_param": 97, "ml_param_spac": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 78, 97, 115, 120], "ml_pi": [30, 68, 104, 127], "ml_pi_sim": 30, "ml_r": [25, 38, 62, 65, 66, 81, 93, 94, 101, 127, 177], "ml_r0": 127, "ml_r1": [66, 94, 127], "ml_t": [37, 91, 127], "mlr": [67, 115, 125], "mlr3": [62, 63, 64, 65, 66, 68, 105, 115, 122, 123, 124, 125, 126, 127, 143, 144, 161, 173, 175, 177, 178], "mlr3book": [67, 115, 123, 125, 126], "mlr3extralearn": [66, 115, 122], "mlr3filter": 67, "mlr3learner": [62, 63, 64, 65, 66, 105, 115, 122, 123, 124, 126, 127, 143, 144, 161, 175, 178], "mlr3measur": 64, "mlr3pipelin": [115, 122, 123, 125, 177], "mlr3tune": [67, 115, 125, 126, 177], "mlr3vers": 66, "mlrmeasur": 64, "mode": [103, 174], "model": [0, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 51, 53, 54, 56, 59, 62, 63, 64, 65, 67, 69, 70, 71, 72, 73, 74, 77, 81, 82, 85, 86, 89, 93, 95, 99, 102, 105, 106, 107, 109, 111, 112, 113, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 126, 129, 130, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 146, 147, 148, 149, 151, 152, 155, 156, 157, 158, 163, 168, 169, 170, 171, 172, 173, 176, 177], "model_data": [66, 94], "model_label": 92, "model_list": 79, "model_select": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 82, 93, 115, 121, 143], "modellist": [79, 90], "modelmlestimatelowerupp": 66, "modelvers": 79, "modern": [67, 115, 125, 173, 175], "modul": [79, 100, 112, 127, 174], "molei": [42, 176], "moment": [20, 21, 65, 93, 127, 130, 131, 132, 144, 161, 162, 163, 172, 175], "monoton": 127, "mont": [31, 32, 35, 41, 83, 84, 87, 88], "montanari": 176, "month": [71, 74], "more": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 42, 53, 62, 64, 66, 71, 72, 73, 74, 77, 78, 79, 80, 81, 83, 84, 86, 90, 92, 94, 95, 97, 100, 102, 103, 105, 114, 115, 116, 120, 121, 123, 125, 126, 127, 128, 129, 130, 131, 132, 133, 142, 144, 152, 161, 162, 163, 168, 172, 175, 178], "moreov": [66, 67, 72, 86, 97, 115, 123, 124, 161, 178], "mortgag": [66, 94, 95], "most": [66, 77, 85, 94, 95, 99, 103, 114, 115, 118, 120, 127, 162, 168, 174], "motiv": [103, 106], "motivation_example_bch": 86, "mp": 64, "mpd": [65, 93], "mpdta": 72, "mpg": 93, "mse": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 67, 86, 115, 125, 126], "mserd": 100, "msr": [67, 115, 125, 126], "mtry": [66, 67, 105, 115, 126, 127, 143, 144, 161, 178], "mu": 70, "mu_": 70, "mu_0": 127, "mu_mean": 70, "much": [66, 67, 78, 79, 94, 100, 101, 103, 178], "muld": [75, 107, 112, 175], "multi": [16, 50, 55, 64, 65, 83, 84, 93, 144, 146, 148, 177], "multiclass": [67, 92], "multiindex": 93, "multioutput": [51, 56], "multioutputregressor": [51, 56], "multipl": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 40, 64, 65, 66, 68, 69, 72, 73, 78, 79, 90, 93, 94, 97, 102, 103, 104, 107, 112, 115, 119, 124, 125, 131, 135, 139, 143, 161, 162, 163, 176, 177, 178], "multipletest": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "multipli": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 82, 113, 114, 144, 178], "multiprocess": [85, 95, 99], "multitest": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "multivari": [39, 97, 127, 140], "multivariate_norm": 30, "multiwai": [44, 65, 93, 176], "mundlak": [127, 140], "music": 176, "must": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 58, 91, 107, 108, 112, 115, 124, 126, 127], "mutat": 67, "mutual": [22, 26, 40, 66, 87, 88, 94, 95, 114], "my_sampl": 143, "my_task": 143, "n": [12, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 62, 63, 65, 67, 68, 70, 71, 74, 78, 79, 80, 81, 82, 85, 86, 89, 93, 96, 97, 98, 99, 100, 101, 104, 105, 106, 114, 115, 125, 127, 133, 140, 143, 161, 173, 174], "n_": [35, 70, 74, 162, 165, 167], "n_aggreg": 13, "n_alpha": 97, "n_coef": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 162, 168], "n_color": [71, 74], "n_complier": 99, "n_core": [85, 95, 99], "n_estim": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 69, 70, 71, 74, 75, 78, 79, 82, 83, 84, 85, 87, 88, 89, 94, 95, 97, 98, 99, 100, 102, 103, 105, 106, 114, 115, 117, 119, 127, 129, 143, 144, 161, 162, 168, 175, 178], "n_eval": [67, 115, 125, 126], "n_featur": [50, 51, 55, 56], "n_features_in_": 97, "n_fold": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 63, 64, 65, 66, 69, 71, 72, 74, 75, 77, 82, 83, 84, 85, 87, 88, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 106, 115, 119, 123, 124, 126, 143, 175, 178], "n_folds_inn": 37, "n_folds_per_clust": [65, 93], "n_folds_tun": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40], "n_framework": 13, "n_iter": [48, 100, 127], "n_iter_randomized_search": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 115, 121], "n_job": [85, 94, 95, 97, 99], "n_jobs_cv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 77], "n_jobs_model": [16, 23, 29, 85, 95, 99], "n_jobs_optuna": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "n_learner": 79, "n_level": [35, 78, 79, 80], "n_model": 78, "n_ob": [7, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 49, 53, 54, 63, 67, 68, 69, 70, 71, 74, 77, 78, 79, 80, 82, 83, 84, 87, 88, 89, 90, 91, 92, 100, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 114, 115, 117, 120, 121, 125, 126, 127, 128, 129, 130, 132, 143, 161, 162, 168, 175], "n_output": [50, 51, 55, 56], "n_period": [17, 19, 71, 74], "n_pre_treat_period": [17, 19, 71, 74], "n_rep": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 63, 64, 65, 68, 69, 71, 74, 75, 77, 79, 80, 82, 89, 90, 93, 100, 102, 103, 104, 106, 115, 116, 117, 119, 123, 124, 143, 162, 168, 175, 178], "n_rep_boot": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 53, 71, 72, 73, 74, 80, 83, 84, 85, 87, 88, 90, 95, 99, 161], "n_sampl": [50, 51, 55, 56, 98, 101], "n_samples_fit": [51, 56], "n_split": 143, "n_t": 70, "n_target": [55, 56], "n_theta": 13, "n_time_period": 70, "n_trial": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 78, 97, 115, 120], "n_true": [85, 99], "n_var": [63, 67, 82, 106, 107, 112, 115, 120, 121, 125, 126, 161, 175], "n_w": 98, "n_x": [32, 83, 84, 87, 88, 89], "na": [5, 6, 7, 8, 9, 63, 65, 68, 106, 177], "na_real_": [65, 177], "naiv": [63, 82, 106], "name": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 50, 51, 52, 55, 56, 57, 63, 64, 65, 70, 71, 72, 74, 78, 79, 87, 88, 89, 92, 93, 97, 100, 102, 103, 115, 117, 124, 126, 174, 177], "named_step": 97, "namespac": 64, "nan": [5, 6, 7, 8, 9, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 69, 70, 77, 78, 79, 80, 82, 85, 87, 88, 92, 94, 95, 99, 104, 106, 115, 116], "nanmean": 82, "narita": 176, "nat": [71, 74], "nathan": 176, "nation": [103, 143, 176], "nativ": 64, "natt": 98, "natur": 103, "nbest": 79, "nbviewer": [78, 97], "ncol": [65, 66, 67, 91, 100, 107, 112, 115, 125, 126, 161, 175], "ncoverag": 77, "ndarrai": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 30, 33, 34, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 57, 107, 112], "nearli": 77, "necess": [65, 93], "necessari": [64, 65, 79, 92, 93, 100, 127, 174], "need": [12, 14, 15, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 62, 63, 64, 66, 68, 79, 81, 82, 92, 95, 97, 101, 104, 115, 118, 119, 124, 143, 144, 155, 162, 172, 177, 178], "neg": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 51, 56], "neg_log_loss": 78, "neg_mean_squared_error": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "neighborhood": [100, 161], "neither": [5, 6, 7, 8, 9, 65, 93, 101, 107, 112], "neng": 176, "neq": [79, 97, 100, 127, 140], "nest": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 55, 56, 115, 117, 119, 124, 144, 160, 162, 168], "net": [95, 102, 178], "net_tfa": [66, 94, 95, 102, 178], "network": 79, "nev": [127, 130, 132, 133], "never": [17, 19, 25, 64, 65, 71, 72, 73, 74, 93, 127, 133, 177], "never_tak": [25, 66, 94], "never_tr": [14, 16, 71, 72, 73, 74, 127, 128, 130, 132], "nevertheless": 90, "new": [62, 63, 64, 65, 66, 67, 68, 83, 84, 92, 94, 98, 105, 106, 107, 112, 114, 115, 123, 124, 125, 126, 127, 143, 144, 161, 173, 175, 176, 177, 178], "new_data": [83, 84, 98], "newei": [10, 11, 46, 65, 86, 93, 103, 106, 173, 176], "newest": 177, "next": [64, 66, 67, 77, 83, 84, 85, 89, 91, 94, 95, 98, 99, 101, 103, 177], "neyman": [65, 93, 105, 113, 162, 172, 173, 176], "nfold": [65, 66, 68, 127], "nh": 127, "nice": [64, 79], "nifa": [94, 95, 102], "nil": 103, "nine": [65, 93], "nlogloss": 79, "nn": 100, "noack": [100, 127, 176, 177], "node": [66, 67, 105, 127, 143, 144, 161, 175, 178], "nois": [49, 96, 98], "nomin": 101, "non": [14, 17, 18, 19, 25, 44, 45, 46, 47, 48, 62, 63, 66, 70, 71, 74, 78, 81, 82, 94, 95, 97, 98, 100, 115, 126, 143, 144, 146, 148, 161, 162, 165, 167], "non_orth_scor": [63, 82, 144], "nondur": 75, "none": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 40, 48, 50, 51, 53, 55, 56, 57, 58, 65, 66, 69, 71, 72, 73, 74, 75, 78, 79, 80, 81, 89, 91, 94, 95, 97, 101, 102, 103, 104, 107, 109, 110, 111, 112, 115, 116, 123, 124, 127, 129, 130, 132, 144, 161, 174, 175], "nonignor": [30, 160], "nonlinear": [17, 19, 21, 66, 71, 74, 94, 100, 127, 144, 153, 154, 177], "nonlinearscoremixin": [0, 144], "nonparametr": [27, 28, 29, 100, 103, 144, 162, 163, 169, 170, 171, 172, 176], "nop": 67, "nor": [5, 6, 7, 8, 9, 65, 93, 101, 107, 112], "norm": 82, "normal": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 63, 68, 69, 70, 71, 73, 74, 81, 82, 85, 89, 95, 96, 97, 98, 99, 100, 101, 104, 106, 107, 112, 115, 120, 121, 127, 140, 144, 145, 146, 147, 148, 161, 175], "normalize_ipw": [22, 23, 24, 25, 26, 27, 28, 29, 30, 68, 90, 95, 104], "not_yet_tr": [14, 16, 71, 74], "notat": [65, 68, 69, 93, 104, 127, 129, 130, 131, 132, 133, 140, 142, 144, 146, 148], "note": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 50, 51, 54, 55, 56, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 127, 140, 143, 144, 173, 175], "notebook": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 114, 115, 116, 123, 127, 140, 177, 178], "notic": [62, 81, 101], "now": [64, 65, 66, 68, 72, 73, 77, 78, 79, 83, 84, 93, 94, 97, 98, 101, 103, 104, 112, 175, 177], "np": [5, 6, 7, 8, 9, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 48, 57, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 112, 114, 115, 116, 117, 119, 120, 121, 127, 128, 129, 130, 132, 143, 144, 161, 162, 168, 175, 178], "nrmse": 79, "nround": [63, 66, 178], "nrow": [64, 65, 67, 100, 107, 112, 115, 125, 126, 161, 175], "ntrue": 78, "nu": [18, 25, 47, 68, 104, 127, 136, 162, 163, 165, 167, 168, 171, 172], "nu2": [77, 162, 168], "nu_0": [162, 172], "nu_i": [68, 104], "nuis_g0": 62, "nuis_g1": 62, "nuis_l": [123, 178], "nuis_m": [62, 123, 178], "nuis_r0": 62, "nuis_r1": 62, "nuis_rmse_ml_l": 86, "nuis_rmse_ml_m": 86, "nuisanc": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 42, 45, 46, 47, 48, 52, 63, 64, 65, 66, 67, 68, 69, 72, 77, 79, 82, 83, 84, 85, 86, 89, 91, 93, 94, 95, 97, 99, 101, 102, 103, 104, 105, 106, 115, 117, 118, 119, 120, 121, 122, 124, 126, 127, 130, 131, 132, 140, 143, 144, 145, 146, 147, 148, 149, 153, 155, 161, 162, 165, 167, 172, 173, 177, 178], "nuisance_el": [162, 164, 165, 166, 167, 169, 170, 171], "nuisance_loss": [77, 79, 115, 116, 177], "nuisance_spac": [37, 144, 155], "nuisance_target": 77, "null": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 64, 102, 115, 123, 162, 168, 177], "null_hypothesi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 102, 162, 168], "num": [66, 67, 105, 115, 123, 124, 126, 127, 143, 144, 161, 175], "num_id": [39, 45, 97, 109, 112, 127], "num_leav": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 70, 85, 95, 99], "num_t": [39, 45, 97, 109, 112, 127], "number": [12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 53, 54, 56, 63, 65, 70, 71, 72, 74, 77, 78, 82, 83, 84, 85, 86, 87, 88, 93, 95, 97, 98, 99, 100, 103, 127, 131, 143, 161, 173, 175, 178], "numer": [17, 19, 21, 58, 62, 67, 90, 96, 115, 123, 124, 144, 162, 169, 170, 177], "numeric_onli": 86, "numpi": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 42, 48, 53, 54, 57, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 109, 112, 114, 115, 117, 120, 121, 127, 128, 129, 130, 132, 143, 144, 161, 162, 168, 175], "nuniqu": [71, 74], "ny": 176, "nyt": [127, 130, 132, 133], "o": [70, 77, 78, 79, 80, 86, 87, 88, 92, 94, 96, 97, 100, 173, 175], "ob": [64, 66, 70, 74, 100, 162, 165], "obei": 144, "obj": 94, "obj_dml_data": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 62, 63, 65, 72, 73, 81, 82, 85, 90, 92, 93, 99, 105, 106, 114, 115, 116, 117, 119, 123, 124, 126, 127, 128, 129, 130, 132, 143, 144, 161, 162, 168, 177], "obj_dml_data_bonu": [107, 112], "obj_dml_data_bonus_df": [107, 112], "obj_dml_data_from_arrai": [5, 6, 7, 8, 9], "obj_dml_data_from_df": [5, 6, 8, 9], "obj_dml_data_sim": [107, 112], "obj_dml_data_sim_clust": [107, 112], "obj_dml_plr": [63, 82, 106], "obj_dml_plr_bonu": [67, 175], "obj_dml_plr_bonus_pip": 67, "obj_dml_plr_bonus_pipe2": 67, "obj_dml_plr_bonus_pipe3": 67, "obj_dml_plr_bonus_pipe_ensembl": 67, "obj_dml_plr_fullsampl": 92, "obj_dml_plr_lesstim": 92, "obj_dml_plr_nonorth": [63, 82], "obj_dml_plr_orth_nosplit": [63, 82], "obj_dml_plr_sim": [67, 175], "obj_dml_plr_sim_pip": 67, "obj_dml_plr_sim_pipe_ensembl": 67, "obj_dml_plr_sim_pipe_tun": 67, "obj_dml_sim": 30, "object": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 62, 66, 67, 68, 69, 71, 72, 73, 74, 75, 78, 80, 83, 84, 85, 89, 90, 91, 92, 94, 95, 97, 99, 100, 104, 107, 109, 110, 111, 112, 114, 115, 116, 120, 122, 125, 126, 127, 128, 129, 130, 132, 143, 144, 161, 173, 175, 176, 177, 178], "obs_confound": [62, 81], "observ": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 49, 53, 54, 59, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 89, 91, 92, 93, 94, 95, 97, 99, 100, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 114, 115, 116, 126, 127, 128, 129, 130, 132, 133, 140, 142, 143, 144, 145, 146, 147, 148, 161, 162, 163, 164, 165, 166, 167, 175, 176, 178], "obtain": [19, 25, 41, 62, 63, 64, 65, 68, 69, 71, 72, 73, 74, 77, 81, 82, 83, 84, 85, 86, 93, 99, 101, 103, 104, 105, 106, 114, 115, 126, 143, 144, 155, 161, 162, 163, 168, 174, 175], "obvious": [71, 74], "occur": [17, 19, 71, 74, 92, 177], "odd": 37, "off": [98, 176], "offer": [17, 19, 64, 66, 94, 95, 103, 178], "offici": 174, "offset": [115, 123], "often": [99, 127, 140], "oka": 176, "ol": [22, 26, 40, 53], "olma": [100, 127, 176, 177], "omega": [89, 114, 127, 128, 144, 149, 152, 162, 169, 170], "omega_": [44, 65, 93], "omega_1": [44, 65, 93], "omega_2": [44, 65, 93], "omega_epsilon": [65, 93], "omega_v": [44, 65, 93], "omega_x": [44, 65, 93], "omit": [71, 74, 97, 102, 103, 127, 131, 140, 144, 146, 148, 162, 163, 172, 176, 177, 178], "ommit": 103, "onc": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 64, 92, 103, 127, 133, 178], "one": [13, 16, 38, 59, 62, 63, 64, 65, 66, 67, 71, 72, 73, 74, 77, 80, 81, 82, 83, 84, 91, 93, 95, 96, 97, 100, 101, 102, 103, 106, 107, 112, 114, 115, 116, 125, 126, 127, 128, 131, 133, 139, 140, 143, 144, 145, 146, 147, 148, 152, 155, 156, 157, 158, 161, 162, 163, 168, 169, 170, 171, 175, 177], "ones": [67, 70, 85, 92, 99, 102, 114], "ones_lik": [79, 80, 99], "onli": [5, 6, 7, 8, 9, 12, 14, 15, 16, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 50, 51, 53, 55, 56, 57, 58, 64, 65, 66, 71, 74, 77, 78, 83, 84, 87, 88, 89, 91, 92, 93, 94, 95, 97, 100, 105, 114, 115, 124, 125, 127, 130, 132, 142, 144, 146, 148, 150, 153, 154, 161, 162, 163, 165, 167, 169, 170, 172, 177], "onlin": 178, "onto": 77, "oo": 92, "oob_error": [67, 115, 123, 124], "oop": 177, "opac": [83, 84], "open": [67, 115, 125, 173, 175], "oper": [67, 177], "opposit": [98, 100, 127], "oprescu": [32, 83, 84, 87, 88, 176], "opt": [70, 71, 72, 73, 74, 93, 94, 103], "optim": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 67, 83, 84, 92, 98, 114, 115, 125, 126, 176], "optimize_kwarg": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "option": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 45, 48, 50, 51, 55, 56, 57, 58, 62, 63, 65, 66, 68, 71, 72, 74, 77, 78, 80, 83, 84, 87, 88, 89, 93, 94, 95, 104, 107, 108, 109, 110, 111, 112, 115, 116, 124, 126, 127, 143, 144, 150, 153, 154, 161, 177], "optuna": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 52, 97, 115, 120, 177], "optuna_set": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 78, 97, 115, 120], "optuna_sett": 78, "optuna_settings_pipelin": 78, "oracl": [35, 49, 71, 74, 78, 79, 80], "oracle_valu": [31, 35, 41, 49, 78, 79, 80], "orang": 63, "orcal": [31, 41], "order": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 55, 64, 65, 66, 67, 72, 90, 93, 94, 97, 100, 115, 118, 123, 124, 127, 143, 144], "org": [33, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 115, 125, 173, 174, 176, 177], "orient": [67, 115, 125, 144, 173, 175, 176, 177], "origin": [50, 51, 54, 55, 56, 64, 67, 71, 72, 73, 74, 97, 98, 102, 103, 114, 144, 152], "orign": [66, 94], "orth_sign": [53, 54, 90], "orthogon": [53, 54, 65, 66, 93, 94, 105, 113, 127, 161, 162, 172, 173, 176], "orthongon": [162, 172], "osx": 174, "other": [5, 6, 7, 8, 9, 37, 38, 39, 40, 45, 50, 51, 55, 56, 63, 65, 66, 67, 68, 69, 71, 72, 73, 74, 77, 78, 80, 82, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 106, 107, 110, 112, 114, 115, 123, 124, 125, 127, 139, 140, 141, 143, 144, 152, 161, 162, 172, 173, 174, 175, 176, 177, 178], "other_ind": 93, "otherwis": [12, 14, 15, 22, 23, 25, 26, 39, 40, 50, 51, 55, 56, 66, 94, 95, 98, 127, 129, 144, 146, 148], "othrac": [67, 75, 107, 112, 175], "our": [63, 64, 66, 67, 69, 71, 74, 77, 78, 79, 82, 83, 84, 85, 92, 94, 95, 97, 99, 100, 102, 103, 106, 127, 173, 175, 177, 178], "ourselv": 77, "out": [38, 39, 40, 65, 67, 69, 70, 71, 72, 73, 74, 75, 77, 86, 92, 93, 95, 97, 102, 103, 104, 105, 107, 112, 113, 114, 115, 116, 117, 119, 120, 126, 127, 128, 129, 130, 132, 140, 144, 156, 157, 158, 161, 162, 163, 168, 171, 173, 175, 177, 178], "outcom": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 35, 37, 38, 39, 40, 41, 42, 49, 62, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 78, 81, 86, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 107, 109, 110, 111, 112, 115, 116, 126, 129, 130, 132, 133, 134, 138, 139, 140, 141, 142, 146, 148, 149, 161, 163, 168, 169, 171, 172, 175, 177, 178], "outcome_0": 81, "outcome_1": 81, "outer": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 115, 117, 119, 124], "outperform": 79, "output": [64, 71, 73, 74, 77, 101, 105, 115, 123, 127, 130, 132, 161, 178], "output_list": 101, "outshr": 93, "outsid": 63, "over": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 63, 71, 72, 74, 77, 80, 82, 86, 106, 113, 115, 120, 121, 126, 127, 128, 140, 162, 168, 177], "overal": [13, 71, 72, 73, 74, 91, 98, 103, 127, 128], "overall_aggregation_weight": [13, 71, 72, 73, 74], "overcom": [113, 144], "overfit": [92, 113, 143], "overlap": [69, 91, 103, 127, 129, 133], "overrid": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 78, 115, 124, 177], "overridden": 127, "overst": [66, 94, 95], "overview": [77, 161, 162, 168, 176], "overwrit": 177, "overwritten": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "ownership": [66, 94], "p": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 37, 38, 39, 40, 41, 42, 45, 49, 57, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 106, 114, 115, 116, 117, 119, 120, 121, 125, 126, 127, 128, 129, 130, 132, 133, 138, 140, 143, 144, 145, 146, 147, 148, 149, 150, 153, 154, 155, 159, 160, 161, 162, 169, 170, 173, 174, 175, 177], "p401": [66, 94, 95], "p_": [17, 19], "p_0": [144, 145, 146, 147], "p_1": 161, "p_adjust": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 143, 161, 173, 175], "p_dbl": [67, 115, 125, 126], "p_hat": 90, "p_i": 42, "p_int": [115, 126], "p_n": 43, "p_val": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "p_x": [44, 65, 93], "p_x0": 96, "p_x1": 96, "packag": [62, 63, 65, 67, 68, 69, 70, 71, 72, 73, 74, 76, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 91, 92, 93, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 114, 115, 118, 122, 123, 124, 125, 126, 127, 129, 133, 140, 142, 143, 144, 161, 162, 163, 173, 175, 176, 177, 178], "packagedata": 93, "packagevers": 66, "pad": 91, "page": [78, 97, 103, 173, 176], "pair": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 62, 81], "pake": [65, 93], "paket": [65, 66, 67], "pal": 65, "palett": [13, 16, 71, 74, 78, 79, 80, 97], "pand": [71, 74], "panda": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 42, 45, 53, 54, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 107, 109, 112, 114, 127, 162, 163, 175], "pandas2ri": 93, "panel": [6, 7, 12, 14, 16, 17, 18, 19, 39, 45, 72, 74, 107, 108, 109, 112, 130, 131, 132, 133, 140, 166, 167, 176, 177], "paper": [33, 43, 67, 92, 96, 100, 102, 103, 162, 172, 173, 175, 176, 177], "par": 75, "par_grid": [67, 115, 121, 125, 126], "paradox": [67, 115, 126, 177], "parallel": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 64, 69, 70, 71, 74, 77, 80, 85, 99, 127, 129, 131, 133], "param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 50, 51, 55, 56, 78, 92, 115, 119, 120, 121, 124, 126], "param_grid": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 115, 126], "param_nam": [64, 78], "param_set": [67, 115, 125, 126], "param_spac": [78, 97, 115, 120], "param_space_pipelin": 78, "param_v": 67, "paramet": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 63, 64, 65, 66, 68, 69, 71, 72, 74, 77, 78, 79, 80, 82, 83, 84, 85, 86, 89, 90, 93, 94, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 112, 113, 114, 115, 116, 119, 120, 121, 123, 124, 125, 126, 129, 130, 131, 132, 133, 134, 135, 136, 137, 140, 142, 143, 153, 154, 161, 162, 163, 168, 170, 172, 173, 175, 176, 177, 178], "parametr": [25, 64, 78, 103, 106, 115, 119, 124, 178], "params_exact": [115, 124], "params_nam": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 52, 64, 78], "params_stacking__final_estimator__c": 78, "params_stacking__lgbm__lambda_l1": 78, "params_stacking__lgbm__lambda_l2": 78, "params_stacking__lgbm__learning_r": 78, "params_stacking__lgbm__min_child_sampl": 78, "parenttoc": 173, "part": [47, 63, 65, 66, 67, 73, 77, 82, 92, 93, 94, 97, 106, 115, 126, 127, 140, 143, 162, 172, 177, 178], "parti": 47, "partial": [21, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 65, 67, 75, 86, 91, 92, 93, 97, 102, 105, 113, 115, 116, 119, 120, 121, 124, 126, 138, 139, 140, 141, 143, 156, 157, 158, 161, 168, 169, 170, 171, 172, 173, 175, 176, 177, 178], "partial_": [144, 161], "partiallli": 102, "particip": [10, 95, 102, 178], "particular": [127, 173], "particularli": [79, 92, 97], "partion": [65, 93], "partit": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 65, 93, 105, 113], "partli": 178, "pass": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 50, 51, 52, 53, 55, 56, 64, 67, 72, 78, 92, 97, 115, 117, 122, 124, 126, 178], "passo": [173, 175], "passthrough": 97, "passthroughpassthrough": 97, "past": 65, "paste0": [65, 68], "pastel": 82, "path": [115, 121, 126, 127], "path_to_r": 86, "patsi": [83, 84, 114], "pattern": 103, "paul": 176, "pd": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 48, 53, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 83, 84, 85, 86, 87, 88, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 114, 127], "pdf": [82, 96], "pedregosa": [173, 175], "pedregosa11a": [173, 175], "pedro": [64, 176], "penal": [68, 72, 104], "penalti": [66, 67, 72, 81, 94, 101, 103, 104, 115, 126, 127], "pennsylvania": [11, 107, 112, 175], "pension": [66, 94, 95, 178], "peopl": [66, 94, 95], "pep8": 177, "per": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 65, 71, 72, 73, 74, 78, 93, 97, 127], "percent": [115, 126], "percentag": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 41], "perf_count": 77, "perfectli": [91, 100, 127], "perform": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 52, 54, 63, 65, 67, 69, 71, 72, 73, 74, 77, 78, 82, 86, 89, 92, 93, 95, 97, 102, 103, 104, 106, 115, 116, 120, 121, 123, 125, 126, 127, 129, 130, 132, 143, 144, 161, 173, 175, 176, 178], "performance_result": 79, "perfrom": 89, "perhap": 178, "period": [12, 14, 16, 17, 19, 45, 64, 69, 70, 73, 97, 109, 112, 128, 129, 130, 131, 132, 133, 140, 146, 148, 176, 177], "perp": [97, 127, 140, 142], "perrot": [173, 175], "person": 178, "pessimist": 103, "peter": 176, "petra": 177, "petronelaj": 177, "pfister": [67, 115, 125, 173, 175], "phi": [65, 93, 114, 161], "philipp": [103, 173, 176], "philippbach": [173, 177], "pi": [30, 34, 43, 47, 114, 127, 144, 159, 160], "pi_": [44, 65, 93], "pi_0": [144, 159, 160], "pi_i": [68, 104, 127], "pick": [100, 178], "pip": [100, 127], "pip3": 174, "pipe": 67, "pipe_forest_classif": 67, "pipe_forest_regr": 67, "pipe_lasso": 67, "pipelin": [50, 51, 55, 56, 67, 94, 122, 123, 125, 177], "pipelineinot": [78, 97], "pipeop": 67, "pira": [66, 94, 95, 102, 178], "pivot": [79, 86, 93, 176], "pivot_logloss": 79, "pivot_rmse_g0": 79, "pivot_rmse_g1": 79, "place": 177, "plai": [92, 178], "plan": [10, 66, 94, 95, 178], "plausibl": [103, 162], "pleas": [50, 51, 55, 56, 64, 69, 70, 72, 78, 80, 92, 97, 103, 115, 143, 173, 174], "plim": 96, "pliv": [20, 21, 38, 65, 93, 105, 114, 139, 156, 173, 177], "plm": [0, 6, 7, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 82, 91, 92, 93, 97, 102, 105, 106, 109, 112, 113, 114, 115, 116, 119, 120, 143, 161, 168, 177, 178], "plot": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 54, 63, 64, 66, 67, 68, 70, 71, 72, 73, 74, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 94, 95, 96, 97, 99, 100, 102, 103, 104, 114, 162, 168], "plot_data": [71, 74], "plot_effect": [13, 16, 71, 72, 73, 74], "plot_optimization_histori": 78, "plot_parallel_coordin": 78, "plot_param_import": 78, "plot_tre": [54, 98, 114], "plotli": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 78, 83, 84, 86, 100, 103], "plpr": [39, 97, 140, 157], "plpr_lasso_cre_norm": 97, "plpr_lasso_fd": 97, "plpr_lasso_wg": 97, "plpr_tune_cre_gener": 97, "plpr_tune_cre_norm": 97, "plpr_tune_fd": 97, "plpr_tune_wg": 97, "plr": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 67, 92, 96, 102, 105, 115, 116, 120, 141, 143, 158, 161, 168, 170, 171, 172, 173, 175, 177, 178], "plr_est": 96, "plr_est1": 96, "plr_est2": 96, "plr_obj": 96, "plr_obj_1": 96, "plr_obj_2": 96, "plr_summari": 94, "plrglmnet": 66, "plrranger": 66, "plrrpart": 66, "plrxgboost8700": 66, "plt": [69, 70, 71, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 103, 104], "plt_smpl": [65, 93], "plt_smpls_cluster": [65, 93], "plug": [89, 162, 164, 165, 166, 167, 168, 169, 170], "pm": [48, 65, 93, 161, 162, 168, 172], "pmatrix": [68, 104], "pmlr": [77, 86, 92], "po": [67, 115, 123, 125], "poe": 176, "point": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 64, 65, 79, 87, 88, 93, 103, 114, 127, 178], "pointwis": [53, 85, 87, 88, 99], "poli": [66, 90, 93, 94, 97], "polici": [26, 37, 38, 39, 40, 54, 91, 113, 127, 138, 139, 140, 141, 175, 176, 177], "policy_tre": [26, 98, 114], "policy_tree_2": 98, "policy_tree_obj": 114, "policytre": 98, "polit": 96, "polselli": [39, 45, 97, 127, 140, 176], "poly_dict": 94, "poly_nam": 97, "poly_x": 97, "poly_x__x2": 97, "poly_x__x21": 97, "poly_x__x5": 97, "poly_x_tr": 97, "polynomi": [10, 11, 49, 66, 75, 90, 94, 97, 100], "polynomial_featur": [10, 11, 66, 75], "polynomialfeatur": [90, 93, 94, 97], "polyplu": 97, "polypluspolyplu": 97, "poor": 101, "pop": [144, 146], "popul": [103, 127, 130, 132, 144, 148], "popular": [77, 127, 162, 163], "porport": 102, "posit": [47, 66, 71, 73, 74, 77, 79, 96, 103, 178], "posixct": [67, 115, 123], "possibl": [5, 6, 7, 8, 9, 51, 55, 56, 64, 67, 71, 73, 74, 77, 78, 83, 84, 87, 88, 89, 90, 92, 97, 98, 100, 101, 102, 103, 115, 116, 117, 120, 122, 125, 127, 131, 134, 135, 140, 161, 162, 163, 177, 178], "possibli": [101, 162, 163], "post": [16, 43, 47, 127, 129, 131, 133, 161, 176], "postdoubl": 176, "poster": 96, "potenti": [17, 19, 22, 23, 24, 27, 28, 30, 31, 35, 37, 49, 68, 69, 71, 74, 78, 90, 96, 100, 104, 129, 133, 134, 140, 142, 149, 150, 161, 169, 174, 177, 178], "potential_level": [78, 79, 80], "power": [67, 92, 101, 103, 115, 126, 176], "pp": [64, 77, 86, 92], "pq": [27, 28, 29, 95, 154, 177], "pq_0": [95, 99], "pq_1": [95, 99], "pr": [30, 62, 65, 66, 67, 68, 115, 123, 124, 125, 126, 127, 143, 144, 161, 175, 178], "practic": [77, 103, 176], "pre": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 64, 68, 69, 71, 72, 73, 74, 97, 104, 115, 119, 124, 127, 129, 130, 131, 132, 144, 146, 148, 177], "precis": [64, 127, 162, 170, 178], "precomput": [51, 56], "pred": [64, 92], "pred_df": 98, "pred_dict": [115, 117], "pred_treat": 98, "predict": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 50, 51, 53, 54, 55, 56, 57, 63, 65, 66, 67, 72, 77, 78, 79, 82, 85, 86, 90, 92, 93, 94, 97, 98, 101, 103, 106, 114, 117, 118, 123, 124, 126, 127, 140, 143, 162, 163, 168, 170, 177, 178], "predict_proba": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 39, 40, 48, 50, 55, 92, 101, 115, 118], "predictor": [22, 26, 40, 53, 54, 83, 84, 87, 88, 97, 103, 105, 127, 140], "prefer": [66, 94, 95, 178], "preliminari": [24, 63, 82, 100, 144, 150, 153, 154, 155, 160], "prepar": [64, 65, 78, 93, 177], "preprint": [101, 176], "preprocess": [66, 72, 78, 90, 93, 94, 95, 115, 123, 125], "preprocessor": 97, "preprocessor_alt": 97, "preprocessor_wg": 97, "prepross": 97, "presenc": [66, 94, 95, 127, 140], "present": [23, 64, 72, 103, 115, 125, 144, 152, 178], "prespecifi": 102, "pretreat": [12, 14, 15, 16, 69], "prettenhof": [173, 175], "preval": 103, "prevent": [143, 177], "previou": [70, 89, 90, 96, 174, 178], "previous": [72, 101, 115, 124, 125, 178], "price": [65, 93], "priliminari": [27, 29], "primari": [79, 80], "principl": [78, 162, 163], "print": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 57, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 119, 120, 121, 127, 128, 129, 130, 132, 143, 144, 161, 162, 168, 174, 175, 177, 178], "print_detail": 64, "print_period": [14, 16], "prior": [77, 79, 127, 142], "privat": 177, "prob": 67, "prob_dist": 101, "prob_dist_": 101, "probabilit": [89, 91], "probability_from_treat": 91, "probabl": [17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 55, 63, 64, 68, 69, 71, 74, 80, 82, 89, 91, 96, 99, 100, 101, 103, 104, 106, 127, 144, 145, 146, 147, 148, 153, 176], "problem": [66, 72, 94, 95, 114, 115, 126], "proce": 78, "procedur": [63, 65, 66, 77, 78, 82, 93, 94, 102, 103, 115, 117, 161, 174, 177], "proceed": [43, 176], "process": [14, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 41, 42, 43, 44, 45, 46, 47, 49, 58, 64, 68, 69, 70, 73, 74, 77, 83, 84, 85, 86, 87, 88, 91, 92, 97, 98, 99, 103, 104, 113, 161, 162, 163, 176, 177], "processor": [57, 58], "produc": 96, "product": [77, 83, 84, 86, 103, 162, 172], "producton": 65, "program": [34, 66, 94, 95, 176, 178], "progress": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 76], "project": [67, 83, 84, 114, 173, 177], "project_z": [83, 84], "prone": 144, "pronounc": 100, "propens": [14, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 41, 42, 57, 58, 66, 68, 69, 71, 72, 74, 77, 78, 79, 89, 90, 94, 95, 103, 104, 114, 127, 130, 132, 134, 144, 146, 148, 162, 169, 177], "propensity_scor": 57, "proper": 79, "properli": [79, 92, 178], "properti": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 66, 67, 71, 72, 73, 74, 77, 78, 94, 95, 96, 97, 101, 102, 107, 108, 110, 111, 112, 115, 123, 124, 127, 162, 168, 175, 177], "proport": [102, 162, 163, 171, 172], "propos": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 65, 67, 93, 100, 162, 163, 176, 177], "provid": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 50, 51, 55, 56, 57, 58, 64, 65, 66, 67, 72, 79, 83, 84, 87, 88, 90, 92, 93, 94, 100, 101, 103, 105, 106, 107, 109, 112, 113, 115, 117, 119, 122, 123, 124, 125, 126, 161, 173, 175, 177, 178], "prune": [26, 54], "ps911c": 93, "ps944": 93, "ps_processor_config": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 83], "pscore1": 96, "pscore2": 96, "psi": [20, 21, 63, 64, 65, 93, 105, 127, 130, 131, 132, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 172, 175], "psi_": [161, 162, 165, 167, 168, 171, 172], "psi_a": [20, 25, 26, 38, 40, 63, 65, 82, 93, 127, 130, 131, 132, 143, 144, 145, 146, 147, 148, 149, 151, 152, 156, 157, 158, 161], "psi_b": [20, 25, 26, 38, 40, 63, 82, 114, 127, 130, 131, 132, 143, 144, 145, 146, 147, 148, 149, 151, 152, 156, 157, 158], "psi_el": [91, 143, 144], "psi_j": 161, "psi_nu2": [162, 168], "psi_sigma2": [162, 168], "psprocessor": [58, 177], "psprocessorconfig": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 57], "public": [62, 81, 177], "publish": [103, 173, 177], "pull": [66, 177], "purchas": 103, "pure": 103, "purp": [83, 84], "purpos": [63, 82, 89, 102, 103, 144, 146, 148, 162, 163, 175], "pval": 161, "px": [86, 100], "py": [70, 71, 72, 73, 74, 83, 93, 94, 100, 101, 103, 173, 174, 177], "py3": 174, "py_al": 82, "py_did": 69, "py_did_pretest": 70, "py_dml": 82, "py_dml_nosplit": 82, "py_dml_po": 82, "py_dml_po_nosplit": 82, "py_double_ml_apo": 80, "py_double_ml_bas": 82, "py_double_ml_basic_iv": 81, "py_double_ml_c": 83, "py_double_ml_cate_plr": 84, "py_double_ml_cvar": 85, "py_double_ml_firststag": 86, "py_double_ml_g": 87, "py_double_ml_gate_plr": 88, "py_double_ml_gate_sensit": 89, "py_double_ml_irm_vs_apo": 90, "py_double_ml_lplr": 91, "py_double_ml_meets_flaml": 92, "py_double_ml_multiway_clust": 93, "py_double_ml_pens": 94, "py_double_ml_pension_qt": 95, "py_double_ml_plm_irm_hetfx": 96, "py_double_ml_plpr": 97, "py_double_ml_policy_tre": 98, "py_double_ml_pq": 99, "py_double_ml_rdflex": 100, "py_double_ml_robust_iv": 101, "py_double_ml_sensit": 102, "py_double_ml_sensitivity_book": 103, "py_double_ml_ssm": 104, "py_learn": 77, "py_non_orthogon": 82, "py_optuna": 78, "py_panel": 71, "py_panel_data_exampl": 72, "py_panel_simpl": 73, "py_po_al": 82, "py_rep_c": 74, "py_tabpfn": 79, "pypi": [176, 177], "pyplot": [69, 70, 71, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 103, 104], "pyproject": 177, "pyreadr": 72, "python": [47, 64, 92, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117, 119, 120, 121, 127, 128, 129, 130, 132, 140, 143, 144, 161, 162, 163, 168, 173, 175, 176, 177, 178], "python3": [70, 71, 72, 73, 74, 93, 94, 100, 103, 174], "pytorch": 79, "q": [67, 85, 97, 99, 100, 115, 125, 127, 140, 144, 157, 173, 175], "q2": [67, 75, 107, 112, 175], "q3": [67, 75, 107, 112, 175], "q4": [67, 75, 107, 112, 175], "q5": [67, 75, 107, 112, 175], "q6": [67, 75, 107, 112, 175], "q_i": [100, 127], "qquad": 34, "qte": [85, 95, 177], "quad": [17, 18, 19, 42, 66, 68, 69, 94, 97, 98, 100, 104, 114, 127, 129, 133, 142, 144, 153, 161, 162, 164, 165], "quadrat": [68, 104], "qualiti": [102, 105, 177], "quanitl": 95, "quant": 85, "quantifi": [103, 127, 133], "quantil": [16, 23, 24, 27, 28, 29, 35, 71, 74, 78, 80, 85, 90, 102, 113, 115, 123, 124, 150, 153, 154, 176, 177], "quantiti": [62, 81, 103], "queri": 94, "question": [103, 178], "quick": 95, "quit": [77, 78, 79, 98, 102, 162, 163], "r": [25, 33, 50, 51, 55, 56, 70, 71, 72, 73, 74, 82, 83, 84, 86, 93, 96, 100, 101, 103, 105, 106, 107, 112, 113, 123, 124, 125, 126, 127, 136, 140, 143, 144, 151, 155, 156, 161, 162, 163, 169, 170, 171, 172, 173, 175, 176, 177, 178], "r2_d": [34, 77], "r2_score": [51, 56], "r2_y": [34, 77], "r6": [67, 177], "r_0": [25, 37, 38, 42, 66, 91, 94, 127, 136, 138, 144, 155], "r_all": 63, "r_d": 34, "r_df": 93, "r_dml": 63, "r_dml_nosplit": 63, "r_dml_po": 63, "r_dml_po_nosplit": 63, "r_double_ml_bas": 63, "r_double_ml_basic_iv": 62, "r_double_ml_did": 64, "r_double_ml_multiway_clust": 65, "r_double_ml_pens": 66, "r_double_ml_pipelin": 67, "r_double_ml_ssm": 68, "r_hat": [38, 91], "r_hat0": 25, "r_hat1": 25, "r_non_orthogon": 63, "r_po_al": 63, "r_y": 34, "rais": [5, 6, 7, 8, 9, 37, 50, 51, 55, 56, 71, 72, 73, 74, 115, 117], "randint": 96, "randn": 30, "random": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 36, 37, 38, 39, 40, 47, 48, 49, 62, 63, 64, 66, 67, 69, 70, 71, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 93, 94, 95, 96, 98, 99, 100, 101, 102, 103, 105, 106, 107, 109, 112, 114, 115, 116, 117, 119, 120, 121, 123, 124, 126, 128, 129, 130, 132, 133, 140, 143, 157, 159, 161, 162, 168, 172, 175, 176, 178], "random_search": [115, 126], "random_st": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 40, 71, 74, 78, 79, 82, 89, 90, 97, 98], "randomforest": [66, 77, 79, 94], "randomforest_class": [66, 83, 91, 94, 98], "randomforest_reg": [83, 91, 98], "randomforestclassifi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 75, 77, 79, 80, 83, 84, 87, 88, 89, 91, 94, 98, 100, 102, 103, 114, 115, 117, 118, 127, 128, 129, 130, 132, 178], "randomforestregressor": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 75, 77, 79, 80, 82, 83, 84, 87, 88, 89, 91, 94, 98, 100, 102, 103, 105, 114, 115, 116, 117, 118, 119, 127, 128, 129, 130, 132, 143, 144, 161, 162, 168, 175, 178], "randomized_search": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 115, 121], "randomizedsearchcv": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 115, 121], "randomli": [63, 65, 82, 93, 106, 143, 178], "rang": [8, 17, 19, 63, 69, 70, 77, 78, 79, 80, 82, 85, 87, 88, 91, 92, 93, 95, 97, 98, 99, 100, 101, 103, 104, 106, 115, 126, 127], "rangeindex": [71, 72, 73, 74, 75, 78, 79, 80, 89, 91, 93, 94, 95, 102, 107, 109, 112, 175], "ranger": [64, 66, 67, 105, 115, 123, 124, 126, 127, 143, 144, 161, 175, 178], "rangl": [32, 98], "rank": 177, "rate": [77, 86, 127], "rather": [100, 103, 127], "ratio": [115, 126, 143, 162, 163], "rational": 72, "ravel": [83, 84], "raw": [57, 66, 72, 73, 86, 94], "raw_res_manual_lasso_r_100_n_100_p_200_rho_0": 86, "rbind": 66, "rbindlist": 66, "rbinom": 62, "rbrace": [25, 26, 33, 34, 36, 65, 93, 105, 127, 134, 136, 137, 143, 144, 149, 161, 162, 169], "rcolorbrew": 65, "rcparam": [70, 75, 83, 84, 85, 87, 88, 90, 93, 94, 95, 99], "rd": [127, 177], "rda": 72, "rdbu": 65, "rdbu_r": 93, "rdbwselect": 127, "rdd": [0, 8, 110, 112, 113, 174], "rdflex": [100, 127, 177], "rdflex_fuzzi": 100, "rdflex_fuzzy_stack": 100, "rdflex_obj": [48, 127], "rdflex_sharp": 100, "rdflex_sharp_stack": 100, "rdrobust": [48, 100, 127, 174, 177], "rdrobust_fuzzi": 100, "rdrobust_fuzzy_noadj": 100, "rdrobust_sharp": 100, "rdrobust_sharp_noadj": 100, "rdt044": 86, "re": [93, 103, 174], "read": [72, 174], "read_csv": [73, 86], "read_r": 72, "readabl": [79, 177], "reader": [72, 101], "readili": 173, "real": [66, 94, 95, 102, 162, 163], "realat": 127, "realiz": [100, 127, 140, 142], "reason": [5, 6, 7, 8, 9, 62, 77, 79, 81, 86, 92, 102, 103, 162, 163, 178], "recal": [75, 162, 172], "receiv": [17, 19, 71, 74, 80, 100, 127, 129, 131], "recent": [92, 127, 129, 133, 176], "recogn": [66, 94, 95], "recommend": [67, 71, 74, 77, 78, 79, 100, 103, 105, 127, 143, 162, 174, 176, 177], "recov": [62, 64, 81, 96], "recsi": 176, "red": [65, 68, 78, 79, 87, 88, 92, 93], "reduc": [66, 78, 89, 92, 94, 100, 102, 103, 127, 177], "redund": 177, "reemploy": [11, 107, 112, 175], "ref": 72, "refactor": 177, "refer": [10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 66, 70, 71, 74, 79, 80, 89, 91, 94, 95, 97, 100, 102, 107, 112, 113, 114, 123, 127, 128, 130, 132, 133, 140, 144, 162, 163, 168, 176, 177], "reference_level": [23, 78, 79, 80, 90, 127], "refin": 177, "refit": [162, 163], "reflect": [98, 103, 114], "reg": [17, 18, 19, 66, 94, 178], "reg_estim": 100, "reg_lambda": 97, "reg_learn": 95, "reg_learner_1": 77, "reg_learner_2": 77, "regard": [103, 173], "regener": 177, "region": [65, 85, 93, 161, 176], "regr": [62, 63, 64, 65, 66, 67, 68, 105, 115, 123, 124, 125, 126, 127, 143, 144, 146, 161, 175, 178], "regravg": [67, 115, 123], "regress": [8, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 53, 58, 62, 64, 65, 67, 68, 69, 71, 72, 73, 74, 78, 80, 81, 86, 91, 92, 93, 96, 97, 101, 102, 103, 104, 105, 106, 110, 112, 113, 114, 115, 116, 119, 120, 121, 122, 124, 126, 129, 130, 132, 136, 137, 138, 139, 140, 141, 143, 148, 161, 163, 168, 169, 170, 171, 172, 173, 175, 176, 177, 178], "regressor": [37, 51, 56, 63, 66, 77, 79, 80, 82, 85, 91, 92, 94, 97, 101, 106], "regular": [43, 113, 115, 121, 144, 161, 176], "reich": [67, 115, 125], "reinforc": 176, "reject": [66, 94], "rel": [66, 72, 78, 79, 94, 101, 127, 128, 162, 163, 169, 170], "relat": [90, 103, 178], "relationship": [62, 79, 81, 86, 103, 161], "relev": [12, 14, 15, 16, 32, 45, 50, 51, 53, 55, 56, 71, 74, 85, 98, 99, 107, 112, 127, 162, 178], "reli": [16, 69, 70, 71, 74, 83, 84, 89, 90, 97, 114, 115, 117, 118, 127, 129, 144, 148, 162, 163, 178], "reload": 66, "remain": [64, 97, 107, 112, 161, 178], "remaind": 97, "remainder__x0": 97, "remainder__x1": 97, "remainder__x10": 97, "remainder__x11": 97, "remainder__x12": 97, "remainder__x13": 97, "remainder__x14": 97, "remainder__x15": 97, "remainder__x16": 97, "remainder__x17": 97, "remainder__x18": 97, "remainder__x19": 97, "remainder__x20": 97, "remainder__x22": 97, "remainder__x23": 97, "remainder__x24": 97, "remainder__x25": 97, "remainder__x26": 97, "remainder__x27": 97, "remainder__x28": 97, "remainder__x29": 97, "remainder__x3": 97, "remainder__x4": 97, "remainder__x6": 97, "remainder__x7": 97, "remainder__x8": 97, "remainder__x9": 97, "remark": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 63, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 82, 83, 84, 85, 87, 88, 89, 90, 95, 97, 102, 114, 115, 117, 127, 128, 129, 130, 132, 143, 144, 145, 146, 147, 148, 153, 154, 161, 162, 165, 167, 170], "remot": 174, "remov": [4, 14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 66, 70, 83, 90, 103, 107, 112, 113, 115, 127, 140, 143, 144, 162, 177], "renam": [71, 74, 94, 177], "render": [78, 97, 102, 103], "renorm": [17, 19], "reorgan": 177, "rep": [63, 68, 106, 115, 124, 161], "repeat": [12, 14, 15, 16, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 62, 63, 65, 66, 67, 68, 71, 72, 73, 78, 82, 89, 93, 94, 95, 96, 97, 100, 102, 104, 106, 108, 112, 113, 115, 116, 125, 130, 131, 132, 133, 161, 164, 165, 175, 177, 178], "repeatedkfold": 93, "repet": 102, "repetit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 53, 72, 77, 83, 84, 86, 87, 88, 89, 113, 115, 116, 161, 175, 177, 178], "replac": [98, 103, 177], "replic": [10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 63, 66, 72, 73, 82, 86, 101, 103], "repo": 177, "report": [66, 92, 94, 173, 177], "repositori": [71, 74, 86, 100, 177], "repr": [63, 65], "repres": [17, 19, 79, 96, 103, 127, 140], "represent": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 41, 78, 97, 102, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 175, 177], "reproduc": 35, "request": [50, 51, 55, 56, 177], "requir": [38, 39, 40, 50, 55, 58, 62, 66, 67, 71, 72, 73, 74, 78, 79, 80, 89, 94, 95, 102, 107, 112, 117, 118, 122, 124, 127, 128, 130, 131, 132, 140, 144, 146, 148, 161, 162, 163, 168, 174, 177, 178], "requirenamespac": 64, "rerun": [72, 78, 97], "res_df": 93, "res_dict": [31, 32, 35, 41, 49], "resampl": [37, 62, 65, 67, 68, 69, 71, 72, 73, 74, 93, 95, 97, 102, 104, 115, 116, 126, 127, 129, 130, 132, 143, 144, 161, 173, 175, 178], "resdat": 74, "research": [65, 67, 93, 96, 103, 143, 173, 175, 176, 178], "resembl": [68, 104], "reset": 64, "reset_index": [71, 74, 86, 93, 94], "reshap": [70, 82, 83, 84, 90, 107, 112], "reshape2": 65, "residu": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 41, 51, 56, 102, 162, 163, 171, 172], "resolut": [67, 115, 125, 126], "resourc": 77, "resourcewis": 77, "respect": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 66, 71, 72, 73, 74, 78, 80, 94, 95, 100, 114, 127, 131, 136, 142, 143, 162, 172, 178], "respons": [10, 67, 78, 115, 123, 124], "rest": 127, "restart": 174, "restrict": 77, "restructur": 177, "restud": 86, "result": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 52, 62, 63, 64, 67, 68, 69, 70, 71, 72, 73, 74, 77, 79, 80, 82, 83, 84, 86, 89, 90, 97, 98, 100, 101, 102, 103, 104, 106, 115, 126, 143, 144, 145, 146, 147, 148, 162, 163, 168, 175, 177], "result_iivm": 66, "result_irm": 66, "result_plr": 66, "result_typ": 16, "results_df": 101, "retain": [50, 51, 55, 56], "retina": 96, "retir": [66, 94, 95, 102], "return": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 59, 63, 64, 65, 67, 68, 70, 71, 72, 73, 74, 77, 78, 82, 85, 92, 93, 96, 97, 98, 99, 101, 102, 103, 104, 105, 115, 116, 120, 144, 162, 163, 177], "return_count": [77, 78, 79, 80], "return_tune_r": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 78], "return_typ": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 63, 66, 67, 68, 69, 77, 82, 90, 92, 94, 95, 102, 104, 105, 106, 107, 108, 110, 111, 112, 114, 115, 116, 117, 119, 123, 124, 127, 129, 143, 144, 161, 162, 168, 175, 178], "rev": 65, "reveal": 89, "review": [43, 86, 176], "revist": [65, 93], "reweight": [127, 128], "rf": 100, "rf_argument": 79, "rho": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 59, 71, 74, 80, 89, 90, 100, 102, 103, 162, 163, 168, 172, 178], "rho_val": 103, "richter": [67, 115, 125, 173, 175], "ridg": 78, "ridgeridg": 78, "riesz": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 41, 102, 162, 163, 164, 165, 166, 167, 168, 171, 172], "riesz_rep": [162, 168], "right": [17, 19, 33, 34, 36, 43, 44, 45, 63, 65, 77, 78, 82, 93, 94, 95, 96, 97, 99, 100, 103, 106, 127, 144, 145, 146, 147, 148, 161, 162, 164, 165, 166, 167, 169, 170], "rightarrow_": [63, 82, 106], "risk": [24, 113, 177], "ritov": 176, "rival": 93, "rival_ind": 93, "rmd": 64, "rmse": [64, 69, 71, 72, 73, 74, 77, 79, 92, 97, 102, 104, 115, 116, 127, 129, 130, 132, 144, 161, 175, 177], "rmse_dml_ml_l_fullsampl": 92, "rmse_dml_ml_l_lesstim": 92, "rmse_dml_ml_l_onfold": 92, "rmse_dml_ml_l_untun": 92, "rmse_dml_ml_m_fullsampl": 92, "rmse_dml_ml_m_lesstim": 92, "rmse_dml_ml_m_onfold": 92, "rmse_dml_ml_m_untun": 92, "rmse_g0": 79, "rmse_g1": 79, "rmse_oos_ml_l": 92, "rmse_oos_ml_m": 92, "rmse_oos_onfolds_ml_l": 92, "rmse_oos_onfolds_ml_m": 92, "rnorm": [62, 67, 107, 112, 115, 125, 126, 161, 175], "robin": [10, 11, 46, 65, 86, 93, 106, 173, 176], "robinson": [63, 82, 106], "robject": 93, "robu": [87, 88], "robust": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 41, 44, 64, 71, 72, 74, 80, 89, 90, 97, 100, 102, 103, 107, 112, 127, 140, 162, 168, 176, 177, 178], "robust_confset": [25, 101, 177], "robust_cov": 101, "robust_length": 101, "robustscal": 78, "robustscalerrobustscal": 78, "roc\u00edo": 176, "role": [4, 5, 6, 7, 8, 9, 63, 82, 92, 106, 110, 112, 178], "romano": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 161], "root": [12, 14, 15, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 79, 86, 106, 115, 116, 144, 176], "rotat": [78, 92, 97, 100], "roth": [100, 127, 129, 133, 176], "rough": [103, 178], "roughli": [71, 74, 103], "round": [57, 66, 72, 77, 78, 79, 80, 90, 96, 103], "rout": [50, 51, 55, 56], "row": [13, 16, 63, 66, 70, 72, 75, 78, 83, 84, 92, 93, 97, 98, 107, 112, 127, 130, 132, 143, 175, 178], "rownam": 65, "rowv": 65, "roxygen2": 177, "royal": [103, 176], "rpart": [66, 67, 115, 123], "rpart_cv": 67, "rprocess": 77, "rpy2": 93, "rpy2pi": 93, "rskf": 90, "rsmp": [67, 115, 125, 126, 143], "rsmp_tune": [67, 115, 125, 126], "rssb": 103, "rtype": 23, "ruben": 176, "ruiz": [62, 81], "rule": [64, 114], "run": [8, 64, 72, 79, 100, 110, 112, 127, 174, 177], "runif": 62, "runner": [70, 71, 73, 74, 101, 103], "runtime_learn": 67, "runtimewarn": 74, "rv": [16, 71, 74, 80, 89, 90, 102, 103, 162, 168, 178], "rva": [71, 74, 80, 89, 90, 102, 103, 162, 168, 178], "rvert": 86, "rvert_": 86, "s1": 92, "s2": 92, "s_": [44, 65, 93, 127, 142], "s_1": 46, "s_2": 46, "s_col": [4, 9, 68, 100, 104, 111, 112, 127], "s_i": [36, 68, 100, 104, 127], "s_x": [44, 65, 93], "safeguard": [69, 115, 117], "sake": [66, 94, 103, 178], "same": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 40, 53, 63, 65, 68, 71, 72, 74, 77, 78, 82, 83, 84, 89, 90, 93, 95, 97, 98, 100, 101, 102, 103, 104, 115, 119, 124, 127, 130, 132, 144, 147, 148, 161, 162, 170, 177], "samii": 96, "sampl": [9, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 36, 37, 38, 39, 40, 41, 44, 48, 50, 51, 55, 56, 62, 64, 65, 67, 69, 71, 72, 73, 74, 77, 81, 87, 88, 90, 93, 95, 97, 98, 101, 102, 111, 112, 113, 115, 116, 126, 129, 130, 132, 133, 140, 142, 145, 161, 162, 165, 167, 175, 176, 177], "sample_weight": [48, 50, 51, 55, 56, 100], "sampler": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 78], "sant": [12, 14, 15, 17, 18, 19, 31, 35, 41, 64, 69, 71, 72, 73, 74, 127, 128, 129, 131, 133, 176], "sara": 176, "sasaki": [44, 65, 93, 176], "satisfi": [68, 72, 104, 115, 118, 122, 144, 161], "save": [63, 66, 72, 77, 78, 82, 87, 88, 92, 94, 95, 115, 117, 162, 168, 178], "savefig": 82, "saveguard": 77, "saver": [66, 94, 95], "sc": [71, 74], "scalar": 127, "scale": [17, 19, 63, 65, 70, 85, 90, 96, 99, 103, 161, 162, 165, 167, 172], "scale_color_manu": 63, "scale_fill_manu": [63, 65], "scaled_psi": 90, "scaler": 78, "scatter": [70, 79, 80, 87, 88, 91, 96, 100, 103], "scatterplot": [79, 80], "scenario": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 71, 74, 80, 89, 90, 102, 103, 127, 162, 168, 178], "scene": [83, 84, 86], "scene_camera": 86, "schacht": [77, 86, 92], "schaefer": 96, "schedul": [107, 112, 177], "scheme": [65, 93, 115, 126, 127, 128, 143, 173], "schneider": 67, "schratz": [67, 115, 125, 173, 175], "scienc": [47, 62, 81, 96, 176], "scikit": [72, 77, 94, 115, 118, 173, 175, 177, 178], "scipi": [82, 91], "score": [0, 8, 12, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 41, 42, 48, 49, 50, 51, 52, 55, 56, 57, 58, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 77, 78, 79, 83, 84, 85, 86, 89, 90, 92, 93, 94, 95, 97, 99, 100, 101, 102, 103, 104, 105, 110, 112, 113, 114, 115, 116, 127, 128, 129, 130, 131, 132, 133, 134, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 168, 170, 171, 172, 173, 177, 178], "score_col": [8, 100, 110, 112, 127], "scorer": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "scoring_method": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 52], "script": 174, "sd": 62, "se": [63, 65, 78, 82, 97, 102, 106, 115, 123, 124, 143, 161, 162, 168, 176, 178], "se_aggr": [115, 123], "se_df": 65, "se_dml": [63, 82, 106], "se_dml_po": [63, 82, 106], "se_nonorth": [63, 82], "se_orth_nosplit": [63, 82], "se_orth_po_nosplit": [63, 82], "seaborn": [13, 16, 69, 71, 74, 75, 77, 78, 79, 80, 82, 93, 94, 95, 97, 103, 104], "seamlessli": 79, "search": [12, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 52, 78, 120, 121, 126, 144], "search_mod": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 115, 121], "searchabl": 66, "second": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 44, 63, 65, 67, 77, 78, 79, 82, 92, 93, 105, 106, 143, 161, 162, 163, 172, 175], "secondari": [79, 80], "section": [15, 16, 18, 19, 64, 65, 66, 67, 70, 78, 89, 92, 93, 95, 97, 103, 108, 112, 128, 130, 131, 132, 133, 152, 164, 165, 177], "secur": 96, "see": [10, 11, 12, 14, 15, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 34, 36, 37, 38, 39, 40, 50, 51, 53, 55, 56, 62, 64, 65, 66, 67, 69, 71, 72, 73, 74, 77, 78, 79, 80, 81, 83, 84, 90, 91, 92, 93, 95, 96, 97, 98, 100, 101, 102, 103, 115, 118, 126, 127, 129, 131, 133, 140, 143, 144, 150, 152, 153, 154, 159, 160, 162, 163, 165, 167, 168, 172, 174, 175, 177], "seed": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 40, 48, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 112, 114, 115, 116, 117, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 132, 143, 144, 161, 162, 168, 175, 178], "seek": 96, "seem": [64, 66, 89, 94, 95, 162, 178], "seen": [87, 88, 90], "sel_cols_chiang": 93, "select": [9, 17, 19, 30, 35, 36, 43, 62, 77, 86, 97, 100, 103, 105, 107, 111, 112, 113, 115, 123, 140, 142, 161, 175, 176, 177, 178], "selected_coef": 77, "selected_featur": [67, 115, 123, 124], "selected_learn": 77, "self": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 50, 51, 53, 54, 55, 56, 77, 92, 97, 101, 178], "selfref": 66, "semenova": [83, 84, 176], "semi": 106, "semiparametr": 10, "sens": [102, 103], "sensemakr": [162, 163], "sensit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 59, 113, 114, 127, 132, 163, 168, 172, 177], "sensitivity_analysi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 71, 74, 80, 89, 90, 102, 103, 162, 168, 178], "sensitivity_benchmark": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 80, 89, 102, 103, 162, 163], "sensitivity_el": [162, 168], "sensitivity_param": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 102, 103, 162, 163, 168], "sensitivity_plot": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 80, 89, 102, 103, 162, 168], "sensitivity_summari": [71, 74, 80, 89, 90, 102, 103, 162, 168, 178], "sensitv": 90, "sensitvity_benchmark": 80, "sensiv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40], "senstiv": [162, 171], "sep": 63, "separ": [13, 78, 79, 96, 97, 102, 110, 111, 112, 115, 124, 127, 140, 143, 177], "seper": [92, 97, 100, 102, 161, 162, 163], "seq_len": [63, 68, 106], "sequenc": 93, "sequenti": [11, 115, 123], "ser": [71, 72, 73, 74], "seri": [71, 72, 73, 74, 97, 103, 127, 140, 176], "serial": [97, 127, 140], "serv": [17, 19, 107, 109, 112, 175, 177], "serverless": [176, 177], "servic": 96, "set": [4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 40, 44, 46, 47, 50, 52, 54, 55, 56, 58, 62, 63, 64, 65, 66, 68, 69, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 112, 114, 119, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 133, 140, 143, 144, 145, 146, 147, 148, 149, 152, 161, 162, 163, 169, 170, 171, 174, 175, 177, 178], "set_as_param": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "set_config": [50, 51, 55, 56], "set_fit_request": [55, 56], "set_fold_specif": [115, 124], "set_index": 94, "set_ml_nuisance_param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 66, 75, 94, 115, 118, 119, 121, 122, 124, 126, 177], "set_param": [50, 51, 55, 56, 92, 115, 118], "set_sample_split": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 77, 90, 143, 177], "set_score_request": [50, 51, 55, 56], "set_styl": [94, 95], "set_text": 77, "set_threshold": [63, 64, 65, 66, 67, 68, 105, 115, 124, 125, 126, 127, 143, 144, 161, 175], "set_tick": 93, "set_ticklabel": 93, "set_titl": [79, 80, 90, 92, 93, 100], "set_x_d": [4, 5, 6, 7, 8, 9], "set_xlabel": [79, 80, 82, 90, 92, 93, 100], "set_xlim": 82, "set_xtick": 96, "set_xticklabel": 96, "set_ylabel": [79, 80, 90, 92, 93, 96, 100], "set_ylim": [85, 90, 92, 93, 99], "setdiff": 177, "setdiff1d": 93, "setminu": [65, 93, 161], "settings_l": 92, "settings_m": 92, "setup": [174, 177], "seven": [65, 93], "sever": [59, 66, 67, 71, 72, 73, 74, 77, 78, 79, 92, 94, 95, 102, 103, 106, 115, 178], "shadow": 91, "shape": [8, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 50, 51, 53, 54, 55, 56, 70, 71, 74, 77, 78, 79, 80, 83, 84, 87, 88, 93, 94, 97, 98, 100, 102, 103, 115, 117, 127], "share": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 42, 65, 66, 93, 94], "sharma": [103, 176], "sharp": 48, "shift": [71, 74, 91], "shock": [65, 93], "short": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 41, 102, 103, 127, 131, 162, 163, 176, 177, 178], "shortcut": 66, "shortli": [65, 67, 93, 115, 125], "shota": 176, "should": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 50, 51, 55, 56, 66, 68, 71, 72, 74, 77, 78, 80, 87, 88, 94, 97, 100, 101, 102, 104, 107, 112, 114, 115, 119, 121, 124, 127, 128, 135, 161, 162, 163, 173], "show": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 62, 63, 65, 68, 69, 71, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 86, 89, 90, 92, 93, 96, 97, 100, 101, 103, 104, 106, 127, 140, 162, 171, 174], "show_progress_bar": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 78, 97], "showcas": 98, "showlabel": 103, "showlegend": 103, "shown": [62, 81, 96, 175], "showscal": [83, 84, 86], "shrink": 100, "shuffl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 143], "side": [100, 127, 162, 168], "sigma": [17, 18, 19, 30, 31, 33, 34, 35, 36, 41, 42, 43, 44, 46, 47, 63, 65, 68, 82, 93, 104, 106, 114, 143, 161, 162, 163, 165, 167, 168, 171, 172], "sigma2": [115, 123, 124, 126, 162, 168], "sigma_": [17, 18, 19, 33, 34, 36, 41, 42, 43, 44, 46, 63, 65, 82, 93, 106], "sigma_0": [162, 172], "sigma_j": 161, "sigmoid": [42, 96], "sign": [101, 103], "signal": [53, 54], "signatur": [25, 26, 27, 28, 29, 38, 40, 144], "signif": [62, 64, 65, 66, 67, 68, 115, 123, 124, 125, 126, 127, 143, 144, 161, 175, 178], "signific": [62, 65, 66, 67, 68, 71, 74, 80, 89, 90, 94, 98, 100, 102, 103, 115, 123, 124, 125, 126, 127, 143, 144, 161, 162, 168, 175, 178], "significantli": 79, "silverman": [27, 28, 29], "sim": [17, 18, 19, 31, 32, 33, 34, 35, 36, 41, 42, 43, 44, 45, 46, 47, 49, 63, 64, 65, 68, 70, 82, 85, 93, 98, 99, 104, 106, 127], "sim_data": 73, "similar": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 40, 41, 64, 67, 72, 83, 84, 89, 92, 95, 97, 100, 101, 102, 103, 127, 144, 145, 146], "similarli": [72, 92, 97, 101, 127, 140], "simpl": [32, 55, 56, 64, 67, 78, 83, 84, 87, 88, 89, 90, 91, 97, 98, 103, 113, 127, 162, 163], "simplest": 114, "simpli": [67, 69, 178], "simplic": [66, 77, 94, 98, 103], "simplif": [162, 164], "simplifi": [71, 74, 90, 96, 103, 114, 162, 171], "simul": [17, 18, 19, 31, 32, 33, 34, 35, 36, 41, 42, 43, 45, 46, 47, 63, 67, 68, 71, 73, 74, 77, 82, 83, 84, 85, 86, 87, 88, 92, 97, 99, 100, 103, 104, 106, 115, 119, 124, 126, 161, 175], "simul_data": 30, "simulaten": [127, 135], "simulation_run": 86, "simult": 64, "simultan": [72, 113, 178], "sin": [32, 35, 42, 47, 70, 83, 84, 87, 88], "sinc": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 41, 50, 55, 66, 68, 69, 70, 77, 78, 80, 87, 88, 89, 91, 92, 94, 96, 104, 115, 117, 127, 129, 162, 168, 170, 174, 177], "singl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 69, 71, 72, 73, 74, 78, 87, 88, 95, 96, 115, 123, 161], "single_learner_pipelin": [115, 123], "singleton": 143, "sinh": 47, "sipp": [66, 94, 95], "site": [70, 71, 72, 73, 74, 93, 94, 100, 103], "situat": [65, 93], "six": [17, 19, 65], "sixth": 93, "size": [13, 16, 30, 63, 65, 66, 67, 70, 71, 72, 73, 74, 77, 79, 82, 85, 86, 89, 92, 94, 96, 98, 99, 101, 103, 105, 107, 112, 115, 120, 121, 127, 128, 143, 144, 161, 162, 165, 167, 175, 178], "sizeabl": 103, "skill": 176, "skip": 52, "sklearn": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 47, 48, 50, 51, 54, 55, 56, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 100, 101, 102, 103, 104, 105, 114, 115, 116, 117, 118, 119, 120, 121, 127, 128, 129, 130, 132, 143, 144, 161, 162, 168, 175, 178], "skotara": 103, "slide": 96, "slight": [127, 130, 132], "slightli": [12, 14, 15, 70, 71, 74, 77, 87, 88, 89, 114, 127, 131, 133, 140, 144, 145, 146, 147, 148, 162, 163], "slow": [63, 82, 106], "slower": [63, 82, 106], "small": [32, 68, 69, 70, 78, 79, 90, 98, 104, 127, 140, 162, 163, 170], "smaller": [42, 66, 69, 87, 88, 89, 92, 94, 100, 103, 127, 178], "smallest": [14, 77, 79], "smooth": 45, "smpl": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 63, 65, 77, 82, 93, 143, 144], "smpls_cluster": [65, 93], "smucler": [101, 177], "sn": [69, 71, 74, 75, 77, 78, 79, 80, 82, 93, 94, 95, 97, 103, 104], "so": [17, 19, 55, 56, 62, 66, 67, 68, 69, 71, 72, 74, 81, 92, 94, 96, 103, 104, 115, 119, 161, 178], "social": [96, 176], "societi": [65, 93, 103, 176], "softmax": [17, 19], "softwar": [67, 115, 125, 173, 175, 176, 177], "solari": 177, "sole": [72, 103], "solut": [105, 114, 144], "solv": [20, 65, 93, 114, 115, 126, 127, 130, 131, 132, 161], "solver": [94, 104, 127], "some": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 50, 51, 55, 56, 66, 67, 68, 69, 70, 72, 75, 77, 78, 92, 94, 95, 100, 101, 102, 104, 114, 115, 116, 117, 118, 119, 122, 124, 127, 136, 140, 174, 177], "sometim": [77, 127, 133, 140], "sonabend": [67, 115, 125], "sophist": [115, 121, 126], "sort": [13, 79, 94, 101, 127], "sort_bi": 13, "sort_valu": [79, 80], "sourc": [67, 115, 125, 175, 177], "sourcefileload": 86, "sp": 64, "space": [65, 78, 79, 93, 97, 115, 120, 126], "spars": [86, 115, 120, 121, 126, 161, 175, 176], "sparsiti": 176, "spec": 176, "special": [65, 91, 93, 113, 127], "specialis": [110, 112], "specif": [12, 14, 15, 16, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 42, 48, 65, 66, 71, 72, 73, 74, 77, 78, 79, 80, 90, 93, 94, 97, 103, 107, 112, 113, 114, 115, 117, 119, 122, 124, 127, 130, 132, 140, 143, 144, 152, 161, 168, 172, 173, 175, 177], "specifi": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 42, 48, 57, 58, 62, 65, 66, 67, 68, 69, 71, 72, 73, 74, 78, 79, 80, 81, 83, 84, 85, 87, 88, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 102, 103, 104, 105, 107, 109, 112, 113, 114, 119, 121, 122, 123, 124, 126, 127, 149, 152, 174, 175, 177, 178], "specifii": 95, "speed": [16, 23, 29, 77], "speedup": 77, "spefici": 25, "spindler": [43, 77, 86, 92, 101, 103, 173, 176, 177], "spine": [94, 95], "spline": [83, 84, 114], "spline_basi": [83, 84, 114], "spline_grid": [83, 84], "split": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 48, 62, 65, 67, 68, 69, 71, 72, 73, 74, 77, 90, 93, 95, 97, 98, 102, 104, 113, 114, 115, 116, 126, 127, 129, 130, 132, 140, 144, 161, 175, 177], "split_sampl": [77, 90], "splitter": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "sponsor": [66, 94, 95], "sprintf": 63, "sq_error": 86, "sqrt": [17, 18, 19, 31, 34, 35, 41, 63, 65, 67, 75, 82, 85, 93, 99, 106, 143, 161, 162, 163, 175], "squar": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 51, 56, 66, 79, 86, 94, 115, 116, 126, 127, 162, 172, 176], "squarederror": [66, 94, 178], "squeez": [69, 85, 91, 99, 104], "src": 94, "ssm": [9, 36, 113, 142], "ssrn": 33, "stabil": [17, 19, 89], "stabl": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 127, 128, 173], "stack": [67, 78, 97, 115, 123], "stacking__final_estimator__alpha": 78, "stacking__final_estimator__c": 78, "stacking__final_estimator__max_it": 78, "stacking__lgbm__lambda_l1": 78, "stacking__lgbm__lambda_l2": 78, "stacking__lgbm__learning_r": 78, "stacking__lgbm__max_depth": 78, "stacking__lgbm__min_child_sampl": 78, "stacking__lgbm__n_estim": 78, "stacking_classifi": 78, "stacking_regressor": 78, "stackingclassifi": [78, 100], "stackingregressor": [78, 100], "stackingregressorstackingregressor": 78, "stacklrn": 67, "stackrel": 127, "stage": [48, 72, 83, 84, 87, 88, 91, 98, 100, 115, 126, 127, 177, 178], "stagger": [7, 109, 112, 127, 133], "stai": [127, 133], "standard": [16, 18, 64, 67, 71, 72, 73, 74, 78, 85, 87, 88, 97, 100, 101, 108, 110, 112, 127, 128, 130, 131, 132, 140, 143, 144, 161, 162, 168, 172, 177, 178], "standard_norm": [107, 112, 115, 120, 121, 161, 175], "standardscal": [94, 97], "standardscalerstandardscal": 97, "star": 127, "start": [17, 19, 64, 66, 67, 71, 72, 73, 74, 77, 78, 79, 83, 84, 86, 89, 92, 93, 94, 99, 103, 127, 129, 173, 178], "start_dat": [17, 19], "start_tim": 79, "stat": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 82, 100, 107, 112, 115, 125, 126, 127, 161, 173, 176], "stat_bin": 63, "stat_dens": 66, "state": [78, 178], "static": [7, 39, 45, 71, 72, 73, 74, 109, 112, 127, 130, 132, 140, 176], "static_panel": [7, 39, 97, 109, 112, 127, 140], "stationar": 69, "stationari": [127, 129], "statist": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 36, 37, 38, 39, 40, 44, 59, 65, 93, 101, 102, 103, 161, 162, 168, 173, 175, 176, 177, 178], "statsmodel": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 53, 100], "statu": [64, 66, 68, 69, 94, 96, 100, 104, 127, 133], "std": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 78, 79, 80, 81, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 102, 103, 104, 114, 115, 116, 117, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 132, 143, 144, 161, 175, 178], "stefan": 176, "step": [12, 14, 15, 16, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 63, 66, 67, 78, 82, 87, 88, 89, 94, 97, 98, 106, 115, 126, 127, 161, 173, 178], "stepdown": 161, "stick": [66, 94], "still": [68, 69, 83, 84, 87, 88, 89, 95, 97, 100, 102, 104, 115, 116, 127, 131], "stochast": [38, 39, 40, 127, 139, 140, 141, 175], "stock": [66, 94, 95, 101], "store": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 79, 101, 105, 115, 116, 143, 144, 161, 162, 168, 177], "store_model": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 92, 97], "store_predict": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 64, 94, 98], "stori": [103, 176], "str": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 42, 45, 48, 50, 51, 52, 53, 55, 56, 57, 58, 66, 71, 74, 78, 79, 80, 87, 88, 99, 100, 101, 114, 127, 177], "straightforward": [71, 74, 77, 87, 88, 114], "strategi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 57, 96, 103, 127, 178], "stratifi": [77, 90], "stratum": 96, "strength": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 41, 101, 102, 103, 162, 163, 168, 171], "strftime": 71, "strictli": 127, "string": [12, 13, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 114, 161, 162, 168, 175, 177], "string_label": 96, "strong": [68, 101, 104, 162, 163], "stronger": [101, 127, 131, 161, 178], "structur": [10, 11, 16, 17, 19, 46, 65, 66, 68, 72, 78, 86, 93, 94, 101, 104, 106, 115, 124, 173, 176, 178], "student": 176, "studi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 40, 52, 65, 66, 77, 78, 86, 92, 93, 94, 95, 101, 102, 127, 128, 140, 175, 178], "study_kwarg": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "style": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 71, 74, 92, 177], "styler": 177, "styliz": 103, "sub": [50, 51, 55, 56, 65, 93], "subclass": [109, 112, 177], "subfold": [115, 126], "subgroup": [25, 66, 94, 177], "subject": [65, 93], "submiss": 177, "submit": [71, 74], "submodel": 78, "submodul": [78, 177], "subobject": [50, 51, 55, 56], "subplot": [65, 70, 77, 79, 80, 82, 83, 84, 85, 87, 88, 90, 92, 93, 94, 95, 96, 99, 100], "subplots_adjust": 77, "subpopul": [127, 142], "subsampl": [37, 67, 77, 144, 146], "subscript": [127, 131, 144, 146, 148, 162, 163], "subsequ": [65, 79, 93], "subset": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 50, 55, 65, 77, 78, 93, 97, 98, 105, 114, 115, 116, 162, 163, 165, 167], "subseteq": 114, "substanti": [66, 94, 96], "substract": 161, "subtract": 161, "sudo": 174, "suffic": 103, "suffici": [77, 92, 103], "suggest": [65, 66, 78, 93, 94, 103, 177], "suggest_float": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 78, 97, 115, 120], "suggest_int": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 78, 97], "suitabl": [68, 83, 84, 104, 115, 120, 127, 131], "sum": [17, 19, 51, 56, 65, 66, 93, 94, 95, 99, 100, 114, 161], "sum_": [17, 19, 45, 49, 63, 65, 82, 93, 97, 100, 105, 106, 114, 127, 128, 133, 140, 161], "sum_i": 96, "sum_oth": 93, "sum_riv": 93, "summar": [13, 64, 71, 72, 73, 74, 79, 80, 96, 103, 105, 162, 168], "summari": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 62, 64, 65, 67, 68, 69, 71, 72, 73, 74, 75, 78, 80, 81, 83, 84, 85, 87, 88, 89, 90, 91, 93, 95, 97, 99, 101, 102, 103, 104, 106, 107, 109, 112, 114, 115, 116, 117, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 132, 143, 144, 161, 162, 175, 177, 178], "summary_df": 101, "summary_result": 66, "summary_stat": 79, "superior": 79, "suppli": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 78, 83, 84, 87, 88, 89, 98, 107, 112, 114, 162, 163, 168, 169], "support": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 40, 48, 58, 64, 65, 71, 72, 73, 74, 77, 91, 93, 98, 100, 111, 112, 115, 117, 119, 124, 126, 127, 136, 178], "support_s": [32, 83, 84, 87, 88, 98], "support_t": 98, "support_w": 98, "suppos": [103, 127, 140], "suppress": [64, 66, 67, 68, 78, 97], "suppresswarn": 63, "suprema": 161, "suptitl": [77, 85, 92, 95, 99], "supxlabel": [85, 95, 99], "supylabel": [85, 95, 99], "sure": [79, 80, 115, 122, 177], "surfac": [78, 83, 84, 86], "surgic": 101, "surpress": [65, 175], "survei": [66, 94, 95, 178], "susan": 176, "sven": [103, 173, 176], "svenklaassen": [173, 177], "svg": [63, 82], "switch": [63, 82, 103, 106], "symbol": 103, "symmetr": 47, "syntax": [100, 127], "syntaxwarn": 93, "synthesi": 176, "synthet": [17, 19, 32, 42, 45, 49, 62, 79, 81, 83, 84, 85, 87, 88, 91, 92, 98, 99, 101], "syrgkani": [101, 103, 176], "system": 176, "szita": 176, "t": [4, 5, 7, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 41, 45, 50, 51, 55, 56, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 78, 79, 80, 81, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 108, 109, 112, 114, 115, 116, 117, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 140, 143, 144, 145, 146, 161, 162, 164, 165, 175, 178], "t_": [16, 71, 72, 73, 74, 127, 130, 131, 132, 144, 146, 148], "t_0": [37, 144, 155], "t_1_start": 77, "t_1_stop": 77, "t_2_start": 77, "t_2_stop": 77, "t_3_start": 77, "t_3_stop": 77, "t_col": [4, 5, 7, 15, 16, 71, 72, 73, 74, 97, 108, 109, 112, 127, 128, 129, 130, 132, 140], "t_df": 98, "t_diff": 70, "t_dml": 63, "t_g": [17, 19], "t_i": [69, 98, 100, 127, 129, 130, 133, 144, 146], "t_idx": 70, "t_nonorth": 63, "t_orth_nosplit": 63, "t_sigmoid": 98, "t_stat": 161, "t_value_ev": 14, "t_value_pr": 14, "tabl": [63, 65, 66, 67, 68, 79, 80, 101, 105, 107, 112, 115, 123, 124, 126, 127, 143, 144, 161, 175, 178], "tabpfn": 177, "tabpfnclassifi": 79, "tabpfnregressor": 79, "tabular": [77, 79, 107, 112, 161, 175, 178], "taddi": 176, "tailor": [108, 112], "takatsu": 101, "take": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 40, 41, 68, 69, 70, 71, 72, 73, 74, 77, 78, 83, 84, 85, 86, 87, 88, 95, 97, 99, 100, 101, 102, 104, 105, 114, 115, 117, 127, 128, 133, 136, 137, 138, 139, 140, 141, 144, 149, 152, 162, 169, 170, 171, 175], "taken": [66, 94, 95, 178], "taker": [25, 177], "talk": 178, "target": [12, 14, 15, 20, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 55, 56, 62, 65, 66, 67, 68, 77, 83, 84, 93, 114, 115, 116, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 142, 143, 144, 153, 154, 161, 162, 170, 172, 173, 175, 177, 178], "task": [62, 79, 92, 107, 112, 123, 143, 178], "task_typ": 177, "tau": [49, 70, 85, 95, 96, 99, 100, 114, 127, 144, 150, 153, 154], "tau_": [96, 100, 127], "tau_0": [100, 127], "tau_1": 96, "tau_2": 96, "tau_vec": [85, 95, 99], "tax": [66, 94, 95], "te": [64, 83, 84, 98], "techniqu": [63, 82, 106, 143, 178], "teen": 72, "teichert": 177, "templat": 177, "ten": 92, "tend": [66, 94, 95, 127], "tensor": [83, 84], "tenth": 176, "term": [7, 14, 63, 65, 66, 67, 70, 82, 86, 93, 94, 96, 97, 103, 106, 127, 133, 140, 173, 178], "termin": [67, 115, 125, 126], "terminatorev": 67, "test": [11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 37, 38, 39, 40, 50, 51, 55, 56, 62, 63, 64, 65, 66, 67, 68, 71, 72, 73, 74, 82, 89, 93, 101, 103, 106, 115, 123, 124, 125, 126, 127, 143, 144, 161, 175, 176, 177, 178], "test_id": [65, 143], "test_ind": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "test_indic": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "test_set": 143, "test_siz": 82, "text": [16, 17, 18, 19, 31, 33, 35, 37, 41, 42, 48, 49, 65, 66, 71, 72, 73, 74, 79, 85, 86, 91, 96, 97, 98, 99, 100, 103, 114, 127, 130, 131, 132, 133, 138, 143, 144, 146, 148, 155, 162, 165, 167], "textbf": [105, 115, 124, 126, 178], "textposit": 103, "textrm": [162, 163, 169, 170, 171, 172], "tg": [67, 75, 107, 112, 175], "th": [65, 93], "than": [26, 63, 64, 66, 77, 82, 86, 90, 94, 95, 96, 100, 101, 102, 103, 106, 127, 131, 162, 168, 178], "thank": [64, 66, 67, 94, 177], "thatw": 70, "thei": [17, 19, 64, 66, 70, 87, 88, 94, 96, 101, 107, 112, 127, 162, 172], "them": [13, 66, 67, 83, 84, 85, 89, 92, 94, 99, 127], "theme": [65, 66], "theme_minim": [63, 66, 68], "theorem": [127, 133, 162, 172], "theoret": [77, 103, 143, 176], "theori": [114, 176], "therebi": [65, 67, 93, 178], "therefor": [71, 73, 74, 80, 96, 97, 100, 102, 127, 140, 143, 144, 162, 171], "thereof": 42, "theta": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 47, 63, 65, 67, 68, 69, 70, 71, 74, 77, 78, 79, 80, 82, 86, 89, 90, 93, 97, 100, 102, 103, 104, 105, 106, 107, 112, 114, 115, 117, 120, 121, 125, 126, 127, 128, 130, 131, 132, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 156, 157, 158, 159, 160, 161, 162, 163, 168, 171, 172, 175, 178], "theta_": [17, 19, 71, 74, 80, 100, 103, 114, 127, 134, 135, 161, 162, 172], "theta_0": [25, 26, 32, 37, 38, 39, 40, 63, 65, 66, 68, 78, 80, 82, 83, 84, 86, 87, 88, 93, 94, 97, 103, 104, 106, 114, 127, 129, 136, 137, 139, 140, 141, 142, 144, 153, 154, 157, 158, 161, 162, 169, 170, 172, 175], "theta_d": 79, "theta_dml": [63, 82, 106], "theta_dml_po": [63, 82, 106], "theta_initi": 82, "theta_nonorth": [63, 82], "theta_orth_nosplit": [63, 82], "theta_orth_po_nosplit": [63, 82], "theta_resc": 63, "theta_t": 70, "thi": [4, 12, 13, 14, 15, 16, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 41, 50, 51, 52, 54, 55, 56, 58, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 114, 115, 117, 120, 121, 125, 126, 127, 131, 133, 134, 135, 136, 137, 140, 143, 144, 145, 146, 147, 148, 150, 152, 153, 154, 161, 162, 163, 168, 169, 170, 173, 174, 175, 176, 177, 178], "think": 67, "third": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 63, 72, 73, 82, 93, 106, 143], "thirion": [173, 175], "this_df": [86, 94], "this_split_ind": 93, "those": [64, 66, 72, 94, 95, 101], "though": [62, 81, 96], "thread": [96, 115, 123, 124, 126], "three": [45, 65, 67, 78, 87, 88, 174, 177], "threshold": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 57, 58, 100, 103, 115, 123, 124, 126, 127], "through": [64, 72, 85, 87, 88, 99, 100, 107, 112, 115, 126, 127], "throughout": [89, 101], "thu": [92, 100, 114, 127], "tibbl": 64, "tick": 16, "tick_param": 100, "tight": 82, "tight_layout": [71, 74, 78, 91, 92, 93, 97, 100], "tighter": [78, 100], "tild": [17, 18, 19, 31, 35, 41, 65, 93, 96, 97, 105, 114, 127, 140, 143, 144, 146, 148, 153, 154, 157, 159, 160, 161, 162, 171, 172], "tile": [78, 101], "time": [5, 7, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 43, 44, 45, 63, 64, 65, 66, 68, 69, 70, 72, 78, 79, 82, 86, 87, 88, 93, 94, 95, 97, 100, 102, 103, 104, 108, 109, 112, 127, 128, 129, 130, 131, 132, 133, 140, 144, 162, 176, 177, 178], "time_budget": 92, "time_col": 97, "time_df": 70, "time_period": 70, "time_typ": [17, 19, 45, 71, 74, 97], "timeout": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "titiunik": [127, 176], "titl": [13, 16, 65, 66, 68, 71, 72, 74, 77, 78, 79, 80, 83, 84, 85, 86, 87, 88, 91, 92, 93, 94, 95, 96, 97, 99, 100, 103, 173], "title_fonts": [71, 74], "tmp": [70, 83, 93], "tname": 64, "tnr": [67, 115, 125, 126], "to_datetim": 71, "to_fram": 98, "to_numpi": [85, 89, 95, 99], "to_str": [78, 79, 97], "todo": [65, 75], "toeplitz": 86, "togeth": [87, 88, 111, 112, 161], "toler": 93, "tomasz": [176, 177], "toml": 177, "tongarlak": 177, "too": [58, 77], "tool": [64, 67, 102, 178], "top": [65, 77, 93, 94, 95, 100, 103, 127, 173], "total": [45, 51, 56, 66, 92, 94, 127, 128], "total_width": [78, 79], "tpe": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "tpesampl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 78], "tpot": 92, "track": [108, 110, 112], "tracker": 173, "tradit": [79, 161], "train": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 55, 56, 63, 65, 67, 77, 79, 82, 83, 84, 85, 87, 88, 90, 91, 93, 94, 98, 99, 105, 106, 143], "train_id": [65, 143], "train_ind": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "train_indic": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "train_set": 143, "train_test_split": 82, "transact": 176, "transform": [31, 39, 41, 42, 49, 79, 90, 96, 103, 115, 120, 127, 140, 144, 157, 178], "transformermixin": 97, "translat": [79, 86], "transpos": 70, "treament": 98, "treat": [16, 17, 18, 19, 26, 57, 64, 69, 70, 71, 72, 73, 74, 78, 80, 89, 91, 98, 100, 103, 114, 127, 129, 130, 132, 133, 137, 144, 146, 148, 161, 178], "treat1_param": 96, "treat2_param": 96, "treat_var": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 115, 119, 124], "treatment": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 41, 42, 46, 48, 57, 62, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 80, 81, 86, 89, 91, 92, 93, 97, 98, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 115, 116, 117, 119, 122, 124, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 140, 142, 143, 145, 146, 147, 148, 149, 150, 152, 153, 154, 161, 165, 167, 168, 169, 171, 173, 175, 176, 177, 178], "treatment_df": 70, "treatment_effect": [32, 83, 84], "treatment_level": [22, 23, 78, 79, 80, 90, 127], "treatment_levels_plot": [78, 79], "treatment_lvl": 78, "treatment_var": [4, 5, 6, 7, 8, 9], "tree": [26, 54, 66, 67, 69, 70, 71, 74, 77, 78, 79, 94, 105, 113, 115, 123, 124, 127, 143, 144, 161, 175, 177], "tree_param": [26, 54], "tree_summari": 94, "trees_class": [66, 94], "trend": [64, 69, 70, 71, 74, 93, 127, 129, 131, 133, 176], "tri": [86, 162, 163], "trial": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 78, 97, 115, 120], "trials_datafram": 78, "triangular": [48, 100, 127], "trim": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 66, 94, 95, 103], "trimming_rul": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 95], "trimming_threshold": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 66, 83, 90, 94, 95, 98, 99, 103], "tripl": [127, 140], "trm": [67, 115, 125, 126], "true": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 36, 37, 38, 39, 40, 42, 48, 49, 50, 51, 52, 55, 56, 58, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 112, 115, 123, 124, 125, 126, 127, 129, 140, 143, 144, 149, 150, 153, 154, 159, 160, 161, 162, 164, 165, 166, 167, 172, 175, 178], "true_effect": [70, 83, 84, 87, 88, 101], "true_gatet_effect": 89, "true_group_effect": 89, "true_tau": 100, "truemfunct": 101, "truncat": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 95], "trust": [78, 97], "try": [77, 78, 97, 102], "tune": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 52, 77, 79, 86, 100, 113, 116, 117, 120, 121, 123, 125, 126, 127, 173, 175, 177], "tune_ml_model": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 78, 97, 115, 120, 177], "tune_on_fold": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 115, 125, 126], "tune_r": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "tune_set": [67, 115, 125, 126], "tuned_model": 92, "tuner": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 115, 126], "tunergridsearch": 67, "tuning_result": 78, "tupl": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 42, 71, 74, 127, 130, 131, 132], "turn": 103, "turrel": 47, "tutori": 66, "tw": [94, 95], "twice": 127, "twinx": [79, 80], "two": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 40, 62, 63, 66, 67, 69, 71, 72, 73, 74, 77, 78, 81, 82, 85, 90, 92, 94, 95, 96, 97, 98, 99, 101, 102, 103, 105, 106, 114, 115, 123, 126, 129, 132, 140, 143, 153, 161, 178], "twoclass": 67, "twoearn": [66, 94, 95, 102, 178], "type": [7, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 41, 42, 45, 48, 49, 50, 51, 53, 54, 55, 56, 57, 59, 63, 64, 65, 66, 67, 71, 74, 77, 82, 91, 92, 93, 97, 100, 103, 106, 113, 115, 123, 124, 127, 144, 156, 157, 158, 161, 162, 171, 177, 178], "typeerror": [71, 72, 73, 74], "typic": [78, 79, 127, 133, 173], "u": [18, 25, 26, 27, 28, 29, 31, 32, 34, 36, 41, 51, 56, 63, 64, 65, 66, 69, 70, 71, 72, 74, 77, 78, 79, 80, 82, 85, 87, 88, 90, 93, 94, 95, 97, 98, 99, 101, 102, 103, 106, 127, 134, 136, 137, 162, 163, 174, 178], "u_": [39, 45, 97, 127, 140], "u_hat": [63, 82, 144], "u_i": [33, 36, 43, 47], "u_t": 18, "ubuntu": 100, "uehara": 176, "uhash": 67, "ulf": 176, "unabl": [78, 97], "unambigu": 103, "unbalanc": 42, "uncap": [17, 19], "uncertainti": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 87, 88, 90, 100, 102, 162, 168, 178], "unchang": [50, 51, 55, 56], "uncondit": [66, 71, 74, 94, 178], "unconfounded": [103, 176], "under": [25, 30, 63, 66, 69, 72, 82, 94, 97, 98, 100, 103, 106, 127, 131, 133, 140, 142, 161, 176], "underbrac": [63, 70, 82, 106, 114], "underfit": 92, "underli": [31, 35, 66, 67, 71, 74, 78, 79, 80, 87, 88, 96, 98, 127, 133, 162, 163, 178], "underlin": [65, 93], "underset": [100, 127], "understand": [71, 72, 74, 79, 103], "undesir": [115, 117], "unevenli": 143, "unifi": 112, "uniform": [18, 48, 49, 70, 81, 83, 84, 85, 98, 99, 161], "uniform_averag": [51, 56], "uniformli": [25, 71, 74, 85, 95, 161], "union": [25, 42], "uniqu": [7, 62, 71, 72, 73, 74, 77, 78, 79, 80, 81, 97, 100, 108, 109, 112, 127, 130, 132, 144, 162, 172], "unique_label": 92, "unit": [7, 17, 19, 45, 63, 64, 68, 69, 70, 71, 72, 73, 74, 89, 91, 97, 100, 104, 109, 112, 127, 129, 130, 131, 132, 133, 140, 144, 145, 146, 147, 148, 162, 165, 167, 177], "univari": [32, 83, 84], "univers": [16, 176], "unknown": 127, "unlik": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 66, 94, 95, 103], "unobserv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 62, 66, 71, 74, 81, 94, 95, 97, 102, 103, 127, 140, 162, 163, 172, 178], "unpen": 64, "unstabl": [162, 163], "unter": [65, 66, 67], "untest": 103, "until": [127, 129, 177], "untreat": [91, 103, 127, 129], "untun": 79, "up": [16, 23, 29, 66, 77, 86, 91, 92, 94, 95, 102, 103, 115, 124, 126, 127, 129, 143, 162, 163, 174, 177, 178], "upcom": 177, "updat": [50, 51, 55, 56, 65, 93, 94, 176, 177], "update_layout": [83, 84, 86, 100, 103], "update_trac": [83, 84], "upload": 177, "upon": [144, 177], "upper": [25, 66, 67, 70, 71, 74, 79, 80, 82, 85, 89, 90, 91, 95, 99, 100, 102, 103, 115, 125, 126, 162, 168, 172, 178], "upper_bound": [83, 84], "upsilon": [68, 104], "upsilon_i": [68, 104], "upward": [66, 94, 95, 103], "upweight": 96, "url": [72, 86, 173, 176], "us": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 55, 56, 57, 58, 63, 65, 66, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 82, 83, 84, 85, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 109, 112, 114, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 132, 140, 143, 144, 145, 146, 147, 148, 161, 162, 163, 168, 170, 171, 172, 173, 174, 175, 177, 178], "usa": 176, "usabl": 77, "usag": [64, 71, 72, 73, 74, 75, 78, 79, 80, 89, 91, 93, 94, 95, 102, 107, 175, 177], "use_label_encod": [94, 178], "use_other_treat_as_covari": [4, 5, 6, 7, 8, 9, 107, 112], "use_pred_offset": [115, 123, 126], "use_weight": [115, 123, 124], "usecolormap": [83, 84], "user": [20, 21, 50, 51, 55, 56, 63, 64, 65, 66, 67, 72, 77, 80, 82, 89, 90, 93, 94, 100, 102, 114, 115, 117, 122, 123, 126, 127, 144, 161, 173, 174, 175, 177, 178], "userwarn": [70, 71, 73, 74, 79, 94, 100, 101, 103], "usual": [65, 69, 71, 72, 73, 74, 77, 78, 79, 83, 84, 93, 100, 102, 103, 114, 115, 117, 143, 162, 172], "utab019": 42, "utaf011": [45, 176], "util": [0, 21, 70, 71, 74, 77, 90, 92, 96, 100, 103, 115, 117, 127, 177], "v": [10, 11, 25, 26, 34, 36, 38, 40, 43, 44, 46, 51, 56, 63, 65, 66, 71, 74, 77, 78, 79, 80, 82, 89, 90, 92, 93, 94, 96, 100, 101, 105, 106, 114, 127, 134, 136, 137, 139, 141, 161, 173, 175, 176, 177, 178], "v108": 173, "v12": [173, 175], "v2": 79, "v22": 67, "v23": 173, "v_": [39, 44, 45, 65, 93, 97, 127, 140], "v_i": [33, 34, 36, 46, 47, 63, 82, 106, 127], "v_j": 161, "val": [34, 71, 72, 73, 74, 143, 176], "val_list": 86, "valid": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 37, 38, 39, 40, 52, 57, 58, 63, 64, 65, 66, 69, 70, 71, 72, 74, 77, 79, 82, 85, 92, 93, 94, 95, 99, 100, 103, 106, 113, 114, 115, 123, 126, 143, 144, 150, 153, 154, 162, 163, 176, 178], "valu": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 50, 51, 55, 56, 58, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 100, 101, 102, 103, 105, 107, 108, 109, 112, 113, 115, 116, 117, 119, 121, 123, 124, 125, 126, 127, 128, 133, 134, 142, 143, 150, 153, 154, 159, 160, 161, 162, 163, 168, 172, 175, 177, 178], "value_count": 94, "van": 176, "vanderpla": [173, 175], "vanish": [63, 82, 106], "var": [17, 18, 19, 31, 35, 41, 65, 93, 96, 100, 162, 163, 169, 170, 171, 172], "var_ep": 103, "varepsilon": [25, 31, 41, 44, 65, 68, 93, 104, 114, 127, 136], "varepsilon_": [17, 19, 44, 65, 71, 74, 93], "varepsilon_0": 18, "varepsilon_1": 18, "varepsilon_d": [35, 41], "varepsilon_i": [35, 43, 68, 85, 99, 104], "vari": [17, 19, 45, 66, 70, 71, 74, 77, 94, 96, 103], "variabl": [4, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 41, 42, 45, 48, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 78, 79, 80, 86, 89, 91, 92, 93, 94, 95, 97, 100, 102, 103, 104, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 119, 122, 123, 124, 125, 126, 127, 129, 130, 132, 133, 134, 136, 137, 138, 139, 140, 141, 143, 144, 157, 161, 162, 163, 168, 172, 175, 176, 177, 178], "varianc": [20, 21, 65, 67, 93, 100, 102, 103, 113, 127, 143, 162, 163, 168, 170, 171, 172, 175, 177], "variant": [64, 72, 90, 127, 140], "variat": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 41, 90, 102, 162, 163, 172], "variou": [64, 92, 103, 115, 126, 178], "varoquaux": [173, 175], "vasili": [103, 176], "vast": 79, "vector": [12, 14, 15, 17, 19, 22, 24, 25, 26, 27, 28, 30, 32, 33, 34, 36, 37, 38, 39, 40, 43, 44, 47, 62, 65, 66, 68, 69, 81, 87, 88, 89, 91, 93, 94, 98, 101, 104, 127, 129, 138, 139, 140, 141, 142, 161, 175, 177], "vee": [144, 146, 148], "venv": [100, 174], "verbos": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 66, 69, 70, 71, 74, 77, 78, 79, 82, 85, 92, 95, 97, 99, 100, 103, 115, 120], "veri": [64, 65, 67, 72, 77, 78, 89, 93, 101, 103, 144, 173], "verifi": 96, "versa": [77, 96, 162, 168], "version": [4, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 50, 51, 55, 56, 65, 66, 67, 69, 70, 83, 103, 105, 107, 112, 114, 115, 127, 130, 131, 132, 144, 161, 162, 164, 165, 166, 167, 169, 170, 173, 177], "versoin": 103, "versu": 91, "vertic": [65, 79, 80, 93], "via": [12, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 41, 57, 64, 68, 69, 70, 71, 72, 73, 74, 77, 78, 85, 86, 87, 88, 89, 90, 93, 100, 102, 104, 105, 107, 112, 113, 114, 115, 117, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 132, 143, 150, 160, 161, 162, 163, 168, 172, 173, 174, 175, 176, 177, 178], "viabl": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40], "vice": [77, 96, 162, 168], "victor": [86, 103, 143, 173, 176], "vignett": [64, 177], "villa": [62, 81], "violat": [71, 74], "violet": [85, 95, 99], "vira": 176, "virtual": [72, 174], "virtualenv": 174, "visibl": [95, 100, 103], "visit": [173, 178], "visual": [13, 65, 71, 72, 73, 74, 78, 89, 90, 92, 93, 100], "vmax": 74, "vmin": 74, "vol": 64, "volum": [103, 173], "voluntari": 96, "vv740": 93, "vv760g": 93, "w": [10, 11, 17, 18, 19, 20, 21, 31, 37, 41, 46, 50, 51, 55, 56, 65, 86, 93, 96, 98, 101, 105, 106, 127, 130, 131, 132, 140, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 175], "w24678": 143, "w30302": 176, "w_": [17, 18, 19, 65, 93, 97, 98, 127, 140, 144, 157], "w_1": [17, 18, 19, 98], "w_2": [17, 18, 19, 98], "w_3": [17, 18, 19], "w_4": [17, 18, 19], "w_df": 98, "w_i": [36, 69, 97, 98, 100, 105, 114, 127, 140, 143, 144, 146, 148, 161], "wa": [52, 65, 70, 71, 74, 92, 93, 97, 100, 103, 177], "wage": 72, "wager": 176, "wai": [66, 77, 92, 94, 97, 101, 103, 115, 122, 144, 174], "wander": 47, "wang": 176, "want": [62, 65, 66, 67, 69, 77, 78, 81, 85, 93, 97, 99, 100, 115, 117, 127, 173, 174, 176], "warn": [13, 16, 57, 62, 63, 64, 65, 66, 67, 68, 70, 71, 73, 74, 77, 78, 79, 82, 94, 97, 101, 103, 105, 115, 120, 124, 125, 126, 127, 143, 144, 161, 175, 177], "warn_msg_prefix": 101, "wave": [97, 127, 140], "wayon": 65, "we": [26, 54, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 114, 115, 116, 117, 118, 119, 120, 121, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 137, 143, 144, 146, 148, 151, 155, 161, 162, 163, 172, 174, 175, 177, 178], "weak": [25, 162, 163, 176, 177], "weakest": [71, 74], "wealth": [10, 102], "websit": [66, 67, 72, 115, 122, 173], "wedg": [65, 93], "week": 177, "wei": 161, "weight": [13, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 50, 51, 55, 56, 65, 66, 67, 68, 71, 72, 73, 74, 79, 80, 89, 90, 93, 94, 100, 104, 113, 115, 123, 124, 127, 128, 144, 149, 152, 161, 162, 169, 170, 177], "weights_bar": [22, 23, 26, 90], "weights_dict": 90, "weiss": [173, 175], "well": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 55, 56, 63, 65, 77, 78, 82, 86, 91, 92, 93, 101, 105, 106, 109, 112, 143, 174, 175], "were": [66, 68, 78, 94, 95, 104, 178], "wg": [97, 127, 140], "wg_approx": [39, 127, 140, 144, 157], "what": [64, 77, 86, 176], "when": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 50, 51, 55, 56, 58, 66, 69, 72, 79, 90, 94, 96, 97, 101, 127, 137, 142, 144, 161, 173, 174, 175, 177], "whenev": [66, 94], "whera": [162, 170], "where": [12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 51, 53, 54, 56, 62, 63, 65, 66, 68, 69, 70, 71, 72, 73, 74, 80, 81, 82, 85, 89, 91, 93, 94, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 114, 115, 117, 119, 127, 128, 129, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 168, 169, 170, 172, 174, 175, 177, 178], "wherea": [32, 68, 69, 71, 74, 80, 101, 103, 104, 127, 130, 132, 144, 152, 162, 169, 178], "whether": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 36, 37, 38, 39, 40, 48, 52, 53, 57, 58, 66, 70, 77, 94, 95, 100, 101, 103, 107, 109, 112, 115, 124, 127, 162, 163, 177], "which": [5, 6, 7, 8, 9, 12, 14, 15, 16, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 40, 50, 55, 57, 58, 62, 63, 64, 66, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 86, 89, 92, 94, 95, 97, 98, 100, 102, 103, 104, 106, 107, 109, 112, 114, 115, 117, 118, 125, 126, 127, 131, 140, 144, 161, 162, 163, 168, 169, 170, 172, 174, 177, 178], "while": [39, 62, 81, 127], "white": [65, 87, 88, 93, 103], "whitegrid": [94, 95], "whitnei": [103, 176], "who": [64, 66, 94, 103], "whole": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 63, 69, 82, 100, 106, 115, 126, 144, 145, 162, 163], "whom": 127, "why": [72, 79], "widehat": [71, 72, 73, 74, 127], "width": [13, 16, 63, 65, 79, 83, 84, 86], "wiki": 177, "wiksel": 176, "wild": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 161], "window": 174, "wise": [87, 88, 97], "wish": 174, "within": [39, 48, 65, 71, 72, 73, 74, 79, 87, 88, 93, 97, 98, 100, 127, 140], "without": [25, 35, 48, 62, 63, 71, 72, 73, 74, 77, 78, 79, 81, 82, 92, 103, 106, 113, 115, 119, 124, 127, 162, 163, 174, 177], "wolf": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 161], "won": 103, "word": [48, 100, 127, 177, 178], "work": [50, 51, 55, 56, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 89, 96, 101, 102, 103, 115, 116, 117, 127, 161, 174, 176], "workflow": [78, 173, 177], "workspac": 94, "world": 176, "worri": 103, "wors": [51, 56, 97], "would": [51, 56, 64, 66, 67, 71, 72, 73, 74, 77, 78, 83, 84, 86, 94, 95, 97, 100, 102, 103, 114, 115, 126, 162, 172, 178], "wrapper": [4, 64, 100, 107, 112, 115, 126], "wright": 101, "write": [63, 64, 68, 69, 82, 97, 104, 106, 127, 140, 162, 172], "written": [127, 144, 162, 169, 170], "wrong": [77, 96], "wspace": 77, "wurd": [65, 66, 67], "www": [173, 174], "x": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 55, 56, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 105, 106, 107, 110, 111, 112, 114, 115, 119, 120, 121, 124, 125, 126, 127, 129, 130, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 169, 170, 171, 172, 175, 178], "x0": [78, 79, 80, 96, 100], "x1": [65, 67, 68, 69, 78, 79, 80, 90, 91, 92, 93, 96, 97, 100, 102, 103, 104, 107, 109, 110, 111, 112, 114, 115, 116, 127, 144, 161, 162, 163, 175], "x10": [65, 67, 68, 90, 91, 92, 93, 97, 104, 107, 111, 112, 115, 116, 127, 144, 161, 175], "x100": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x10_lag": 127, "x10_mean": 97, "x11": [65, 67, 68, 91, 93, 97, 104, 107, 111, 112, 115, 116, 127, 144, 161, 175], "x11_lag": 127, "x11_mean": 97, "x12": [65, 67, 68, 91, 93, 97, 104, 107, 111, 112, 115, 116, 127, 144, 161, 175], "x12_lag": 127, "x12_mean": 97, "x13": [65, 67, 68, 91, 93, 97, 104, 107, 111, 112, 115, 116, 127, 144, 161, 175], "x13_lag": 127, "x13_mean": 97, "x14": [65, 67, 68, 91, 93, 97, 104, 107, 111, 112, 115, 116, 127, 144, 161, 175], "x14_lag": 127, "x14_mean": 97, "x15": [65, 67, 68, 91, 93, 97, 104, 107, 111, 112, 115, 116, 127, 144, 161, 175], "x15_lag": 127, "x15_mean": 97, "x16": [65, 67, 68, 91, 93, 97, 104, 107, 111, 112, 115, 116, 127, 144, 161, 175], "x16_lag": 127, "x16_mean": 97, "x17": [65, 67, 68, 91, 93, 97, 104, 107, 111, 112, 115, 116, 127, 144, 161, 175], "x17_lag": 127, "x17_mean": 97, "x18": [65, 67, 68, 91, 93, 97, 104, 107, 111, 112, 115, 116, 127, 144, 161, 175], "x18_lag": 127, "x18_mean": 97, "x19": [65, 67, 68, 91, 93, 97, 104, 107, 111, 112, 115, 116, 127, 144, 161, 175], "x19_lag": 127, "x19_mean": 97, "x1_demean": 97, "x1_lag": 127, "x1_mean": 97, "x1x2x3x4x5x6x7x8x9x10": 65, "x2": [65, 67, 68, 69, 78, 79, 80, 90, 91, 92, 93, 97, 100, 102, 103, 104, 107, 109, 110, 111, 112, 114, 115, 116, 127, 144, 161, 175], "x20": [65, 67, 68, 91, 93, 97, 104, 107, 111, 112, 115, 116, 127, 144, 161, 175], "x20_lag": 127, "x20_mean": 97, "x21": [65, 67, 68, 93, 97, 104, 107, 111, 112, 127, 175], "x21_demean": 97, "x21_lag": [97, 127], "x21_mean": 97, "x22": [65, 67, 68, 93, 97, 104, 107, 111, 112, 127, 175], "x22_demean": 97, "x22_lag": [97, 127], "x22_mean": 97, "x23": [65, 67, 68, 93, 97, 104, 107, 111, 112, 127, 175], "x23_demean": 97, "x23_lag": [97, 127], "x23_mean": 97, "x24": [65, 67, 68, 93, 97, 104, 107, 111, 112, 127, 175], "x24_demean": 97, "x24_lag": [97, 127], "x24_mean": 97, "x25": [65, 67, 68, 93, 97, 104, 107, 111, 112, 127, 175], "x25_demean": 97, "x25_lag": [97, 127], "x25_mean": 97, "x26": [65, 67, 68, 93, 97, 104, 107, 111, 112, 127, 175], "x26_demean": 97, "x26_lag": [97, 127], "x26_mean": 97, "x27": [65, 67, 68, 78, 93, 97, 104, 107, 111, 112, 127, 175], "x27_demean": 97, "x27_lag": [97, 127], "x27_mean": 97, "x28": [65, 67, 68, 93, 97, 104, 107, 111, 112, 127, 175], "x28_demean": 97, "x28_lag": [97, 127], "x28_mean": 97, "x29": [65, 67, 68, 93, 97, 104, 107, 111, 112, 127, 175], "x29_demean": 97, "x29_lag": [97, 127], "x29_mean": 97, "x2_demean": 97, "x2_dummi": 103, "x2_lag": 127, "x2_mean": 97, "x2_preds_control": 103, "x2_preds_treat": 103, "x3": [65, 67, 68, 69, 78, 79, 80, 90, 91, 92, 93, 97, 102, 103, 104, 107, 109, 110, 111, 112, 114, 115, 116, 127, 144, 161, 175], "x30": [65, 67, 68, 93, 97, 104, 107, 111, 112, 127, 175], "x30_demean": 97, "x30_lag": [97, 127], "x30_mean": 97, "x31": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x32": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x33": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x34": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x35": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x36": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x37": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x38": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x39": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x3_demean": 97, "x3_lag": 127, "x3_mean": 97, "x4": [65, 67, 68, 69, 78, 79, 80, 90, 91, 92, 93, 97, 102, 103, 104, 107, 109, 111, 112, 115, 116, 127, 144, 161, 175], "x40": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x41": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x42": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x43": [65, 67, 68, 92, 93, 104, 107, 111, 112, 127, 175], "x44": [65, 67, 68, 92, 93, 104, 107, 111, 112, 127, 175], "x45": [65, 67, 68, 92, 93, 104, 107, 111, 112, 127, 175], "x46": [65, 67, 68, 92, 93, 104, 107, 111, 112, 127, 175], "x47": [65, 67, 68, 92, 93, 104, 107, 111, 112, 127, 175], "x48": [65, 67, 68, 92, 93, 104, 107, 111, 112, 127, 175], "x49": [65, 67, 68, 92, 93, 104, 107, 111, 112, 127, 175], "x4_demean": 97, "x4_lag": 127, "x4_mean": 97, "x5": [65, 67, 68, 90, 91, 92, 93, 97, 103, 104, 107, 109, 111, 112, 115, 116, 127, 144, 161, 175], "x50": [65, 67, 68, 92, 93, 104, 107, 111, 112, 127, 175], "x51": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x52": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x53": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x54": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x55": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x56": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x57": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x58": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x59": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x5_demean": 97, "x5_lag": 127, "x5_mean": 97, "x6": [65, 67, 68, 90, 91, 92, 93, 97, 104, 107, 111, 112, 115, 116, 127, 144, 161, 175], "x60": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x61": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x62": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x63": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x64": [65, 67, 68, 70, 71, 72, 73, 74, 93, 94, 103, 104, 107, 111, 112, 127, 175], "x65": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x66": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x67": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x68": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x69": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x6_demean": 97, "x6_lag": 127, "x6_mean": 97, "x7": [65, 67, 68, 90, 91, 92, 93, 97, 104, 107, 111, 112, 115, 116, 127, 144, 161, 175], "x70": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x71": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x72": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x73": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x74": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x75": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x76": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x77": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x78": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x79": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x7_lag": 127, "x7_mean": 97, "x8": [65, 67, 68, 90, 91, 92, 93, 97, 104, 107, 111, 112, 115, 116, 127, 144, 161, 175], "x80": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x81": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x82": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x83": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x84": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x85": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x86": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x87": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x88": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x89": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x8_lag": 127, "x8_mean": 97, "x9": [65, 67, 68, 90, 91, 92, 93, 97, 104, 107, 111, 112, 115, 116, 127, 144, 161, 175], "x90": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x91": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x92": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x93": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x94": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x95": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x96": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x96x97x98x99x100ydcluster_var_icluster_var_jz": 65, "x97": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x98": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x99": [65, 67, 68, 93, 104, 107, 111, 112, 127, 175], "x9_lag": 127, "x9_mean": 97, "x_": [17, 19, 39, 42, 44, 45, 46, 63, 65, 70, 82, 93, 97, 103, 106, 127, 140, 144, 157], "x_0": [70, 83, 84, 87, 88, 89], "x_1": [17, 18, 19, 31, 35, 37, 38, 40, 41, 70, 83, 84, 85, 87, 88, 89, 91, 99, 103, 127, 138, 139, 141, 162, 163, 175], "x_1x_3": [85, 99], "x_2": [17, 18, 19, 31, 35, 41, 70, 83, 84, 85, 87, 88, 89, 99, 103, 162, 163], "x_3": [17, 18, 19, 31, 35, 41, 70, 83, 84, 87, 88, 89, 162, 163], "x_4": [17, 18, 19, 31, 35, 41, 83, 84, 85, 87, 88, 89, 99], "x_5": [31, 35, 41, 83, 84, 87, 88], "x_6": [83, 84, 87, 88], "x_7": [83, 84, 87, 88], "x_8": [83, 84, 87, 88], "x_9": [83, 84, 87, 88], "x_binary_control": 103, "x_binary_tr": 103, "x_center": [78, 79], "x_col": [4, 5, 6, 7, 8, 9, 16, 62, 65, 66, 67, 71, 72, 73, 74, 81, 86, 93, 94, 95, 97, 98, 100, 101, 102, 103, 107, 108, 109, 112, 115, 117, 127, 128, 130, 132, 175, 177, 178], "x_cols_bench": 103, "x_cols_binari": 103, "x_cols_poli": 93, "x_cols_to_pr": 97, "x_conf": 99, "x_conf_tru": 99, "x_df": 70, "x_domain": 67, "x_end": [78, 79], "x_extra": 97, "x_i": [32, 33, 34, 36, 42, 43, 46, 47, 49, 63, 68, 69, 82, 85, 87, 88, 96, 97, 99, 100, 104, 106, 114, 127, 129, 130, 132, 133, 140, 142, 144, 146, 148], "x_it": [127, 140], "x_jitter": [78, 79], "x_p": [37, 38, 40, 91, 127, 138, 139, 141, 175], "x_poli": 97, "x_rang": [78, 79], "x_start": [78, 79], "x_train": 92, "x_true": [85, 99], "x_var": 67, "xaxis_titl": [83, 84, 86, 100, 103], "xformla": 64, "xgb": 92, "xgb_untuned_l": 92, "xgb_untuned_m": 92, "xgbclassifi": [77, 94, 96, 178], "xgboost": [63, 66, 77, 94, 96, 178], "xgbregressor": [77, 92, 94, 96, 178], "xi": [17, 18, 19, 35, 127], "xi_": 161, "xi_0": [44, 65, 93], "xi_deman": 97, "xi_i": [68, 97, 104, 127, 140], "xiaoji": 176, "xintercept": [63, 68], "xlab": [63, 65, 66], "xlabel": [70, 71, 72, 74, 78, 79, 80, 83, 84, 85, 87, 88, 91, 92, 94, 95, 99], "xlim": [63, 66, 78, 79], "xmax": [78, 79], "xmax_rel": [78, 79], "xmin": [78, 79], "xmin_rel": [78, 79], "xtdml": [127, 140], "xtick": [78, 79, 80, 92, 97], "xval": [67, 115, 123], "xx": 82, "y": [4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 55, 56, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 132, 134, 136, 137, 138, 139, 141, 143, 144, 145, 146, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 161, 162, 163, 168, 169, 170, 171, 172, 175, 178], "y0": [64, 71, 74, 78, 79, 80, 85, 99], "y0_cvar": 85, "y0_quant": [85, 99], "y1": [64, 71, 74, 85, 99], "y1_cvar": 85, "y1_quant": [85, 99], "y_": [16, 17, 19, 39, 44, 45, 65, 68, 69, 70, 71, 74, 93, 97, 104, 127, 129, 130, 132, 133, 140, 142, 144, 146, 148, 157], "y_0": [12, 14, 18, 49, 144, 147], "y_1": [12, 14, 18, 49, 144, 147], "y_col": [4, 5, 6, 7, 8, 9, 16, 62, 63, 65, 66, 67, 68, 71, 72, 73, 74, 81, 83, 84, 86, 87, 88, 90, 93, 94, 95, 97, 98, 100, 101, 102, 105, 106, 107, 108, 109, 111, 112, 115, 123, 124, 127, 128, 130, 132, 143, 144, 175, 177, 178], "y_deman": 97, "y_demean": 97, "y_df": [70, 98], "y_diff": [70, 97, 127], "y_i": [32, 33, 34, 36, 42, 43, 46, 47, 63, 68, 69, 82, 85, 96, 97, 98, 99, 100, 104, 106, 127, 129, 140, 142], "y_it": [127, 140], "y_label": [13, 16], "y_lower_quantil": [71, 74], "y_mean": [71, 74], "y_pred": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 51, 56, 77, 115, 116], "y_train": 92, "y_true": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 51, 56, 77, 115, 116], "y_upper_quantil": [71, 74], "ya": 176, "yasui": 176, "yata": 176, "yaxis_titl": [83, 84, 86, 100, 103], "year": [72, 173], "yerr": [70, 78, 79, 80, 87, 88, 92, 94, 96, 97, 100], "yet": [65, 71, 73, 74, 76, 127, 130, 132, 133], "yggvpl": 93, "yi": [42, 176], "yield": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 127], "yintercept": 66, "ylab": [63, 65, 66], "ylabel": [70, 71, 72, 74, 78, 79, 80, 83, 84, 85, 87, 88, 91, 92, 94, 95, 97, 99], "ylim": 94, "ymax": 66, "ymin": 66, "yname": 64, "york": 176, "you": [50, 51, 55, 56, 62, 63, 70, 71, 72, 73, 74, 78, 79, 81, 93, 102, 127, 173, 174, 178], "your": [77, 174], "ython": 173, "yukun": 176, "yusuk": 176, "yuya": 176, "yy": 82, "z": [4, 5, 6, 7, 8, 9, 17, 18, 19, 25, 27, 30, 31, 33, 35, 36, 38, 41, 43, 44, 62, 65, 66, 68, 71, 74, 81, 83, 84, 86, 93, 94, 99, 101, 103, 104, 114, 127, 136, 139, 144, 151, 153, 156, 160, 161, 177], "z1": [7, 16, 38, 71, 74, 108, 109, 112, 127, 128, 129, 130, 132], "z2": [7, 16, 71, 74, 108, 109, 112, 127, 128, 129, 130, 132], "z3": [7, 16, 71, 74, 108, 109, 112, 127, 128, 129, 130, 132], "z4": [7, 16, 71, 74, 108, 109, 112, 127, 128, 129, 130, 132], "z_": [44, 65, 93], "z_0": [144, 155], "z_1": [31, 35, 41, 71, 74], "z_2": [31, 35, 41], "z_3": [31, 35, 41], "z_4": [31, 35, 41, 71, 74], "z_5": 31, "z_col": [4, 5, 6, 7, 8, 9, 25, 27, 38, 62, 65, 66, 68, 81, 93, 94, 95, 101, 104, 107, 108, 112, 114, 127, 177], "z_i": [36, 43, 68, 99, 104, 127], "z_j": [17, 18, 19, 31, 35, 41], "z_true": 99, "zadik": 176, "zaxis_titl": [83, 84, 86], "zero": [14, 18, 45, 49, 69, 70, 71, 74, 77, 85, 90, 98, 99, 102, 103, 127, 144, 146, 148, 161, 162, 165, 167], "zeros_lik": 99, "zeta": [25, 38, 40, 66, 94, 114, 127, 136, 139, 141, 175], "zeta_": [44, 65, 93], "zeta_0": [44, 65, 93], "zeta_i": [34, 43, 46, 63, 82, 106], "zeta_j": 161, "zhang": [42, 176], "zhao": [12, 14, 15, 18, 31, 35, 41, 64, 69, 71, 74, 127, 129, 133, 176], "zhou": [42, 176], "zimmert": [69, 127, 133, 176], "zip": [83, 84], "zorder": [78, 79, 80, 97], "\u03c4_x0": 96, "\u03c4_x1": 96, "\u2139": 63}, "titles": ["API Reference", "<span class=\"section-number\">1. </span>DoubleML Data Class", "<span class=\"section-number\">3. </span>Datasets", "<span class=\"section-number\">2. </span>DoubleML Models", "<span class=\"section-number\">1.2. </span>doubleml.data.DoubleMLClusterData", "<span class=\"section-number\">1.6. </span>doubleml.data.DoubleMLDIDData", "<span class=\"section-number\">1.1. </span>doubleml.data.DoubleMLData", "<span class=\"section-number\">1.3. </span>doubleml.data.DoubleMLPanelData", "<span class=\"section-number\">1.5. </span>doubleml.data.DoubleMLRDDData", "<span class=\"section-number\">1.4. </span>doubleml.data.DoubleMLSSMData", "<span class=\"section-number\">3.1.1. </span>doubleml.datasets.fetch_401K", "<span class=\"section-number\">3.1.2. </span>doubleml.datasets.fetch_bonus", "<span class=\"section-number\">2.3.4. </span>doubleml.did.DoubleMLDID", "<span class=\"section-number\">2.3.2. </span>doubleml.did.DoubleMLDIDAggregation", "<span class=\"section-number\">2.3.3. </span>doubleml.did.DoubleMLDIDBinary", "<span class=\"section-number\">2.3.5. </span>doubleml.did.DoubleMLDIDCS", "<span class=\"section-number\">2.3.1. </span>doubleml.did.DoubleMLDIDMulti", "<span class=\"section-number\">3.2.15. </span>doubleml.did.datasets.make_did_CS2021", "<span class=\"section-number\">3.2.14. </span>doubleml.did.datasets.make_did_SZ2020", "<span class=\"section-number\">3.2.16. </span>doubleml.did.datasets.make_did_cs_CS2021", "<span class=\"section-number\">5.1. </span>doubleml.double_ml_score_mixins.LinearScoreMixin", "<span class=\"section-number\">5.2. </span>doubleml.double_ml_score_mixins.NonLinearScoreMixin", "<span class=\"section-number\">2.2.2. </span>doubleml.irm.DoubleMLAPO", "<span class=\"section-number\">2.2.3. </span>doubleml.irm.DoubleMLAPOS", "<span class=\"section-number\">2.2.7. </span>doubleml.irm.DoubleMLCVAR", "<span class=\"section-number\">2.2.4. </span>doubleml.irm.DoubleMLIIVM", "<span class=\"section-number\">2.2.1. </span>doubleml.irm.DoubleMLIRM", "<span class=\"section-number\">2.2.6. </span>doubleml.irm.DoubleMLLPQ", "<span class=\"section-number\">2.2.5. </span>doubleml.irm.DoubleMLPQ", "<span class=\"section-number\">2.2.8. </span>doubleml.irm.DoubleMLQTE", "<span class=\"section-number\">2.2.9. </span>doubleml.irm.DoubleMLSSM", "<span class=\"section-number\">3.2.5. </span>doubleml.irm.datasets.make_confounded_irm_data", "<span class=\"section-number\">3.2.3. </span>doubleml.irm.datasets.make_heterogeneous_data", "<span class=\"section-number\">3.2.2. </span>doubleml.irm.datasets.make_iivm_data", "<span class=\"section-number\">3.2.1. </span>doubleml.irm.datasets.make_irm_data", "<span class=\"section-number\">3.2.4. </span>doubleml.irm.datasets.make_irm_data_discrete_treatments", "<span class=\"section-number\">3.2.6. </span>doubleml.irm.datasets.make_ssm_data", "<span class=\"section-number\">2.1.2. </span>doubleml.plm.DoubleMLLPLR", "<span class=\"section-number\">2.1.4. </span>doubleml.plm.DoubleMLPLIV", "<span class=\"section-number\">2.1.3. </span>doubleml.plm.DoubleMLPLPR", "<span class=\"section-number\">2.1.1. </span>doubleml.plm.DoubleMLPLR", "<span class=\"section-number\">3.2.13. </span>doubleml.plm.datasets.make_confounded_plr_data", "<span class=\"section-number\">3.2.9. </span>doubleml.plm.datasets.make_lplr_LZZ2020", "<span class=\"section-number\">3.2.11. </span>doubleml.plm.datasets.make_pliv_CHS2015", "<span class=\"section-number\">3.2.12. </span>doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021", "<span class=\"section-number\">3.2.10. </span>doubleml.plm.datasets.make_plpr_CP2025", "<span class=\"section-number\">3.2.7. </span>doubleml.plm.datasets.make_plr_CCDDHNR2018", "<span class=\"section-number\">3.2.8. </span>doubleml.plm.datasets.make_plr_turrell2018", "<span class=\"section-number\">2.4.1. </span>doubleml.rdd.RDFlex", "<span class=\"section-number\">3.2.17. </span>doubleml.rdd.datasets.make_simple_rdd_data", "<span class=\"section-number\">4.1.2. </span>doubleml.utils.DMLDummyClassifier", "<span class=\"section-number\">4.1.1. </span>doubleml.utils.DMLDummyRegressor", "<span class=\"section-number\">4.1.3. </span>doubleml.utils.DMLOptunaResult", "<span class=\"section-number\">4.1.4. </span>doubleml.utils.DoubleMLBLP", "<span class=\"section-number\">4.1.5. </span>doubleml.utils.DoubleMLPolicyTree", "<span class=\"section-number\">4.1.7. </span>doubleml.utils.GlobalClassifier", "<span class=\"section-number\">4.1.6. </span>doubleml.utils.GlobalRegressor", "<span class=\"section-number\">4.1.9. </span>doubleml.utils.PSProcessor", "<span class=\"section-number\">4.1.8. </span>doubleml.utils.PSProcessorConfig", "<span class=\"section-number\">4.2.1. </span>doubleml.utils.gain_statistics", "<span class=\"section-number\">5. </span>Score Mixin Classes for DoubleML Models", "<span class=\"section-number\">4. </span>Utility Classes and Functions", "R: Basic Instrumental Variables Calculation", "R: Basics of Double Machine Learning", "R: DoubleML for Difference-in-Differences", "R: Cluster Robust Double Machine Learning", "R: Impact of 401(k) on Financial Wealth", "R: Ensemble Learners and More with <code class=\"docutils literal notranslate\"><span class=\"pre\">mlr3pipelines</span></code>", "R: Sample Selection Models", "Python: Difference-in-Differences", "Python: Difference-in-Differences Pre-Testing", "Python: Panel Data with Multiple Time Periods", "Python: Real-Data Example for Multi-Period Difference-in-Differences", "Python: Panel Data Introduction", "Python: Repeated Cross-Sectional Data with Multiple Time Periods", "DML: Bonus Data", "Examples", "Python: Choice of learners", "Python: Hyperparametertuning with Optuna", "Python: Causal Machine Learning with TabPFN", "Python: Average Potential Outcome (APO) Models", "Python: Basic Instrumental Variables calculation", "Python: Basics of Double Machine Learning", "Python: Conditional Average Treatment Effects (CATEs) for IRM models", "Python: Conditional Average Treatment Effects (CATEs) for PLR models", "Python: Conditional Value at Risk of potential outcomes", "Python: First Stage and Causal Estimation", "Python: Group Average Treatment Effects (GATEs) for IRM models", "Python: Group Average Treatment Effects (GATEs) for PLR models", "Python: GATE Sensitivity Analysis", "Python: IRM and APO Model Comparison", "Python: Log-Odds Effects for Logistic PLR models", "DoubleML meets FLAML - How to tune learners automatically within <code class=\"docutils literal notranslate\"><span class=\"pre\">DoubleML</span></code>", "Python: Cluster Robust Double Machine Learning", "Python: Impact of 401(k) on Financial Wealth", "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)", "Python: PLM and IRM for Multiple Treatments", "Python: Static Panel Models with Fixed Effects", "Python: Policy Learning with Trees", "Python: Potential Quantiles and Quantile Treatment Effects", "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)", "Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments", "Python: Sensitivity Analysis", "Python: Sensitivity Analysis for Causal ML", "Python: Sample Selection Models", "<span class=\"section-number\">6. </span>Double machine learning algorithms", "<span class=\"section-number\">1. </span>The basics of double/debiased machine learning", "DoubleMLData from dataframes", "Key arguments", "Key arguments", "Key arguments", "Key arguments", "<span class=\"section-number\">2. </span>Data Backend", "User Guide", "<span class=\"section-number\">4. </span>Heterogeneous treatment effects", "<span class=\"section-number\">7. </span>Learners, hyperparameters and hyperparameter tuning", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "References", "&lt;no title&gt;", "<span class=\"section-number\">3. </span>Models", "&lt;no title&gt;", "Panel Data", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">9. </span>Sample-splitting, cross-fitting and repeated cross-fitting", "<span class=\"section-number\">5. </span>Score functions", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">8. </span>Variance estimation and confidence intervals", "<span class=\"section-number\">10. </span>Sensitivity analysis", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "DoubleML", "Installing DoubleML", "Getting Started", "Double Machine Learning Literature", "Release Notes", "DoubleML Workflow"], "titleterms": {"": 92, "0": 178, "1": [92, 103, 178], "2": [92, 103, 178], "2011": 103, "2023": 103, "3": [92, 103, 178], "4": [103, 178], "401": [66, 94, 95, 102], "5": [103, 178], "6": 178, "7": 178, "95": 92, "A": [65, 93], "ATE": [68, 89, 96, 104], "No": [65, 93], "One": [65, 83, 84, 93], "That": 101, "The": [66, 94, 96, 106, 175], "acknowledg": [64, 173], "acycl": [62, 81], "addit": 96, "adjust": [71, 74, 100], "advanc": [100, 115, 161], "aggreg": [71, 72, 73, 74, 127], "al": 103, "algorithm": [105, 162, 173, 175], "all": [71, 74], "altern": 144, "analysi": [71, 74, 78, 80, 89, 90, 102, 103, 162, 178], "anticip": [71, 74], "api": [0, 92], "apo": [80, 90, 127, 144, 162], "applic": [65, 93, 102], "approach": [63, 77, 82, 97, 106], "ar": 101, "arah": 103, "arbitrari": 96, "archiv": 76, "argument": [108, 109, 110, 111, 112], "arrai": [107, 112], "asset": [66, 94], "assumpt": [97, 103], "att": [69, 71, 72, 73, 74], "augment": 96, "automat": 92, "automl": 92, "averag": [66, 79, 80, 83, 84, 87, 88, 90, 94, 114, 127, 144, 162], "backend": [65, 66, 93, 94, 112, 175, 178], "band": 161, "base": [67, 71, 74], "basic": [62, 63, 71, 74, 78, 81, 82, 106], "benchmark": [102, 103, 162], "bia": [63, 82, 106], "binari": [127, 144], "bonu": 75, "bootstrap": 161, "build": 174, "calcul": [62, 81], "call": 92, "callabl": 144, "case": 76, "cate": [83, 84, 96, 114], "causal": [72, 75, 79, 80, 86, 103, 144, 175, 178], "chernozhukov": 103, "choic": 77, "citat": 173, "class": [1, 60, 61, 65, 93], "cluster": [65, 93], "code": 173, "coeffici": 92, "combin": [71, 74, 86], "compar": [77, 92], "comparison": [64, 78, 79, 90, 92], "comput": [77, 92], "conclus": [92, 103], "conda": 174, "condit": [72, 83, 84, 85, 95, 114, 144], "confid": [92, 101, 161], "construct": 115, "contrast": 80, "control": [71, 74], "correl": 97, "covari": [71, 74, 100], "coverag": [69, 86], "cran": 174, "cre_gener": 97, "cre_norm": 97, "creat": [79, 92], "cross": [65, 69, 74, 93, 127, 129, 143, 144, 162, 175], "custom": [77, 92], "cvar": [85, 95, 114, 144], "dag": [62, 81], "data": [1, 4, 5, 6, 7, 8, 9, 62, 63, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 102, 103, 104, 106, 112, 127, 129, 144, 162, 175, 178], "datafram": [107, 112], "dataset": [2, 10, 11, 17, 18, 19, 31, 32, 33, 34, 35, 36, 41, 42, 43, 44, 45, 46, 47, 49, 75], "debias": [63, 82, 106, 175], "default": 92, "defin": [65, 78, 93], "demo": 64, "depend": 174, "descript": [71, 74], "design": [100, 127], "detail": [64, 71, 72, 73, 74, 78, 127], "develop": 174, "dgp": [63, 78, 79, 80, 82], "did": [3, 12, 13, 14, 15, 16, 17, 18, 19, 64, 127], "differ": [64, 69, 70, 72, 76, 77, 127, 144, 161, 162], "dimension": [83, 84], "direct": [62, 81], "disclaim": 103, "discontinu": [100, 127], "distribut": [68, 104], "dml": [65, 75, 93, 143, 175, 178], "dml1": 105, "dml2": 105, "dmldummyclassifi": 50, "dmldummyregressor": 51, "dmloptunaresult": 52, "doubl": [63, 65, 82, 93, 105, 106, 173, 175, 176], "double_ml_score_mixin": [20, 21], "doubleml": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 64, 66, 67, 79, 81, 92, 94, 102, 103, 161, 173, 174, 178], "doublemlapo": [22, 23, 78], "doublemlblp": 53, "doublemlclusterdata": [4, 65], "doublemlcvar": 24, "doublemldata": [6, 66, 79, 93, 94, 107, 112, 175], "doublemldid": 12, "doublemldidaggreg": 13, "doublemldidbinari": 14, "doublemldidc": 15, "doublemldiddata": [5, 112], "doublemldidmulti": 16, "doublemliivm": 25, "doublemlirm": 26, "doublemllplr": 37, "doublemllpq": 27, "doublemlpaneldata": [7, 71, 74, 112], "doublemlpliv": [38, 65, 93], "doublemlplpr": 39, "doublemlplr": 40, "doublemlpolicytre": 54, "doublemlpq": 28, "doublemlqt": 29, "doublemlrdddata": [8, 112], "doublemlssm": 30, "doublemlssmdata": [9, 112], "effect": [66, 71, 72, 73, 74, 76, 79, 83, 84, 85, 87, 88, 90, 91, 94, 95, 96, 97, 99, 102, 103, 114, 127], "elig": [66, 94], "empir": 86, "ensembl": [67, 100], "error": [65, 93], "estim": [62, 66, 68, 69, 71, 72, 73, 74, 75, 79, 81, 86, 89, 92, 94, 95, 96, 97, 99, 102, 103, 104, 143, 144, 161, 175, 178], "et": 103, "evalu": [77, 79, 92, 115], "event": [71, 72, 73, 74], "exampl": [64, 65, 72, 76, 78, 83, 84, 93, 102, 108, 109, 110, 111, 112], "exploit": [64, 67], "extern": [115, 143], "fd_exact": 97, "featur": [67, 97, 173], "fetch_401k": 10, "fetch_bonu": 11, "figur": 96, "file": 174, "final": 64, "financi": [66, 94, 95], "first": 86, "fit": [65, 92, 93, 143, 175], "fix": 97, "flaml": 92, "flexibl": 100, "fold": [92, 143], "forest": 75, "formul": [103, 178], "from": [64, 67, 107, 112, 174], "full": 92, "function": [61, 64, 65, 93, 144, 175], "fuzzi": [100, 127], "gain_statist": 59, "gate": [87, 88, 89, 114], "gatet": 89, "gener": [2, 63, 76, 78, 79, 80, 82, 92, 100, 106, 162], "get": 175, "github": 174, "global": 100, "globalclassifi": 55, "globalregressor": 56, "graph": [62, 81], "grid": 115, "group": [71, 73, 74, 87, 88, 114], "guid": [78, 113], "helper": [65, 93], "heterogen": [76, 90, 96, 114], "how": [67, 92], "hyperparamet": [78, 90, 97, 115], "hyperparametertun": 78, "identif": 103, "iivm": [66, 94, 127, 144], "impact": [66, 94, 95], "implement": [105, 127, 144, 162], "import": 79, "induc": [63, 82, 106], "infer": [161, 178], "initi": [65, 92, 93], "insight": 79, "instal": 174, "instrument": [62, 81, 101], "integr": 64, "interact": [66, 87, 94, 98, 127, 144, 162], "interv": [92, 101, 161], "introduct": 73, "invers": 96, "irm": [3, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 66, 75, 83, 87, 90, 94, 96, 98, 102, 114, 127, 144, 162], "iv": [62, 66, 81, 94, 127, 144], "k": [66, 94, 95, 102, 143], "kei": [79, 108, 109, 110, 111, 112], "lambda": 86, "lasso": [75, 86], "latest": 174, "lear": [65, 93], "learn": [63, 65, 79, 82, 93, 98, 105, 106, 114, 173, 175, 176], "learner": [67, 75, 77, 78, 90, 92, 100, 115, 175], "less": 92, "level": 127, "linear": [66, 71, 74, 88, 94, 96, 100, 127, 144, 162], "linearscoremixin": 20, "literatur": 176, "load": [65, 75, 93, 103], "loader": 2, "local": [66, 94, 95, 99, 100, 144], "log": 91, "logist": [91, 127, 144], "loss": 86, "lplr": [127, 144], "lpq": [99, 144], "lqte": [95, 99], "m": 143, "machin": [63, 65, 79, 82, 93, 105, 106, 173, 175, 176], "main": 173, "mainten": 173, "make_confounded_irm_data": 31, "make_confounded_plr_data": 41, "make_did_cs2021": 17, "make_did_cs_cs2021": 19, "make_did_sz2020": 18, "make_heterogeneous_data": 32, "make_iivm_data": 33, "make_irm_data": 34, "make_irm_data_discrete_treat": 35, "make_lplr_lzz2020": 42, "make_pliv_chs2015": 43, "make_pliv_multiway_cluster_ckms2021": 44, "make_plpr_cp2025": 45, "make_plr_ccddhnr2018": 46, "make_plr_turrell2018": 47, "make_simple_rdd_data": 49, "make_ssm_data": 36, "mar": [68, 104], "market": [65, 93], "matric": [107, 112], "meet": 92, "method": [79, 92, 178], "metric": [77, 92], "minimum": 115, "miss": [68, 104], "missing": [127, 144], "mixin": 60, "ml": [63, 64, 82, 103, 106, 178], "mlr3": 67, "mlr3extralearn": 67, "mlr3learner": 67, "mlr3pipelin": 67, "model": [3, 60, 66, 68, 75, 78, 79, 80, 83, 84, 87, 88, 90, 91, 92, 94, 96, 97, 98, 101, 103, 104, 114, 127, 143, 144, 161, 162, 175, 178], "modul": 75, "more": 67, "motiv": [65, 93], "multi": 72, "multipl": [71, 74, 80, 96, 127], "multipli": 161, "naiv": [62, 81], "net": [66, 94], "neyman": [144, 175], "nonignor": [68, 104, 127, 144], "nonlinearscoremixin": 21, "nonrespons": [68, 104, 127, 144], "note": 177, "nuisanc": [78, 92, 175], "object": [65, 79, 93, 102], "odd": 91, "option": 174, "optuna": 78, "orthogon": [63, 82, 106, 144, 175], "out": [63, 82, 106], "outcom": [68, 69, 79, 80, 85, 104, 114, 127, 144, 162], "over": 161, "overcom": [63, 82, 106], "overfit": [63, 82, 106], "overlap": 96, "packag": [64, 66, 94, 174], "panel": [69, 71, 73, 97, 127, 129, 144, 162], "parallel": 72, "paramet": [67, 75, 92, 127, 144], "partial": [63, 66, 82, 88, 94, 96, 106, 127, 144, 162], "particip": [66, 94], "partit": 143, "penalti": 86, "perform": [64, 79, 96], "period": [71, 72, 74, 127, 144, 162], "pip": 174, "pipelin": [78, 97, 115], "pliv": [127, 144], "plm": [3, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 96, 127, 144, 162], "plot": [65, 92, 93], "plpr": [127, 144], "plr": [66, 75, 84, 88, 91, 94, 114, 127, 144, 162], "polici": [98, 114], "potenti": [79, 80, 85, 95, 99, 114, 127, 144, 162], "pq": [99, 114, 144], "pre": 70, "predict": [64, 115], "preprocess": [67, 97], "problem": 178, "process": [63, 65, 78, 79, 80, 82, 93, 106], "product": [65, 93], "propens": 96, "provid": 143, "psprocessor": 57, "psprocessorconfig": 58, "python": [69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 115, 174], "qte": [99, 114], "qualiti": 86, "quantil": [95, 99, 114, 144], "question": 72, "r": [62, 63, 64, 65, 66, 67, 68, 76, 115, 174], "random": [68, 75, 97, 104, 127, 144], "rank": 96, "rdd": [3, 48, 49, 100, 127], "rdflex": 48, "real": [65, 72, 93], "refer": [0, 62, 64, 65, 67, 77, 81, 86, 92, 93, 96, 101, 103, 106, 115, 125, 143, 161, 173, 175], "regress": [66, 87, 88, 94, 98, 100, 127, 144, 162], "regular": [63, 82, 106], "releas": [174, 177], "remark": 64, "remov": [63, 82, 106], "repeat": [69, 74, 127, 129, 143, 144, 162], "repetit": 143, "requir": 115, "research": 72, "respect": [65, 93], "result": [65, 66, 78, 93, 94, 96], "risk": [85, 95, 114, 144], "robust": [65, 93, 101], "run": 101, "sampl": [63, 68, 82, 92, 104, 106, 127, 143, 144], "sandbox": 76, "score": [60, 63, 82, 96, 106, 144, 175], "search": 115, "section": [69, 74, 127, 129, 144, 162], "select": [68, 71, 74, 104, 127, 144], "sensit": [71, 74, 80, 89, 90, 102, 103, 162, 178], "set": [67, 115], "setup": 79, "sharp": [100, 127], "simpl": [63, 82, 106], "simul": [62, 65, 69, 81, 93, 101, 102], "simultan": 161, "singl": 80, "small": 101, "sourc": [173, 174], "special": 112, "specif": [162, 178], "specifi": [75, 115, 144], "split": [63, 82, 106, 143], "ssm": 127, "stack": 100, "stage": 86, "standard": [65, 77, 93], "start": 175, "static": 97, "step": 92, "structur": 79, "studi": [71, 72, 73, 74, 76], "summari": [66, 79, 92, 94, 96], "tabpfn": 79, "takeawai": 79, "test": 70, "theori": 162, "time": [71, 73, 74, 77, 92], "train": 92, "transform": 97, "treat": 90, "treatment": [66, 79, 83, 84, 85, 87, 88, 90, 94, 95, 96, 99, 114, 127, 144, 162], "tree": [98, 114], "trend": 72, "tune": [67, 78, 92, 97, 115], "two": [65, 83, 84, 93, 127, 144, 162], "type": 112, "uncondit": 72, "under": [68, 96, 104], "univers": [71, 74], "untun": [78, 92], "up": 67, "us": [62, 64, 67, 75, 81, 92, 115], "usag": [108, 109, 110, 111, 112], "user": 113, "util": [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61], "v": 86, "valid": 161, "valu": [85, 95, 114, 144], "vanderweel": 103, "variabl": [62, 81, 101], "varianc": 161, "version": 174, "via": 144, "visual": [79, 91], "wai": [65, 93], "weak": 101, "wealth": [66, 94, 95], "weight": [96, 114], "wg_approx": 97, "when": 92, "whl": 174, "within": 92, "without": [100, 143], "workflow": 178, "xgboost": 92, "zero": [65, 93]}})