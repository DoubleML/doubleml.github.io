Search.setIndex({"alltitles": {"(One-Way) Cluster Robust Double Machine Learing": [[54, "(One-Way)-Cluster-Robust-Double-Machine-Learing"], [77, "(One-Way)-Cluster-Robust-Double-Machine-Learing"]], "0. Problem Formulation": [[141, "problem-formulation"]], "1. Data Backend": [[141, "data-backend"]], "1. Formulation of Causal Model & Identification Assumptions": [[85, "1.-Formulation-of-Causal-Model-&-Identification-Assumptions"]], "2. Causal Model": [[141, "causal-model"]], "2. Estimation of Causal Effect": [[85, "2.-Estimation-of-Causal-Effect"]], "3. ML Methods": [[141, "ml-methods"]], "3. Sensitivity Analysis": [[85, "3.-Sensitivity-Analysis"]], "4. Benchmarking Analysis": [[85, "4.-Benchmarking-Analysis"]], "4. DML Specifications": [[141, "dml-specifications"]], "5. Conclusion": [[85, "5.-Conclusion"]], "5. Estimation": [[141, "estimation"]], "6. Inference": [[141, "inference"]], "7. Sensitivity Analysis": [[141, "sensitivity-analysis"]], "A Motivating Example: Two-Way Cluster Robust DML": [[54, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"], [77, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"]], "API Reference": [[0, null]], "ATE Estimation and Sensitivity": [[73, "ATE-Estimation-and-Sensitivity"]], "ATE estimates distribution": [[57, "ATE-estimates-distribution"], [57, "id3"], [86, "ATE-estimates-distribution"], [86, "id3"]], "ATT Estimation": [[60, "ATT-Estimation"], [60, "id1"], [61, "ATT-Estimation"]], "ATTE Estimation": [[58, "ATTE-Estimation"], [58, "id2"]], "Acknowledgements": [[136, "acknowledgements"]], "Acknowledgements and Final Remarks": [[53, "Acknowledgements-and-Final-Remarks"]], "Additional Results: CATE estimates": [[80, "Additional-Results:-CATE-estimates"]], "Advanced: External Predictions": [[94, "advanced-external-predictions"]], "Advanced: Global and Local Learners, Stacked Ensembles": [[83, "Advanced:-Global-and-Local-Learners,-Stacked-Ensembles"]], "Aggregated Effects": [[60, "Aggregated-Effects"]], "Aggregation Details": [[60, "Aggregation-Details"], [61, "Aggregation-Details"]], "Algorithm DML1": [[87, "algorithm-dml1"]], "Algorithm DML2": [[87, "algorithm-dml2"]], "All combinations": [[60, "All-combinations"]], "Anticipation": [[60, "Anticipation"]], "Application Results": [[54, "Application-Results"], [77, "Application-Results"]], "Application: 401(k)": [[84, "Application:-401(k)"]], "AutoML with less Computation time": [[76, "AutoML-with-less-Computation-time"]], "Average Potential Outcome (APOs)": [[64, "Average-Potential-Outcome-(APOs)"]], "Average Potential Outcomes (APOs)": [[95, "average-potential-outcomes-apos"], [109, "average-potential-outcomes-apos"], [125, "average-potential-outcomes-apos"]], "Average Potential Outcomes (APOs) for Multiple Treatment Levels": [[95, "average-potential-outcomes-apos-for-multiple-treatment-levels"]], "Average Treatment Effect": [[74, "Average-Treatment-Effect"]], "Average Treatment Effect on the Treated": [[74, "Average-Treatment-Effect-on-the-Treated"]], "Basics": [[60, "Basics"]], "Benchmarking": [[125, "benchmarking"]], "Benchmarking Analysis": [[84, "Benchmarking-Analysis"]], "Binary Interactive Regression Model (IRM)": [[95, "binary-interactive-regression-model-irm"], [109, "binary-interactive-regression-model-irm"]], "CATEs for IRM models": [[93, "cates-for-irm-models"]], "CATEs for PLR models": [[93, "cates-for-plr-models"]], "CVaR Treatment Effects": [[69, "CVaR-Treatment-Effects"]], "CVaR of potential outcomes": [[93, "cvar-of-potential-outcomes"]], "CVaR treatment effects": [[93, "cvar-treatment-effects"]], "Causal Analysis with DoubleML": [[85, "Causal-Analysis-with-DoubleML"]], "Causal Contrasts": [[64, "Causal-Contrasts"]], "Causal estimation vs. lasso penalty \\lambda": [[70, "Causal-estimation-vs.-lasso-penalty-\\lambda"]], "Chernozhukov et al. (2023): Benchmarking": [[85, "Chernozhukov-et-al.-(2023):-Benchmarking"]], "Citation": [[136, "citation"]], "Cluster Robust Cross Fitting": [[54, "Cluster-Robust-Cross-Fitting"], [77, "Cluster-Robust-Cross-Fitting"]], "Cluster Robust Standard Errors": [[54, "Cluster-Robust-Standard-Errors"], [77, "Cluster-Robust-Standard-Errors"]], "Clustering and double machine learning": [[54, "Clustering-and-double-machine-learning"], [77, "Clustering-and-double-machine-learning"]], "Combined loss vs. lasso penalty \\lambda": [[70, "Combined-loss-vs.-lasso-penalty-\\lambda"]], "Compare Metrics for Nuisance Estimation": [[76, "Compare-Metrics-for-Nuisance-Estimation"]], "Comparing different learners": [[75, "Comparing-different-learners"]], "Comparison and summary": [[76, "Comparison-and-summary"]], "Comparison to AutoML with less Computation time and Untuned XGBoost Learners": [[76, "Comparison-to-AutoML-with-less-Computation-time-and-Untuned-XGBoost-Learners"]], "Comparison to did package": [[53, "Comparison-to-did-package"]], "Computation time": [[75, "Computation-time"]], "Conclusion": [[76, "Conclusion"]], "Conditional Value at Risk (CVaR)": [[69, "Conditional-Value-at-Risk-(CVaR)"]], "Conditional average treatment effects (CATEs)": [[93, "conditional-average-treatment-effects-cates"]], "Conditional value at risk (CVaR)": [[93, "conditional-value-at-risk-cvar"], [109, "conditional-value-at-risk-cvar"]], "Confidence bands and multiplier bootstrap for valid simultaneous inference": [[124, "confidence-bands-and-multiplier-bootstrap-for-valid-simultaneous-inference"]], "Control Groups": [[60, "Control-Groups"]], "Coverage Simulation": [[58, "Coverage-Simulation"], [58, "id3"]], "Cross-fitting with K folds": [[108, "cross-fitting-with-k-folds"]], "Cross-fitting, DML algorithms and Neyman-orthogonal score functions": [[138, "cross-fitting-dml-algorithms-and-neyman-orthogonal-score-functions"]], "Custom evaluation metrics": [[75, "Custom-evaluation-metrics"]], "DML: Bonus Data": [[62, null]], "Data": [[55, "Data"], [57, "Data"], [57, "id1"], [58, "Data"], [58, "id1"], [60, "Data"], [61, "Data"], [67, "Data"], [68, "Data"], [69, "Data"], [71, "Data"], [72, "Data"], [73, "Data"], [74, "Data"], [78, "Data"], [79, "Data"], [81, "Data"], [82, "Data"], [82, "id1"], [84, "Data"], [86, "Data"], [86, "id1"], [138, "data"]], "Data Backend": [[91, null]], "Data Description": [[60, "Data-Description"]], "Data Details": [[60, "Data-Details"]], "Data Generating Process (DGP)": [[52, "Data-Generating-Process-(DGP)"], [64, "Data-Generating-Process-(DGP)"], [66, "Data-Generating-Process-(DGP)"]], "Data Generation": [[76, "Data-Generation"]], "Data Simulation": [[51, "Data-Simulation"], [65, "Data-Simulation"]], "Data and Effect Estimation": [[84, "Data-and-Effect-Estimation"]], "Data generating process": [[88, "data-generating-process"]], "Data preprocessing": [[56, "Data-preprocessing"]], "Data with Anticipation": [[60, "Data-with-Anticipation"]], "Data-Backend for Cluster Data": [[54, "Data-Backend-for-Cluster-Data"], [77, "Data-Backend-for-Cluster-Data"]], "Dataset Generators": [[2, "dataset-generators"]], "Dataset Loaders": [[2, "dataset-loaders"]], "Datasets": [[2, null]], "Define Helper Functions for Plotting": [[54, "Define-Helper-Functions-for-Plotting"], [77, "Define-Helper-Functions-for-Plotting"]], "Demo Example from did": [[53, "Demo-Example-from-did"]], "Details on Predictive Performance": [[53, "Details-on-Predictive-Performance"]], "Difference-in-Differences": [[63, "difference-in-differences"]], "Difference-in-Differences Models": [[109, "difference-in-differences-models"], [125, "difference-in-differences-models"]], "Difference-in-Differences Models (DID)": [[95, "difference-in-differences-models-did"]], "Difference-in-Differences for Panel Data": [[125, "difference-in-differences-for-panel-data"]], "Difference-in-Differences for repeated cross-sections": [[125, "difference-in-differences-for-repeated-cross-sections"]], "Disclaimer": [[85, "Disclaimer"]], "Double Machine Learning Algorithm": [[136, "double-machine-learning-algorithm"]], "Double Machine Learning Literature": [[139, null]], "Double machine learning algorithms": [[87, null]], "Double/debiased machine learning": [[52, "Double/debiased-machine-learning"], [66, "Double/debiased-machine-learning"], [88, "double-debiased-machine-learning"]], "DoubleML": [[136, null]], "DoubleML Data Class": [[1, null]], "DoubleML Models": [[3, null]], "DoubleML Object": [[84, "DoubleML-Object"]], "DoubleML Workflow": [[141, null]], "DoubleML meets FLAML - How to tune learners automatically within DoubleML": [[76, null]], "DoubleMLData": [[91, "doublemldata"]], "DoubleMLData from arrays and matrices": [[89, "doublemldata-from-arrays-and-matrices"], [91, "doublemldata-from-arrays-and-matrices"]], "DoubleMLData from dataframes": [[89, null], [91, "doublemldata-from-dataframes"]], "DoubleMLPanelData": [[60, "DoubleMLPanelData"], [91, "doublemlpaneldata"]], "Effect Aggregation": [[61, "Effect-Aggregation"], [95, "effect-aggregation"]], "Effect Heterogeneity": [[63, "effect-heterogeneity"], [74, "Effect-Heterogeneity"]], "Empirical coverage vs. lasso penalty \\lambda": [[70, "Empirical-coverage-vs.-lasso-penalty-\\lambda"]], "Estimate DML models without sample-splitting": [[108, "estimate-dml-models-without-sample-splitting"]], "Estimate double/debiased machine learning models": [[138, "estimate-double-debiased-machine-learning-models"]], "Estimating Potential Quantiles and Quantile Treatment Effects": [[79, "Estimating-Potential-Quantiles-and-Quantile-Treatment-Effects"]], "Estimating local quantile treatment effects (LQTEs)": [[79, "Estimating-local-quantile-treatment-effects-(LQTEs)"]], "Estimating the Average Treatment Effect of 401(k) Eligibility on Net Financial Assets": [[55, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"], [78, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"]], "Estimating the treatment effect on the Conditional Value a Risk (CVaR)": [[79, "Estimating-the-treatment-effect-on-the-Conditional-Value-a-Risk-(CVaR)"]], "Estimation": [[57, "Estimation"], [57, "id2"], [86, "Estimation"], [86, "id2"]], "Estimation quality vs. \\lambda": [[70, "Estimation-quality-vs.-\\lambda"]], "Evaluate learners": [[94, "evaluate-learners"]], "Event Study Aggregation": [[60, "Event-Study-Aggregation"], [61, "Event-Study-Aggregation"]], "Example: Sensitivity Analysis for Causal ML": [[85, null]], "Examples": [[63, null]], "Exploiting the Functionalities of did": [[53, "Exploiting-the-Functionalities-of-did"]], "Externally provide a sample splitting / partition": [[108, "externally-provide-a-sample-splitting-partition"]], "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)": [[83, null]], "Fuzzy RDD": [[83, "Fuzzy-RDD"]], "Fuzzy RDD Without Adjustment": [[83, "Fuzzy-RDD-Without-Adjustment"]], "Fuzzy RDD with Flexible Adjustment": [[83, "Fuzzy-RDD-with-Flexible-Adjustment"]], "Fuzzy RDD with Linear Adjustment": [[83, "Fuzzy-RDD-with-Linear-Adjustment"]], "Fuzzy Regression Discontinuity Design": [[95, "fuzzy-regression-discontinuity-design"]], "GATE Estimation and Sensitivity": [[73, "GATE-Estimation-and-Sensitivity"]], "GATET Estimation and Sensitivity": [[73, "GATET-Estimation-and-Sensitivity"]], "GATEs for IRM models": [[93, "gates-for-irm-models"]], "GATEs for PLR models": [[93, "gates-for-plr-models"]], "General Examples": [[63, "general-examples"]], "General algorithm": [[125, "general-algorithm"]], "Generate Fuzzy Data": [[83, "Generate-Fuzzy-Data"]], "Generate Sharp Data": [[83, "Generate-Sharp-Data"]], "Getting Started": [[138, null]], "Group Aggregation": [[60, "Group-Aggregation"], [61, "Group-Aggregation"]], "Group Average Treatment Effects (GATEs)": [[71, "Group-Average-Treatment-Effects-(GATEs)"], [72, "Group-Average-Treatment-Effects-(GATEs)"]], "Group average treatment effects (GATEs)": [[93, "group-average-treatment-effects-gates"]], "Group-Time Combinations": [[60, "Group-Time-Combinations"]], "Heterogeneous treatment effects": [[93, null]], "How to exploit more features of mlr3pipelines in DoubleML": [[56, "How-to-exploit-more-features-of-mlr3pipelines-in-DoubleML"]], "Hyperparameter tuning": [[94, "hyperparameter-tuning"], [94, "id16"]], "Hyperparameter tuning with pipelines": [[94, "hyperparameter-tuning-with-pipelines"]], "Implementation": [[125, "implementation"]], "Implementation Details": [[95, "implementation-details"]], "Implementation of the double machine learning algorithms": [[87, "implementation-of-the-double-machine-learning-algorithms"]], "Implementation of the score function and the estimate of the causal parameter": [[109, "implementation-of-the-score-function-and-the-estimate-of-the-causal-parameter"]], "Implemented Neyman orthogonal score functions": [[109, "implemented-neyman-orthogonal-score-functions"]], "Initialize DoubleMLClusterData object": [[54, "Initialize-DoubleMLClusterData-object"], [77, "Initialize-DoubleMLClusterData-object"]], "Initialize the objects of class DoubleMLPLIV": [[54, "Initialize-the-objects-of-class-DoubleMLPLIV"], [77, "Initialize-the-objects-of-class-DoubleMLPLIV"]], "Installing DoubleML": [[137, null]], "Instrumental Variables Directed Acyclic Graph (IV - DAG)": [[51, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"], [65, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"]], "Interactive IV Model (IIVM)": [[55, "Interactive-IV-Model-(IIVM)"], [78, "Interactive-IV-Model-(IIVM)"]], "Interactive IV model (IIVM)": [[95, "interactive-iv-model-iivm"], [109, "interactive-iv-model-iivm"]], "Interactive Regression Model (IRM)": [[55, "Interactive-Regression-Model-(IRM)"], [71, "Interactive-Regression-Model-(IRM)"], [78, "Interactive-Regression-Model-(IRM)"], [81, "Interactive-Regression-Model-(IRM)"]], "Interactive regression model (IRM)": [[125, "interactive-regression-model-irm"]], "Interactive regression models (IRM)": [[95, "interactive-regression-models-irm"], [109, "interactive-regression-models-irm"], [125, "interactive-regression-models-irm"]], "Learners and Hyperparameters": [[74, "Learners-and-Hyperparameters"]], "Learners to estimate the nuisance models": [[138, "learners-to-estimate-the-nuisance-models"]], "Learners, hyperparameters and hyperparameter tuning": [[94, null]], "Linear Covariate Adjustment": [[60, "Linear-Covariate-Adjustment"]], "Load Data": [[85, "Load-Data"]], "Load and Process Data": [[54, "Load-and-Process-Data"], [77, "Load-and-Process-Data"]], "Load bonus data using the dml datasets module": [[62, "Load-bonus-data-using-the-dml-datasets-module"]], "Local Average Treatment Effects of 401(k) Participation on Net Financial Assets": [[55, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"], [78, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"]], "Local Potential Quantile Estimation": [[82, "Local-Potential-Quantile-Estimation"]], "Local Potential Quantiles (LPQs)": [[82, "Local-Potential-Quantiles-(LPQs)"]], "Local Quantile Treatment Effects (LQTEs)": [[82, "Local-Quantile-Treatment-Effects-(LQTEs)"]], "Local potential quantiles (LPQs)": [[109, "local-potential-quantiles-lpqs"]], "Main Features": [[136, "main-features"]], "Minimum requirements for learners": [[94, "minimum-requirements-for-learners"], [94, "id2"]], "Missingness at Random": [[95, "missingness-at-random"], [109, "missingness-at-random"]], "Model-specific implementations": [[125, "model-specific-implementations"]], "Models": [[95, null]], "Motivation": [[54, "Motivation"], [77, "Motivation"]], "Multiple Average Potential Outcome Models (APOS)": [[64, "Multiple-Average-Potential-Outcome-Models-(APOS)"]], "Naive estimation": [[51, "Naive-estimation"], [65, "Naive-estimation"]], "No Clustering / Zero-Way Clustering": [[54, "No-Clustering-/-Zero-Way-Clustering"], [77, "No-Clustering-/-Zero-Way-Clustering"]], "Nonignorable Nonresponse": [[95, "nonignorable-nonresponse"], [109, "nonignorable-nonresponse"]], "One-Way Clustering with Respect to the Market": [[54, "One-Way-Clustering-with-Respect-to-the-Market"], [77, "One-Way-Clustering-with-Respect-to-the-Market"]], "One-Way Clustering with Respect to the Product": [[54, "One-Way-Clustering-with-Respect-to-the-Product"], [77, "One-Way-Clustering-with-Respect-to-the-Product"]], "One-dimensional Example": [[67, "One-dimensional-Example"], [68, "One-dimensional-Example"]], "Outcome missing at random (MAR)": [[57, "Outcome-missing-at-random-(MAR)"], [86, "Outcome-missing-at-random-(MAR)"]], "Outcome missing under nonignorable nonresponse": [[57, "Outcome-missing-under-nonignorable-nonresponse"], [86, "Outcome-missing-under-nonignorable-nonresponse"]], "Overcoming regularization bias by orthogonalization": [[52, "Overcoming-regularization-bias-by-orthogonalization"], [66, "Overcoming-regularization-bias-by-orthogonalization"], [88, "overcoming-regularization-bias-by-orthogonalization"]], "Panel Data": [[95, "id9"], [97, null], [109, "panel-data"], [109, "id3"], [125, "panel-data"]], "Panel Data (Repeated Outcomes)": [[58, "Panel-Data-(Repeated-Outcomes)"]], "Panel data": [[95, "panel-data"]], "Parameter tuning": [[56, "Parameter-tuning"]], "Partialling out score": [[52, "Partialling-out-score"], [66, "Partialling-out-score"], [88, "partialling-out-score"]], "Partially Linear Regression Model (PLR)": [[55, "Partially-Linear-Regression-Model-(PLR)"], [72, "Partially-Linear-Regression-Model-(PLR)"], [78, "Partially-Linear-Regression-Model-(PLR)"]], "Partially linear IV regression model (PLIV)": [[95, "partially-linear-iv-regression-model-pliv"], [109, "partially-linear-iv-regression-model-pliv"]], "Partially linear models (PLM)": [[95, "partially-linear-models-plm"], [109, "partially-linear-models-plm"], [125, "partially-linear-models-plm"]], "Partially linear regression model (PLR)": [[95, "partially-linear-regression-model-plr"], [109, "partially-linear-regression-model-plr"], [125, "partially-linear-regression-model-plr"]], "Plot Coefficients and 95% Confidence Intervals": [[76, "Plot-Coefficients-and-95%-Confidence-Intervals"]], "Policy Learning with Trees": [[81, "Policy-Learning-with-Trees"], [93, "policy-learning-with-trees"]], "Potential Quantile Estimation": [[82, "Potential-Quantile-Estimation"]], "Potential Quantiles (PQs)": [[82, "Potential-Quantiles-(PQs)"]], "Potential quantiles (PQs)": [[93, "potential-quantiles-pqs"], [109, "potential-quantiles-pqs"]], "Python: Average Potential Outcome (APO) Models": [[64, null]], "Python: Basic Instrumental Variables calculation": [[65, null]], "Python: Basics of Double Machine Learning": [[66, null]], "Python: Building the package from source": [[137, "python-building-the-package-from-source"]], "Python: Case studies": [[63, "python-case-studies"]], "Python: Choice of learners": [[75, null]], "Python: Cluster Robust Double Machine Learning": [[77, null]], "Python: Conditional Average Treatment Effects (CATEs) for IRM models": [[67, null]], "Python: Conditional Average Treatment Effects (CATEs) for PLR models": [[68, null]], "Python: Conditional Value at Risk of potential outcomes": [[69, null]], "Python: Difference-in-Differences": [[58, null]], "Python: Difference-in-Differences Pre-Testing": [[59, null]], "Python: First Stage and Causal Estimation": [[70, null]], "Python: GATE Sensitivity Analysis": [[73, null]], "Python: Group Average Treatment Effects (GATEs) for IRM models": [[71, null]], "Python: Group Average Treatment Effects (GATEs) for PLR models": [[72, null]], "Python: IRM and APO Model Comparison": [[74, null]], "Python: Impact of 401(k) on Financial Wealth": [[78, null]], "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)": [[79, null]], "Python: Installing DoubleML": [[137, "python-installing-doubleml"]], "Python: Installing a released version from a .whl file": [[137, "python-installing-a-released-version-from-a-whl-file"]], "Python: Installing the latest release from pip or conda": [[137, "python-installing-the-latest-release-from-pip-or-conda"]], "Python: Learners and hyperparameters": [[94, "python-learners-and-hyperparameters"]], "Python: Optional Dependencies": [[137, "python-optional-dependencies"]], "Python: PLM and IRM for Multiple Treatments": [[80, null]], "Python: Panel Data Introduction": [[61, null]], "Python: Panel Data with Multiple Time Periods": [[60, null]], "Python: Policy Learning with Trees": [[81, null]], "Python: Potential Quantiles and Quantile Treatment Effects": [[82, null]], "Python: Sample Selection Models": [[86, null]], "Python: Sensitivity Analysis": [[84, null]], "Quantile Treatment Effects (QTEs)": [[82, "Quantile-Treatment-Effects-(QTEs)"]], "Quantile treatment effects (QTEs)": [[93, "quantile-treatment-effects-qtes"]], "Quantiles": [[93, "quantiles"]], "R: Basic Instrumental Variables Calculation": [[51, null]], "R: Basics of Double Machine Learning": [[52, null]], "R: Case studies": [[63, "r-case-studies"]], "R: Cluster Robust Double Machine Learning": [[54, null]], "R: DoubleML for Difference-in-Differences": [[53, null]], "R: Ensemble Learners and More with mlr3pipelines": [[56, null]], "R: Impact of 401(k) on Financial Wealth": [[55, null]], "R: Installing DoubleML": [[137, "r-installing-doubleml"]], "R: Installing the development version from GitHub": [[137, "r-installing-the-development-version-from-github"]], "R: Installing the latest release from CRAN": [[137, "r-installing-the-latest-release-from-cran"]], "R: Learners and hyperparameters": [[94, "r-learners-and-hyperparameters"]], "R: Sample Selection Models": [[57, null]], "Ranking Treatment Effects under Treatment Propensity and Treatment Effect Heterogeneity": [[80, "Ranking-Treatment-Effects-under-Treatment-Propensity-and-Treatment-Effect-Heterogeneity"]], "Real-Data Application": [[54, "Real-Data-Application"], [77, "Real-Data-Application"]], "References": [[51, "References"], [53, "References"], [54, "References"], [56, "References"], [65, "References"], [70, "References"], [75, "References"], [76, "References"], [77, "References"], [80, "References"], [85, "References"], [88, "references"], [94, "references"], [108, "references"], [124, "references"], [136, "references"], [138, "references"]], "Regression Discontinuity Designs (RDD)": [[95, "regression-discontinuity-designs-rdd"]], "Regularization Bias in Simple ML-Approaches": [[52, "Regularization-Bias-in-Simple-ML-Approaches"], [66, "Regularization-Bias-in-Simple-ML-Approaches"]], "Regularization bias in simple ML-approaches": [[88, "regularization-bias-in-simple-ml-approaches"]], "Release Notes": [[140, null]], "Repeated Cross-Sectional Data": [[58, "Repeated-Cross-Sectional-Data"], [109, "repeated-cross-sectional-data"], [109, "id4"], [125, "repeated-cross-sectional-data"]], "Repeated cross-fitting with K folds and M repetitions": [[108, "repeated-cross-fitting-with-k-folds-and-m-repetitions"]], "Repeated cross-sections": [[95, "repeated-cross-sections"], [95, "id10"], [97, "repeated-cross-sections"]], "Sample Selection Models": [[109, "sample-selection-models"]], "Sample Selection Models (SSM)": [[95, "sample-selection-models-ssm"]], "Sample splitting to remove bias induced by overfitting": [[52, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [66, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [88, "sample-splitting-to-remove-bias-induced-by-overfitting"]], "Sample-splitting without cross-fitting": [[108, "sample-splitting-without-cross-fitting"]], "Sample-splitting, cross-fitting and repeated cross-fitting": [[108, null]], "Sandbox": [[63, "sandbox"]], "Score Mixin Classes for DoubleML Models": [[49, null]], "Score functions": [[109, null]], "Selected Combinations": [[60, "Selected-Combinations"]], "Sensitivity Analysis": [[60, "Sensitivity-Analysis"], [64, "Sensitivity-Analysis"], [74, "Sensitivity-Analysis"], [84, "Sensitivity-Analysis"], [84, "id1"]], "Sensitivity Analysis with IRM": [[84, "Sensitivity-Analysis-with-IRM"]], "Sensitivity analysis": [[125, null]], "Set up learners based on mlr3pipelines": [[56, "Set-up-learners-based-on-mlr3pipelines"]], "Sharp RDD": [[83, "Sharp-RDD"]], "Sharp RDD Without Adjustment": [[83, "Sharp-RDD-Without-Adjustment"]], "Sharp RDD with Flexible Adjustment": [[83, "Sharp-RDD-with-Flexible-Adjustment"]], "Sharp RDD with Linear Adjustment": [[83, "Sharp-RDD-with-Linear-Adjustment"]], "Sharp Regression Discontinuity Design": [[95, "sharp-regression-discontinuity-design"]], "Simulate two-way cluster data": [[54, "Simulate-two-way-cluster-data"], [77, "Simulate-two-way-cluster-data"]], "Simulation Example": [[84, "Simulation-Example"]], "Simultaneous inference over different DoubleML models (advanced)": [[124, "simultaneous-inference-over-different-doubleml-models-advanced"]], "Single Average Potential Outcome Models (APO)": [[64, "Single-Average-Potential-Outcome-Models-(APO)"]], "Source code and maintenance": [[136, "source-code-and-maintenance"]], "Special Data Types": [[91, "special-data-types"]], "Specify learner and estimate causal parameter: IRM model with Lasso as learner": [[62, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: IRM model with random forest as learner": [[62, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-random-forest-as-learner"]], "Specify learner and estimate causal parameter: PLR model with Lasso as learner": [[62, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: PLR model with random forest as learner": [[62, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-random-forest-as-learner"]], "Specifying alternative score functions via callables": [[109, "specifying-alternative-score-functions-via-callables"]], "Specifying learners and set hyperparameters": [[94, "specifying-learners-and-set-hyperparameters"], [94, "id9"]], "Standard approach": [[75, "Standard-approach"]], "Step 1: Custom API for FLAML Models within DoubleML": [[76, "Step-1:-Custom-API-for-FLAML-Models-within-DoubleML"]], "Step 1: Initialize and Train the AutoML Models:": [[76, "Step-1:-Initialize-and-Train-the-AutoML-Models:"]], "Step 2: Evaluate the Tuned Models": [[76, "Step-2:-Evaluate-the-Tuned-Models"]], "Step 2: Using the API when calling DoubleML\u2019s .fit() Method": [[76, "Step-2:-Using-the-API-when-calling-DoubleML's-.fit()-Method"]], "Step 3: Create and Fit DoubleML Model": [[76, "Step-3:-Create-and-Fit-DoubleML-Model"]], "Summary Figure": [[80, "Summary-Figure"]], "Summary of Results": [[55, "Summary-of-Results"], [78, "Summary-of-Results"]], "The Augmented Inverse Propensity Weighting Model estimates the ATE under arbitrary effect and propensity score heterogeneity": [[80, "The-Augmented-Inverse-Propensity-Weighting-Model-estimates-the-ATE-under-arbitrary-effect-and-propensity-score-heterogeneity"]], "The Data Backend: DoubleMLData": [[55, "The-Data-Backend:-DoubleMLData"], [78, "The-Data-Backend:-DoubleMLData"]], "The DoubleML package": [[55, "The-DoubleML-package"], [78, "The-DoubleML-package"]], "The Partially Linear Model performs overlap weighting": [[80, "The-Partially-Linear-Model-performs-overlap-weighting"]], "The basics of double/debiased machine learning": [[88, null]], "The causal model": [[138, "the-causal-model"]], "The data-backend DoubleMLData": [[138, "the-data-backend-doublemldata"]], "Theory": [[125, "theory"]], "Time Aggregation": [[60, "Time-Aggregation"], [61, "Time-Aggregation"]], "Tuning on the Folds": [[76, "Tuning-on-the-Folds"]], "Tuning on the full Sample": [[76, "Tuning-on-the-full-Sample"]], "Two treatment periods": [[95, "two-treatment-periods"], [109, "two-treatment-periods"], [125, "two-treatment-periods"]], "Two-Dimensional Example": [[67, "Two-Dimensional-Example"], [68, "Two-Dimensional-Example"]], "Two-Way Clustering with Respect to Product and Market": [[54, "Two-Way-Clustering-with-Respect-to-Product-and-Market"], [77, "Two-Way-Clustering-with-Respect-to-Product-and-Market"]], "Untuned (default parameter) XGBoost": [[76, "Untuned-(default-parameter)-XGBoost"]], "Use ensemble learners based on mlr3pipelines": [[56, "Use-ensemble-learners-based-on-mlr3pipelines"]], "User Guide": [[92, null]], "Using DoubleML": [[51, "Using-DoubleML"], [65, "Using-DoubleML"]], "Using ML for DiD: Integrating DoubleML in did": [[53, "Using-ML-for-DiD:-Integrating-DoubleML-in-did"]], "Using learners from mlr3, mlr3learners and mlr3extralearners": [[56, "Using-learners-from-mlr3,-mlr3learners-and-mlr3extralearners"]], "Using pipelines to construct learners": [[94, "using-pipelines-to-construct-learners"]], "Utility Classes": [[50, "utility-classes"]], "Utility Classes and Functions": [[50, null]], "Utility Functions": [[50, "utility-functions"]], "VanderWeele and Arah (2011): Benchmarking": [[85, "VanderWeele-and-Arah-(2011):-Benchmarking"]], "Variance estimation": [[124, "variance-estimation"]], "Variance estimation and confidence intervals": [[124, null]], "Weighted Average Treatment Effects": [[93, "weighted-average-treatment-effects"]], "doubleml.data.DoubleMLClusterData": [[4, null]], "doubleml.data.DoubleMLData": [[5, null]], "doubleml.data.DoubleMLPanelData": [[6, null]], "doubleml.datasets.fetch_401K": [[7, null]], "doubleml.datasets.fetch_bonus": [[8, null]], "doubleml.datasets.make_confounded_irm_data": [[9, null]], "doubleml.datasets.make_confounded_plr_data": [[10, null]], "doubleml.datasets.make_heterogeneous_data": [[11, null]], "doubleml.datasets.make_iivm_data": [[12, null]], "doubleml.datasets.make_irm_data": [[13, null]], "doubleml.datasets.make_irm_data_discrete_treatments": [[14, null]], "doubleml.datasets.make_pliv_CHS2015": [[15, null]], "doubleml.datasets.make_pliv_multiway_cluster_CKMS2021": [[16, null]], "doubleml.datasets.make_plr_CCDDHNR2018": [[17, null]], "doubleml.datasets.make_plr_turrell2018": [[18, null]], "doubleml.datasets.make_ssm_data": [[19, null]], "doubleml.did": [[3, "doubleml-did"]], "doubleml.did.DoubleMLDID": [[20, null]], "doubleml.did.DoubleMLDIDAggregation": [[21, null]], "doubleml.did.DoubleMLDIDBinary": [[22, null]], "doubleml.did.DoubleMLDIDCS": [[23, null]], "doubleml.did.DoubleMLDIDMulti": [[24, null]], "doubleml.did.datasets.make_did_CS2021": [[25, null]], "doubleml.did.datasets.make_did_SZ2020": [[26, null]], "doubleml.double_ml_score_mixins.LinearScoreMixin": [[27, null]], "doubleml.double_ml_score_mixins.NonLinearScoreMixin": [[28, null]], "doubleml.irm": [[3, "doubleml-irm"]], "doubleml.irm.DoubleMLAPO": [[29, null]], "doubleml.irm.DoubleMLAPOS": [[30, null]], "doubleml.irm.DoubleMLCVAR": [[31, null]], "doubleml.irm.DoubleMLIIVM": [[32, null]], "doubleml.irm.DoubleMLIRM": [[33, null]], "doubleml.irm.DoubleMLLPQ": [[34, null]], "doubleml.irm.DoubleMLPQ": [[35, null]], "doubleml.irm.DoubleMLQTE": [[36, null]], "doubleml.irm.DoubleMLSSM": [[37, null]], "doubleml.plm": [[3, "doubleml-plm"]], "doubleml.plm.DoubleMLPLIV": [[38, null]], "doubleml.plm.DoubleMLPLR": [[39, null]], "doubleml.rdd": [[3, "doubleml-rdd"]], "doubleml.rdd.RDFlex": [[40, null]], "doubleml.rdd.datasets.make_simple_rdd_data": [[41, null]], "doubleml.utils.DMLDummyClassifier": [[42, null]], "doubleml.utils.DMLDummyRegressor": [[43, null]], "doubleml.utils.DoubleMLBLP": [[44, null]], "doubleml.utils.DoubleMLPolicyTree": [[45, null]], "doubleml.utils.GlobalClassifier": [[46, null]], "doubleml.utils.GlobalRegressor": [[47, null]], "doubleml.utils.gain_statistics": [[48, null]]}, "docnames": ["api/api", "api/data_class", "api/datasets", "api/dml_models", "api/generated/doubleml.data.DoubleMLClusterData", "api/generated/doubleml.data.DoubleMLData", "api/generated/doubleml.data.DoubleMLPanelData", "api/generated/doubleml.datasets.fetch_401K", "api/generated/doubleml.datasets.fetch_bonus", "api/generated/doubleml.datasets.make_confounded_irm_data", "api/generated/doubleml.datasets.make_confounded_plr_data", "api/generated/doubleml.datasets.make_heterogeneous_data", "api/generated/doubleml.datasets.make_iivm_data", "api/generated/doubleml.datasets.make_irm_data", "api/generated/doubleml.datasets.make_irm_data_discrete_treatments", "api/generated/doubleml.datasets.make_pliv_CHS2015", "api/generated/doubleml.datasets.make_pliv_multiway_cluster_CKMS2021", "api/generated/doubleml.datasets.make_plr_CCDDHNR2018", "api/generated/doubleml.datasets.make_plr_turrell2018", "api/generated/doubleml.datasets.make_ssm_data", "api/generated/doubleml.did.DoubleMLDID", "api/generated/doubleml.did.DoubleMLDIDAggregation", "api/generated/doubleml.did.DoubleMLDIDBinary", "api/generated/doubleml.did.DoubleMLDIDCS", "api/generated/doubleml.did.DoubleMLDIDMulti", "api/generated/doubleml.did.datasets.make_did_CS2021", "api/generated/doubleml.did.datasets.make_did_SZ2020", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin", "api/generated/doubleml.irm.DoubleMLAPO", "api/generated/doubleml.irm.DoubleMLAPOS", "api/generated/doubleml.irm.DoubleMLCVAR", "api/generated/doubleml.irm.DoubleMLIIVM", "api/generated/doubleml.irm.DoubleMLIRM", "api/generated/doubleml.irm.DoubleMLLPQ", "api/generated/doubleml.irm.DoubleMLPQ", "api/generated/doubleml.irm.DoubleMLQTE", "api/generated/doubleml.irm.DoubleMLSSM", "api/generated/doubleml.plm.DoubleMLPLIV", "api/generated/doubleml.plm.DoubleMLPLR", "api/generated/doubleml.rdd.RDFlex", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data", "api/generated/doubleml.utils.DMLDummyClassifier", "api/generated/doubleml.utils.DMLDummyRegressor", "api/generated/doubleml.utils.DoubleMLBLP", "api/generated/doubleml.utils.DoubleMLPolicyTree", "api/generated/doubleml.utils.GlobalClassifier", "api/generated/doubleml.utils.GlobalRegressor", "api/generated/doubleml.utils.gain_statistics", "api/mixins", "api/utility", "examples/R_double_ml_basic_iv", "examples/R_double_ml_basics", "examples/R_double_ml_did", "examples/R_double_ml_multiway_cluster", "examples/R_double_ml_pension", "examples/R_double_ml_pipeline", "examples/R_double_ml_ssm", "examples/did/py_did", "examples/did/py_did_pretest", "examples/did/py_panel", "examples/did/py_panel_simple", "examples/double_ml_bonus_data", "examples/index", "examples/py_double_ml_apo", "examples/py_double_ml_basic_iv", "examples/py_double_ml_basics", "examples/py_double_ml_cate", "examples/py_double_ml_cate_plr", "examples/py_double_ml_cvar", "examples/py_double_ml_firststage", "examples/py_double_ml_gate", "examples/py_double_ml_gate_plr", "examples/py_double_ml_gate_sensitivity", "examples/py_double_ml_irm_vs_apo", "examples/py_double_ml_learner", "examples/py_double_ml_meets_flaml", "examples/py_double_ml_multiway_cluster", "examples/py_double_ml_pension", "examples/py_double_ml_pension_qte", "examples/py_double_ml_plm_irm_hetfx", "examples/py_double_ml_policy_tree", "examples/py_double_ml_pq", "examples/py_double_ml_rdflex", "examples/py_double_ml_sensitivity", "examples/py_double_ml_sensitivity_booking", "examples/py_double_ml_ssm", "guide/algorithms", "guide/basics", "guide/data/base_data", "guide/data/panel_data", "guide/data_backend", "guide/guide", "guide/heterogeneity", "guide/learners", "guide/models", "guide/models/did/did_aggregation", "guide/models/did/did_binary", "guide/models/did/did_cs", "guide/models/did/did_pa", "guide/models/did/did_setup", "guide/models/irm/apo", "guide/models/irm/apos", "guide/models/irm/iivm", "guide/models/irm/irm", "guide/models/plm/pliv", "guide/models/plm/plr", "guide/models/ssm/ssm", "guide/resampling", "guide/scores", "guide/scores/did/did_cs_binary_score", "guide/scores/did/did_cs_score", "guide/scores/did/did_pa_binary_score", "guide/scores/did/did_pa_score", "guide/scores/irm/apo_score", "guide/scores/irm/cvar_score", "guide/scores/irm/iivm_score", "guide/scores/irm/irm_score", "guide/scores/irm/lpq_score", "guide/scores/irm/pq_score", "guide/scores/plm/pliv_score", "guide/scores/plm/plr_score", "guide/scores/ssm/mar_score", "guide/scores/ssm/nr_score", "guide/se_confint", "guide/sensitivity", "guide/sensitivity/benchmarking", "guide/sensitivity/did/did_cs_binary_sensitivity", "guide/sensitivity/did/did_cs_sensitivity", "guide/sensitivity/did/did_pa_binary_sensitivity", "guide/sensitivity/did/did_pa_sensitivity", "guide/sensitivity/implementation", "guide/sensitivity/irm/apo_sensitivity", "guide/sensitivity/irm/irm_sensitivity", "guide/sensitivity/plm/plr_sensitivity", "guide/sensitivity/theory", "index", "intro/install", "intro/intro", "literature/literature", "release/release", "workflow/workflow"], "envversion": {"nbsphinx": 4, "sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1}, "filenames": ["api/api.rst", "api/data_class.rst", "api/datasets.rst", "api/dml_models.rst", "api/generated/doubleml.data.DoubleMLClusterData.rst", "api/generated/doubleml.data.DoubleMLData.rst", "api/generated/doubleml.data.DoubleMLPanelData.rst", "api/generated/doubleml.datasets.fetch_401K.rst", "api/generated/doubleml.datasets.fetch_bonus.rst", "api/generated/doubleml.datasets.make_confounded_irm_data.rst", "api/generated/doubleml.datasets.make_confounded_plr_data.rst", "api/generated/doubleml.datasets.make_heterogeneous_data.rst", "api/generated/doubleml.datasets.make_iivm_data.rst", "api/generated/doubleml.datasets.make_irm_data.rst", "api/generated/doubleml.datasets.make_irm_data_discrete_treatments.rst", "api/generated/doubleml.datasets.make_pliv_CHS2015.rst", "api/generated/doubleml.datasets.make_pliv_multiway_cluster_CKMS2021.rst", "api/generated/doubleml.datasets.make_plr_CCDDHNR2018.rst", "api/generated/doubleml.datasets.make_plr_turrell2018.rst", "api/generated/doubleml.datasets.make_ssm_data.rst", "api/generated/doubleml.did.DoubleMLDID.rst", "api/generated/doubleml.did.DoubleMLDIDAggregation.rst", "api/generated/doubleml.did.DoubleMLDIDBinary.rst", "api/generated/doubleml.did.DoubleMLDIDCS.rst", "api/generated/doubleml.did.DoubleMLDIDMulti.rst", "api/generated/doubleml.did.datasets.make_did_CS2021.rst", "api/generated/doubleml.did.datasets.make_did_SZ2020.rst", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin.rst", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin.rst", "api/generated/doubleml.irm.DoubleMLAPO.rst", "api/generated/doubleml.irm.DoubleMLAPOS.rst", "api/generated/doubleml.irm.DoubleMLCVAR.rst", "api/generated/doubleml.irm.DoubleMLIIVM.rst", "api/generated/doubleml.irm.DoubleMLIRM.rst", "api/generated/doubleml.irm.DoubleMLLPQ.rst", "api/generated/doubleml.irm.DoubleMLPQ.rst", "api/generated/doubleml.irm.DoubleMLQTE.rst", "api/generated/doubleml.irm.DoubleMLSSM.rst", "api/generated/doubleml.plm.DoubleMLPLIV.rst", "api/generated/doubleml.plm.DoubleMLPLR.rst", "api/generated/doubleml.rdd.RDFlex.rst", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data.rst", "api/generated/doubleml.utils.DMLDummyClassifier.rst", "api/generated/doubleml.utils.DMLDummyRegressor.rst", "api/generated/doubleml.utils.DoubleMLBLP.rst", "api/generated/doubleml.utils.DoubleMLPolicyTree.rst", "api/generated/doubleml.utils.GlobalClassifier.rst", "api/generated/doubleml.utils.GlobalRegressor.rst", "api/generated/doubleml.utils.gain_statistics.rst", "api/mixins.rst", "api/utility.rst", "examples/R_double_ml_basic_iv.ipynb", "examples/R_double_ml_basics.ipynb", "examples/R_double_ml_did.ipynb", "examples/R_double_ml_multiway_cluster.ipynb", "examples/R_double_ml_pension.ipynb", "examples/R_double_ml_pipeline.ipynb", "examples/R_double_ml_ssm.ipynb", "examples/did/py_did.ipynb", "examples/did/py_did_pretest.ipynb", "examples/did/py_panel.ipynb", "examples/did/py_panel_simple.ipynb", "examples/double_ml_bonus_data.ipynb", "examples/index.rst", "examples/py_double_ml_apo.ipynb", "examples/py_double_ml_basic_iv.ipynb", "examples/py_double_ml_basics.ipynb", "examples/py_double_ml_cate.ipynb", "examples/py_double_ml_cate_plr.ipynb", "examples/py_double_ml_cvar.ipynb", "examples/py_double_ml_firststage.ipynb", "examples/py_double_ml_gate.ipynb", "examples/py_double_ml_gate_plr.ipynb", "examples/py_double_ml_gate_sensitivity.ipynb", "examples/py_double_ml_irm_vs_apo.ipynb", "examples/py_double_ml_learner.ipynb", "examples/py_double_ml_meets_flaml.ipynb", "examples/py_double_ml_multiway_cluster.ipynb", "examples/py_double_ml_pension.ipynb", "examples/py_double_ml_pension_qte.ipynb", "examples/py_double_ml_plm_irm_hetfx.ipynb", "examples/py_double_ml_policy_tree.ipynb", "examples/py_double_ml_pq.ipynb", "examples/py_double_ml_rdflex.ipynb", "examples/py_double_ml_sensitivity.ipynb", "examples/py_double_ml_sensitivity_booking.ipynb", "examples/py_double_ml_ssm.ipynb", "guide/algorithms.rst", "guide/basics.rst", "guide/data/base_data.rst", "guide/data/panel_data.rst", "guide/data_backend.rst", "guide/guide.rst", "guide/heterogeneity.rst", "guide/learners.rst", "guide/models.rst", "guide/models/did/did_aggregation.rst", "guide/models/did/did_binary.rst", "guide/models/did/did_cs.rst", "guide/models/did/did_pa.rst", "guide/models/did/did_setup.rst", "guide/models/irm/apo.rst", "guide/models/irm/apos.rst", "guide/models/irm/iivm.rst", "guide/models/irm/irm.rst", "guide/models/plm/pliv.rst", "guide/models/plm/plr.rst", "guide/models/ssm/ssm.rst", "guide/resampling.rst", "guide/scores.rst", "guide/scores/did/did_cs_binary_score.rst", "guide/scores/did/did_cs_score.rst", "guide/scores/did/did_pa_binary_score.rst", "guide/scores/did/did_pa_score.rst", "guide/scores/irm/apo_score.rst", "guide/scores/irm/cvar_score.rst", "guide/scores/irm/iivm_score.rst", "guide/scores/irm/irm_score.rst", "guide/scores/irm/lpq_score.rst", "guide/scores/irm/pq_score.rst", "guide/scores/plm/pliv_score.rst", "guide/scores/plm/plr_score.rst", "guide/scores/ssm/mar_score.rst", "guide/scores/ssm/nr_score.rst", "guide/se_confint.rst", "guide/sensitivity.rst", "guide/sensitivity/benchmarking.rst", "guide/sensitivity/did/did_cs_binary_sensitivity.rst", "guide/sensitivity/did/did_cs_sensitivity.rst", "guide/sensitivity/did/did_pa_binary_sensitivity.rst", "guide/sensitivity/did/did_pa_sensitivity.rst", "guide/sensitivity/implementation.rst", "guide/sensitivity/irm/apo_sensitivity.rst", "guide/sensitivity/irm/irm_sensitivity.rst", "guide/sensitivity/plm/plr_sensitivity.rst", "guide/sensitivity/theory.rst", "index.rst", "intro/install.rst", "intro/intro.rst", "literature/literature.rst", "release/release.rst", "workflow/workflow.rst"], "indexentries": {"aggregate() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.aggregate", false]], "aggregate_over_splits() (doubleml.rdd.rdflex method)": [[40, "doubleml.rdd.RDFlex.aggregate_over_splits", false]], "bootstrap() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.bootstrap", false]], "bootstrap() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.bootstrap", false]], "capo() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.capo", false]], "cate() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.cate", false]], "cate() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.cate", false]], "causal_contrast() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.causal_contrast", false]], "confint() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.confint", false]], "confint() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.confint", false]], "confint() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.confint", false]], "confint() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.confint", false]], "confint() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.confint", false]], "confint() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.confint", false]], "confint() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.confint", false]], "confint() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.confint", false]], "confint() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.confint", false]], "confint() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.confint", false]], "confint() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.confint", false]], "confint() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.confint", false]], "confint() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.confint", false]], "confint() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.confint", false]], "confint() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.confint", false]], "confint() (doubleml.rdd.rdflex method)": [[40, "doubleml.rdd.RDFlex.confint", false]], "confint() (doubleml.utils.doublemlblp method)": [[44, "doubleml.utils.DoubleMLBLP.confint", false]], "construct_framework() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.construct_framework", false]], "construct_framework() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.construct_framework", false]], "dmldummyclassifier (class in doubleml.utils)": [[42, "doubleml.utils.DMLDummyClassifier", false]], "dmldummyregressor (class in doubleml.utils)": [[43, "doubleml.utils.DMLDummyRegressor", false]], "doublemlapo (class in doubleml.irm)": [[29, "doubleml.irm.DoubleMLAPO", false]], "doublemlapos (class in doubleml.irm)": [[30, "doubleml.irm.DoubleMLAPOS", false]], "doublemlblp (class in doubleml.utils)": [[44, "doubleml.utils.DoubleMLBLP", false]], "doublemlclusterdata (class in doubleml.data)": [[4, "doubleml.data.DoubleMLClusterData", false]], "doublemlcvar (class in doubleml.irm)": [[31, "doubleml.irm.DoubleMLCVAR", false]], "doublemldata (class in doubleml.data)": [[5, "doubleml.data.DoubleMLData", false]], "doublemldid (class in doubleml.did)": [[20, "doubleml.did.DoubleMLDID", false]], "doublemldidaggregation (class in doubleml.did)": [[21, "doubleml.did.DoubleMLDIDAggregation", false]], "doublemldidbinary (class in doubleml.did)": [[22, "doubleml.did.DoubleMLDIDBinary", false]], "doublemldidcs (class in doubleml.did)": [[23, "doubleml.did.DoubleMLDIDCS", false]], "doublemldidmulti (class in doubleml.did)": [[24, "doubleml.did.DoubleMLDIDMulti", false]], "doublemliivm (class in doubleml.irm)": [[32, "doubleml.irm.DoubleMLIIVM", false]], "doublemlirm (class in doubleml.irm)": [[33, "doubleml.irm.DoubleMLIRM", false]], "doublemllpq (class in doubleml.irm)": [[34, "doubleml.irm.DoubleMLLPQ", false]], "doublemlpaneldata (class in doubleml.data)": [[6, "doubleml.data.DoubleMLPanelData", false]], "doublemlpliv (class in doubleml.plm)": [[38, "doubleml.plm.DoubleMLPLIV", false]], "doublemlplr (class in doubleml.plm)": [[39, "doubleml.plm.DoubleMLPLR", false]], "doublemlpolicytree (class in doubleml.utils)": [[45, "doubleml.utils.DoubleMLPolicyTree", false]], "doublemlpq (class in doubleml.irm)": [[35, "doubleml.irm.DoubleMLPQ", false]], "doublemlqte (class in doubleml.irm)": [[36, "doubleml.irm.DoubleMLQTE", false]], "doublemlssm (class in doubleml.irm)": [[37, "doubleml.irm.DoubleMLSSM", false]], "draw_sample_splitting() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.draw_sample_splitting", false]], "evaluate_learners() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.evaluate_learners", false]], "fetch_401k() (in module doubleml.datasets)": [[7, "doubleml.datasets.fetch_401K", false]], "fetch_bonus() (in module doubleml.datasets)": [[8, "doubleml.datasets.fetch_bonus", false]], "fit() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.fit", false]], "fit() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.fit", false]], "fit() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.fit", false]], "fit() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.fit", false]], "fit() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.fit", false]], "fit() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.fit", false]], "fit() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.fit", false]], "fit() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.fit", false]], "fit() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.fit", false]], "fit() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.fit", false]], "fit() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.fit", false]], "fit() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.fit", false]], "fit() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.fit", false]], "fit() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.fit", false]], "fit() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.fit", false]], "fit() (doubleml.rdd.rdflex method)": [[40, "doubleml.rdd.RDFlex.fit", false]], "fit() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.fit", false]], "fit() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.fit", false]], "fit() (doubleml.utils.doublemlblp method)": [[44, "doubleml.utils.DoubleMLBLP.fit", false]], "fit() (doubleml.utils.doublemlpolicytree method)": [[45, "doubleml.utils.DoubleMLPolicyTree.fit", false]], "fit() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.fit", false]], "fit() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.fit", false]], "from_arrays() (doubleml.data.doublemlclusterdata class method)": [[4, "doubleml.data.DoubleMLClusterData.from_arrays", false]], "from_arrays() (doubleml.data.doublemldata class method)": [[5, "doubleml.data.DoubleMLData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlpaneldata class method)": [[6, "doubleml.data.DoubleMLPanelData.from_arrays", false]], "gain_statistics() (in module doubleml.utils)": [[48, "doubleml.utils.gain_statistics", false]], "gapo() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.gapo", false]], "gate() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.gate", false]], "gate() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.gate", false]], "get_metadata_routing() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.get_metadata_routing", false]], "get_params() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.get_params", false]], "get_params() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.get_params", false]], "get_params() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.get_params", false]], "get_params() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.get_params", false]], "get_params() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.get_params", false]], "get_params() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.get_params", false]], "get_params() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.get_params", false]], "get_params() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.get_params", false]], "get_params() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.get_params", false]], "get_params() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.get_params", false]], "get_params() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.get_params", false]], "get_params() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.get_params", false]], "get_params() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.get_params", false]], "get_params() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.get_params", false]], "globalclassifier (class in doubleml.utils)": [[46, "doubleml.utils.GlobalClassifier", false]], "globalregressor (class in doubleml.utils)": [[47, "doubleml.utils.GlobalRegressor", false]], "linearscoremixin (class in doubleml.double_ml_score_mixins)": [[27, "doubleml.double_ml_score_mixins.LinearScoreMixin", false]], "make_confounded_irm_data() (in module doubleml.datasets)": [[9, "doubleml.datasets.make_confounded_irm_data", false]], "make_confounded_plr_data() (in module doubleml.datasets)": [[10, "doubleml.datasets.make_confounded_plr_data", false]], "make_did_cs2021() (in module doubleml.did.datasets)": [[25, "doubleml.did.datasets.make_did_CS2021", false]], "make_did_sz2020() (in module doubleml.did.datasets)": [[26, "doubleml.did.datasets.make_did_SZ2020", false]], "make_heterogeneous_data() (in module doubleml.datasets)": [[11, "doubleml.datasets.make_heterogeneous_data", false]], "make_iivm_data() (in module doubleml.datasets)": [[12, "doubleml.datasets.make_iivm_data", false]], "make_irm_data() (in module doubleml.datasets)": [[13, "doubleml.datasets.make_irm_data", false]], "make_irm_data_discrete_treatments() (in module doubleml.datasets)": [[14, "doubleml.datasets.make_irm_data_discrete_treatments", false]], "make_pliv_chs2015() (in module doubleml.datasets)": [[15, "doubleml.datasets.make_pliv_CHS2015", false]], "make_pliv_multiway_cluster_ckms2021() (in module doubleml.datasets)": [[16, "doubleml.datasets.make_pliv_multiway_cluster_CKMS2021", false]], "make_plr_ccddhnr2018() (in module doubleml.datasets)": [[17, "doubleml.datasets.make_plr_CCDDHNR2018", false]], "make_plr_turrell2018() (in module doubleml.datasets)": [[18, "doubleml.datasets.make_plr_turrell2018", false]], "make_simple_rdd_data() (in module doubleml.rdd.datasets)": [[41, "doubleml.rdd.datasets.make_simple_rdd_data", false]], "make_ssm_data() (in module doubleml.datasets)": [[19, "doubleml.datasets.make_ssm_data", false]], "nonlinearscoremixin (class in doubleml.double_ml_score_mixins)": [[28, "doubleml.double_ml_score_mixins.NonLinearScoreMixin", false]], "p_adjust() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.p_adjust", false]], "p_adjust() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.p_adjust", false]], "plot_effects() (doubleml.did.doublemldidaggregation method)": [[21, "doubleml.did.DoubleMLDIDAggregation.plot_effects", false]], "plot_effects() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.plot_effects", false]], "plot_tree() (doubleml.utils.doublemlpolicytree method)": [[45, "doubleml.utils.DoubleMLPolicyTree.plot_tree", false]], "policy_tree() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.policy_tree", false]], "predict() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.predict", false]], "predict() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.predict", false]], "predict() (doubleml.utils.doublemlpolicytree method)": [[45, "doubleml.utils.DoubleMLPolicyTree.predict", false]], "predict() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.predict", false]], "predict() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.predict", false]], "predict_proba() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.predict_proba", false]], "predict_proba() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.predict_proba", false]], "rdflex (class in doubleml.rdd)": [[40, "doubleml.rdd.RDFlex", false]], "score() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.score", false]], "score() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.score", false]], "score() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.score", false]], "score() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.score", false]], "sensitivity_analysis() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_analysis", false]], "sensitivity_benchmark() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_benchmark", false]], "sensitivity_plot() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_plot", false]], "set_fit_request() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.set_fit_request", false]], "set_fit_request() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.set_fit_request", false]], "set_ml_nuisance_params() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.set_ml_nuisance_params", false]], "set_params() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.set_params", false]], "set_params() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.set_params", false]], "set_params() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.set_params", false]], "set_params() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.set_params", false]], "set_sample_splitting() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.set_sample_splitting", false]], "set_score_request() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.set_score_request", false]], "set_score_request() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.set_score_request", false]], "set_x_d() (doubleml.data.doublemlclusterdata method)": [[4, "doubleml.data.DoubleMLClusterData.set_x_d", false]], "set_x_d() (doubleml.data.doublemldata method)": [[5, "doubleml.data.DoubleMLData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlpaneldata method)": [[6, "doubleml.data.DoubleMLPanelData.set_x_d", false]], "tune() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.tune", false]], "tune() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.tune", false]], "tune() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.tune", false]], "tune() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.tune", false]], "tune() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.tune", false]], "tune() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.tune", false]], "tune() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.tune", false]], "tune() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.tune", false]], "tune() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.tune", false]], "tune() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.tune", false]], "tune() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.tune", false]], "tune() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.tune", false]]}, "objects": {"doubleml.data": [[4, 0, 1, "", "DoubleMLClusterData"], [5, 0, 1, "", "DoubleMLData"], [6, 0, 1, "", "DoubleMLPanelData"]], "doubleml.data.DoubleMLClusterData": [[4, 1, 1, "", "from_arrays"], [4, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLData": [[5, 1, 1, "", "from_arrays"], [5, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLPanelData": [[6, 1, 1, "", "from_arrays"], [6, 1, 1, "", "set_x_d"]], "doubleml.datasets": [[7, 2, 1, "", "fetch_401K"], [8, 2, 1, "", "fetch_bonus"], [9, 2, 1, "", "make_confounded_irm_data"], [10, 2, 1, "", "make_confounded_plr_data"], [11, 2, 1, "", "make_heterogeneous_data"], [12, 2, 1, "", "make_iivm_data"], [13, 2, 1, "", "make_irm_data"], [14, 2, 1, "", "make_irm_data_discrete_treatments"], [15, 2, 1, "", "make_pliv_CHS2015"], [16, 2, 1, "", "make_pliv_multiway_cluster_CKMS2021"], [17, 2, 1, "", "make_plr_CCDDHNR2018"], [18, 2, 1, "", "make_plr_turrell2018"], [19, 2, 1, "", "make_ssm_data"]], "doubleml.did": [[20, 0, 1, "", "DoubleMLDID"], [21, 0, 1, "", "DoubleMLDIDAggregation"], [22, 0, 1, "", "DoubleMLDIDBinary"], [23, 0, 1, "", "DoubleMLDIDCS"], [24, 0, 1, "", "DoubleMLDIDMulti"]], "doubleml.did.DoubleMLDID": [[20, 1, 1, "", "bootstrap"], [20, 1, 1, "", "confint"], [20, 1, 1, "", "construct_framework"], [20, 1, 1, "", "draw_sample_splitting"], [20, 1, 1, "", "evaluate_learners"], [20, 1, 1, "", "fit"], [20, 1, 1, "", "get_params"], [20, 1, 1, "", "p_adjust"], [20, 1, 1, "", "sensitivity_analysis"], [20, 1, 1, "", "sensitivity_benchmark"], [20, 1, 1, "", "sensitivity_plot"], [20, 1, 1, "", "set_ml_nuisance_params"], [20, 1, 1, "", "set_sample_splitting"], [20, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDAggregation": [[21, 1, 1, "", "plot_effects"]], "doubleml.did.DoubleMLDIDBinary": [[22, 1, 1, "", "bootstrap"], [22, 1, 1, "", "confint"], [22, 1, 1, "", "construct_framework"], [22, 1, 1, "", "draw_sample_splitting"], [22, 1, 1, "", "evaluate_learners"], [22, 1, 1, "", "fit"], [22, 1, 1, "", "get_params"], [22, 1, 1, "", "p_adjust"], [22, 1, 1, "", "sensitivity_analysis"], [22, 1, 1, "", "sensitivity_benchmark"], [22, 1, 1, "", "sensitivity_plot"], [22, 1, 1, "", "set_ml_nuisance_params"], [22, 1, 1, "", "set_sample_splitting"], [22, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDCS": [[23, 1, 1, "", "bootstrap"], [23, 1, 1, "", "confint"], [23, 1, 1, "", "construct_framework"], [23, 1, 1, "", "draw_sample_splitting"], [23, 1, 1, "", "evaluate_learners"], [23, 1, 1, "", "fit"], [23, 1, 1, "", "get_params"], [23, 1, 1, "", "p_adjust"], [23, 1, 1, "", "sensitivity_analysis"], [23, 1, 1, "", "sensitivity_benchmark"], [23, 1, 1, "", "sensitivity_plot"], [23, 1, 1, "", "set_ml_nuisance_params"], [23, 1, 1, "", "set_sample_splitting"], [23, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDMulti": [[24, 1, 1, "", "aggregate"], [24, 1, 1, "", "bootstrap"], [24, 1, 1, "", "confint"], [24, 1, 1, "", "fit"], [24, 1, 1, "", "p_adjust"], [24, 1, 1, "", "plot_effects"], [24, 1, 1, "", "sensitivity_analysis"], [24, 1, 1, "", "sensitivity_benchmark"], [24, 1, 1, "", "sensitivity_plot"]], "doubleml.did.datasets": [[25, 2, 1, "", "make_did_CS2021"], [26, 2, 1, "", "make_did_SZ2020"]], "doubleml.double_ml_score_mixins": [[27, 0, 1, "", "LinearScoreMixin"], [28, 0, 1, "", "NonLinearScoreMixin"]], "doubleml.irm": [[29, 0, 1, "", "DoubleMLAPO"], [30, 0, 1, "", "DoubleMLAPOS"], [31, 0, 1, "", "DoubleMLCVAR"], [32, 0, 1, "", "DoubleMLIIVM"], [33, 0, 1, "", "DoubleMLIRM"], [34, 0, 1, "", "DoubleMLLPQ"], [35, 0, 1, "", "DoubleMLPQ"], [36, 0, 1, "", "DoubleMLQTE"], [37, 0, 1, "", "DoubleMLSSM"]], "doubleml.irm.DoubleMLAPO": [[29, 1, 1, "", "bootstrap"], [29, 1, 1, "", "capo"], [29, 1, 1, "", "confint"], [29, 1, 1, "", "construct_framework"], [29, 1, 1, "", "draw_sample_splitting"], [29, 1, 1, "", "evaluate_learners"], [29, 1, 1, "", "fit"], [29, 1, 1, "", "gapo"], [29, 1, 1, "", "get_params"], [29, 1, 1, "", "p_adjust"], [29, 1, 1, "", "sensitivity_analysis"], [29, 1, 1, "", "sensitivity_benchmark"], [29, 1, 1, "", "sensitivity_plot"], [29, 1, 1, "", "set_ml_nuisance_params"], [29, 1, 1, "", "set_sample_splitting"], [29, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLAPOS": [[30, 1, 1, "", "bootstrap"], [30, 1, 1, "", "causal_contrast"], [30, 1, 1, "", "confint"], [30, 1, 1, "", "draw_sample_splitting"], [30, 1, 1, "", "fit"], [30, 1, 1, "", "sensitivity_analysis"], [30, 1, 1, "", "sensitivity_benchmark"], [30, 1, 1, "", "sensitivity_plot"], [30, 1, 1, "", "set_sample_splitting"]], "doubleml.irm.DoubleMLCVAR": [[31, 1, 1, "", "bootstrap"], [31, 1, 1, "", "confint"], [31, 1, 1, "", "construct_framework"], [31, 1, 1, "", "draw_sample_splitting"], [31, 1, 1, "", "evaluate_learners"], [31, 1, 1, "", "fit"], [31, 1, 1, "", "get_params"], [31, 1, 1, "", "p_adjust"], [31, 1, 1, "", "sensitivity_analysis"], [31, 1, 1, "", "sensitivity_benchmark"], [31, 1, 1, "", "sensitivity_plot"], [31, 1, 1, "", "set_ml_nuisance_params"], [31, 1, 1, "", "set_sample_splitting"], [31, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLIIVM": [[32, 1, 1, "", "bootstrap"], [32, 1, 1, "", "confint"], [32, 1, 1, "", "construct_framework"], [32, 1, 1, "", "draw_sample_splitting"], [32, 1, 1, "", "evaluate_learners"], [32, 1, 1, "", "fit"], [32, 1, 1, "", "get_params"], [32, 1, 1, "", "p_adjust"], [32, 1, 1, "", "sensitivity_analysis"], [32, 1, 1, "", "sensitivity_benchmark"], [32, 1, 1, "", "sensitivity_plot"], [32, 1, 1, "", "set_ml_nuisance_params"], [32, 1, 1, "", "set_sample_splitting"], [32, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLIRM": [[33, 1, 1, "", "bootstrap"], [33, 1, 1, "", "cate"], [33, 1, 1, "", "confint"], [33, 1, 1, "", "construct_framework"], [33, 1, 1, "", "draw_sample_splitting"], [33, 1, 1, "", "evaluate_learners"], [33, 1, 1, "", "fit"], [33, 1, 1, "", "gate"], [33, 1, 1, "", "get_params"], [33, 1, 1, "", "p_adjust"], [33, 1, 1, "", "policy_tree"], [33, 1, 1, "", "sensitivity_analysis"], [33, 1, 1, "", "sensitivity_benchmark"], [33, 1, 1, "", "sensitivity_plot"], [33, 1, 1, "", "set_ml_nuisance_params"], [33, 1, 1, "", "set_sample_splitting"], [33, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLLPQ": [[34, 1, 1, "", "bootstrap"], [34, 1, 1, "", "confint"], [34, 1, 1, "", "construct_framework"], [34, 1, 1, "", "draw_sample_splitting"], [34, 1, 1, "", "evaluate_learners"], [34, 1, 1, "", "fit"], [34, 1, 1, "", "get_params"], [34, 1, 1, "", "p_adjust"], [34, 1, 1, "", "sensitivity_analysis"], [34, 1, 1, "", "sensitivity_benchmark"], [34, 1, 1, "", "sensitivity_plot"], [34, 1, 1, "", "set_ml_nuisance_params"], [34, 1, 1, "", "set_sample_splitting"], [34, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLPQ": [[35, 1, 1, "", "bootstrap"], [35, 1, 1, "", "confint"], [35, 1, 1, "", "construct_framework"], [35, 1, 1, "", "draw_sample_splitting"], [35, 1, 1, "", "evaluate_learners"], [35, 1, 1, "", "fit"], [35, 1, 1, "", "get_params"], [35, 1, 1, "", "p_adjust"], [35, 1, 1, "", "sensitivity_analysis"], [35, 1, 1, "", "sensitivity_benchmark"], [35, 1, 1, "", "sensitivity_plot"], [35, 1, 1, "", "set_ml_nuisance_params"], [35, 1, 1, "", "set_sample_splitting"], [35, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLQTE": [[36, 1, 1, "", "bootstrap"], [36, 1, 1, "", "confint"], [36, 1, 1, "", "draw_sample_splitting"], [36, 1, 1, "", "fit"], [36, 1, 1, "", "p_adjust"], [36, 1, 1, "", "set_sample_splitting"]], "doubleml.irm.DoubleMLSSM": [[37, 1, 1, "", "bootstrap"], [37, 1, 1, "", "confint"], [37, 1, 1, "", "construct_framework"], [37, 1, 1, "", "draw_sample_splitting"], [37, 1, 1, "", "evaluate_learners"], [37, 1, 1, "", "fit"], [37, 1, 1, "", "get_params"], [37, 1, 1, "", "p_adjust"], [37, 1, 1, "", "sensitivity_analysis"], [37, 1, 1, "", "sensitivity_benchmark"], [37, 1, 1, "", "sensitivity_plot"], [37, 1, 1, "", "set_ml_nuisance_params"], [37, 1, 1, "", "set_sample_splitting"], [37, 1, 1, "", "tune"]], "doubleml.plm": [[38, 0, 1, "", "DoubleMLPLIV"], [39, 0, 1, "", "DoubleMLPLR"]], "doubleml.plm.DoubleMLPLIV": [[38, 1, 1, "", "bootstrap"], [38, 1, 1, "", "confint"], [38, 1, 1, "", "construct_framework"], [38, 1, 1, "", "draw_sample_splitting"], [38, 1, 1, "", "evaluate_learners"], [38, 1, 1, "", "fit"], [38, 1, 1, "", "get_params"], [38, 1, 1, "", "p_adjust"], [38, 1, 1, "", "sensitivity_analysis"], [38, 1, 1, "", "sensitivity_benchmark"], [38, 1, 1, "", "sensitivity_plot"], [38, 1, 1, "", "set_ml_nuisance_params"], [38, 1, 1, "", "set_sample_splitting"], [38, 1, 1, "", "tune"]], "doubleml.plm.DoubleMLPLR": [[39, 1, 1, "", "bootstrap"], [39, 1, 1, "", "cate"], [39, 1, 1, "", "confint"], [39, 1, 1, "", "construct_framework"], [39, 1, 1, "", "draw_sample_splitting"], [39, 1, 1, "", "evaluate_learners"], [39, 1, 1, "", "fit"], [39, 1, 1, "", "gate"], [39, 1, 1, "", "get_params"], [39, 1, 1, "", "p_adjust"], [39, 1, 1, "", "sensitivity_analysis"], [39, 1, 1, "", "sensitivity_benchmark"], [39, 1, 1, "", "sensitivity_plot"], [39, 1, 1, "", "set_ml_nuisance_params"], [39, 1, 1, "", "set_sample_splitting"], [39, 1, 1, "", "tune"]], "doubleml.rdd": [[40, 0, 1, "", "RDFlex"]], "doubleml.rdd.RDFlex": [[40, 1, 1, "", "aggregate_over_splits"], [40, 1, 1, "", "confint"], [40, 1, 1, "", "fit"]], "doubleml.rdd.datasets": [[41, 2, 1, "", "make_simple_rdd_data"]], "doubleml.utils": [[42, 0, 1, "", "DMLDummyClassifier"], [43, 0, 1, "", "DMLDummyRegressor"], [44, 0, 1, "", "DoubleMLBLP"], [45, 0, 1, "", "DoubleMLPolicyTree"], [46, 0, 1, "", "GlobalClassifier"], [47, 0, 1, "", "GlobalRegressor"], [48, 2, 1, "", "gain_statistics"]], "doubleml.utils.DMLDummyClassifier": [[42, 1, 1, "", "fit"], [42, 1, 1, "", "get_metadata_routing"], [42, 1, 1, "", "get_params"], [42, 1, 1, "", "predict"], [42, 1, 1, "", "predict_proba"], [42, 1, 1, "", "score"], [42, 1, 1, "", "set_params"], [42, 1, 1, "", "set_score_request"]], "doubleml.utils.DMLDummyRegressor": [[43, 1, 1, "", "fit"], [43, 1, 1, "", "get_metadata_routing"], [43, 1, 1, "", "get_params"], [43, 1, 1, "", "predict"], [43, 1, 1, "", "score"], [43, 1, 1, "", "set_params"], [43, 1, 1, "", "set_score_request"]], "doubleml.utils.DoubleMLBLP": [[44, 1, 1, "", "confint"], [44, 1, 1, "", "fit"]], "doubleml.utils.DoubleMLPolicyTree": [[45, 1, 1, "", "fit"], [45, 1, 1, "", "plot_tree"], [45, 1, 1, "", "predict"]], "doubleml.utils.GlobalClassifier": [[46, 1, 1, "", "fit"], [46, 1, 1, "", "get_metadata_routing"], [46, 1, 1, "", "get_params"], [46, 1, 1, "", "predict"], [46, 1, 1, "", "predict_proba"], [46, 1, 1, "", "score"], [46, 1, 1, "", "set_fit_request"], [46, 1, 1, "", "set_params"], [46, 1, 1, "", "set_score_request"]], "doubleml.utils.GlobalRegressor": [[47, 1, 1, "", "fit"], [47, 1, 1, "", "get_metadata_routing"], [47, 1, 1, "", "get_params"], [47, 1, 1, "", "predict"], [47, 1, 1, "", "score"], [47, 1, 1, "", "set_fit_request"], [47, 1, 1, "", "set_params"], [47, 1, 1, "", "set_score_request"]]}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function"}, "terms": {"": [4, 5, 6, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 46, 47, 51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 64, 70, 71, 72, 73, 75, 77, 78, 79, 83, 84, 85, 86, 87, 89, 90, 91, 94, 95, 97, 99, 100, 107, 109, 122, 123, 124, 125, 126, 136, 138, 139, 140, 141], "0": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 122, 123, 124, 125, 126, 127, 129, 130, 131, 133, 137, 138, 140], "00": [60, 72, 74, 78, 79, 108], "000": [124, 141], "00000": 74, "000000": [60, 61, 62, 64, 74, 78, 79, 89, 91, 93, 138], "0000000": 124, "0000000000000010000100": [56, 89, 91, 138], "000000e": [60, 72, 74, 78, 79], "00000591": 82, "000006": [64, 82], "000017": 82, "000025": 77, "000034": 78, "000039": 77, "000064": 65, "000067": 77, "000076": 95, "000091": 77, "0001": [62, 78], "000219": [35, 93], "000242": [36, 93], "000341": 77, "000387": 60, "000442": 77, "00047580260495": 51, "000488": 77, "000494": 73, "0005": 62, "000522": 77, "000532": 60, "000555": 60, "000590": 60, "000597": 60, "0005a80b528f": 56, "000604": 60, "000607": 60, "000609": 60, "000670": 77, "000743": 84, "000824": 61, "0008890575": 108, "000915799": 124, "0009157990": 124, "000943": [67, 68], "001": [25, 51, 53, 54, 55, 56, 57, 66, 94, 95, 108, 109, 124, 138, 141], "001049": [95, 99], "001051": 77, "00121": 108, "001234": 79, "00133": 56, "00138944": [87, 109], "001403": 83, "001471": 74, "001494": [93, 94, 95], "0016": [55, 78], "001603": [95, 99], "001698": 74, "001714": 93, "0018": [55, 78], "0019": 62, "001907": 74, "002169338": 124, "0021693380": 124, "0021693381": 124, "002277": 67, "002290": 59, "0023": 53, "002388": 76, "002436": 73, "002443": 60, "0026": 62, "002601": [95, 96], "002779": 84, "0028": [53, 55, 78], "002821": 85, "0028213335041910427": 85, "002983": 77, "003": [9, 10, 26], "003045": 74, "003074": 95, "003134": 82, "003187": 67, "003220": 64, "003328": 82, "0034": 70, "003404": 64, "003415": 64, "003427": 77, "003607": 68, "003779": 73, "003836": 82, "003924": 73, "003944": 67, "003975": 67, "004": 80, "00409412": [87, 109], "0042": [55, 78], "004253": 64, "004392": 73, "004526": 64, "004542": 74, "004688": 32, "0047": [55, 78], "004846": 85, "005": 80, "00518448": [95, 99], "005339": [67, 68], "005857": 77, "005e": 95, "006": 80, "006055": 64, "0061": 108, "006267": 68, "006425": 79, "0068101213851626": 76, "006922": 62, "006958": [67, 68], "0069873011": 108, "007210e": 79, "00728": 138, "0073": 62, "007332": 69, "007332393760465": 69, "007421": 93, "00778625": 108, "0078540263583833": 76, "008": 85, "008023": 79, "008223": [67, 68], "008229852": 108, "008266e": 79, "008274": 80, "008487": 62, "0084871742256079": 76, "008642": 93, "008883698": 109, "00888458890362062": 87, "008884589": 87, "008939": 60, "008dbd": 80, "008e80": 80, "009": [80, 85], "009122": 82, "009152": 60, "00916672": 108, "009255": 67, "009329847": 109, "009428": 69, "00944171905420782": 85, "00950122695463054": 87, "009501226954630540": 87, "009501227": 87, "009645422": 54, "009656": 82, "009708": 80, "00972": 62, "009727": [95, 96], "009790": 79, "009951": 60, "009986": 82, "01": [20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 45, 51, 54, 55, 56, 57, 60, 67, 68, 74, 78, 79, 80, 81, 82, 83, 94, 95, 99, 108, 109, 124, 138, 141], "010213": 84, "010269": 77, "010381": 60, "010450": 54, "010848": 80, "010940": 77, "011044": 60, "011131": 82, "0112": 53, "011204": 74, "011274": 60, "01128": 62, "011323": [95, 96], "011598": 82, "011653": 60, "0118095": 54, "011823": 84, "011988e": 82, "011991": 80, "012051": 61, "01219": 56, "012378": 60, "01274": 85, "012780": 79, "012831": 85, "01288423": 108, "012924": 61, "013034": 85, "013128": 67, "013195": 80, "013313": 76, "013450": 74, "01351638": 54, "013593": 84, "013617": 79, "013677": 83, "013712": 67, "013908": 60, "01398951": 54, "013990": 124, "014": 83, "01403089": 54, "014080": [67, 68], "014432": 59, "014541": 60, "014637": 77, "014681": 84, "0148343": 108, "014873e": 67, "015": [56, 80], "015038": 69, "015552": 67, "015565": 82, "0156853566737638": 76, "015698": 82, "01574297": 82, "015743": 82, "015831": 67, "016": 80, "016011": 68, "016154": 77, "016200": [67, 68], "016315": 71, "01643": 139, "016866": 60, "017": 56, "017057": 60, "017140": 67, "017393e": 124, "017660": 74, "01772": 126, "017777e": 68, "017800092": 124, "0178000920": 124, "017818": 60, "017968": 60, "018": [51, 56], "018023": 81, "018092": 93, "018148": 82, "018508": 67, "01903": [56, 94, 136, 138], "019051": 60, "019071": 60, "01916030e": 108, "01925597": 54, "019335": 61, "019439633": 124, "0194396330": 124, "0194396331": 124, "019569": 80, "019596": 69, "019660": [36, 93], "019776": 60, "01990373": 86, "019974": 79, "02": [60, 67, 68, 78, 79, 82, 93, 95, 99, 108], "02013742": 60, "02016117": 138, "020166": 82, "020271": 77, "020272": 74, "020360838": 124, "0203608380": 124, "0203608381": 124, "02052929": [87, 109], "020597": 60, "02079162e": 108, "020819": 93, "02092": 138, "021269": [71, 72], "02163217": 54, "021690": 72, "021823": 76, "021866": 81, "021926": 69, "0219322": 108, "022": 80, "022181": 67, "022258": 74, "022295e": 67, "022325": 60, "022331": 60, "02247976": 54, "02260304": [95, 99], "022768": 62, "022783": 84, "022915": 77, "022969": 79, "023": 80, "023020e": [78, 79], "023052": 68, "023206": 60, "023256": 82, "023537": 76, "023563": 124, "023955": 79, "024266": 74, "024346": 67, "024355": 59, "024364": 125, "024401": [71, 72], "024566": 60, "024604": 77, "024665": 60, "024782": 82, "024926": 59, "025": [67, 68, 71, 72, 74, 80], "025077": [68, 124], "02528067": 75, "0253": 56, "025300e": 68, "025443": 62, "025496": 67, "0257": 53, "025813114": 124, "0258131140": 124, "02584": 56, "025841": 74, "025964": 74, "026640": 60, "026669": 79, "026723": 69, "026822": 76, "026966": 74, "027024": 60, "02726833": 108, "027418": 60, "0275813": 108, "02791": 62, "02807553": 108, "0281": 56, "028520": [67, 68], "028630": [95, 96], "028731": 93, "02897287": 58, "02900983": 82, "029010": 82, "029022": 68, "029209": 141, "029364": [125, 131], "029831": 82, "029910e": [78, 79], "029986": 60, "02e": 55, "03": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 57, 60, 64, 67, 68, 69, 73, 74, 78, 79, 82, 83, 84, 85, 95, 96, 99, 108, 125, 131, 141], "030059": 93, "0301": 56, "03018": 34, "030346": 138, "03045": 57, "030629": 60, "030649": 61, "0307": 56, "030934": 82, "030962": 82, "031007": 76, "03113": 86, "031134": 94, "031156": 68, "031269": 62, "031619": 60, "031639": 82, "031820": 68, "03191": 139, "03220": 140, "0323": 53, "03244552": 94, "0325": 138, "03258": 76, "032580": 76, "032738": 76, "032941": 67, "032953": 84, "033": 80, "033265": 76, "033756": 69, "033812": 80, "033946": [71, 72], "034": 80, "034065": 68, "03411": 138, "034161": 61, "034226": 79, "03438": 57, "034650": 60, "034690": 69, "034812763": 124, "0348127630": 124, "0348127631": 124, "034846": 78, "03489": [16, 54, 77], "035057": 61, "035119185": 124, "0351191850": 124, "0351191851": 124, "035264": 68, "03536": 138, "03538": 56, "03539": 56, "035391": 62, "0354": 56, "035411": 138, "035441": 68, "03545": 56, "035545": 62, "035572": 62, "035689": [95, 99], "035730": 82, "03574": 62, "035762": 82, "035785": 68, "035874": 60, "0359": 56, "035904": 60, "036129015": 124, "0361290150": 124, "0361290151": 124, "036143": 82, "036147": 82, "036240": 64, "036729": 77, "0368": 53, "036945": 79, "03698487": 82, "036985": 82, "037008": [71, 72], "037053e": 60, "037114": 74, "0374": 56, "037504": 68, "037509": 86, "037529": 74, "037577": [95, 96], "037747": [67, 68], "037817": 60, "038103": 74, "038694": 61, "038845": 67, "038851": 61, "039036": 67, "039141": 64, "03917696": [109, 124], "03920960e": 108, "039310e": 69, "039661": 74, "039895": 74, "039991e": 67, "04": [10, 40, 55, 60, 64, 67, 68, 78, 79, 82, 83, 84, 95, 96, 99, 108, 141], "040010": 74, "040079": 68, "040112": 124, "040139": [67, 68], "040384": 61, "040533": [109, 124], "04053339": 124, "040562": 67, "040615": 60, "040629": 39, "040688": 67, "040784": 64, "0408": 67, "040912": 68, "040919": 68, "041057": 61, "041147": 69, "041284": 69, "041387": 69, "041459": 79, "041491e": 69, "04165": 95, "0418": 53, "041808": 80, "041831": 69, "041925": 67, "042034": 85, "042249": 68, "042265": 69, "0425": 94, "0428": 86, "042804": 74, "042822e": 79, "042844e": 82, "043082": [95, 99], "043108": 79, "0433": 53, "0434e374": 56, "043827": 60, "04387": 94, "043998": 68, "044113": 69, "04415": 56, "044176": 74, "044239": 74, "04424": 56, "044443": 60, "04444978": 124, "044449780": 124, "0445": 94, "044536": 60, "04465": 54, "044704": 68, "044786": 60, "04486": 138, "04487585": [125, 131], "044903": 60, "04491": 95, "044929": 74, "04497975": [125, 131], "045": 80, "04501612": 124, "04502": [94, 109, 124], "045144": 77, "045172": 74, "045313": 67, "045379": 138, "04552": 77, "045553": 69, "045624": 59, "04563": 94, "045638": 67, "045738": 61, "045754": 82, "04586": 94, "045932": 82, "045984": 74, "045993": 94, "046": 80, "04625": 94, "046451": 74, "046507": [95, 99], "046525": 93, "046527": 69, "04653976": 82, "046540": 82, "046551": 61, "046587": 68, "0466028": 54, "046728": 84, "04682310e": 108, "046922": 94, "047007": 61, "047194": 32, "04722457": 108, "047239": 74, "047288": 93, "047652e": 68, "047724": 67, "047873": 74, "047954": 77, "048": 80, "048220": 76, "048308": 72, "048476": 74, "048699": 86, "048723": 94, "048853": 68, "04898685": 108, "049264": 64, "049573": [95, 99], "04973": 68, "05": [40, 51, 53, 54, 55, 56, 57, 60, 67, 68, 69, 70, 75, 77, 78, 79, 80, 82, 83, 85, 94, 95, 96, 99, 108, 109, 124, 138, 141], "050152": 60, "05039": 84, "050494e": 68, "050538": 68, "051": 56, "051353": 61, "051396": 60, "051651": 83, "051867e": 69, "052": 83, "052000e": 79, "052023": 74, "052178": 61, "052298": 82, "052380": 67, "052488": 72, "052502": 82, "052745": 69, "053": [56, 95], "053049": 68, "05327406": 108, "0533": 53, "053331": 69, "053342": 79, "053353": 61, "053389": 124, "053436": 33, "053541": 82, "053558": 69, "053849e": 67, "053930": 61, "054": [56, 83], "054068": 77, "054162": 77, "05428249": 108, "054348": 124, "054370": 69, "054529": 124, "054771e": 82, "055165": 84, "055171": 68, "055439": [76, 79], "055493": 85, "055680": 124, "055908": 61, "056": 83, "056357": 61, "056499": 72, "056619": 60, "056745": 67, "056764": 67, "056915": 74, "057022": 60, "057095": 82, "057230": 60, "0576": [55, 78], "057762": 82, "057792": 67, "057962": 69, "057997": 80, "058042": 124, "058276": 79, "058375": 64, "058463": 82, "058508": 86, "058595": 67, "05891": 95, "0590": 53, "059128": 67, "059384": 82, "059627": 79, "059630": 59, "059685": 82, "06": [9, 10, 26, 60, 64, 67, 68, 69, 78, 79, 82, 93, 94, 108], "060016": 64, "06008533": 95, "060201": 82, "060212": [78, 79], "060417": 67, "060581": 75, "060764": 80, "060845": 124, "060933": 67, "0611": 53, "06111111": 56, "0615": 53, "06161": 95, "061750": 60, "062": [83, 95], "062031": 61, "062322": 61, "062371": 60, "062414": 79, "062507": 82, "0628": 53, "062964": 124, "062988": 67, "063017": 64, "063139": 61, "0632": 53, "063234e": 68, "063327": 74, "0635": 53, "063593": 68, "0636": 53, "063700": 67, "0638": 53, "063881": 95, "063994": 61, "064": 80, "0640": 53, "064161": 79, "064213": 68, "06428": 78, "064280": 78, "064289": 60, "0645": 53, "064575": 61, "0646222": 55, "0647": 53, "064841": 61, "0649": 53, "065": 85, "0653": 53, "065356": [71, 72], "065368": 76, "0654": 53, "065451": 79, "065494": 61, "0655": 53, "065610": 61, "06568598": 108, "065725": 69, "0659": 53, "065969": 95, "065976": 74, "066": 51, "0662": 53, "066222": 61, "066228": 61, "066295": 74, "066425": 61, "066464": 84, "066889": 82, "0669": 53, "06692492": 108, "067046e": 67, "0671": 53, "067212": 74, "067240": 82, "06724028": 82, "0673": 53, "067303": 61, "0674563": 108, "0675": 53, "067528": 85, "067721": 124, "068073": 68, "068160": 61, "06827": 84, "06834315": 58, "068371": 61, "068377": 79, "068514": 67, "068700": 93, "068934": 64, "06895837": 54, "069443": 64, "0695854": 54, "069589": 74, "069600": 79, "069882e": 67, "07": [67, 68, 79, 82, 83, 85, 108], "070020": 82, "070196": 69, "0701961897676835": 69, "0702127": 54, "0704": 53, "070433": 74, "070497": 85, "070534": 37, "070552": 67, "070574e": 79, "0707": 53, "070751": 67, "07085301": 95, "070884": 82, "0711": 53, "071285": 124, "07136": [54, 77], "071362": 67, "071488e": 69, "0716": 53, "07168291": 54, "071777": 94, "071782": [36, 93], "0719": 53, "071972": 60, "07202564": [71, 72], "07222222": 56, "072293": 81, "07229774": [95, 99], "072516": 74, "072605": 64, "0727": 95, "073": 83, "073013": 82, "073207": 77, "073275": 67, "073384": 74, "073447": [95, 99], "07347676": 54, "07350015": [16, 19, 54, 77], "073520": 69, "0736": 53, "07366": [56, 94], "073694": 68, "0737": 108, "073888": 60, "0739130271918385": 76, "073929": 76, "0743": 53, "074304": 124, "07436521": 108, "074426": 82, "074545": 80, "07456127": 54, "074617": 68, "07479278": 84, "074927": 64, "075261": 59, "075384": 82, "07538443": 82, "07544271e": 108, "07561": 138, "07564554e": 108, "0758": 85, "075809": 64, "075869": 94, "075942": 76, "076019": 78, "076156": 124, "076179312": 124, "0761793120": 124, "07626874": 108, "076322": 82, "076347": 69, "0765": 56, "076596": 67, "076653": [95, 99], "076684": 138, "076748": 60, "07685043": 108, "07689": 56, "07691847": 108, "076953": [71, 72], "076971": 62, "076977": 60, "077144e": 68, "077161": 79, "07727773e": 108, "077319": 82, "077502": [125, 131], "077555": 67, "077592": 74, "077702": 64, "0777777777777778": 94, "07777778": [56, 94], "077840": 79, "077841": 60, "077883": 82, "07788588": [95, 99], "077923e": 67, "0779365": 108, "07796": 95, "078017": 67, "078096": 124, "078141": 61, "078207": 62, "07828372": 124, "078328": 60, "078426": 93, "078474": 124, "078709": 68, "078810": 82, "078927": 60, "079085": 62, "07915": 56, "07919896": 108, "07942v3": 139, "079458e": 78, "079489": 60, "079500e": 67, "07961": 84, "07978296": 108, "08": [69, 79, 82, 85, 95], "08005229": 108, "08031571": 108, "08057": 108, "080726": 60, "080854": 79, "08091581": 108, "080947": 62, "081": [56, 80], "081100": 82, "081230": [67, 68], "081396": 72, "081402e": 60, "081488": 77, "08154161": 108, "08181827e": 108, "0820": 53, "082197": 74, "082263": 38, "082297": 108, "082400e": 67, "082558": 60, "082574": 33, "082804": 59, "082858": 68, "082905": [95, 99], "082934": 79, "082973": 77, "083258": 124, "083318": 124, "08333333": 56, "08333617": 108, "083527": 80, "0835771416": 54, "0836": 74, "08364": 74, "083706": 85, "083750": 79, "08387876": 108, "083949": 85, "084": 54, "084156": 68, "084184": 69, "0841842065698133": 69, "084212": 73, "084269": 79, "084323": 67, "084337": 95, "08434194": 108, "08451984": 108, "084633": 71, "0853505": 54, "08537661": 108, "085395": 67, "085566": 69, "085592": 74, "085671": 67, "08576647": 108, "085965": 79, "086004": 74, "08602774e": 108, "086068": 61, "086135": 60, "0862": 136, "086264": 69, "08664208": 108, "086679": 94, "08670501": 108, "086889": 71, "08710447": 108, "0872": 53, "087222": 68, "08746114": 108, "087561": 67, "08759785": 108, "087634": 67, "087745": 68, "0878192": 108, "087947": 82, "088": 80, "088048": 82, "088119": 60, "088282": 72, "088357": 82, "08848": 94, "088482": [36, 93], "088504e": 38, "088832e": 60, "08888889": 56, "08907015": 108, "0894": 53, "08968939": 54, "08985707": 108, "089964": 76, "089996": 60, "08e": 55, "09": [67, 68, 69, 78, 79, 82, 93], "09000000000000001": 94, "090025": 79, "09015": 53, "09019833": 108, "090255": 82, "090436": 68, "090942": 61, "091179e": 67, "09120417": 108, "091263": 76, "091391": 124, "091406": 125, "091535": 67, "0916": 53, "091810": 60, "091824": 68, "091992": 81, "092": 80, "092076": 61, "092229": 85, "092247": 82, "092263": 95, "092365": 124, "09240374": 108, "092442": 60, "092919": 126, "092935": 67, "093043": 82, "09310496": 124, "093153": 82, "093401": 60, "09341": 108, "093474": 82, "09347419": 82, "09351167": 95, "093746": 124, "093788": 60, "093950": 77, "094026": 77, "094118": 82, "094378e": 67, "094381": 77, "094416": 60, "09444444": 56, "094581e": 68, "094829": 95, "094999": 82, "095104": 64, "095654": 67, "095781": 31, "095785": 64, "09603": 136, "096337": 77, "096418": 64, "096550": 71, "096616": 93, "09666487": 108, "096688": 68, "096741": 58, "09682314": 95, "096915": 85, "097": 83, "097009": 74, "097157": 85, "097468": 69, "09779675": 124, "097796750": 124, "098": 55, "098256": 82, "09830758": 84, "098308": 84, "098317": 79, "098319": 82, "098512": 60, "0986": 53, "098712": 82, "09879814e": 108, "098901": 74, "099": 83, "099001": 68, "099307": 68, "099485": [95, 96], "099647": 81, "099670": 79, "099695": 61, "099731": [67, 68], "09980311": 124, "09988": 139, "0_": 15, "0ff823b17d45": 56, "0x1747bdd4520": 62, "0x1747bdd6b90": 62, "0x2920d7b7150": 81, "0x7f47599a3da0": 95, "0x7f475a612540": 95, "0x7f475a613050": 95, "0x7f475a63f3b0": 96, "0x7f475a63f620": 94, "0x7f475a662630": 94, "0x7f475a663050": 94, "0x7f475a66d8e0": 124, "0x7f475a66f440": 124, "0x7f475ab70950": 96, "0x7f475ac57bf0": 141, "0x7f475ae01b80": 95, "0x7f475ae02660": 95, "0x7f475ae1be00": 95, "0x7f475afc4620": 131, "0x7f475b2f93d0": 124, "0x7f475b2f9400": 124, "0x7f475b5b2bd0": 125, "0x7f76c21f8950": 85, "1": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 100, 101, 103, 104, 107, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140], "10": [7, 8, 9, 10, 12, 14, 16, 17, 19, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 131, 136, 138, 139, 141], "100": [16, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 56, 57, 58, 67, 68, 70, 73, 74, 75, 77, 80, 85, 86, 87, 89, 91, 93, 94, 95, 97, 108, 109, 124, 125, 131, 138, 140], "1000": [25, 32, 34, 52, 58, 59, 65, 66, 71, 72, 73, 75, 76, 78, 79, 83, 84, 85, 88, 95], "10000": [51, 59, 67, 68, 78, 79, 82], "100000e": 79, "100044": 72, "100154": 76, "100208": 93, "100356": 69, "10038": 84, "100385": 76, "10039862": [86, 95], "100517": 124, "100715": 67, "10079785": 95, "100807": [67, 68], "100858": 84, "100881": 60, "10089588": 82, "100896": 82, "10092": 79, "100923": 82, "100_000": 80, "101": [9, 10, 26, 53, 83, 93, 139, 140], "10126": 79, "10127930": 124, "101279300": 124, "1014927": 108, "1015": [55, 78], "101535": 60, "1016": [9, 10, 25, 26, 53], "1016010": 55, "1016736": 108, "1018": [79, 85], "101998": 64, "102": [89, 91, 93, 138, 140], "10235": 79, "10258": 79, "102616": 69, "102775": 69, "10299": 78, "103": [67, 77, 83, 86, 93, 140], "10307": 124, "1031": 79, "103189": 79, "10348": 78, "103497": 82, "1038": 79, "103806": 69, "103951906910721": 69, "103952": 69, "10396": 78, "104": [55, 78, 80, 86, 93, 140], "10406": 79, "104087": 67, "1041": 53, "10414": 79, "104492": 74, "1045303": 54, "104787": 77, "104849": 67, "104956": 60, "105": [15, 51, 54, 74, 77, 93, 140], "105318": 82, "1054": 56, "105461": 93, "1055": 53, "106": [56, 93, 140], "10607": [62, 89, 91, 138], "10618": 79, "10637173e": 108, "106391": 124, "1065": [70, 75, 76], "106595": [95, 97], "106676": 61, "106746": 82, "106952": 73, "107": [56, 85, 93, 140], "107073": 69, "107156": 74, "107295": 124, "1073": 79, "107413": 67, "10747": [62, 89, 91, 138], "10799": 79, "108": [93, 136, 139, 140], "1080": [16, 19, 53, 54, 77], "10824": [62, 89, 91, 138], "108257e": 79, "108259": 64, "10831": [62, 89, 91, 138], "108387": 60, "108507": 60, "108624e": 60, "10878571": 82, "108786": 82, "108870": [95, 99], "108980": 61, "109": [67, 93], "109005": 82, "10903": 78, "109069": 124, "109079e": 82, "1092604": 108, "109273": 77, "109277": 74, "10928": 79, "1093": 70, "109454": 79, "109470": 68, "109473": 60, "1096": 53, "10967": 78, "109679": 60, "109693": 60, "109861": 138, "1099472942084532": 65, "10e": [69, 82], "11": [39, 51, 54, 55, 56, 57, 58, 60, 61, 62, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 82, 83, 84, 85, 87, 89, 91, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 131, 138, 141], "110": [93, 140], "110079": 60, "110081": 74, "1101": 79, "11019365749799062": 85, "110194": 85, "110253": 60, "110359": 77, "110365": 85, "110681": 84, "1107": 79, "11071087": [86, 95], "110717": 124, "1109": 79, "110902": 69, "110902411746278": 69, "111": [68, 93, 95, 140], "1111": [7, 8, 17, 52, 54, 66, 70, 77, 85, 88, 95, 125, 131, 136], "111164": 81, "11120": 79, "111517": 61, "1117": [70, 75, 76], "1118": 55, "111949": 60, "11199615e": 108, "112": [56, 93, 140], "1120": 78, "112078": 93, "11208236": [87, 109], "1121950": 108, "1122": 79, "112216": 69, "1129": 79, "113": [7, 93, 140], "113022": 74, "11311": 78, "113149": 74, "113151925333744455860677074768891969798": 108, "113151925333744455860677074768891969798312172226313435424648505564757781909295567102341434763666869787984858687899924914182832383940495152535462829394100": 108, "1131519253337444558606770747688919697983121722263134354246485055647577819092955671023414347636668697879848586878999811162021242729303656575961657172738083": 108, "11315192533374445586067707476889196979831217222631343542464850556475778190929581116202124272930365657596165717273808324914182832383940495152535462829394100": 108, "113151925333744455860677074768891969798567102341434763666869787984858687899981116202124272930365657596165717273808324914182832383940495152535462829394100": 108, "113207": 82, "113270": 69, "113415": 79, "113644": 60, "11364904": 108, "11375": 79, "113780": 77, "113897": 60, "113952": 74, "11399": 78, "114": [93, 140], "114447": 60, "1144500": 54, "11447": 84, "114530": 71, "1145370": 54, "114570": 68, "11458": 79, "114647": 69, "1147": 53, "1148": 79, "114834": 79, "11488": 79, "11495": 79, "114989": 64, "115": [80, 93, 140], "11500": [78, 141], "115060e": 82, "1151610541568202": 76, "115296e": 79, "115297e": 78, "1154245": 108, "1155142425200442": 76, "11552911": 84, "11559": 79, "115636": 68, "11570": 78, "115792e": 79, "115901": 64, "115972": 67, "116": [93, 140], "116027": 69, "11617": 79, "116274": 69, "116569": 79, "1166": [78, 139], "1167": 78, "11673": 79, "11675": 79, "116965": 60, "117": [67, 93], "1170": 84, "11700": 141, "117072": 71, "117112": 68, "117242": 82, "11724226": 82, "117366": 82, "1174": 108, "11743": 141, "11750": 79, "1176": 53, "117646": 60, "1177": 53, "117710": 69, "11792": 55, "11796": 79, "118": 93, "11802": 79, "1182": 55, "11823404": 85, "118255": 82, "1186": 55, "118601": 77, "11861": 55, "1187339840850312": 77, "11879": 79, "118799": 79, "118938e": 95, "118952": 77, "119": [85, 93, 140], "11932": 79, "119343": 60, "11935": 84, "11942111": [95, 99], "11944377": 108, "119523": 60, "119766": 82, "1198": [54, 77], "12": [21, 24, 25, 37, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 64, 66, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 80, 82, 83, 84, 85, 87, 89, 91, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 131, 136, 138, 139, 140, 141], "120": [57, 58, 76, 86, 93, 140], "12002": 78, "1200x600": 60, "1200x800": 60, "1202": 139, "120468": 82, "12046836": 82, "120567": [71, 72], "120721": 77, "1208": 108, "12097": [7, 8, 17, 54, 70, 77, 88, 136], "121": [79, 93, 140], "1210": 79, "12101": 79, "12105472": 124, "121054720": 124, "1211": 79, "121206": 60, "1213405": 54, "121355": 60, "121399": 79, "1214": 124, "121584e": 82, "121711": 79, "121774": 73, "121824": 68, "12196389e": 108, "122": [9, 10, 26, 53, 60, 83, 89, 91, 93, 139, 140], "12214": 55, "12223182e": 108, "122408": 69, "122421": 74, "122777": 124, "122850": 61, "123": [40, 55, 56, 60, 78, 85, 93, 140, 141], "1230": 79, "123192": 85, "12323": 79, "1234": [51, 52, 53, 62, 65, 66, 83, 88, 94, 108, 124], "12348": 85, "1238": 79, "123917": 79, "123950": [95, 96, 99], "124": 93, "12410": 79, "124306": 76, "124480": 76, "124805": 78, "124825": 68, "125": [93, 140], "12500": 78, "125065": 124, "12539340": 124, "1255": 79, "12579": 79, "1258": 54, "126": [93, 140], "12606": 79, "12612": 79, "126290": 60, "126777": 124, "126802": 79, "12689": 79, "127": [9, 93, 140], "127006": 79, "12705095": [109, 124], "12707800": 54, "1272404618426184": 76, "127337": 74, "12752825": 124, "127563": 84, "1277": 80, "127778": 79, "127812": 61, "127889": 93, "128": [55, 93, 140], "12802": 55, "12814": 79, "128229": 74, "128270e": 60, "128300e": 68, "128312": 82, "128408": 77, "1285": 53, "12861": 79, "12864141": 108, "128651": 68, "129": [77, 93, 140], "12945": 139, "1295": [53, 79], "129514": 79, "12955": 78, "129606": 67, "129798": 67, "1298": 79, "12980769e": 108, "12983057": 95, "13": [10, 12, 14, 25, 26, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 66, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 80, 82, 83, 84, 85, 87, 89, 91, 93, 94, 95, 97, 108, 109, 124, 125, 131, 138, 141], "130": [56, 71, 77, 93, 140], "130092": 60, "130122": 84, "130125": 61, "13034980e": 108, "130370": 69, "1306": 84, "130829": 82, "13084": 95, "13091": 79, "1309844442144665": 76, "131": [93, 140], "13102231": 95, "131024": 76, "13119": 84, "1312": 141, "131211": 79, "1313": [55, 141], "13137893e": 108, "131483": 74, "131602": 60, "1318": 53, "131842": 74, "132": [56, 67, 77, 80, 93, 140], "13208": 141, "1321": [78, 141], "132245": 60, "132248": 93, "1324": [55, 78], "132454": 59, "132481": 93, "1325": 55, "13257": 78, "132671": 69, "132903": 79, "132982": 67, "133": [56, 89, 91, 93, 139, 140], "13300": 79, "133180": 61, "133202": 79, "133421": 79, "13356": 79, "133596": 82, "133839": 74, "13398": 85, "133f5a": 80, "134": [77, 86, 93, 140], "134037": 61, "1340371": 53, "1341": 55, "134146": 79, "1342": 79, "134211": 82, "1343": 78, "1344495": 108, "134542": 67, "134567": 79, "1346035": 55, "134687": 79, "13474": 79, "134765": 79, "134784": 95, "134784e": 67, "1348": 78, "134828": 61, "1349": 84, "13490": 79, "135": [56, 93, 95, 140], "13505272": 54, "135142": 68, "135344": 64, "135352": 20, "135379": 124, "135509": 60, "135665": 68, "135707": 94, "135856": 82, "13585644": 82, "135871": 77, "136": [62, 77, 85, 93, 140], "1360": 55, "13602": 85, "136089": 77, "1361": 79, "136102": 67, "1362430723104844": 76, "13642": 79, "136442": 77, "1366": 80, "136836": 77, "137": [9, 56, 62, 93, 140], "1371": 79, "137165": 95, "137213": 68, "137396": 82, "137400": 60, "1378": 79, "137829": 60, "137999": 95, "138": [93, 140], "1380": 78, "138068": 71, "13809": 79, "138264": 85, "138357": 60, "138378": 69, "1384": 78, "1386": 53, "13868238": 124, "138682380": 124, "138698": 124, "1387": 53, "138731": 60, "138774": 60, "138851": 71, "13893": 79, "139": [85, 93, 138], "139117e": 67, "1393368": 108, "139491": 124, "139508": 74, "13956": 84, "1398": 79, "139830": [95, 99], "1399": 53, "14": [52, 54, 55, 56, 57, 58, 60, 61, 62, 64, 66, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 82, 83, 84, 85, 87, 89, 91, 93, 94, 95, 97, 108, 109, 124, 125, 131, 138, 139, 141], "140": [57, 58, 74, 79, 86, 93, 140], "1400": 79, "14000073": 108, "140073": 67, "140081": 76, "1401": 53, "14018": 95, "140770": [67, 68], "140833": 69, "140861": 54, "140926": 82, "140991": 60, "141": [79, 93, 140], "141002": 68, "141098e": 79, "14114": 84, "1412881": 108, "14141": 79, "141460": 64, "141481": 61, "141546": 124, "141820": 69, "142": [93, 140], "14200098": 124, "142119": 67, "142270": 59, "142382": 67, "1424": 94, "1425632": 108, "14268": 95, "14281403493938022": 94, "14289": 79, "143": [89, 91, 93, 140], "143342": 68, "143495": 93, "1435": 79, "143534": 67, "14368145": 124, "144": [93, 140], "14400": 78, "14405": 79, "14406": 79, "144084": 69, "1441": 53, "144137": 58, "144241": 71, "1443": 79, "1443014": 108, "144500e": 79, "144669": 82, "1447": 79, "144800": 69, "144908": 81, "144971": 78, "145": [93, 140], "145027": 64, "145245": 82, "14532650": 124, "145465": 61, "1456060": 108, "145625": 82, "145748": 124, "14587": 79, "146": [93, 140], "146037": 82, "146087": 138, "146142808990006": 69, "146143": 69, "146191": 60, "14625": 79, "146435": 68, "1465": 55, "146641": 124, "14667": 79, "1468115": 54, "146973": 69, "1469734445741286": 69, "147": [93, 140], "1470": 60, "147015e": 79, "14702": 62, "147121": 82, "14744": 79, "147511": 60, "147529": 60, "14772": 79, "1479": 79, "14790924": 124, "147909240": 124, "147927": 62, "14798": 79, "148": [93, 140], "148005": 64, "14803": 79, "148134": [67, 68], "148161": 82, "14845": 62, "1485": 79, "148750e": [78, 79], "148790": 79, "148802": 79, "149": [93, 140], "1492": 51, "149215e": 68, "149228": 85, "149285": 82, "14937299": 108, "149472": 85, "149714": 77, "14984": 79, "149858": [35, 93], "149898": 82, "149901": 61, "149963": 60, "15": [8, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 54, 55, 56, 58, 60, 64, 66, 67, 68, 69, 71, 73, 74, 75, 76, 77, 78, 79, 82, 83, 84, 85, 89, 91, 93, 94, 95, 97, 108, 109, 124, 125, 131, 138, 141], "150": [15, 56, 85, 93, 140], "15000": [55, 78], "150000": 55, "15000000000000002": [69, 79, 82, 94], "150000e": 79, "1502": 54, "150200": 77, "150334": 79, "150408": 54, "150467": 60, "150614": 62, "150719e": 78, "151": [80, 93, 140], "151047e": 71, "151063": 67, "15113": 79, "151636": 69, "151819": 82, "15194": 78, "152": [93, 140], "152034": 79, "152148": [67, 68], "152353": 68, "152706": 95, "15285": 79, "152896": 76, "152926": 59, "153": [85, 93, 140], "1530959776797396": 69, "153096": 69, "153119": 69, "153314": 68, "15347": 79, "15354": 84, "153587": 77, "153633": 62, "153639": 108, "153862": 60, "153935": 68, "154": 93, "1540106": 108, "15430": 141, "154421": 124, "1545": 79, "154557": 82, "154758": 124, "154828": 69, "154890": 76, "155": [93, 140], "155000": 78, "155025": 82, "155120": 82, "155160": 64, "155174": 64, "155423": 64, "155516": 81, "15556": 79, "1557093": 54, "1557577": 108, "155789": 60, "156": [93, 140], "1560": 79, "156021": 82, "156169": 68, "156202": [67, 68], "156317": [67, 68], "1564": 124, "156545": 124, "156684": 68, "1569": 79, "156969": 69, "157": [68, 93, 140], "157077": 60, "157091": 124, "157154": 67, "1576": 79, "157733": 76, "1577657": 54, "157e": 95, "158": [93, 140], "158007": 82, "15815035": 55, "158178": 69, "1582": 79, "158311": 60, "1586": 79, "158697": 124, "158726": 93, "158862": 60, "1589": 79, "15891559": 82, "158916": 82, "159": [51, 140], "15915": 61, "15916": [53, 61], "159386": 84, "159526": 60, "1596": 56, "159633e": 68, "159841": 67, "159959": 79, "16": [31, 51, 52, 54, 55, 56, 57, 60, 61, 64, 67, 68, 69, 72, 73, 74, 77, 78, 79, 82, 83, 84, 85, 89, 91, 93, 94, 95, 97, 108, 109, 124, 125, 131, 138, 141], "160": [57, 58, 86, 140], "1604": 55, "160586": 60, "160836": 74, "160932": 69, "161": [56, 139, 140], "161049": 68, "161141": 77, "161198": 81, "161236": 82, "161243": 82, "161269": 67, "161288": [68, 76], "161543": 79, "1619": 55, "162": 140, "16201": 79, "16211": 78, "162153": 82, "1622": 79, "16241": 79, "162436": 85, "162593": 76, "1626685": 54, "162683": 85, "162710": 69, "1628": 78, "162930": 79, "163": [79, 140], "163194": 82, "163566": 79, "163895": 69, "164": [60, 64, 83, 140], "164034": 124, "164467": 78, "164608": 82, "164613": 60, "164684": 61, "164698": 73, "1648": 53, "164801": 82, "164805": 69, "164864": 77, "165": 140, "16500": 78, "165178": 82, "165203": 60, "16536299": 124, "165362990": 124, "16539906e": 108, "1654": 79, "165419": 82, "16553": 78, "165549": 138, "165707": 64, "16590": 79, "16597": 79, "166": 140, "1661": 78, "166238": 76, "166375": 93, "166388": 60, "166517": 74, "167": [55, 78, 140], "16725": 79, "167547": 82, "167581e": 67, "1676": 79, "167765": 79, "167993": 124, "168": [60, 140], "16803512": 124, "168089": 76, "168092": 124, "1681": [53, 78], "168195": 84, "168480": 60, "168614": 82, "168645": 61, "168677": 60, "168931": 82, "169": [56, 140], "1691": [53, 79], "16910": 79, "169117": 85, "169196": 82, "169230e": 69, "16951": 79, "169623": 60, "16984": 79, "17": [52, 54, 55, 56, 60, 64, 67, 68, 73, 74, 77, 78, 79, 82, 83, 84, 85, 89, 91, 93, 94, 95, 97, 108, 109, 124, 125, 131, 138, 141], "170": 140, "1704": 79, "170705": 93, "170709e": 68, "17083": 79, "170933": [95, 99], "171": 140, "1712": 139, "1714": 55, "171575": 82, "171696": 93, "171815": 94, "171833": 68, "171848e": 67, "1719": [60, 61], "171921": 60, "171942": 79, "172": [83, 140], "172022": 124, "172083": 68, "172628": 76, "172793": 82, "173": [60, 140], "173504": 72, "17372": 79, "1738": 79, "17385178": 94, "173969": 124, "173e": 83, "174": [60, 140], "174106": 84, "174185": 82, "1741980": 108, "174499": 124, "174516e": 82, "17453": 79, "1746": 79, "174835": 68, "174968": 78, "17499": 79, "175": 140, "1751": 78, "175176": 82, "17522": 79, "175254": 78, "175284": 69, "175369": 68, "175635027": 54, "1757276415": 108, "17576": 79, "175894": 85, "175931": [93, 94, 95], "176267": 60, "176495": 82, "17655394": 82, "176554": 82, "176759": 60, "176929": 124, "176961": 61, "177": [139, 140], "177007": 82, "17700723": 82, "177043": [67, 68], "1773": 79, "1774": 53, "177463": 81, "177496": 82, "177611": 82, "177740": 67, "177751": 82, "17778": 79, "177830": 68, "17799": 79, "177995": 82, "178": [80, 140], "178169": 71, "178191": 60, "178218": 68, "17823": 56, "178466": 60, "178704": 124, "178763": 82, "178920": 60, "178934": 124, "179": [71, 140], "179026": 68, "179101": 93, "1795850": 54, "179588e": 82, "179777": 68, "1798913180930109556": 80, "18": [52, 54, 55, 56, 60, 62, 67, 68, 73, 74, 75, 77, 78, 79, 82, 83, 84, 85, 89, 91, 93, 94, 95, 97, 108, 124, 138, 141], "180": [57, 58, 86, 140], "180143": 64, "18015": 79, "180176e": 79, "180262": 68, "180271": 74, "1803": 53, "18030": 79, "180575": [71, 72], "180633": 60, "1807": [53, 79], "1809": 139, "180951": 82, "181": [60, 140], "1812": 79, "1814": 53, "18141": 79, "181446": 124, "181806": 60, "182": [60, 140], "1820": 53, "182427": 68, "182524": 61, "182633": 82, "182849": 82, "183": [56, 78, 95, 140], "183339": 67, "183373": 95, "183526": 69, "183553": 83, "18356413": 95, "18368": 79, "183855": 94, "183888": 77, "184": [56, 139, 140], "184224": 64, "184247": 67, "184303": [95, 99], "184347": 68, "185": [55, 56], "18500": 79, "185130": 83, "18516129": [95, 99], "1855": 79, "185984": 67, "186": [79, 140], "18604": 79, "1861272": 108, "1862": 53, "186237": 68, "18631": 79, "18637": 136, "1865334": 108, "186589": 64, "18666": 79, "186679": 60, "186689": 95, "186735": 82, "186775": [95, 96, 99], "18678094e": 108, "186836": 82, "187": 140, "187153": 124, "187331e": 60, "187664": 67, "187690": 82, "18789": 79, "188": 140, "188175": 82, "1881752": 82, "188223": 82, "188250": 60, "188400": 68, "188541": [95, 96], "1887": [93, 94, 95], "188760": 74, "18888149e": 108, "188882": 68, "188991": 124, "189": [56, 60, 83, 140], "189195": 79, "189248": 67, "189293": 79, "189471": 61, "1895815": [16, 54, 77], "189737": 82, "189739": 64, "189927": 79, "189998": 82, "19": [52, 54, 55, 56, 60, 67, 68, 74, 76, 77, 78, 79, 82, 83, 84, 85, 93, 94, 95, 97, 108, 124, 138, 141], "190": [56, 60, 140], "19000": 79, "190096": 124, "19031969": 82, "190320": 82, "19033538": 54, "190648": 32, "19073905e": 108, "190809": 82, "190892": 85, "1909": [16, 54, 77], "190915": 69, "190921": 72, "190976": 83, "190982": 82, "191": [56, 60, 139, 140], "191119": 61, "191192": 67, "1912": 139, "191223": 68, "1912705": 88, "191294": 68, "191534": 78, "191716": 79, "1918": 53, "192": 140, "1922": 79, "192222": 61, "192240": 124, "19242": 60, "192505": 81, "192526": 84, "19252647": 84, "192539": [36, 93], "192587": 82, "192778": 60, "192952": 64, "193": [60, 140], "193060": 82, "193253": 67, "193285": 67, "193308": [36, 93], "193341": 68, "193375": 74, "19374710e": 108, "19382": 79, "193849": 83, "19385": 79, "193f0d909729": 56, "194": [75, 79, 140], "194092": 67, "1941": 55, "19413": [78, 79], "194303": 68, "194601": 58, "194786": [95, 96], "195": [51, 60, 90, 91, 140], "19508": 85, "19508031003642462": 85, "19509680e": 108, "195116": 60, "195377": 82, "195396": 82, "195547": 79, "195564": 77, "19559": [55, 78], "195761": 82, "195781": 67, "1959": 139, "195963": 74, "196": [60, 140], "196189": 82, "196437": 79, "196478e": 68, "196655": 74, "19680840": 124, "196e": 40, "197": 140, "1970": 79, "197000e": 79, "19705": 79, "197225": [62, 89, 91, 138], "1972250000001000100001": [56, 89, 91, 138], "1974": 79, "197424": 94, "197484": 124, "19756": 79, "19758": 79, "197600": 59, "197711": 79, "197920": 67, "19793": 79, "19794": 79, "198": [60, 140], "198218": 77, "19824": 79, "198351": 82, "198493": 74, "198503": 83, "198549": 62, "198687": 55, "1988": [52, 66, 88, 95], "199": [60, 140], "1990": [55, 78, 79], "1991": [55, 78, 79, 141], "199281e": 82, "199282e": 79, "199412": 68, "199458": 124, "1995": [54, 77], "199648": 60, "1998": 80, "19983954": 86, "199893": 71, "1999": [80, 86], "1_": [69, 82], "1d": 21, "1e": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 74, 79], "1f77b4": 59, "1x_4x_3": 59, "2": [7, 9, 10, 11, 12, 13, 14, 15, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 45, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 100, 103, 108, 109, 124, 125, 126, 131, 132, 133, 134, 135, 137, 138, 139, 140], "20": [9, 10, 12, 13, 14, 16, 17, 18, 20, 22, 23, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 55, 56, 57, 58, 60, 67, 68, 69, 71, 73, 74, 75, 77, 78, 79, 82, 84, 85, 86, 87, 88, 93, 94, 95, 97, 108, 109, 124, 125, 131, 138, 141], "200": [11, 14, 15, 25, 53, 57, 58, 60, 69, 70, 75, 81, 82, 86, 88, 94, 139, 140], "2000": [8, 37, 55, 57, 64, 67, 68, 69, 74, 78, 79, 82, 86, 93, 95], "20000": [55, 78], "20000000000000004": [69, 79, 82], "200000e": 79, "200049": 67, "20010": 79, "200110": 79, "2003": [7, 139], "200303": 138, "2005": 58, "20055": 79, "2006": 79, "20073763": 75, "20074": 79, "201": [56, 79, 140], "2010": [54, 77], "2011": [54, 77, 136, 138], "2013": [70, 124, 139], "2014": [124, 139], "2015": [15, 139], "201528": [67, 68], "20158": 79, "2016": 80, "2017": [13, 139], "201768": 77, "201781": 60, "201788e": 79, "2018": [7, 8, 17, 18, 52, 54, 55, 58, 66, 70, 75, 77, 78, 79, 84, 88, 95, 100, 108, 109, 117, 124, 136, 139, 140], "2019": [11, 56, 67, 68, 69, 71, 72, 79, 82, 84, 94, 109, 115, 118, 119, 136, 138, 139], "201e": 83, "202": [60, 140], "2020": [9, 10, 12, 14, 20, 22, 23, 25, 26, 53, 56, 58, 60, 85, 94, 95, 97, 100, 125, 126, 139], "2020435": 54, "2021": [16, 25, 53, 54, 56, 60, 61, 67, 68, 77, 95, 96, 99, 100, 139, 140], "20219609": 54, "2022": [84, 85, 95, 97, 100, 125, 126, 135, 136, 139], "2023": [19, 57, 86, 95, 107, 109, 122, 123, 139], "2024": [51, 65, 70, 75, 76, 80, 83, 85, 95, 136, 139], "2025": [25, 60, 95, 96, 99], "202603": 68, "202650e": 69, "20269": 79, "20274": 79, "202846": 68, "203": [55, 60, 67, 78, 140], "203284": 69, "20329": 79, "2036": 79, "203828": 79, "203841": 60, "204": 140, "204007": 82, "20400735": 82, "204355": 60, "204362": 85, "204455": 68, "204482": 82, "204794": 82, "204893": 64, "205": [83, 84, 140], "205187": 69, "205224": 84, "205333e": 78, "205938": 77, "206": 140, "2061": 79, "206253": [78, 79], "206256": 74, "2064": 79, "206614": 82, "207": [83, 95, 140], "207445": 60, "2075": 53, "207771": 60, "207834": 68, "20783816": 54, "207840": 72, "207885": 78, "207912": 124, "208": [60, 64, 140], "208034e": 79, "2080787": 54, "20823898": 54, "208360": 60, "2086": 79, "208766": 60, "209": 64, "209014": 82, "209219e": 84, "209257": 20, "209537": 60, "209546e": 79, "209894": 82, "21": [7, 8, 17, 52, 54, 55, 56, 60, 67, 68, 70, 77, 78, 79, 82, 84, 85, 88, 93, 94, 95, 97, 108, 124, 136, 138, 139, 141], "210": [10, 14, 25, 26, 60, 64, 80], "2103": [79, 136], "2103034": 54, "210319": [67, 68], "210323": 82, "2104": 140, "210624": 60, "2107": 139, "21078": 79, "211": [64, 83, 140], "211002": [95, 99], "21105": [56, 94, 136, 138], "2112": 85, "21142": 79, "211534": 69, "21155656": 82, "211557": 82, "2117581663": 108, "212": [60, 64, 140], "2122": 79, "21257396e": 108, "212811": 64, "212817": 60, "212844": 77, "212863": 67, "213": [60, 64, 83, 139, 140], "213026": 79, "213070": 68, "213135": 68, "213199": 95, "21361": 79, "213743e": 68, "2139": 12, "214": 60, "214458": 76, "214764": 84, "214769": 74, "215": [60, 64, 80], "215069": 82, "215342": 82, "215475": 61, "2155": 79, "21550": 79, "21562": 79, "21573": 79, "215967": 124, "216": [60, 64], "2160": 108, "216207": 94, "21624417": 54, "2163": 79, "216344": 82, "21669513e": 108, "216761": 81, "216943": 93, "217": [64, 83, 139], "21716": 79, "2171802": [54, 77], "217244": 34, "217684": 74, "218": [60, 64], "21804": [55, 78], "218383": 64, "218767": 79, "2189": 79, "218938": 79, "219": [9, 10, 26, 53, 64, 139], "219067": 60, "219124": 60, "2191274": 54, "219585": 64, "2197237644227434": 76, "22": [52, 54, 55, 56, 60, 67, 68, 76, 77, 78, 82, 83, 84, 85, 93, 94, 95, 97, 108, 124, 138, 141], "220": [40, 60, 64, 140], "220088": 79, "220398": 67, "220407": 76, "220446e": 60, "220500": 60, "220772": 82, "22080281": 108, "221": [60, 64, 140], "221175": 60, "2213": 77, "2214": 77, "221419": 79, "2215": 77, "2216": 77, "2217": [54, 77], "2218": 139, "222": 140, "2222": [52, 54, 66, 95], "22222": 79, "222261": 93, "222636": 60, "22272803e": 108, "222814": 60, "222843": 82, "222882": [68, 76], "223": [83, 140], "223158": 76, "223167": 61, "22336235": 54, "223485956098176": [71, 72], "223617": 76, "22375856": 54, "22390": 78, "223928": 76, "224": [80, 83, 140], "2244": 139, "224897": [67, 68], "225": [25, 51, 53, 60, 86, 139, 140], "225034": 58, "22505965": 54, "22507006e": 108, "225175": 82, "225222": 82, "22522221": 82, "22528": 79, "225350": 68, "225427": 64, "225459760731946": 69, "225460": 69, "225574": 77, "2256": 79, "22562": 79, "225670": 76, "225776": 85, "225899": [95, 99], "226": [60, 140], "2264": 53, "226479": 74, "226524": 82, "226598": 77, "226938": 72, "226969": 64, "227": [60, 79, 140], "2271071": 19, "2276": 53, "2279": 79, "227932e": 78, "228035": 79, "2281": 79, "228404": 76, "228597e": 68, "228630": 68, "228648": 55, "229": [55, 60, 140], "22925": 79, "22937": 79, "22938365": 108, "229443": 82, "229452": [93, 94, 95], "229472": 78, "2295": 79, "229759": 94, "2298": 53, "229961": [67, 68], "229994": [67, 68], "23": [23, 43, 47, 54, 55, 56, 58, 60, 67, 68, 75, 77, 78, 79, 82, 84, 85, 89, 91, 93, 94, 95, 108, 124, 136, 138, 139, 141], "230": [25, 53, 139], "230009": [71, 72], "2307": [54, 77, 88], "2308": 84, "230842": 67, "230956": 59, "231": [7, 140], "23113": 95, "231153": 68, "231310": 82, "231430": 124, "231467": 95, "231734": 95, "231798": 95, "231986": 82, "231e": 83, "232134": [67, 68], "232157": 68, "2328": 79, "232868e": 68, "232959": [71, 72], "232e": 95, "233": [13, 60, 141], "233029": 67, "233154": 141, "2335": 53, "233705": 68, "233776": 60, "234": 139, "234137": 85, "234153": 85, "234205": 79, "234431": 76, "234534": 69, "234605": 62, "234798": 79, "234910": 77, "235": [139, 140], "235042": 61, "235291": 67, "2359": 141, "23590": 79, "236": 108, "236008": 69, "236015e": 67, "236309": 79, "236543": 61, "236801e": 60, "236884": 76, "23690345e": 108, "236961": 60, "237": 56, "237115": 68, "237200e": 67, "237252": 79, "237341": 67, "237461": 84, "23748": 79, "23751359e": 108, "237896": 82, "23789633": 82, "238": [54, 77, 140], "238101": 82, "238112": 60, "238225": 124, "238251": 69, "238529": 33, "23856": 79, "238619": 64, "238794": 82, "239": [140, 141], "239019": 74, "239243": 67, "239267": 76, "239313": 68, "23965": 79, "23e": 55, "24": [54, 55, 56, 60, 67, 68, 75, 76, 77, 78, 79, 82, 84, 85, 86, 93, 94, 95, 108, 124, 138, 139, 140, 141], "240127": [67, 68], "240146": 68, "240295": 84, "240532": [67, 68], "2407": 53, "24080030a4d": 56, "240813": 73, "241049": 82, "241064": 68, "241258": 60, "241503": 93, "2416": 53, "241609": 79, "241645": 68, "241678": 67, "241827": 68, "241962": 85, "24199": 79, "241e": 83, "242": [139, 141], "242000": 79, "242124": [78, 79], "242139": 124, "242158": [78, 79], "2424": 74, "242427": 74, "2424596822": 72, "242815": 124, "242902": 82, "243056": 61, "2430561": 53, "243246": 82, "2438": 79, "2439": 79, "243e": 83, "244": [40, 79], "244009": 60, "244090": 79, "244455": 82, "244622": 124, "24469564": 138, "244696": 80, "245": [139, 140], "245062": 82, "2451": 53, "24510393": 55, "245370": 77, "245400": 80, "245512": 82, "245531": 67, "245720": 59, "246": 140, "246624": 93, "2467506": 54, "246753": 82, "246879": 82, "246911": 61, "247": [83, 140], "247020": 69, "247057e": 82, "2471": 79, "2472": 79, "247300": 60, "247617": 93, "247717": 79, "24774": [78, 79], "247826": 77, "247977": 67, "248171": 82, "248441": 93, "248638": 69, "249": [54, 77, 80, 140], "2491": 79, "24914182832383940495152535462829394100": 108, "24917": 79, "249601": [95, 96, 99], "2499": [90, 91], "249986": 76, "25": [9, 10, 14, 15, 16, 17, 25, 26, 36, 37, 54, 55, 56, 59, 60, 67, 68, 69, 70, 75, 76, 77, 78, 79, 82, 85, 86, 93, 94, 95, 108, 124, 138, 141], "250": [80, 140], "2500": [79, 90, 91], "25000000000000006": [69, 79, 82], "250073": 79, "250210": 69, "2503": 79, "250354": 82, "250425": 69, "251": [78, 79, 80, 84], "251194": 60, "251412": 68, "251480": 68, "251953": 79, "252133": 79, "252253": 84, "25240463": 95, "252524": 82, "252601": 124, "25301488": 108, "253026": [67, 68], "2532": 79, "253437": 81, "253675": 78, "253724": 82, "25374": 79, "254": [79, 140], "25401679": 54, "254035": 76, "254038": 72, "254083": 68, "2543": 79, "254324": 69, "254400": 124, "254971": 60, "255": [79, 140], "255034e": 68, "255995": 67, "256": [51, 79, 94], "2560577": 108, "256082": 93, "256416": 82, "256567": 77, "25672": 79, "256944": 82, "256992": 79, "257019": 68, "257207": 54, "257377": 59, "257523": 67, "258083": 68, "258158": [67, 68], "2583": 79, "258357": 60, "258522": 67, "258541e": 37, "258612": 108, "258951": 82, "25904155": 108, "259164": 68, "259316": 60, "259367": 93, "259395": 73, "2594": [55, 78], "259828": [67, 68], "259875": 68, "25x_3": 59, "26": [54, 55, 56, 58, 60, 62, 67, 68, 75, 77, 78, 79, 86, 89, 91, 93, 94, 95, 108, 124, 138], "260057": 60, "26016": 79, "260161": [35, 93], "260211": [67, 68], "260356": 78, "260360": 82, "260738": [95, 99], "260762": 64, "261": 83, "2610": 79, "2613": 79, "261624": [78, 79], "261685": 79, "26175": 79, "261777": 79, "261903": 77, "2619317": 54, "262423e": 79, "262621": 77, "262829": 108, "262867": 60, "263": [7, 79, 140], "2633": 79, "263942e": 68, "263974e": 82, "264": [139, 140], "264086": 59, "264274e": 79, "264884": 79, "265": 140, "2651": 95, "265119": 81, "2652": [56, 78, 79], "265547": 79, "265744": 76, "2658": 72, "265929": 83, "266": 140, "266147": 83, "266686": 67, "266922": 124, "267": 80, "2670691": 54, "267500": 77, "267581": 79, "267950": 82, "268": 140, "268055": 79, "268343": 76, "268628e": 68, "268853": 60, "268942": 82, "268943": 67, "268998": 55, "269043": 82, "269112": 95, "269179": 60, "269977": 79, "26bd56a6": 56, "26e": 55, "27": [10, 14, 25, 26, 52, 54, 55, 56, 57, 58, 60, 62, 67, 68, 75, 77, 78, 79, 86, 89, 91, 93, 94, 95, 108, 124, 138, 139], "270": 140, "2700": 56, "270248": [95, 99], "270644": [67, 68], "270706": 60, "271": 140, "271004": [78, 79], "271083": 79, "271183": 78, "271556": [95, 96], "272296": 79, "272332e": 67, "272408": 68, "272662": 79, "272778": 60, "273": 56, "273295": 60, "273299": 68, "273322": 60, "273356": 69, "27371": [55, 78], "27372": [55, 78], "274": [56, 79], "2740991": 53, "274251e": 78, "274267": 77, "27429763": [95, 97], "274535": 60, "27461519": [95, 99], "274793": 82, "274825": [36, 93], "27487": 79, "2754": 53, "275587": 60, "275596": 124, "275671": 60, "276": [56, 140], "276148": 82, "276189e": 77, "2764": 79, "2766091": 55, "27713": 79, "277160": 60, "277299": 62, "27751": 79, "277512": 68, "277561e": 77, "277716": 60, "277968": 82, "278": [84, 140], "2780": 54, "278000": 77, "278035": 64, "278391": 79, "278434": 71, "278454": 74, "2786": 124, "278804": 68, "279": 140, "279151": 60, "27951256e": 108, "279595": 64, "27986": 79, "279933e": 68, "28": [54, 55, 56, 60, 61, 67, 68, 70, 75, 77, 78, 86, 93, 94, 95, 108, 124, 138, 140], "280196": 72, "280454dd": 56, "280514": 124, "280963": 81, "281": [83, 140], "281024": 82, "28111364": 55, "2815": 79, "281567": 60, "2818": 53, "2819": 124, "282": [83, 139, 140], "282200": 72, "2825": [136, 138], "28251": 79, "282605": 60, "282870": 79, "2830": [136, 138], "283041": 67, "283207": 67, "28326": 79, "283386": 67, "2836": 53, "2836059": 54, "28382": 79, "283974": 82, "283992": 67, "283994": 82, "283e": 83, "284": 140, "28425026": 84, "284271": 73, "284397": 141, "28452": [55, 78], "2849": 79, "284987": 79, "285": [83, 95, 140], "285001": 64, "285203": 60, "285483": 74, "285e": 83, "286": 140, "286203": 67, "286371": 67, "2865": [53, 79], "286507": 69, "286563e": 79, "286593": 79, "287041": 82, "287123": 93, "287196": 67, "287815": 84, "287926": 82, "288": 80, "288279": 60, "2886505": 108, "288965": 61, "288976": 79, "289": 139, "289062": 78, "289357": 68, "289440": [67, 68], "289555": 74, "29": [54, 55, 56, 60, 67, 68, 75, 77, 78, 84, 86, 93, 94, 95, 108, 124, 138], "290": 95, "290511": 60, "290565": 68, "290736e": 68, "290901": 64, "290987": 78, "291": [51, 79, 83], "2910": 79, "291008": 67, "291011": [95, 97], "291071": 82, "29107127": 82, "291405": 82, "291406": 82, "291434": 68, "291500e": [78, 79], "291517": [67, 68], "29168951": [95, 99], "291811": 60, "291963": 82, "292": 81, "292028": 69, "292047": 124, "292105": 82, "292302995303554": 69, "292303": 69, "2925": 56, "2927": 79, "292728": 60, "292997": 82, "29299726": 82, "293218": 82, "293255": 60, "293617e": 79, "294067": [67, 68], "294123": 93, "294449": 67, "2946758": 108, "2947706": 108, "295": 139, "295165": 60, "295307": 67, "295481": 82, "29548121": 82, "295837": [62, 89, 91, 138], "2958370000000100000100": [56, 89, 91, 138], "2958370001000010011100": [56, 89, 91, 138], "2958371000000010010100": [56, 89, 91, 138], "296": 51, "296099": 64, "296228": 79, "296729": 77, "29678199": [87, 109], "296901": 67, "297276": [95, 99], "297287": [67, 68], "2973": 79, "297349": [71, 72], "297682": 82, "297687": 79, "297749": 79, "29784405": 84, "298": [13, 56], "298076": 67, "298120": 69, "298228e": 79, "299": [56, 80, 83], "299537": 72, "299712": 71, "299804": 60, "2999": 64, "29999": 60, "2_": [19, 57, 86, 125, 126, 135], "2_x": [19, 57, 86], "2d": [21, 109, 118], "2dx_5": [69, 82], "2e": [51, 53, 54, 55, 56, 57, 94, 95, 109, 124, 138], "2f": 73, "2m": [125, 131, 135], "2n_t": 59, "2x": 82, "2x_0": [11, 67, 68, 71, 72], "2x_4": 59, "3": [9, 10, 11, 12, 13, 14, 17, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 100, 108, 109, 124, 125, 131, 136, 137, 138, 139, 140], "30": [11, 51, 52, 54, 56, 57, 58, 60, 61, 64, 65, 66, 67, 68, 69, 75, 77, 78, 79, 82, 83, 86, 93, 94, 95, 108, 124, 138], "300": [52, 66, 69, 79, 82, 88, 139], "3000": 64, "30000": 60, "30000000000000004": [69, 79, 82], "300031": 67, "300181": 60, "30031116e": 108, "300892e": 68, "30093956": 84, "301": 56, "301366": 124, "301371": 82, "3016": 78, "301737": 67, "30189": 79, "302149": 67, "30226": 108, "302357": 82, "302376": 60, "302382": 76, "302389": 60, "302648": 77, "3026600": 108, "303007": 67, "3032574": 108, "303324": 77, "303489": 82, "3034964": 108, "303613": 82, "30361321": 82, "3037654": 108, "30383": 79, "303835": 77, "303f00f0bd62": 56, "304130": 82, "304159": 82, "304201": 59, "304978": 61, "30527": 79, "305341": 82, "305612": 77, "3057085": 108, "305735": 60, "305775": 82, "305b": 56, "306297": 67, "30645": 79, "30672815": 54, "306915": 77, "306963": 82, "307407": 82, "308": 79, "308064": 60, "308568": 68, "30859411": 108, "308774": 67, "30915165": 108, "30917769": [71, 72], "309539": 76, "309772": 77, "309823e": 79, "30982972": 82, "309830": 82, "31": [54, 55, 56, 57, 60, 61, 67, 68, 75, 77, 78, 79, 86, 93, 94, 95, 108, 124, 138, 141], "310000e": 79, "310008": 60, "310145": 76, "310703": 108, "310761": 81, "310907": 60, "311": 83, "311253": 79, "311321": 68, "311667": 68, "311712": 71, "31183136": 108, "311869": 93, "3120": 79, "312172226313435424648505564757781909295": 108, "312172226313435424648505564757781909295567102341434763666869787984858687899981116202124272930365657596165717273808324914182832383940495152535462829394100": 108, "312652": 83, "313": 95, "313056": 124, "313209": 69, "313324": 79, "31337878": 79, "313535": 82, "31378": 56, "313870": 74, "314": 108, "3141": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 56, 57, 62, 77, 87, 89, 91, 93, 94, 95, 109, 124, 138], "314247": 85, "314341": 67, "3145176": 108, "3146": 37, "314625": 68, "314651": 71, "31476": [78, 79], "315031": 85, "315036": 67, "3151": 79, "315155": 68, "315290": [71, 72], "315310": 67, "315769e": 67, "316": 56, "316193": 82, "31632": 79, "316407": 95, "316540": 77, "316717": [67, 68], "316826": 67, "316863": 68, "317326": 60, "317394": 59, "317487": 82, "317607": 82, "318": 56, "318000e": 79, "3183322": 108, "318438": 79, "31845078": 60, "318552": 79, "318584": 124, "318753": [71, 72], "319": 56, "319100": [71, 72], "31910229": [95, 99], "319420": 74, "319759": 82, "319850": 82, "32": [39, 54, 55, 56, 60, 61, 67, 68, 75, 76, 77, 78, 79, 86, 93, 94, 95, 97, 108, 109, 124, 138], "320": [79, 80], "320034": 61, "320220": 60, "320314": 78, "320633": 69, "3214387": 108, "321537": 60, "321686": 124, "3218": 108, "322186e": 67, "32236455588136": 58, "322367": 60, "322404": 84, "322615": 60, "3226346": 108, "3226848": 108, "322751": 68, "3234": 79, "323636": 78, "323679": 77, "324": [55, 79], "324249": 60, "324518": 81, "32458367": 54, "3245837": 82, "324692": 60, "325056": 82, "325090": 79, "325486": 67, "325599": 67, "326": 83, "326148": 68, "326372": 60, "326721": 68, "326740": 82, "32674263": 61, "326871": 85, "3268714482135234": 85, "327257": 67, "327803": 93, "327958": 64, "328471": 67, "32867133": 60, "32875335": 61, "329258": 60, "329339": 58, "329446": 60, "32950022e": 108, "329972": 60, "33": [51, 54, 55, 56, 60, 67, 68, 71, 75, 76, 77, 78, 79, 86, 93, 94, 95, 108, 124, 138, 139], "3300": [55, 78], "330068": 67, "330100": 67, "330143": 82, "33014346": 82, "330163": 68, "330285": [67, 68], "3304269": 54, "330436718": 108, "330615": 82, "330731": [36, 93], "331365": 71, "331521": 82, "331602": 79, "33175566": 82, "331756": 82, "332271": 60, "332502": 68, "332782": [36, 93], "3329": 79, "332996": 77, "3333": [52, 54, 66, 93, 94, 95], "3333333": 56, "33333333": [60, 61], "33335939e": 108, "3335": 79, "333581": 78, "333655": 67, "333704": 68, "333955": 68, "334": 55, "334279": 60, "334356": 60, "334732": 60, "334750": 69, "335": 80, "33500": 79, "335176": 79, "335446": 64, "335609e": 82, "335846": 82, "335853": 79, "33613": [95, 96], "336200": 60, "336382": 68, "336410": 60, "336461": 79, "336612": 59, "337380": 82, "3376": 53, "337619": 58, "3378132": 108, "338": 84, "33849": 79, "3386": 108, "338603": 67, "338775": 69, "338908": 69, "339269": 84, "33928": 79, "339443": 68, "339526": 60, "339570": 82, "339875": [71, 72], "34": [51, 52, 53, 54, 55, 56, 57, 60, 61, 67, 68, 72, 75, 77, 78, 79, 84, 86, 93, 94, 95, 108, 124, 141], "340": [55, 79], "340029": 68, "340142": 95, "340213": 60, "340235": 74, "340274": 84, "340990": 60, "341336": 34, "341472": 76, "341755e": 67, "3420": 79, "342117": 74, "342326": 61, "342362": 64, "342559": 60, "342632": 76, "342675": 54, "34287815": 84, "342989": 79, "342992": 77, "343": [79, 83], "343639": 93, "343685": 67, "34375": 78, "343828": 67, "344212": 141, "344418": 60, "344440": 93, "34450402": 61, "344505": [78, 79], "344640": 82, "344787": [67, 68], "344834": 59, "345": 51, "345065e": 79, "345381": 69, "3453813031813522": 69, "3454": 79, "345852": 68, "345903": 82, "345989": 67, "346107": 78, "346206": 82, "346238": 84, "346269": 68, "346678": 81, "346964": 67, "347310": [36, 93], "347696": 69, "34769649731686": 69, "347929": 79, "348": 83, "348319": 68, "348429": 60, "34858240261807": 58, "348617": 82, "348622": 83, "348700": 68, "348761": 60, "348980e": 68, "349213": 61, "3492131": 53, "349383": 77, "34943627": 75, "349638": 68, "34967621": 54, "349772": 72, "35": [51, 55, 56, 60, 67, 68, 69, 77, 78, 79, 82, 93, 94, 95, 108, 124, 125, 131, 141], "3500000000000001": [69, 79, 82], "350165": 94, "350208": 67, "350386": 61, "350518": 82, "350712": [71, 72], "35077502": [125, 131], "351220": 68, "351629": 79, "351766": 81, "352": [55, 77], "352250e": 78, "352259e": 79, "3522697": 54, "35287789": 60, "35292": 79, "352990": 79, "352998": 79, "353105": 37, "353412": 82, "35341202": 82, "35365143": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "353748e": 82, "3538": 53, "354": 79, "354186": 60, "354188": 59, "354371": 82, "354688": 38, "3547479": 108, "355065": 64, "355209": 82, "355627": 93, "355651": 68, "3561169": 108, "356136e": 79, "356167": 72, "356183": 79, "35620768e": 108, "3564": 79, "3565": 79, "356788": 60, "3568": 95, "357": 79, "357170": 67, "35731523": [95, 97], "358158": [78, 141], "358289": 77, "358395": 84, "358799": 124, "358822": 60, "359": [83, 141], "359100": 79, "359161e": 67, "359229": 64, "3593": 84, "359307": 68, "35th": 139, "36": [55, 56, 61, 67, 68, 77, 78, 93, 94, 95, 108, 124], "360004": 82, "360065": 124, "360249": 73, "360475": [67, 68], "360572": 68, "360655": 79, "360683": 69, "360801": 69, "361": 83, "361518": 69, "361518457569366": 69, "361521": 38, "361623": 74, "3619201": 12, "362157": 67, "36231307e": 108, "363276": 54, "364221": 67, "3643": 124, "364595": 54, "3647": 56, "364800": 82, "36501": 79, "365551": 68, "36557195e": 108, "36566025e": 108, "366": 79, "36616": 79, "366310": 67, "366529": 81, "366718627": 54, "366950": 67, "36696349": [95, 99], "367": [40, 83], "367181": 67, "367323": 82, "367366": 74, "367398": 76, "367571": 69, "367625": 82, "368": [60, 61], "368152": 77, "3682": [55, 78, 79], "368324": 77, "368499": 69, "3684990272106954": 69, "368577": 93, "369556": 69, "3696": 84, "369796": 82, "369869": 78, "369981": 77, "37": [55, 60, 61, 67, 68, 74, 77, 78, 79, 93, 94, 95, 108, 124], "370254e": 78, "3702770": 54, "3704408": 108, "370736": 77, "3707775": 54, "370908": 67, "3710": 79, "371357": [78, 79], "371429": 69, "371850e": 68, "371916": 80, "372": 139, "37200": [78, 79], "372097": 69, "3722": 79, "372233": 60, "37231324": 86, "3724": 79, "372427": 68, "3727679": 54, "372817": 60, "373218e": 76, "373451": 93, "3738573": 54, "374364": 82, "37436439": 82, "3745": 79, "374821e": 79, "374862": 67, "374917e": 67, "375081": 79, "375274": 67, "375465": 82, "375621": 76, "375844": 76, "376376": 60, "3766": 74, "376617": 74, "376760": 68, "376789": 108, "376806": 68, "377060": 79, "377147": 93, "377311": 82, "377669": 68, "378351": 32, "378588": 67, "378596": 77, "378688": 82, "378727": 67, "378834": 82, "378835": 60, "3788859": 54, "379": 139, "379038": 82, "37939": 79, "379614": 82, "379626": 67, "379981e": 68, "38": [56, 67, 68, 78, 80, 93, 94, 95, 108, 124], "3800694": 54, "380071": 60, "380837": [78, 79], "380952": 60, "381": 83, "381072": 82, "381603": 67, "381685e": [78, 79], "381689": 82, "3817": 79, "382286": 79, "382582e": 31, "382872": 69, "383297": 82, "383531": 76, "384": 79, "384443": 68, "384677": 64, "38470495": [95, 99], "384777": 79, "384865": 68, "384928": 67, "3851": 79, "385160": 68, "385240": 124, "385917": 77, "385963": 60, "386": [56, 79], "386102": 69, "38639": 108, "386502": 79, "38654634": 61, "386831": 64, "386988": 58, "387": 56, "3871": 53, "387426": 82, "387780": 82, "388026": 67, "388071": 82, "388185": 64, "38818693": 108, "388216e": 94, "388668": 82, "38866808": 82, "388871": 79, "389": 56, "389126": 95, "389164": 76, "389489": [95, 96], "389566": 81, "38973512e": 108, "389755": 67, "38990574": 108, "39": [51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 64, 67, 68, 72, 73, 75, 77, 78, 79, 80, 84, 85, 86, 93, 94, 95, 108, 124], "39010121e": 108, "390111": 60, "390379": 82, "390801": 60, "391": 51, "391377": 85, "392066": 60, "392128": 68, "392242": 73, "39236801": 75, "392400": 79, "3924608": 108, "392623": 68, "392752": 58, "392833": 84, "392864e": [78, 79], "392917": 67, "393604": 69, "393654": 64, "394226": 68, "39425708": 54, "395076e": 79, "395136": 77, "395268": 93, "395569": 67, "395603": 67, "3958": 95, "395889": 79, "396": [40, 95], "39611477": 55, "396173": 71, "39621961e": 108, "396300": 71, "3964": 79, "396531": 79, "396985": 77, "396992": [67, 68], "39702421": 61, "397140": 69, "397155": 68, "39727": 79, "397313": 53, "397536": 76, "397578": 73, "397811": 84, "3979": 61, "398": [89, 91, 138], "398166": 64, "398279": 60, "3984405": 108, "3985": 79, "398770": 82, "39879287": 61, "398999": 93, "399": 55, "399056": 82, "399223": 59, "399343e": 67, "399355": 59, "399679": 95, "399692": 82, "399741": 60, "399858": 85, "3cd0": 56, "3dx_1": [69, 82], "3e1c": 56, "3ec2": 56, "3f5d93": 80, "3x_": 82, "3x_4": [69, 82], "4": [10, 14, 19, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 100, 108, 109, 124, 125, 131, 138, 140], "40": [54, 57, 58, 67, 68, 69, 76, 78, 79, 82, 86, 89, 91, 93, 94, 95, 108, 124, 125, 131], "400": 77, "4000": 60, "4000000000000001": 94, "40000000000000013": [69, 79, 82], "400113": 74, "40029364": [125, 131], "400823": 82, "400855956463958": 69, "400856": 69, "400905": 64, "401": [7, 141], "401021": 61, "401247": [109, 124], "40127723e": 108, "401690e": 68, "4018738": 108, "401931": [71, 72], "402077": 79, "402113": 124, "402301e": 94, "40247163": 108, "402619": 39, "402902": 79, "403": 83, "403425": 82, "403626490670169": 87, "4036264906701690": 87, "403626491": 87, "403715": 31, "403771948": 109, "403798": 61, "403869": 60, "4039": 53, "40401708": 61, "404267": 67, "404300": 64, "404318": 53, "404411": 67, "404517": 60, "40452": 79, "404550": 81, "404978": 60, "405050": 68, "4051695": 61, "405203": 59, "405374": 79, "4054328": 61, "40583": 53, "405890": [36, 93], "406285": 82, "406446": 69, "4065173": 108, "40676": 53, "407": 83, "40732": 74, "407558": 67, "407565": 67, "40775095": 61, "408027": 61, "408476": [125, 131], "40847623": [125, 131], "408479": 77, "408509": 68, "408539": 82, "408565": 82, "408990": 60, "409154": 53, "4093": 84, "409328": 79, "409395": 82, "409746": 69, "409848": [67, 68], "409863": 61, "41": [67, 68, 78, 79, 93, 94, 95, 108, 124], "410100": 67, "410393": 69, "410667": 93, "410681": 59, "410682": 67, "410756": 60, "410795": 77, "41093655": 108, "411146e": 68, "411190": [67, 68], "411231": 61, "411291": 81, "411295": 82, "411304": [67, 68], "411447": 79, "411582": 82, "411768": 68, "412004": 71, "412127": 82, "412304": 85, "412477": 59, "412653": 77, "412714": 69, "412726": 68, "412941e": 68, "413": 80, "413247e": 67, "41336": 94, "413376": 95, "41341040": 54, "413608": 82, "414": 83, "414073": 33, "414080": 60, "41440101": 61, "414533": 68, "41525168e": 108, "415375": 67, "41550119": 61, "415556": 93, "41566": 95, "415812": 141, "415988": 79, "416052": 64, "416132": 68, "4166": 79, "4166667": 56, "416757": 82, "416899": 67, "41690989": 108, "416919": 68, "416e": 83, "417640": 67, "417727": 78, "417736": 76, "417767": [71, 72], "417834": 64, "41798768e": 108, "418": 40, "418056": 82, "41805621": 82, "418089": 60, "418400": 74, "418741": 64, "418806e": 69, "418969": 93, "41918406e": 108, "419371": 82, "419871": 64, "41989983e": 108, "4199952": 54, "41e5": 56, "42": [14, 20, 23, 24, 57, 58, 59, 61, 64, 66, 67, 68, 69, 71, 72, 73, 74, 75, 79, 81, 82, 84, 85, 86, 90, 91, 93, 94, 95, 96, 97, 99, 108, 124, 139], "4200": 79, "420316e": 79, "420376": 61, "420608e": 84, "42073312": 54, "42094064": [95, 99], "420967": 69, "421083": 53, "4211349413": 54, "421163": 67, "421200": 85, "421297e": 68, "421357": [71, 72], "421576e": 79, "421793": 84, "421919": 79, "422007": 84, "422266": 79, "422293e": 93, "422325": 69, "422591": 68, "42338": 79, "42340476": 61, "4235839": [71, 72], "42388745": 86, "42390712": 61, "423921e": 93, "423951": 53, "424108": 69, "424127": 108, "42412729": 54, "4242088": 61, "424292": 67, "424328": 82, "424651": 95, "424717": 69, "424748": 85, "425": 77, "42500164": 61, "425103": 53, "425208": 79, "425325": 74, "425493": 53, "42550": 79, "425636": 74, "426055": 53, "426540": 77, "426540301": 54, "42654656": 61, "426697": 60, "426736": 79, "42679686": 61, "427": 79, "427337": 60, "427486": [67, 68], "42755087": 84, "427551": 84, "427573": 77, "427654": 74, "427725": 82, "428": [124, 141], "428046": 81, "42811700": 141, "428255": 82, "428411": [78, 79], "428467": 82, "4284675": 82, "428771": [36, 93], "4290": 53, "429057": 68, "429230": 67, "429705": 67, "42ba": 56, "43": [55, 60, 61, 64, 67, 68, 78, 93, 94, 95, 108, 124], "430298e": [78, 79], "430465": [95, 99], "430595": 68, "430608": 67, "431061e": 67, "4311947070055128": 94, "431253": 76, "431306": 82, "431701914": 136, "431998": 64, "432130e": 78, "432300e": 82, "43231359e": 108, "432707": 83, "43294": 56, "432f": 56, "433": [56, 83], "433221": 69, "4336": 79, "43374433": 86, "433750": 67, "433753": 76, "4339": 53, "434054e": 74, "434121": 76, "434535": 82, "43453524": 82, "435": 56, "43503345": 95, "43511": 79, "435189": 60, "435401": 77, "4357": 79, "435927": 79, "435967": 77, "43597565": 82, "435976": 82, "436": [56, 79], "436016": 76, "43627032": 58, "436327": 79, "436394": 76, "43662522": 61, "436764": 83, "436806": 79, "436817": 76, "437": 51, "437667": 78, "437924": 79, "438": 77, "438219": 82, "438289": 79, "438569": 79, "438578e": 79, "43883": 72, "438834": 68, "4389": 79, "438960": 77, "439366": 60, "439370": 60, "439401e": 68, "439541": [78, 79], "439675": 83, "439699": 64, "43989": 93, "439958": 76, "43f0": 56, "44": [58, 61, 64, 67, 68, 93, 94, 95, 97, 108, 124], "440320": 79, "44060256": 108, "440605": 94, "440747": 67, "440a": 56, "441153": 82, "441209": 82, "441219": 71, "44124313": 95, "441282": 67, "4416552": 54, "44182814": 108, "441849": 67, "441996": 61, "442847": [95, 96], "443016": 69, "443032": 78, "44312177": 55, "443672": [95, 99], "443686": 82, "4437": 79, "443e": 83, "444046": 79, "4444": [52, 54, 66, 95], "444500": [78, 79], "444850": 79, "4449272": 79, "445": 83, "445476": 67, "44563945e": 108, "4461928741399595": 69, "446193": 69, "4462": 56, "44647451": 84, "446510": 61, "44713577e": 108, "447492": 79, "447624": [67, 68], "447706": 69, "447849": 58, "448": 79, "448252": 68, "448456e": 68, "448569": 67, "448587": 69, "448745": 82, "448842": 68, "4489": 79, "44890536": 108, "448923": 73, "449107": 23, "449150": [36, 93], "44950": 79, "449579": 60, "449677": 74, "44fa97767be8": 56, "45": [67, 68, 69, 71, 73, 76, 79, 82, 93, 94, 95, 108, 124], "4500": 78, "45000000000000007": [69, 79, 82, 94], "450031": 76, "450152": 77, "450812e": 68, "450870601": 54, "451312e": 67, "451569": 60, "451808": 60, "452": 56, "452091": 79, "452114": 93, "452488701": 54, "452489": 77, "452623": 68, "453": 56, "453279": 67, "4535": 79, "4539": 56, "454081": 79, "454397": 82, "454406": 64, "45467447": 108, "455": 56, "45500": 79, "455078": 69, "455091": 68, "455107": 69, "455120": 82, "4552": 56, "455293": 69, "4552b8af": 56, "455448": 84, "455672": 79, "455981": 125, "456370": 77, "456458e": 67, "4566031": 124, "45660310": 124, "4567": 84, "456892": 69, "457088": 82, "457496": 60, "457667": 79, "458114": 79, "45828237": 61, "458307": 126, "458420": 79, "4584447": 54, "458784": 67, "458814": 74, "458855": 55, "459174": 60, "4592": 54, "459200": 77, "459383": 69, "459418": 68, "459436": 76, "459473": 60, "45957837": 108, "459760": 79, "459812": 69, "46": [61, 67, 68, 73, 75, 80, 93, 94, 95, 108, 124], "460": 79, "4601": 79, "460207": [67, 68], "460218": 69, "460289": 82, "460744": 78, "4610": 141, "461227e": 67, "461412": 93, "461629": 85, "462117": 60, "462451": 69, "462567": 68, "462979": 67, "463046": 60, "463325": 82, "4634": 79, "463418": 85, "463668": 79, "463766": 72, "463816": [95, 99], "463857": 79, "463903": 68, "463b": 56, "464": 95, "464076": 69, "464099": 80, "464284": 77, "46448227": 108, "464668": 34, "465": 64, "46507214": 84, "465424": 76, "465649": 85, "465730": 85, "4659651": 87, "465965114589023": 87, "4659651145890230": 87, "466047": 82, "46618738": 108, "466440": 69, "466756": 82, "466918": 60, "467": 79, "46709481": 108, "46722576e": 108, "467266": 60, "467613": 77, "467613401": 54, "467681": [67, 68], "467770": 69, "468023": 60, "468072": 68, "468075": 82, "46807543": 82, "46811985": 82, "468120": 82, "468406": 79, "468449": [95, 99], "468907": 64, "468919": 79, "468d": 56, "469": 56, "469474": 85, "469676": 64, "469825": 69, "469846": 61, "469895": 68, "469905": 68, "47": [55, 58, 64, 67, 68, 78, 84, 93, 94, 95, 108, 124, 140], "470055": 68, "470904": 67, "471": 64, "471435": 83, "471622": 67, "472": 79, "47222159": 86, "472255": 79, "472699": 68, "472891": 82, "472e": 56, "473099": 69, "47319": 95, "473491": 60, "47419634": 138, "474214": [71, 72], "474731": 93, "475304": 79, "475517": 76, "475569": 67, "475e": 83, "476856": 69, "477130": [67, 68], "477150": 82, "477247": 68, "477357": 83, "477474": 77, "47759584": 108, "47761563": 58, "478032": 79, "478059": 68, "478064": 68, "4781": 79, "47857478": 108, "479655": 76, "47966100e": 108, "479722": 68, "479860": 79, "479876": [71, 72], "479882": 68, "479928": 82, "479942": 61, "479959": [95, 99], "47be": 56, "48": [56, 64, 67, 68, 72, 78, 79, 93, 94, 95, 108, 124], "480": 64, "480133e": 82, "480199": 74, "48029755": 84, "480579": 68, "48069071": [87, 109, 124], "480691": [109, 124], "480800e": 82, "481172": 82, "481218": 79, "481399": [78, 79], "481705": 95, "481713": 74, "481761e": 79, "482": [56, 64], "482012": 71, "482038": 69, "48208358": 82, "482084": 82, "482179": 67, "482251": 39, "482461": [125, 131], "48246134": [125, 131], "482483": 82, "482616": 76, "482790": 59, "482898e": 68, "48296": 84, "483": [83, 95], "48315": 84, "483186": 59, "483192": [78, 79], "48331": 84, "4835": 79, "4836424": 108, "483711": 82, "483717": 69, "48390784": 95, "48404": 54, "484303": 68, "4845": 79, "484640": 82, "4849": 56, "485": [56, 79], "485197": 67, "485281": 60, "48550": 85, "485617": [78, 79], "485812e": 79, "48583": [78, 79], "485871": 72, "486": [15, 79], "486178e": 67, "486202": 69, "486532": 82, "48661": 79, "4869555": 108, "487": [64, 79], "487329": 61, "487467": 79, "487524": 74, "487641e": 82, "487793": 68, "487872": 65, "488394": 67, "488460": 79, "488485": 79, "48873663": 58, "488811": 82, "488909": [78, 79], "488982e": 69, "489488": [95, 99], "4895498": 82, "489550": 82, "489699": 69, "489951": 68, "49": [56, 64, 67, 68, 93, 94, 95, 108, 124], "490000e": 79, "490070931": 54, "490488e": 78, "490504e": 79, "490700": 82, "490896": 64, "490941": 79, "491034": 67, "491245": 77, "49135": 37, "4915707": [95, 97], "492": 79, "4923156": 87, "49231564722955": 87, "492315647229550": 87, "492417e": 95, "492637": 73, "492656": 68, "4926616": 108, "49270769e": 108, "493": [83, 95, 139], "493102e": 76, "493144": 85, "493195": [60, 74], "493219": 82, "493313": 79, "493325": 20, "493426": 83, "494": 83, "494089": 68, "494129": 82, "494324": 77, "494324401": 54, "494716": 60, "49475284": 108, "495": 81, "495108": 76, "49530782": 54, "495657": 69, "495752": 82, "49596416e": 108, "496": 81, "49650883": 84, "496551": 82, "496591": [95, 99], "496714": 85, "496749": 80, "496777": 141, "49693": 94, "497": 81, "497168": 76, "497422": 68, "497647": 60, "497655": 23, "497674": 58, "497964": 93, "498": 81, "498122": 74, "498235": 80, "498286": 76, "498921": 82, "498979": 79, "498992": 67, "498f": 56, "499": [79, 81, 89, 91, 138], "499000e": [78, 79], "499776": 79, "49d4": 56, "4a53": 56, "4b8f": 56, "4dba": 56, "4dd2": 56, "4e": [54, 55], "4ecd": 56, "4fee": 56, "4x": 82, "4x_0": [11, 67, 68, 71, 72], "4x_1": [11, 67, 68], "5": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 108, 109, 117, 124, 125, 131, 137, 138, 140], "50": [36, 54, 56, 57, 59, 64, 69, 72, 75, 76, 78, 79, 80, 82, 93, 94, 108, 124, 141], "500": [6, 9, 10, 12, 13, 17, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 52, 56, 60, 61, 62, 66, 67, 68, 71, 72, 75, 78, 81, 83, 84, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 109, 124, 125, 131, 138, 141], "5000": [41, 60, 61, 67, 68, 69, 82], "50000": 77, "500000": [78, 79], "5000000000000001": [69, 79, 82], "500084": 82, "500267": 73, "5003517412": 54, "500517": 82, "50093148e": 108, "501021": 79, "501047e": 67, "501765": 80, "501983": 82, "502005": 93, "502016": 76, "502084": 95, "502205": 67, "502494": 69, "5025850": 54, "502595": 68, "502612": 82, "502901": 68, "502995": 82, "503": 40, "503241": 60, "503251": 80, "503374": 76, "503504": 94, "503511": 79, "503700": 64, "50398782e": 108, "504286": 77, "5042861": 54, "504548e": 68, "505020": 60, "5050973": 54, "505264": 67, "505353": 68, "506050": 67, "506644": 67, "506659": 79, "506687": 79, "50672034": 54, "506900e": 82, "506903": 69, "507": 83, "507285": 74, "507379": 60, "50768b": 80, "508153": 81, "508433": 67, "508459": 77, "5085": 79, "508947": [95, 97], "509059": 79, "509196": 82, "509461": 82, "50967": 85, "5097": 85, "5098": [62, 89, 91, 138], "509853": 82, "5099": [56, 62, 89, 91, 138], "509951": 69, "509958": 77, "51": [53, 55, 56, 61, 64, 71, 76, 78, 93, 94, 108, 124, 140], "510000e": [78, 79], "510121": 67, "510385": 77, "510555": 64, "51079110": 54, "511257": 76, "511515": 79, "511540": 79, "5115547": 87, "5115547181877": 87, "51155471818770": 87, "511665": 67, "511668": 85, "5116683753999614": 85, "511862": 82, "512": 77, "512108": 82, "512149": 82, "51214922": 82, "51243406e": 108, "512519": 77, "512572": 82, "512672": [95, 125, 131], "5131": 78, "513222": 74, "513624": 93, "513992": 82, "514": 56, "514173": 68, "514545": 79, "51494845": [95, 99], "515031": 67, "515338e": 67, "515358": 69, "5154": 79, "515474": 60, "5154789948092002": 77, "5155": 56, "515672": 68, "516": 56, "516125": 69, "516222": 82, "516242": 68, "516255": 82, "516256": 82, "516528": 82, "516600": 60, "516797": 68, "517": [56, 77], "517132e": 80, "517279": 68, "517438": 60, "5175": 79, "517753": 67, "517798": 64, "518175": 77, "518375": 68, "518446": 79, "518478": 64, "518610": 95, "518782": 79, "518846": 77, "518854": 64, "519080": 60, "51966955": 54, "519710": 82, "52": [53, 56, 73, 76, 83, 93, 94, 108, 124, 141], "520": 79, "520171": 60, "520415": 67, "520641": 84, "520930": 69, "521002": 69, "521233": 64, "521611": 68, "521632": 67, "521788": 67, "522753": 38, "522835": 59, "523030": 85, "523163": 69, "5232": 75, "52343523e": 108, "523794e": 82, "523807": 81, "523977545": 54, "52424539": 54, "524657": 82, "524934": [67, 68], "5250": 79, "525064": 64, "52510803": 55, "5251546891842586": 85, "5255": 56, "525722": 67, "525804": 60, "52590": [55, 78], "526": 77, "526532": 79, "526582": 74, "526769": [67, 68], "526984": 68, "527226": 67, "52732": 94, "527452": 68, "527540": 67, "528377": 60, "528381e": 86, "528580": 82, "528680": 60, "528763": 68, "528937": [71, 72], "528996901": 54, "528997": 77, "529": 77, "529405": 53, "529468": 93, "529782": 53, "53": [53, 56, 64, 73, 89, 91, 93, 94, 95, 97, 108, 124, 136, 139, 141], "530138e": 60, "530659": 73, "530793": 67, "530940": 82, "53094017": 82, "531": 56, "531223": 69, "531594": 79, "53209683": 95, "532266": 69, "53257": 94, "532738": 82, "53273833": 82, "532751": 71, "5329": 79, "533": 83, "533188": 60, "533283": 95, "533489": 59, "533900": 82, "534139": 64, "5346": 56, "535179": 82, "535318": 82, "535609": 79, "535718e": 79, "53606675": 82, "536067": 82, "536143": 79, "536328": 60, "536746": 82, "536778e": 68, "536798e": [78, 79], "537240": 82, "53724023": 82, "537438": [95, 96], "537641": 60, "53791422": 95, "538": 56, "538013": 79, "538105": 68, "5382": 84, "538937": [78, 79], "539374": 61, "539455": 82, "539475": 82, "53947541": 82, "539491": [71, 72], "5396261": 108, "539767": 69, "54": [53, 55, 56, 58, 83, 88, 93, 94, 108, 124, 140], "540240": 79, "540375": 74, "5404821": 108, "5405": 74, "540549": 74, "5408": 53, "541": 83, "541060": 74, "541159": 82, "5414915": 108, "54163": 84, "5416844": 87, "541684435562712": 87, "541821": 79, "541990": 79, "542": 83, "542136": 67, "542159": 68, "542170": 68, "542268": [95, 96, 99], "542333": 79, "542446": 72, "542451": 82, "542560": 85, "542584": 69, "5425843074324594": 69, "542647": 82, "542648": 93, "542671": 77, "542883": [125, 131], "5428834": [125, 131], "542919": 93, "542989": 82, "543": [77, 79], "543052": 64, "543075": 69, "543136": 69, "543380": 77, "5434231": 87, "543423145188043": 87, "5436005": 54, "543691": 68, "543764": 72, "54378": 84, "543832": 82, "5439141": 108, "544097": 82, "544383": 85, "544555": 77, "544669": 71, "54483": [109, 124], "5448331": [109, 124], "54517706e": 108, "545492": 64, "54550506": 83, "545605e": 82, "545919": 79, "545930": 93, "54604": 80, "546294": 79, "5467606094959261": 69, "546761": 69, "546953": 68, "547039": 67, "54716": 84, "547161": 61, "547324": 68, "547431": 81, "5476": 79, "5479": 79, "547909": 79, "54864780": 108, "548652": 61, "549109e": 79, "55": [55, 56, 69, 78, 79, 82, 93, 94, 108, 124, 141], "5500000000000002": [69, 79, 82], "550242": 76, "550579": 60, "551317": 68, "551586928482123": 69, "551587": 69, "551686": 69, "55176": 94, "5518": 79, "552": 79, "552058": 84, "552508": 79, "552694e": 67, "552727": 77, "552776": 82, "553004": 64, "55307": 94, "553522": 68, "553754": 93, "553878": [35, 93], "553916": 79, "554076": 69, "554283": 61, "554793e": 108, "555": 77, "555026859": 108, "555137": 68, "555150": 79, "555445": 81, "555498": 82, "5555": [52, 66], "555536": 67, "555949e": 79, "555954": 79, "556191": [67, 68], "556792": 82, "557": 51, "557421": 61, "5574dcd4": 56, "557595": 77, "557731": 81, "557999": 77, "558134": [67, 68], "5584": 77, "5585": 77, "55863386": 95, "558655": 69, "5589": 77, "559": 141, "5590": 77, "559144": 69, "559186": 69, "5592": 77, "559394": 82, "559522": 82, "559592e": 67, "559680": 79, "55dc37e31fb1": 56, "55e": 55, "56": [56, 88, 93, 94, 108, 124, 136, 139, 141], "560135": [109, 124], "56018481": 82, "560185": 82, "5602727": 58, "560530": 68, "560689": 53, "560723": 73, "561197": 60, "561348": 68, "5616": 78, "561711": 79, "561785": [95, 97], "561883": 39, "562013": 82, "56223": 84, "562288": 85, "562390": 93, "562452": 76, "562518": 79, "562556": 61, "5625561": 53, "562712": [67, 68], "563067": 83, "563374e": 69, "563503": 82, "563528": 79, "563563": 74, "563673": 79, "56387280e": 108, "56390147e": 108, "564045": 82, "564073": 79, "5641": 79, "564142": 69, "564232": [67, 68], "564451": 68, "564577": 79, "565": 95, "565066": 69, "565373": 67, "566": 85, "566024": 82, "566091": 79, "566388": 67, "567004": 84, "5671023414347636668697879848586878999": 108, "567215": 76, "567343": 79, "567364": 68, "567529": 82, "567695": 67, "567945": [71, 72], "568287": 74, "568841": 60, "568932": [95, 99], "569315e": 68, "569449": 93, "569540": 68, "569590": 76, "56965663": 82, "569657": 82, "569911": 54, "5699994715": 54, "57": [56, 83, 93, 94, 108, 124, 141], "570038": 69, "5700384030890744": 69, "570111": 81, "5702": 79, "570279": 60, "570486": 53, "570562": 53, "570707": 60, "570722": 138, "570936": 67, "571778": 53, "5718": 79, "5722": 78, "572408e": 68, "57245066": 82, "572451": 82, "572991": 68, "573700": 59, "574": 56, "574160": 74, "574332": 60, "5748": 94, "57496671": 54, "575": 8, "575381": 78, "57572422": 84, "575810": 67, "57585824": 84, "57592948e": 108, "57599221": 84, "575e": 83, "576": 56, "5763996": 54, "57643609": 84, "577": 56, "5770": 78, "57715074": 54, "577271": 77, "577273": 67, "5776971": 84, "57775704": 84, "577807": [67, 68], "577813": 67, "577e": 83, "578081": 79, "578307": 82, "578523": 77, "578557": 68, "578846e": 69, "579": 51, "579125": 78, "57914935": 55, "579197": 74, "579213": 85, "579238": 69, "579322e": 78, "579875e": 67, "57e": 55, "58": [9, 55, 78, 85, 93, 94, 108, 124, 140], "5800": 79, "58000": 78, "580303": 61, "5804": 56, "580414": 85, "580751": 78, "580853": 67, "580922": 71, "581655": 79, "581827": 76, "581849": 68, "581990": 60, "582031": 78, "582146": 68, "58241568": 108, "582761": 69, "582991": 78, "583034": 71, "583195": [67, 68], "583201": 68, "5833333": 56, "583534": 82, "583692": 76, "584012": 79, "584057e": 67, "584742": 72, "584849": 69, "584928": 67, "584942e": 77, "5852": 79, "585394": [95, 96], "585426": 108, "585479": 74, "585793": 69, "586362": 82, "5864": 53, "5866": 79, "586677": 61, "586719": 69, "586719493648897": 69, "586794": 67, "5868472": 54, "586921": 76, "587129": 60, "587135": 68, "587292": 79, "587994": 60, "588": 79, "58812": 94, "5882": 78, "588233": 67, "588364": 93, "588854": 67, "589147e": 76, "589248": 84, "589440": 69, "589672": 60, "5897502": 108, "589958": 68, "59": [60, 68, 78, 93, 94, 108, 124], "590320": 59, "5905": 78, "590625": 60, "590736": 82, "590813": 82, "590904": 68, "590911": 69, "590991": 69, "591080": 59, "591411": 71, "591441": 31, "591652": 78, "591678": 78, "591782": 82, "59199423e": 108, "592186": 68, "592681e": 69, "59307502e": 108, "593648": 94, "593981": 93, "594": 8, "594316e": 82, "594979": 60, "595353": 69, "59563003": [95, 99], "596": 79, "596069e": 79, "596270": [71, 72], "5964": 75, "596460": 68, "596758": 67, "597": 55, "597098": 79, "59722638": 108, "597923": 79, "598178": 79, "59854797": 95, "5985730": 55, "59861": 79, "598761e": 68, "599297": [93, 94, 95], "599334": [95, 99], "599537": 60, "5cb31a99b9cc": 56, "5d": [69, 82], "5x_2": 59, "5x_3": 59, "5z_i": 82, "6": [8, 9, 10, 14, 15, 20, 21, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 91, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 131, 136, 138, 139, 140], "60": [54, 57, 58, 69, 79, 80, 82, 86, 93, 94, 108, 124, 139], "600": 77, "6000": 79, "6000000000000002": [69, 79, 82], "600000e": 79, "600195": 67, "600254": 81, "600694": 95, "600776": 64, "601": [51, 55], "601061": 69, "601598": 77, "601783e": 68, "601984": 68, "602079": 72, "602168": 69, "602322": 93, "602386e": 67, "602492": 67, "602587": 82, "602628": 69, "6029": 79, "604016": 79, "604111": 79, "604603": 23, "604825": 79, "604841": [78, 79], "605": 79, "605195": 72, "606034": 82, "606129": 82, "606342": 69, "606759": 79, "6068": 54, "606800": 77, "606954": 69, "607264": 67, "6075": 141, "607505": 60, "607600": 79, "607900e": 68, "608": [70, 83], "608108": 60, "608392": 82, "60857": 53, "608818": 84, "609": 83, "609522": 76, "609575": [95, 97], "61": [60, 64, 93, 94, 108, 124, 140], "610318": 64, "610392": 60, "611": 141, "6110": 79, "611269": 77, "61170069": 83, "611859": 72, "612": 83, "612246": 74, "612792": 79, "613244": 68, "6132764": 108, "6133": 55, "613314": 69, "613408": 82, "613498": 79, "613514": 60, "613574": 73, "613622": 68, "613691": [95, 97], "614": 83, "61404894": 95, "614188": 77, "614678": 79, "615": 64, "615498": 31, "615863": [71, 72], "61602950": 108, "616372": 78, "616617": 68, "61669761": [125, 131], "616698": [125, 131], "616811": 60, "616828": 79, "617": 77, "61728": 93, "617283": 79, "6173": 56, "61771229": 83, "61784": 60, "617877": 82, "618069": 78, "61810738": 55, "618574": 68, "618776": 58, "618881": 68, "619": 83, "619128": 68, "619294": 64, "619351": [67, 68], "619390": [67, 68], "619454": 59, "619613": 78, "619903": 68, "61e": [55, 141], "62": [31, 64, 72, 73, 93, 94, 108, 124], "620156": 82, "620874e": 95, "620995": 86, "621094": [78, 79], "621318": 82, "62131806": 82, "621359": 93, "621490": 82, "6215": 78, "622": [51, 79, 83], "622153": 79, "622272": 64, "6224": 54, "623024": 69, "623173": 67, "624": 77, "6240": 84, "62403053": 58, "6243811": 54, "624535": 94, "624764": 68, "624798": 78, "624818": 68, "624919": 79, "624988": 79, "625": [54, 77], "625159": 73, "6253456": 108, "625477": 82, "625766": 71, "625767": 67, "625891": [71, 72], "626433": 82, "6266": 79, "626633": 68, "627505": [71, 72], "627560": 82, "627564": 69, "627588e": 79, "627982": 80, "628": 83, "628069": 77, "628883": 60, "629346": 79, "629549": 68, "629595": 37, "629740": 67, "629831": 60, "629e": 83, "62e": 108, "63": [54, 64, 77, 93, 94, 108, 124, 139, 140], "630150e": 82, "630584": 60, "630880": 83, "630914": 73, "631083": 68, "63117637": 95, "631333": 82, "6318": [78, 141], "632058": 77, "632372": 60, "63245862e": 108, "632747e": 82, "632958": 81, "6330631": 124, "633433": 77, "634": 83, "63407762": 141, "634078": [78, 141], "634577": 124, "63499": 79, "635": 40, "635000e": [78, 79], "635199": [78, 79], "635768": 67, "63593298": 95, "636048": 95, "636453": 34, "636511": 60, "636575": 69, "637326": 82, "6379": 78, "638264": 82, "638461": 76, "638488": 73, "639": 78, "639135": 77, "63916605": 55, "639345": 79, "639580": 68, "639603": 68, "64": [64, 72, 78, 79, 83, 93, 94, 108, 124, 138], "640": 79, "640900": 79, "641528": 82, "641547": 82, "64154727": 82, "641783": 60, "64197957": 82, "641980": 82, "642": 83, "6420": 79, "642016": 82, "642329": 64, "642648": [95, 96], "64269": 84, "643133": 79, "64340": 84, "643512": 69, "643679": [95, 99], "643752": 82, "643939": 64, "644113": 93, "644371": 68, "644381": 60, "644665": 69, "64476745e": 108, "644799": 59, "644985": 67, "645": 79, "64525651": 108, "64550911": 108, "645583": 64, "64579": 53, "6458": 54, "645800": 77, "646117": 68, "646937": 59, "647002": 79, "647004": 95, "647010": 79, "647196": 59, "64723": 84, "647254e": 67, "647689": 85, "647811": 61, "647864": 93, "647873": 82, "64797": 84, "648": 78, "648355": 67, "648690": 68, "648769": 68, "649": 139, "649158": 82, "649514": 67, "649556": 60, "649738": 67, "65": [64, 69, 73, 79, 82, 83, 93, 94, 108, 124], "650": [70, 95], "6500000000000001": [69, 79, 82], "650000e": 79, "650234": 64, "650810": 79, "650867": 69, "651": 51, "651127": 68, "652071": 79, "6522": 139, "652312": 71, "652324": 64, "652349": 82, "652350": 77, "652450e": [78, 79], "6527": 70, "652778": 77, "6528": 79, "6530": 79, "653008e": 78, "653829": 74, "653846": 69, "653901": [67, 68], "654070e": 95, "654755": 59, "655284": 82, "6553": 141, "6554": 139, "655422": 79, "655547": 67, "65557405e": 108, "655959": 74, "657": 56, "657120": 60, "658": 77, "658021": [95, 99], "658267": 82, "658592": 68, "6586": 53, "658702": 68, "658984585": 108, "659": 56, "659245": [67, 68], "659339": 68, "659387": 61, "6593871": 53, "659423": [67, 68], "659473": 85, "659636": 69, "659735": 67, "659755": 83, "6598": 75, "659835": 68, "66": [64, 74, 75, 80, 93, 94, 108, 124, 138, 140], "660": [56, 95], "660073": 68, "660320": 72, "660479": [95, 97], "6607402": 83, "660776": 82, "66133": 95, "661369": 81, "661388": 67, "66204822": 61, "66207717": 61, "66243302": 61, "6625": 79, "662975": 76, "663081975281988": 69, "663082": 69, "663177": 64, "663182": 69, "6634357241067617": 85, "663529": 82, "663533": 79, "663672": 74, "663765": 68, "664103e": 79, "664147": 79, "664409": 68, "664797": 67, "664824": 79, "664850": 77, "665264": 82, "6653523": 108, "665653": [95, 99], "66601815": [95, 97], "666104": 82, "666307": 59, "6666667": 56, "666742": 93, "666959e": 74, "667": 77, "667274": 73, "667492e": 79, "667536": 82, "667614": 69, "667614205604159": 69, "667981": 67, "667985": 73, "668337": 79, "668452": 73, "668584": 59, "668981": 71, "669579": 68, "66989604": 58, "67": [51, 56, 78, 80, 85, 93, 94, 108, 124, 138], "670867": [36, 93], "671224": 68, "671271": [67, 68], "67136": 79, "67161113": 60, "6716717587835648": 69, "671672": 69, "671690": 67, "6722": 56, "672234": [67, 68], "672368": 69, "6723684718264447": 69, "672384": [67, 68], "67245350": 54, "672511": 67, "673092": [67, 68], "673302": 77, "673330": 68, "674": 80, "67410934": 54, "6745349414": 54, "674552": 79, "67456": 85, "674609": 69, "674747": 76, "674949e": 84, "675233": 68, "675293": 81, "675625": 93, "675653": [95, 96], "675775": 76, "6761885": 108, "676405": 69, "6765": [55, 78], "676534": 124, "67656674": 60, "676641": 67, "676756": 82, "676807": 78, "676838": 60, "677123": 67, "6771264": 108, "677614": 82, "677980": 69, "678": 83, "678117": 79, "67838213": 60, "678826": 69, "67936506": [95, 97], "679539": 77, "67956284": 61, "679744": 60, "679789e": 67, "67980345": 60, "67983069": 61, "67ad635a": 56, "68": [56, 64, 84, 93, 94, 108, 124], "680": 79, "68001716": 61, "68066756": 60, "6810775": 84, "681176": 77, "681246": 68, "681448": 79, "681521": 67, "681562": 79, "681817dcfcda": 56, "682": 95, "682122": 76, "682269": 79, "6826": 78, "682875": 69, "683487": 68, "683581": 95, "683637e": 74, "683687": 68, "683942": 82, "683984": 38, "684": 141, "68410364": 55, "68411700": [55, 141], "684128": 68, "684142": 67, "684502": 82, "685104": 20, "685107": 82, "68554404e": 108, "68562150e": 108, "685807": 82, "685989": 95, "686270": 68, "686627": 67, "686641": 60, "687345": 82, "687612": 68, "687647": 82, "687854": 59, "687871": 77, "6878711": 54, "688": 139, "688540": 93, "688641": 76, "688747": 79, "688886": 93, "688918": 79, "688956": 67, "689": 141, "689088": [67, 68], "689188": 59, "689392": 82, "689600": 76, "689932": 67, "69": [73, 93, 94, 108, 124, 140], "690334": 69, "6903344145051182": 69, "69068634": 61, "690760": 61, "69080063": 61, "691097": 67, "691157": 58, "69135757": 61, "69140475e": 108, "691423": 67, "691511": 78, "691788e": 60, "691848e": 68, "691911": 93, "692297": 68, "692460": 76, "692579": 68, "692725": 82, "692907": 79, "692959": 67, "693316": 79, "693497e": 79, "693690": 79, "693796": 77, "694154": 69, "694561": 83, "694845e": 79, "694919": 77, "6950": 79, "695045": 67, "69508862": 95, "695581": 73, "69562150e": 108, "695711": 76, "69572427": [95, 99], "695928": 67, "696011": [35, 93], "696224": 93, "696289": [71, 72], "69684828": 95, "696966": 68, "697": 77, "697000": 69, "697286": 60, "697420": [71, 72], "697545": 82, "697616": 68, "697693": 67, "698223": 59, "698244": 59, "69840389e": 108, "698508": 60, "698509": 67, "698642": [95, 99], "698694": 77, "698751": 76, "698913": 60, "699035": 82, "699082": 69, "69921": 56, "699259e": 82, "699286": 60, "699333": 69, "699616": 76, "699697": 68, "6_design_1a": 70, "6_r2d_0": 70, "6_r2y_0": 70, "6b": 124, "6cea": 56, "7": [10, 14, 17, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 131, 138, 139, 140], "70": [55, 57, 69, 71, 78, 79, 82, 93, 94, 108, 124, 140], "700": [67, 68, 70, 77], "7000000000000002": [69, 79, 82], "700015": 82, "700102": 82, "700314": 64, "700458": 67, "70075582": 60, "701078": 82, "701088": 78, "701106": 73, "701265": 71, "701413": 79, "701672e": 69, "701841e": 72, "701866": 82, "7018663": 82, "701966": 79, "702307": 60, "702489": 79, "703049": 67, "70305686": [95, 99], "703108e": 39, "703239": 60, "703325": 74, "70344386": [95, 99], "70375981": 60, "703772": 79, "703880": 60, "70390324": 60, "7040": 79, "704482": 74, "7045": 74, "704558": 74, "704814": 67, "704896": 74, "705090": 68, "705354": 67, "70557077": [95, 99], "705581": 79, "7055958": 87, "705595810371231": 87, "7055958103712310": 87, "705794": 68, "70583": 84, "70596816": 60, "706056": 79, "706077": 68, "706122": 68, "706430": 67, "706645": 69, "706657": 69, "706862": 23, "707125": 68, "707441": 68, "707523": 60, "707738": 68, "70774361": [95, 99], "707868": 82, "707963e": 78, "708190": 77, "708235": 67, "708459": 82, "708472": 68, "708821": 64, "708837": 64, "709026": 59, "709596": 67, "709606": [36, 93], "71": [93, 94, 108, 124, 140], "710059": 64, "710319": 68, "7104": 60, "710515": 67, "710586e": 77, "711024": 79, "711328": 79, "711383e": 67, "711518": 79, "711638": 95, "712064": 67, "712082": 79, "712095": 64, "712157": 81, "712268": 67, "712372e": 68, "712503": 84, "712592": 78, "712774": 71, "712846": 93, "712960": 69, "713": 79, "71322008": 60, "713407": 79, "713457": 67, "71381530": 108, "713986": 79, "713993": 68, "714240": 77, "714250": 68, "714321": 78, "714534e": 68, "714651": 82, "71465114": 82, "715013": 79, "715180e": 79, "7154": 79, "715407": 69, "7155": 79, "7158581": 54, "716013e": 67, "716098": 64, "7161": 79, "716387": 67, "716427e": 68, "716456": 82, "716595e": 79, "716615": 64, "716762": 69, "716793": 69, "716799": 77, "7167991": 54, "716801": 76, "717": 79, "717130": 79, "717185": 82, "717831": 60, "718686": 85, "719552": 68, "72": [57, 74, 93, 94, 108, 124, 140], "7204309": [95, 99], "720559": 67, "720571": 82, "720573": 67, "720589": 83, "720664": 77, "721018": 67, "721071": 82, "721245": 68, "7215093d9089": 56, "72155839e": 108, "721609": 79, "72185641": 60, "722316": 82, "722634": 82, "72263499": 60, "72269685": [95, 99], "722848": 69, "722881": 82, "7229": 79, "722983": 60, "723": 56, "723314": 82, "723342": 93, "723345e": 82, "723657": 67, "723846": 64, "7239": 79, "7241399": 54, "724338": 82, "724767": [71, 72], "724918": 85, "725": 56, "725010": 64, "725061": 67, "725087": 79, "725166": 82, "725565": 67, "725802": 23, "725820": 76, "725919": 67, "726": [56, 83], "72613435": 60, "72646837": 60, "726658": 76, "7268131": 54, "72690400": 108, "727159e": 68, "727543": 59, "727693": 79, "727704": 79, "727976": 69, "72802973": 60, "7282094": [95, 97], "728294": 81, "728710": 82, "72875815e": 108, "728852": 79, "728e": 83, "729485": 60, "729668": 93, "729867": 67, "72987186": [95, 99], "73": [55, 64, 93, 94, 108, 124], "730023": 79, "7308": 53, "730809": 67, "731174": 67, "731317": 69, "732": 83, "732067": 67, "732137": 67, "732150": 68, "7326": 79, "732638": 82, "73285": 34, "732918": 71, "733": 79, "733047": 68, "7332": 60, "733644": 67, "734635": 67, "734747": 60, "734770": 68, "734948": 82, "735369e": 93, "7357": 79, "735848": 93, "735941": 33, "735964": 59, "736082": [67, 68], "736084": 82, "73608412": 82, "736823": 68, "737052": 79, "7375615": 55, "73764317e": 108, "737951": [67, 68], "738": [51, 78], "738065": 68, "738223": 79, "738315": 79, "738534": 61, "738659e": 79, "738876": 68, "739": 79, "739063": 67, "739089": [95, 99], "7395359436844482": 69, "739536": 69, "739595": 83, "739720": 79, "739817": 73, "74": [9, 55, 68, 78, 93, 94, 108, 124, 140], "740": 77, "740180e": 82, "740367": 67, "740417": 78, "740505": 64, "740785": 67, "740869": 69, "741104": 69, "741380": 83, "741523": 64, "741702": 82, "7418": 53, "74189": 56, "742": 108, "742128": 82, "742375": 67, "742407": 81, "742411": 67, "742677": 60, "742907": 82, "742976": 60, "7432": 53, "743247": 79, "743341": 68, "743467": 61, "743609": 67, "7437": 79, "74402577": 82, "744026": 82, "744236": 84, "74461783e": 108, "744689": 60, "74475816": [95, 99], "745": 79, "745022": 64, "745444": 67, "745638": 78, "745881": 67, "745982": 60, "746105": 80, "746361": 82, "746843": 72, "7470": 79, "747646": 79, "747945": 54, "747961": 79, "748084": 68, "748377": 78, "748513": 79, "748880": 79, "748942": 61, "749382": 60, "74938952": [95, 97], "749443": 79, "749854893": 109, "75": [9, 26, 36, 56, 59, 64, 69, 74, 78, 79, 82, 93, 94, 108, 124, 140], "75000": 85, "7500000000000002": [69, 79, 82], "750000e": 79, "750597": 68, "750701": 64, "751013": 79, "751261": 79, "751633": 79, "75171": 78, "751710": [69, 78], "751712655588833": 87, "7517126555888330": 87, "751712656": 87, "752015": 32, "752283": 79, "752696": 64, "752909": 74, "7533": 78, "753323": 67, "753393": 67, "753523": 82, "753866": 68, "754469": 67, "754499": 68, "754678": 76, "754692": 93, "7548": 85, "754870": 77, "755154": 60, "755688": 67, "755701e": 67, "755885": 93, "755910": 79, "7559417564883749": 69, "755942": 69, "756": 80, "7560824": 54, "756200": 64, "756805": 77, "756867e": 79, "756905": 23, "756934": 60, "756969": 69, "757": [83, 139], "757151": [67, 68], "757183": 69, "757411": 82, "757559": 74, "757819": 77, "757917e": 82, "758": 51, "758391": 79, "758831": 68, "75887": 56, "759006": 58, "759054": 68, "759833": 68, "76": [93, 94, 108, 124, 139, 140], "760104": 82, "7603": 53, "760386": [95, 97], "760778": 77, "760915": 59, "761": [54, 77], "761224": 64, "761429": 68, "761714": 69, "762284": 82, "76228406": 82, "762299": [95, 99], "762748": 79, "763691": 79, "764": 141, "764093": [67, 68], "76419024e": 108, "764315": 82, "76444177e": 108, "764478": 81, "7646": 79, "764798": 82, "764953": 78, "765202": 79, "76535102": [95, 99], "765363": [67, 68], "765500e": [78, 79], "765580": 60, "765710e": 86, "765792": 82, "765864": 84, "76591188": 54, "765960": 67, "7660": 53, "7663": 79, "766499": 82, "766832": 61, "766940": 64, "76702611e": 108, "767119": 60, "767188": [71, 72], "767435": 85, "767549": 68, "768071": 82, "768273": [71, 72], "768763": 68, "768798": 64, "7692": 60, "769361": 82, "76950978": 108, "769805": 82, "77": [83, 93, 94, 108, 124], "770556": 79, "770944": [71, 72], "7710": 84, "771157": 124, "771390e": 79, "7714": 80, "7716982": 55, "771741": 79, "771965": 79, "771966": 80, "772104": 67, "772157": 74, "77227783e": 108, "772396": 68, "772444": 93, "772791": 79, "77289874e": 108, "773": 56, "773177": 69, "773339": 74, "773488": 82, "77348822": 82, "773632": 60, "773769": 76, "77401500e": 108, "774271e": 79, "775": [56, 79], "775191": [67, 68], "775285": 67, "775969": 84, "776254e": 67, "7763": 78, "776728e": 77, "776887": 78, "77746575": [95, 99], "7776071": 54, "777718": 76, "777728": 93, "777867": 74, "777e": 83, "778400": 67, "7786": 53, "778657": 61, "779": [51, 83], "779068": 74, "779108": 67, "779167": 31, "779517": [67, 68], "779682": 69, "7797047": 60, "7799": 75, "779912": 79, "78": [83, 93, 94, 108, 124, 140], "780": 56, "780068": 76, "780338": 67, "780458": 82, "780856": 78, "781": 79, "781233": 79, "781530": 82, "781681": 82, "782": 56, "782050": 82, "782555": 79, "783": 56, "783276": 95, "7833": 53, "7838": 53, "78386025": [95, 99], "784": 124, "784238": 77, "784405": 84, "784483": 77, "784624": 69, "784792": 76, "784872": 64, "785": 56, "785038": 68, "785153": 68, "785815": 64, "785900": 60, "785911": 82, "785e": 40, "786": [56, 80], "786090": 76, "786237": 67, "786563": 76, "786744": 69, "786986": 64, "78711285e": 108, "7872": 60, "78777": 84, "7879170": 108, "788": [108, 139], "788023": 60, "78818": 56, "788868": 68, "789032": 67, "789039": 68, "789330": 68, "789671": 69, "789671060840732": 69, "79": [64, 93, 94, 108, 140], "790039e": 67, "790115": 79, "790261": 93, "790723": [71, 72], "791097": 78, "791241": 82, "791297": [36, 93], "791834": 61, "792396": 64, "792939": 69, "792972": 93, "79330022": [95, 99], "793315": 93, "79338596e": 108, "793570": 82, "793598": 68, "793735": 82, "793818": [67, 68], "794": 95, "794366": 79, "79458848e": 108, "794805": 71, "795": 83, "795647": 82, "7957": 79, "795932": 94, "796": 108, "796014": 68, "796155": 61, "796384": 68, "796444": 79, "796596e": 67, "796e": 83, "797086": 67, "797189": [95, 99], "797280": 82, "797299": 80, "797454": 95, "797737": 124, "797827": 80, "797868": 68, "79792890e": 108, "797965": 124, "798071": 20, "798308": 78, "798783": [71, 72], "799403": 82, "79953099": [95, 99], "799713": 60, "7999": 86, "7b428990": 56, "7x": 82, "8": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 66, 67, 68, 69, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 91, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 131, 138, 140, 141], "80": [57, 58, 69, 79, 82, 86, 93, 94, 108, 140], "800": 77, "8000": [19, 57, 86], "8000000000000002": [69, 79, 82], "800143": 67, "800281": 60, "800326e": 67, "800351": 67, "801623": 79, "80190628": 108, "802": 83, "802201": 60, "802289": 79, "802738": 93, "803112": 76, "803300": 67, "803492e": 82, "803563": 79, "803902e": 79, "804": 79, "804219": 82, "804284": 84, "804316": 82, "804484": 82, "8048": 55, "804828": 82, "804889": 79, "805": 51, "805007": 77, "805153e": [78, 79], "805220": 61, "805293": 68, "8055563": 54, "805774": 67, "8059": 78, "806218e": 79, "806531": 79, "806554": 67, "806691": 80, "806732": 73, "80696592e": 108, "80714504e": 108, "807879": 82, "808": [55, 124], "808246": 78, "808284": 79, "808640": 79, "809125": 67, "8095": 80, "809913": [67, 68], "80a8": 56, "81": [54, 67, 70, 73, 75, 93, 94, 108, 140], "810044": 78, "810134": 82, "8102": [53, 78], "810306": 64, "810322": 67, "810363": 79, "810382": [78, 79], "810419": 68, "810707": 79, "810895": 68, "811011": 68, "811155": 73, "811162021242729303656575961657172738083": 108, "811458": 78, "811513": 68, "8116912": 124, "811696": 67, "811795": 60, "811825": 77, "811901": 82, "81190107": 82, "812": 83, "812028": [95, 96], "8132463": 54, "813293": 82, "813342": 124, "813682": 79, "8141289": 108, "814136": 69, "814246e": 68, "814351": 69, "814913": 77, "8152": 79, "815213e": 68, "815224": 124, "815226": [95, 97], "81568484": 82, "815685": 82, "815993": 82, "816014": 60, "816176": 85, "816318": 77, "816373": 67, "816752": 79, "816982": 67, "817119": 67, "817291": 79, "8173602": 75, "817967": 95, "81827267": 82, "818273": 82, "818289": 82, "81828926": 82, "818380": [67, 68], "81856": 56, "819223": 93, "819507": 76, "82": [85, 93, 94, 108, 140], "8202": 55, "820366": 77, "820801": 80, "8209": 55, "820963": 64, "821": 139, "8210": 55, "821021": 69, "821457": 79, "821566": 82, "821855": 93, "821970": 76, "821995": 68, "8221": 53, "822244": 61, "822289": [78, 141], "82228913": 141, "822482": 69, "8227": 79, "8227583": 108, "822822": 69, "823247": 82, "823273": [67, 68], "824350": [67, 68], "824657": 76, "824701": 69, "824750": 69, "824800": 60, "824889": 69, "824961e": 79, "8250": 53, "825587": 68, "825617": 77, "825862": 82, "825980": 69, "8259803249536914": 69, "826": 51, "8260": 78, "826065": [67, 68], "826426": [95, 97], "826467e": 67, "826492": 82, "826519": [36, 93], "826554": 60, "82666866e": 108, "82684324": 84, "827375": 58, "827381": 82, "827735": 82, "827938162750831": [71, 72], "828058": 79, "828157": 64, "828618": 74, "828778e": 67, "828912": 67, "828915": [71, 72], "829543": 69, "829730e": 68, "829764": 93, "82985": 73, "83": [93, 94, 108, 140], "830263": 76, "830273": 64, "8302804": 108, "830301": 81, "830442": 67, "830467": 67, "830641": 61, "831": 83, "831019": 69, "831190": 68, "831278": 67, "831741": 67, "831833": [95, 99], "832086": 82, "8326928": 84, "832693": 84, "832875": 82, "83287529": 82, "833024": 77, "833065": 64, "833227e": 94, "833464": 79, "833907": 77, "834133": 74, "835": 83, "8350": 79, "835035": 76, "835239": [95, 99], "835344": 64, "835596": 79, "835750": 74, "835935": 68, "836234": 95, "838114": 82, "838235": 80, "838457": 79, "83905": 20, "839235": 60, "84": [56, 73, 83, 93, 94, 95, 108, 140], "840041": 79, "840303": 82, "84030318": 82, "840673": 67, "840718": [95, 97], "840836": 82, "840995e": 78, "841": [54, 77], "841132": 78, "8415": 55, "841847": 79, "842132": 95, "842405": 69, "842625": 77, "842746": 82, "8428": 78, "842853": 82, "843018": 93, "843730": 77, "843734e": 60, "843796": 67, "8440": 79, "844107e": 68, "844304": 80, "844308": 82, "844549": [71, 72], "844663": 64, "844667": 124, "844707": 82, "844889": 77, "845241": 83, "845534": 76, "846351": 80, "846388": 69, "847029": 68, "847555": 67, "847595": [35, 93], "847653": 60, "847948": 69, "847962": 67, "847966": 79, "848193": 60, "848688e": 76, "848757e": 78, "848868": 69, "84930915e": 108, "849427": 93, "849747": 84, "8497f641": 56, "8499": 79, "85": [13, 69, 73, 79, 82, 86, 93, 94, 108], "8500000000000002": [69, 79, 82], "850038": 64, "850321": 77, "850439": 68, "850575": [67, 68], "850656": 74, "850794": 82, "851": 139, "851198": 79, "8513": 56, "851366": 77, "851375": 61, "8516769342": 108, "852": 79, "85234351": 60, "85265193": 75, "85280376": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "85397773": [95, 97], "855035": 67, "855780": 82, "855862": 68, "856404": 72, "856758": 93, "8571": 53, "857161": 82, "857515": 93, "857544": 77, "857765": 79, "858212e": 68, "858386": 61, "858952": 64, "859": 79, "85911521e": 108, "85912862": 124, "859129": [109, 124], "8597": 78, "85974356": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "85c5": 56, "85e": 55, "86": [93, 94, 108, 140], "860663": 124, "860804": 82, "860992": 79, "861019": 64, "861210": [95, 99], "86131835": 108, "861519": 67, "862043": [71, 72], "862359": 69, "86247065": 60, "863772": 78, "863955": 60, "863982270": 109, "864": 83, "86415573": 55, "86424193e": 108, "8644": 56, "864523": 60, "864664": 64, "864741e": 79, "865313": 79, "865562": [67, 68], "865854": 79, "865860": [78, 79], "865914": 68, "866102": [67, 68], "866179899731091": 87, "866179900": 87, "866579": 79, "866798": 79, "86703441": 60, "867201": 76, "867565": 82, "8679": 79, "868": 56, "8685788": 82, "868579": 82, "86883342": 60, "86897905": [95, 99], "869": [56, 83], "869020": 69, "869195": 68, "869398": 68, "869477": 67, "869585": 39, "869586": 73, "87": [55, 67, 73, 77, 93, 94, 108, 140], "870": 61, "8700": 55, "870099": [71, 72], "870260": 82, "870332": 82, "870444": 78, "870857": 82, "871": 56, "871317": 61, "871545e": 67, "871923": 68, "871972": 74, "872": 83, "872126": 80, "872132": 68, "872222": 79, "872727": 67, "872768": 82, "872852": 82, "87290240e": 108, "872994": 79, "87309461": [95, 99], "873198": 79, "873677": [71, 72], "873848": 61, "87384812361": 53, "87384812362": 53, "87430335": 124, "874303353": 124, "874702": [71, 72], "8750": 79, "8759": 79, "876": 83, "876083": 79, "876233": 61, "87623301": 53, "876431e": 69, "876549": 79, "87674597e": 108, "8768": 53, "8771": 79, "87714619": 60, "877153": 79, "877455": 81, "877833": [67, 68], "877903": 64, "878086": 60, "878281": 82, "878289": 79, "878402": 67, "878746": 64, "878847e": 79, "878895": 64, "878968e": 67, "879": 95, "879049": 79, "879058": 76, "879103": 69, "879509": 67, "879845": 61, "87e": 55, "88": [55, 73, 83, 93, 108], "880": 80, "880106": 77, "880579": 82, "880591": 81, "880808e": 79, "880880e": 79, "880886": 78, "8810": 78, "881201": 79, "88125046e": 108, "881465": 59, "881581": 33, "88173062": 54, "881937": 64, "882307": 60, "882475": 69, "882641": 78, "882928": 64, "883485": 68, "883622": 82, "88390139": 60, "883914": 69, "88393996": 60, "883953": 76, "884132": 82, "8843": 84, "8845": 53, "884769": 60, "884821": 93, "884996": 69, "8850": 55, "885065": 82, "885207": 60, "885832": 83, "885956": 67, "885978": [71, 72], "886041": 68, "886086": [67, 68], "8862353": 108, "886266": 79, "88629": 53, "886314": 68, "88664": 56, "887182": [95, 96], "887345": 79, "887556": 69, "887648": 68, "887680": 67, "888146": 77, "8881461": 54, "888352": 64, "888445": 68, "888775": 72, "888804": 79, "889293": 82, "889326": 68, "889638": 64, "889733": 82, "889792": 68, "88988263e": 108, "889913": [67, 68], "889963": 82, "88ad": 56, "89": [55, 68, 93, 108, 139, 140], "890": [54, 77], "890204": [95, 99], "89027368": 124, "890273683": 124, "890318": 67, "89035917": 73, "890372": [62, 89, 91, 138], "8903720000100010000010": [56, 89, 91, 138], "8904": 51, "890454": 94, "890665": 74, "890855": 64, "8909": [54, 78, 141], "891527": [95, 99], "891606": 78, "891752": 68, "891997": 67, "892": 56, "89214907": 60, "892648": 82, "89273": 95, "892796": [67, 68], "892828": 74, "893": 56, "8932105": 54, "893461": 64, "893649": [67, 68], "893851": 82, "894": 56, "894307e": 79, "894448": 68, "8946549": 57, "89472978": [95, 99], "895106": [67, 68], "895280": 60, "895308": 79, "895333": 82, "895442": 78, "895690": [67, 68], "895768e": 69, "896023": 82, "896182e": 74, "896263": 74, "896647": 60, "897220": 82, "897240": 79, "897261": 61, "8974": 78, "897451": 67, "897495e": 68, "898183": 64, "89861168": 60, "898722": 82, "899021": 93, "899250": 64, "899460": 82, "899654": 64, "899662e": 67, "899716": 68, "8bdee1a1d83d": 56, "8da924c": 56, "8e3aa840": 56, "9": [20, 22, 23, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 89, 91, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 131, 137, 138, 140, 141], "90": [15, 55, 57, 58, 69, 79, 82, 86, 93, 108, 140], "9000000000000002": [69, 79, 82], "900000e": 79, "900021": 94, "900118": 60, "900127": [95, 99], "900829": 74, "901013": 68, "901148": 82, "90136": 78, "901360": 78, "90145324": [95, 99], "901526": 73, "901683": 79, "901705": 67, "902": 124, "902573": 69, "9028925": 60, "902920": 93, "90302166": 60, "903056e": 82, "90313274": 60, "903339": 69, "903351e": 69, "903418": 77, "903674": 67, "903681": 82, "903767": [67, 68], "904156": 69, "9041560442482157": 69, "904315": 67, "904396": 67, "905042": 68, "905494": 69, "905858": 85, "905951": 84, "906072": 93, "9061": 79, "906716732639898": [71, 72], "906757": 65, "907115": 82, "907176": 82, "9073": 79, "907491": 69, "9077462": 108, "907801": 77, "907879": 64, "90794478": 124, "907944783": 124, "907961": 79, "908024": 85, "908663": 68, "908767": 76, "909304": [67, 68], "909571": 64, "90963122e": 108, "909942e": 93, "909975": 79, "909997": [78, 141], "91": [93, 108, 140], "910000e": 79, "91001138": 60, "910895": 68, "9109": 56, "91102953": 82, "911030": 82, "9112": 78, "911277": 68, "911662": 71, "912230": [67, 68], "9126": [55, 141], "9127": [55, 141], "912903": 67, "913": [56, 141], "91315015": 54, "913164": 61, "913280": 85, "913371": 68, "913415e": 67, "913485": 79, "913585": 74, "913774": 69, "914": 51, "9142": 79, "91438767e": 108, "9145": 53, "914598": 64, "915": [55, 56, 78, 79], "915000e": [78, 79], "91503053": 108, "915260e": 67, "915488": [71, 72], "9158080176561963": 76, "916236": 53, "916359": 64, "916528": 71, "9166667": 56, "916914": 82, "916930": 67, "917": 56, "917000": 68, "917066": 79, "917248": 82, "91724807": 82, "917436": 82, "918": 83, "918227": 69, "918293": [95, 99], "919432": 82, "9197": 79, "919969": 67, "91e": 55, "92": [60, 93, 94, 108, 140], "920052": 68, "920335": 79, "920337": 72, "920645": 79, "9209": 53, "9210": 79, "921061": 85, "921198": 74, "921256e": 68, "921347": 60, "921372": 69, "921581": 61, "921613": 60, "921778": 74, "921913": 77, "921956": [67, 68], "921e4f0d": 56, "922160": 79, "922251": 67, "9223": 79, "9225282": 108, "922668": 74, "922996": 77, "923074e": 69, "9234022": 108, "923517": 86, "923607": 82, "92369755": 54, "923804": 69, "923943": 141, "923977": 79, "924002": 82, "9243": 79, "924386": 61, "924396": [71, 72], "924443": 64, "924634": 59, "9248": 56, "924821": 69, "924843": 77, "924921": 93, "925": 58, "925248": [71, 72], "925336": 61, "925660": 67, "925736": 69, "925957": 71, "925994": 78, "925995": 68, "926227": 68, "926621": 69, "926901": 76, "927": [52, 80], "927074": 82, "927232": 79, "9274": 79, "927950": 79, "92811018": 60, "92827999": 95, "92881435e": 108, "928947": 77, "92905": 54, "929643": 67, "929690": 61, "92972925e": 124, "929729e": [109, 124], "93": [55, 74, 93, 94, 108, 140], "9304028": 54, "93074943": [95, 99], "931": 88, "931479": 82, "931978": 138, "932027": 69, "932404e": 79, "9325": 53, "9327": 53, "932973": 82, "933": 80, "93300": 95, "933210": 61, "933322": 68, "933671": 68, "933841": 61, "933857": 68, "933996": 69, "934058": 67, "934243": 68, "934433": [67, 68], "9345": 56, "934500": 68, "934511": 124, "934549": 79, "93458": 84, "934963": 68, "934992": 69, "935": [40, 75, 95], "935220": [95, 96], "935591": 82, "935730": 82, "935764": 68, "935989": 77, "9359891": 54, "93648": 86, "936494": 67, "936739": 82, "937116": 77, "937586": 79, "938": 124, "938263": [95, 99], "938836": [95, 99], "939068": [71, 72], "9392": 79, "939250": 67, "939458": 67, "9395": 79, "93958082416": 141, "94": [58, 75, 93, 108, 140, 141], "940354721701296": 69, "940355": 69, "940373": 79, "941440": 67, "941724": 79, "941788": 71, "942139": 72, "942312": 82, "942460e": 82, "942489": 79, "9425": 53, "942550": 79, "942661": 77, "942823": 79, "94309994e": 108, "943548": 74, "943693": [95, 96], "943938": 82, "943949e": 82, "944253e": 82, "944266": [71, 72], "94427158": [95, 99], "944280": 79, "94441007e": 108, "94473": 57, "945881": 67, "946180": 74, "94629": 86, "946297": 69, "946406": 72, "946433": 82, "946533": 67, "946658": 79, "94667221": 60, "946968": 69, "947": 51, "947440": 81, "947466": 94, "947613": 68, "947855": 64, "9480": 79, "948112": 83, "948154e": 71, "94840552": 60, "94860613": 60, "948785e": 67, "948868": 79, "948975": 73, "94906344": 54, "949241": 124, "949456": 82, "949866": 68, "95": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 53, 55, 57, 58, 59, 60, 61, 64, 67, 68, 69, 70, 71, 72, 73, 74, 75, 78, 79, 82, 83, 84, 85, 86, 93, 95, 108, 124, 125, 131, 140, 141], "9500": 79, "950158": 67, "950545": 65, "95062986e": 108, "9507364": 60, "951502": 82, "951532": 77, "951920": 81, "952": [55, 83, 141], "952146": [95, 99], "95224339": 108, "9523": 53, "95262806": 60, "952839": 82, "95305": 57, "95311164": [95, 99], "9534": 79, "953641": 61, "953683": 77, "953704": 67, "95372559e": 108, "953884": 95, "954": 124, "95401167e": 108, "954156": 61, "954391": 61, "955005e": 79, "9551": 79, "9552": 53, "955541": [36, 93], "95559917": 94, "955701": 67, "955e": 95, "956047": 54, "9561": 53, "956115": 60, "956574": 79, "956588": 95, "956724": 69, "9567242535070148": 69, "956877": 68, "956892": 79, "957218": 60, "957229": 72, "957375": 77, "957745": 69, "9579": 55, "957996": 69, "958": 124, "9580": 55, "958105": 93, "958541": 79, "958891": 60, "959132": 68, "959384": 68, "95948692": 60, "959613": 93, "95e": 55, "96": [55, 67, 68, 80, 93, 108, 140], "960236": 95, "9605": 79, "960808": 69, "960834": 68, "9609": 53, "96113075": 60, "961539": 79, "961962": 69, "962364": 64, "962373": 68, "962408": 60, "962954": 68, "963055": 79, "96309975": 60, "963427e": 68, "963437": 80, "963465": 60, "964025e": 82, "964065e": 68, "964261e": 77, "964318": 79, "9647": 53, "965341": 68, "965531": 95, "965696": 67, "965774": 79, "96582": 94, "966015": 82, "966097": 37, "966659": 69, "9666592590622916": 69, "96679584": 60, "967092": 68, "96745993": 60, "967467": 84, "968127": 64, "968134e": 82, "968232": 60, "968258e": 67, "968577": 58, "969141": [93, 94, 95], "9699": 78, "969925e": 68, "97": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 58, 59, 60, 61, 62, 64, 65, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 93, 94, 95, 96, 97, 99, 108, 109, 124, 138, 140, 141], "970065": 82, "970150": 68, "971058": [71, 72], "971083": 61, "972509": 68, "972745": 67, "972748": 69, "97276281": 82, "972763": 82, "97314470": 54, "973156": 93, "973229": 68, "973241": 82, "973259": 60, "973331": 79, "973741": 68, "973890": 68, "974202": 69, "974213": 68, "97441062": [71, 72], "974414": 69, "974487": 67, "97470872": 84, "974801": 60, "9748910611": 54, "975": [67, 68, 71, 72, 74, 75, 80], "975289": 64, "9753": 56, "975447": 58, "975450": 68, "975461": 77, "975592": 64, "975942": 60, "976061": 60, "976088": 82, "97619643": [95, 99], "976548e": 68, "976562": 82, "977202": 68, "977280": [67, 68], "977295": 79, "977507": 68, "977820": 67, "978303": 76, "978309": 61, "9787": 79, "978977": 82, "979": 83, "97913064": 108, "979384": 74, "979475": 67, "979702": 67, "979857": 67, "979966": 74, "979971e": 67, "98": [67, 68, 79, 93, 108, 140], "980026": 79, "9802393": 54, "980440": 68, "980643e": 69, "981104": 81, "981403": 68, "98143076": 60, "981438": 67, "981672": 69, "981715": 67, "982019e": 68, "982353e": 79, "982417": 69, "982720": 67, "982797": 81, "983192": 82, "983253": 67, "983759": 141, "98393441": 84, "98399144": 60, "984024": 81, "984083": [71, 72], "984551": 32, "984562": 82, "984866": 124, "984872": [67, 68], "984937": 69, "984952": 61, "985": 51, "98505871e": 108, "985207": [67, 68], "985654": 68, "986": 80, "986249": 78, "986383": 79, "986417": 67, "98673": 73, "987": 80, "9870004": 56, "987102936": 108, "987220": 79, "987307": 76, "9875": 53, "987726": 68, "9880384": 56, "988421": [67, 68], "988463": 82, "988541": 67, "988709": 79, "988780": 79, "989": 80, "989003": 61, "989152": 80, "99": [55, 64, 67, 68, 80, 93, 108, 140], "990210": 79, "990292": 80, "990385": 61, "990903": 67, "991": 56, "9914": [78, 79, 84], "99140823": 60, "991444e": 71, "9915": [55, 78, 79, 84], "991512": 55, "991845": 60, "991963": [67, 68], "991977": 79, "991988": 67, "992": 83, "99232145": 84, "992582": [67, 68], "992890": 60, "993011": 60, "993201": 68, "993282": 61, "993416": 74, "993575": 79, "994": 83, "994168239": 54, "994208": 64, "994214": 79, "994332": 65, "994377": 67, "9944": [75, 95, 97], "9948104": 57, "994851": 79, "994937": 72, "995015": 79, "9951": 53, "995248": 82, "99549118e": 108, "99571372e": 108, "9961": 78, "9961392": 54, "996313": 67, "996892": 76, "996934": 77, "997": 80, "9970": 79, "997034": 86, "997494": 86, "997571": 77, "997621": 69, "997934": [71, 72], "998": 80, "998063": 65, "99864670889": 141, "998766": 79, "998808": 60, "999": [58, 59, 73, 84, 141], "999207": 82, "9995": [59, 67, 68], "9996": [59, 67, 68], "99960462": 108, "9996553": 55, "9997": [59, 67, 68], "9998": [59, 67, 68], "9999": [59, 67, 68], "99c8": 56, "A": [7, 9, 10, 13, 14, 18, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 51, 52, 53, 55, 56, 60, 61, 65, 66, 70, 72, 75, 76, 80, 81, 83, 84, 85, 88, 89, 91, 93, 94, 95, 96, 99, 107, 124, 125, 126, 132, 133, 134, 135, 136, 138, 139, 141], "ATE": [9, 33, 37, 55, 62, 64, 74, 78, 84, 85, 93, 95, 104, 107, 109, 117, 125, 133], "ATEs": [64, 80], "And": [57, 80, 86, 125, 127], "As": [52, 53, 54, 55, 56, 57, 58, 60, 61, 64, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 82, 85, 86, 87, 94, 95, 99, 108, 109, 113, 115, 124, 125, 126, 135, 141], "At": [9, 10, 26, 54, 58, 59, 64, 73, 75, 77, 79, 82, 141], "Being": 141, "But": [74, 75], "By": [53, 54, 77, 83, 85, 94, 95, 99, 125, 131], "For": [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 33, 39, 43, 47, 51, 53, 54, 56, 58, 60, 61, 64, 65, 73, 74, 75, 76, 77, 79, 81, 83, 84, 85, 87, 88, 89, 91, 93, 94, 95, 96, 97, 99, 100, 101, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 133, 135, 137, 138, 141], "ITE": [14, 64], "ITEs": 64, "If": [4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 52, 54, 58, 60, 61, 66, 67, 68, 74, 75, 77, 79, 83, 88, 89, 91, 93, 94, 95, 97, 102, 109, 110, 112, 113, 114, 117, 124, 125, 126, 127, 129, 130, 131, 134, 135, 136, 141], "In": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 46, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141], "It": [21, 53, 54, 55, 67, 68, 70, 71, 72, 77, 78, 79, 83, 85, 94, 108, 136, 140], "No": [12, 51, 53, 55, 56, 57, 58, 60, 61, 62, 64, 73, 78, 79, 83, 84, 86, 89, 90, 91, 94, 95, 97, 99, 109, 124, 138, 139], "Not": [95, 100], "Of": [75, 124, 141], "On": [52, 66, 76, 80, 88, 139], "One": [55, 60, 78, 79, 85, 93, 124], "Or": 40, "Such": [85, 94], "That": [40, 141], "The": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 103, 105, 106, 107, 108, 109, 114, 117, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 135, 136, 137, 139, 140, 141], "Then": [26, 69, 82, 95, 124, 125, 135, 136, 137], "There": [55, 78, 85, 95, 100, 137, 141], "These": [55, 56, 63, 76, 78, 81, 83, 84, 93, 95, 141], "To": [28, 51, 52, 54, 55, 56, 57, 58, 60, 61, 64, 65, 66, 67, 68, 69, 71, 72, 74, 75, 77, 78, 79, 81, 82, 83, 84, 85, 86, 88, 89, 91, 93, 94, 95, 108, 124, 125, 126, 131, 135, 137, 138, 141], "Will": [95, 98, 109, 111, 125, 128], "With": [13, 67, 68, 94, 139], "_": [52, 54, 59, 60, 61, 66, 67, 68, 69, 70, 71, 72, 77, 78, 79, 81, 82, 83, 87, 88, 93, 95, 100, 108, 109, 124, 125, 126, 131], "_0": [52, 54, 66, 70, 77, 87, 88, 108, 109, 122, 123, 124, 125, 135], "_1": [9, 10, 14, 25, 26, 57, 80, 86, 109, 122, 123], "_2": [9, 10, 14, 25, 26, 80], "_3": [9, 10, 14, 25, 26], "_4": [9, 10, 14, 25, 26], "_5": [9, 14], "__": [46, 47], "__init__": 76, "__version__": 137, "_a": [109, 113], "_all_coef": 108, "_all_s": 108, "_b": [109, 113], "_compute_scor": 28, "_compute_score_deriv": 28, "_coordinate_desc": 77, "_d": [83, 95], "_est_causal_pars_and_s": 140, "_estimator_typ": 76, "_h": [83, 95], "_i": [52, 57, 66, 82, 86, 88, 95, 100], "_id": 108, "_j": [9, 10, 14, 16, 25, 26, 54, 77, 124], "_l": 94, "_lower_quantil": 60, "_m": [94, 108], "_mean": 60, "_n": [109, 110, 112, 113, 117, 124, 125, 131, 134], "_n_folds_per_clust": 77, "_offset": 94, "_pred": 94, "_rmse": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "_upper_quantil": 60, "_x": 41, "_y": [83, 95], "a0": 76, "a09a": 56, "a09b": 56, "a1": 76, "a3d9": 56, "a4a147": 80, "a5e6": 56, "a5e7": 56, "a6ba": 56, "a79359d2da46": 56, "a840": 56, "a_": [57, 86], "a_0": 17, "a_1": 17, "a_j": [95, 101], "ab": [53, 136], "ab71": 56, "abadi": [7, 58], "abb0fd28": 56, "abdt": [62, 89, 91, 138], "abl": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 52, 66, 75, 79, 80, 94, 125, 126, 135], "about": [22, 24, 55, 75, 78, 95, 136, 138, 141], "abov": [52, 55, 64, 66, 67, 68, 71, 72, 75, 76, 78, 80, 81, 82, 83, 85, 88, 93, 94, 95, 99, 100, 137], "absolut": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 75, 94], "abstract": [28, 53, 54, 77, 109, 136, 140], "abus": [60, 95, 99, 100], "acc": [30, 53], "accept": [25, 93, 94], "access": [42, 43, 53, 55, 60, 61, 71, 72, 73, 75, 84, 94, 125, 131, 141], "accord": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 55, 57, 58, 64, 66, 69, 78, 82, 83, 85, 86, 94, 95, 124, 125, 127, 129, 130, 132, 133, 141], "accordingli": [57, 58, 75, 76, 78, 83, 86], "account": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 54, 55, 77, 78, 79, 84, 85, 125, 131, 135, 141], "accumul": [55, 78, 79, 84], "accuraci": [42, 46, 53, 95], "acemoglu": 139, "achiev": [54, 74, 77, 81, 85, 95, 124], "acic_2024_post": 80, "acknowledg": [55, 56, 78], "acm": 139, "acov": 139, "across": [21, 25, 55, 78, 80, 141], "action": 140, "activ": [4, 5, 6, 137, 140], "actual": [40, 60, 73, 85], "acycl": [57, 86, 141], "ad": [4, 5, 6, 7, 8, 28, 42, 43, 46, 47, 73, 89, 91, 94, 95, 124, 125, 126, 140], "adapt": [32, 78, 140], "add": [53, 54, 57, 58, 59, 62, 64, 71, 72, 73, 80, 82, 83, 84, 85, 86, 94, 95, 139, 140], "add_trac": 85, "addit": [14, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 44, 60, 61, 70, 85, 90, 91, 94, 95, 96, 109, 118, 125, 132, 133, 135, 139, 140], "addition": [9, 10, 64, 69, 79, 84, 94, 95, 108, 124, 125, 131, 138], "additional_inform": 21, "additional_paramet": 21, "address": 85, "adel": 139, "adj": [83, 85], "adj_coef_bench": 85, "adj_est": 85, "adj_vanderweelearah": 85, "adjust": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 45, 54, 59, 74, 77, 79, 84, 85, 93, 95, 100, 124, 125, 131, 139, 140, 141], "adopt": [58, 95, 97, 100], "advanc": [76, 92, 108, 139], "advantag": [52, 53, 55, 64, 66, 78, 79, 88, 137], "advers": [125, 126], "adversari": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 84, 125, 131, 135], "ae": [52, 54, 55, 57], "ae56": 56, "ae89": 56, "aesthet": 52, "aeturrel": 18, "afd9e4": 80, "affect": [64, 70, 95, 140, 141], "after": [53, 55, 56, 57, 58, 70, 78, 79, 85, 86, 93, 94, 95, 100, 125, 127, 131, 137, 141], "after_stat": 52, "ag": [55, 78, 79, 81, 84, 141], "again": [52, 53, 54, 55, 57, 58, 60, 64, 66, 73, 76, 77, 78, 83, 84, 85, 86, 88, 125, 127], "against": [58, 73, 75, 81, 94], "agebra": 93, "agegt54": [56, 62, 89, 91, 138], "agelt35": [56, 62, 89, 91, 138], "agg": [53, 60], "agg_df": 60, "agg_df_anticip": 60, "agg_dict": 60, "agg_dictionari": 60, "agg_did_obj": [95, 96], "aggrag": [95, 96], "aggreg": [21, 24, 53, 87, 96, 108, 140], "aggregate_over_split": 40, "aggregated_eventstudi": [60, 61], "aggregated_framework": [60, 61, 95, 96], "aggregated_group": 60, "aggregated_tim": [60, 61], "aggregation_0": 21, "aggregation_1": 21, "aggregation_color_idx": 21, "aggregation_method_nam": 21, "aggregation_nam": 21, "aggregation_weight": [21, 60, 61], "aggt": 53, "ai": 61, "aim": 83, "aipw": 80, "aipw_est_1": 80, "aipw_est_2": 80, "aipw_obj_1": 80, "aipw_obj_2": 80, "air": [54, 77], "al": [7, 8, 11, 13, 16, 17, 52, 54, 55, 56, 58, 66, 67, 68, 69, 70, 71, 72, 75, 77, 78, 79, 82, 84, 88, 95, 97, 100, 108, 109, 115, 117, 118, 119, 124, 125, 126, 135, 136, 138, 140], "alexandr": [70, 139], "algebra": 95, "algorithm": [51, 53, 54, 56, 57, 58, 60, 61, 64, 66, 69, 74, 75, 77, 79, 82, 84, 86, 92, 94, 95, 97, 99, 108, 109, 124, 140, 141], "alia": [42, 43, 46, 47], "align": [52, 54, 57, 59, 60, 66, 69, 75, 77, 78, 80, 81, 82, 86, 95, 99, 100, 109, 113, 140], "all": [4, 5, 6, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 46, 47, 48, 52, 53, 54, 55, 57, 58, 64, 66, 73, 74, 75, 76, 77, 78, 79, 81, 83, 85, 86, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 100, 108, 109, 113, 124, 125, 135, 136, 137, 140], "all_coef": 108, "all_dml1_coef": 87, "all_s": 108, "all_smpl": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 74], "all_smpls_clust": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "all_z_col": [54, 77], "allow": [4, 5, 6, 20, 21, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 55, 60, 64, 78, 79, 83, 91, 93, 94, 95, 108, 109, 124, 136, 140, 141], "almqvist": 139, "along": 94, "alpha": [15, 17, 20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 55, 57, 60, 62, 64, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 82, 87, 88, 93, 94, 95, 108, 109, 124, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135], "alpha_": [16, 54, 77, 94], "alpha_0": [125, 135], "alpha_ml_l": 70, "alpha_ml_m": 70, "alpha_x": [12, 32, 95], "alreadi": [26, 57, 58, 60, 86, 94, 95, 97], "also": [20, 22, 23, 24, 29, 32, 33, 39, 51, 52, 53, 54, 55, 56, 58, 60, 64, 65, 66, 67, 68, 71, 72, 73, 75, 76, 77, 78, 79, 81, 83, 84, 85, 88, 93, 94, 95, 108, 109, 124, 125, 126, 137, 138, 140, 141], "alter": [54, 77], "altern": [53, 55, 56, 78, 81, 92, 94, 124, 136, 138], "although": 85, "alwai": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 47, 53, 83, 140], "always_tak": [32, 55, 78], "alyssa": 139, "amamb": 77, "american": [15, 80], "amgrem": 77, "amhorn": 77, "amit": [85, 139], "amjavl": 77, "ammata": 77, "among": [55, 70, 78, 79, 84, 85], "amount": [24, 55, 76, 78, 79, 141], "amp": [51, 54, 56, 57, 58, 60, 61, 77, 79, 84, 86], "an": [4, 5, 6, 9, 10, 14, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 52, 53, 54, 55, 56, 57, 60, 61, 64, 66, 67, 68, 70, 73, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 91, 93, 94, 95, 96, 100, 101, 108, 109, 124, 125, 126, 131, 136, 138, 139, 140, 141], "analog": [27, 28, 54, 60, 77, 79, 84, 93, 95, 97, 109, 110, 112, 113, 124, 125, 131], "analys": 141, "analysi": [20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 52, 54, 55, 66, 77, 78, 79, 88, 92, 93, 95, 99, 126, 131, 135, 136, 140], "analyt": [80, 82], "analyz": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 55, 78, 79, 84, 141], "ancillari": 85, "andrea": 139, "angl": 55, "angrist": 80, "ani": [51, 52, 53, 56, 57, 58, 65, 66, 85, 86, 88, 95, 137, 141], "anna": [9, 10, 14, 20, 22, 23, 25, 26, 53, 58, 60, 61, 95, 96, 97, 99, 100, 139], "annal": [124, 139], "anneal": 94, "annot": 52, "annual": 139, "anoth": [52, 53, 54, 55, 66, 75, 76, 77, 88, 94, 95, 102], "anticip": [22, 24, 25, 53, 61, 95, 96, 99, 100], "anticipation_period": [22, 24, 25, 60], "anymor": [54, 77], "aos1161": 124, "aos1230": 124, "aos1671": 124, "ap": [55, 78], "ape_e401_uncond": 55, "ape_p401_uncond": 55, "api": [89, 91, 136, 140], "apo": [29, 30, 101, 114, 132], "apoorva": 140, "apoorva__l": 80, "apoorval": 80, "app": 140, "appeal": 85, "append": [66, 75, 88], "appendix": [13, 19, 57, 60, 84, 86, 125, 126], "appli": [8, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 66, 74, 75, 77, 78, 79, 83, 85, 86, 88, 95, 108, 109, 124, 136, 138, 140, 141], "applic": [52, 58, 66, 80, 85, 88, 93, 108, 139, 141], "apply_along_axi": 81, "apply_cross_fit": [52, 108], "apply_crossfit": 140, "appreci": 136, "approach": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 53, 54, 64, 77, 83, 84, 85, 92, 94, 108, 124, 125, 126, 137, 139, 141], "appropri": [55, 70, 78, 95, 108, 141], "approx": [93, 95, 99], "approxim": [52, 66, 67, 68, 69, 75, 82, 85, 88, 93, 95, 124, 140, 141], "april": 60, "apt": 137, "ar": [4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 66, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 93, 94, 95, 96, 97, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 112, 113, 114, 116, 117, 120, 121, 124, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141], "arang": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 59, 66, 69, 79, 81, 82, 84, 85, 94], "arbitrarili": [43, 47], "architectur": [109, 139], "arellano": 139, "arg": [76, 83, 93, 95], "argmin": 75, "argu": [52, 55, 66, 78, 79, 84, 88, 141], "argument": [16, 17, 18, 20, 22, 23, 25, 26, 29, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 44, 55, 58, 60, 61, 67, 68, 73, 75, 78, 79, 87, 93, 94, 95, 96, 140, 141], "aris": [52, 53, 54, 66, 77, 85, 88, 141], "aronow": 80, "around": [53, 55, 78, 79, 83, 95, 109], "arr": 81, "arrai": [4, 5, 6, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 45, 46, 47, 57, 58, 60, 61, 64, 66, 67, 68, 69, 75, 77, 80, 81, 84, 85, 86, 87, 88, 93, 94, 108, 124, 125, 131, 138, 140, 141], "arrang": 54, "array_lik": 36, "articl": [18, 136], "arxiv": [16, 53, 54, 77, 85, 136, 139, 140], "as_learn": [56, 94], "asarrai": [67, 68], "aspect": [55, 78, 79], "assert": 94, "assess": 53, "asset": [79, 84, 141], "assign": [4, 5, 6, 25, 55, 60, 72, 78, 83, 93, 94, 95, 96, 107, 125, 141], "assmput": [95, 107], "associ": [55, 70, 78, 95, 124, 139], "assum": [51, 54, 58, 65, 77, 80, 81, 85, 95, 97, 99, 100, 109, 110, 112, 113, 124, 125, 135, 141], "assumpt": [53, 54, 55, 57, 58, 59, 60, 75, 77, 78, 80, 83, 86, 95, 97, 99, 100, 107, 124, 141], "assur": 140, "astyp": [60, 61, 65, 83, 85], "asymptot": [27, 28, 52, 54, 66, 77, 88, 108, 124, 139], "ate": 64, "ate_estim": [57, 86], "ates": 64, "athei": 139, "att": [9, 24, 25, 33, 53, 59, 73, 74, 81, 85, 93, 95, 96, 97, 99, 100, 104, 109, 117, 125, 133, 140], "att_": [95, 99], "att_gt": [53, 61], "attach": 53, "atte_estim": 58, "attempt": [42, 43], "attenu": [55, 78], "attr": 55, "attribut": [4, 5, 6, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 45, 46, 47, 75, 76, 87, 90, 91, 94, 108, 109, 124], "attributeerror": [42, 43], "attrict": 95, "attrit": [37, 57, 86, 95, 107], "au": [56, 94, 136, 138], "auc": 53, "author": [53, 85, 136], "auto_ml": 76, "autodoubleml": 76, "autom": 76, "automat": [21, 24, 52, 60, 61, 66, 73, 88, 93, 125, 131], "automl": 140, "automl_l": 76, "automl_l_lesstim": 76, "automl_m": 76, "automl_m_lesstim": 76, "automobil": [54, 77], "autos": 70, "autosklearn": 76, "auxiliari": [52, 66, 88], "avail": [12, 53, 55, 56, 58, 60, 64, 70, 75, 78, 79, 80, 81, 83, 85, 88, 93, 94, 95, 96, 97, 99, 125, 135, 136, 137, 140, 141], "avaiv": 45, "aver": 64, "averag": [9, 10, 25, 26, 29, 30, 32, 33, 39, 51, 53, 56, 57, 58, 59, 60, 61, 65, 73, 79, 80, 81, 83, 84, 85, 86, 92, 96, 97, 100, 101, 102, 103, 104, 107, 114, 117, 124, 132, 133, 139, 140, 141], "average_it": 64, "avoid": [52, 53, 66, 83, 95, 108, 137, 140], "awai": 84, "ax": [21, 24, 59, 60, 61, 64, 66, 67, 68, 69, 71, 72, 75, 76, 77, 78, 79, 80, 82, 83], "ax1": [64, 69, 74, 79, 82], "ax2": [64, 69, 74, 79, 82], "axhlin": [59, 76, 83], "axi": [21, 24, 54, 55, 64, 70, 74, 75, 77, 78, 80, 81, 83], "axvlin": [64, 66], "b": [18, 20, 22, 23, 24, 25, 52, 54, 56, 66, 67, 68, 77, 80, 82, 83, 85, 88, 93, 94, 95, 100, 124, 125, 135, 136, 138, 139], "b208": 56, "b371": 56, "b5d34a6f42b": 56, "b5d7": 56, "b_": 95, "b_0": 17, "b_1": 17, "b_j": 18, "bach": [70, 75, 76, 85, 136, 139, 140], "backbon": 75, "backend": [4, 5, 6, 53, 79, 84, 85, 89, 90, 92, 140], "backward": 140, "bad": 80, "balanc": [55, 60, 78, 79], "band": [53, 92, 141], "bandwidth": [34, 35, 36, 40, 83, 95], "bar": [73, 76, 78, 93, 95, 109, 114, 117, 125, 132], "base": [11, 14, 20, 22, 23, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 45, 52, 53, 54, 55, 57, 58, 59, 60, 61, 64, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 91, 93, 94, 95, 96, 100, 107, 108, 109, 124, 125, 126, 131, 136, 138, 139, 140, 141], "base_estim": [46, 47, 83], "baselin": [14, 22, 55, 76, 78], "basi": [29, 33, 39, 44, 67, 68, 74, 93], "basic": [53, 54, 55, 58, 61, 77, 78, 79, 80, 83, 84, 85, 92, 94], "basis_df": 74, "basis_matrix": 74, "batch": 56, "battocchi": 139, "bay": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 124], "bb2913dc": 56, "bbotk": [56, 94, 140], "bbox_inch": 66, "bbox_to_anchor": 66, "bcallaway11": 53, "bd929a9e": 56, "bde4": 56, "becam": [55, 78, 79], "becaus": [43, 47, 51, 52, 53, 54, 65, 66, 72, 73, 77, 80, 85, 88, 141], "becker": [56, 94], "becom": [54, 72, 76, 77, 93, 108], "bee": 59, "been": [21, 24, 54, 55, 60, 61, 76, 77, 78, 79, 84, 85, 93, 94, 95, 100, 140], "befor": [25, 53, 55, 59, 60, 61, 64, 73, 78, 82, 85, 95, 97, 141], "begin": [12, 15, 16, 52, 54, 55, 56, 57, 59, 60, 66, 69, 75, 77, 78, 80, 81, 82, 86, 87, 89, 91, 94, 95, 99, 100, 108, 109, 113, 124, 138, 141], "behav": [60, 61, 72], "behavior": [55, 80, 94], "behaviour": 72, "behind": 95, "being": [14, 19, 25, 27, 28, 41, 46, 47, 54, 77, 83, 85, 95, 99, 100, 108, 109, 115, 124, 125, 131, 136], "belloni": [13, 70, 124, 139], "below": [51, 55, 65, 78, 80, 95, 137, 138], "bench_x1": 85, "bench_x2": 85, "benchmark": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 64, 73, 126, 140], "benchmark_dict": [48, 84], "benchmark_inc": 84, "benchmark_pira": 84, "benchmark_result": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39], "benchmark_twoearn": 84, "benchmarking_set": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 64, 73, 84, 85, 125, 126], "benchmarking_vari": 73, "benefit": [52, 55, 66, 78, 88], "bernoulli": 12, "berri": [54, 77], "besid": 138, "best": [29, 33, 39, 43, 44, 47, 60, 67, 68, 71, 72, 76, 137], "best_loss": 76, "beta": [12, 13, 15, 19, 37, 55, 57, 78, 81, 83, 86, 95], "beta_": [57, 86], "beta_0": [11, 57, 81, 86, 93], "beta_a": [9, 10, 85], "beta_j": [12, 13, 15, 19], "better": [53, 60, 64, 75, 85, 95, 100], "between": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 57, 59, 60, 64, 65, 69, 70, 76, 80, 82, 84, 85, 86, 95, 102, 109, 110, 112, 113, 117, 120, 121, 124, 125, 135, 138, 140], "betwen": [51, 65], "beyond": 139, "bia": [19, 51, 57, 65, 70, 83, 85, 86, 92, 95, 107, 108, 109, 122, 123, 125, 135, 139, 140], "bias": [51, 55, 60, 65, 78, 79, 84, 141], "bias_bench": 85, "bibtex": 136, "big": [70, 87, 108, 109, 110, 118, 124, 125, 127, 129, 130, 133, 134, 135], "bigg": [54, 77, 109, 116, 117, 125, 133], "bilia": 8, "bilinski": 139, "bin": [52, 64, 66, 137], "binari": [11, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 37, 39, 41, 51, 53, 55, 56, 58, 65, 73, 74, 75, 78, 80, 81, 85, 93, 94, 97, 100, 103, 104, 107, 125, 132, 133, 140, 141], "binary_outcom": 41, "binary_treat": [11, 67, 71, 73], "bind": 140, "binder": [56, 94, 136, 138, 140], "binomi": [65, 80, 81, 82], "bischl": [56, 94, 136, 138], "black": [52, 56, 62, 89, 91, 138], "blob": 53, "blog": 18, "blondel": [136, 138], "blp": [44, 54, 77], "blp_data": [54, 77], "blp_model": [71, 72], "blue": [52, 54, 57, 77], "bodori": 139, "bond": [55, 78, 79], "bonferroni": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 124], "bonu": [8, 56, 89, 91, 138], "book": [56, 85, 94], "bool": [4, 5, 6, 9, 11, 14, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 73, 83], "boolean": [19, 71, 72, 89, 91, 108], "boost": [51, 55, 58, 60, 65, 75, 78], "boost_class": [55, 78], "boost_summari": 78, "boostrap": [69, 140], "bootstrap": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 60, 61, 64, 67, 68, 69, 71, 72, 79, 82, 92, 93, 95, 96, 108, 109, 136, 138, 140, 141], "both": [10, 11, 20, 21, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 55, 56, 58, 59, 74, 75, 76, 78, 79, 81, 83, 84, 85, 89, 91, 94, 95, 99, 124, 125, 126, 131, 134, 135, 140, 141], "bottom": [54, 55, 75, 77, 78, 79], "bound": [14, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 60, 64, 73, 74, 78, 84, 85, 125, 126, 131, 135, 141], "branch": 56, "brantli": [53, 139], "break": [52, 140], "breviti": 141, "brew": 137, "brewer": 54, "bridg": 85, "brief": 88, "bring": [51, 65], "brucher": [136, 138], "bsd": 140, "bst": 78, "budget": [76, 94], "bug": [136, 140], "build": [54, 75, 77, 81], "build_design_matric": [67, 68], "build_sim_dataset": 53, "built": [45, 76, 94, 136], "bureau": [85, 108, 139], "busi": [16, 19, 54, 77, 85, 139], "b\u00fchlmann": 139, "c": [7, 8, 10, 13, 15, 17, 25, 26, 51, 52, 53, 54, 55, 56, 59, 62, 65, 66, 70, 71, 72, 77, 78, 80, 83, 88, 89, 91, 94, 95, 100, 109, 113, 125, 130, 136, 137, 138, 139, 141], "c1": [7, 8, 17, 54, 70, 77, 88, 136, 139], "c68": [7, 8, 17, 54, 70, 77, 88, 136, 139], "c895": 56, "c_": [95, 99, 100, 109, 113, 124, 125, 130], "c__": 24, "c_d": [13, 125, 133, 134, 135], "c_y": [13, 125, 135], "ca1af7be64b2": 56, "caac5a95": 56, "calcualt": 81, "calcul": [29, 33, 39, 53, 55, 60, 64, 67, 68, 69, 71, 72, 75, 76, 78, 82, 84, 125, 131, 135], "calendar": [60, 61], "calibr": [75, 76, 85], "call": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 43, 46, 47, 51, 53, 54, 55, 56, 57, 58, 60, 61, 65, 67, 68, 69, 71, 72, 77, 78, 79, 81, 82, 83, 84, 85, 86, 89, 91, 94, 108, 109, 124, 125, 131, 135, 138, 140, 141], "callabl": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 66, 67, 68, 75, 92, 94, 136], "callawai": [25, 53, 60, 61, 95, 96, 99, 100, 139], "camera": 70, "cameron": [54, 77], "can": [4, 5, 6, 9, 11, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 45, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 97, 99, 100, 101, 107, 108, 109, 110, 112, 113, 114, 117, 120, 121, 124, 125, 126, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141], "candid": 85, "cannot": [75, 83, 85, 95, 141], "capabl": [4, 5, 6, 51, 65], "capo": [29, 74], "capo0": 74, "capo1": 74, "capsiz": [64, 76, 80, 83], "capthick": [64, 83], "cardin": [54, 77], "care": 94, "carlo": [9, 10, 11, 14, 67, 68, 71, 72, 85, 139], "casalicchio": [56, 94, 136, 138], "case": [4, 5, 6, 8, 11, 22, 29, 32, 33, 40, 51, 54, 55, 60, 65, 67, 68, 69, 70, 72, 73, 74, 76, 77, 81, 82, 83, 84, 85, 89, 91, 93, 94, 95, 97, 100, 107, 108, 109, 113, 124, 125, 131, 138, 140, 141], "cat": [52, 140], "catboost": 75, "cate": [33, 39, 44, 74, 92, 140], "cate_obj": 93, "cattaneo": [95, 139], "caus": [52, 66, 83, 88], "causal": [12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 54, 55, 56, 57, 65, 66, 74, 75, 76, 77, 78, 80, 84, 86, 87, 88, 89, 91, 92, 95, 100, 108, 124, 125, 131, 139], "causal_contrast": [30, 64, 74, 95], "causal_contrast_att": 74, "causal_contrast_c": 74, "causal_contrast_model": [64, 95], "causaldml": 139, "causalweight": 139, "caution": 124, "caveat": [72, 85], "cbind": 54, "cbook": [60, 61], "cc": 78, "ccp_alpha": [33, 45, 78], "cd": 137, "cd_fast": 77, "cda85647": 56, "cdf": 93, "cdid": [54, 77], "cdot": [9, 10, 14, 25, 26, 41, 54, 59, 60, 69, 73, 77, 80, 82, 83, 85, 93, 95, 96, 99, 100, 109, 113, 114, 117, 118, 122, 123, 124, 125, 130, 132], "cdot1": 73, "cell": 76, "center": [60, 70], "central": [108, 140], "certain": [72, 95, 109, 113], "cexcol": 54, "cexrow": 54, "cf_d": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 60, 64, 73, 74, 84, 85, 125, 126, 131, 132, 133, 134, 135, 141], "cf_y": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 60, 64, 73, 74, 84, 85, 125, 126, 131, 132, 133, 134, 135, 141], "chad": 85, "chain": 72, "chainedassignmenterror": 72, "challeng": [54, 77, 125, 126], "chang": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 55, 57, 58, 61, 72, 79, 84, 85, 86, 95, 100, 109, 117, 124, 125, 126, 127, 129, 130, 131, 132, 133, 137, 139, 140], "channel": 141, "chapter": [27, 28, 56, 94, 125, 135], "charact": [55, 56, 94, 140], "characterist": [84, 141], "chart": 76, "check": [42, 43, 46, 47, 52, 55, 60, 66, 75, 76, 78, 79, 87, 88, 136, 137, 140], "check_data": 140, "check_scor": 140, "checkmat": 140, "chernozhukov": [7, 8, 13, 15, 17, 52, 54, 55, 66, 70, 75, 76, 77, 78, 79, 84, 88, 108, 109, 117, 124, 125, 126, 135, 136, 139, 140], "chetverikov": [7, 8, 17, 54, 70, 77, 88, 124, 136, 139], "chiang": [16, 54, 77, 139], "chieh": 139, "choic": [4, 5, 6, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 55, 60, 61, 70, 78, 81, 93, 94, 95, 99, 109, 113, 125, 126, 131, 135, 140], "choos": [51, 55, 65, 66, 70, 75, 78, 79, 87, 95, 99, 108, 109, 110, 112, 113, 117, 120, 121, 124, 138, 141], "chosen": [10, 14, 29, 75, 94, 95], "chou": 80, "chr": 55, "christian": [70, 139], "christoph": 139, "chunk": 94, "ci": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 58, 59, 60, 61, 64, 67, 68, 69, 71, 72, 73, 74, 76, 78, 79, 82, 83, 84, 85, 93, 95, 125, 131, 140, 141], "ci_at": 64, "ci_cvar": [69, 79], "ci_cvar_0": 69, "ci_cvar_1": 69, "ci_joint": [60, 61, 64], "ci_joint_cvar": 69, "ci_joint_lqt": 82, "ci_joint_qt": 82, "ci_length": 58, "ci_low": 64, "ci_lpq_0": 82, "ci_lpq_1": 82, "ci_lqt": [79, 82], "ci_pointwis": 64, "ci_pq_0": [79, 82], "ci_pq_1": [79, 82], "ci_qt": [79, 82], "ci_upp": 64, "cinelli": [85, 125, 126, 139], "circumv": 141, "citat": 140, "claim": 56, "clarifi": [60, 61], "clash": 53, "class": [0, 4, 5, 6, 20, 21, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 55, 56, 57, 58, 60, 61, 62, 64, 73, 74, 76, 78, 79, 84, 86, 87, 89, 90, 91, 93, 94, 95, 96, 108, 109, 124, 136, 138, 140], "class_estim": 83, "class_learn": 79, "class_learner_1": 75, "class_learner_2": 75, "classes_": 76, "classic": [53, 54, 77, 141], "classif": [33, 42, 46, 51, 53, 55, 56, 57, 60, 61, 75, 76, 81, 84, 93, 94, 95, 97, 99, 141], "classifavg": 56, "classifi": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 39, 40, 42, 46, 56, 64, 76, 83, 94, 140], "classmethod": [4, 5, 6], "claudia": [139, 140], "claus": 140, "clean": 140, "cleaner": 75, "cleanup": 140, "clear": [54, 77], "clearli": [60, 83], "clever": 75, "clone": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 56, 66, 75, 77, 79, 87, 94, 95, 108, 109, 124, 125, 131, 137, 138], "close": [53, 55, 78, 85, 125, 126], "cluster": [4, 16, 20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 61, 139, 140], "cluster_col": [4, 54, 77], "cluster_var": [4, 16], "cluster_var_i": [4, 54, 77], "cluster_var_j": [4, 54, 77], "cmap": 77, "cmd": 140, "co": [18, 59], "codaci": 140, "code": [18, 29, 33, 39, 51, 53, 54, 55, 56, 57, 65, 70, 78, 88, 93, 94, 95, 108, 109, 124, 137, 138, 140, 141], "codecov": 140, "coef": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 99, 108, 109, 124, 138, 141], "coef_": 85, "coef_df": 54, "coef_valu": 76, "coeffici": [9, 10, 11, 24, 43, 44, 47, 55, 57, 71, 72, 75, 78, 80, 81, 83, 85, 86, 93, 124, 125, 131, 141], "coefs_t": 81, "coefs_w": 81, "coffici": [125, 131], "cofid": 44, "coincid": [59, 74, 79], "col": [52, 54, 72, 78], "col_nam": 60, "collect": [56, 57, 58, 77, 86], "colnam": [54, 75], "color": [21, 24, 55, 57, 59, 64, 66, 67, 68, 69, 76, 77, 78, 79, 80, 82, 83, 85], "color_palett": [21, 24, 60, 64, 66, 77, 78, 79], "colorbar": 77, "colorblind": [21, 24, 60, 64], "colorramppalett": 54, "colorscal": [67, 68], "colour": [52, 54], "column": [4, 5, 6, 58, 59, 60, 61, 62, 64, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 83, 84, 85, 86, 89, 90, 91, 93, 94, 95, 99, 108, 138, 140, 141], "column_stack": [59, 64, 71, 72, 83, 84, 85, 95], "colv": 54, "com": [18, 53, 55, 56, 61, 70, 80, 85, 94, 137], "comb": 70, "combin": [22, 24, 53, 54, 56, 58, 61, 64, 74, 75, 76, 77, 85, 94, 95, 99, 108, 125, 131, 140], "combind": 79, "combined_loss": 70, "come": [87, 94, 109, 125, 126, 136, 141], "command": [137, 140], "comment": [89, 91], "common": [75, 84, 85, 93, 95, 139], "companion": 139, "compar": [52, 54, 59, 60, 66, 67, 68, 69, 71, 72, 74, 77, 80, 82, 83, 85, 88, 94, 95, 125, 126], "comparevers": 55, "comparison": [60, 64, 75, 80], "compat": [51, 53, 65, 140], "complement": 85, "complet": [76, 88, 125, 131, 137], "complex": [33, 53, 76], "compli": [83, 95], "complianc": [82, 83, 95, 109, 118], "complic": [56, 141], "complier": [55, 78, 79, 82, 83, 93, 95], "compon": [25, 46, 47, 53, 55, 70, 75, 76, 78, 81, 93, 94, 108, 109, 110, 112, 113, 114, 116, 117, 118, 120, 121, 140], "compont": 53, "composit": 139, "compris": 124, "comput": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 48, 52, 53, 55, 56, 60, 61, 66, 78, 79, 84, 85, 108, 109, 125, 126, 127, 129, 130, 131, 132, 133, 136, 139, 140, 141], "computation": [125, 126], "concat": [76, 77, 78, 81, 124], "concaten": [59, 78, 124], "concentr": 124, "concern": 85, "conclud": [83, 85, 141], "cond": [95, 97, 107], "conda": [77, 139, 140], "condit": [9, 10, 11, 27, 28, 29, 31, 33, 39, 52, 54, 55, 57, 58, 59, 60, 64, 66, 73, 74, 77, 78, 81, 83, 85, 86, 88, 92, 95, 99, 100, 124, 125, 132, 133, 135, 138, 139, 140, 141], "conduct": [93, 95, 97, 99, 141], "conf": [53, 82], "confer": 139, "confid": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 53, 54, 55, 57, 58, 60, 61, 64, 67, 68, 69, 71, 72, 74, 77, 79, 82, 83, 84, 86, 92, 93, 108, 109, 125, 131, 138, 139, 141], "confidenceband": 69, "confidenti": 85, "config": 80, "configur": [56, 76], "confint": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 55, 57, 58, 59, 60, 61, 64, 67, 68, 69, 71, 72, 74, 75, 79, 81, 82, 83, 84, 86, 93, 108, 124, 136, 138, 141], "conflict": 137, "confound": [9, 10, 11, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 51, 55, 60, 65, 73, 78, 82, 84, 85, 89, 91, 95, 105, 106, 124, 125, 126, 131, 134, 135, 138, 139, 140, 141], "congress": 139, "connect": [55, 78, 79], "consequ": [9, 10, 54, 73, 77, 84, 93, 95, 97, 125, 126, 132, 133, 135], "conserv": [84, 85, 125, 135], "consid": [31, 32, 33, 34, 35, 41, 52, 54, 55, 57, 58, 59, 60, 61, 66, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 100, 103, 104, 108, 109, 124, 125, 126, 136, 141], "consider": [85, 95], "consist": [38, 39, 43, 47, 55, 58, 76, 78, 79, 80, 85, 88, 89, 91, 95, 100, 105, 106, 138, 140], "consol": [52, 140], "constant": [13, 25, 43, 47, 70, 81, 93, 95, 124], "constrained_layout": 66, "construct": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 44, 56, 59, 60, 67, 68, 69, 74, 79, 84, 87, 93, 109, 115, 123, 124, 140, 141], "construct_framework": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "construct_iv": 77, "constructiv": 54, "constructor": 56, "consum": [54, 77], "cont": 14, "cont_d": 64, "contain": [14, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 46, 47, 52, 54, 55, 60, 64, 66, 67, 68, 71, 72, 75, 77, 78, 88, 90, 91, 93, 94, 95, 96, 99, 124, 125, 126, 131, 140], "context": [85, 95, 107, 141], "contin": [14, 76], "continu": [14, 51, 56, 64, 65, 70, 80, 83, 95, 125, 135, 140, 141], "contour": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 70, 73, 84, 85, 125, 131], "contour_plot": 85, "contours_z": [67, 68], "contrast": [30, 58, 69, 74, 95, 102], "contribut": [137, 140], "contributor": 140, "control": [15, 22, 24, 25, 41, 53, 61, 70, 74, 79, 81, 83, 85, 95, 96, 99, 100, 109, 113, 125, 130, 141], "control_group": [22, 24, 60, 61, 95, 96, 99], "convent": [40, 55, 78, 79, 83, 95, 100], "converg": [52, 66, 75, 77, 88], "convergencewarn": 77, "convers": 77, "convert": [69, 77, 82], "convex": 80, "cooper": 140, "coor": [56, 94, 136, 138], "coordin": 85, "copi": [72, 76, 78, 81, 85], "cor": [125, 135], "core": [25, 58, 60, 61, 62, 64, 69, 73, 77, 78, 79, 82, 84, 86, 89, 90, 91, 94, 138, 140], "cores_us": [69, 79, 82], "correct": [73, 74, 85, 93, 124, 140], "correctli": [42, 46, 58, 80, 84, 125, 135], "correl": [20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 54, 57, 70, 77, 84, 86, 95, 125, 126, 135], "correpond": [60, 95], "correspond": [9, 10, 14, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 55, 56, 58, 59, 60, 61, 64, 66, 67, 68, 70, 74, 75, 77, 78, 79, 81, 82, 84, 85, 88, 93, 94, 95, 97, 99, 100, 101, 107, 108, 109, 113, 124, 125, 126, 130, 131, 133, 135, 140, 141], "correspondingli": 60, "cosh": 18, "coul": 54, "could": [51, 56, 60, 65, 67, 68, 76, 85, 140, 141], "counfound": [9, 10, 82, 84, 93, 125, 135], "count": [64, 78, 79], "countour": [125, 131], "coupl": [55, 78, 79], "cournapeau": [136, 138], "cours": [55, 75, 78, 85, 124, 141], "cov": [9, 37, 41, 83], "cov_nam": [83, 95], "cov_typ": [29, 33, 39, 44, 140], "covari": [4, 5, 6, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 33, 38, 39, 40, 41, 44, 45, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 64, 66, 67, 68, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 84, 85, 86, 88, 89, 90, 91, 93, 94, 95, 97, 99, 105, 106, 107, 109, 110, 112, 113, 124, 125, 126, 138, 139, 140], "cover": [53, 70, 84], "coverag": [60, 75, 83, 93, 140], "cp": [55, 56, 94], "cpu": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "cpu_count": [69, 79, 82], "cran": [56, 139, 140], "creat": [11, 21, 24, 25, 51, 54, 56, 60, 64, 65, 66, 67, 68, 69, 71, 72, 77, 79, 81, 82, 85, 94, 125, 126, 131, 135, 137, 140], "create_synthetic_group_data": 81, "critic": [85, 141], "cross": [20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 53, 55, 56, 57, 66, 75, 76, 78, 79, 83, 85, 88, 92, 94, 110, 112, 123, 124, 127, 131, 140, 141], "cross_sectional_data": [23, 26, 58, 95, 97], "crossfit": [75, 95], "crosstab": 80, "crucial": [70, 95, 141], "csail": [136, 138], "csdid": 61, "csv": [61, 70], "cumul": 95, "current": [45, 53, 60, 72, 74, 109, 125, 135, 136, 137, 141], "custom": [21, 24, 52, 53, 66, 85, 94], "custom_measur": 53, "cut": 81, "cutoff": [40, 41, 83, 95], "cv": [56, 78, 94, 108], "cv_glmnet": [54, 55, 56, 57, 94, 95, 124, 138], "cvar": [31, 36, 92, 115, 140], "cvar_0": 69, "cvar_1": 69, "d": [4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 101, 103, 104, 105, 106, 107, 108, 109, 110, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 131, 132, 133, 134, 135, 136, 138, 139, 141], "d0": [69, 82, 124], "d0_true": 82, "d0cdb0ea4795": 56, "d1": [69, 80, 82, 124], "d10": 124, "d1_true": 82, "d2": [80, 124], "d21ee5775b5f": 56, "d2cml": 61, "d3": 124, "d4": 124, "d5": 124, "d5a0c70f1d98": 56, "d6": 124, "d7": 124, "d8": 124, "d9": 124, "d_": [14, 16, 54, 59, 64, 77, 95, 97, 100, 124], "d_0": [95, 101], "d_1": [80, 124], "d_2": 80, "d_col": [4, 5, 6, 24, 51, 52, 54, 55, 56, 57, 60, 61, 65, 67, 68, 71, 72, 74, 77, 78, 79, 81, 83, 84, 87, 88, 89, 90, 91, 94, 95, 96, 99, 108, 109, 138, 140, 141], "d_i": [11, 12, 13, 15, 17, 18, 19, 52, 57, 58, 64, 66, 69, 80, 82, 83, 86, 88, 95, 97, 107], "d_j": [64, 95, 101, 102, 124], "d_k": [95, 102, 124], "d_l": [95, 101], "d_w": 81, "da1440": 80, "dag": [57, 85, 86, 141], "dai": 60, "dark": [52, 66], "darkblu": 54, "darkr": 54, "dash": [57, 60, 64], "dat": [89, 91], "data": [0, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 46, 47, 53, 59, 70, 75, 80, 87, 89, 90, 92, 93, 94, 96, 99, 100, 108, 124, 129, 130, 131, 139, 140], "data_apo": 64, "data_cvar": 79, "data_dict": [40, 67, 68, 71, 72, 73, 83, 95], "data_dml": 84, "data_dml_bas": [55, 67, 68, 71, 72, 78, 79, 81], "data_dml_base_iv": [55, 78, 79], "data_dml_flex": [55, 78], "data_dml_flex_iv": 55, "data_dml_iv_flex": 78, "data_dml_new": 81, "data_fram": 141, "data_lqt": 79, "data_pq": 79, "data_qt": 79, "data_transf": [54, 77, 78], "datafram": [4, 5, 6, 7, 8, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 45, 54, 57, 58, 59, 60, 61, 62, 64, 65, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 90, 93, 94, 95, 97, 109, 124, 125, 126, 131, 138, 141], "dataset": [0, 4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 57, 58, 60, 64, 66, 67, 68, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 131, 138, 141], "datatyp": [61, 140], "date": [24, 25, 94], "date_format": 24, "datetim": [6, 24, 25, 60, 90, 91], "datetime64": [60, 90, 91], "datetime_unit": [6, 24, 60, 90, 91, 95, 96, 99], "db": [55, 78, 79, 84, 141], "dbl": [53, 54, 55, 56, 89, 91, 124, 138, 141], "dc13a11076b3": 56, "ddc9": 56, "de": [51, 65, 139], "deal": [51, 65], "debias": [7, 8, 16, 17, 54, 70, 77, 92, 94, 108, 136, 139, 140], "debt": [55, 78, 79], "decai": [57, 86], "decid": [55, 78], "decis": [33, 51, 55, 65, 78, 79, 93, 95, 139, 141], "decision_effect": 51, "decision_impact": [51, 65], "decisiontreeclassifi": [33, 45, 78], "decisiontreeregressor": 78, "declar": 141, "decreas": 83, "deep": [42, 43, 46, 47, 76], "deeper": 33, "def": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 60, 66, 69, 75, 76, 77, 80, 81, 82, 85, 94, 109], "default": [4, 5, 6, 9, 10, 11, 14, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 53, 54, 57, 58, 60, 61, 71, 72, 75, 77, 81, 83, 84, 85, 86, 87, 93, 94, 95, 108, 124, 125, 131, 132, 138, 141], "default_arg": 60, "default_convert": 77, "default_jitt": 24, "defier": [83, 95], "defin": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 40, 41, 43, 47, 52, 55, 56, 58, 64, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 78, 79, 81, 82, 83, 84, 85, 93, 94, 95, 97, 99, 100, 107, 109, 110, 112, 113, 125, 126, 131, 135], "definit": [18, 60, 71, 72, 74, 95, 99, 125, 132, 133], "defint": 125, "degre": [41, 55, 67, 68, 74, 77, 78, 83, 93, 125, 126], "dekel": 139, "delete_origin": 56, "deliber": 80, "delta": [15, 24, 53, 58, 60, 85, 95, 97, 99, 100, 109, 113, 125, 130], "delta_bench": 85, "delta_i": 53, "delta_j": 15, "delta_t": [25, 60], "delta_theta": [48, 64, 73, 84, 85, 125, 126], "delta_v": 85, "demand": [54, 77, 125, 126], "demir": [7, 8, 17, 54, 70, 77, 88, 108, 136, 139], "demo": 85, "demonstr": [52, 53, 54, 66, 77, 83, 85, 89, 91, 95, 124, 136, 138], "deni": 139, "denomin": [125, 126, 132, 133], "denot": [38, 54, 55, 57, 58, 59, 77, 78, 83, 85, 86, 93, 95, 97, 99, 100, 101, 105, 109, 125, 126, 131, 133, 135], "dens_net_tfa": 55, "densiti": [34, 35, 36, 52, 57, 64, 66], "dep": 62, "dep1": [56, 62, 89, 91, 138], "dep2": [56, 62, 89, 91, 138], "depend": [11, 25, 29, 31, 33, 34, 36, 56, 58, 60, 67, 68, 71, 72, 73, 75, 76, 81, 83, 87, 93, 94, 95, 99, 109, 118, 119, 125, 126, 132, 135, 138, 139], "deprec": [60, 61, 87, 95, 108, 109, 125], "depreci": 140, "depth": [33, 45, 55, 56, 81, 87, 93, 94, 95, 108, 109, 124, 138, 141], "deriv": [20, 22, 23, 25, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 95, 124], "describ": [21, 53, 54, 60, 77, 78, 79, 85, 94, 108, 137, 140], "descript": [55, 61, 62, 84, 94, 108, 109, 113, 125, 126, 130], "deserv": 95, "design": [25, 40, 41, 64, 76, 92, 139, 140], "design_info": [67, 68], "design_matrix": [67, 68, 93], "desir": [10, 56, 81, 95, 137], "detail": [20, 22, 23, 27, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 52, 55, 56, 58, 59, 64, 66, 70, 76, 79, 83, 84, 85, 88, 89, 91, 93, 94, 96, 97, 99, 100, 107, 109, 115, 117, 118, 119, 122, 123, 124, 125, 126, 130, 135, 136, 137, 138, 140, 141], "determin": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 47, 55, 69, 78, 79, 82, 83, 84, 95, 124, 125, 135], "determinist": [81, 83, 93, 95], "deutsch": 136, "dev": [137, 140], "develop": [53, 54, 56, 77, 85, 95, 97, 100, 140], "deviat": [75, 95, 125, 135], "dezeur": 139, "df": [4, 5, 6, 24, 51, 52, 54, 57, 59, 60, 64, 65, 67, 68, 69, 72, 74, 77, 80, 82, 83, 84, 85, 86, 88, 90, 91, 93, 95, 96, 99], "df_agg": 70, "df_anticip": 60, "df_apo": 64, "df_apo_ci": 64, "df_apos_ci": 64, "df_ate": 64, "df_bench": 85, "df_binari": 85, "df_bonu": [56, 89, 91, 138], "df_capo0": 74, "df_capo1": 74, "df_cate": [67, 68, 74], "df_causal_contrast_c": 74, "df_ci": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44], "df_coef": 75, "df_cvar": 79, "df_fuzzi": 83, "df_lqte": 79, "df_ml_g0": 75, "df_ml_g1": 75, "df_ml_m": 75, "df_pa": [58, 86], "df_plot": 54, "df_post_treat": 60, "df_pq": 79, "df_qte": 79, "df_result": 70, "df_sharp": 83, "df_sort": 64, "df_summari": 78, "df_wide": 77, "dfg": 136, "dgp": [25, 26, 54, 57, 59, 60, 69, 70, 77, 80, 81, 82, 85, 86], "dgp1": [25, 26], "dgp2": [25, 26], "dgp3": [25, 26], "dgp4": [25, 26], "dgp5": [25, 26], "dgp6": [25, 26], "dgp_dict": 85, "dgp_tpye": 58, "dgp_type": [25, 26, 58, 60], "diagon": 85, "diagram": [51, 65, 95], "dichotom": [51, 65], "dict": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 45, 46, 47, 48, 60, 67, 68, 70, 76, 85, 94], "dict_kei": [125, 131], "dictionari": [9, 10, 11, 14, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 48, 60, 67, 68, 71, 72, 84, 93, 94, 95, 96, 125, 131], "dictonari": [55, 78], "did": [0, 4, 5, 6, 52, 58, 59, 60, 61, 77, 90, 91, 92, 96, 97, 99, 100, 109, 113, 140, 141], "did_aggreg": [60, 61], "did_multi": [95, 96], "diff": 78, "differ": [9, 10, 11, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 51, 52, 54, 55, 56, 57, 60, 61, 64, 65, 66, 69, 71, 72, 73, 74, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 92, 93, 94, 96, 97, 99, 100, 108, 110, 112, 113, 137, 138, 139, 140, 141], "differenti": 95, "difficult": 85, "dillon": 139, "dim": [41, 55], "dim_x": [12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 25, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 52, 54, 56, 66, 74, 75, 76, 77, 88, 93, 94, 95, 125, 131], "dim_z": [15, 38, 95], "dimens": [11, 16, 25, 54, 77, 81, 108], "dimension": [11, 13, 21, 38, 39, 70, 93, 95, 105, 106, 108, 124, 125, 131, 138, 139], "direct": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 57, 59, 66, 74, 86, 88, 95, 141], "directli": [40, 52, 53, 55, 64, 66, 75, 84, 88, 125, 131, 138, 141], "discontinu": [40, 41, 92, 139, 140], "discret": [14, 30, 64, 77, 95, 101, 140], "discretis": 79, "discuss": [12, 54, 55, 77, 78, 95, 96, 100, 139, 140, 141], "disjoint": [54, 71, 72, 77], "displai": [21, 54, 60, 61, 64, 77, 85, 93, 94, 125, 131], "displot": 78, "disproportion": [55, 78], "disregard": [43, 47], "dist": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "distinguish": [24, 60], "distr": 94, "distribut": [41, 52, 58, 64, 66, 75, 85, 88, 95, 97, 100, 125, 133, 137, 139, 140], "diverg": [40, 52, 66, 88], "divid": [60, 95], "dmatrix": [67, 68, 93], "dml": [20, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 56, 57, 58, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 99, 109, 124, 125, 131, 137], "dml1": [92, 138, 140, 141], "dml2": [51, 54, 56, 57, 58, 62, 77, 79, 92, 95, 109, 124, 138, 140, 141], "dml_apo": 74, "dml_apo_obj": 95, "dml_apos_att": 74, "dml_apos_obj": 95, "dml_base": 77, "dml_combin": 124, "dml_cv_predict": 140, "dml_cvar": [69, 79], "dml_cvar_0": 69, "dml_cvar_1": 69, "dml_cvar_obj": [31, 93], "dml_data": [6, 24, 53, 54, 57, 58, 59, 60, 61, 62, 64, 73, 74, 75, 77, 80, 84, 85, 86, 90, 91, 93, 94, 95, 96, 99, 124, 141], "dml_data_anticip": 60, "dml_data_bench": 85, "dml_data_bonu": [56, 138], "dml_data_df": 141, "dml_data_fuzzi": 83, "dml_data_lasso": 62, "dml_data_sharp": 83, "dml_data_sim": [56, 138], "dml_df": [54, 77], "dml_did": [58, 59], "dml_did_obj": [20, 23, 24, 95, 96, 97, 99], "dml_iivm_boost": [55, 78], "dml_iivm_forest": [55, 78], "dml_iivm_lasso": [55, 78], "dml_iivm_obj": [32, 65, 95], "dml_iivm_tre": [55, 78], "dml_irm": [67, 71, 74, 75, 81], "dml_irm_at": 73, "dml_irm_att": 74, "dml_irm_boost": [55, 78], "dml_irm_forest": [55, 78], "dml_irm_gat": 73, "dml_irm_gatet": 73, "dml_irm_lasso": [55, 62, 78], "dml_irm_new": 81, "dml_irm_obj": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 84, 93, 94, 95], "dml_irm_obj_ext": 94, "dml_irm_rf": 62, "dml_irm_tre": [55, 78], "dml_irm_weighted_att": 74, "dml_kwarg": 74, "dml_long": 48, "dml_lpq_0": 82, "dml_lpq_1": 82, "dml_lpq_obj": [34, 93], "dml_lqte": [79, 82], "dml_obj": [53, 60, 61, 64, 84, 85], "dml_obj_al": 60, "dml_obj_anticip": 60, "dml_obj_bench": 85, "dml_obj_linear": 60, "dml_obj_nyt": 60, "dml_pliv": [54, 77], "dml_pliv_obj": [38, 54, 77, 95], "dml_plr": [68, 72, 124], "dml_plr_1": 124, "dml_plr_2": 124, "dml_plr_boost": [55, 78], "dml_plr_forest": [55, 78, 141], "dml_plr_lasso": [55, 62, 78], "dml_plr_no_split": 108, "dml_plr_obj": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 84, 87, 93, 94, 95, 108, 109, 124, 125, 126, 131], "dml_plr_obj_extern": 108, "dml_plr_obj_intern": 108, "dml_plr_obj_onfold": 76, "dml_plr_obj_untun": 76, "dml_plr_rf": 62, "dml_plr_tree": [55, 78, 141], "dml_pq_0": [79, 82], "dml_pq_1": [79, 82], "dml_pq_obj": [35, 93], "dml_procedur": [62, 87, 138, 140, 141], "dml_qte": [79, 82], "dml_qte_obj": [36, 93], "dml_short": 48, "dml_ssm": [57, 86, 95], "dml_tune": 140, "dmldummyclassifi": 94, "dmldummyregressor": 94, "dmlmt": 139, "dnorm": 52, "do": [53, 54, 55, 56, 74, 75, 77, 78, 79, 80, 85, 93, 94, 125, 135, 138, 141], "doabl": 109, "doc": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 136, 140], "doccument": 140, "docu": 140, "document": [59, 60, 61, 63, 67, 68, 71, 72, 74, 76, 85, 95, 100, 109, 125, 136, 140], "doe": [24, 30, 36, 53, 54, 55, 64, 74, 77, 78, 80, 84, 85, 109, 113, 125, 135, 141], "doesn": [51, 65], "doi": [7, 8, 9, 10, 12, 16, 17, 19, 25, 26, 53, 54, 56, 70, 77, 85, 88, 94, 108, 124, 136, 138, 140], "domain": 81, "don": [53, 76], "done": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 76, 79, 94, 108, 125, 126], "dosag": 64, "dot": [37, 59, 60, 81, 89, 91, 93, 94, 95, 99, 100, 101, 124, 138], "doubl": [4, 5, 6, 7, 8, 16, 17, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 55, 70, 75, 76, 78, 80, 92, 94, 108, 109, 124, 125, 126, 140], "double_ml": 60, "double_ml_bonus_data": 62, "double_ml_data_from_data_fram": [52, 88, 89, 91, 141], "double_ml_data_from_matrix": [53, 56, 89, 91, 94, 124, 138], "double_ml_framework": [95, 96], "double_ml_irm": [62, 81], "double_ml_score_mixin": 0, "doubleiivm": 136, "doubleml": [0, 52, 54, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 79, 80, 81, 82, 83, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 108, 109, 125, 131, 138, 139, 140], "doubleml2022python": 136, "doubleml2024r": 136, "doubleml_did_eval_linear": 53, "doubleml_did_eval_rf": 53, "doubleml_did_linear": 53, "doubleml_did_rf": 53, "doubleml_framework": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "doublemlapo": [64, 74, 95, 109, 114, 140], "doublemlblp": [29, 33, 39, 67, 68, 74, 93, 140], "doublemlclusterdata": [0, 16], "doublemlcvar": [69, 93, 109, 115, 140], "doublemldata": [0, 6, 7, 8, 12, 13, 15, 17, 18, 19, 20, 22, 23, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 54, 56, 57, 58, 59, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 92, 93, 94, 95, 97, 108, 109, 124, 125, 131, 140, 141], "doublemldid": [58, 59, 95, 97, 109, 112, 140], "doublemldidaggreg": [60, 61, 95, 96], "doublemldidc": [58, 95, 97, 109, 110, 140], "doublemldidmulti": [60, 61, 95, 96, 99, 109, 113], "doublemlframework": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 61, 95, 96, 108, 124, 140], "doublemlframwork": 30, "doublemlidid": [95, 97], "doublemlididc": [95, 97], "doublemliivm": [51, 55, 65, 78, 94, 95, 108, 109, 116, 140], "doublemlirm": [20, 22, 23, 29, 31, 32, 34, 35, 37, 38, 39, 53, 55, 62, 64, 67, 71, 73, 74, 75, 78, 80, 81, 84, 85, 93, 94, 95, 108, 109, 117, 136, 140], "doublemllpq": [82, 93, 109, 118, 140], "doublemlpaneldata": [0, 22, 24, 61, 90, 95, 96, 99], "doublemlpliv": [94, 95, 108, 109, 120, 136, 140], "doublemlplr": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 52, 55, 56, 62, 66, 68, 72, 76, 78, 80, 84, 87, 88, 93, 94, 95, 108, 109, 121, 124, 125, 131, 136, 138, 140, 141], "doublemlpolicytre": [33, 93], "doublemlpq": [79, 82, 93, 109, 119, 140], "doublemlqt": [69, 79, 82, 93, 124, 140], "doublemlresampl": [74, 75], "doublemlsmm": 140, "doublemlssm": [57, 86, 95, 109, 122, 123], "doubli": [9, 10, 26, 53, 139], "down": 85, "download": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 89, 91, 137, 138], "downward": 85, "dpg_dict": 84, "dpi": [52, 66, 80], "dr": [95, 99], "dramat": 53, "draw": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 85, 108, 140], "draw_sample_split": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 74, 75, 108], "drawn": [11, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 55, 60, 78, 79, 81, 108], "drive": [52, 66, 88], "driven": [85, 141], "drop": [53, 76, 77, 80, 89, 91, 94, 109, 110, 112, 113, 124], "dropna": 60, "dt": [60, 109, 110, 125, 127], "dt_bonu": [89, 91], "dta": [53, 61], "dtrain": 78, "dtype": [58, 60, 61, 62, 64, 71, 72, 73, 75, 77, 78, 79, 84, 86, 89, 90, 91, 93, 138], "dualiti": 77, "dubourg": [136, 138], "duchesnai": [136, 138], "due": [52, 53, 66, 67, 68, 73, 84, 85, 88, 95, 107, 125, 126, 140, 141], "duflo": [7, 8, 17, 54, 70, 77, 88, 108, 136, 139], "dummi": [29, 33, 39, 42, 43, 44, 76, 85, 93, 94, 95, 97, 140], "dummyclassifi": 42, "dummyregressor": 43, "duplic": 140, "durabl": [56, 62, 89, 91, 138], "durat": 8, "dure": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 54, 55, 56, 57, 76, 77, 78, 94, 108, 138, 140, 141], "dx": 12, "dynam": [53, 139], "e": [5, 6, 7, 8, 9, 10, 14, 17, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 52, 53, 54, 55, 57, 58, 60, 61, 64, 66, 67, 68, 70, 73, 75, 76, 77, 78, 79, 80, 83, 84, 85, 86, 88, 90, 91, 93, 94, 95, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 112, 113, 114, 115, 117, 118, 119, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141], "e20ea26": 56, "e401": [55, 78, 79, 84, 141], "e4016553": 141, "e45228": 80, "e57c": 56, "each": [14, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 54, 56, 59, 60, 61, 64, 71, 72, 75, 76, 77, 79, 80, 81, 84, 85, 87, 89, 90, 91, 94, 95, 100, 108, 124, 125, 131, 141], "earlier": [60, 141], "earn": [55, 78, 79], "earner": [55, 78, 84], "easi": [56, 109], "easier": 76, "easili": [56, 75, 76, 79, 140], "ec973f": 80, "ecolor": [59, 64, 78, 80], "econ": 139, "econml": 139, "econom": [15, 16, 18, 19, 54, 70, 77, 80, 85, 108, 139], "econometr": [7, 8, 9, 10, 17, 18, 25, 26, 53, 54, 70, 77, 88, 136, 139], "econometrica": [13, 54, 77, 80, 88, 139], "ecosystem": [136, 141], "ectj": [7, 8, 17, 54, 70, 77, 88, 136], "ed": 139, "edge_color": 66, "edgecolor": 66, "edit": [137, 139], "edu": [136, 138], "educ": [55, 78, 79, 84, 141], "ee97bda7": 56, "effect": [9, 10, 11, 14, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 46, 47, 51, 52, 53, 54, 56, 57, 58, 59, 64, 65, 66, 70, 73, 77, 81, 83, 86, 88, 92, 94, 96, 97, 99, 100, 102, 103, 104, 107, 108, 109, 117, 124, 125, 126, 138, 139, 140, 141], "effici": [95, 139], "effort": 109, "eight": [54, 77], "either": [11, 20, 22, 23, 24, 25, 29, 31, 32, 33, 34, 35, 37, 38, 39, 56, 59, 60, 70, 81, 83, 93, 94, 95, 100, 141], "eleanor": 139, "element": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 57, 58, 60, 61, 67, 68, 69, 75, 77, 79, 82, 84, 86, 95, 99, 109, 110, 112, 113, 114, 125, 130, 131, 134, 135, 140], "element_text": [54, 55], "elementari": 139, "elif": [71, 72, 81], "elig": [79, 84, 141], "eligibl": [55, 78, 84], "ell": [52, 54, 66, 70, 77, 88, 109, 120, 121, 138], "ell_0": [32, 38, 39, 52, 66, 70, 76, 88, 95, 103], "ell_2": 75, "els": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 44, 53, 54, 55, 59, 71, 72, 77, 81, 85], "em": 139, "emphas": [54, 77], "empir": [27, 28, 52, 54, 66, 77, 80, 85, 88, 95, 99, 108, 109, 124], "emploi": [54, 70, 77, 85, 109, 116], "employ": [55, 78, 79], "employe": 141, "empti": 77, "emul": [125, 126], "enabl": [21, 64, 81, 84, 93, 125, 126, 140], "enable_metadata_rout": [42, 43, 46, 47], "encapsul": [42, 43, 46, 47], "encod": 80, "end": [12, 15, 16, 52, 53, 54, 55, 57, 59, 60, 66, 69, 70, 75, 77, 78, 80, 81, 82, 86, 87, 89, 91, 94, 95, 99, 100, 108, 109, 113, 124, 138, 141], "endogen": [55, 78, 79, 141], "enet_coordinate_descent_gram": 77, "engin": [56, 139], "enrol": [55, 78, 79], "ensembl": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 62, 64, 66, 67, 68, 71, 72, 73, 75, 78, 81, 84, 85, 87, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 131, 138, 141], "ensemble_learner_pipelin": 94, "ensemble_pipe_classif": 56, "ensemble_pipe_regr": 56, "ensur": [46, 47, 54, 72, 74, 76, 77, 81], "entir": [24, 52, 55, 66, 78, 88, 125, 126], "entri": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 52, 54, 58, 60, 61, 62, 64, 66, 73, 77, 78, 79, 84, 86, 88, 89, 90, 91, 94, 136, 138, 140], "enumer": [59, 64, 69, 71, 72, 75, 77, 78, 79, 82, 87, 94, 108], "env": [77, 137], "environ": 137, "ep": 80, "epanechnikov": 40, "epsilon": [55, 58, 59, 69, 78, 82, 93, 95, 97, 100], "epsilon_": [54, 59, 60, 77], "epsilon_0": 41, "epsilon_1": 41, "epsilon_i": [11, 69, 80, 81, 82], "epsilon_sampl": 81, "epsilon_tru": [69, 82], "eqnarrai": 55, "equal": [21, 25, 29, 33, 54, 57, 60, 77, 80, 86, 93, 94, 95, 125, 133], "equat": [41, 54, 55, 77, 78, 85, 87, 124, 141], "equilibrium": [54, 77], "equiv": [95, 100, 109, 113], "equival": [70, 74, 108], "err": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 58, 60, 61, 62, 64, 65, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 80, 82, 84, 85, 86, 93, 94, 95, 96, 97, 99, 108, 109, 124, 138, 141], "error": [4, 5, 6, 20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 51, 52, 53, 55, 56, 57, 59, 66, 70, 71, 72, 75, 76, 78, 83, 85, 88, 94, 95, 105, 106, 108, 109, 124, 125, 131, 138, 140, 141], "errorbar": [59, 64, 71, 72, 76, 78, 80, 83], "erstellt": [54, 55, 56], "esim": 83, "especi": [75, 76], "essenti": 85, "est": 83, "est_method": 53, "esther": [108, 139], "estim": [4, 5, 7, 9, 10, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 52, 53, 54, 56, 59, 64, 66, 67, 68, 69, 71, 72, 74, 75, 77, 81, 83, 87, 88, 92, 93, 94, 95, 96, 97, 99, 102, 104, 110, 112, 115, 118, 119, 123, 125, 126, 131, 136, 139, 140], "estimatior": [4, 5], "estimator_list": 76, "et": [7, 8, 11, 13, 16, 17, 52, 54, 55, 56, 58, 66, 67, 68, 69, 70, 71, 72, 75, 77, 78, 79, 82, 84, 88, 95, 97, 100, 108, 109, 115, 117, 118, 119, 124, 125, 126, 135, 136, 138, 140], "eta": [27, 28, 52, 54, 55, 59, 76, 77, 78, 82, 83, 87, 93, 95, 99, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 135, 138, 141], "eta1": 80, "eta2": 80, "eta_": [124, 125, 135], "eta_0": [40, 87, 95, 99, 109, 124], "eta_d": [83, 95], "eta_i": [11, 25, 59, 60, 81, 82, 83, 95], "eta_sampl": 81, "eta_tru": 82, "etc": [20, 21, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 75, 76, 77, 140], "ev": [52, 66, 88], "eval": [24, 56, 60, 61, 94, 95, 99, 109, 113, 125, 130], "eval_metr": [55, 78, 141], "eval_pr": 53, "eval_predict": 53, "evalu": [13, 20, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 56, 59, 60, 61, 67, 68, 69, 73, 74, 79, 82, 84, 87, 95, 99, 100, 139, 140], "evaluate_learn": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 75, 76, 94, 140], "evalut": 94, "even": [55, 56, 60, 78, 80, 83, 94, 95, 141], "event": [95, 96], "eventstudi": [24, 60, 61, 95, 96], "eventu": [54, 77], "everi": [54, 77], "everyth": 136, "evid": [73, 76], "exact": [74, 85], "exactli": [83, 85, 95], "exampl": [4, 5, 6, 11, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 86, 87, 88, 93, 94, 95, 96, 99, 107, 108, 109, 124, 125, 131, 136, 138, 140, 141], "example_attgt": 53, "example_attgt_dml_eval_linear": 53, "example_attgt_dml_eval_rf": 53, "example_attgt_dml_linear": 53, "example_attgt_dml_rf": 53, "except": [43, 47, 70, 85, 140], "excess": 75, "exclud": 48, "exclus": [29, 33, 39, 71, 72, 93], "execut": [56, 141], "exemplarili": 138, "exemplatori": 81, "exhaust": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 94], "exhibit": [54, 77], "exist": [42, 43, 46, 47, 74, 95, 97, 100, 125, 135], "exogen": [55, 78, 79, 95, 141], "exp": [9, 10, 11, 13, 14, 17, 25, 26, 52, 59, 66, 67, 68, 71, 72, 80, 81, 88], "expect": [9, 10, 43, 47, 53, 57, 58, 60, 64, 73, 75, 76, 83, 85, 86, 93, 95, 108, 124, 125, 132, 138], "experi": [8, 12, 13, 52, 55, 66, 78, 85, 88, 89, 91, 108, 138, 139], "experiment": [20, 22, 23, 24, 25, 26, 109, 110, 112, 113, 125, 127, 129, 130], "expertis": 85, "explain": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 84, 125, 126, 134, 135], "explan": [54, 58, 77, 84, 125, 134, 136, 141], "explanatori": [85, 124], "explicit": 85, "explicitli": [60, 61, 73, 141], "exploit": [52, 66, 88, 95, 141], "explor": 76, "exponenti": 124, "export": [76, 140], "expos": [95, 100], "exposur": [6, 25, 59, 60, 61], "express": [54, 70, 83, 125, 135], "ext": 24, "extend": [85, 91, 94, 136, 140], "extendend": [125, 135], "extens": [94, 109, 136, 139, 140], "extent": 70, "extern": [52, 66, 74, 76, 92, 125, 126, 140], "external_predict": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 66, 94], "externalptr": 55, "extra": 56, "extract": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 76], "extralearn": 56, "extrem": [55, 78], "ey": 70, "f": [55, 56, 58, 59, 60, 64, 66, 69, 70, 75, 77, 78, 79, 81, 82, 84, 85, 86, 94, 125, 135, 136, 138], "f00584a57972": 56, "f1718fdeb9b0": 56, "f2e7": 56, "f3d24993": 56, "f6ebc": 80, "f_": [9, 25, 26, 59, 93], "f_loc": [69, 82], "f_p": 59, "f_scale": [69, 82], "f_t": 60, "f_x": 95, "face_color": 66, "facet_wrap": 55, "facilit": 76, "fact": [55, 78, 79], "factor": [41, 52, 53, 54, 55, 56, 66, 75, 88, 94, 141], "faculti": 139, "fail": 140, "fair": 75, "fake": [51, 65], "fals": [4, 5, 6, 7, 8, 9, 11, 14, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 52, 55, 56, 57, 58, 60, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 85, 86, 89, 91, 94, 95, 108, 109, 110, 112, 113, 124, 125, 127, 129, 130, 141], "famili": [55, 78, 94], "fanci": 53, "far": [55, 78], "farbmach": 12, "fast": [75, 81, 94], "faster": 70, "fb5c25fa": 56, "fc9e": 56, "fd8a": 56, "featur": [7, 8, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 43, 45, 47, 53, 60, 62, 73, 74, 75, 78, 81, 93, 94, 95], "featureless": [56, 94], "features_bas": [55, 78, 79, 84], "features_flex": 55, "featureunion": 56, "februari": [60, 85], "femal": [56, 62, 89, 91, 138], "fern\u00e1ndez": [13, 108, 139], "fetch": [55, 77, 78, 79, 89, 91], "fetch_401k": [55, 78, 79, 84, 141], "fetch_bonu": [56, 62, 89, 91, 138], "few": [55, 78, 79], "ff7f0e": 59, "field": [54, 77, 94, 141], "fifteenth": 139, "fifth": [54, 60], "fig": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 59, 60, 61, 64, 67, 68, 69, 70, 74, 75, 76, 79, 80, 82, 83, 85], "fig_al": 66, "fig_dml": 66, "fig_non_orth": 66, "fig_orth_nosplit": 66, "fig_po_al": 66, "fig_po_dml": 66, "fig_po_nosplit": 66, "figsiz": [21, 24, 59, 60, 62, 64, 67, 68, 69, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83], "figur": [17, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 54, 59, 60, 62, 64, 66, 67, 68, 69, 70, 71, 72, 74, 76, 77, 78, 79, 82, 85, 88], "figure_format": 80, "file": [7, 8, 70, 80, 139, 140], "filenam": 52, "fill": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 54, 55, 57, 58, 75, 78, 86], "fill_between": [67, 68, 69, 74, 79, 82], "fill_valu": 75, "fillna": 60, "filter": 56, "filterwarn": 66, "final": [52, 56, 57, 59, 60, 61, 64, 66, 67, 68, 69, 71, 72, 73, 79, 82, 83, 86, 88, 95, 107, 109, 113, 141], "final_estim": 83, "financi": [7, 84, 141], "find": [55, 59, 78, 85, 93, 94, 141], "finish": 56, "finit": [52, 55], "firm": [54, 77, 84], "firmid": 77, "first": [6, 9, 10, 16, 20, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 71, 72, 73, 74, 75, 77, 78, 79, 81, 82, 83, 85, 86, 88, 93, 95, 96, 99, 100, 108, 124, 125, 131, 137, 138, 140, 141], "fit": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 92, 93, 94, 95, 96, 97, 99, 100, 109, 110, 112, 123, 124, 125, 126, 131, 136, 140, 141], "fit_arg": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39], "fit_transform": [74, 77, 78], "five": 77, "fix": [59, 60, 75, 140], "flag": [26, 108, 137], "flake8": 140, "flamlclassifierdoubleml": 76, "flamlregressordoubleml": 76, "flatten": [76, 80], "flexibl": [40, 51, 53, 55, 56, 58, 65, 78, 95, 136, 139, 140, 141], "flexibli": [55, 78, 84], "float": [9, 10, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 60, 61, 90, 91], "float32": [78, 79, 84], "float64": [58, 60, 61, 62, 64, 72, 73, 77, 78, 84, 86, 89, 90, 91, 94, 138], "floor": 56, "floor_divid": 77, "flt": 56, "flush": 52, "fmt": [59, 64, 71, 72, 76, 78, 80, 83], "fobj": 78, "focu": [54, 55, 74, 77, 78, 79, 85, 93, 95, 97, 100, 107, 141], "focus": [79, 84, 85, 141], "fold": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 54, 55, 56, 57, 58, 60, 61, 75, 77, 78, 79, 84, 86, 87, 92, 94, 95, 97, 99, 109, 110, 112, 124, 138, 141], "follow": [9, 10, 11, 14, 25, 26, 52, 54, 55, 57, 58, 59, 60, 61, 66, 67, 68, 69, 71, 72, 76, 77, 78, 79, 82, 83, 84, 85, 86, 88, 89, 91, 93, 94, 95, 96, 99, 108, 109, 113, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135, 137, 138, 141], "font_scal": [77, 78, 79], "fontsiz": [60, 69, 79, 82], "force_all_d_finit": [5, 6], "force_all_x_finit": [4, 5, 6], "forest": [12, 51, 52, 53, 55, 56, 58, 65, 66, 73, 75, 78, 84, 88, 94, 138, 141], "forest_summari": 78, "forg": [137, 139, 140], "form": [9, 10, 11, 20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 46, 47, 55, 57, 58, 59, 67, 68, 69, 71, 72, 73, 75, 78, 82, 83, 84, 86, 93, 95, 96, 97, 100, 103, 104, 105, 106, 109, 113, 114, 117, 125, 126, 131, 132, 133, 134, 135, 137, 138], "format": [6, 24, 66, 73, 125, 131], "formula": [54, 55, 77, 78, 83, 85, 140], "formula_flex": 55, "forschungsgemeinschaft": 136, "forthcom": [85, 139], "forum": 140, "forward": [33, 45], "found": [61, 67, 68, 70, 71, 72, 76, 88, 89, 91, 94, 95, 107, 138], "foundat": [136, 139], "four": [55, 75, 78, 140], "fourth": [54, 77], "frac": [9, 10, 12, 13, 15, 17, 18, 19, 25, 26, 28, 32, 43, 47, 52, 54, 56, 59, 66, 70, 73, 77, 80, 83, 87, 88, 93, 95, 99, 103, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 122, 123, 124, 125, 126, 127, 129, 130, 132, 133, 134, 135], "fraction": 56, "frame": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 51, 52, 54, 55, 57, 58, 60, 61, 62, 64, 67, 68, 71, 72, 73, 77, 78, 79, 80, 81, 84, 86, 88, 89, 90, 91, 138, 141], "framealpha": 60, "frameon": 60, "framework": [21, 24, 28, 52, 54, 56, 66, 75, 76, 77, 80, 85, 88, 94, 124, 136, 138, 140, 141], "freez": 137, "fribourg": 139, "friendli": [60, 64], "from": [4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 47, 51, 52, 54, 55, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 131, 138, 140, 141], "from_arrai": [4, 5, 6, 37, 40, 58, 59, 66, 69, 82, 88, 89, 91, 94, 124, 138], "from_product": 77, "front": 64, "fr\u00e9chet": [125, 135], "fs_kernel": [40, 95], "fs_specif": [40, 95], "fsize": [55, 78, 79, 84, 141], "full": [58, 59, 64, 66, 69, 71, 72, 75, 78, 79, 82, 83, 86, 88, 95], "fulli": [33, 55, 63, 76, 78, 95, 104], "fun": 52, "func": 53, "function": [0, 4, 5, 6, 17, 18, 20, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 55, 56, 57, 58, 60, 61, 65, 66, 67, 68, 69, 74, 75, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 92, 94, 95, 96, 97, 99, 100, 103, 108, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 129, 130, 135, 136, 139, 140, 141], "fund": [55, 78, 79, 136], "further": [9, 10, 11, 14, 16, 21, 24, 25, 26, 54, 56, 57, 58, 59, 60, 61, 64, 67, 68, 69, 73, 74, 75, 77, 79, 81, 82, 83, 84, 85, 86, 94, 95, 97, 99, 107, 109, 115, 118, 119, 122, 123, 124, 125, 126, 131, 134, 135, 136, 138, 140, 141], "furthermor": [66, 90, 91, 109, 114, 117], "futur": [60, 61, 95, 109, 125], "futurewarn": [60, 61, 72], "fuzzi": [40, 41], "g": [5, 6, 9, 10, 20, 22, 23, 24, 25, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 52, 53, 56, 58, 59, 60, 61, 62, 66, 67, 68, 70, 73, 75, 79, 80, 81, 84, 86, 88, 90, 91, 93, 94, 95, 96, 97, 99, 100, 109, 110, 112, 113, 114, 116, 117, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 141], "g_": [41, 64, 95, 99, 109, 110, 112, 113, 115, 118, 119, 124], "g_0": [17, 18, 20, 22, 23, 24, 29, 32, 33, 35, 38, 39, 40, 41, 52, 54, 55, 66, 75, 77, 78, 88, 93, 94, 95, 101, 102, 103, 104, 105, 106, 109, 113, 114, 122, 123, 125, 132, 133, 135, 138, 141], "g_1": [41, 75], "g_all": [52, 55], "g_all_po": 52, "g_ci": 55, "g_d": [109, 115, 119], "g_dml": 52, "g_dml_po": 52, "g_hat": [38, 39, 52, 66, 109], "g_hat0": [32, 33], "g_hat1": [32, 33], "g_i": [25, 95, 99, 100, 109, 113], "g_k": 93, "g_nonorth": 52, "g_nosplit": 52, "g_nosplit_po": 52, "g_valu": 22, "g_x": 59, "gain": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 75, 125, 126, 133, 140], "gain_statist": 140, "galleri": [88, 93, 94, 95, 96, 99, 107, 136, 140], "gama": 76, "gamma": [15, 18, 19, 54, 77, 80, 81, 83, 85, 95, 109, 115, 118], "gamma_0": [11, 57, 81, 86, 109, 115, 118], "gamma_a": [9, 10, 85], "gamma_bench": 85, "gamma_v": 85, "gap": [77, 85], "gapo": 29, "gate": [29, 33, 39, 44, 80, 81, 92, 140], "gate_obj": 93, "gatet": 93, "gaussian": [34, 35, 36, 52, 66, 88, 93, 94, 124, 139], "ge": [9, 11, 26, 73, 81, 93, 95, 99, 100], "geer": 139, "gelbach": [54, 77], "gener": [0, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 43, 47, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 89, 91, 92, 93, 94, 95, 99, 100, 101, 108, 109, 113, 114, 117, 124, 126, 127, 129, 130, 132, 133, 135, 139, 140, 141], "generate_treat": 82, "geom_bar": 55, "geom_dens": [55, 57], "geom_errorbar": 55, "geom_funct": 52, "geom_histogram": 52, "geom_hlin": 55, "geom_point": 55, "geom_til": 54, "geom_vlin": [52, 57], "geq": [25, 83, 95], "german": 136, "get": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 56, 60, 61, 64, 75, 80, 84, 85, 125, 126, 136, 137], "get_dummi": 80, "get_feature_names_out": [74, 77, 78], "get_legend_handles_label": 64, "get_level_valu": 76, "get_logg": [52, 53, 54, 55, 56, 57, 87, 94, 95, 108, 109, 124, 138], "get_metadata_rout": [42, 43, 46, 47], "get_param": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 76, 94], "get_ylim": 74, "ggdid": 53, "ggplot": [52, 54, 55, 57], "ggplot2": [52, 54, 55, 57], "ggsave": 52, "ggtitl": 55, "gh": 140, "git": 137, "github": [53, 55, 70, 76, 80, 136, 139, 140], "githubusercont": [61, 70], "give": [55, 74, 78], "given": [9, 10, 13, 17, 18, 20, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 46, 47, 52, 54, 57, 59, 61, 64, 66, 71, 72, 77, 79, 80, 83, 85, 86, 88, 93, 95, 99, 109, 114, 124, 125, 131, 132, 133, 134, 135, 138, 140], "glmnet": [55, 56, 94, 140], "global": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 46, 47, 94, 95], "globalclassifi": 83, "globallearn": 83, "globalregressor": 83, "glrn": 56, "glrn_lasso": 56, "gname": 53, "go": [67, 68, 70, 74, 76, 83, 85], "goal": [64, 71, 72, 95], "goe": 95, "goldman": 139, "good": [70, 125, 126, 141], "gradient": [55, 78], "gradientboostingclassifi": 75, "gradientboostingregressor": 75, "gradual": 85, "gramfort": [136, 138], "graph": [56, 57, 86, 141], "graph_ensemble_classif": 56, "graph_ensemble_regr": 56, "graph_obj": 83, "graph_object": [67, 68, 70, 85], "graphlearn": [56, 94], "grasp": [64, 125, 126], "great": [59, 141], "greater": 141, "green": [52, 67, 68, 69, 82], "greg": 139, "grei": [55, 64], "grenand": 139, "grey50": 54, "grid": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 54, 56, 60, 64, 67, 68, 69, 70, 74, 79, 80, 82, 85, 94, 125, 131], "grid_arrai": [67, 68], "grid_basi": 74, "grid_bound": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 85], "grid_search": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 56, 94], "grid_siz": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 67, 68], "gridextra": 54, "gridsearchcv": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 94], "grisel": [136, 138], "grob": 54, "group": [6, 22, 24, 25, 29, 33, 39, 51, 53, 64, 65, 73, 74, 79, 80, 81, 85, 92, 95, 96, 99, 100, 109, 113, 125, 130], "group_0": 93, "group_1": [71, 72, 93], "group_2": [71, 72, 93], "group_3": [71, 72], "group_effect": 81, "group_ind": 73, "group_treat": 73, "groupbi": [60, 70, 78], "gruber": 12, "gt": [51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 64, 65, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 89, 91, 138], "gt_combin": [24, 60, 95, 96, 99], "gt_dict": 60, "guarante": [54, 77], "guber": 12, "guess": [84, 125, 126], "guid": [27, 28, 42, 43, 46, 47, 52, 53, 54, 56, 59, 60, 61, 64, 66, 73, 74, 77, 83, 84, 94, 136, 138, 140], "guidelin": 140, "gunion": [56, 94], "gxidclusterperiodytreat": 53, "h": [9, 10, 12, 16, 25, 26, 53, 54, 77, 83, 90, 91, 95, 139], "h20": 76, "h_0": [60, 64, 73, 74, 84, 85, 125, 131, 141], "h_f": [40, 95], "ha": [20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 45, 46, 47, 52, 53, 54, 55, 66, 70, 75, 76, 77, 78, 79, 80, 83, 84, 85, 93, 94, 95, 97, 100, 125, 126, 131, 132, 133, 134, 135, 141], "half": [52, 66, 80, 88, 108], "hand": [40, 75, 76, 80, 141], "handbook": 80, "handl": [53, 64, 75, 90, 91, 94, 140], "hansen": [7, 8, 13, 15, 17, 54, 70, 77, 88, 136, 139], "happend": 75, "hard": [84, 125, 126], "harold": 139, "harsh": [42, 46], "hasn": [21, 24, 60, 61], "hat": [52, 54, 66, 70, 73, 77, 80, 83, 87, 88, 93, 95, 108, 109, 124, 125, 126, 131, 134], "have": [11, 14, 24, 29, 30, 33, 36, 39, 44, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 67, 68, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 89, 91, 93, 94, 95, 109, 113, 124, 125, 126, 132, 135, 137, 138, 140, 141], "hazlett": [85, 125, 126], "hc": [53, 139], "hc0": [44, 140], "hdm": [54, 77], "he": [57, 86], "head": [53, 54, 56, 60, 61, 62, 67, 68, 71, 72, 74, 76, 77, 78, 80, 83, 85, 89, 91, 93, 138], "heat": [54, 77], "heatmap": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 54, 77, 85], "heavili": 75, "hei": 139, "height": [21, 24, 52, 54, 70, 76], "help": [53, 55, 69, 75, 79, 81, 85, 108, 141], "helper": 140, "henc": [53, 55, 56, 78, 85, 94, 109, 141], "here": [34, 35, 36, 53, 54, 55, 56, 57, 58, 59, 60, 61, 67, 68, 69, 71, 72, 73, 75, 77, 78, 79, 81, 82, 83, 85, 86, 89, 91, 94, 95, 137], "heterogen": [11, 25, 33, 55, 73, 78, 79, 81, 92, 95, 104, 108, 139, 140, 141], "heteroskedast": [71, 72], "heurist": [52, 66, 88], "high": [13, 38, 39, 55, 59, 70, 78, 79, 87, 95, 105, 106, 124, 136, 138, 139], "higher": [53, 55, 70, 78, 79, 80, 83, 140, 141], "highli": [55, 78, 136], "highlight": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 58, 74, 76, 85, 140], "highlightcolor": [67, 68], "hint": 76, "hispan": 62, "hist": 64, "hist_e401": 55, "hist_p401": 55, "histogram": 64, "histplot": 66, "hjust": 55, "hline": [89, 91, 124, 138, 141], "hold": [19, 54, 55, 57, 76, 77, 78, 86, 93, 94, 95, 99], "holdout": [94, 108], "holm": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39], "home": [55, 60, 61, 78, 85], "homogen": 95, "hopefulli": 79, "horizont": [54, 59, 77], "hostedtoolcach": [60, 61, 78], "hot": 80, "hotstart_backward": [56, 94], "hotstart_forward": [56, 94], "household": [55, 78, 79, 84], "how": [21, 25, 42, 43, 46, 47, 51, 53, 54, 55, 57, 58, 59, 60, 61, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 81, 82, 83, 84, 85, 86, 88, 94, 95, 136, 137], "howev": [52, 55, 57, 66, 76, 78, 83, 85, 86, 88, 95, 141], "hown": [55, 78, 79, 84, 141], "hpwt": [54, 77], "hpwt0": 54, "hpwtairmpdspac": 54, "href": 136, "hspace": 75, "hstack": [37, 59], "html": [56, 72, 136, 138, 140], "http": [12, 18, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 94, 136, 137, 138, 140], "huber": [19, 57, 86, 95, 107, 109, 122, 123, 139], "hue": [60, 78], "huge": 75, "hugo": 139, "husd": [56, 62, 89, 91, 138], "hyperparamet": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 55, 56, 62, 70, 75, 76, 78, 92, 138], "hypothes": [124, 139], "hypothesi": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 78, 84, 125, 131, 139], "hypothet": 85, "i": [4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 112, 113, 114, 115, 117, 118, 119, 123, 124, 125, 126, 131, 132, 133, 135, 136, 137, 138, 140, 141], "i0": [58, 59, 95, 97], "i03": 136, "i1": [58, 95, 97], "i_": [15, 77, 81], "i_1": [54, 77], "i_2": [54, 77], "i_3": [54, 77], "i_4": 59, "i_est": 66, "i_fold": 54, "i_k": [54, 77, 87, 108, 124], "i_learn": 75, "i_level": 64, "i_rep": [52, 57, 58, 66, 75, 86, 88], "i_split": 77, "i_train": 66, "icp": 139, "id": [6, 24, 53, 54, 56, 60, 61, 77, 90, 91, 95, 96, 99], "id_col": [6, 24, 60, 61, 90, 91, 95, 96, 99], "id_var": 77, "idea": [55, 56, 78, 79, 85, 94, 95, 125, 126, 141], "ident": [9, 10, 11, 14, 15, 25, 26, 45, 56, 64, 74, 76, 83, 94, 95, 100, 109, 117, 125, 131], "identfi": 85, "identif": [60, 61, 83, 95, 141], "identifi": [6, 54, 55, 58, 73, 77, 78, 79, 83, 85, 90, 91, 93, 95, 97, 99, 100, 107, 125, 135, 140], "identifii": 93, "idnam": 53, "idx_gt_att": 24, "idx_tau": [69, 79, 82], "idx_treat": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 64, 125, 131], "ieee": 139, "ifels": 53, "ignor": [42, 43, 46, 47, 66, 83], "ii": [54, 77], "iid": [60, 95, 97], "iivm": [12, 27, 28, 32, 79, 87, 93, 103, 116, 136, 140], "iivm_summari": 78, "iivmglmnet": 55, "iivmrang": 55, "iivmrpart": 55, "iivmxgboost11861": 55, "ij": [16, 54, 57, 64, 77, 86], "ilia": 139, "illustr": [52, 54, 55, 56, 57, 58, 59, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 81, 82, 84, 85, 86, 88, 94, 141], "iloc": [58, 59, 60, 61, 64, 75, 77, 80], "immedi": 137, "immun": [108, 139], "impact": [51, 65, 75, 80, 84], "implement": [20, 22, 23, 24, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 46, 47, 52, 53, 54, 55, 56, 57, 58, 60, 61, 66, 70, 74, 75, 77, 78, 80, 83, 84, 85, 86, 88, 92, 93, 94, 96, 97, 98, 99, 100, 107, 108, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 136, 138, 139, 140, 141], "impli": [9, 10, 54, 55, 77, 78, 79, 83, 93, 95, 100, 125, 127, 129, 130, 132, 133], "implicitli": [95, 99], "implment": [59, 95, 96], "import": [4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 131, 137, 138, 140, 141], "importlib": 70, "impos": 85, "improv": [58, 60, 75, 81, 95, 140], "in_sample_norm": [20, 22, 23, 24, 58, 109, 110, 112, 113, 125, 127, 129, 130], "inbuild": 75, "inbuilt": 75, "inc": [55, 78, 79, 84, 141], "includ": [20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 53, 55, 59, 60, 61, 64, 71, 72, 74, 78, 83, 84, 85, 93, 95, 124, 125, 131, 132, 133, 135, 137, 140, 141], "include_bia": [74, 77, 78], "include_never_tr": 25, "include_scenario": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 85], "incom": [55, 78, 79, 81, 84, 141], "incorpor": [56, 84, 125, 131], "increas": [60, 73, 75, 77, 85, 141], "increment": 140, "ind": 78, "independ": [9, 10, 11, 20, 22, 23, 24, 26, 41, 54, 56, 59, 73, 77, 81, 95, 100, 107, 109, 110, 112, 113, 140], "index": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 54, 59, 62, 66, 70, 71, 72, 76, 77, 78, 80, 81, 88, 89, 91, 108, 109, 110, 112, 113, 138], "index_col": 70, "india": [108, 139], "indic": [4, 5, 6, 11, 14, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 54, 55, 57, 59, 60, 61, 73, 77, 78, 79, 83, 85, 86, 87, 89, 91, 93, 95, 97, 100, 101, 107, 108], "individu": [25, 29, 33, 46, 47, 53, 55, 59, 60, 64, 71, 72, 73, 76, 78, 79, 83, 84, 93, 95, 141], "individual_df": 59, "induc": [92, 108], "industri": [54, 77], "inf": [4, 5, 6, 53, 60, 61], "inf_model": 109, "infer": [13, 15, 51, 52, 54, 65, 66, 70, 75, 76, 77, 88, 92, 95, 108, 136, 138, 139, 140], "inferenti": 141, "infinit": [4, 5, 6, 140], "influenc": [43, 47, 95], "info": [51, 56, 58, 60, 61, 62, 64, 73, 76, 77, 78, 79, 84, 86, 89, 90, 91, 138, 140, 141], "inform": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 46, 47, 51, 56, 60, 61, 65, 67, 68, 75, 83, 84, 85, 95, 96, 125, 126, 139], "infti": [52, 66, 88, 95, 100], "inher": 85, "inherit": [80, 90, 91, 140], "initi": [4, 5, 6, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 55, 56, 57, 58, 60, 61, 69, 78, 79, 82, 83, 84, 85, 86, 89, 90, 91, 93, 94, 95, 108, 138, 140, 141], "inlin": [62, 80], "inlinebackend": 80, "inner": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 94], "innermost": 94, "input": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 56, 84, 87, 95, 99, 124, 125, 126, 131], "insensit": 95, "insid": [42, 43, 46, 47], "insight": [70, 85], "insignific": 84, "inspect": 138, "inspir": [9, 12, 13, 19, 60, 85], "instal": [55, 76, 83, 95, 140], "install_github": 137, "instanc": [46, 47, 55, 56, 78, 94], "instanti": [54, 55, 77, 78, 94, 108], "instead": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 51, 53, 55, 60, 61, 64, 65, 72, 73, 76, 78, 79, 93, 94, 95, 109, 113, 125, 127, 129, 130, 133, 134, 140], "instruct": [137, 140], "instrument": [4, 5, 6, 7, 12, 15, 32, 38, 54, 55, 56, 57, 58, 60, 61, 62, 64, 73, 77, 78, 79, 82, 84, 86, 89, 90, 91, 94, 95, 97, 99, 103, 105, 109, 118, 124, 138, 141], "instrument_effect": 51, "instrument_impact": 65, "insuffienct": 76, "int": [9, 10, 11, 14, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 53, 54, 55, 58, 65, 69, 81, 82, 85, 86, 90, 91], "int64": [60, 61, 62, 75, 77, 89, 90, 91, 138], "int8": [78, 79, 84], "integ": [26, 56, 94], "integr": [76, 85, 125, 135, 140], "intend": [40, 56, 85, 141], "intent": [95, 141], "inter": 94, "interact": [9, 12, 13, 14, 29, 30, 32, 33, 40, 41, 64, 85, 92, 94, 103, 104, 132, 133, 136, 140, 141], "interchang": 124, "interest": [9, 10, 32, 33, 38, 39, 52, 55, 57, 58, 66, 70, 78, 79, 83, 86, 88, 93, 95, 97, 99, 101, 102, 103, 104, 105, 106, 107, 109, 124, 138, 141], "interfac": [53, 55, 56, 89, 91, 94, 108, 138], "intermedi": [72, 85], "intern": [53, 55, 56, 64, 76, 79, 94, 139], "internet": [55, 78, 79], "interpret": [60, 61, 71, 72, 85, 93, 95, 99, 125, 126, 132, 133, 134, 135, 137, 141], "intersect": [85, 125, 131, 140], "interv": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 53, 54, 55, 57, 58, 60, 61, 64, 67, 68, 69, 71, 72, 74, 77, 79, 82, 83, 84, 86, 92, 93, 108, 109, 125, 131, 138, 139, 141], "intial": 83, "introduc": [52, 66, 88, 89, 91, 124, 140, 141], "introduct": [52, 54, 56, 66, 77, 79, 84, 94, 95, 97, 100, 125, 126], "introductori": [53, 85], "intrument": [57, 86], "intspecifi": 40, "intuit": 85, "inuidur1": [56, 62, 89, 91, 138], "inuidur1femaleblackothracedep1dep2q2q3q4q5q6agelt35agegt54durablelusdhusdtg": [56, 89, 91, 138], "inuidur2": [62, 89, 91, 138], "inv_sigmoid": 80, "invalid": [52, 66, 88], "invari": [95, 97], "invers": [29, 31, 32, 33, 34, 35, 36, 37, 57, 86, 125, 132, 133], "invert_yaxi": 77, "investig": [70, 76, 85], "involv": [93, 94, 109, 141], "io": [80, 140], "ipw_norm": 140, "ipykernel_45969": 72, "ipynb": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "ira": [55, 78, 79], "irm": [0, 13, 14, 20, 22, 23, 27, 28, 38, 39, 44, 45, 75, 85, 87, 92, 94, 99, 104, 117, 132, 133, 136, 140, 141], "irm_summari": 78, "irmglmnet": 55, "irmrang": 55, "irmrpart": 55, "irmxgboost8047": 55, "irrespect": 85, "irrevers": [95, 100], "is_classifi": [20, 22, 23, 29, 32, 33, 39], "is_gat": [29, 33, 39, 44], "isfinit": [60, 61], "isnan": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 75, 94], "isoton": 85, "isotonicregress": 85, "issn": 70, "issu": [21, 24, 85, 136, 139, 140], "ite": [60, 64, 71, 72, 73], "ite_lower_quantil": 60, "ite_mean": 60, "ite_upper_quantil": 60, "item": [32, 78, 87, 94, 108], "iter": [40, 51, 57, 58, 77, 78, 83, 86, 94, 124, 141], "itertool": 70, "its": [42, 43, 85, 87, 93, 94, 95, 97, 99, 108, 109, 124], "iv": [12, 15, 16, 32, 38, 39, 52, 54, 66, 77, 88, 89, 91, 103, 105, 120, 121, 125, 134, 136, 140, 141], "iv_2": 51, "iv_var": [54, 77], "iv\u00e1n": [108, 139], "j": [7, 8, 9, 10, 12, 13, 15, 16, 17, 18, 19, 25, 26, 52, 53, 54, 56, 57, 64, 66, 70, 77, 80, 86, 88, 94, 95, 101, 124, 136, 138], "j_": [54, 77], "j_0": 124, "j_1": [54, 77], "j_2": [54, 77], "j_3": [54, 77], "j_k": [54, 77], "jame": 139, "janari": 60, "janni": [55, 78], "januari": 60, "jasenakova": 140, "javanmard": 139, "jbe": [54, 77], "jeconom": [9, 10, 25, 26, 53], "jerzi": 139, "jia": 85, "jitter": 24, "jitter_valu": 24, "jk": [95, 102], "jmlr": [56, 136, 138, 140], "job": [55, 78, 79], "john": 139, "joint": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 60, 61, 64, 67, 68, 69, 71, 72, 74, 79, 82, 95, 97, 124, 140, 141], "jointli": [82, 93], "jonathan": 139, "joss": [56, 94, 136, 138], "journal": [7, 8, 9, 10, 16, 17, 19, 25, 26, 53, 54, 56, 70, 77, 80, 85, 88, 94, 136, 138, 139, 140], "jss": 136, "jump": [81, 83, 95], "jun": [53, 139], "jupyt": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "juraj": 139, "just": [53, 56, 58, 59, 60, 64, 69, 71, 72, 73, 74, 81, 82, 95, 109, 110, 112, 113, 125, 126], "justif": [108, 125, 126], "k": [7, 10, 12, 13, 15, 16, 17, 19, 25, 26, 52, 54, 56, 66, 75, 76, 77, 83, 87, 88, 92, 93, 95, 124, 141], "k_h": [83, 95], "kaggl": [55, 78], "kallu": [69, 79, 82, 84, 109, 115, 118, 119, 139], "kappa": 95, "kato": [16, 54, 77, 124, 139], "kb": [58, 61, 64, 73, 77, 78, 79, 84, 89, 90, 91, 138], "kde": [34, 35, 36, 78], "kdeplot": [58, 75, 86], "kdeunivari": [34, 35, 36], "kecsk\u00e9sov\u00e1": 140, "keep": [43, 47, 53, 72, 74, 85, 141], "kei": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 45, 54, 55, 67, 68, 71, 72, 76, 77, 78, 79, 83, 85, 94, 95, 109, 125, 131, 140], "keith": 139, "kengo": 139, "kernel": [34, 35, 36, 40, 43, 47, 83, 95], "kernel_regress": 83, "kernelreg": 83, "keyword": [16, 17, 18, 25, 26, 29, 33, 39, 41, 44], "kf": 108, "kfold": [77, 108], "kind": [51, 65, 78], "kj": [10, 12, 13, 15, 16, 17, 19, 25, 26, 52, 54, 66, 77, 88], "klaassen": [12, 70, 75, 76, 85, 136, 139], "klaa\u00dfen": 12, "knau": 139, "know": [58, 81], "knowledg": [51, 65, 75, 80, 81], "known": [73, 75, 83, 85, 94, 95, 100], "kohei": 139, "kotthof": 56, "kotthoff": [56, 94, 136, 138], "krueger": 80, "kueck": [55, 78], "kurz": [136, 139, 140], "kwarg": [9, 10, 14, 16, 17, 18, 25, 26, 29, 33, 39, 40, 41, 42, 44, 76, 95], "l": [54, 56, 57, 62, 67, 68, 77, 85, 86, 94, 125, 134, 136, 138], "l1": [78, 86, 95], "l_hat": [38, 39, 52, 66, 109], "lab": 57, "label": [21, 24, 42, 46, 59, 64, 66, 67, 68, 69, 71, 72, 74, 76, 79, 80, 82, 83], "labor": 80, "laffer": 139, "laff\u00e9r": [19, 57, 86, 95, 107, 109, 122, 123], "lal": [80, 140], "lambda": [54, 55, 56, 57, 60, 78, 80, 81, 94, 95, 109, 110, 124, 138], "lambda_": 70, "lambda_0": [109, 110], "lambda_t": 26, "land": 81, "lang": [56, 94, 136, 138], "langl": [11, 81], "lappli": 108, "larg": [52, 66, 73, 75, 76, 80, 85, 95], "larger": [33, 53, 83, 85, 125, 131], "largest": 75, "largli": 75, "lasso": [54, 55, 56, 57, 78, 86, 94, 138, 139], "lasso_class": [55, 78], "lasso_pip": [56, 94], "lasso_summari": 78, "lassocv": [37, 70, 77, 78, 86, 94, 95, 124, 138], "last": [26, 56, 137], "late": [32, 51, 55, 78, 95, 103, 109, 116], "latent": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 84, 125, 134, 135], "later": [55, 56, 83, 85, 94, 141], "latter": [46, 47, 95], "layout": 70, "lbrace": [12, 13, 19, 32, 33, 54, 77, 87, 95, 101, 103, 104, 108, 109, 114, 124, 125, 132], "ldot": [38, 39, 54, 57, 77, 86, 87, 95, 105, 106, 108, 124, 138], "le": [26, 58, 81, 93, 95, 97, 109, 118, 119], "lead": [53, 85, 95], "leadsto": 124, "lear": [56, 94, 136, 138], "learn": [4, 5, 6, 7, 8, 16, 17, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 55, 56, 62, 64, 65, 69, 70, 74, 75, 76, 78, 79, 80, 82, 83, 85, 89, 91, 92, 94, 108, 109, 124, 125, 126, 140, 141], "learner": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55, 57, 58, 60, 61, 66, 67, 68, 70, 77, 78, 79, 84, 85, 86, 87, 88, 92, 95, 97, 99, 108, 109, 124, 125, 131, 140, 141], "learner_class": [37, 140], "learner_cv": 56, "learner_forest_classif": 56, "learner_forest_regr": 56, "learner_l": 84, "learner_lasso": 56, "learner_list": 75, "learner_m": 84, "learner_nam": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 94], "learner_param_v": 56, "learner_rf": 124, "learnerclassif": 56, "learnerregr": 56, "learnerregrcvglmnet": 56, "learnerregrrang": [56, 94], "learning_r": [60, 66, 69, 79, 82, 83, 85, 88], "least": [51, 55, 65, 78, 79, 84, 95, 99, 108], "leav": [57, 85, 86], "left": [12, 13, 15, 16, 19, 25, 52, 54, 64, 66, 75, 77, 78, 79, 80, 82, 83, 88, 95, 109, 110, 112, 113, 124, 125, 127, 129, 130, 132, 133], "legend": [55, 59, 60, 64, 66, 67, 68, 69, 71, 72, 74, 75, 79, 80, 82], "len": [64, 69, 75, 76, 77, 79, 82], "length": [20, 21, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 56, 58, 60, 94], "leq": [54, 77], "less": [53, 55, 78, 79, 83, 85], "lester": 139, "let": [9, 10, 14, 25, 26, 52, 53, 55, 56, 57, 58, 60, 64, 66, 69, 71, 72, 74, 75, 78, 79, 82, 85, 86, 87, 88, 94, 95, 97, 100, 107, 125, 126, 135, 141], "level": [14, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 54, 55, 57, 58, 59, 60, 61, 64, 67, 68, 69, 71, 72, 73, 74, 77, 78, 79, 82, 84, 85, 86, 94, 101, 102, 109, 114, 125, 131, 132, 141], "level_0": [56, 77], "level_1": 77, "level_bound": 64, "levinsohn": [54, 77], "lewi": 139, "lgbmclassifi": [58, 59, 60, 69, 75, 79, 82, 83, 85], "lgbmregressor": [58, 59, 60, 66, 69, 75, 79, 83, 85, 88], "lgr": [52, 53, 54, 55, 56, 57, 87, 94, 95, 108, 109, 124, 138], "lib": [60, 61, 77, 78], "liblinear": [78, 86, 95], "librari": [51, 52, 53, 54, 55, 56, 57, 87, 88, 89, 91, 94, 95, 108, 109, 124, 137, 138, 141], "licens": 140, "lie": 139, "lightgbm": [58, 59, 60, 66, 69, 75, 79, 82, 83, 85], "like": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 53, 55, 56, 60, 61, 70, 72, 78, 79, 85, 94, 95, 96, 108, 138, 141], "lim": 80, "lim_": [83, 95], "limegreen": [67, 68], "limit": [80, 95, 100, 139], "limits_": 93, "lin": [83, 85, 95], "line": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 59, 64, 85], "linear": [9, 10, 14, 15, 16, 17, 18, 25, 27, 28, 29, 33, 38, 39, 44, 51, 52, 53, 54, 56, 58, 59, 64, 65, 66, 67, 68, 70, 71, 74, 75, 76, 77, 84, 85, 87, 88, 92, 93, 94, 99, 105, 106, 108, 110, 112, 113, 114, 116, 117, 120, 121, 124, 131, 133, 134, 135, 136, 138, 139, 140, 141], "linear_learn": 60, "linear_model": [29, 33, 37, 39, 44, 60, 61, 62, 64, 65, 70, 74, 75, 77, 78, 83, 85, 86, 94, 95, 124, 138], "linearli": [83, 95], "linearregress": [51, 60, 61, 64, 65, 74, 75, 83, 85], "linearscoremixin": [0, 109], "lineplot": [60, 64], "linestyl": [59, 60, 64, 76, 83], "linetyp": 57, "linewidth": [59, 60], "link": [85, 140], "linspac": [67, 68, 74, 85], "lint": 140, "linux": 137, "list": [4, 5, 6, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 47, 52, 53, 54, 55, 56, 60, 66, 67, 68, 77, 79, 81, 88, 94, 108, 109, 137, 140], "listedcolormap": 77, "literatur": [85, 95, 97, 100], "littl": 73, "ll": [56, 124, 141], "lllllllllllllllll": [89, 91, 138], "lm": [51, 53, 85], "ln_alpha_ml_l": 70, "ln_alpha_ml_m": 70, "load": [51, 53, 55, 56, 70, 78, 79, 89, 91, 137, 138], "loader": 0, "loc": [59, 60, 61, 64, 66, 69, 70, 72, 77, 80, 82, 84, 85], "local": [32, 34, 93, 95, 103, 139, 140], "localconvert": 77, "locat": [69, 82, 95], "log": [54, 60, 61, 70, 75, 77, 80, 84, 94, 95, 97, 99], "log_odd": 81, "log_p": [54, 77], "log_reg": [51, 53], "logarithm": 70, "logic": [32, 56, 94], "logical_not": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 75, 94], "logist": [9, 41, 51, 53, 55, 57, 64, 65, 78, 85, 86, 141], "logisticregress": [51, 60, 61, 62, 64, 65, 74, 83, 85], "logisticregressioncv": [37, 75, 78, 86, 95], "logit": [75, 80], "loglik": 56, "logloss": [55, 78, 141], "logo": 140, "logspac": 78, "long": [6, 9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 66, 75, 84, 85, 125, 126, 135, 139], "longer": 60, "look": [53, 55, 56, 58, 59, 60, 61, 69, 75, 78, 79, 82, 83, 84], "loop": 64, "loss": [60, 61, 75, 76, 83, 84, 94, 95, 97, 99], "loss_ml_g0": 75, "loss_ml_g1": 75, "loss_ml_m": 75, "low": [59, 73, 93, 139], "lower": [55, 56, 59, 60, 64, 69, 70, 73, 74, 79, 80, 82, 83, 84, 85, 94, 125, 131, 135, 141], "lower_bound": [67, 68], "lpq": [34, 36, 79, 93, 118, 140], "lpq_0": 82, "lpq_1": 82, "lqte": 93, "lr": 83, "lrn": [51, 52, 53, 54, 55, 56, 57, 87, 94, 95, 108, 109, 124, 138, 141], "lrn_0": 56, "lt": [51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 64, 73, 77, 78, 79, 81, 84, 85, 86, 89, 91, 138], "lucien": 140, "luka": 139, "luk\u00e1\u0161": 19, "lusd": [56, 62, 89, 91, 138], "lvert": 70, "m": [6, 7, 8, 9, 15, 16, 17, 24, 37, 52, 54, 56, 60, 62, 66, 70, 73, 75, 76, 77, 80, 88, 90, 91, 92, 93, 94, 95, 96, 99, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 125, 127, 129, 130, 132, 133, 134, 135, 136, 137, 138, 139, 140], "m_": [64, 95, 99, 101, 109, 113, 114, 118, 124], "m_0": [17, 18, 20, 22, 23, 24, 29, 31, 32, 33, 35, 38, 39, 40, 52, 54, 55, 66, 70, 73, 76, 77, 78, 88, 93, 94, 95, 103, 104, 105, 106, 109, 110, 112, 113, 115, 118, 119, 122, 123, 138, 141], "m_hat": [32, 33, 38, 39, 52, 66, 74, 109], "m_i": [83, 95], "ma": [16, 54, 77, 95, 96, 139], "mac": 137, "machin": [4, 5, 6, 7, 8, 16, 17, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 55, 56, 57, 58, 60, 61, 62, 64, 65, 69, 70, 74, 75, 76, 78, 79, 80, 82, 83, 84, 85, 86, 92, 94, 95, 97, 99, 108, 109, 124, 125, 126, 140, 141], "machineri": [70, 139], "mackei": 139, "maco": 137, "made": [95, 107, 141], "mae": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 75, 94], "maggi": 139, "magnitud": [125, 126], "mai": [43, 47, 57, 58, 86], "main": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 61, 70, 79, 85, 95, 124, 125, 126, 139, 141], "mainli": 85, "maintain": [53, 136, 140], "mainten": 140, "major": [56, 85, 140], "make": [51, 64, 65, 75, 76, 85, 93, 94, 140, 141], "make_confounded_irm_data": [85, 140], "make_confounded_plr_data": 84, "make_did_cs2021": [6, 24, 60, 90, 91, 95, 96, 99], "make_did_sz2020": [20, 23, 58, 95, 97], "make_heterogeneous_data": [67, 68, 71, 72, 73], "make_iivm_data": [32, 34, 93, 95], "make_irm_data": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 74, 75, 93, 94, 95], "make_irm_data_discrete_treat": 64, "make_pipelin": 78, "make_pliv_chs2015": [38, 95], "make_pliv_multiway_cluster_ckms2021": [4, 54, 77], "make_plr_ccddhnr2018": [5, 6, 20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 66, 76, 87, 88, 93, 94, 95, 108, 109, 124, 125, 131], "make_simple_rdd_data": [40, 83, 95], "make_spd_matrix": 18, "make_ssm_data": [57, 86, 95], "malt": [136, 139], "maltekurz": 136, "man": [51, 65], "manag": [94, 137], "mani": [15, 27, 28, 52, 53, 54, 56, 58, 66, 76, 77, 88, 109, 124, 141], "manili": 44, "manipul": [55, 56, 83, 95], "manual": [55, 74, 76, 84, 141], "mao": 139, "map": [32, 42, 43, 46, 47, 53, 54, 77, 93, 95, 103], "mapsto": [87, 93], "mar": [19, 95], "march": [70, 75, 76], "margin": [67, 68, 85], "marit": [55, 78], "marker": [60, 64, 85], "markers": 80, "market": 80, "markettwo": 54, "markov": [18, 139], "marr": [55, 78, 79, 84, 141], "marshal": 94, "martin": [19, 85, 136, 139, 140], "masatoshi": 139, "mask": 24, "maskedarrai": [95, 96], "master": 53, "mat": 54, "match": [94, 125, 134], "math": [37, 60, 61], "mathbb": [9, 10, 14, 25, 26, 27, 28, 32, 33, 38, 39, 54, 57, 58, 59, 60, 64, 73, 75, 76, 77, 80, 83, 86, 93, 95, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 112, 113, 114, 115, 117, 118, 119, 122, 123, 124, 125, 127, 129, 130, 131, 132, 133, 134, 135, 138, 141], "mathcal": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 41, 52, 54, 57, 59, 66, 69, 77, 81, 82, 86, 88, 95, 96, 99, 100], "mathop": 93, "mathrm": [9, 10, 60, 61, 83, 95, 96, 99, 100, 109, 113, 125, 130], "matia": 139, "matplotlib": [21, 24, 58, 59, 60, 61, 62, 64, 66, 67, 68, 69, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 85, 86], "matric": [81, 140], "matrix": [9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 43, 47, 52, 54, 55, 56, 57, 66, 77, 86, 88, 89, 91, 94, 124, 138, 140, 141], "matt": 139, "matter": [75, 80], "max": [25, 55, 56, 74, 78, 79, 87, 93, 94, 95, 108, 109, 113, 115, 124, 125, 130, 138, 141], "max_depth": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 62, 78, 84, 87, 93, 94, 95, 97, 108, 109, 124, 125, 131, 138, 141], "max_featur": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 62, 78, 84, 87, 93, 94, 95, 108, 109, 124, 125, 131, 138, 141], "max_it": [77, 78, 85], "maxim": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 81, 93, 95], "maxima": 124, "maximum": [93, 94], "mb": [60, 62, 86, 89, 91, 138], "mb706": 140, "mea": 12, "mean": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 51, 52, 54, 55, 58, 60, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 82, 84, 85, 88, 94, 95, 124, 141], "mean_absolute_error": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 75, 94], "meant": [93, 140], "measir": 84, "measur": [53, 56, 70, 76, 84, 85, 94, 95, 96, 107, 125, 126, 132, 133, 134, 135], "measure_col": 70, "measure_func": 53, "measure_pr": 53, "measures_r": 53, "mechan": [42, 43, 46, 47, 85], "median": [85, 108], "melt": 54, "membership": 85, "memori": [58, 60, 61, 62, 64, 73, 77, 78, 79, 84, 86, 89, 90, 91, 138], "mention": [73, 93], "merg": [55, 78], "mert": [108, 139], "meshgrid": [67, 68, 85], "messag": [52, 53, 54, 55, 56, 57, 138, 140], "meta": [42, 43, 46, 47, 94, 138], "metadata": [42, 43, 46, 47], "metadata_rout": [42, 43, 46, 47], "metadatarequest": [42, 43, 46, 47], "method": [4, 5, 6, 20, 21, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 64, 65, 66, 67, 68, 69, 71, 72, 74, 75, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 126, 131, 136, 138, 140], "methodolog": 139, "methodologi": 85, "metric": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 46, 94], "michael": 139, "michaela": 140, "michel": [136, 138], "michela": [19, 139], "mid": [55, 78, 80, 83, 95], "mid_point": 64, "might": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 54, 69, 74, 75, 77, 81, 83, 84, 85, 94, 95], "mild": [52, 66, 88], "militari": 80, "miller": [54, 77], "mimic": 85, "min": [54, 55, 56, 57, 60, 61, 69, 74, 77, 78, 79, 82, 83, 87, 94, 95, 99, 108, 109, 124, 138, 141], "min_": 93, "min_samples_leaf": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 73, 78, 84, 87, 93, 94, 95, 97, 108, 109, 124, 125, 131, 141], "min_samples_split": [78, 95, 96, 99], "minim": [33, 45, 55, 75, 78, 83, 95], "minor": [52, 66, 88, 109, 140], "minsplit": 55, "minut": 76, "miruna": 139, "mislead": 140, "miss": [4, 5, 6, 37, 56, 94, 95, 109, 122, 140], "missing": [19, 57, 86], "misspecif": 58, "misspecifi": 58, "mit": [136, 138], "mixin": [0, 27, 28, 109], "ml": [18, 54, 55, 56, 70, 76, 77, 78, 83, 87, 92, 94, 95, 108, 136, 139, 140], "ml_g": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 55, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 69, 71, 73, 74, 75, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 93, 94, 95, 96, 97, 99, 140], "ml_g0": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 53, 55, 58, 60, 61, 62, 75, 78, 84, 94, 95, 97, 99], "ml_g1": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 53, 55, 58, 60, 61, 62, 75, 78, 84, 94, 95, 97, 99], "ml_g_d0": [86, 95], "ml_g_d0_t0": [58, 95, 97], "ml_g_d0_t1": [58, 95, 97], "ml_g_d1": [86, 95], "ml_g_d1_t0": [58, 95, 97], "ml_g_d1_t1": [58, 95, 97], "ml_g_d_lvl0": 95, "ml_g_d_lvl1": 95, "ml_g_sim": 37, "ml_l": [38, 39, 52, 54, 55, 56, 62, 66, 68, 72, 76, 77, 78, 80, 84, 87, 88, 94, 95, 108, 109, 124, 125, 131, 138, 140, 141], "ml_l_bonu": 138, "ml_l_forest": 56, "ml_l_forest_pip": 56, "ml_l_lasso": 56, "ml_l_lasso_pip": 56, "ml_l_rf": 141, "ml_l_sim": 138, "ml_l_tune": 94, "ml_l_xgb": 141, "ml_m": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 131, 138, 140, 141], "ml_m_bench_control": 85, "ml_m_bench_treat": 85, "ml_m_bonu": 138, "ml_m_forest": 56, "ml_m_forest_pip": 56, "ml_m_lasso": 56, "ml_m_lasso_pip": 56, "ml_m_rf": 141, "ml_m_sim": [37, 138], "ml_m_tune": 94, "ml_m_xgb": 141, "ml_pi": [37, 57, 86, 95], "ml_pi_sim": 37, "ml_r": [32, 38, 51, 54, 55, 65, 77, 78, 95, 140], "ml_r0": 95, "ml_r1": [55, 78, 95], "mlr": [56, 94], "mlr3": [51, 52, 53, 54, 55, 57, 87, 94, 95, 108, 109, 124, 136, 138, 140, 141], "mlr3book": [56, 94], "mlr3extralearn": [55, 94], "mlr3filter": 56, "mlr3learner": [51, 52, 53, 54, 55, 87, 94, 95, 108, 109, 124, 138, 141], "mlr3measur": 53, "mlr3pipelin": [94, 140], "mlr3tune": [56, 94, 140], "mlr3vers": 55, "mlrmeasur": 53, "mode": [85, 137], "model": [0, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 47, 48, 51, 52, 53, 54, 56, 58, 59, 60, 61, 65, 66, 69, 70, 73, 75, 77, 79, 82, 84, 87, 88, 89, 90, 91, 92, 94, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 110, 112, 113, 114, 116, 117, 120, 121, 126, 131, 132, 133, 134, 135, 136, 139, 140], "model_data": [55, 78], "model_label": 76, "model_select": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 66, 77, 94, 108], "modellist": 74, "modelmlestimatelowerupp": 55, "modern": [56, 94, 136, 138], "modul": [83, 95, 137], "moment": [27, 28, 54, 77, 95, 99, 109, 124, 125, 126, 135, 138], "monoton": 95, "mont": [9, 10, 11, 14, 67, 68, 71, 72], "montanari": 139, "month": 60, "more": [33, 44, 51, 53, 55, 60, 61, 64, 65, 67, 68, 70, 74, 75, 76, 78, 79, 83, 84, 85, 87, 93, 94, 95, 96, 97, 99, 100, 107, 109, 117, 124, 125, 126, 131, 135, 138, 141], "moreov": [55, 56, 70, 94, 124, 141], "mortgag": [55, 78, 79], "most": [55, 69, 75, 78, 79, 82, 85, 93, 94, 95, 125, 131, 137], "motiv": [85, 88], "motivation_example_bch": 70, "mp": 53, "mpd": [54, 77], "mpg": 77, "mse": [56, 70, 94], "mserd": 83, "msg": 60, "msr": [56, 94], "mtry": [55, 56, 87, 94, 95, 108, 109, 124, 141], "mu": 59, "mu_": 59, "mu_0": 95, "mu_mean": 59, "much": [55, 56, 78, 83, 85, 141], "muld": [62, 89, 91, 138], "multi": [24, 42, 46, 53, 54, 67, 68, 77, 109, 113], "multiclass": [56, 76], "multiindex": 77, "multioutput": [43, 47], "multioutputregressor": [43, 47], "multipl": [4, 5, 6, 14, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 54, 55, 57, 58, 61, 74, 77, 78, 84, 85, 86, 89, 91, 94, 99, 102, 105, 108, 124, 125, 126, 139, 140, 141], "multipletest": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39], "multipli": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 66, 92, 93, 109, 141], "multiprocess": [69, 79, 82], "multitest": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39], "multivariate_norm": 37, "multiwai": [16, 54, 77, 139], "music": 139, "must": [20, 21, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 94, 95], "mutat": 56, "mutual": [29, 33, 39, 55, 71, 72, 78, 79, 93], "my_sampl": 108, "my_task": 108, "n": [9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 25, 26, 29, 31, 32, 33, 34, 35, 37, 38, 39, 41, 51, 52, 54, 56, 57, 59, 60, 64, 65, 66, 69, 70, 73, 77, 80, 81, 82, 83, 86, 87, 88, 93, 94, 95, 100, 108, 124, 136, 137], "n_": [14, 59], "n_aggreg": 21, "n_coef": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 125, 131], "n_color": 60, "n_complier": 82, "n_core": [69, 79, 82], "n_estim": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 58, 59, 60, 62, 66, 67, 68, 69, 71, 72, 73, 78, 79, 81, 82, 83, 84, 85, 87, 88, 93, 94, 95, 97, 108, 109, 124, 125, 131, 138, 141], "n_eval": [56, 94], "n_featur": [42, 43, 46, 47], "n_fold": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 54, 55, 58, 60, 62, 66, 67, 68, 69, 71, 72, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 88, 94, 108, 138, 141], "n_folds_per_clust": [54, 77], "n_folds_tun": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "n_framework": 21, "n_iter": [40, 83, 95], "n_iter_randomized_search": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 94], "n_job": 78, "n_jobs_cv": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 75], "n_jobs_model": [24, 30, 36, 69, 79, 82], "n_level": [14, 64], "n_ob": [6, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 44, 45, 52, 56, 57, 58, 59, 60, 64, 66, 67, 68, 71, 72, 73, 74, 75, 76, 83, 84, 85, 86, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 108, 124, 125, 131, 138], "n_output": [42, 43, 46, 47], "n_period": [25, 60], "n_pre_treat_period": [25, 60], "n_rep": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 54, 57, 58, 60, 62, 64, 66, 73, 74, 75, 77, 83, 84, 85, 86, 88, 94, 108, 125, 131, 138, 141], "n_rep_boot": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 60, 61, 64, 67, 68, 69, 71, 72, 74, 79, 82, 124], "n_sampl": [42, 43, 46, 47, 81], "n_samples_fit": [43, 47], "n_split": 108, "n_t": 59, "n_target": [46, 47], "n_theta": 21, "n_time_period": 59, "n_true": [69, 82], "n_var": [52, 56, 66, 88, 89, 91, 94, 124, 138], "n_w": 81, "n_x": [11, 67, 68, 71, 72, 73], "na": [4, 5, 6, 52, 54, 57, 88, 140], "na_real_": [54, 140], "naiv": [52, 66, 88], "name": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 52, 53, 54, 60, 71, 72, 73, 76, 77, 83, 84, 85, 94, 137, 140], "namespac": 53, "nan": [4, 5, 6, 20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 58, 59, 64, 66, 69, 71, 72, 75, 76, 78, 79, 82, 86, 88, 94], "nanmean": 66, "narita": 139, "nat": 60, "nathan": 139, "nation": [85, 108, 139], "nativ": 53, "natt": 81, "natur": 85, "ncol": [54, 55, 56, 83, 89, 91, 94, 124, 138], "ncoverag": 75, "ndarrai": [4, 5, 6, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 89, 91], "nearli": 75, "necess": [54, 77], "necessari": [53, 54, 76, 77, 83, 95, 137], "need": [20, 22, 23, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 53, 55, 57, 65, 66, 76, 79, 86, 94, 108, 125, 135, 140, 141], "neg": [43, 47], "neighborhood": [83, 124], "neither": [4, 5, 6, 54, 77, 89, 91], "neng": 139, "neq": [83, 95], "nest": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 46, 47, 94, 109, 123, 125, 131], "net": [79, 84, 141], "net_tfa": [55, 78, 79, 84, 141], "nev": [95, 99, 100], "never": [25, 32, 53, 54, 60, 61, 72, 77, 95, 100, 140], "never_tak": [32, 55, 78], "never_tr": [22, 24, 60, 61, 95, 96, 99], "nevertheless": 74, "new": [51, 52, 53, 54, 55, 56, 57, 67, 68, 76, 78, 81, 87, 88, 89, 91, 93, 94, 95, 108, 109, 124, 136, 138, 139, 140, 141], "new_data": [67, 68, 81], "newei": [7, 8, 17, 54, 70, 77, 85, 88, 136, 139], "newest": 140, "next": [53, 55, 56, 67, 68, 69, 73, 75, 78, 79, 81, 82, 85, 140], "neyman": [54, 77, 87, 92, 125, 135, 136, 139], "nfold": [54, 55, 57, 95], "nh": 95, "nice": 53, "nifa": [78, 79, 84], "nil": 85, "nine": [54, 77], "nn": 83, "noack": [83, 95, 139, 140], "node": [55, 56, 87, 95, 108, 109, 124, 138, 141], "nois": [41, 80, 81], "non": [16, 17, 18, 22, 25, 26, 40, 51, 52, 55, 59, 60, 65, 66, 78, 79, 81, 83, 94, 108, 109, 113, 124, 125, 130], "non_orth_scor": [52, 66, 109], "nondur": 62, "none": [4, 5, 6, 14, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 54, 55, 58, 60, 61, 62, 64, 65, 73, 78, 79, 84, 85, 86, 89, 90, 91, 94, 95, 97, 99, 109, 124, 137, 138], "nonignor": [37, 123], "nonlinear": [25, 28, 55, 60, 78, 83, 95, 109, 118, 119, 140], "nonlinearscoremixin": [0, 109], "nonparametr": [34, 35, 36, 83, 85, 109, 125, 126, 132, 133, 134, 135, 139], "nop": 56, "nor": [4, 5, 6, 54, 77, 89, 91], "norm": 66, "normal": [20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 57, 58, 59, 60, 61, 65, 66, 69, 73, 79, 80, 81, 82, 83, 86, 88, 89, 91, 94, 95, 109, 110, 112, 113, 124, 138], "normalize_ipw": [29, 30, 31, 32, 33, 34, 35, 36, 37, 57, 74, 79, 86], "not_yet_tr": [22, 24, 60], "notat": [54, 57, 58, 77, 86, 95, 97, 99, 100, 107, 109, 113], "note": [4, 5, 6, 21, 24, 27, 28, 32, 33, 37, 38, 39, 42, 43, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 108, 109, 136, 138], "notebook": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 93, 94, 95, 141], "notic": [51, 65], "now": [53, 54, 55, 57, 61, 67, 68, 75, 77, 78, 81, 85, 86, 138, 140], "np": [4, 5, 6, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 58, 59, 60, 61, 62, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 131, 138, 141], "nround": [52, 55, 141], "nrow": [53, 54, 56, 83, 89, 91, 94, 124, 138], "nu": [18, 26, 32, 57, 86, 95, 103, 125, 126, 131, 134, 135], "nu2": [60, 125, 131], "nu_0": [125, 135], "nu_i": [57, 86], "nuis_g0": 51, "nuis_g1": 51, "nuis_l": 141, "nuis_m": [51, 141], "nuis_r0": 51, "nuis_r1": 51, "nuis_rmse_ml_l": 70, "nuis_rmse_ml_m": 70, "nuisanc": [4, 5, 6, 17, 18, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 54, 55, 56, 57, 58, 66, 67, 68, 69, 70, 73, 75, 77, 78, 79, 82, 84, 85, 86, 87, 88, 94, 95, 99, 108, 109, 110, 112, 113, 114, 118, 124, 125, 130, 135, 136, 140, 141], "nuisance_el": [125, 127, 129, 130, 132, 133, 134], "nuisance_loss": [75, 94, 140], "nuisance_target": 75, "null": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 53, 84, 94, 125, 131, 140], "null_hypothesi": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 84, 125, 131], "num": [55, 56, 87, 94, 95, 108, 109, 124, 138], "num_leav": [59, 69, 79, 82], "number": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 47, 52, 54, 59, 60, 66, 67, 68, 69, 70, 71, 72, 75, 77, 79, 81, 82, 83, 85, 95, 99, 108, 124, 136, 138, 141], "numer": [28, 51, 56, 74, 80, 94, 109, 125, 132, 133, 140], "numeric_onli": 70, "numpi": [4, 5, 6, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 45, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 91, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 131, 138], "nuniqu": 60, "ny": 139, "nyt": [95, 99, 100], "o": [59, 64, 70, 71, 72, 75, 76, 78, 80, 83, 124, 136, 138], "ob": [53, 55, 59, 83], "obei": 109, "obj": 78, "obj_dml_data": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 54, 61, 65, 66, 69, 74, 76, 77, 82, 87, 88, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 131, 140], "obj_dml_data_bonu": [89, 91], "obj_dml_data_bonus_df": [89, 91], "obj_dml_data_from_arrai": [4, 5, 6], "obj_dml_data_from_df": [4, 5], "obj_dml_data_sim": [89, 91], "obj_dml_plr": [52, 66, 88], "obj_dml_plr_bonu": [56, 138], "obj_dml_plr_bonus_pip": 56, "obj_dml_plr_bonus_pipe2": 56, "obj_dml_plr_bonus_pipe3": 56, "obj_dml_plr_bonus_pipe_ensembl": 56, "obj_dml_plr_fullsampl": 76, "obj_dml_plr_lesstim": 76, "obj_dml_plr_nonorth": [52, 66], "obj_dml_plr_orth_nosplit": [52, 66], "obj_dml_plr_sim": [56, 138], "obj_dml_plr_sim_pip": 56, "obj_dml_plr_sim_pipe_ensembl": 56, "obj_dml_plr_sim_pipe_tun": 56, "obj_dml_sim": 37, "object": [4, 5, 6, 7, 8, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 51, 55, 56, 57, 58, 60, 61, 62, 64, 67, 68, 69, 72, 73, 74, 76, 78, 79, 82, 83, 86, 89, 90, 91, 93, 94, 95, 96, 97, 99, 108, 109, 124, 136, 138, 139, 140, 141], "obs_confound": [51, 65], "observ": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 37, 38, 39, 41, 44, 45, 48, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 73, 75, 76, 77, 78, 79, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 100, 107, 108, 109, 110, 112, 113, 124, 125, 126, 127, 129, 130, 138, 139, 141], "obtain": [10, 51, 52, 53, 54, 57, 58, 60, 61, 65, 66, 67, 68, 69, 70, 75, 77, 82, 85, 86, 87, 88, 93, 94, 108, 109, 124, 125, 126, 131, 137, 138], "obvious": 60, "occur": [25, 60, 76, 140], "off": [81, 139], "offer": [25, 53, 55, 78, 79, 85, 141], "offici": 137, "offset": 94, "often": 82, "oka": 139, "ol": [29, 33, 39, 44], "olma": [83, 95, 139, 140], "omega": [73, 93, 95, 96, 109, 114, 117, 125, 132, 133], "omega_": [16, 54, 77], "omega_1": [16, 54, 77], "omega_2": [16, 54, 77], "omega_epsilon": [54, 77], "omega_v": [16, 54, 77], "omega_x": [16, 54, 77], "omit": [60, 84, 85, 95, 99, 109, 113, 125, 126, 135, 139, 140, 141], "ommit": 85, "onc": [53, 76, 85, 95, 100, 141], "one": [21, 24, 38, 48, 51, 52, 53, 54, 55, 56, 60, 61, 64, 65, 66, 67, 68, 75, 77, 79, 80, 83, 84, 85, 88, 93, 94, 95, 96, 99, 100, 105, 108, 109, 110, 112, 113, 117, 120, 121, 124, 125, 126, 131, 132, 133, 134, 138, 140], "ones": [56, 59, 69, 76, 82, 84, 93], "ones_lik": [64, 82], "onli": [4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 46, 47, 53, 54, 55, 60, 67, 68, 71, 72, 73, 75, 76, 77, 78, 79, 83, 87, 93, 94, 95, 99, 107, 109, 113, 115, 118, 119, 124, 125, 126, 130, 132, 133, 135, 140], "onlin": 141, "onto": 75, "oo": 76, "oob_error": [56, 94], "oop": 140, "opac": [67, 68], "open": [56, 94, 136, 138], "oper": 56, "opposit": [81, 83, 95], "oprescu": [11, 67, 68, 71, 72, 139], "opt": [60, 61, 78], "optim": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 40, 56, 67, 68, 76, 81, 93, 94, 139], "option": [20, 21, 22, 23, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 51, 52, 54, 55, 57, 60, 64, 67, 68, 71, 72, 73, 75, 77, 78, 79, 86, 94, 95, 108, 109, 115, 118, 119, 124, 140], "oracl": [14, 41, 60, 64], "oracle_valu": [9, 10, 14, 41, 64], "orang": 52, "orcal": [9, 10], "order": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 46, 53, 54, 55, 56, 74, 77, 78, 83, 94, 95, 108, 109], "org": [12, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 94, 136, 137, 140], "orient": [56, 94, 109, 136, 138, 139, 140], "origin": [42, 43, 45, 46, 47, 53, 56, 60, 61, 72, 81, 84, 85, 93, 109, 117], "orign": [55, 78], "orth_sign": [44, 45, 74], "orthogon": [44, 45, 54, 55, 60, 77, 78, 87, 92, 95, 124, 125, 135, 136, 139], "orthongon": [125, 135], "osx": 137, "other": [4, 5, 6, 38, 39, 42, 43, 46, 47, 52, 54, 55, 56, 57, 58, 60, 61, 64, 66, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 88, 89, 91, 93, 94, 95, 105, 106, 108, 109, 117, 124, 125, 135, 136, 137, 138, 139, 140, 141], "other_ind": 77, "otherwis": [20, 22, 23, 29, 32, 33, 39, 42, 43, 46, 47, 55, 78, 79, 81, 95, 97, 109, 113], "othrac": [56, 62, 89, 91, 138], "our": [52, 53, 55, 56, 58, 60, 66, 67, 68, 69, 75, 76, 78, 79, 82, 83, 84, 85, 88, 95, 136, 138, 140, 141], "ourselv": 75, "out": [38, 39, 54, 56, 58, 60, 61, 62, 70, 75, 76, 77, 79, 84, 85, 86, 87, 89, 91, 92, 93, 94, 95, 96, 97, 99, 109, 120, 121, 124, 125, 126, 131, 134, 136, 138, 140, 141], "outcom": [4, 5, 6, 9, 10, 14, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 51, 53, 54, 55, 56, 59, 60, 61, 62, 65, 70, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 89, 90, 91, 94, 97, 99, 100, 101, 105, 106, 107, 113, 114, 124, 126, 131, 132, 134, 135, 138, 140, 141], "outcome_0": 65, "outcome_1": 65, "outer": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 94], "output": [53, 60, 61, 75, 87, 95, 99, 124, 141], "outshr": 77, "outsid": 52, "over": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 37, 38, 39, 52, 60, 64, 66, 70, 75, 88, 92, 94, 125, 131], "overal": [21, 60, 61, 81, 85, 95, 96], "overall_aggregation_weight": [21, 60, 61], "overcom": [92, 109], "overfit": [76, 92, 108], "overlap": [58, 85, 95, 97, 100], "overrid": [94, 140], "overridden": 95, "overst": [55, 78, 79], "overview": [75, 124, 125, 131, 139], "overwrit": 140, "ownership": [55, 78], "p": [9, 10, 11, 13, 20, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 93, 94, 95, 96, 97, 99, 100, 108, 109, 110, 112, 113, 114, 115, 118, 119, 122, 123, 124, 125, 132, 133, 136, 137, 138, 140], "p401": [55, 78, 79], "p_0": [109, 110, 112], "p_1": 124, "p_adjust": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 108, 124, 136, 138], "p_dbl": [56, 94], "p_hat": 74, "p_int": 94, "p_n": 15, "p_val": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39], "p_x": [16, 54, 77], "p_x0": 80, "p_x1": 80, "packag": [51, 52, 54, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 76, 77, 79, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 97, 100, 107, 108, 109, 124, 125, 126, 136, 138, 139, 140, 141], "packagedata": 77, "packagevers": 55, "page": [85, 136, 139], "pair": [51, 65], "pake": [54, 77], "paket": [54, 55, 56], "pal": 54, "palett": [21, 24, 60, 64], "pand": 60, "panda": [4, 5, 6, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 44, 45, 57, 58, 59, 60, 61, 62, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 93, 95, 125, 126, 138], "pandas2ri": 77, "panel": [5, 6, 20, 22, 25, 26, 90, 91, 99, 100, 129, 130, 139, 140], "paper": [12, 15, 56, 76, 80, 83, 84, 85, 125, 135, 136, 138, 139, 140], "par": 62, "par_grid": [56, 94], "paradox": [56, 94, 140], "parallel": [53, 58, 59, 60, 64, 69, 75, 82, 95, 97, 99, 100], "param": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 76, 94], "param_grid": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 94], "param_nam": 53, "param_set": [56, 94], "param_v": 56, "paramet": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 55, 57, 58, 60, 64, 66, 67, 68, 69, 70, 73, 74, 75, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 91, 92, 93, 94, 95, 97, 99, 100, 101, 102, 103, 104, 107, 108, 118, 119, 124, 125, 126, 131, 133, 135, 136, 138, 139, 140, 141], "parametr": [53, 85, 88, 94, 141], "params_exact": 94, "params_nam": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 53], "parenttoc": 136, "part": [18, 52, 54, 55, 56, 61, 66, 75, 76, 77, 78, 88, 94, 108, 125, 135, 140, 141], "parti": 18, "partial": [10, 15, 16, 17, 18, 28, 38, 39, 54, 56, 62, 70, 76, 77, 84, 87, 92, 94, 105, 106, 108, 120, 121, 124, 131, 132, 133, 134, 135, 136, 138, 140, 141], "partial_": [109, 124], "partiallli": 84, "particip": [7, 79, 84, 141], "particular": [95, 136], "particularli": 76, "partion": [54, 77], "partit": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 77, 87, 92], "partli": 141, "pass": [29, 33, 39, 42, 43, 44, 46, 47, 53, 56, 76, 94, 141], "passo": [136, 138], "past": 54, "paste0": [54, 57], "pastel": 66, "path": [94, 95], "path_to_r": 70, "patsi": [67, 68, 93], "pattern": 85, "paul": 139, "pd": [4, 5, 6, 7, 8, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 58, 59, 60, 61, 64, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 93, 95], "pdf": [66, 80], "pedregosa": [136, 138], "pedregosa11a": [136, 138], "pedro": [53, 139], "penal": [57, 86], "penalti": [55, 56, 65, 78, 85, 86, 94, 95], "pennsylvania": [8, 89, 91, 138], "pension": [55, 78, 79, 141], "peopl": [55, 78, 79], "pep8": 140, "per": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 60, 61, 77], "percent": 94, "percentag": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39], "perf_count": 75, "perfectli": [83, 95], "perform": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 45, 52, 54, 56, 58, 60, 61, 66, 70, 72, 73, 75, 76, 77, 79, 84, 85, 86, 88, 94, 95, 97, 99, 108, 109, 124, 136, 138, 139, 141], "perfrom": 73, "perhap": 141, "period": [20, 22, 24, 25, 53, 58, 59, 61, 90, 91, 96, 97, 99, 100, 113, 139], "perp": [95, 107], "perrot": [136, 138], "person": 141, "pessimist": 85, "peter": 139, "petra": 140, "petronelaj": 140, "pfister": [56, 94, 136, 138], "phi": [54, 77, 93, 124], "philipp": [85, 136, 139], "philippbach": [136, 140], "pi": [13, 15, 18, 37, 93, 95, 109, 122, 123], "pi_": [16, 54, 77], "pi_0": [109, 122, 123], "pi_i": [57, 86, 95], "pick": [83, 141], "pip": [83, 95], "pip3": 137, "pipe": 56, "pipe_forest_classif": 56, "pipe_forest_regr": 56, "pipe_lasso": 56, "pipelin": [42, 43, 46, 47, 56, 78, 140], "pipeop": 56, "pira": [55, 78, 79, 84, 141], "pivot": [70, 77, 139], "plai": [76, 141], "plan": [7, 55, 78, 79, 141], "plausibl": [85, 125], "pleas": [42, 43, 46, 47, 53, 64, 76, 85, 108, 137], "plim": 80, "pliv": [27, 28, 38, 54, 77, 87, 93, 105, 120, 136, 140], "plm": [0, 92, 94, 124, 131, 141], "plot": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 45, 52, 53, 55, 56, 57, 59, 60, 61, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 78, 79, 80, 82, 83, 84, 85, 86, 93, 125, 131], "plot_data": 60, "plot_effect": [21, 24, 60, 61], "plot_tre": [45, 81, 93], "plotli": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 67, 68, 70, 83, 85], "plr": [27, 28, 39, 56, 76, 80, 84, 87, 94, 106, 108, 121, 124, 131, 133, 134, 135, 136, 138, 140, 141], "plr_est": 80, "plr_est1": 80, "plr_est2": 80, "plr_obj": 80, "plr_obj_1": 80, "plr_obj_2": 80, "plr_summari": 78, "plrglmnet": 55, "plrranger": 55, "plrrpart": 55, "plrxgboost8700": 55, "plt": [58, 59, 60, 62, 64, 66, 67, 68, 69, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 85, 86], "plt_smpl": [54, 77], "plt_smpls_cluster": [54, 77], "plug": [73, 125, 127, 129, 130, 131, 132, 133], "pm": [40, 54, 77, 124, 125, 131, 135], "pmatrix": [57, 86], "pmlr": [70, 75, 76], "po": [56, 94], "poe": 139, "point": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 53, 54, 71, 72, 77, 85, 93, 95, 141], "pointwis": [44, 69, 71, 72, 82], "poli": [55, 74, 77, 78], "polici": [33, 38, 39, 45, 92, 95, 105, 106, 138, 139, 140], "policy_tre": [33, 81, 93], "policy_tree_2": 81, "policy_tree_obj": 93, "policytre": 81, "polit": 80, "poly_dict": 78, "polynomi": [7, 8, 41, 55, 62, 74, 78, 83], "polynomial_featur": [7, 8, 55, 62], "polynomialfeatur": [74, 77, 78], "popul": [85, 95, 99, 109, 113], "popular": [75, 95, 125, 126], "porport": 84, "posit": [18, 55, 60, 61, 80, 85, 141], "posixct": [56, 94], "possibl": [4, 5, 6, 43, 46, 47, 53, 56, 60, 61, 67, 68, 71, 72, 73, 74, 75, 76, 81, 83, 84, 85, 94, 95, 99, 101, 102, 124, 125, 126, 140, 141], "possibli": [125, 126], "post": [15, 18, 24, 95, 97, 99, 100, 124, 139], "postdoubl": 139, "poster": 80, "potenti": [9, 14, 25, 29, 30, 31, 34, 35, 37, 41, 57, 58, 60, 74, 80, 83, 86, 97, 100, 101, 107, 114, 115, 124, 132, 137, 140, 141], "potential_level": 64, "power": [56, 76, 85, 94, 139], "pp": [53, 70, 75, 76], "pq": [34, 35, 36, 79, 119, 140], "pq_0": [79, 82], "pq_1": [79, 82], "pr": [37, 51, 54, 55, 56, 57, 94, 95, 108, 109, 124, 138, 141], "practic": [75, 85, 139], "pre": [22, 24, 25, 53, 57, 58, 60, 61, 86, 94, 95, 97, 99, 109, 113], "precis": [53, 95, 125, 133, 141], "precomput": [43, 47], "pred": [53, 76], "pred_df": 81, "pred_dict": 94, "pred_treat": 81, "predict": [4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 52, 54, 55, 56, 66, 69, 70, 74, 75, 76, 77, 78, 81, 85, 88, 93, 108, 125, 126, 131, 133, 140, 141], "predict_proba": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 39, 40, 42, 46, 76, 94], "predictor": [29, 33, 39, 44, 45, 67, 68, 71, 72, 85, 87], "prefer": [55, 78, 79, 141], "preliminari": [31, 52, 66, 83, 109, 115, 118, 119, 123], "prepar": [53, 54, 77, 140], "preprint": 139, "preprocess": [55, 74, 77, 78, 79, 94], "presenc": [55, 78, 79], "present": [53, 85, 94, 109, 117, 141], "prespecifi": 84, "pretest": 53, "pretreat": [20, 22, 23, 24, 53, 58], "prettenhof": [136, 138], "preval": 85, "prevent": [108, 140], "previou": [59, 73, 74, 80, 137, 141], "previous": [94, 141], "price": [54, 77], "priliminari": [34, 36], "primari": 64, "principl": [125, 126], "print": [22, 24, 40, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 131, 137, 138, 140, 141], "print_detail": 53, "print_period": [22, 24], "prior": [75, 95, 107], "privat": 140, "prob": 56, "probabilit": 73, "probabl": [14, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 46, 52, 53, 57, 58, 60, 64, 66, 73, 80, 82, 83, 85, 86, 88, 95, 109, 110, 112, 113, 118, 139], "problem": [55, 78, 79, 93, 94], "procedur": [52, 54, 55, 66, 75, 77, 78, 84, 85, 94, 124, 137, 140], "proceed": [15, 139], "process": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 41, 53, 57, 58, 59, 61, 67, 68, 69, 70, 71, 72, 75, 76, 81, 82, 85, 86, 92, 124, 125, 126, 139, 140], "produc": 80, "product": [67, 68, 70, 75, 85, 125, 135], "producton": 54, "program": [13, 55, 78, 79, 139, 141], "progress": 63, "project": [56, 67, 68, 93, 136, 140], "project_z": [67, 68], "prone": 109, "pronounc": 83, "propens": [9, 10, 25, 34, 36, 55, 57, 58, 60, 73, 74, 75, 78, 79, 85, 86, 93, 95, 99, 101, 109, 113, 125, 132], "properli": [76, 141], "properti": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 56, 60, 61, 75, 78, 79, 80, 84, 94, 95, 125, 131, 138, 140], "proport": [84, 125, 126, 134, 135], "propos": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 54, 56, 77, 83, 125, 126, 139, 140], "provid": [4, 5, 6, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 53, 54, 55, 56, 67, 68, 71, 72, 74, 76, 77, 78, 83, 85, 87, 88, 89, 90, 91, 92, 94, 124, 136, 138, 140, 141], "prune": [33, 45], "ps911c": 77, "ps944": 77, "pscore1": 80, "pscore2": 80, "psi": [27, 28, 52, 53, 54, 77, 87, 95, 99, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 135, 138], "psi_": [124, 125, 131, 134, 135], "psi_a": [27, 32, 33, 38, 39, 52, 54, 66, 77, 95, 99, 108, 109, 110, 112, 113, 114, 116, 117, 120, 121, 124], "psi_b": [27, 32, 33, 38, 39, 52, 66, 93, 95, 99, 108, 109, 110, 112, 113, 114, 116, 117, 120, 121], "psi_el": [108, 109], "psi_j": 124, "psi_nu2": [125, 131], "psi_sigma2": [125, 131], "public": [51, 65, 140], "publish": [85, 140], "pull": [55, 140], "purchas": 85, "pure": 85, "purp": [67, 68], "purpos": [52, 66, 73, 84, 85, 109, 113, 125, 126, 138], "pval": 124, "px": [70, 83], "py": [60, 61, 72, 77, 78, 85, 136, 137, 140], "py3": 137, "py_al": 66, "py_did": 58, "py_did_pretest": 59, "py_dml": 66, "py_dml_nosplit": 66, "py_dml_po": 66, "py_dml_po_nosplit": 66, "py_double_ml_apo": 64, "py_double_ml_bas": 66, "py_double_ml_basic_iv": 65, "py_double_ml_c": 67, "py_double_ml_cate_plr": 68, "py_double_ml_cvar": 69, "py_double_ml_firststag": 70, "py_double_ml_g": 71, "py_double_ml_gate_plr": 72, "py_double_ml_gate_sensit": 73, "py_double_ml_irm_vs_apo": 74, "py_double_ml_learn": 75, "py_double_ml_meets_flaml": 76, "py_double_ml_multiway_clust": 77, "py_double_ml_pens": 78, "py_double_ml_pension_qt": 79, "py_double_ml_plm_irm_hetfx": 80, "py_double_ml_policy_tre": 81, "py_double_ml_pq": 82, "py_double_ml_rdflex": 83, "py_double_ml_sensit": 84, "py_double_ml_sensitivity_book": 85, "py_double_ml_ssm": 86, "py_non_orthogon": 66, "py_panel": 60, "py_panel_simpl": 61, "py_po_al": 66, "pydata": 72, "pypi": [139, 140], "pyplot": [58, 59, 60, 62, 64, 66, 67, 68, 69, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 85, 86], "pyproject": 140, "python": [18, 53, 76, 85, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 99, 108, 109, 124, 125, 126, 131, 136, 138, 139, 140, 141], "python3": [60, 61, 78, 137], "q": [56, 69, 82, 83, 94, 136, 138], "q2": [56, 62, 89, 91, 138], "q3": [56, 62, 89, 91, 138], "q4": [56, 62, 89, 91, 138], "q5": [56, 62, 89, 91, 138], "q6": [56, 62, 89, 91, 138], "q_i": [83, 95], "qquad": 13, "qte": [69, 79, 140], "quad": [25, 26, 55, 57, 58, 78, 81, 83, 86, 93, 95, 97, 100, 107, 109, 118, 124, 125, 127], "quadrat": [57, 86], "qualiti": [84, 87, 140], "quanitl": 79, "quant": 69, "quantifi": [85, 95, 100], "quantil": [14, 24, 30, 31, 34, 35, 36, 60, 64, 69, 74, 84, 92, 94, 115, 118, 119, 139, 140], "quantiti": [51, 65, 85], "queri": 78, "question": [85, 141], "quick": 79, "quit": [75, 81, 84, 125, 126], "r": [12, 32, 42, 43, 46, 47, 59, 60, 61, 66, 67, 68, 70, 77, 80, 83, 85, 87, 88, 89, 91, 92, 95, 103, 108, 109, 116, 120, 124, 125, 126, 132, 133, 134, 135, 136, 138, 139, 140, 141], "r2_d": [13, 75], "r2_score": [43, 47], "r2_y": [13, 75], "r6": [56, 140], "r_0": [32, 38, 55, 78, 95, 103], "r_all": 52, "r_d": 13, "r_df": 77, "r_dml": 52, "r_dml_nosplit": 52, "r_dml_po": 52, "r_dml_po_nosplit": 52, "r_double_ml_bas": 52, "r_double_ml_basic_iv": 51, "r_double_ml_did": 53, "r_double_ml_multiway_clust": 54, "r_double_ml_pens": 55, "r_double_ml_pipelin": 56, "r_double_ml_ssm": 57, "r_hat": 38, "r_hat0": 32, "r_hat1": 32, "r_non_orthogon": 52, "r_po_al": 52, "r_y": 13, "rais": [4, 5, 6, 42, 43, 46, 47, 60, 61, 94], "randint": 80, "randn": 37, "random": [11, 14, 18, 19, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 51, 52, 53, 55, 56, 58, 59, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 93, 94, 96, 97, 99, 100, 108, 122, 124, 125, 131, 135, 138, 139, 141], "random_search": 94, "random_st": [14, 60, 66, 73, 74, 81], "randomforest": [55, 75, 78], "randomforest_class": [55, 67, 78, 81], "randomforest_reg": [67, 81], "randomforestclassifi": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 62, 64, 67, 68, 71, 72, 73, 75, 78, 81, 83, 84, 85, 93, 94, 95, 96, 97, 99, 141], "randomforestregressor": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 62, 64, 66, 67, 68, 71, 72, 73, 75, 78, 81, 83, 84, 85, 87, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 131, 138, 141], "randomized_search": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 94], "randomizedsearchcv": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 94], "randomli": [52, 54, 66, 77, 88, 108, 141], "rang": [52, 58, 59, 64, 66, 69, 71, 72, 75, 76, 77, 79, 81, 82, 83, 85, 86, 88, 94, 95], "rangeindex": [58, 60, 61, 62, 64, 73, 77, 78, 79, 84, 86, 89, 90, 91, 138], "ranger": [53, 55, 56, 87, 94, 95, 108, 109, 124, 138, 141], "rangl": [11, 81], "rank": 140, "rate": [70, 75, 95], "rather": [83, 85, 95], "ratio": [94, 108, 125, 126], "ravel": [67, 68], "raw": [55, 61, 70, 78], "raw_res_manual_lasso_r_100_n_100_p_200_rho_0": 70, "rbind": 55, "rbindlist": 55, "rbinom": 51, "rbrace": [12, 13, 19, 32, 33, 54, 77, 87, 95, 101, 103, 104, 108, 109, 114, 124, 125, 132], "rcolorbrew": 54, "rcparam": [59, 62, 67, 68, 69, 71, 72, 74, 77, 78, 79, 82], "rd": [95, 140], "rdbu": 54, "rdbu_r": 77, "rdbwselect": 95, "rdd": [0, 4, 5, 6, 92, 137], "rdflex": [83, 95, 140], "rdflex_fuzzi": 83, "rdflex_fuzzy_stack": 83, "rdflex_obj": [40, 95], "rdflex_sharp": 83, "rdflex_sharp_stack": 83, "rdrobust": [40, 83, 95, 137, 140], "rdrobust_fuzzi": 83, "rdrobust_fuzzy_noadj": 83, "rdrobust_sharp": 83, "rdrobust_sharp_noadj": 83, "rdt044": 70, "re": [60, 77, 85, 137], "read": 137, "read_csv": [61, 70], "readabl": 140, "readili": 136, "real": [55, 78, 79, 84, 125, 126], "realat": 95, "realiz": [83, 95, 107], "reason": [4, 5, 6, 51, 65, 70, 75, 76, 84, 85, 125, 126, 141], "recal": [62, 125, 135], "receiv": [25, 60, 64, 83, 95, 97], "recent": [76, 95, 97, 100, 139], "recogn": [55, 78, 79], "recommend": [56, 60, 75, 83, 85, 87, 95, 108, 125, 137, 139, 140], "recov": [51, 53, 65, 80], "recsi": 139, "red": [54, 57, 71, 72, 76, 77], "reduc": [55, 73, 76, 78, 83, 84, 85, 95, 140], "redund": 140, "reemploy": [8, 89, 91, 138], "refactor": 140, "refer": [7, 8, 9, 10, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 59, 60, 64, 73, 78, 79, 83, 84, 89, 91, 92, 93, 95, 96, 99, 100, 109, 125, 126, 131, 139, 140], "reference_level": [30, 64, 74, 95], "refin": 140, "refit": [125, 126], "reflect": [81, 85, 93], "reg": [25, 26, 55, 78, 141], "reg_estim": 83, "reg_learn": 79, "reg_learner_1": 75, "reg_learner_2": 75, "regard": [85, 136], "regener": 140, "region": [54, 69, 77, 124, 139], "regr": [51, 52, 53, 54, 55, 56, 57, 87, 94, 95, 108, 109, 124, 138, 141], "regravg": [56, 94], "regress": [9, 10, 12, 13, 14, 15, 16, 17, 18, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 44, 51, 53, 54, 56, 57, 60, 61, 64, 65, 70, 76, 77, 80, 84, 85, 86, 87, 88, 92, 93, 94, 97, 99, 103, 104, 105, 106, 108, 113, 124, 126, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141], "regressor": [43, 47, 52, 55, 64, 66, 69, 75, 76, 78, 88], "regular": [15, 92, 94, 109, 124, 139], "reich": [56, 94], "reinforc": 139, "reject": [55, 78], "rel": [55, 78, 125, 126, 132, 133], "relat": [74, 85, 141], "relationship": [51, 65, 70, 85, 124], "relev": [4, 5, 6, 11, 20, 22, 23, 24, 42, 43, 44, 46, 47, 60, 69, 81, 82, 95, 125, 141], "reli": [58, 59, 60, 67, 68, 73, 74, 93, 94, 95, 97, 109, 113, 125, 126, 141], "reload": 55, "remain": [53, 124, 141], "remark": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 52, 59, 60, 61, 64, 66, 67, 68, 69, 71, 72, 73, 74, 75, 79, 84, 93, 94, 95, 97, 99, 109, 110, 112, 113, 118, 119, 124, 125, 130, 133], "remot": 137, "remov": [55, 74, 85, 92, 95, 108, 109, 125, 140], "renam": [60, 78, 140], "render": [84, 85], "reorgan": 140, "rep": [52, 57, 88, 94, 124], "repeat": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 54, 55, 56, 57, 60, 61, 66, 73, 77, 78, 79, 80, 83, 84, 86, 88, 92, 94, 99, 124, 127, 138, 140, 141], "repeatedkfold": 77, "repet": 84, "repetit": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 67, 68, 70, 71, 72, 73, 75, 92, 94, 124, 138, 141], "repetiton": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40], "replac": [81, 85, 140], "replic": [7, 8, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 55, 61, 66, 70, 85], "repo": 140, "report": [55, 76, 78, 136, 140], "repositori": [60, 70, 83, 140], "repr": [52, 54], "repres": [25, 80, 85, 95], "represent": [10, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 84, 124, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135, 138, 140], "reproduc": 14, "request": [42, 43, 46, 47, 140], "requir": [38, 39, 42, 46, 51, 55, 56, 60, 61, 64, 73, 78, 79, 84, 95, 96, 99, 109, 113, 124, 125, 126, 131, 137, 140, 141], "requirenamespac": 53, "res_df": 77, "res_dict": [9, 10, 11, 14, 41], "resampl": [51, 54, 56, 57, 58, 60, 61, 77, 79, 84, 86, 94, 95, 97, 99, 108, 109, 124, 136, 138, 141], "research": [54, 56, 77, 80, 85, 108, 136, 138, 139, 141], "resembl": [57, 86], "reset": 53, "reset_index": [60, 70, 77, 78], "reshap": [59, 66, 67, 68, 74], "reshape2": 54, "residu": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 84, 125, 126, 134, 135], "resolut": [56, 94], "resourc": 75, "resourcewis": 75, "respect": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 60, 61, 64, 78, 79, 83, 93, 95, 99, 103, 107, 108, 125, 135, 141], "respons": [7, 56, 94], "rest": 95, "restart": 137, "restrict": 75, "restructur": 140, "restud": 70, "result": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 51, 52, 53, 56, 57, 58, 59, 60, 61, 64, 66, 67, 68, 70, 73, 74, 75, 81, 83, 84, 85, 86, 88, 94, 108, 109, 110, 112, 113, 125, 126, 131, 138, 140], "result_iivm": 55, "result_irm": 55, "result_plr": 55, "retain": [42, 43, 46, 47], "retina": 80, "retir": [55, 78, 79, 84], "return": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 56, 57, 60, 61, 66, 69, 72, 75, 76, 77, 80, 81, 82, 84, 85, 86, 87, 94, 109, 125, 126, 140], "return_count": [64, 75], "return_tune_r": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "return_typ": [4, 5, 6, 7, 8, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 55, 56, 57, 58, 66, 74, 75, 76, 78, 79, 84, 86, 87, 88, 89, 91, 93, 94, 95, 97, 108, 109, 124, 125, 131, 138, 141], "rev": 54, "reveal": 73, "review": [15, 70, 139], "revist": [54, 77], "rf": 83, "rho": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 60, 64, 73, 74, 83, 84, 85, 125, 126, 131, 135, 141], "rho_val": 85, "richter": [56, 94, 136, 138], "riesz": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 84, 125, 126, 127, 129, 130, 131, 134, 135], "riesz_rep": [125, 131], "right": [12, 13, 15, 16, 19, 25, 52, 54, 66, 75, 77, 78, 79, 80, 82, 83, 85, 88, 95, 109, 110, 112, 113, 124, 125, 127, 129, 130, 132, 133], "rightarrow_": [52, 66, 88], "risk": [31, 92, 140], "ritov": 139, "rival": 77, "rival_ind": 77, "rmd": 53, "rmse": [53, 58, 60, 61, 75, 76, 79, 84, 86, 94, 95, 97, 99, 109, 124, 138, 140], "rmse_dml_ml_l_fullsampl": 76, "rmse_dml_ml_l_lesstim": 76, "rmse_dml_ml_l_onfold": 76, "rmse_dml_ml_l_untun": 76, "rmse_dml_ml_m_fullsampl": 76, "rmse_dml_ml_m_lesstim": 76, "rmse_dml_ml_m_onfold": 76, "rmse_dml_ml_m_untun": 76, "rmse_oos_ml_l": 76, "rmse_oos_ml_m": 76, "rmse_oos_onfolds_ml_l": 76, "rmse_oos_onfolds_ml_m": 76, "rnorm": [51, 56, 89, 91, 94, 124, 138], "robin": [7, 8, 17, 54, 70, 77, 88, 136, 139], "robinson": [52, 66, 88], "robject": 77, "robu": [71, 72], "robust": [9, 10, 16, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 53, 60, 64, 73, 74, 83, 84, 85, 95, 125, 131, 139, 141], "roc\u00edo": 139, "role": [4, 5, 6, 52, 66, 76, 88, 141], "romano": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 124], "root": [20, 22, 23, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 70, 88, 94, 109, 139], "rotat": [76, 83], "roth": [83, 95, 97, 100, 139], "rough": [85, 141], "roughli": [60, 85], "round": [55, 64, 74, 75, 80, 85], "rout": [42, 43, 46, 47], "row": [21, 52, 55, 59, 62, 67, 68, 76, 77, 81, 89, 91, 95, 99, 108, 138, 141], "row_index": 72, "rownam": 54, "rowv": 54, "roxygen2": 140, "royal": [85, 139], "rpart": [55, 56, 94], "rpart_cv": 56, "rprocess": 75, "rpy2": 77, "rpy2pi": 77, "rskf": 74, "rsmp": [56, 94, 108], "rsmp_tune": [56, 94], "rssb": 85, "rtype": 30, "ruben": 139, "ruiz": [51, 65], "rule": [53, 93], "run": [53, 83, 95, 137, 140], "runif": 51, "runner": [60, 61, 85], "runtime_learn": 56, "rv": [60, 64, 73, 74, 84, 85, 125, 131, 141], "rva": [60, 64, 73, 74, 84, 85, 125, 131, 141], "rvert": 70, "rvert_": 70, "s1": 76, "s2": 76, "s_": [16, 54, 77, 95, 107], "s_1": 17, "s_2": 17, "s_col": [4, 5, 57, 83, 86, 95], "s_i": [19, 57, 83, 86, 95], "s_x": [16, 54, 77], "safeguard": [58, 94], "sake": [55, 78, 85, 141], "same": [14, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 44, 52, 54, 57, 60, 66, 67, 68, 73, 74, 75, 77, 79, 81, 83, 84, 85, 86, 94, 95, 99, 109, 110, 112, 113, 124, 125, 133, 140], "samii": 80, "sampl": [9, 10, 16, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 51, 53, 54, 56, 58, 60, 61, 65, 71, 72, 74, 75, 77, 79, 81, 84, 92, 94, 97, 99, 100, 107, 124, 138, 139, 140], "sample_weight": [40, 42, 43, 46, 47, 83], "sant": [9, 10, 14, 20, 22, 23, 25, 26, 53, 58, 60, 61, 95, 96, 97, 99, 100, 139], "sara": 139, "sasaki": [16, 54, 77, 139], "satisfi": [57, 86, 94, 109, 124], "save": [52, 55, 66, 71, 72, 75, 76, 78, 79, 94, 125, 131, 141], "savefig": 66, "saveguard": 75, "saver": [55, 78, 79], "sc": 60, "scalar": 95, "scale": [25, 52, 54, 59, 69, 74, 80, 82, 85, 124, 125, 135], "scale_color_manu": 52, "scale_fill_manu": [52, 54], "scaled_psi": 74, "scatter": [59, 64, 71, 72, 80, 83, 85], "scatterplot": 64, "scenario": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 64, 73, 74, 84, 85, 95, 125, 131, 141], "scene": [67, 68, 70], "scene_camera": 70, "schacht": [70, 75, 76], "schaefer": 80, "schedul": 140, "scheme": [54, 77, 94, 95, 96, 108, 136], "schneider": 56, "schratz": [56, 94, 136, 138], "scienc": [18, 51, 65, 80, 139], "scikit": [75, 78, 94, 136, 138, 140, 141], "scipi": 66, "score": [0, 4, 5, 6, 9, 10, 20, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 82, 83, 84, 85, 86, 87, 92, 93, 94, 95, 96, 97, 99, 100, 101, 108, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 129, 130, 131, 133, 134, 135, 136, 140, 141], "scoring_method": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "script": 137, "sd": 51, "se": [52, 54, 66, 84, 88, 94, 108, 124, 125, 131, 139, 141], "se_df": 54, "se_dml": [52, 66, 88], "se_dml_po": [52, 66, 88], "se_nonorth": [52, 66], "se_orth_nosplit": [52, 66], "se_orth_po_nosplit": [52, 66], "seaborn": [21, 24, 58, 60, 62, 64, 66, 75, 77, 78, 79, 85, 86], "search": [20, 22, 23, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 94, 109], "search_mod": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 94], "searchabl": 55, "second": [16, 20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 56, 66, 75, 76, 77, 87, 88, 108, 124, 125, 126, 135, 138], "secondari": 64, "section": [23, 26, 53, 54, 55, 56, 73, 76, 77, 79, 85, 96, 99, 117, 127, 140], "secur": 80, "see": [7, 8, 13, 19, 20, 22, 23, 27, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 46, 47, 51, 53, 54, 55, 56, 58, 60, 61, 64, 65, 67, 68, 72, 74, 76, 77, 79, 80, 81, 83, 84, 85, 94, 95, 97, 100, 108, 109, 115, 117, 118, 119, 122, 123, 125, 126, 130, 131, 135, 137, 138, 140], "seed": [14, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55, 56, 57, 58, 59, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 131, 138, 141], "seek": 80, "seem": [53, 55, 73, 78, 79, 125, 141], "seen": [71, 72, 74], "sel_cols_chiang": 77, "select": [4, 5, 6, 14, 15, 19, 25, 37, 51, 70, 75, 83, 85, 87, 89, 91, 92, 94, 107, 124, 138, 139, 140, 141], "selected_coef": 75, "selected_featur": [56, 94], "selected_learn": 75, "self": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 75, 76, 141], "selfref": 55, "semenova": [67, 68, 139], "semi": 88, "semiparametr": 7, "sens": [84, 85], "sensemakr": [125, 126], "sensit": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 92, 93, 95, 99, 126, 131, 135, 140], "sensitivity_analysi": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 64, 73, 74, 84, 85, 125, 131, 141], "sensitivity_benchmark": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 64, 73, 84, 85, 125, 126], "sensitivity_el": [125, 131], "sensitivity_param": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 84, 85, 125, 126, 131], "sensitivity_plot": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 64, 73, 84, 85, 125, 131], "sensitivity_summari": [60, 64, 73, 74, 84, 85, 125, 131, 141], "sensitv": 74, "sensitvity_benchmark": 64, "sensiv": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39], "senstiv": [125, 134], "sep": 52, "separ": [21, 80, 84, 94, 95, 140], "seper": [76, 83, 84, 108, 124, 125, 126], "seq_len": [52, 57, 88], "sequenti": 8, "ser": [60, 61], "seri": [60, 61, 72, 85, 139], "serv": [25, 89, 90, 91, 138, 140], "serverless": [139, 140], "servic": 80, "set": [4, 5, 6, 7, 8, 9, 11, 16, 17, 18, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 45, 46, 47, 51, 52, 53, 54, 55, 57, 58, 60, 61, 62, 64, 65, 66, 67, 68, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 83, 84, 85, 86, 87, 88, 89, 91, 93, 95, 96, 99, 100, 108, 109, 110, 112, 113, 114, 117, 124, 125, 126, 132, 133, 134, 137, 138, 140, 141], "set_as_param": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "set_config": [42, 43, 46, 47], "set_fit_request": [46, 47], "set_fold_specif": 94, "set_index": 78, "set_ml_nuisance_param": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 55, 62, 78, 94, 140], "set_param": [42, 43, 46, 47, 76, 94], "set_sample_split": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 74, 75, 108, 140], "set_score_request": [42, 43, 46, 47], "set_styl": [78, 79], "set_text": 75, "set_threshold": [52, 53, 54, 55, 56, 57, 87, 94, 95, 108, 109, 124, 138], "set_tick": 77, "set_ticklabel": 77, "set_titl": [64, 74, 76, 77, 83], "set_x_d": [4, 5, 6], "set_xlabel": [64, 66, 74, 76, 77, 83], "set_xlim": 66, "set_xtick": 80, "set_xticklabel": 80, "set_ylabel": [64, 74, 76, 77, 80, 83], "set_ylim": [69, 74, 76, 77, 82], "setdiff": 140, "setdiff1d": 77, "setminu": [54, 77, 124], "settings_l": 76, "settings_m": 76, "setup": [137, 140], "seven": [54, 77], "sever": [48, 55, 56, 60, 61, 75, 76, 78, 79, 84, 85, 88, 94, 141], "shape": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 45, 46, 47, 59, 60, 64, 67, 68, 71, 72, 75, 77, 78, 81, 83, 84, 85, 94, 95], "share": [54, 55, 77, 78], "sharma": [85, 139], "sharp": 40, "shift": 60, "shock": [54, 77], "short": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 84, 85, 95, 99, 125, 126, 139, 140, 141], "shortcut": 55, "shortli": [54, 56, 77, 94], "shota": 139, "should": [4, 5, 6, 20, 21, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 55, 57, 60, 64, 71, 72, 75, 78, 83, 84, 86, 89, 91, 93, 94, 95, 102, 124, 125, 126, 136], "show": [51, 52, 54, 57, 58, 60, 62, 64, 65, 66, 67, 68, 70, 73, 74, 75, 76, 77, 80, 83, 85, 86, 88, 125, 134, 137], "showcas": 81, "showlabel": 85, "showlegend": 85, "shown": [51, 65, 80, 138], "showscal": [67, 68, 70], "shrink": 83, "shuffl": 108, "side": [83, 95, 125, 131], "sigma": [9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 37, 52, 54, 57, 66, 77, 86, 88, 93, 108, 124, 125, 126, 131, 134, 135], "sigma2": [125, 131], "sigma_": [10, 12, 13, 15, 16, 17, 19, 25, 26, 52, 54, 66, 77, 88], "sigma_0": [125, 135], "sigma_j": 124, "sigmoid": 80, "sign": 85, "signal": [44, 45], "signatur": [32, 33, 34, 35, 36, 38, 39, 109], "signif": [51, 53, 54, 55, 56, 57, 94, 95, 108, 109, 124, 138, 141], "signific": [51, 54, 55, 56, 57, 60, 64, 73, 74, 78, 81, 83, 84, 85, 94, 95, 108, 109, 124, 125, 131, 138, 141], "silverman": [34, 35, 36], "sim": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 41, 52, 53, 54, 57, 59, 66, 69, 77, 81, 82, 86, 88, 95], "sim_data": 61, "similar": [10, 14, 53, 56, 67, 68, 73, 76, 79, 83, 84, 85, 95], "similarli": 76, "simpl": [11, 20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 46, 47, 53, 56, 67, 68, 71, 72, 73, 74, 81, 85, 92, 95, 125, 126], "simplest": 93, "simpli": [56, 58, 141], "simplic": [55, 75, 78, 81, 85], "simplif": [125, 127], "simplifi": [60, 74, 80, 85, 93, 125, 134], "simul": [9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 25, 26, 52, 56, 57, 60, 61, 66, 67, 68, 69, 70, 71, 72, 75, 76, 82, 83, 85, 86, 88, 94, 124, 138], "simul_data": 37, "simulaten": [95, 102], "simulation_run": 70, "simult": 53, "simultan": [92, 141], "sin": [11, 14, 18, 59, 67, 68, 71, 72], "sinc": [9, 10, 42, 46, 55, 57, 58, 59, 64, 71, 72, 73, 75, 76, 78, 80, 86, 94, 95, 97, 125, 131, 133, 137, 140], "singl": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 58, 60, 61, 71, 72, 79, 80, 94, 124], "single_learner_pipelin": 94, "singleton": 108, "sinh": 18, "sipp": [55, 78, 79], "site": [60, 61, 77, 78], "situat": [54, 77], "six": [25, 54], "sixth": 77, "size": [21, 24, 37, 52, 54, 55, 56, 59, 60, 61, 66, 69, 70, 73, 75, 76, 78, 80, 81, 82, 85, 87, 89, 91, 94, 95, 96, 108, 109, 124, 138, 141], "sizeabl": 85, "skill": 139, "sklearn": [18, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 45, 46, 47, 60, 61, 62, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 83, 84, 85, 86, 87, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 131, 138, 141], "skotara": 85, "slide": 80, "slight": [95, 99], "slightli": [59, 60, 71, 72, 73, 75, 93, 95, 100, 109, 110, 112, 113, 125, 126], "sligthli": [20, 22, 23], "slow": [52, 66, 88], "slower": [52, 66, 88], "small": [11, 57, 58, 59, 74, 81, 86, 95, 125, 126, 133], "smaller": [55, 58, 71, 72, 73, 76, 78, 83, 85, 95, 141], "smallest": [22, 75], "smpl": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 66, 75, 77, 108, 109], "smpls_cluster": [54, 77], "sn": [58, 60, 62, 64, 66, 75, 77, 78, 79, 85, 86], "so": [46, 47, 51, 55, 56, 57, 58, 60, 65, 76, 78, 80, 85, 86, 94, 124, 141], "social": [80, 139], "societi": [54, 77, 85, 139], "softwar": [56, 94, 136, 138, 139, 140], "solari": 140, "sole": 85, "solut": [87, 93, 109], "solv": [27, 54, 77, 93, 94, 95, 99, 124], "solver": [78, 86, 95], "some": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 55, 56, 57, 58, 59, 62, 75, 76, 78, 79, 83, 84, 86, 93, 94, 95, 103, 137, 140], "sometim": [75, 95, 100], "sonabend": [56, 94], "soon": [95, 98, 109, 111, 125, 128], "sophist": 94, "sort": [21, 78, 95], "sort_bi": 21, "sort_valu": 64, "sourc": [56, 94, 138, 140], "sourcefileload": 70, "sp": 53, "space": [54, 77, 94], "spars": [70, 94, 124, 138, 139], "sparsiti": 139, "spec": 139, "special": [54, 77, 92, 95], "specif": [20, 22, 23, 25, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 40, 54, 55, 60, 61, 64, 74, 75, 77, 78, 85, 89, 91, 92, 93, 94, 95, 99, 108, 109, 117, 124, 131, 135, 136, 138], "specifi": [4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 54, 55, 56, 57, 58, 60, 61, 64, 65, 67, 68, 69, 71, 72, 74, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 95, 114, 117, 137, 138, 140, 141], "specifii": 79, "speed": [24, 30, 36, 75], "speedup": 75, "spefici": 32, "spindler": [15, 70, 75, 76, 85, 136, 139, 140], "spine": [78, 79], "spline": [67, 68, 93], "spline_basi": [67, 68, 93], "spline_grid": [67, 68], "split": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 54, 56, 57, 58, 60, 61, 74, 75, 77, 79, 81, 84, 86, 92, 93, 94, 95, 97, 99, 109, 124, 138, 140], "split_sampl": [74, 75], "sponsor": [55, 78, 79], "sprintf": 52, "sq_error": 70, "sqrt": [9, 10, 13, 14, 25, 26, 52, 54, 56, 62, 66, 69, 77, 82, 88, 108, 124, 125, 126, 138], "squar": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 55, 70, 78, 94, 95, 125, 135, 139], "squarederror": [55, 78, 141], "squeez": [58, 69, 82, 86], "src": 78, "ssm": [4, 5, 6, 19, 92, 107], "ssrn": 12, "stabil": 73, "stabl": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 136], "stack": [56, 94], "stackingclassifi": 83, "stackingregressor": 83, "stacklrn": 56, "stackrel": 95, "stage": [40, 67, 68, 71, 72, 81, 83, 94, 95, 140, 141], "stagger": [95, 100], "stai": [95, 100], "standard": [24, 26, 53, 56, 60, 61, 69, 71, 72, 83, 95, 96, 99, 108, 109, 124, 125, 131, 135, 140, 141], "standard_norm": [89, 91, 94, 124, 138], "standardscal": 78, "star": 95, "start": [25, 53, 55, 56, 60, 61, 67, 68, 70, 73, 75, 76, 77, 78, 82, 85, 95, 97, 136, 141], "start_dat": 25, "stat": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 66, 83, 89, 91, 94, 95, 124, 136, 139], "stat_bin": 52, "stat_dens": 55, "state": 141, "stationar": 58, "stationari": [95, 97], "statist": [16, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 54, 77, 84, 85, 124, 125, 131, 136, 138, 139, 140, 141], "statsmodel": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 83], "statu": [53, 55, 57, 58, 78, 80, 83, 86, 95, 100], "std": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 64, 65, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 80, 82, 84, 85, 86, 93, 94, 95, 96, 97, 99, 108, 109, 124, 138, 141], "stefan": 139, "step": [52, 55, 56, 66, 71, 72, 73, 78, 81, 88, 94, 95, 124, 136, 141], "stepdown": 124, "stick": [55, 78], "still": [57, 58, 67, 68, 71, 72, 73, 79, 83, 84, 86, 94, 95, 99], "stochast": [38, 39, 95, 105, 106, 138], "stock": [55, 78, 79], "store": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 87, 94, 108, 109, 124, 125, 131, 140], "store_model": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 76], "store_predict": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 78, 81], "stori": [85, 139], "str": [4, 5, 6, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 55, 60, 64, 71, 72, 82, 83, 93, 95, 140], "straightforward": [71, 72, 75, 93], "strategi": [80, 85, 95, 141], "stratifi": [74, 75], "stratum": 80, "strength": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 84, 85, 125, 126, 131, 134], "strftime": 60, "strictli": 95, "string": [20, 21, 22, 23, 24, 29, 31, 32, 33, 34, 35, 37, 38, 39, 93, 124, 125, 131, 138, 140], "string_label": 80, "strong": [57, 86, 125, 126], "stronger": [124, 141], "structur": [7, 8, 17, 25, 54, 55, 57, 70, 77, 78, 86, 88, 94, 136, 139, 141], "student": 139, "studi": [19, 54, 55, 70, 75, 76, 77, 78, 79, 84, 95, 96, 138, 141], "style": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 76, 140], "styler": 140, "styliz": 85, "sub": [42, 43, 46, 47, 54, 77], "subclass": [90, 91, 140], "subfold": 94, "subgroup": [32, 55, 78, 140], "subject": [54, 77], "submiss": 140, "submit": 60, "subobject": [42, 43, 46, 47], "subplot": [54, 59, 64, 66, 67, 68, 69, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83], "subplots_adjust": 75, "subpopul": [95, 107], "subsampl": [56, 75], "subscript": [95, 99, 109, 113, 125, 126], "subsequ": [54, 77], "subset": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 46, 54, 75, 77, 81, 87, 93, 94, 125, 126], "subseteq": 93, "substanti": [55, 78, 80], "substract": 124, "subtract": 124, "sudo": 137, "suffic": 85, "suffici": [75, 76, 85], "suggest": [54, 55, 77, 78, 85, 140], "suitabl": [57, 67, 68, 86, 95, 99], "sum": [43, 47, 54, 55, 77, 78, 79, 82, 83, 93, 124], "sum_": [25, 41, 52, 54, 66, 77, 83, 87, 88, 93, 95, 96, 100, 124], "sum_i": 80, "sum_oth": 77, "sum_riv": 77, "summar": [21, 53, 60, 61, 64, 80, 85, 87, 125, 131], "summari": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 53, 54, 56, 57, 58, 60, 61, 62, 64, 65, 67, 68, 69, 71, 72, 73, 74, 77, 79, 82, 84, 85, 86, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 108, 109, 124, 125, 138, 140, 141], "summary_result": 55, "suppli": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 67, 68, 71, 72, 73, 81, 93, 125, 126, 131, 132], "support": [11, 32, 40, 53, 54, 60, 61, 75, 77, 81, 83, 91, 94, 95, 103, 141], "support_s": [11, 67, 68, 71, 72, 81], "support_t": 81, "support_w": 81, "suppos": 85, "suppress": [53, 55, 56, 57], "suppresswarn": 52, "suprema": 124, "suptitl": [69, 75, 76, 79, 82], "supxlabel": [69, 79, 82], "supylabel": [69, 79, 82], "sure": [64, 94, 140], "surfac": [67, 68, 70], "surpress": [54, 138], "survei": [55, 78, 79, 141], "susan": 139, "sven": [85, 136, 139], "svenk": 77, "svenklaassen": [136, 140], "svg": [52, 66], "switch": [52, 66, 85, 88], "symbol": 85, "symmetr": 18, "syntax": [83, 95], "synthesi": 139, "synthet": [11, 25, 41, 51, 65, 67, 68, 69, 71, 72, 76, 81, 82], "syrgkani": [85, 139], "system": 139, "szita": 139, "t": [4, 5, 6, 9, 10, 14, 20, 21, 23, 24, 25, 26, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 90, 91, 93, 94, 95, 96, 97, 99, 100, 108, 109, 110, 124, 125, 127, 138, 141], "t_": [24, 60, 61, 95, 99, 109, 113, 125, 130], "t_1_start": 75, "t_1_stop": 75, "t_2_start": 75, "t_2_stop": 75, "t_3_start": 75, "t_3_stop": 75, "t_col": [4, 5, 6, 23, 24, 60, 61, 90, 91, 95, 96, 97, 99], "t_df": 81, "t_diff": 59, "t_dml": 52, "t_g": 25, "t_i": [58, 81, 83, 95, 97], "t_idx": 59, "t_nonorth": 52, "t_orth_nosplit": 52, "t_sigmoid": 81, "t_stat": 124, "t_value_ev": 22, "t_value_pr": 22, "tabl": [52, 54, 55, 56, 57, 64, 87, 89, 91, 94, 95, 108, 109, 124, 138, 141], "tabular": [75, 89, 91, 124, 138, 141], "taddi": 139, "take": [9, 10, 11, 32, 33, 38, 39, 57, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 75, 79, 82, 83, 84, 86, 87, 93, 94, 95, 96, 100, 103, 104, 105, 106, 109, 114, 117, 125, 132, 133, 134, 138], "taken": [55, 78, 79, 141], "taker": [32, 140], "talk": 141, "target": [20, 22, 23, 27, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 46, 47, 51, 54, 55, 56, 57, 67, 68, 75, 77, 93, 94, 95, 97, 99, 100, 101, 102, 103, 104, 107, 108, 109, 118, 119, 124, 125, 133, 135, 136, 138, 140, 141], "task": [51, 76, 89, 91, 108, 141], "task_typ": 140, "tau": [41, 59, 69, 79, 80, 82, 83, 93, 95, 109, 115, 118, 119], "tau_": [80, 83, 95], "tau_0": [83, 95], "tau_1": 80, "tau_2": 80, "tau_vec": [69, 79, 82], "tax": [55, 78, 79], "te": [53, 67, 68, 81], "techniqu": [52, 66, 88, 108, 141], "templat": 140, "ten": 76, "tend": [55, 78, 79, 95], "tensor": [67, 68], "tenth": 139, "term": [6, 22, 52, 54, 55, 56, 59, 66, 70, 77, 78, 80, 85, 88, 95, 100, 136, 141], "termin": [56, 94], "terminatorev": 56, "test": [8, 12, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 60, 61, 66, 73, 77, 85, 88, 94, 95, 108, 109, 124, 138, 139, 140, 141], "test_id": [54, 108], "test_ind": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "test_set": 108, "test_siz": 66, "text": [9, 10, 12, 14, 25, 26, 40, 41, 54, 55, 60, 61, 69, 70, 80, 81, 82, 83, 85, 93, 95, 99, 100, 108, 109, 113, 125, 130], "textbf": [87, 94, 141], "textposit": 85, "textrm": [125, 126, 132, 133, 134, 135], "tg": [56, 62, 89, 91, 138], "th": [54, 77], "than": [33, 52, 53, 55, 66, 70, 74, 75, 78, 79, 80, 83, 84, 85, 88, 95, 99, 125, 131, 141], "thank": [53, 55, 56, 78, 140], "thatw": 59, "thei": [53, 55, 59, 71, 72, 78, 80, 95, 125, 135], "them": [21, 55, 56, 67, 68, 69, 73, 76, 78, 82, 95], "theme": [54, 55], "theme_minim": [52, 55, 57], "theorem": [95, 100, 125, 135], "theoret": [75, 85, 108, 139], "theori": [93, 139], "therebi": [54, 56, 77, 141], "therefor": [61, 64, 80, 83, 84, 108, 109, 125, 134], "theta": [9, 10, 12, 13, 14, 16, 18, 19, 20, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 56, 57, 58, 59, 60, 64, 66, 70, 73, 74, 75, 77, 83, 84, 85, 86, 87, 88, 89, 91, 93, 94, 95, 96, 99, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 131, 134, 135, 138, 141], "theta_": [25, 60, 64, 83, 85, 93, 95, 101, 102, 124, 125, 135], "theta_0": [11, 32, 33, 38, 39, 52, 54, 55, 57, 64, 66, 67, 68, 70, 71, 72, 77, 78, 85, 86, 88, 93, 95, 97, 103, 104, 105, 106, 107, 109, 118, 119, 124, 125, 132, 133, 135, 138], "theta_dml": [52, 66, 88], "theta_dml_po": [52, 66, 88], "theta_initi": 66, "theta_nonorth": [52, 66], "theta_orth_nosplit": [52, 66], "theta_orth_po_nosplit": [52, 66], "theta_resc": 52, "theta_t": 59, "thi": [9, 10, 20, 21, 22, 23, 24, 25, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 93, 94, 95, 99, 100, 101, 102, 103, 104, 108, 109, 110, 112, 113, 115, 117, 118, 119, 124, 125, 126, 131, 132, 133, 136, 137, 138, 139, 140, 141], "think": 56, "third": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 61, 66, 77, 88, 108], "thirion": [136, 138], "this_df": [70, 78], "this_split_ind": 77, "those": [53, 55, 78, 79], "though": [51, 65, 80], "thread": [80, 94], "three": [54, 56, 71, 72, 137, 140], "threshold": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 83, 85, 95], "through": [53, 69, 71, 72, 82, 83, 94, 95], "throughout": 73, "thu": [76, 83, 93, 95], "tibbl": 53, "tick": 24, "tick_param": 83, "tight": 66, "tight_layout": [60, 76, 77, 83], "tighter": 83, "tild": [9, 10, 14, 25, 26, 54, 77, 80, 87, 93, 108, 109, 113, 118, 119, 122, 123, 124, 125, 134, 135], "time": [4, 5, 6, 15, 16, 20, 22, 24, 25, 52, 53, 54, 55, 57, 58, 59, 66, 70, 71, 72, 77, 78, 79, 83, 84, 85, 86, 90, 91, 95, 96, 97, 99, 100, 109, 125, 139, 140, 141], "time_budget": 76, "time_df": 59, "time_period": 59, "time_typ": [25, 60], "titiunik": [95, 139], "titl": [21, 24, 54, 55, 57, 60, 64, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 82, 83, 85, 136], "title_fonts": 60, "tmp": 72, "tname": 53, "tnr": [56, 94], "to_datetim": 60, "to_fram": 81, "to_numpi": [69, 73, 79, 82], "todo": [54, 62], "toeplitz": 70, "togeth": [71, 72, 124], "toler": 77, "tomasz": [139, 140], "toml": 140, "too": 75, "tool": [53, 56, 84, 141], "top": [54, 75, 77, 78, 79, 83, 85, 95, 136], "total": [43, 47, 55, 76, 78], "tpot": 76, "tracker": 136, "tradit": 124, "train": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 46, 47, 52, 54, 56, 66, 67, 68, 69, 71, 72, 74, 75, 77, 78, 81, 82, 87, 88, 108], "train_id": [54, 108], "train_ind": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "train_set": 108, "train_test_split": 66, "transact": 139, "transform": [9, 10, 41, 74, 80, 85, 141], "translat": 70, "transpos": 59, "treament": 81, "treat": [24, 25, 26, 33, 53, 58, 59, 60, 61, 64, 73, 81, 83, 85, 93, 95, 97, 99, 100, 104, 109, 113, 124, 141], "treat1_param": 80, "treat2_param": 80, "treat_var": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 94], "treatment": [4, 5, 6, 7, 8, 9, 10, 11, 14, 17, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 53, 54, 56, 57, 58, 59, 60, 61, 62, 64, 65, 70, 73, 75, 76, 77, 81, 83, 84, 85, 86, 88, 89, 90, 91, 92, 94, 96, 97, 99, 100, 101, 102, 103, 104, 107, 108, 110, 112, 113, 114, 115, 117, 118, 119, 124, 130, 131, 132, 134, 136, 138, 139, 140, 141], "treatment_df": 59, "treatment_effect": [11, 67, 68], "treatment_level": [29, 30, 64, 74, 95], "treatment_var": [4, 5, 6], "tree": [33, 45, 55, 56, 58, 59, 60, 75, 78, 87, 92, 94, 95, 108, 109, 124, 138, 140], "tree_param": [33, 45], "tree_summari": 78, "trees_class": [55, 78], "trend": [53, 58, 59, 60, 77, 95, 97, 99, 100, 139], "tri": [70, 125, 126], "triangular": [40, 83, 95], "trim": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 55, 78, 79, 85], "trimming_rul": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 79], "trimming_threshold": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 55, 67, 74, 78, 79, 81, 82, 85], "trm": [56, 94], "true": [4, 5, 6, 7, 8, 9, 14, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 91, 94, 95, 97, 108, 109, 114, 115, 118, 119, 122, 123, 124, 125, 127, 129, 130, 135, 138, 141], "true_effect": [59, 67, 68, 71, 72], "true_gatet_effect": 73, "true_group_effect": 73, "true_tau": 83, "truncat": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 79], "try": [75, 84], "tune": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 70, 75, 83, 92, 95, 136, 138, 140], "tune_on_fold": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 94], "tune_r": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "tune_set": [56, 94], "tuned_model": 76, "tuner": 94, "tunergridsearch": 56, "tupl": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 60, 95, 99], "turn": 85, "turrel": 18, "tutori": 55, "tw": [78, 79], "twice": 95, "twinx": 64, "two": [11, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 55, 56, 58, 60, 61, 65, 66, 69, 74, 75, 76, 78, 79, 80, 81, 82, 84, 85, 87, 88, 93, 94, 97, 99, 108, 118, 124, 141], "twoclass": 56, "twoearn": [55, 78, 79, 84, 141], "type": [6, 9, 10, 11, 14, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 55, 56, 60, 66, 75, 76, 77, 83, 85, 88, 92, 94, 95, 109, 120, 121, 124, 125, 134, 140, 141], "typeerror": [60, 61], "typic": [72, 95, 100, 136], "u": [9, 10, 11, 13, 19, 26, 32, 33, 34, 35, 36, 43, 47, 52, 53, 54, 55, 58, 59, 60, 64, 66, 69, 71, 72, 74, 75, 77, 78, 79, 81, 82, 84, 85, 88, 95, 101, 103, 104, 125, 126, 137, 141], "u_hat": [52, 66, 109], "u_i": [12, 15, 18, 19], "u_t": 26, "uehara": 139, "uhash": 56, "ulf": 139, "unambigu": 85, "uncertainti": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 71, 72, 74, 83, 84, 125, 131, 141], "unchang": [42, 43, 46, 47], "uncondit": [55, 60, 78, 141], "unconfounded": [85, 139], "under": [37, 52, 55, 58, 66, 78, 81, 83, 85, 88, 95, 99, 100, 107, 124, 139], "underbrac": [52, 59, 66, 88, 93], "underfit": 76, "underli": [9, 14, 55, 56, 60, 64, 71, 72, 80, 81, 95, 100, 125, 126, 141], "underlin": [54, 77], "underset": [83, 95], "understand": [60, 85], "undesir": 94, "unevenli": 108, "uniform": [26, 40, 41, 59, 65, 67, 68, 69, 81, 82, 124], "uniform_averag": [43, 47], "uniformli": [60, 69, 79, 124], "uniqu": [6, 51, 60, 61, 64, 65, 75, 83, 90, 91, 109, 125, 135], "unique_label": 76, "unit": [6, 25, 52, 53, 57, 58, 59, 60, 61, 73, 83, 86, 90, 91, 95, 97, 99, 100, 109, 110, 112, 113, 125, 130, 140], "univari": [11, 67, 68], "univers": 139, "unknown": 95, "unlik": [55, 78, 79, 85], "unobserv": [9, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 51, 55, 60, 65, 78, 79, 84, 85, 95, 125, 126, 135, 141], "unpen": 53, "unstabl": [125, 126], "unter": [54, 55, 56], "untest": 85, "until": [95, 97, 140], "untreat": [85, 95, 97], "up": [24, 30, 36, 55, 70, 75, 76, 78, 79, 84, 85, 94, 95, 97, 108, 125, 126, 137, 140, 141], "upcom": 140, "updat": [42, 43, 46, 47, 54, 72, 77, 78, 139, 140], "update_layout": [67, 68, 70, 83, 85], "update_trac": [67, 68], "upload": 140, "upon": [109, 140], "upper": [55, 56, 59, 60, 64, 66, 69, 73, 74, 79, 82, 83, 84, 85, 94, 125, 131, 135, 141], "upper_bound": [67, 68], "upsilon": [57, 86], "upsilon_i": [57, 86], "upward": [55, 78, 79, 85], "upweight": 80, "url": [70, 136, 139], "us": [4, 5, 6, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 52, 54, 55, 57, 58, 59, 60, 61, 64, 66, 67, 68, 69, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 91, 93, 95, 99, 108, 109, 110, 112, 113, 124, 125, 126, 131, 133, 134, 135, 136, 137, 138, 140, 141], "usa": 139, "usabl": 75, "usag": [53, 58, 60, 61, 62, 64, 73, 77, 78, 79, 84, 86, 89, 90, 91, 138, 140], "use_label_encod": [78, 141], "use_other_treat_as_covari": [4, 5, 6, 89, 91], "use_pred_offset": 94, "usecolormap": [67, 68], "user": [27, 28, 42, 43, 46, 47, 52, 53, 54, 55, 56, 64, 66, 73, 74, 75, 77, 78, 83, 84, 93, 94, 95, 109, 124, 136, 137, 138, 140, 141], "user_guid": 72, "userwarn": [60, 61, 78, 85], "usual": [54, 58, 60, 61, 67, 68, 75, 77, 83, 84, 85, 93, 94, 108, 125, 135], "util": [0, 28, 74, 75, 76, 80, 83, 94, 95, 140], "v": [7, 8, 13, 15, 16, 17, 19, 32, 33, 38, 39, 43, 47, 52, 54, 55, 60, 64, 66, 73, 74, 75, 76, 77, 78, 80, 83, 87, 88, 93, 95, 101, 103, 104, 105, 106, 124, 136, 138, 139, 140, 141], "v108": 136, "v12": [136, 138], "v22": 56, "v23": 136, "v_": [16, 54, 77, 95], "v_i": [12, 13, 17, 18, 19, 52, 66, 88, 95], "v_j": 124, "val": [13, 60, 61, 108, 139], "val_list": 70, "valid": [4, 5, 6, 12, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 53, 54, 55, 58, 66, 69, 75, 76, 77, 78, 79, 82, 88, 92, 93, 94, 108, 109, 115, 118, 119, 125, 126, 139, 141], "valu": [4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 46, 47, 48, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 83, 84, 85, 87, 90, 91, 92, 94, 95, 96, 100, 101, 107, 108, 115, 118, 119, 122, 123, 124, 125, 126, 131, 135, 138, 140, 141], "value_count": 78, "van": 139, "vanderpla": [136, 138], "vanish": [52, 66, 88], "var": [9, 10, 14, 25, 26, 54, 77, 80, 83, 125, 126, 132, 133, 134, 135], "var_ep": 85, "varepsilon": [9, 10, 16, 32, 54, 57, 77, 86, 93, 95, 103], "varepsilon_": [16, 25, 54, 60, 77], "varepsilon_0": 26, "varepsilon_1": 26, "varepsilon_d": [10, 14], "varepsilon_i": [14, 15, 57, 69, 82, 86], "vari": [25, 55, 59, 60, 75, 78, 80, 85], "variabl": [4, 5, 6, 7, 10, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 54, 55, 56, 57, 58, 60, 61, 62, 64, 70, 73, 76, 77, 78, 79, 83, 84, 85, 86, 89, 90, 91, 93, 94, 95, 97, 99, 100, 101, 103, 104, 105, 106, 108, 109, 124, 125, 126, 131, 135, 138, 139, 140, 141], "varianc": [27, 28, 54, 56, 77, 83, 84, 85, 92, 95, 108, 125, 126, 131, 133, 134, 135, 138], "variant": [53, 74], "variat": [10, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 74, 84, 125, 126, 135], "variou": [53, 76, 85, 94, 141], "varoquaux": [136, 138], "vasili": [85, 139], "vector": [11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 25, 29, 31, 32, 33, 34, 35, 37, 38, 39, 51, 54, 55, 57, 58, 65, 71, 72, 73, 77, 78, 81, 86, 95, 97, 105, 106, 107, 124, 138, 140], "vee": [109, 113, 125, 130], "venv": 137, "verbos": [55, 59, 60, 66, 75, 76, 83, 85], "veri": [53, 54, 56, 73, 75, 77, 85, 109, 136], "verifi": 80, "versa": [75, 80, 125, 131], "version": [9, 42, 43, 46, 47, 54, 55, 56, 85, 87, 93, 95, 99, 109, 124, 125, 127, 129, 130, 132, 133, 140], "versoin": 85, "versu": 72, "vertic": [54, 64, 77], "via": [9, 10, 20, 22, 23, 26, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 53, 57, 58, 59, 60, 61, 69, 70, 71, 72, 73, 74, 75, 83, 84, 86, 87, 89, 91, 92, 93, 94, 95, 96, 97, 99, 108, 115, 123, 124, 125, 126, 131, 135, 136, 137, 138, 139, 140, 141], "viabl": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "vice": [75, 80, 125, 131], "victor": [70, 85, 108, 136, 139], "view": 72, "vignett": [53, 140], "villa": [51, 65], "violat": 60, "violet": [69, 79, 82], "vira": 139, "virtual": 137, "virtualenv": 137, "visibl": [79, 83, 85], "visit": [136, 141], "visual": [21, 54, 60, 61, 73, 74, 76, 77, 83], "vol": 53, "volum": [85, 136], "voluntari": 80, "vv740": 77, "vv760g": 77, "w": [7, 8, 9, 10, 17, 25, 26, 27, 28, 42, 43, 46, 47, 54, 70, 77, 80, 81, 87, 88, 95, 99, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 129, 130, 131, 132, 133, 134, 135, 136, 138], "w24678": 108, "w30302": 139, "w_": [25, 26, 54, 77, 81, 95], "w_1": [25, 26, 81], "w_2": [25, 26, 81], "w_3": [25, 26], "w_4": [25, 26], "w_df": 81, "w_i": [19, 58, 81, 83, 87, 93, 95, 108, 109, 113, 124], "wa": [54, 59, 76, 77, 85, 140], "wager": 139, "wai": [55, 75, 76, 78, 85, 94, 109, 137], "wander": 18, "wang": 139, "want": [51, 54, 55, 56, 58, 65, 69, 75, 77, 82, 83, 94, 95, 136, 137, 139], "warn": [21, 24, 51, 52, 53, 54, 55, 56, 57, 60, 61, 66, 78, 85, 87, 94, 95, 108, 109, 124, 138, 140], "wayon": 54, "we": [33, 45, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 93, 94, 95, 96, 99, 100, 104, 108, 109, 113, 116, 124, 125, 126, 135, 137, 138, 140, 141], "weak": [125, 126, 139], "weakest": 60, "wealth": [7, 84], "websit": [55, 56, 94, 136], "wedg": [54, 77], "week": 140, "wei": 124, "weight": [21, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 42, 43, 46, 47, 54, 55, 56, 57, 60, 61, 64, 73, 74, 77, 78, 83, 86, 92, 94, 95, 96, 109, 114, 117, 124, 125, 132, 133, 140], "weights_bar": [29, 33, 74], "weights_dict": 74, "weiss": [136, 138], "well": [4, 5, 6, 46, 47, 52, 54, 66, 70, 75, 76, 77, 87, 88, 91, 108, 137, 138], "were": [55, 57, 78, 79, 86, 141], "what": [53, 70, 75, 139], "when": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 55, 58, 72, 74, 78, 80, 95, 104, 107, 109, 124, 136, 137, 138, 140], "whenev": [55, 78], "whera": [125, 133], "where": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 43, 44, 45, 47, 51, 52, 54, 55, 57, 58, 59, 60, 61, 64, 65, 66, 69, 73, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 93, 94, 95, 96, 97, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 131, 132, 133, 135, 137, 138, 140, 141], "wherea": [11, 57, 58, 60, 64, 85, 86, 95, 99, 109, 117, 125, 132, 141], "whether": [4, 5, 6, 11, 14, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 55, 59, 75, 78, 79, 83, 85, 89, 91, 94, 95, 125, 126, 140], "which": [4, 5, 6, 9, 11, 20, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 46, 51, 52, 53, 55, 56, 57, 58, 60, 61, 63, 64, 65, 66, 70, 72, 73, 75, 76, 78, 79, 81, 83, 84, 85, 86, 88, 89, 91, 93, 94, 95, 99, 109, 124, 125, 126, 131, 132, 133, 135, 137, 140, 141], "while": [51, 65, 95], "white": [54, 71, 72, 77, 85], "whitegrid": [78, 79], "whitnei": [85, 139], "who": [53, 55, 78, 85], "whole": [52, 58, 66, 83, 88, 94, 125, 126], "whom": 95, "widehat": [60, 61, 95], "width": [21, 24, 52, 54, 67, 68, 70], "wiki": 140, "wiksel": 139, "wild": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 124], "window": 137, "wise": [71, 72], "wish": 137, "within": [40, 54, 60, 61, 71, 72, 77, 81, 83], "without": [14, 20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 40, 51, 52, 60, 61, 65, 66, 75, 76, 85, 88, 92, 94, 95, 125, 126, 137, 140], "wolf": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 124], "won": 85, "word": [40, 83, 95, 140, 141], "work": [42, 43, 46, 47, 60, 61, 63, 64, 72, 73, 75, 80, 84, 85, 94, 95, 124, 137, 139], "workflow": [136, 140], "workspac": 78, "world": 139, "worri": 85, "wors": [43, 47], "would": [43, 47, 53, 55, 56, 60, 61, 67, 68, 70, 75, 78, 79, 83, 84, 85, 93, 94, 125, 135, 141], "wrapper": [53, 83, 94], "write": [52, 53, 57, 58, 66, 72, 86, 88, 125, 135], "written": [95, 109, 125, 132, 133], "wrong": [75, 80], "wspace": 75, "wurd": [54, 55, 56], "www": [136, 137], "x": [4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 31, 32, 33, 35, 37, 38, 39, 40, 41, 42, 43, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 87, 88, 89, 91, 93, 94, 95, 97, 99, 101, 102, 103, 104, 105, 106, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 129, 130, 132, 133, 134, 135, 138, 141], "x0": [64, 80, 83], "x1": [54, 56, 57, 58, 64, 74, 76, 77, 80, 83, 84, 85, 86, 89, 91, 93, 94, 95, 109, 124, 125, 126, 138], "x10": [54, 56, 57, 74, 76, 77, 86, 89, 91, 94, 95, 109, 124, 138], "x100": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x11": [54, 56, 57, 77, 86, 89, 91, 94, 95, 109, 124, 138], "x12": [54, 56, 57, 77, 86, 89, 91, 94, 95, 109, 124, 138], "x13": [54, 56, 57, 77, 86, 89, 91, 94, 95, 109, 124, 138], "x14": [54, 56, 57, 77, 86, 89, 91, 94, 95, 109, 124, 138], "x15": [54, 56, 57, 77, 86, 89, 91, 94, 95, 109, 124, 138], "x16": [54, 56, 57, 77, 86, 89, 91, 94, 95, 109, 124, 138], "x17": [54, 56, 57, 77, 86, 89, 91, 94, 95, 109, 124, 138], "x18": [54, 56, 57, 77, 86, 89, 91, 94, 95, 109, 124, 138], "x19": [54, 56, 57, 77, 86, 89, 91, 94, 95, 109, 124, 138], "x1x2x3x4x5x6x7x8x9x10": 54, "x2": [54, 56, 57, 58, 64, 74, 76, 77, 83, 84, 85, 86, 89, 91, 93, 94, 95, 109, 124, 138], "x20": [54, 56, 57, 77, 86, 89, 91, 94, 95, 109, 124, 138], "x21": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x22": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x23": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x24": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x25": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x26": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x27": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x28": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x29": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x2_dummi": 85, "x2_preds_control": 85, "x2_preds_treat": 85, "x3": [54, 56, 57, 58, 64, 74, 76, 77, 84, 85, 86, 89, 91, 93, 94, 95, 109, 124, 138], "x30": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x31": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x32": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x33": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x34": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x35": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x36": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x37": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x38": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x39": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x4": [54, 56, 57, 58, 64, 74, 76, 77, 84, 85, 86, 89, 91, 94, 95, 109, 124, 138], "x40": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x41": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x42": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x43": [54, 56, 57, 76, 77, 86, 89, 91, 95, 138], "x44": [54, 56, 57, 76, 77, 86, 89, 91, 95, 138], "x45": [54, 56, 57, 76, 77, 86, 89, 91, 95, 138], "x46": [54, 56, 57, 76, 77, 86, 89, 91, 95, 138], "x47": [54, 56, 57, 76, 77, 86, 89, 91, 95, 138], "x48": [54, 56, 57, 76, 77, 86, 89, 91, 95, 138], "x49": [54, 56, 57, 76, 77, 86, 89, 91, 95, 138], "x5": [54, 56, 57, 74, 76, 77, 85, 86, 89, 91, 94, 95, 109, 124, 138], "x50": [54, 56, 57, 76, 77, 86, 89, 91, 95, 138], "x51": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x52": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x53": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x54": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x55": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x56": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x57": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x58": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x59": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x6": [54, 56, 57, 74, 76, 77, 86, 89, 91, 94, 95, 109, 124, 138], "x60": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x61": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x62": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x63": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x64": [54, 56, 57, 60, 61, 77, 78, 86, 89, 91, 95, 138], "x65": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x66": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x67": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x68": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x69": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x7": [54, 56, 57, 74, 76, 77, 86, 89, 91, 94, 95, 109, 124, 138], "x70": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x71": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x72": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x73": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x74": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x75": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x76": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x77": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x78": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x79": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x8": [54, 56, 57, 74, 76, 77, 86, 89, 91, 94, 95, 109, 124, 138], "x80": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x81": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x82": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x83": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x84": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x85": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x86": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x87": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x88": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x89": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x9": [54, 56, 57, 74, 76, 77, 86, 89, 91, 94, 95, 109, 124, 138], "x90": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x91": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x92": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x93": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x94": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x95": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x96": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x96x97x98x99x100ydcluster_var_icluster_var_jz": 54, "x97": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x98": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x99": [54, 56, 57, 77, 86, 89, 91, 95, 138], "x_": [16, 17, 25, 52, 54, 59, 66, 77, 85, 88], "x_0": [59, 67, 68, 71, 72, 73], "x_1": [9, 10, 14, 25, 26, 38, 39, 59, 67, 68, 69, 71, 72, 73, 82, 85, 95, 105, 106, 125, 126, 138], "x_1x_3": [69, 82], "x_2": [9, 10, 14, 25, 26, 59, 67, 68, 69, 71, 72, 73, 82, 85, 125, 126], "x_3": [9, 10, 14, 25, 26, 59, 67, 68, 71, 72, 73, 125, 126], "x_4": [9, 10, 14, 25, 26, 67, 68, 69, 71, 72, 73, 82], "x_5": [9, 10, 14, 67, 68, 71, 72], "x_6": [67, 68, 71, 72], "x_7": [67, 68, 71, 72], "x_8": [67, 68, 71, 72], "x_9": [67, 68, 71, 72], "x_binary_control": 85, "x_binary_tr": 85, "x_col": [4, 5, 6, 24, 51, 54, 55, 56, 60, 61, 65, 70, 77, 78, 79, 81, 83, 84, 85, 89, 90, 91, 94, 95, 96, 99, 138, 140, 141], "x_cols_bench": 85, "x_cols_binari": 85, "x_cols_poli": 77, "x_conf": 82, "x_conf_tru": 82, "x_df": 59, "x_domain": 56, "x_i": [11, 12, 13, 15, 17, 18, 19, 41, 52, 57, 58, 66, 69, 71, 72, 80, 82, 83, 86, 88, 93, 95, 97, 99, 100, 107, 109, 113], "x_p": [38, 39, 95, 105, 106, 138], "x_train": 76, "x_true": [69, 82], "x_var": 56, "xaxis_titl": [67, 68, 70, 83, 85], "xformla": 53, "xgb": 76, "xgb_untuned_l": 76, "xgb_untuned_m": 76, "xgbclassifi": [75, 78, 80, 141], "xgboost": [52, 55, 75, 78, 80, 141], "xgbregressor": [75, 76, 78, 80, 141], "xi": [14, 25, 26, 95], "xi_": 124, "xi_0": [16, 54, 77], "xi_i": [57, 86], "xiaoji": 139, "xintercept": [52, 57], "xlab": [52, 54, 55], "xlabel": [59, 60, 64, 67, 68, 69, 71, 72, 76, 78, 79, 82], "xlim": [52, 55], "xtick": [64, 76], "xval": [56, 94], "xx": 66, "y": [4, 5, 6, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 101, 103, 104, 105, 106, 108, 109, 110, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 131, 132, 133, 134, 135, 138, 141], "y0": [53, 60, 64, 69, 82], "y0_cvar": 69, "y0_quant": [69, 82], "y1": [53, 60, 69, 82], "y1_cvar": 69, "y1_quant": [69, 82], "y_": [16, 24, 25, 54, 57, 58, 59, 60, 77, 86, 95, 97, 99, 100, 107, 109, 113], "y_0": [20, 22, 26, 41, 109, 112], "y_1": [20, 22, 26, 41, 109, 112], "y_col": [4, 5, 6, 24, 51, 52, 54, 55, 56, 57, 60, 61, 65, 67, 68, 70, 71, 72, 74, 77, 78, 79, 81, 83, 84, 87, 88, 89, 90, 91, 94, 95, 96, 99, 108, 109, 138, 140, 141], "y_df": [59, 81], "y_diff": 59, "y_i": [11, 12, 13, 15, 17, 18, 19, 52, 57, 58, 66, 69, 80, 81, 82, 83, 86, 88, 95, 97, 107], "y_label": [21, 24], "y_lower_quantil": 60, "y_mean": 60, "y_pred": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 75, 94], "y_train": 76, "y_true": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 75, 94], "y_upper_quantil": 60, "ya": 139, "yasui": 139, "yata": 139, "yaxis_titl": [67, 68, 70, 83, 85], "year": 136, "yerr": [59, 64, 71, 72, 76, 78, 80, 83], "yet": [54, 60, 61, 63, 95, 99, 100], "yggvpl": 77, "yield": 95, "yintercept": 55, "ylab": [52, 54, 55], "ylabel": [59, 60, 64, 67, 68, 69, 71, 72, 76, 78, 79, 82], "ylim": 78, "ymax": 55, "ymin": 55, "yname": 53, "york": 139, "you": [42, 43, 46, 47, 51, 52, 59, 60, 61, 65, 72, 77, 84, 95, 136, 137, 141], "your": [75, 137], "ython": 136, "yukun": 139, "yusuk": 139, "yuya": 139, "yy": 66, "z": [4, 5, 6, 9, 10, 12, 14, 15, 16, 19, 25, 26, 32, 34, 37, 38, 51, 54, 55, 57, 60, 65, 67, 68, 70, 77, 78, 82, 85, 86, 93, 95, 103, 105, 109, 116, 118, 120, 123, 124, 140], "z1": [6, 24, 38, 60, 90, 91, 95, 96, 97, 99], "z2": [6, 24, 60, 90, 91, 95, 96, 97, 99], "z3": [6, 24, 60, 90, 91, 95, 96, 97, 99], "z4": [6, 24, 60, 90, 91, 95, 96, 97, 99], "z_": [16, 54, 77], "z_1": [9, 10, 14, 60], "z_2": [9, 10, 14], "z_3": [9, 10, 14], "z_4": [9, 10, 14, 60], "z_5": 9, "z_col": [4, 5, 6, 32, 34, 38, 51, 54, 55, 57, 65, 77, 78, 79, 86, 89, 91, 93, 95, 140], "z_i": [15, 19, 57, 82, 86, 95], "z_j": [9, 10, 14, 25, 26], "z_true": 82, "zadik": 139, "zaxis_titl": [67, 68, 70], "zero": [22, 26, 41, 58, 59, 60, 69, 74, 75, 81, 82, 84, 85, 95, 109, 113, 124, 125, 130], "zeros_lik": 82, "zeta": [32, 38, 39, 55, 78, 93, 95, 103, 105, 106, 138], "zeta_": [16, 54, 77], "zeta_0": [16, 54, 77], "zeta_i": [13, 15, 17, 52, 66, 88], "zeta_j": 124, "zhang": 139, "zhao": [9, 10, 14, 20, 22, 23, 26, 53, 58, 60, 95, 97, 100, 139], "zimmert": [58, 95, 100, 139], "zip": [67, 68], "zorder": 64, "\u03c4_x0": 80, "\u03c4_x1": 80, "\u2139": 52}, "titles": ["API Reference", "<span class=\"section-number\">1. </span>DoubleML Data Class", "<span class=\"section-number\">3. </span>Datasets", "<span class=\"section-number\">2. </span>DoubleML Models", "<span class=\"section-number\">1.2. </span>doubleml.data.DoubleMLClusterData", "<span class=\"section-number\">1.1. </span>doubleml.data.DoubleMLData", "<span class=\"section-number\">1.3. </span>doubleml.data.DoubleMLPanelData", "<span class=\"section-number\">3.1.1. </span>doubleml.datasets.fetch_401K", "<span class=\"section-number\">3.1.2. </span>doubleml.datasets.fetch_bonus", "<span class=\"section-number\">3.2.9. </span>doubleml.datasets.make_confounded_irm_data", "<span class=\"section-number\">3.2.8. </span>doubleml.datasets.make_confounded_plr_data", "<span class=\"section-number\">3.2.10. </span>doubleml.datasets.make_heterogeneous_data", "<span class=\"section-number\">3.2.4. </span>doubleml.datasets.make_iivm_data", "<span class=\"section-number\">3.2.3. </span>doubleml.datasets.make_irm_data", "<span class=\"section-number\">3.2.11. </span>doubleml.datasets.make_irm_data_discrete_treatments", "<span class=\"section-number\">3.2.2. </span>doubleml.datasets.make_pliv_CHS2015", "<span class=\"section-number\">3.2.6. </span>doubleml.datasets.make_pliv_multiway_cluster_CKMS2021", "<span class=\"section-number\">3.2.1. </span>doubleml.datasets.make_plr_CCDDHNR2018", "<span class=\"section-number\">3.2.5. </span>doubleml.datasets.make_plr_turrell2018", "<span class=\"section-number\">3.2.7. </span>doubleml.datasets.make_ssm_data", "<span class=\"section-number\">2.3.4. </span>doubleml.did.DoubleMLDID", "<span class=\"section-number\">2.3.2. </span>doubleml.did.DoubleMLDIDAggregation", "<span class=\"section-number\">2.3.3. </span>doubleml.did.DoubleMLDIDBinary", "<span class=\"section-number\">2.3.5. </span>doubleml.did.DoubleMLDIDCS", "<span class=\"section-number\">2.3.1. </span>doubleml.did.DoubleMLDIDMulti", "<span class=\"section-number\">3.2.13. </span>doubleml.did.datasets.make_did_CS2021", "<span class=\"section-number\">3.2.12. </span>doubleml.did.datasets.make_did_SZ2020", "<span class=\"section-number\">5.1. </span>doubleml.double_ml_score_mixins.LinearScoreMixin", "<span class=\"section-number\">5.2. </span>doubleml.double_ml_score_mixins.NonLinearScoreMixin", "<span class=\"section-number\">2.2.2. </span>doubleml.irm.DoubleMLAPO", "<span class=\"section-number\">2.2.3. </span>doubleml.irm.DoubleMLAPOS", "<span class=\"section-number\">2.2.7. </span>doubleml.irm.DoubleMLCVAR", "<span class=\"section-number\">2.2.4. </span>doubleml.irm.DoubleMLIIVM", "<span class=\"section-number\">2.2.1. </span>doubleml.irm.DoubleMLIRM", "<span class=\"section-number\">2.2.6. </span>doubleml.irm.DoubleMLLPQ", "<span class=\"section-number\">2.2.5. </span>doubleml.irm.DoubleMLPQ", "<span class=\"section-number\">2.2.8. </span>doubleml.irm.DoubleMLQTE", "<span class=\"section-number\">2.2.9. </span>doubleml.irm.DoubleMLSSM", "<span class=\"section-number\">2.1.2. </span>doubleml.plm.DoubleMLPLIV", "<span class=\"section-number\">2.1.1. </span>doubleml.plm.DoubleMLPLR", "<span class=\"section-number\">2.4.1. </span>doubleml.rdd.RDFlex", "<span class=\"section-number\">3.2.14. </span>doubleml.rdd.datasets.make_simple_rdd_data", "<span class=\"section-number\">4.1.2. </span>doubleml.utils.DMLDummyClassifier", "<span class=\"section-number\">4.1.1. </span>doubleml.utils.DMLDummyRegressor", "<span class=\"section-number\">4.1.3. </span>doubleml.utils.DoubleMLBLP", "<span class=\"section-number\">4.1.4. </span>doubleml.utils.DoubleMLPolicyTree", "<span class=\"section-number\">4.1.6. </span>doubleml.utils.GlobalClassifier", "<span class=\"section-number\">4.1.5. </span>doubleml.utils.GlobalRegressor", "<span class=\"section-number\">4.2.1. </span>doubleml.utils.gain_statistics", "<span class=\"section-number\">5. </span>Score Mixin Classes for DoubleML Models", "<span class=\"section-number\">4. </span>Utility Classes and Functions", "R: Basic Instrumental Variables Calculation", "R: Basics of Double Machine Learning", "R: DoubleML for Difference-in-Differences", "R: Cluster Robust Double Machine Learning", "R: Impact of 401(k) on Financial Wealth", "R: Ensemble Learners and More with <code class=\"docutils literal notranslate\"><span class=\"pre\">mlr3pipelines</span></code>", "R: Sample Selection Models", "Python: Difference-in-Differences", "Python: Difference-in-Differences Pre-Testing", "Python: Panel Data with Multiple Time Periods", "Python: Panel Data Introduction", "DML: Bonus Data", "Examples", "Python: Average Potential Outcome (APO) Models", "Python: Basic Instrumental Variables calculation", "Python: Basics of Double Machine Learning", "Python: Conditional Average Treatment Effects (CATEs) for IRM models", "Python: Conditional Average Treatment Effects (CATEs) for PLR models", "Python: Conditional Value at Risk of potential outcomes", "Python: First Stage and Causal Estimation", "Python: Group Average Treatment Effects (GATEs) for IRM models", "Python: Group Average Treatment Effects (GATEs) for PLR models", "Python: GATE Sensitivity Analysis", "Python: IRM and APO Model Comparison", "Python: Choice of learners", "DoubleML meets FLAML - How to tune learners automatically within <code class=\"docutils literal notranslate\"><span class=\"pre\">DoubleML</span></code>", "Python: Cluster Robust Double Machine Learning", "Python: Impact of 401(k) on Financial Wealth", "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)", "Python: PLM and IRM for Multiple Treatments", "Python: Policy Learning with Trees", "Python: Potential Quantiles and Quantile Treatment Effects", "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)", "Python: Sensitivity Analysis", "Example: Sensitivity Analysis for Causal ML", "Python: Sample Selection Models", "<span class=\"section-number\">6. </span>Double machine learning algorithms", "<span class=\"section-number\">1. </span>The basics of double/debiased machine learning", "DoubleMLData from dataframes", "&lt;no title&gt;", "<span class=\"section-number\">2. </span>Data Backend", "User Guide", "<span class=\"section-number\">4. </span>Heterogeneous treatment effects", "<span class=\"section-number\">7. </span>Learners, hyperparameters and hyperparameter tuning", "<span class=\"section-number\">3. </span>Models", "&lt;no title&gt;", "Panel Data", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">9. </span>Sample-splitting, cross-fitting and repeated cross-fitting", "<span class=\"section-number\">5. </span>Score functions", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">8. </span>Variance estimation and confidence intervals", "<span class=\"section-number\">10. </span>Sensitivity analysis", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "DoubleML", "Installing DoubleML", "Getting Started", "Double Machine Learning Literature", "Release Notes", "DoubleML Workflow"], "titleterms": {"": 76, "0": 141, "1": [76, 85, 141], "2": [76, 85, 141], "2011": 85, "2023": 85, "3": [76, 85, 141], "4": [85, 141], "401": [55, 78, 79, 84], "5": [85, 141], "6": 141, "7": 141, "95": 76, "A": [54, 77], "ATE": [57, 73, 80, 86], "No": [54, 77], "One": [54, 67, 68, 77], "The": [55, 78, 80, 88, 138], "acknowledg": [53, 136], "acycl": [51, 65], "addit": 80, "adjust": [60, 83], "advanc": [83, 94, 124], "aggreg": [60, 61, 95], "al": 85, "algorithm": [87, 125, 136, 138], "all": 60, "altern": 109, "analysi": [60, 64, 73, 74, 84, 85, 125, 141], "anticip": 60, "api": [0, 76], "apo": [64, 74, 95, 109, 125], "applic": [54, 77, 84], "approach": [52, 66, 75, 88], "arah": 85, "arbitrari": 80, "arrai": [89, 91], "asset": [55, 78], "assumpt": 85, "att": [58, 60, 61], "augment": 80, "automat": 76, "automl": 76, "averag": [55, 64, 67, 68, 71, 72, 74, 78, 93, 95, 109, 125], "backend": [54, 55, 77, 78, 91, 138, 141], "band": 124, "base": 56, "basic": [51, 52, 60, 65, 66, 88], "benchmark": [84, 85, 125], "bia": [52, 66, 88], "binari": [95, 109], "bonu": 62, "bootstrap": 124, "build": 137, "calcul": [51, 65], "call": 76, "callabl": 109, "case": 63, "cate": [67, 68, 80, 93], "causal": [62, 64, 70, 85, 109, 138, 141], "chernozhukov": 85, "choic": 75, "citat": 136, "class": [1, 49, 50, 54, 77], "cluster": [54, 77], "code": 136, "coeffici": 76, "combin": [60, 70], "compar": [75, 76], "comparison": [53, 74, 76], "comput": [75, 76], "conclus": [76, 85], "conda": 137, "condit": [67, 68, 69, 79, 93, 109], "confid": [76, 124], "construct": 94, "contrast": 64, "control": 60, "covari": [60, 83], "coverag": [58, 70], "cran": 137, "creat": 76, "cross": [54, 58, 77, 95, 97, 108, 109, 125, 138], "custom": [75, 76], "cvar": [69, 79, 93, 109], "dag": [51, 65], "data": [1, 4, 5, 6, 51, 52, 54, 55, 56, 57, 58, 60, 61, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 88, 91, 95, 97, 109, 125, 138, 141], "datafram": [89, 91], "dataset": [2, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 41, 62], "debias": [52, 66, 88, 138], "default": 76, "defin": [54, 77], "demo": 53, "depend": 137, "descript": 60, "design": [83, 95], "detail": [53, 60, 61, 95], "develop": 137, "dgp": [52, 64, 66], "did": [3, 20, 21, 22, 23, 24, 25, 26, 53, 95], "differ": [53, 58, 59, 63, 75, 95, 109, 124, 125], "dimension": [67, 68], "direct": [51, 65], "disclaim": 85, "discontinu": [83, 95], "distribut": [57, 86], "dml": [54, 62, 77, 108, 138, 141], "dml1": 87, "dml2": 87, "dmldummyclassifi": 42, "dmldummyregressor": 43, "doubl": [52, 54, 66, 77, 87, 88, 136, 138, 139], "double_ml_score_mixin": [27, 28], "doubleml": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 53, 55, 56, 65, 76, 78, 84, 85, 124, 136, 137, 141], "doublemlapo": [29, 30], "doublemlblp": 44, "doublemlclusterdata": [4, 54, 77], "doublemlcvar": 31, "doublemldata": [5, 55, 78, 89, 91, 138], "doublemldid": 20, "doublemldidaggreg": 21, "doublemldidbinari": 22, "doublemldidc": 23, "doublemldidmulti": 24, "doublemliivm": 32, "doublemlirm": 33, "doublemllpq": 34, "doublemlpaneldata": [6, 60, 91], "doublemlpliv": [38, 54, 77], "doublemlplr": 39, "doublemlpolicytre": 45, "doublemlpq": 35, "doublemlqt": 36, "doublemlssm": 37, "effect": [55, 60, 61, 63, 67, 68, 69, 71, 72, 74, 78, 79, 80, 82, 84, 85, 93, 95], "elig": [55, 78], "empir": 70, "ensembl": [56, 83], "error": [54, 77], "estim": [51, 55, 57, 58, 60, 61, 62, 65, 70, 73, 76, 78, 79, 80, 82, 84, 85, 86, 108, 109, 124, 138, 141], "et": 85, "evalu": [75, 76, 94], "event": [60, 61], "exampl": [53, 54, 63, 67, 68, 77, 84, 85], "exploit": [53, 56], "extern": [94, 108], "featur": [56, 136], "fetch_401k": 7, "fetch_bonu": 8, "figur": 80, "file": 137, "final": 53, "financi": [55, 78, 79], "first": 70, "fit": [54, 76, 77, 108, 138], "flaml": 76, "flexibl": 83, "fold": [76, 108], "forest": 62, "formul": [85, 141], "from": [53, 56, 89, 91, 137], "full": 76, "function": [50, 53, 54, 77, 109, 138], "fuzzi": [83, 95], "gain_statist": 48, "gate": [71, 72, 73, 93], "gatet": 73, "gener": [2, 52, 63, 64, 66, 76, 83, 88, 125], "get": 138, "github": 137, "global": 83, "globalclassifi": 46, "globalregressor": 47, "graph": [51, 65], "group": [60, 61, 71, 72, 93], "guid": 92, "helper": [54, 77], "heterogen": [63, 74, 80, 93], "how": [56, 76], "hyperparamet": [74, 94], "identif": 85, "iivm": [55, 78, 95, 109], "impact": [55, 78, 79], "implement": [87, 95, 109, 125], "induc": [52, 66, 88], "infer": [124, 141], "initi": [54, 76, 77], "instal": 137, "instrument": [51, 65], "integr": 53, "interact": [55, 71, 78, 81, 95, 109, 125], "interv": [76, 124], "introduct": 61, "invers": 80, "irm": [3, 29, 30, 31, 32, 33, 34, 35, 36, 37, 55, 62, 67, 71, 74, 78, 80, 81, 84, 93, 95, 109, 125], "iv": [51, 55, 65, 78, 95, 109], "k": [55, 78, 79, 84, 108], "lambda": 70, "lasso": [62, 70], "latest": 137, "lear": [54, 77], "learn": [52, 54, 66, 77, 81, 87, 88, 93, 136, 138, 139], "learner": [56, 62, 74, 75, 76, 83, 94, 138], "less": 76, "level": 95, "linear": [55, 60, 72, 78, 80, 83, 95, 109, 125], "linearscoremixin": 27, "literatur": 139, "load": [54, 62, 77, 85], "loader": 2, "local": [55, 78, 79, 82, 83, 109], "loss": 70, "lpq": [82, 109], "lqte": [79, 82], "m": 108, "machin": [52, 54, 66, 77, 87, 88, 136, 138, 139], "main": 136, "mainten": 136, "make_confounded_irm_data": 9, "make_confounded_plr_data": 10, "make_did_cs2021": 25, "make_did_sz2020": 26, "make_heterogeneous_data": 11, "make_iivm_data": 12, "make_irm_data": 13, "make_irm_data_discrete_treat": 14, "make_pliv_chs2015": 15, "make_pliv_multiway_cluster_ckms2021": 16, "make_plr_ccddhnr2018": 17, "make_plr_turrell2018": 18, "make_simple_rdd_data": 41, "make_ssm_data": 19, "mar": [57, 86], "market": [54, 77], "matric": [89, 91], "meet": 76, "method": [76, 141], "metric": [75, 76], "minimum": 94, "miss": [57, 86], "missing": [95, 109], "mixin": 49, "ml": [52, 53, 66, 85, 88, 141], "mlr3": 56, "mlr3extralearn": 56, "mlr3learner": 56, "mlr3pipelin": 56, "model": [3, 49, 55, 57, 62, 64, 67, 68, 71, 72, 74, 76, 78, 80, 81, 85, 86, 93, 95, 108, 109, 124, 125, 138, 141], "modul": 62, "more": 56, "motiv": [54, 77], "multipl": [60, 64, 80, 95], "multipli": 124, "naiv": [51, 65], "net": [55, 78], "neyman": [109, 138], "nonignor": [57, 86, 95, 109], "nonlinearscoremixin": 28, "nonrespons": [57, 86, 95, 109], "note": 140, "nuisanc": [76, 138], "object": [54, 77, 84], "option": 137, "orthogon": [52, 66, 88, 109, 138], "out": [52, 66, 88], "outcom": [57, 58, 64, 69, 86, 93, 95, 109, 125], "over": 124, "overcom": [52, 66, 88], "overfit": [52, 66, 88], "overlap": 80, "packag": [53, 55, 78, 137], "panel": [58, 60, 61, 95, 97, 109, 125], "paramet": [56, 62, 76, 109], "partial": [52, 55, 66, 72, 78, 80, 88, 95, 109, 125], "particip": [55, 78], "partit": 108, "penalti": 70, "perform": [53, 80], "period": [60, 95, 109, 125], "pip": 137, "pipelin": 94, "pliv": [95, 109], "plm": [3, 38, 39, 80, 95, 109, 125], "plot": [54, 76, 77], "plr": [55, 62, 68, 72, 78, 93, 95, 109, 125], "polici": [81, 93], "potenti": [64, 69, 79, 82, 93, 95, 109, 125], "pq": [82, 93, 109], "pre": 59, "predict": [53, 94], "preprocess": 56, "problem": 141, "process": [52, 54, 64, 66, 77, 88], "product": [54, 77], "propens": 80, "provid": 108, "python": [58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 84, 86, 94, 137], "qte": [82, 93], "qualiti": 70, "quantil": [79, 82, 93, 109], "r": [51, 52, 53, 54, 55, 56, 57, 63, 94, 137], "random": [57, 62, 86, 95, 109], "rank": 80, "rdd": [3, 40, 41, 83, 95], "rdflex": 40, "real": [54, 77], "refer": [0, 51, 53, 54, 56, 65, 70, 75, 76, 77, 80, 85, 88, 94, 108, 124, 136, 138], "regress": [55, 71, 72, 78, 81, 83, 95, 109, 125], "regular": [52, 66, 88], "releas": [137, 140], "remark": 53, "remov": [52, 66, 88], "repeat": [58, 95, 97, 108, 109, 125], "repetit": 108, "requir": 94, "respect": [54, 77], "result": [54, 55, 77, 78, 80], "risk": [69, 79, 93, 109], "robust": [54, 77], "sampl": [52, 57, 66, 76, 86, 88, 95, 108, 109], "sandbox": 63, "score": [49, 52, 66, 80, 88, 109, 138], "section": [58, 95, 97, 109, 125], "select": [57, 60, 86, 95, 109], "sensit": [60, 64, 73, 74, 84, 85, 125, 141], "set": [56, 94], "sharp": [83, 95], "simpl": [52, 66, 88], "simul": [51, 54, 58, 65, 77, 84], "simultan": 124, "singl": 64, "sourc": [136, 137], "special": 91, "specif": [125, 141], "specifi": [62, 94, 109], "split": [52, 66, 88, 108], "ssm": 95, "stack": 83, "stage": 70, "standard": [54, 75, 77], "start": 138, "step": 76, "studi": [60, 61, 63], "summari": [55, 76, 78, 80], "test": 59, "theori": 125, "time": [60, 61, 75, 76], "train": 76, "treat": 74, "treatment": [55, 67, 68, 69, 71, 72, 74, 78, 79, 80, 82, 93, 95, 109, 125], "tree": [81, 93], "tune": [56, 76, 94], "two": [54, 67, 68, 77, 95, 109, 125], "type": 91, "under": [57, 80, 86], "untun": 76, "up": 56, "us": [51, 53, 56, 62, 65, 76, 94], "user": 92, "util": [42, 43, 44, 45, 46, 47, 48, 50], "v": 70, "valid": 124, "valu": [69, 79, 93, 109], "vanderweel": 85, "variabl": [51, 65], "varianc": 124, "version": 137, "via": 109, "wai": [54, 77], "wealth": [55, 78, 79], "weight": [80, 93], "when": 76, "whl": 137, "within": 76, "without": [83, 108], "workflow": 141, "xgboost": 76, "zero": [54, 77]}})