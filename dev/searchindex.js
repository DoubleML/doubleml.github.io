Search.setIndex({"alltitles": {"(One-Way) Cluster Robust Double Machine Learing": [[63, "(One-Way)-Cluster-Robust-Double-Machine-Learing"], [91, "(One-Way)-Cluster-Robust-Double-Machine-Learing"]], "0. Problem Formulation": [[173, "problem-formulation"]], "1. Data Backend": [[173, "data-backend"]], "1. Formulation of Causal Model & Identification Assumptions": [[100, "1.-Formulation-of-Causal-Model-&-Identification-Assumptions"]], "2. Causal Model": [[173, "causal-model"]], "2. Estimation of Causal Effect": [[100, "2.-Estimation-of-Causal-Effect"]], "3. ML Methods": [[173, "ml-methods"]], "3. Sensitivity Analysis": [[100, "3.-Sensitivity-Analysis"]], "4. Benchmarking Analysis": [[100, "4.-Benchmarking-Analysis"]], "4. DML Specifications": [[173, "dml-specifications"]], "5. Conclusion": [[100, "5.-Conclusion"]], "5. Hyperparameter Tuning": [[173, "hyperparameter-tuning"]], "6. Estimation": [[173, "estimation"]], "7. Inference": [[173, "inference"]], "8. Sensitivity Analysis": [[173, "sensitivity-analysis"]], "A Motivating Example: Two-Way Cluster Robust DML": [[63, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"], [91, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"]], "API Reference": [[0, null]], "ATE Estimation and Sensitivity": [[87, "ATE-Estimation-and-Sensitivity"]], "ATE estimates distribution": [[66, "ATE-estimates-distribution"], [66, "id3"], [101, "ATE-estimates-distribution"], [101, "id3"]], "ATT Estimation": [[69, "ATT-Estimation"], [69, "id1"], [71, "ATT-Estimation"], [72, "ATT-Estimation"], [72, "id1"]], "ATT Estimation: Conditional Parallel Trends": [[70, "ATT-Estimation:-Conditional-Parallel-Trends"]], "ATT Estimation: Unconditional Parallel Trends": [[70, "ATT-Estimation:-Unconditional-Parallel-Trends"]], "ATTE Estimation": [[67, "ATTE-Estimation"], [67, "id2"]], "Acknowledgements": [[168, "acknowledgements"]], "Acknowledgements and Final Remarks": [[62, "Acknowledgements-and-Final-Remarks"]], "Additional Results: CATE estimates": [[94, "Additional-Results:-CATE-estimates"]], "Advanced: External Predictions": [[112, "advanced-external-predictions"]], "Advanced: Global and Local Learners, Stacked Ensembles": [[97, "Advanced:-Global-and-Local-Learners,-Stacked-Ensembles"]], "Aggregated Effects": [[69, "Aggregated-Effects"], [72, "Aggregated-Effects"]], "Aggregation Details": [[69, "Aggregation-Details"], [70, "Aggregation-Details"], [71, "Aggregation-Details"], [72, "Aggregation-Details"]], "Algorithm DML1": [[102, "algorithm-dml1"]], "Algorithm DML2": [[102, "algorithm-dml2"]], "All Combinations": [[69, "All-Combinations"]], "All combinations": [[72, "All-combinations"]], "Anticipation": [[69, "Anticipation"], [72, "Anticipation"]], "Application Results": [[63, "Application-Results"], [91, "Application-Results"]], "Application: 401(k)": [[99, "Application:-401(k)"]], "AutoML with less Computation time": [[90, "AutoML-with-less-Computation-time"]], "Average Potential Outcome (APOs)": [[78, "Average-Potential-Outcome-(APOs)"]], "Average Potential Outcomes (APOs)": [[124, "average-potential-outcomes-apos"], [140, "average-potential-outcomes-apos"], [157, "average-potential-outcomes-apos"]], "Average Potential Outcomes (APOs) for Multiple Treatment Levels": [[124, "average-potential-outcomes-apos-for-multiple-treatment-levels"]], "Average Treatment Effect": [[88, "Average-Treatment-Effect"]], "Average Treatment Effect on the Treated": [[88, "Average-Treatment-Effect-on-the-Treated"]], "Basic Tuning Example": [[76, "Basic-Tuning-Example"]], "Basics": [[69, "Basics"], [72, "Basics"]], "Benchmarking": [[157, "benchmarking"]], "Benchmarking Analysis": [[99, "Benchmarking-Analysis"]], "Binary Interactive Regression Model (IRM)": [[124, "binary-interactive-regression-model-irm"], [140, "binary-interactive-regression-model-irm"]], "CATEs for IRM models": [[111, "cates-for-irm-models"]], "CATEs for PLR models": [[111, "cates-for-plr-models"]], "CVaR Treatment Effects": [[83, "CVaR-Treatment-Effects"]], "CVaR of potential outcomes": [[111, "cvar-of-potential-outcomes"]], "CVaR treatment effects": [[111, "cvar-treatment-effects"]], "Causal Analysis with DoubleML": [[100, "Causal-Analysis-with-DoubleML"]], "Causal Contrasts": [[78, "Causal-Contrasts"]], "Causal Research Question": [[70, "Causal-Research-Question"]], "Causal estimation vs. lasso penalty \\lambda": [[84, "Causal-estimation-vs.-lasso-penalty-\\lambda"]], "Chernozhukov et al. (2023): Benchmarking": [[100, "Chernozhukov-et-al.-(2023):-Benchmarking"]], "Citation": [[168, "citation"]], "Cluster Robust Cross Fitting": [[63, "Cluster-Robust-Cross-Fitting"], [91, "Cluster-Robust-Cross-Fitting"]], "Cluster Robust Standard Errors": [[63, "Cluster-Robust-Standard-Errors"], [91, "Cluster-Robust-Standard-Errors"]], "Clustering and double machine learning": [[63, "Clustering-and-double-machine-learning"], [91, "Clustering-and-double-machine-learning"]], "Combined loss vs. lasso penalty \\lambda": [[84, "Combined-loss-vs.-lasso-penalty-\\lambda"]], "Compare Metrics for Nuisance Estimation": [[90, "Compare-Metrics-for-Nuisance-Estimation"]], "Comparing different learners": [[75, "Comparing-different-learners"]], "Comparison": [[76, "Comparison"], [76, "id2"]], "Comparison and summary": [[90, "Comparison-and-summary"]], "Comparison to AutoML with less Computation time and Untuned XGBoost Learners": [[90, "Comparison-to-AutoML-with-less-Computation-time-and-Untuned-XGBoost-Learners"]], "Comparison to did package": [[62, "Comparison-to-did-package"]], "Computation time": [[75, "Computation-time"]], "Conclusion": [[90, "Conclusion"]], "Conditional Value at Risk (CVaR)": [[83, "Conditional-Value-at-Risk-(CVaR)"]], "Conditional average treatment effects (CATEs)": [[111, "conditional-average-treatment-effects-cates"]], "Conditional value at risk (CVaR)": [[111, "conditional-value-at-risk-cvar"], [140, "conditional-value-at-risk-cvar"]], "Confidence bands and multiplier bootstrap for valid simultaneous inference": [[156, "confidence-bands-and-multiplier-bootstrap-for-valid-simultaneous-inference"]], "Control Groups": [[69, "Control-Groups"], [72, "Control-Groups"]], "Coverage Simulation": [[67, "Coverage-Simulation"], [67, "id3"]], "Creating the DoubleMLData Object": [[77, "Creating-the-DoubleMLData-Object"]], "Cross-fitting with K folds": [[139, "cross-fitting-with-k-folds"]], "Cross-fitting, DML algorithms and Neyman-orthogonal score functions": [[170, "cross-fitting-dml-algorithms-and-neyman-orthogonal-score-functions"]], "Custom evaluation metrics": [[75, "Custom-evaluation-metrics"]], "DML: Bonus Data": [[73, null]], "Data": [[64, "Data"], [66, "Data"], [66, "id1"], [67, "Data"], [67, "id1"], [69, "Data"], [70, "Data"], [71, "Data"], [72, "Data"], [81, "Data"], [82, "Data"], [83, "Data"], [85, "Data"], [86, "Data"], [87, "Data"], [88, "Data"], [89, "Data"], [92, "Data"], [93, "Data"], [95, "Data"], [96, "Data"], [96, "id1"], [99, "Data"], [101, "Data"], [101, "id1"], [170, "data"]], "Data Backend": [[109, null]], "Data Description": [[69, "Data-Description"], [72, "Data-Description"]], "Data Details": [[69, "Data-Details"], [72, "Data-Details"]], "Data Generating Process (DGP)": [[61, "Data-Generating-Process-(DGP)"], [76, "Data-Generating-Process-(DGP)"], [77, "Data-Generating-Process-(DGP)"], [78, "Data-Generating-Process-(DGP)"], [80, "Data-Generating-Process-(DGP)"]], "Data Generation": [[90, "Data-Generation"]], "Data Simulation": [[60, "Data-Simulation"], [79, "Data-Simulation"]], "Data and Effect Estimation": [[99, "Data-and-Effect-Estimation"]], "Data generating process": [[103, "data-generating-process"]], "Data preprocessing": [[65, "Data-preprocessing"]], "Data with Anticipation": [[69, "Data-with-Anticipation"], [72, "Data-with-Anticipation"]], "Data-Backend for Cluster Data": [[63, "Data-Backend-for-Cluster-Data"], [91, "Data-Backend-for-Cluster-Data"]], "Dataset Generators": [[2, "dataset-generators"]], "Dataset Loaders": [[2, "dataset-loaders"]], "Datasets": [[2, null]], "Define Helper Functions for Plotting": [[63, "Define-Helper-Functions-for-Plotting"], [91, "Define-Helper-Functions-for-Plotting"]], "Define Nuisance Learners": [[76, "Define-Nuisance-Learners"], [76, "id1"]], "Demo Example from did": [[62, "Demo-Example-from-did"]], "Detailed Hyperparameter Tuning Guide": [[76, "Detailed-Hyperparameter-Tuning-Guide"]], "Detailed Tuning Result Analysis": [[76, "Detailed-Tuning-Result-Analysis"]], "Details on Predictive Performance": [[62, "Details-on-Predictive-Performance"]], "Difference-in-Differences": [[74, "difference-in-differences"]], "Difference-in-Differences Models": [[140, "difference-in-differences-models"], [157, "difference-in-differences-models"]], "Difference-in-Differences Models (DID)": [[124, "difference-in-differences-models-did"]], "Difference-in-Differences for Panel Data": [[157, "difference-in-differences-for-panel-data"]], "Difference-in-Differences for repeated cross-sections": [[157, "difference-in-differences-for-repeated-cross-sections"]], "Disclaimer": [[100, "Disclaimer"]], "Double Machine Learning Algorithm": [[168, "double-machine-learning-algorithm"]], "Double Machine Learning Literature": [[171, null]], "Double machine learning algorithms": [[102, null]], "Double/debiased machine learning": [[61, "Double/debiased-machine-learning"], [80, "Double/debiased-machine-learning"], [103, "double-debiased-machine-learning"]], "DoubleML": [[168, null]], "DoubleML Data Class": [[1, null]], "DoubleML Models": [[3, null]], "DoubleML Object": [[99, "DoubleML-Object"]], "DoubleML Workflow": [[173, null]], "DoubleML meets FLAML - How to tune learners automatically within DoubleML": [[90, null]], "DoubleML with TabPFN": [[77, "DoubleML-with-TabPFN"]], "DoubleMLAPOS Tuning Example": [[76, "DoubleMLAPOS-Tuning-Example"]], "DoubleMLDIDData": [[109, "doublemldiddata"]], "DoubleMLData": [[109, "doublemldata"]], "DoubleMLData from arrays and matrices": [[104, "doublemldata-from-arrays-and-matrices"], [109, "doublemldata-from-arrays-and-matrices"]], "DoubleMLData from dataframes": [[104, null], [109, "doublemldata-from-dataframes"]], "DoubleMLPanelData": [[69, "DoubleMLPanelData"], [72, "DoubleMLPanelData"], [109, "doublemlpaneldata"]], "DoubleMLRDDData": [[109, "doublemlrdddata"]], "DoubleMLSSMData": [[109, "doublemlssmdata"]], "Effect Aggregation": [[70, "Effect-Aggregation"], [71, "Effect-Aggregation"], [124, "effect-aggregation"]], "Effect Heterogeneity": [[74, "effect-heterogeneity"], [88, "Effect-Heterogeneity"]], "Empirical coverage vs. lasso penalty \\lambda": [[84, "Empirical-coverage-vs.-lasso-penalty-\\lambda"]], "Estimate DML models without sample-splitting": [[139, "estimate-dml-models-without-sample-splitting"]], "Estimate double/debiased machine learning models": [[170, "estimate-double-debiased-machine-learning-models"]], "Estimating Potential Quantiles and Quantile Treatment Effects": [[93, "Estimating-Potential-Quantiles-and-Quantile-Treatment-Effects"]], "Estimating local quantile treatment effects (LQTEs)": [[93, "Estimating-local-quantile-treatment-effects-(LQTEs)"]], "Estimating the Average Treatment Effect of 401(k) Eligibility on Net Financial Assets": [[64, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"], [92, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"]], "Estimating the treatment effect on the Conditional Value a Risk (CVaR)": [[93, "Estimating-the-treatment-effect-on-the-Conditional-Value-a-Risk-(CVaR)"]], "Estimation": [[66, "Estimation"], [66, "id2"], [101, "Estimation"], [101, "id2"]], "Estimation of Average Potential Outcomes": [[77, "Estimation-of-Average-Potential-Outcomes"]], "Estimation quality vs. \\lambda": [[84, "Estimation-quality-vs.-\\lambda"]], "Evaluate learners": [[112, "evaluate-learners"]], "Event Study Aggregation": [[69, "Event-Study-Aggregation"], [70, "Event-Study-Aggregation"], [71, "Event-Study-Aggregation"], [72, "Event-Study-Aggregation"]], "Example usage": [[105, "example-usage"], [106, "example-usage"], [107, "example-usage"], [108, "example-usage"], [109, "example-usage"], [109, "id6"], [109, "id8"], [109, "id10"]], "Examples": [[74, null]], "Exploiting the Functionalities of did": [[62, "Exploiting-the-Functionalities-of-did"]], "Externally provide a sample splitting / partition": [[139, "externally-provide-a-sample-splitting-partition"]], "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)": [[97, null]], "Fuzzy RDD": [[97, "Fuzzy-RDD"]], "Fuzzy RDD Without Adjustment": [[97, "Fuzzy-RDD-Without-Adjustment"]], "Fuzzy RDD with Flexible Adjustment": [[97, "Fuzzy-RDD-with-Flexible-Adjustment"]], "Fuzzy RDD with Linear Adjustment": [[97, "Fuzzy-RDD-with-Linear-Adjustment"]], "Fuzzy Regression Discontinuity Design": [[124, "fuzzy-regression-discontinuity-design"]], "GATE Estimation and Sensitivity": [[87, "GATE-Estimation-and-Sensitivity"]], "GATET Estimation and Sensitivity": [[87, "GATET-Estimation-and-Sensitivity"]], "GATEs for IRM models": [[111, "gates-for-irm-models"]], "GATEs for PLR models": [[111, "gates-for-plr-models"]], "General Examples": [[74, "general-examples"]], "General algorithm": [[157, "general-algorithm"]], "Generate Fuzzy Data": [[97, "Generate-Fuzzy-Data"]], "Generate Sharp Data": [[97, "Generate-Sharp-Data"]], "Getting Started": [[170, null]], "Group Aggregation": [[69, "Group-Aggregation"], [71, "Group-Aggregation"], [72, "Group-Aggregation"]], "Group Average Treatment Effects (GATEs)": [[85, "Group-Average-Treatment-Effects-(GATEs)"], [86, "Group-Average-Treatment-Effects-(GATEs)"]], "Group average treatment effects (GATEs)": [[111, "group-average-treatment-effects-gates"]], "Group-Time Combinations": [[69, "Group-Time-Combinations"], [72, "Group-Time-Combinations"]], "Heterogeneous treatment effects": [[111, null]], "How to exploit more features of mlr3pipelines in DoubleML": [[65, "How-to-exploit-more-features-of-mlr3pipelines-in-DoubleML"]], "Hyperparameter Tuning": [[76, "Hyperparameter-Tuning"], [76, "id4"]], "Hyperparameter Tuning with Pipelines": [[76, "Hyperparameter-Tuning-with-Pipelines"]], "Hyperparameter tuning": [[112, "hyperparameter-tuning"], [112, "r-tune-params"]], "Hyperparameter tuning (Grid Search)": [[112, "hyperparameter-tuning-grid-search"]], "Hyperparameter tuning with pipelines": [[112, "hyperparameter-tuning-with-pipelines"]], "Implementation": [[157, "implementation"]], "Implementation Details": [[124, "implementation-details"]], "Implementation of the double machine learning algorithms": [[102, "implementation-of-the-double-machine-learning-algorithms"]], "Implementation of the score function and the estimate of the causal parameter": [[140, "implementation-of-the-score-function-and-the-estimate-of-the-causal-parameter"]], "Implemented Neyman orthogonal score functions": [[140, "implemented-neyman-orthogonal-score-functions"]], "Imports and Setup": [[77, "Imports-and-Setup"]], "Initialize DoubleMLClusterData object": [[63, "Initialize-DoubleMLClusterData-object"]], "Initialize DoubleMLData object with clusters": [[91, "Initialize-DoubleMLData-object-with-clusters"]], "Initialize the objects of class DoubleMLPLIV": [[63, "Initialize-the-objects-of-class-DoubleMLPLIV"], [91, "Initialize-the-objects-of-class-DoubleMLPLIV"]], "Installing DoubleML": [[169, null]], "Instrumental Variables Directed Acyclic Graph (IV - DAG)": [[60, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"], [79, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"]], "Interactive IV Model (IIVM)": [[64, "Interactive-IV-Model-(IIVM)"], [92, "Interactive-IV-Model-(IIVM)"]], "Interactive IV model (IIVM)": [[124, "interactive-iv-model-iivm"], [140, "interactive-iv-model-iivm"]], "Interactive Regression Model (IRM)": [[64, "Interactive-Regression-Model-(IRM)"], [85, "Interactive-Regression-Model-(IRM)"], [92, "Interactive-Regression-Model-(IRM)"], [95, "Interactive-Regression-Model-(IRM)"]], "Interactive regression model (IRM)": [[157, "interactive-regression-model-irm"]], "Interactive regression models (IRM)": [[124, "interactive-regression-models-irm"], [140, "interactive-regression-models-irm"], [157, "interactive-regression-models-irm"]], "Key Takeaways": [[77, "Key-Takeaways"]], "Key arguments": [[105, null], [106, null], [107, null], [108, null], [109, "key-arguments"], [109, "id5"], [109, "id7"], [109, "id9"]], "Learners and Hyperparameters": [[88, "Learners-and-Hyperparameters"]], "Learners to estimate the nuisance models": [[170, "learners-to-estimate-the-nuisance-models"]], "Learners, hyperparameters and hyperparameter tuning": [[112, null]], "Linear Covariate Adjustment": [[69, "Linear-Covariate-Adjustment"], [72, "Linear-Covariate-Adjustment"]], "Load Data": [[100, "Load-Data"]], "Load and Process Data": [[63, "Load-and-Process-Data"], [91, "Load-and-Process-Data"]], "Load bonus data using the dml datasets module": [[73, "Load-bonus-data-using-the-dml-datasets-module"]], "Local Average Treatment Effects of 401(k) Participation on Net Financial Assets": [[64, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"], [92, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"]], "Local Potential Quantile Estimation": [[96, "Local-Potential-Quantile-Estimation"]], "Local Potential Quantiles (LPQs)": [[96, "Local-Potential-Quantiles-(LPQs)"]], "Local Quantile Treatment Effects (LQTEs)": [[96, "Local-Quantile-Treatment-Effects-(LQTEs)"]], "Local potential quantiles (LPQs)": [[140, "local-potential-quantiles-lpqs"]], "Logistic partial linear regression (LPLR)": [[140, "logistic-partial-linear-regression-lplr"]], "Logistic partially linear regression model (LPLR)": [[124, "logistic-partially-linear-regression-model-lplr"]], "Machine Learning Methods Comparison": [[77, "Machine-Learning-Methods-Comparison"]], "Main Features": [[168, "main-features"]], "Minimum requirements for learners": [[112, "minimum-requirements-for-learners"], [112, "r-learner-req"]], "Missingness at Random": [[124, "missingness-at-random"], [140, "missingness-at-random"]], "Model": [[89, "Model"]], "Model Performance Evaluation": [[77, "Model-Performance-Evaluation"]], "Model-specific implementations": [[157, "model-specific-implementations"]], "Models": [[124, null]], "Motivation": [[63, "Motivation"], [91, "Motivation"]], "Multiple Average Potential Outcome Models (APOS)": [[78, "Multiple-Average-Potential-Outcome-Models-(APOS)"]], "Naive estimation": [[60, "Naive-estimation"], [79, "Naive-estimation"]], "No Clustering / Zero-Way Clustering": [[63, "No-Clustering-/-Zero-Way-Clustering"], [91, "No-Clustering-/-Zero-Way-Clustering"]], "Nonignorable Nonresponse": [[124, "nonignorable-nonresponse"], [140, "nonignorable-nonresponse"]], "One-Way Clustering with Respect to the Market": [[63, "One-Way-Clustering-with-Respect-to-the-Market"], [91, "One-Way-Clustering-with-Respect-to-the-Market"]], "One-Way Clustering with Respect to the Product": [[63, "One-Way-Clustering-with-Respect-to-the-Product"], [91, "One-Way-Clustering-with-Respect-to-the-Product"]], "One-dimensional Example": [[81, "One-dimensional-Example"], [82, "One-dimensional-Example"]], "Outcome missing at random (MAR)": [[66, "Outcome-missing-at-random-(MAR)"], [101, "Outcome-missing-at-random-(MAR)"]], "Outcome missing under nonignorable nonresponse": [[66, "Outcome-missing-under-nonignorable-nonresponse"], [101, "Outcome-missing-under-nonignorable-nonresponse"]], "Overcoming regularization bias by orthogonalization": [[61, "Overcoming-regularization-bias-by-orthogonalization"], [80, "Overcoming-regularization-bias-by-orthogonalization"], [103, "overcoming-regularization-bias-by-orthogonalization"]], "Panel Data": [[124, "id9"], [126, null], [140, "panel-data"], [140, "id3"], [157, "panel-data"]], "Panel Data (Repeated Outcomes)": [[67, "Panel-Data-(Repeated-Outcomes)"]], "Panel data": [[124, "panel-data"]], "Parameter tuning": [[65, "Parameter-tuning"]], "Parameters & Implementation": [[124, "parameters-implementation"]], "Partialling out score": [[61, "Partialling-out-score"], [80, "Partialling-out-score"], [103, "partialling-out-score"]], "Partially Linear Regression Model (PLR)": [[64, "Partially-Linear-Regression-Model-(PLR)"], [86, "Partially-Linear-Regression-Model-(PLR)"], [92, "Partially-Linear-Regression-Model-(PLR)"]], "Partially linear IV regression model (PLIV)": [[124, "partially-linear-iv-regression-model-pliv"], [140, "partially-linear-iv-regression-model-pliv"]], "Partially linear models (PLM)": [[124, "partially-linear-models-plm"], [140, "partially-linear-models-plm"], [157, "partially-linear-models-plm"]], "Partially linear regression model (PLR)": [[124, "partially-linear-regression-model-plr"], [140, "partially-linear-regression-model-plr"], [157, "partially-linear-regression-model-plr"]], "Performance Summary and Insights": [[77, "Performance-Summary-and-Insights"]], "Plot Coefficients and 95% Confidence Intervals": [[90, "Plot-Coefficients-and-95%-Confidence-Intervals"]], "Policy Learning with Trees": [[95, "Policy-Learning-with-Trees"], [111, "policy-learning-with-trees"]], "Potential Quantile Estimation": [[96, "Potential-Quantile-Estimation"]], "Potential Quantiles (PQs)": [[96, "Potential-Quantiles-(PQs)"]], "Potential quantiles (PQs)": [[111, "potential-quantiles-pqs"], [140, "potential-quantiles-pqs"]], "Python: Average Potential Outcome (APO) Models": [[78, null]], "Python: Basic Instrumental Variables calculation": [[79, null]], "Python: Basics of Double Machine Learning": [[80, null]], "Python: Building the package from source": [[169, "python-building-the-package-from-source"]], "Python: Case studies": [[74, "python-case-studies"]], "Python: Causal Machine Learning with TabPFN": [[77, null]], "Python: Choice of learners": [[75, null]], "Python: Cluster Robust Double Machine Learning": [[91, null]], "Python: Conditional Average Treatment Effects (CATEs) for IRM models": [[81, null]], "Python: Conditional Average Treatment Effects (CATEs) for PLR models": [[82, null]], "Python: Conditional Value at Risk of potential outcomes": [[83, null]], "Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments": [[98, null]], "Python: Difference-in-Differences": [[67, null]], "Python: Difference-in-Differences Pre-Testing": [[68, null]], "Python: First Stage and Causal Estimation": [[84, null]], "Python: GATE Sensitivity Analysis": [[87, null]], "Python: Group Average Treatment Effects (GATEs) for IRM models": [[85, null]], "Python: Group Average Treatment Effects (GATEs) for PLR models": [[86, null]], "Python: Hyperparametertuning with Optuna": [[76, null]], "Python: IRM and APO Model Comparison": [[88, null]], "Python: Impact of 401(k) on Financial Wealth": [[92, null]], "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)": [[93, null]], "Python: Installing DoubleML": [[169, "python-installing-doubleml"]], "Python: Installing a released version from a .whl file": [[169, "python-installing-a-released-version-from-a-whl-file"]], "Python: Installing the latest release from pip or conda": [[169, "python-installing-the-latest-release-from-pip-or-conda"]], "Python: Learners and hyperparameters": [[112, "python-learners-and-hyperparameters"]], "Python: Log-Odds Effects for Logistic PLR models": [[89, null]], "Python: Optional Dependencies": [[169, "python-optional-dependencies"]], "Python: PLM and IRM for Multiple Treatments": [[94, null]], "Python: Panel Data Introduction": [[71, null]], "Python: Panel Data with Multiple Time Periods": [[69, null]], "Python: Policy Learning with Trees": [[95, null]], "Python: Potential Quantiles and Quantile Treatment Effects": [[96, null]], "Python: Real-Data Example for Multi-Period Difference-in-Differences": [[70, null]], "Python: Repeated Cross-Sectional Data with Multiple Time Periods": [[72, null]], "Python: Sample Selection Models": [[101, null]], "Python: Sensitivity Analysis": [[99, null]], "Python: Sensitivity Analysis for Causal ML": [[100, null]], "Quantile Treatment Effects (QTEs)": [[96, "Quantile-Treatment-Effects-(QTEs)"]], "Quantile treatment effects (QTEs)": [[111, "quantile-treatment-effects-qtes"]], "Quantiles": [[111, "quantiles"]], "R: Basic Instrumental Variables Calculation": [[60, null]], "R: Basics of Double Machine Learning": [[61, null]], "R: Case studies": [[74, "r-case-studies"]], "R: Cluster Robust Double Machine Learning": [[63, null]], "R: DoubleML for Difference-in-Differences": [[62, null]], "R: Ensemble Learners and More with mlr3pipelines": [[65, null]], "R: Impact of 401(k) on Financial Wealth": [[64, null]], "R: Installing DoubleML": [[169, "r-installing-doubleml"]], "R: Installing the development version from GitHub": [[169, "r-installing-the-development-version-from-github"]], "R: Installing the latest release from CRAN": [[169, "r-installing-the-latest-release-from-cran"]], "R: Learners and hyperparameters": [[112, "r-learners-and-hyperparameters"]], "R: Sample Selection Models": [[66, null]], "Ranking Treatment Effects under Treatment Propensity and Treatment Effect Heterogeneity": [[94, "Ranking-Treatment-Effects-under-Treatment-Propensity-and-Treatment-Effect-Heterogeneity"]], "Real-Data Application": [[63, "Real-Data-Application"], [91, "Real-Data-Application"]], "References": [[60, "References"], [62, "References"], [63, "References"], [65, "References"], [75, "References"], [79, "References"], [84, "References"], [90, "References"], [91, "References"], [94, "References"], [98, "References"], [100, "References"], [103, "references"], [112, "references"], [122, null], [139, "references"], [156, "references"], [168, "references"], [170, "references"]], "Regression Discontinuity Designs (RDD)": [[124, "regression-discontinuity-designs-rdd"]], "Regularization Bias in Simple ML-Approaches": [[61, "Regularization-Bias-in-Simple-ML-Approaches"], [80, "Regularization-Bias-in-Simple-ML-Approaches"]], "Regularization bias in simple ML-approaches": [[103, "regularization-bias-in-simple-ml-approaches"]], "Release Notes": [[172, null]], "Repeated Cross-Sectional Data": [[67, "Repeated-Cross-Sectional-Data"], [140, "repeated-cross-sectional-data"], [140, "id4"], [157, "repeated-cross-sectional-data"]], "Repeated cross-fitting with K folds and M repetitions": [[139, "repeated-cross-fitting-with-k-folds-and-m-repetitions"]], "Repeated cross-sections": [[124, "repeated-cross-sections"], [124, "id10"], [126, "repeated-cross-sections"]], "Running a small simulation": [[98, "Running-a-small-simulation"]], "Sample Selection Models": [[140, "sample-selection-models"]], "Sample Selection Models (SSM)": [[124, "sample-selection-models-ssm"]], "Sample splitting to remove bias induced by overfitting": [[61, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [80, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [103, "sample-splitting-to-remove-bias-induced-by-overfitting"]], "Sample-splitting without cross-fitting": [[139, "sample-splitting-without-cross-fitting"]], "Sample-splitting, cross-fitting and repeated cross-fitting": [[139, null]], "Sandbox/Archive": [[74, "sandbox-archive"]], "Score Mixin Classes for DoubleML Models": [[58, null]], "Score functions": [[140, null]], "Selected Combinations": [[69, "Selected-Combinations"], [72, "Selected-Combinations"]], "Sensitivity Analysis": [[69, "Sensitivity-Analysis"], [72, "Sensitivity-Analysis"], [78, "Sensitivity-Analysis"], [88, "Sensitivity-Analysis"], [99, "Sensitivity-Analysis"], [99, "id1"]], "Sensitivity Analysis with IRM": [[99, "Sensitivity-Analysis-with-IRM"]], "Sensitivity analysis": [[157, null]], "Set up learners based on mlr3pipelines": [[65, "Set-up-learners-based-on-mlr3pipelines"]], "Sharp RDD": [[97, "Sharp-RDD"]], "Sharp RDD Without Adjustment": [[97, "Sharp-RDD-Without-Adjustment"]], "Sharp RDD with Flexible Adjustment": [[97, "Sharp-RDD-with-Flexible-Adjustment"]], "Sharp RDD with Linear Adjustment": [[97, "Sharp-RDD-with-Linear-Adjustment"]], "Sharp Regression Discontinuity Design": [[124, "sharp-regression-discontinuity-design"]], "Simulate two-way cluster data": [[63, "Simulate-two-way-cluster-data"], [91, "Simulate-two-way-cluster-data"]], "Simulation Example": [[99, "Simulation-Example"]], "Simultaneous inference over different DoubleML models (advanced)": [[156, "simultaneous-inference-over-different-doubleml-models-advanced"]], "Single Average Potential Outcome Models (APO)": [[78, "Single-Average-Potential-Outcome-Models-(APO)"]], "Source code and maintenance": [[168, "source-code-and-maintenance"]], "Special Data Types": [[109, "special-data-types"]], "Specify learner and estimate causal parameter: IRM model with Lasso as learner": [[73, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: IRM model with random forest as learner": [[73, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-random-forest-as-learner"]], "Specify learner and estimate causal parameter: PLR model with Lasso as learner": [[73, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: PLR model with random forest as learner": [[73, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-random-forest-as-learner"]], "Specifying alternative score functions via callables": [[140, "specifying-alternative-score-functions-via-callables"]], "Specifying learners and set hyperparameters": [[112, "specifying-learners-and-set-hyperparameters"], [112, "r-set-params"]], "Standard approach": [[75, "Standard-approach"]], "Step 1: Custom API for FLAML Models within DoubleML": [[90, "Step-1:-Custom-API-for-FLAML-Models-within-DoubleML"]], "Step 1: Initialize and Train the AutoML Models:": [[90, "Step-1:-Initialize-and-Train-the-AutoML-Models:"]], "Step 2: Evaluate the Tuned Models": [[90, "Step-2:-Evaluate-the-Tuned-Models"]], "Step 2: Using the API when calling DoubleML\u2019s .fit() Method": [[90, "Step-2:-Using-the-API-when-calling-DoubleML's-.fit()-Method"]], "Step 3: Create and Fit DoubleML Model": [[90, "Step-3:-Create-and-Fit-DoubleML-Model"]], "Summary Figure": [[94, "Summary-Figure"]], "Summary of Results": [[64, "Summary-of-Results"], [92, "Summary-of-Results"]], "The Augmented Inverse Propensity Weighting Model estimates the ATE under arbitrary effect and propensity score heterogeneity": [[94, "The-Augmented-Inverse-Propensity-Weighting-Model-estimates-the-ATE-under-arbitrary-effect-and-propensity-score-heterogeneity"]], "The Data Backend: DoubleMLData": [[64, "The-Data-Backend:-DoubleMLData"], [92, "The-Data-Backend:-DoubleMLData"]], "The DoubleML package": [[64, "The-DoubleML-package"], [92, "The-DoubleML-package"]], "The Partially Linear Model performs overlap weighting": [[94, "The-Partially-Linear-Model-performs-overlap-weighting"]], "The basics of double/debiased machine learning": [[103, null]], "The causal model": [[170, "the-causal-model"]], "The data-backend DoubleMLData": [[170, "the-data-backend-doublemldata"]], "Theory": [[157, "theory"]], "Time Aggregation": [[69, "Time-Aggregation"], [71, "Time-Aggregation"], [72, "Time-Aggregation"]], "Tuning on the Folds": [[90, "Tuning-on-the-Folds"]], "Tuning on the full Sample": [[90, "Tuning-on-the-full-Sample"]], "Two treatment periods": [[124, "two-treatment-periods"], [140, "two-treatment-periods"], [157, "two-treatment-periods"]], "Two-Dimensional Example": [[81, "Two-Dimensional-Example"], [82, "Two-Dimensional-Example"]], "Two-Way Clustering with Respect to Product and Market": [[63, "Two-Way-Clustering-with-Respect-to-Product-and-Market"], [91, "Two-Way-Clustering-with-Respect-to-Product-and-Market"]], "Universal Base Period": [[69, "Universal-Base-Period"], [72, "Universal-Base-Period"]], "Untuned (default parameter) XGBoost": [[90, "Untuned-(default-parameter)-XGBoost"]], "Untuned Model": [[76, "Untuned-Model"], [76, "id3"]], "Untuned Model with Pipeline": [[76, "Untuned-Model-with-Pipeline"]], "Use ensemble learners based on mlr3pipelines": [[65, "Use-ensemble-learners-based-on-mlr3pipelines"]], "User Guide": [[110, null]], "Using DoubleML": [[60, "Using-DoubleML"], [79, "Using-DoubleML"]], "Using ML for DiD: Integrating DoubleML in did": [[62, "Using-ML-for-DiD:-Integrating-DoubleML-in-did"]], "Using learners from mlr3, mlr3learners and mlr3extralearners": [[65, "Using-learners-from-mlr3,-mlr3learners-and-mlr3extralearners"]], "Using pipelines to construct learners": [[112, "using-pipelines-to-construct-learners"]], "Utility Classes": [[59, "utility-classes"]], "Utility Classes and Functions": [[59, null]], "Utility Functions": [[59, "utility-functions"]], "VanderWeele and Arah (2011): Benchmarking": [[100, "VanderWeele-and-Arah-(2011):-Benchmarking"]], "Variance estimation": [[156, "variance-estimation"]], "Variance estimation and confidence intervals": [[156, null]], "Visualizations": [[89, "Visualizations"]], "Visualizing Average Potential Outcomes": [[77, "Visualizing-Average-Potential-Outcomes"]], "Visualizing Average Treatment Effects": [[77, "Visualizing-Average-Treatment-Effects"]], "Visualizing the Treatment Effect Structure": [[77, "Visualizing-the-Treatment-Effect-Structure"]], "Weighted Average Treatment Effects": [[111, "weighted-average-treatment-effects"]], "doubleml.data.DoubleMLClusterData": [[4, null]], "doubleml.data.DoubleMLDIDData": [[5, null]], "doubleml.data.DoubleMLData": [[6, null]], "doubleml.data.DoubleMLPanelData": [[7, null]], "doubleml.data.DoubleMLRDDData": [[8, null]], "doubleml.data.DoubleMLSSMData": [[9, null]], "doubleml.datasets.fetch_401K": [[10, null]], "doubleml.datasets.fetch_bonus": [[11, null]], "doubleml.did": [[3, "doubleml-did"]], "doubleml.did.DoubleMLDID": [[12, null]], "doubleml.did.DoubleMLDIDAggregation": [[13, null]], "doubleml.did.DoubleMLDIDBinary": [[14, null]], "doubleml.did.DoubleMLDIDCS": [[15, null]], "doubleml.did.DoubleMLDIDMulti": [[16, null]], "doubleml.did.datasets.make_did_CS2021": [[17, null]], "doubleml.did.datasets.make_did_SZ2020": [[18, null]], "doubleml.did.datasets.make_did_cs_CS2021": [[19, null]], "doubleml.double_ml_score_mixins.LinearScoreMixin": [[20, null]], "doubleml.double_ml_score_mixins.NonLinearScoreMixin": [[21, null]], "doubleml.irm": [[3, "doubleml-irm"]], "doubleml.irm.DoubleMLAPO": [[22, null]], "doubleml.irm.DoubleMLAPOS": [[23, null]], "doubleml.irm.DoubleMLCVAR": [[24, null]], "doubleml.irm.DoubleMLIIVM": [[25, null]], "doubleml.irm.DoubleMLIRM": [[26, null]], "doubleml.irm.DoubleMLLPQ": [[27, null]], "doubleml.irm.DoubleMLPQ": [[28, null]], "doubleml.irm.DoubleMLQTE": [[29, null]], "doubleml.irm.DoubleMLSSM": [[30, null]], "doubleml.irm.datasets.make_confounded_irm_data": [[31, null]], "doubleml.irm.datasets.make_heterogeneous_data": [[32, null]], "doubleml.irm.datasets.make_iivm_data": [[33, null]], "doubleml.irm.datasets.make_irm_data": [[34, null]], "doubleml.irm.datasets.make_irm_data_discrete_treatments": [[35, null]], "doubleml.irm.datasets.make_ssm_data": [[36, null]], "doubleml.plm": [[3, "doubleml-plm"]], "doubleml.plm.DoubleMLLPLR": [[37, null]], "doubleml.plm.DoubleMLPLIV": [[38, null]], "doubleml.plm.DoubleMLPLR": [[39, null]], "doubleml.plm.datasets.make_confounded_plr_data": [[40, null]], "doubleml.plm.datasets.make_lplr_LZZ2020": [[41, null]], "doubleml.plm.datasets.make_pliv_CHS2015": [[42, null]], "doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021": [[43, null]], "doubleml.plm.datasets.make_plr_CCDDHNR2018": [[44, null]], "doubleml.plm.datasets.make_plr_turrell2018": [[45, null]], "doubleml.rdd": [[3, "doubleml-rdd"]], "doubleml.rdd.RDFlex": [[46, null]], "doubleml.rdd.datasets.make_simple_rdd_data": [[47, null]], "doubleml.utils.DMLDummyClassifier": [[48, null]], "doubleml.utils.DMLDummyRegressor": [[49, null]], "doubleml.utils.DMLOptunaResult": [[50, null]], "doubleml.utils.DoubleMLBLP": [[51, null]], "doubleml.utils.DoubleMLPolicyTree": [[52, null]], "doubleml.utils.GlobalClassifier": [[53, null]], "doubleml.utils.GlobalRegressor": [[54, null]], "doubleml.utils.PSProcessor": [[55, null]], "doubleml.utils.PSProcessorConfig": [[56, null]], "doubleml.utils.gain_statistics": [[57, null]]}, "docnames": ["api/api", "api/data_class", "api/datasets", "api/dml_models", "api/generated/doubleml.data.DoubleMLClusterData", "api/generated/doubleml.data.DoubleMLDIDData", "api/generated/doubleml.data.DoubleMLData", "api/generated/doubleml.data.DoubleMLPanelData", "api/generated/doubleml.data.DoubleMLRDDData", "api/generated/doubleml.data.DoubleMLSSMData", "api/generated/doubleml.datasets.fetch_401K", "api/generated/doubleml.datasets.fetch_bonus", "api/generated/doubleml.did.DoubleMLDID", "api/generated/doubleml.did.DoubleMLDIDAggregation", "api/generated/doubleml.did.DoubleMLDIDBinary", "api/generated/doubleml.did.DoubleMLDIDCS", "api/generated/doubleml.did.DoubleMLDIDMulti", "api/generated/doubleml.did.datasets.make_did_CS2021", "api/generated/doubleml.did.datasets.make_did_SZ2020", "api/generated/doubleml.did.datasets.make_did_cs_CS2021", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin", "api/generated/doubleml.irm.DoubleMLAPO", "api/generated/doubleml.irm.DoubleMLAPOS", "api/generated/doubleml.irm.DoubleMLCVAR", "api/generated/doubleml.irm.DoubleMLIIVM", "api/generated/doubleml.irm.DoubleMLIRM", "api/generated/doubleml.irm.DoubleMLLPQ", "api/generated/doubleml.irm.DoubleMLPQ", "api/generated/doubleml.irm.DoubleMLQTE", "api/generated/doubleml.irm.DoubleMLSSM", "api/generated/doubleml.irm.datasets.make_confounded_irm_data", "api/generated/doubleml.irm.datasets.make_heterogeneous_data", "api/generated/doubleml.irm.datasets.make_iivm_data", "api/generated/doubleml.irm.datasets.make_irm_data", "api/generated/doubleml.irm.datasets.make_irm_data_discrete_treatments", "api/generated/doubleml.irm.datasets.make_ssm_data", "api/generated/doubleml.plm.DoubleMLLPLR", "api/generated/doubleml.plm.DoubleMLPLIV", "api/generated/doubleml.plm.DoubleMLPLR", "api/generated/doubleml.plm.datasets.make_confounded_plr_data", "api/generated/doubleml.plm.datasets.make_lplr_LZZ2020", "api/generated/doubleml.plm.datasets.make_pliv_CHS2015", "api/generated/doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021", "api/generated/doubleml.plm.datasets.make_plr_CCDDHNR2018", "api/generated/doubleml.plm.datasets.make_plr_turrell2018", "api/generated/doubleml.rdd.RDFlex", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data", "api/generated/doubleml.utils.DMLDummyClassifier", "api/generated/doubleml.utils.DMLDummyRegressor", "api/generated/doubleml.utils.DMLOptunaResult", "api/generated/doubleml.utils.DoubleMLBLP", "api/generated/doubleml.utils.DoubleMLPolicyTree", "api/generated/doubleml.utils.GlobalClassifier", "api/generated/doubleml.utils.GlobalRegressor", "api/generated/doubleml.utils.PSProcessor", "api/generated/doubleml.utils.PSProcessorConfig", "api/generated/doubleml.utils.gain_statistics", "api/mixins", "api/utility", "examples/R_double_ml_basic_iv", "examples/R_double_ml_basics", "examples/R_double_ml_did", "examples/R_double_ml_multiway_cluster", "examples/R_double_ml_pension", "examples/R_double_ml_pipeline", "examples/R_double_ml_ssm", "examples/did/py_did", "examples/did/py_did_pretest", "examples/did/py_panel", "examples/did/py_panel_data_example", "examples/did/py_panel_simple", "examples/did/py_rep_cs", "examples/double_ml_bonus_data", "examples/index", "examples/learners/py_learner", "examples/learners/py_optuna", "examples/learners/py_tabpfn", "examples/py_double_ml_apo", "examples/py_double_ml_basic_iv", "examples/py_double_ml_basics", "examples/py_double_ml_cate", "examples/py_double_ml_cate_plr", "examples/py_double_ml_cvar", "examples/py_double_ml_firststage", "examples/py_double_ml_gate", "examples/py_double_ml_gate_plr", "examples/py_double_ml_gate_sensitivity", "examples/py_double_ml_irm_vs_apo", "examples/py_double_ml_lplr", "examples/py_double_ml_meets_flaml", "examples/py_double_ml_multiway_cluster", "examples/py_double_ml_pension", "examples/py_double_ml_pension_qte", "examples/py_double_ml_plm_irm_hetfx", "examples/py_double_ml_policy_tree", "examples/py_double_ml_pq", "examples/py_double_ml_rdflex", "examples/py_double_ml_robust_iv", "examples/py_double_ml_sensitivity", "examples/py_double_ml_sensitivity_booking", "examples/py_double_ml_ssm", "guide/algorithms", "guide/basics", "guide/data/base_data", "guide/data/did_data", "guide/data/panel_data", "guide/data/rdd_data", "guide/data/ssm_data", "guide/data_backend", "guide/guide", "guide/heterogeneity", "guide/learners", "guide/learners/python/evaluate_learners", "guide/learners/python/external_preds", "guide/learners/python/minimum_req", "guide/learners/python/set_hyperparams", "guide/learners/python/tune_hyperparams", "guide/learners/python/tune_hyperparams_old", "guide/learners/r/minimum_req", "guide/learners/r/pipelines", "guide/learners/r/set_hyperparams", "guide/learners/r/tune_and_pipelines", "guide/learners/r/tune_hyperparams", "guide/models", "guide/models/did/did_aggregation", "guide/models/did/did_binary", "guide/models/did/did_cs", "guide/models/did/did_implementation", "guide/models/did/did_pa", "guide/models/did/did_setup", "guide/models/irm/apo", "guide/models/irm/apos", "guide/models/irm/iivm", "guide/models/irm/irm", "guide/models/plm/lplr", "guide/models/plm/pliv", "guide/models/plm/plr", "guide/models/ssm/ssm", "guide/resampling", "guide/scores", "guide/scores/did/did_cs_binary_score", "guide/scores/did/did_cs_score", "guide/scores/did/did_pa_binary_score", "guide/scores/did/did_pa_score", "guide/scores/irm/apo_score", "guide/scores/irm/cvar_score", "guide/scores/irm/iivm_score", "guide/scores/irm/irm_score", "guide/scores/irm/lpq_score", "guide/scores/irm/pq_score", "guide/scores/plm/lplr_score", "guide/scores/plm/pliv_score", "guide/scores/plm/plr_score", "guide/scores/ssm/mar_score", "guide/scores/ssm/nr_score", "guide/se_confint", "guide/sensitivity", "guide/sensitivity/benchmarking", "guide/sensitivity/did/did_cs_binary_sensitivity", "guide/sensitivity/did/did_cs_sensitivity", "guide/sensitivity/did/did_pa_binary_sensitivity", "guide/sensitivity/did/did_pa_sensitivity", "guide/sensitivity/implementation", "guide/sensitivity/irm/apo_sensitivity", "guide/sensitivity/irm/irm_sensitivity", "guide/sensitivity/plm/plr_sensitivity", "guide/sensitivity/theory", "index", "intro/install", "intro/intro", "literature/literature", "release/release", "workflow/workflow"], "envversion": {"nbsphinx": 4, "sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1}, "filenames": ["api/api.rst", "api/data_class.rst", "api/datasets.rst", "api/dml_models.rst", "api/generated/doubleml.data.DoubleMLClusterData.rst", "api/generated/doubleml.data.DoubleMLDIDData.rst", "api/generated/doubleml.data.DoubleMLData.rst", "api/generated/doubleml.data.DoubleMLPanelData.rst", "api/generated/doubleml.data.DoubleMLRDDData.rst", "api/generated/doubleml.data.DoubleMLSSMData.rst", "api/generated/doubleml.datasets.fetch_401K.rst", "api/generated/doubleml.datasets.fetch_bonus.rst", "api/generated/doubleml.did.DoubleMLDID.rst", "api/generated/doubleml.did.DoubleMLDIDAggregation.rst", "api/generated/doubleml.did.DoubleMLDIDBinary.rst", "api/generated/doubleml.did.DoubleMLDIDCS.rst", "api/generated/doubleml.did.DoubleMLDIDMulti.rst", "api/generated/doubleml.did.datasets.make_did_CS2021.rst", "api/generated/doubleml.did.datasets.make_did_SZ2020.rst", "api/generated/doubleml.did.datasets.make_did_cs_CS2021.rst", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin.rst", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin.rst", "api/generated/doubleml.irm.DoubleMLAPO.rst", "api/generated/doubleml.irm.DoubleMLAPOS.rst", "api/generated/doubleml.irm.DoubleMLCVAR.rst", "api/generated/doubleml.irm.DoubleMLIIVM.rst", "api/generated/doubleml.irm.DoubleMLIRM.rst", "api/generated/doubleml.irm.DoubleMLLPQ.rst", "api/generated/doubleml.irm.DoubleMLPQ.rst", "api/generated/doubleml.irm.DoubleMLQTE.rst", "api/generated/doubleml.irm.DoubleMLSSM.rst", "api/generated/doubleml.irm.datasets.make_confounded_irm_data.rst", "api/generated/doubleml.irm.datasets.make_heterogeneous_data.rst", "api/generated/doubleml.irm.datasets.make_iivm_data.rst", "api/generated/doubleml.irm.datasets.make_irm_data.rst", "api/generated/doubleml.irm.datasets.make_irm_data_discrete_treatments.rst", "api/generated/doubleml.irm.datasets.make_ssm_data.rst", "api/generated/doubleml.plm.DoubleMLLPLR.rst", "api/generated/doubleml.plm.DoubleMLPLIV.rst", "api/generated/doubleml.plm.DoubleMLPLR.rst", "api/generated/doubleml.plm.datasets.make_confounded_plr_data.rst", "api/generated/doubleml.plm.datasets.make_lplr_LZZ2020.rst", "api/generated/doubleml.plm.datasets.make_pliv_CHS2015.rst", "api/generated/doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021.rst", "api/generated/doubleml.plm.datasets.make_plr_CCDDHNR2018.rst", "api/generated/doubleml.plm.datasets.make_plr_turrell2018.rst", "api/generated/doubleml.rdd.RDFlex.rst", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data.rst", "api/generated/doubleml.utils.DMLDummyClassifier.rst", "api/generated/doubleml.utils.DMLDummyRegressor.rst", "api/generated/doubleml.utils.DMLOptunaResult.rst", "api/generated/doubleml.utils.DoubleMLBLP.rst", "api/generated/doubleml.utils.DoubleMLPolicyTree.rst", "api/generated/doubleml.utils.GlobalClassifier.rst", "api/generated/doubleml.utils.GlobalRegressor.rst", "api/generated/doubleml.utils.PSProcessor.rst", "api/generated/doubleml.utils.PSProcessorConfig.rst", "api/generated/doubleml.utils.gain_statistics.rst", "api/mixins.rst", "api/utility.rst", "examples/R_double_ml_basic_iv.ipynb", "examples/R_double_ml_basics.ipynb", "examples/R_double_ml_did.ipynb", "examples/R_double_ml_multiway_cluster.ipynb", "examples/R_double_ml_pension.ipynb", "examples/R_double_ml_pipeline.ipynb", "examples/R_double_ml_ssm.ipynb", "examples/did/py_did.ipynb", "examples/did/py_did_pretest.ipynb", "examples/did/py_panel.ipynb", "examples/did/py_panel_data_example.ipynb", "examples/did/py_panel_simple.ipynb", "examples/did/py_rep_cs.ipynb", "examples/double_ml_bonus_data.ipynb", "examples/index.rst", "examples/learners/py_learner.ipynb", "examples/learners/py_optuna.ipynb", "examples/learners/py_tabpfn.ipynb", "examples/py_double_ml_apo.ipynb", "examples/py_double_ml_basic_iv.ipynb", "examples/py_double_ml_basics.ipynb", "examples/py_double_ml_cate.ipynb", "examples/py_double_ml_cate_plr.ipynb", "examples/py_double_ml_cvar.ipynb", "examples/py_double_ml_firststage.ipynb", "examples/py_double_ml_gate.ipynb", "examples/py_double_ml_gate_plr.ipynb", "examples/py_double_ml_gate_sensitivity.ipynb", "examples/py_double_ml_irm_vs_apo.ipynb", "examples/py_double_ml_lplr.ipynb", "examples/py_double_ml_meets_flaml.ipynb", "examples/py_double_ml_multiway_cluster.ipynb", "examples/py_double_ml_pension.ipynb", "examples/py_double_ml_pension_qte.ipynb", "examples/py_double_ml_plm_irm_hetfx.ipynb", "examples/py_double_ml_policy_tree.ipynb", "examples/py_double_ml_pq.ipynb", "examples/py_double_ml_rdflex.ipynb", "examples/py_double_ml_robust_iv.ipynb", "examples/py_double_ml_sensitivity.ipynb", "examples/py_double_ml_sensitivity_booking.ipynb", "examples/py_double_ml_ssm.ipynb", "guide/algorithms.rst", "guide/basics.rst", "guide/data/base_data.rst", "guide/data/did_data.rst", "guide/data/panel_data.rst", "guide/data/rdd_data.rst", "guide/data/ssm_data.rst", "guide/data_backend.rst", "guide/guide.rst", "guide/heterogeneity.rst", "guide/learners.rst", "guide/learners/python/evaluate_learners.rst", "guide/learners/python/external_preds.rst", "guide/learners/python/minimum_req.rst", "guide/learners/python/set_hyperparams.rst", "guide/learners/python/tune_hyperparams.rst", "guide/learners/python/tune_hyperparams_old.rst", "guide/learners/r/minimum_req.rst", "guide/learners/r/pipelines.rst", "guide/learners/r/set_hyperparams.rst", "guide/learners/r/tune_and_pipelines.rst", "guide/learners/r/tune_hyperparams.rst", "guide/models.rst", "guide/models/did/did_aggregation.rst", "guide/models/did/did_binary.rst", "guide/models/did/did_cs.rst", "guide/models/did/did_implementation.rst", "guide/models/did/did_pa.rst", "guide/models/did/did_setup.rst", "guide/models/irm/apo.rst", "guide/models/irm/apos.rst", "guide/models/irm/iivm.rst", "guide/models/irm/irm.rst", "guide/models/plm/lplr.rst", "guide/models/plm/pliv.rst", "guide/models/plm/plr.rst", "guide/models/ssm/ssm.rst", "guide/resampling.rst", "guide/scores.rst", "guide/scores/did/did_cs_binary_score.rst", "guide/scores/did/did_cs_score.rst", "guide/scores/did/did_pa_binary_score.rst", "guide/scores/did/did_pa_score.rst", "guide/scores/irm/apo_score.rst", "guide/scores/irm/cvar_score.rst", "guide/scores/irm/iivm_score.rst", "guide/scores/irm/irm_score.rst", "guide/scores/irm/lpq_score.rst", "guide/scores/irm/pq_score.rst", "guide/scores/plm/lplr_score.rst", "guide/scores/plm/pliv_score.rst", "guide/scores/plm/plr_score.rst", "guide/scores/ssm/mar_score.rst", "guide/scores/ssm/nr_score.rst", "guide/se_confint.rst", "guide/sensitivity.rst", "guide/sensitivity/benchmarking.rst", "guide/sensitivity/did/did_cs_binary_sensitivity.rst", "guide/sensitivity/did/did_cs_sensitivity.rst", "guide/sensitivity/did/did_pa_binary_sensitivity.rst", "guide/sensitivity/did/did_pa_sensitivity.rst", "guide/sensitivity/implementation.rst", "guide/sensitivity/irm/apo_sensitivity.rst", "guide/sensitivity/irm/irm_sensitivity.rst", "guide/sensitivity/plm/plr_sensitivity.rst", "guide/sensitivity/theory.rst", "index.rst", "intro/install.rst", "intro/intro.rst", "literature/literature.rst", "release/release.rst", "workflow/workflow.rst"], "indexentries": {"adjust_ps() (doubleml.utils.psprocessor method)": [[55, "doubleml.utils.PSProcessor.adjust_ps", false]], "aggregate() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.aggregate", false]], "aggregate_over_splits() (doubleml.rdd.rdflex method)": [[46, "doubleml.rdd.RDFlex.aggregate_over_splits", false]], "bootstrap() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.bootstrap", false]], "bootstrap() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.bootstrap", false]], "bootstrap() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.bootstrap", false]], "capo() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.capo", false]], "cate() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.cate", false]], "cate() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.cate", false]], "causal_contrast() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.causal_contrast", false]], "confint() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.confint", false]], "confint() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.confint", false]], "confint() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.confint", false]], "confint() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.confint", false]], "confint() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.confint", false]], "confint() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.confint", false]], "confint() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.confint", false]], "confint() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.confint", false]], "confint() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.confint", false]], "confint() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.confint", false]], "confint() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.confint", false]], "confint() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.confint", false]], "confint() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.confint", false]], "confint() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.confint", false]], "confint() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.confint", false]], "confint() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.confint", false]], "confint() (doubleml.rdd.rdflex method)": [[46, "doubleml.rdd.RDFlex.confint", false]], "confint() (doubleml.utils.doublemlblp method)": [[51, "doubleml.utils.DoubleMLBLP.confint", false]], "construct_framework() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.construct_framework", false]], "construct_framework() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.construct_framework", false]], "construct_framework() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.construct_framework", false]], "dmldummyclassifier (class in doubleml.utils)": [[48, "doubleml.utils.DMLDummyClassifier", false]], "dmldummyregressor (class in doubleml.utils)": [[49, "doubleml.utils.DMLDummyRegressor", false]], "dmloptunaresult (class in doubleml.utils)": [[50, "doubleml.utils.DMLOptunaResult", false]], "doublemlapo (class in doubleml.irm)": [[22, "doubleml.irm.DoubleMLAPO", false]], "doublemlapos (class in doubleml.irm)": [[23, "doubleml.irm.DoubleMLAPOS", false]], "doublemlblp (class in doubleml.utils)": [[51, "doubleml.utils.DoubleMLBLP", false]], "doublemlclusterdata (class in doubleml.data)": [[4, "doubleml.data.DoubleMLClusterData", false]], "doublemlcvar (class in doubleml.irm)": [[24, "doubleml.irm.DoubleMLCVAR", false]], "doublemldata (class in doubleml.data)": [[6, "doubleml.data.DoubleMLData", false]], "doublemldid (class in doubleml.did)": [[12, "doubleml.did.DoubleMLDID", false]], "doublemldidaggregation (class in doubleml.did)": [[13, "doubleml.did.DoubleMLDIDAggregation", false]], "doublemldidbinary (class in doubleml.did)": [[14, "doubleml.did.DoubleMLDIDBinary", false]], "doublemldidcs (class in doubleml.did)": [[15, "doubleml.did.DoubleMLDIDCS", false]], "doublemldiddata (class in doubleml.data)": [[5, "doubleml.data.DoubleMLDIDData", false]], "doublemldidmulti (class in doubleml.did)": [[16, "doubleml.did.DoubleMLDIDMulti", false]], "doublemliivm (class in doubleml.irm)": [[25, "doubleml.irm.DoubleMLIIVM", false]], "doublemlirm (class in doubleml.irm)": [[26, "doubleml.irm.DoubleMLIRM", false]], "doublemllplr (class in doubleml.plm)": [[37, "doubleml.plm.DoubleMLLPLR", false]], "doublemllpq (class in doubleml.irm)": [[27, "doubleml.irm.DoubleMLLPQ", false]], "doublemlpaneldata (class in doubleml.data)": [[7, "doubleml.data.DoubleMLPanelData", false]], "doublemlpliv (class in doubleml.plm)": [[38, "doubleml.plm.DoubleMLPLIV", false]], "doublemlplr (class in doubleml.plm)": [[39, "doubleml.plm.DoubleMLPLR", false]], "doublemlpolicytree (class in doubleml.utils)": [[52, "doubleml.utils.DoubleMLPolicyTree", false]], "doublemlpq (class in doubleml.irm)": [[28, "doubleml.irm.DoubleMLPQ", false]], "doublemlqte (class in doubleml.irm)": [[29, "doubleml.irm.DoubleMLQTE", false]], "doublemlrdddata (class in doubleml.data)": [[8, "doubleml.data.DoubleMLRDDData", false]], "doublemlssm (class in doubleml.irm)": [[30, "doubleml.irm.DoubleMLSSM", false]], "doublemlssmdata (class in doubleml.data)": [[9, "doubleml.data.DoubleMLSSMData", false]], "draw_sample_splitting() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.draw_sample_splitting", false]], "evaluate_learners() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.evaluate_learners", false]], "fetch_401k() (in module doubleml.datasets)": [[10, "doubleml.datasets.fetch_401K", false]], "fetch_bonus() (in module doubleml.datasets)": [[11, "doubleml.datasets.fetch_bonus", false]], "fit() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.fit", false]], "fit() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.fit", false]], "fit() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.fit", false]], "fit() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.fit", false]], "fit() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.fit", false]], "fit() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.fit", false]], "fit() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.fit", false]], "fit() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.fit", false]], "fit() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.fit", false]], "fit() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.fit", false]], "fit() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.fit", false]], "fit() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.fit", false]], "fit() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.fit", false]], "fit() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.fit", false]], "fit() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.fit", false]], "fit() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.fit", false]], "fit() (doubleml.rdd.rdflex method)": [[46, "doubleml.rdd.RDFlex.fit", false]], "fit() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.fit", false]], "fit() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.fit", false]], "fit() (doubleml.utils.doublemlblp method)": [[51, "doubleml.utils.DoubleMLBLP.fit", false]], "fit() (doubleml.utils.doublemlpolicytree method)": [[52, "doubleml.utils.DoubleMLPolicyTree.fit", false]], "fit() (doubleml.utils.globalclassifier method)": [[53, "doubleml.utils.GlobalClassifier.fit", false]], "fit() (doubleml.utils.globalregressor method)": [[54, "doubleml.utils.GlobalRegressor.fit", false]], "from_arrays() (doubleml.data.doublemlclusterdata class method)": [[4, "doubleml.data.DoubleMLClusterData.from_arrays", false]], "from_arrays() (doubleml.data.doublemldata class method)": [[6, "doubleml.data.DoubleMLData.from_arrays", false]], "from_arrays() (doubleml.data.doublemldiddata class method)": [[5, "doubleml.data.DoubleMLDIDData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlpaneldata class method)": [[7, "doubleml.data.DoubleMLPanelData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlrdddata class method)": [[8, "doubleml.data.DoubleMLRDDData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlssmdata class method)": [[9, "doubleml.data.DoubleMLSSMData.from_arrays", false]], "from_config() (doubleml.utils.psprocessor class method)": [[55, "doubleml.utils.PSProcessor.from_config", false]], "gain_statistics() (in module doubleml.utils)": [[57, "doubleml.utils.gain_statistics", false]], "gapo() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.gapo", false]], "gate() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.gate", false]], "gate() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.gate", false]], "get_metadata_routing() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalclassifier method)": [[53, "doubleml.utils.GlobalClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalregressor method)": [[54, "doubleml.utils.GlobalRegressor.get_metadata_routing", false]], "get_params() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.get_params", false]], "get_params() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.get_params", false]], "get_params() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.get_params", false]], "get_params() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.get_params", false]], "get_params() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.get_params", false]], "get_params() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.get_params", false]], "get_params() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.get_params", false]], "get_params() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.get_params", false]], "get_params() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.get_params", false]], "get_params() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.get_params", false]], "get_params() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.get_params", false]], "get_params() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.get_params", false]], "get_params() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.get_params", false]], "get_params() (doubleml.utils.globalclassifier method)": [[53, "doubleml.utils.GlobalClassifier.get_params", false]], "get_params() (doubleml.utils.globalregressor method)": [[54, "doubleml.utils.GlobalRegressor.get_params", false]], "globalclassifier (class in doubleml.utils)": [[53, "doubleml.utils.GlobalClassifier", false]], "globalregressor (class in doubleml.utils)": [[54, "doubleml.utils.GlobalRegressor", false]], "linearscoremixin (class in doubleml.double_ml_score_mixins)": [[20, "doubleml.double_ml_score_mixins.LinearScoreMixin", false]], "make_confounded_irm_data() (in module doubleml.irm.datasets)": [[31, "doubleml.irm.datasets.make_confounded_irm_data", false]], "make_confounded_plr_data() (in module doubleml.plm.datasets)": [[40, "doubleml.plm.datasets.make_confounded_plr_data", false]], "make_did_cs2021() (in module doubleml.did.datasets)": [[17, "doubleml.did.datasets.make_did_CS2021", false]], "make_did_cs_cs2021() (in module doubleml.did.datasets)": [[19, "doubleml.did.datasets.make_did_cs_CS2021", false]], "make_did_sz2020() (in module doubleml.did.datasets)": [[18, "doubleml.did.datasets.make_did_SZ2020", false]], "make_heterogeneous_data() (in module doubleml.irm.datasets)": [[32, "doubleml.irm.datasets.make_heterogeneous_data", false]], "make_iivm_data() (in module doubleml.irm.datasets)": [[33, "doubleml.irm.datasets.make_iivm_data", false]], "make_irm_data() (in module doubleml.irm.datasets)": [[34, "doubleml.irm.datasets.make_irm_data", false]], "make_irm_data_discrete_treatments() (in module doubleml.irm.datasets)": [[35, "doubleml.irm.datasets.make_irm_data_discrete_treatments", false]], "make_lplr_lzz2020() (in module doubleml.plm.datasets)": [[41, "doubleml.plm.datasets.make_lplr_LZZ2020", false]], "make_pliv_chs2015() (in module doubleml.plm.datasets)": [[42, "doubleml.plm.datasets.make_pliv_CHS2015", false]], "make_pliv_multiway_cluster_ckms2021() (in module doubleml.plm.datasets)": [[43, "doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021", false]], "make_plr_ccddhnr2018() (in module doubleml.plm.datasets)": [[44, "doubleml.plm.datasets.make_plr_CCDDHNR2018", false]], "make_plr_turrell2018() (in module doubleml.plm.datasets)": [[45, "doubleml.plm.datasets.make_plr_turrell2018", false]], "make_simple_rdd_data() (in module doubleml.rdd.datasets)": [[47, "doubleml.rdd.datasets.make_simple_rdd_data", false]], "make_ssm_data() (in module doubleml.irm.datasets)": [[36, "doubleml.irm.datasets.make_ssm_data", false]], "nonlinearscoremixin (class in doubleml.double_ml_score_mixins)": [[21, "doubleml.double_ml_score_mixins.NonLinearScoreMixin", false]], "p_adjust() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.p_adjust", false]], "p_adjust() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.p_adjust", false]], "p_adjust() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.p_adjust", false]], "plot_effects() (doubleml.did.doublemldidaggregation method)": [[13, "doubleml.did.DoubleMLDIDAggregation.plot_effects", false]], "plot_effects() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.plot_effects", false]], "plot_tree() (doubleml.utils.doublemlpolicytree method)": [[52, "doubleml.utils.DoubleMLPolicyTree.plot_tree", false]], "policy_tree() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.policy_tree", false]], "predict() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.predict", false]], "predict() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.predict", false]], "predict() (doubleml.utils.doublemlpolicytree method)": [[52, "doubleml.utils.DoubleMLPolicyTree.predict", false]], "predict() (doubleml.utils.globalclassifier method)": [[53, "doubleml.utils.GlobalClassifier.predict", false]], "predict() (doubleml.utils.globalregressor method)": [[54, "doubleml.utils.GlobalRegressor.predict", false]], "predict_proba() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.predict_proba", false]], "predict_proba() (doubleml.utils.globalclassifier method)": [[53, "doubleml.utils.GlobalClassifier.predict_proba", false]], "psprocessor (class in doubleml.utils)": [[55, "doubleml.utils.PSProcessor", false]], "psprocessorconfig (class in doubleml.utils)": [[56, "doubleml.utils.PSProcessorConfig", false]], "rdflex (class in doubleml.rdd)": [[46, "doubleml.rdd.RDFlex", false]], "robust_confset() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.robust_confset", false]], "score() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.score", false]], "score() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.score", false]], "score() (doubleml.utils.globalclassifier method)": [[53, "doubleml.utils.GlobalClassifier.score", false]], "score() (doubleml.utils.globalregressor method)": [[54, "doubleml.utils.GlobalRegressor.score", false]], "sensitivity_analysis() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_analysis", false]], "sensitivity_benchmark() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_benchmark", false]], "sensitivity_plot() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_plot", false]], "set_fit_request() (doubleml.utils.globalclassifier method)": [[53, "doubleml.utils.GlobalClassifier.set_fit_request", false]], "set_fit_request() (doubleml.utils.globalregressor method)": [[54, "doubleml.utils.GlobalRegressor.set_fit_request", false]], "set_ml_nuisance_params() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.set_ml_nuisance_params", false]], "set_params() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.set_params", false]], "set_params() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.set_params", false]], "set_params() (doubleml.utils.globalclassifier method)": [[53, "doubleml.utils.GlobalClassifier.set_params", false]], "set_params() (doubleml.utils.globalregressor method)": [[54, "doubleml.utils.GlobalRegressor.set_params", false]], "set_sample_splitting() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.set_sample_splitting", false]], "set_score_request() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.set_score_request", false]], "set_score_request() (doubleml.utils.globalclassifier method)": [[53, "doubleml.utils.GlobalClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.globalregressor method)": [[54, "doubleml.utils.GlobalRegressor.set_score_request", false]], "set_x_d() (doubleml.data.doublemlclusterdata method)": [[4, "doubleml.data.DoubleMLClusterData.set_x_d", false]], "set_x_d() (doubleml.data.doublemldata method)": [[6, "doubleml.data.DoubleMLData.set_x_d", false]], "set_x_d() (doubleml.data.doublemldiddata method)": [[5, "doubleml.data.DoubleMLDIDData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlpaneldata method)": [[7, "doubleml.data.DoubleMLPanelData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlrdddata method)": [[8, "doubleml.data.DoubleMLRDDData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlssmdata method)": [[9, "doubleml.data.DoubleMLSSMData.set_x_d", false]], "tune() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.tune", false]], "tune() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.tune", false]], "tune() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.tune", false]], "tune() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.tune", false]], "tune() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.tune", false]], "tune() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.tune", false]], "tune() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.tune", false]], "tune() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.tune", false]], "tune() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.tune", false]], "tune() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.tune", false]], "tune() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.tune", false]], "tune() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.tune", false]], "tune() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.tune", false]], "tune_ml_models() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.tune_ml_models", false]], "tune_ml_models() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.tune_ml_models", false]], "tune_ml_models() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.tune_ml_models", false]], "tune_ml_models() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.tune_ml_models", false]], "tune_ml_models() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.tune_ml_models", false]], "tune_ml_models() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.tune_ml_models", false]], "tune_ml_models() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.tune_ml_models", false]]}, "objects": {"doubleml.data": [[4, 0, 1, "", "DoubleMLClusterData"], [5, 0, 1, "", "DoubleMLDIDData"], [6, 0, 1, "", "DoubleMLData"], [7, 0, 1, "", "DoubleMLPanelData"], [8, 0, 1, "", "DoubleMLRDDData"], [9, 0, 1, "", "DoubleMLSSMData"]], "doubleml.data.DoubleMLClusterData": [[4, 1, 1, "", "from_arrays"], [4, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLDIDData": [[5, 1, 1, "", "from_arrays"], [5, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLData": [[6, 1, 1, "", "from_arrays"], [6, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLPanelData": [[7, 1, 1, "", "from_arrays"], [7, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLRDDData": [[8, 1, 1, "", "from_arrays"], [8, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLSSMData": [[9, 1, 1, "", "from_arrays"], [9, 1, 1, "", "set_x_d"]], "doubleml.datasets": [[10, 2, 1, "", "fetch_401K"], [11, 2, 1, "", "fetch_bonus"]], "doubleml.did": [[12, 0, 1, "", "DoubleMLDID"], [13, 0, 1, "", "DoubleMLDIDAggregation"], [14, 0, 1, "", "DoubleMLDIDBinary"], [15, 0, 1, "", "DoubleMLDIDCS"], [16, 0, 1, "", "DoubleMLDIDMulti"]], "doubleml.did.DoubleMLDID": [[12, 1, 1, "", "bootstrap"], [12, 1, 1, "", "confint"], [12, 1, 1, "", "construct_framework"], [12, 1, 1, "", "draw_sample_splitting"], [12, 1, 1, "", "evaluate_learners"], [12, 1, 1, "", "fit"], [12, 1, 1, "", "get_params"], [12, 1, 1, "", "p_adjust"], [12, 1, 1, "", "sensitivity_analysis"], [12, 1, 1, "", "sensitivity_benchmark"], [12, 1, 1, "", "sensitivity_plot"], [12, 1, 1, "", "set_ml_nuisance_params"], [12, 1, 1, "", "set_sample_splitting"], [12, 1, 1, "", "tune"], [12, 1, 1, "", "tune_ml_models"]], "doubleml.did.DoubleMLDIDAggregation": [[13, 1, 1, "", "plot_effects"]], "doubleml.did.DoubleMLDIDBinary": [[14, 1, 1, "", "bootstrap"], [14, 1, 1, "", "confint"], [14, 1, 1, "", "construct_framework"], [14, 1, 1, "", "draw_sample_splitting"], [14, 1, 1, "", "evaluate_learners"], [14, 1, 1, "", "fit"], [14, 1, 1, "", "get_params"], [14, 1, 1, "", "p_adjust"], [14, 1, 1, "", "sensitivity_analysis"], [14, 1, 1, "", "sensitivity_benchmark"], [14, 1, 1, "", "sensitivity_plot"], [14, 1, 1, "", "set_ml_nuisance_params"], [14, 1, 1, "", "set_sample_splitting"], [14, 1, 1, "", "tune"], [14, 1, 1, "", "tune_ml_models"]], "doubleml.did.DoubleMLDIDCS": [[15, 1, 1, "", "bootstrap"], [15, 1, 1, "", "confint"], [15, 1, 1, "", "construct_framework"], [15, 1, 1, "", "draw_sample_splitting"], [15, 1, 1, "", "evaluate_learners"], [15, 1, 1, "", "fit"], [15, 1, 1, "", "get_params"], [15, 1, 1, "", "p_adjust"], [15, 1, 1, "", "sensitivity_analysis"], [15, 1, 1, "", "sensitivity_benchmark"], [15, 1, 1, "", "sensitivity_plot"], [15, 1, 1, "", "set_ml_nuisance_params"], [15, 1, 1, "", "set_sample_splitting"], [15, 1, 1, "", "tune"], [15, 1, 1, "", "tune_ml_models"]], "doubleml.did.DoubleMLDIDMulti": [[16, 1, 1, "", "aggregate"], [16, 1, 1, "", "bootstrap"], [16, 1, 1, "", "confint"], [16, 1, 1, "", "fit"], [16, 1, 1, "", "p_adjust"], [16, 1, 1, "", "plot_effects"], [16, 1, 1, "", "sensitivity_analysis"], [16, 1, 1, "", "sensitivity_benchmark"], [16, 1, 1, "", "sensitivity_plot"], [16, 1, 1, "", "tune_ml_models"]], "doubleml.did.datasets": [[17, 2, 1, "", "make_did_CS2021"], [18, 2, 1, "", "make_did_SZ2020"], [19, 2, 1, "", "make_did_cs_CS2021"]], "doubleml.double_ml_score_mixins": [[20, 0, 1, "", "LinearScoreMixin"], [21, 0, 1, "", "NonLinearScoreMixin"]], "doubleml.irm": [[22, 0, 1, "", "DoubleMLAPO"], [23, 0, 1, "", "DoubleMLAPOS"], [24, 0, 1, "", "DoubleMLCVAR"], [25, 0, 1, "", "DoubleMLIIVM"], [26, 0, 1, "", "DoubleMLIRM"], [27, 0, 1, "", "DoubleMLLPQ"], [28, 0, 1, "", "DoubleMLPQ"], [29, 0, 1, "", "DoubleMLQTE"], [30, 0, 1, "", "DoubleMLSSM"]], "doubleml.irm.DoubleMLAPO": [[22, 1, 1, "", "bootstrap"], [22, 1, 1, "", "capo"], [22, 1, 1, "", "confint"], [22, 1, 1, "", "construct_framework"], [22, 1, 1, "", "draw_sample_splitting"], [22, 1, 1, "", "evaluate_learners"], [22, 1, 1, "", "fit"], [22, 1, 1, "", "gapo"], [22, 1, 1, "", "get_params"], [22, 1, 1, "", "p_adjust"], [22, 1, 1, "", "sensitivity_analysis"], [22, 1, 1, "", "sensitivity_benchmark"], [22, 1, 1, "", "sensitivity_plot"], [22, 1, 1, "", "set_ml_nuisance_params"], [22, 1, 1, "", "set_sample_splitting"], [22, 1, 1, "", "tune"], [22, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLAPOS": [[23, 1, 1, "", "bootstrap"], [23, 1, 1, "", "causal_contrast"], [23, 1, 1, "", "confint"], [23, 1, 1, "", "draw_sample_splitting"], [23, 1, 1, "", "fit"], [23, 1, 1, "", "sensitivity_analysis"], [23, 1, 1, "", "sensitivity_benchmark"], [23, 1, 1, "", "sensitivity_plot"], [23, 1, 1, "", "set_sample_splitting"], [23, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLCVAR": [[24, 1, 1, "", "bootstrap"], [24, 1, 1, "", "confint"], [24, 1, 1, "", "construct_framework"], [24, 1, 1, "", "draw_sample_splitting"], [24, 1, 1, "", "evaluate_learners"], [24, 1, 1, "", "fit"], [24, 1, 1, "", "get_params"], [24, 1, 1, "", "p_adjust"], [24, 1, 1, "", "sensitivity_analysis"], [24, 1, 1, "", "sensitivity_benchmark"], [24, 1, 1, "", "sensitivity_plot"], [24, 1, 1, "", "set_ml_nuisance_params"], [24, 1, 1, "", "set_sample_splitting"], [24, 1, 1, "", "tune"], [24, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLIIVM": [[25, 1, 1, "", "bootstrap"], [25, 1, 1, "", "confint"], [25, 1, 1, "", "construct_framework"], [25, 1, 1, "", "draw_sample_splitting"], [25, 1, 1, "", "evaluate_learners"], [25, 1, 1, "", "fit"], [25, 1, 1, "", "get_params"], [25, 1, 1, "", "p_adjust"], [25, 1, 1, "", "robust_confset"], [25, 1, 1, "", "sensitivity_analysis"], [25, 1, 1, "", "sensitivity_benchmark"], [25, 1, 1, "", "sensitivity_plot"], [25, 1, 1, "", "set_ml_nuisance_params"], [25, 1, 1, "", "set_sample_splitting"], [25, 1, 1, "", "tune"], [25, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLIRM": [[26, 1, 1, "", "bootstrap"], [26, 1, 1, "", "cate"], [26, 1, 1, "", "confint"], [26, 1, 1, "", "construct_framework"], [26, 1, 1, "", "draw_sample_splitting"], [26, 1, 1, "", "evaluate_learners"], [26, 1, 1, "", "fit"], [26, 1, 1, "", "gate"], [26, 1, 1, "", "get_params"], [26, 1, 1, "", "p_adjust"], [26, 1, 1, "", "policy_tree"], [26, 1, 1, "", "sensitivity_analysis"], [26, 1, 1, "", "sensitivity_benchmark"], [26, 1, 1, "", "sensitivity_plot"], [26, 1, 1, "", "set_ml_nuisance_params"], [26, 1, 1, "", "set_sample_splitting"], [26, 1, 1, "", "tune"], [26, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLLPQ": [[27, 1, 1, "", "bootstrap"], [27, 1, 1, "", "confint"], [27, 1, 1, "", "construct_framework"], [27, 1, 1, "", "draw_sample_splitting"], [27, 1, 1, "", "evaluate_learners"], [27, 1, 1, "", "fit"], [27, 1, 1, "", "get_params"], [27, 1, 1, "", "p_adjust"], [27, 1, 1, "", "sensitivity_analysis"], [27, 1, 1, "", "sensitivity_benchmark"], [27, 1, 1, "", "sensitivity_plot"], [27, 1, 1, "", "set_ml_nuisance_params"], [27, 1, 1, "", "set_sample_splitting"], [27, 1, 1, "", "tune"], [27, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLPQ": [[28, 1, 1, "", "bootstrap"], [28, 1, 1, "", "confint"], [28, 1, 1, "", "construct_framework"], [28, 1, 1, "", "draw_sample_splitting"], [28, 1, 1, "", "evaluate_learners"], [28, 1, 1, "", "fit"], [28, 1, 1, "", "get_params"], [28, 1, 1, "", "p_adjust"], [28, 1, 1, "", "sensitivity_analysis"], [28, 1, 1, "", "sensitivity_benchmark"], [28, 1, 1, "", "sensitivity_plot"], [28, 1, 1, "", "set_ml_nuisance_params"], [28, 1, 1, "", "set_sample_splitting"], [28, 1, 1, "", "tune"], [28, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLQTE": [[29, 1, 1, "", "bootstrap"], [29, 1, 1, "", "confint"], [29, 1, 1, "", "draw_sample_splitting"], [29, 1, 1, "", "fit"], [29, 1, 1, "", "p_adjust"], [29, 1, 1, "", "set_sample_splitting"], [29, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLSSM": [[30, 1, 1, "", "bootstrap"], [30, 1, 1, "", "confint"], [30, 1, 1, "", "construct_framework"], [30, 1, 1, "", "draw_sample_splitting"], [30, 1, 1, "", "evaluate_learners"], [30, 1, 1, "", "fit"], [30, 1, 1, "", "get_params"], [30, 1, 1, "", "p_adjust"], [30, 1, 1, "", "sensitivity_analysis"], [30, 1, 1, "", "sensitivity_benchmark"], [30, 1, 1, "", "sensitivity_plot"], [30, 1, 1, "", "set_ml_nuisance_params"], [30, 1, 1, "", "set_sample_splitting"], [30, 1, 1, "", "tune"], [30, 1, 1, "", "tune_ml_models"]], "doubleml.irm.datasets": [[31, 2, 1, "", "make_confounded_irm_data"], [32, 2, 1, "", "make_heterogeneous_data"], [33, 2, 1, "", "make_iivm_data"], [34, 2, 1, "", "make_irm_data"], [35, 2, 1, "", "make_irm_data_discrete_treatments"], [36, 2, 1, "", "make_ssm_data"]], "doubleml.plm": [[37, 0, 1, "", "DoubleMLLPLR"], [38, 0, 1, "", "DoubleMLPLIV"], [39, 0, 1, "", "DoubleMLPLR"]], "doubleml.plm.DoubleMLLPLR": [[37, 1, 1, "", "bootstrap"], [37, 1, 1, "", "confint"], [37, 1, 1, "", "construct_framework"], [37, 1, 1, "", "draw_sample_splitting"], [37, 1, 1, "", "evaluate_learners"], [37, 1, 1, "", "fit"], [37, 1, 1, "", "get_params"], [37, 1, 1, "", "p_adjust"], [37, 1, 1, "", "sensitivity_analysis"], [37, 1, 1, "", "sensitivity_benchmark"], [37, 1, 1, "", "sensitivity_plot"], [37, 1, 1, "", "set_ml_nuisance_params"], [37, 1, 1, "", "set_sample_splitting"], [37, 1, 1, "", "tune"], [37, 1, 1, "", "tune_ml_models"]], "doubleml.plm.DoubleMLPLIV": [[38, 1, 1, "", "bootstrap"], [38, 1, 1, "", "confint"], [38, 1, 1, "", "construct_framework"], [38, 1, 1, "", "draw_sample_splitting"], [38, 1, 1, "", "evaluate_learners"], [38, 1, 1, "", "fit"], [38, 1, 1, "", "get_params"], [38, 1, 1, "", "p_adjust"], [38, 1, 1, "", "sensitivity_analysis"], [38, 1, 1, "", "sensitivity_benchmark"], [38, 1, 1, "", "sensitivity_plot"], [38, 1, 1, "", "set_ml_nuisance_params"], [38, 1, 1, "", "set_sample_splitting"], [38, 1, 1, "", "tune"], [38, 1, 1, "", "tune_ml_models"]], "doubleml.plm.DoubleMLPLR": [[39, 1, 1, "", "bootstrap"], [39, 1, 1, "", "cate"], [39, 1, 1, "", "confint"], [39, 1, 1, "", "construct_framework"], [39, 1, 1, "", "draw_sample_splitting"], [39, 1, 1, "", "evaluate_learners"], [39, 1, 1, "", "fit"], [39, 1, 1, "", "gate"], [39, 1, 1, "", "get_params"], [39, 1, 1, "", "p_adjust"], [39, 1, 1, "", "sensitivity_analysis"], [39, 1, 1, "", "sensitivity_benchmark"], [39, 1, 1, "", "sensitivity_plot"], [39, 1, 1, "", "set_ml_nuisance_params"], [39, 1, 1, "", "set_sample_splitting"], [39, 1, 1, "", "tune"], [39, 1, 1, "", "tune_ml_models"]], "doubleml.plm.datasets": [[40, 2, 1, "", "make_confounded_plr_data"], [41, 2, 1, "", "make_lplr_LZZ2020"], [42, 2, 1, "", "make_pliv_CHS2015"], [43, 2, 1, "", "make_pliv_multiway_cluster_CKMS2021"], [44, 2, 1, "", "make_plr_CCDDHNR2018"], [45, 2, 1, "", "make_plr_turrell2018"]], "doubleml.rdd": [[46, 0, 1, "", "RDFlex"]], "doubleml.rdd.RDFlex": [[46, 1, 1, "", "aggregate_over_splits"], [46, 1, 1, "", "confint"], [46, 1, 1, "", "fit"]], "doubleml.rdd.datasets": [[47, 2, 1, "", "make_simple_rdd_data"]], "doubleml.utils": [[48, 0, 1, "", "DMLDummyClassifier"], [49, 0, 1, "", "DMLDummyRegressor"], [50, 0, 1, "", "DMLOptunaResult"], [51, 0, 1, "", "DoubleMLBLP"], [52, 0, 1, "", "DoubleMLPolicyTree"], [53, 0, 1, "", "GlobalClassifier"], [54, 0, 1, "", "GlobalRegressor"], [55, 0, 1, "", "PSProcessor"], [56, 0, 1, "", "PSProcessorConfig"], [57, 2, 1, "", "gain_statistics"]], "doubleml.utils.DMLDummyClassifier": [[48, 1, 1, "", "fit"], [48, 1, 1, "", "get_metadata_routing"], [48, 1, 1, "", "get_params"], [48, 1, 1, "", "predict"], [48, 1, 1, "", "predict_proba"], [48, 1, 1, "", "score"], [48, 1, 1, "", "set_params"], [48, 1, 1, "", "set_score_request"]], "doubleml.utils.DMLDummyRegressor": [[49, 1, 1, "", "fit"], [49, 1, 1, "", "get_metadata_routing"], [49, 1, 1, "", "get_params"], [49, 1, 1, "", "predict"], [49, 1, 1, "", "score"], [49, 1, 1, "", "set_params"], [49, 1, 1, "", "set_score_request"]], "doubleml.utils.DoubleMLBLP": [[51, 1, 1, "", "confint"], [51, 1, 1, "", "fit"]], "doubleml.utils.DoubleMLPolicyTree": [[52, 1, 1, "", "fit"], [52, 1, 1, "", "plot_tree"], [52, 1, 1, "", "predict"]], "doubleml.utils.GlobalClassifier": [[53, 1, 1, "", "fit"], [53, 1, 1, "", "get_metadata_routing"], [53, 1, 1, "", "get_params"], [53, 1, 1, "", "predict"], [53, 1, 1, "", "predict_proba"], [53, 1, 1, "", "score"], [53, 1, 1, "", "set_fit_request"], [53, 1, 1, "", "set_params"], [53, 1, 1, "", "set_score_request"]], "doubleml.utils.GlobalRegressor": [[54, 1, 1, "", "fit"], [54, 1, 1, "", "get_metadata_routing"], [54, 1, 1, "", "get_params"], [54, 1, 1, "", "predict"], [54, 1, 1, "", "score"], [54, 1, 1, "", "set_fit_request"], [54, 1, 1, "", "set_params"], [54, 1, 1, "", "set_score_request"]], "doubleml.utils.PSProcessor": [[55, 1, 1, "", "adjust_ps"], [55, 1, 1, "", "from_config"]]}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function"}, "terms": {"": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 42, 43, 44, 45, 46, 51, 53, 54, 60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 78, 84, 85, 86, 87, 89, 91, 92, 93, 97, 99, 100, 101, 102, 104, 106, 107, 108, 109, 112, 113, 120, 122, 123, 124, 126, 127, 129, 130, 138, 140, 154, 155, 156, 157, 158, 168, 170, 171, 172, 173], "0": [4, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 51, 52, 54, 55, 56, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 165, 169, 170, 172], "00": [60, 72, 76, 85, 86, 88, 92, 93, 139], "000": [98, 156, 173], "00000": 88, "000000": [69, 71, 73, 78, 88, 92, 93, 104, 109, 111, 170], "0000000": 156, "0000000000000010000100": [65, 104, 109, 170], "000000e": [72, 85, 86, 88, 92, 93], "000006": 78, "000017": 96, "000025": 91, "000034": 92, "000039": 91, "000041": 70, "000056": 111, "000064": 79, "000067": 91, "000078": 124, "000091": 91, "000099": 70, "0001": [70, 73, 92], "000107": 70, "000128": 89, "000129": 37, "000147": 70, "000219": 28, "000242": 29, "000297": 70, "000320": 96, "00032016": 96, "000341": 91, "000343": 72, "000375": 72, "000413": 69, "000422": 72, "000438": 72, "000442": 91, "000455": 69, "00047580260495": 60, "000488": 91, "000494": 87, "0005": 73, "000506": 72, "000517": 70, "000522": [91, 124, 125], "000545": 69, "0005582241": 139, "000573": 72, "000594": 72, "000597": 69, "000599": 69, "0005a80b528f": 65, "000616": 72, "000617": 72, "000619": 69, "000646": 69, "000670": 91, "000743": 99, "000769": 70, "000811": 70, "0008928445": 139, "000915799": 156, "0009157990": 156, "000916": [105, 109], "000936": 69, "000943": [81, 82], "001": [17, 19, 55, 60, 62, 63, 64, 65, 66, 76, 80, 94, 112, 120, 121, 122, 123, 124, 139, 140, 156, 170, 173], "001006": 76, "001008": 76, "001051": 91, "001054": 70, "001073": 76, "001080": 76, "001097": 76, "0011": 70, "001145": 93, "0011563701553192595": 76, "001159": 76, "001181": 76, "001184": 76, "001190": 70, "001193277": 139, "001278144": 139, "001289": 82, "00133": 65, "001353": 76, "0013978426220758982": 76, "001403": 97, "001463": [111, 112, 114, 124], "001471": 88, "001494": 111, "0015": 70, "001553": 70, "001576": 76, "001592": 82, "0016": [64, 92], "00162673": [102, 140], "001698": 88, "001738": 76, "001779": 76, "0018": [64, 92], "001820": 70, "001852": 81, "0019": 73, "001902912": 139, "001907": 88, "002": 120, "002005": 81, "002037900301454": 90, "002042": 70, "002074": 72, "002169": 81, "002169338": 156, "0021693380": 156, "0021693381": 156, "002290": 68, "0023": 62, "002395": 72, "002436": 87, "002509": 70, "002545": 70, "0026": 73, "002683": 70, "002686": 70, "0027": 70, "002779": 99, "0028": [62, 64, 92], "002821": 100, "00282133350419121": 100, "002983": 91, "003": [18, 31, 40, 98], "003045": 88, "003062": 124, "003134": 96, "003196": 70, "003215": 70, "003217": 70, "003220": 78, "003277": [124, 125], "003279": 70, "0033": 70, "003328": 96, "0034": 84, "003404": 78, "003415": 78, "003423": 70, "003427": 91, "003432334": 139, "003497": 70, "003605": 69, "003689": [124, 125], "003779": 87, "003836": 96, "003924": 87, "004": 60, "0040": 70, "004050": 70, "00410838": [102, 140], "004124": 70, "004184458": 139, "0042": [64, 92], "004241": 70, "004253": 78, "004340": 81, "004392": 87, "004526": 78, "004542": 88, "0046": 70, "004657": 69, "0047": [64, 92], "004827": 82, "004846": 100, "004868": 76, "004897": 70, "005": 94, "005182": [124, 125], "005305": 69, "005339": [81, 82], "005414": 71, "005485": 76, "005734": 76, "005757": 69, "005817": 70, "005857": 91, "005884": 69, "005e": 124, "006": 94, "006055": 78, "0060715124549546": 90, "006080": 69, "006367": 72, "0065": 70, "006593": 111, "006604": 70, "006922": 73, "006958": [81, 82], "00728": 170, "0073": 73, "007330": 69, "007421": 111, "007596": 76, "007640": 72, "007680021": 139, "007789e": 90, "008": [94, 100], "008023": 93, "008084": 72, "008142": [124, 127], "008223": [81, 82], "008487": 73, "008596": 70, "008674": 16, "008825": 83, "008825189994473": 83, "008883698": 140, "00888458890362062": 102, "008884589": 102, "008931": 72, "008972": 72, "008dbd": 94, "008e80": 94, "009": [94, 100], "009037": 81, "009122": 96, "009150158": 139, "0092": 70, "009301697": 139, "009309331": 139, "009329847": 140, "009354": 81, "009363": 70, "009428": 83, "00944171905420782": 100, "00950122695463054": 102, "009501226954630540": 102, "009501227": 102, "009645422": 63, "009656": 96, "00972": 73, "009790": 93, "00996473e": 139, "009986": 96, "01": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 52, 55, 56, 60, 63, 64, 65, 66, 69, 72, 76, 77, 81, 82, 88, 92, 93, 94, 95, 96, 97, 112, 118, 120, 121, 122, 123, 124, 127, 129, 139, 140, 156, 170, 173], "010042": 72, "010045": 90, "010081": 94, "010213": 99, "010269": 91, "010421": 111, "010436": 71, "010450": 63, "010470": 72, "0105": 70, "010502": 72, "010726": 76, "010865": 94, "010940": 91, "011": 99, "011016": 77, "011131": 96, "011204": 88, "01123152": 139, "01128": 73, "011448e": 81, "011598": 96, "011721": 70, "011756": 69, "0118095": 63, "011823": 99, "011829": 70, "011966": 94, "011988e": 96, "012128": 70, "01219": 65, "0124105481660435": 90, "012468": 72, "012591": 70, "01269238": 139, "01274": 100, "012831": 100, "012962": 82, "013034": 100, "013088": 25, "013223": 94, "013286": 70, "013318": 70, "013441": 81, "013450": 88, "01351638": 63, "013593": 99, "013598": 69, "013654": 72, "013677": 97, "013849": 76, "01398951": 63, "013990": 156, "014": 97, "014013": 70, "01403089": 63, "014080": [81, 82], "014217": 70, "014229": 72, "014361": 81, "0144": 62, "014431": 93, "014432": 68, "014487": 69, "014562": 72, "014593": 76, "014637": 91, "014681": 99, "01471548": 139, "014721": 93, "014861": 72, "0149": 62, "014914": 76, "015": 65, "015038": 83, "015040": 69, "015061": 70, "0151": 70, "015112": 70, "0151932": 139, "015565": 96, "015651": 70, "015698": 96, "015723": 69, "015739": 85, "01574297": 96, "015743": 96, "015858": 72, "015860": 81, "016154": 91, "016200": [81, 82], "016313": 70, "016372": 70, "0164": 70, "01643": 171, "016518": 93, "0167": 70, "016724": 70, "016786": 76, "016815": 70, "016848": 70, "017": [65, 120], "017185": 93, "017393e": 156, "017484823": 139, "017660": 88, "017750": 70, "01777521e": 139, "017800092": 156, "0178000920": 156, "017898": 70, "0179": 70, "017979": 70, "018": [65, 94], "018092": 111, "018148": 96, "018158": 158, "018213706": 139, "018416": 70, "018490": 70, "018600": 81, "0187512020118494": 90, "01900305": [124, 127], "01903": [65, 112, 122, 168, 170], "019043": [124, 129], "019096": 70, "01925597": 63, "019377": 70, "019439633": 156, "0194396330": 156, "0194396331": 156, "019480": 70, "0195": 70, "019527": 69, "019596": 83, "019660": 29, "019700": 72, "019776": 70, "01990373": 101, "019936": 70, "019948": 82, "019989": 70, "02": [16, 69, 72, 81, 82, 92, 93, 96, 124, 127, 129, 139], "02016117": 170, "020166": 96, "0202": 70, "020221": 70, "020267": 70, "020268": 70, "020271": 91, "020272": 88, "020360838": 156, "0203608380": 156, "0203608381": 156, "020427": 69, "02047": 99, "020753": 81, "020809": 72, "02081109": [124, 129], "020815": 81, "02092": 170, "021269": [85, 86], "021272": 70, "02145978": 139, "02152395": [102, 140], "02163217": 63, "021652": 70, "021728": 77, "021866": 95, "021885e": 82, "021926": 83, "021929": 70, "022141": 72, "022258": 88, "022282": 70, "022314": 70, "022380": 69, "02247976": 63, "022508": 81, "022511": 93, "022629": 111, "022749": 111, "022768": 73, "022880": 71, "022915": 91, "022950": 93, "022991": 69, "023": 94, "023020e": [92, 93], "02302115e": 139, "023092": 70, "023190": 70, "023214": 72, "023256": 96, "0234": 70, "023563": 156, "02380945": 139, "023974": 69, "024": 94, "024175": 72, "0242": 70, "024266": 88, "024355": 68, "024401": [85, 86], "024441": 70, "024583": 69, "024604": 91, "02467": 98, "02470531e": 139, "024782": 96, "024926": 68, "024985": 77, "025": [81, 82, 85, 86, 88, 94], "025077": 156, "0253": 65, "025407": 90, "025443": 73, "025468": 173, "025487": 82, "025523": 70, "025813114": 156, "0258131140": 156, "02584": 65, "025841": 88, "025963": 173, "025964": 88, "026": 94, "0261": 70, "026275": 70, "02630005": [124, 127], "026434": 70, "02649507e": 139, "026556": 70, "026723": 83, "026876": 70, "026933": 70, "026966": 88, "02699695": 75, "027014": 72, "027220": 69, "027692": 82, "027694": 157, "027872": 81, "027880": 70, "02791": 73, "027924": 70, "028": [60, 139], "0281": 65, "028129": 70, "028146": 90, "028503": 82, "028520": [81, 82], "028549": 69, "028563": 69, "0286318": 139, "028681": 111, "028682": 81, "028731": 111, "028868": 93, "028872": 70, "02900983": 96, "029010": 96, "029172": 70, "0294": 70, "029430": 72, "029455": 82, "029760": 69, "029827": 173, "029831": 96, "02988702e": 139, "029910e": [92, 93], "029970": 70, "02e": 64, "03": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 66, 69, 72, 78, 81, 82, 83, 87, 88, 92, 93, 96, 97, 99, 100, 124, 125, 127, 129, 139, 157, 163, 173], "0300": 70, "0301": 65, "03018": 27, "030277": 70, "030346": 170, "0304": 70, "030405": 69, "03045": 66, "030573": [69, 70], "0306": 70, "030665": 70, "030696": 72, "0307": 65, "030711": 71, "03084929": 139, "030858": 76, "030934": 96, "030962": 96, "030964": 77, "031075": 90, "0311": 70, "03112770e": 139, "03113": 101, "031134": [112, 118], "0312": 70, "0312190390696285": 76, "031259": 70, "031269": 73, "031323": 90, "031444": 94, "031491": 90, "031639": 96, "031652": 72, "031692": 90, "031712": 93, "03178646": 139, "03191": 171, "032016": 69, "032022": 70, "032140": 111, "032187": 94, "03220": 172, "032328": 69, "032465": 94, "0325": 170, "032532": 70, "032953": 99, "032974": 82, "033045": 112, "033132": 70, "033271": 72, "033327": 117, "033353": 26, "0336": 70, "033600": 70, "033639": 70, "033661": 90, "033702": 70, "033779": 90, "033946": [85, 86], "03411": 170, "034153": 69, "034185": 70, "034192": 69, "034244": 71, "034276": 70, "0343": 70, "034362": 69, "03438": 66, "034482": 70, "034690": 83, "034712": 72, "0348": 70, "034812763": 156, "0348127630": 156, "0348127631": 156, "034846": 92, "03489": [43, 63, 91], "034916": 70, "03492403": [112, 113], "034956": 70, "0350": 70, "035044": 69, "035077": 71, "035119185": 156, "0351191850": 156, "0351191851": 156, "035161": 82, "035265": 83, "035306": 70, "035334": 72, "03536": 170, "03538": 65, "03539": 65, "035391": 73, "0354": 65, "035411": 170, "03545": 65, "0355": 70, "035545": 73, "035572": 73, "035695": 72, "035730": 96, "03574": 73, "035762": 96, "035766": 70, "0359": 65, "0361": 70, "036129015": 156, "0361290150": 156, "0361290151": 156, "036143": 96, "036146": 70, "036147": 96, "036240": 78, "0363": 62, "036577": 77, "03658519e": 139, "036613": 70, "036729": 91, "036778": 69, "037008": [85, 86], "037042": 70, "037097": 72, "037114": 88, "037117": 82, "037144": 81, "037301": 76, "0374": 65, "037509": 101, "037529": 88, "037747": [81, 82], "03783666": 96, "037837": 96, "037994": 69, "038103": 88, "038157": 70, "038240": 81, "038429": 72, "038725": 71, "038812": 95, "038831": 90, "038848": 81, "038896": 70, "038913": 70, "038933": 82, "039006": 81, "039033": 70, "03907122389107094": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "039092": 70, "039141": 78, "039154": 93, "039191": 72, "039240": 69, "039302e": 83, "0394": 70, "039506": 82, "039552": 111, "0396": 70, "039661": 88, "03968904": [124, 127], "03975998": [140, 156], "039868": 82, "039895": 88, "04": [16, 40, 64, 69, 72, 78, 81, 82, 92, 93, 96, 97, 99, 124, 125, 127, 129, 139, 173], "040010": 88, "040112": 156, "040118": 77, "040139": [81, 82], "040273e": 139, "040484": 71, "04050481e": 139, "040533": 39, "040547": [140, 156], "04054744": 156, "040784": 78, "040803": 70, "040826": 81, "040912": 82, "0410": 70, "041147": 83, "0412": 70, "041262": 90, "041275": 70, "041284": 83, "041387": 83, "041466": 94, "041475": 70, "041491e": 83, "041510": 71, "04165": 124, "041689": 70, "041746": 82, "0418": 62, "041816": 81, "041831": 83, "041863e": 99, "0419": 70, "041912": 70, "041953": [124, 127], "042034": 100, "042060": 90, "042068": 72, "042265": 83, "04230353e": 139, "042360": 70, "0424": 70, "042407": 70, "042455": 81, "0425": [112, 122, 123], "042517": 69, "042583": 93, "0428": [62, 101], "042804": 88, "042844e": 96, "042954": [124, 129], "042957": 82, "04300012336462904": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "043107": 69, "043173": 70, "0433": 62, "043303": 86, "043376": 70, "043386": 71, "043428": 69, "043444": 81, "0434e374": 65, "043820": 76, "043839": 81, "04387": [112, 121], "043969": 70, "044051": 82, "044113": 83, "04415": 65, "044176": 88, "044239": 88, "04424": 65, "044289": 70, "044414e": 81, "044447": [105, 109], "04444978": 156, "044449780": 156, "0445": [77, 112, 121], "044640": 70, "04465": 63, "04480325": [157, 163], "044834": 124, "04486": 170, "04490111": [157, 163], "044929": 88, "04501612": 156, "04502": [112, 121, 140, 156], "04502117e": 139, "045062": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "045144": 91, "045172": 88, "045206": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "045379": 170, "045484e": 81, "04552": 91, "045553": 83, "045624": 68, "04563": [112, 123], "045670": 70, "045744": 71, "045754": 96, "04583": [112, 120], "04586": [112, 121], "045932": 96, "045984": 88, "046057": [112, 113, 116], "046088": 96, "04608822": 96, "04625": [112, 120], "046262": 70, "046355e": 72, "046428": 82, "046451": 88, "04650787e": 139, "046527": 83, "0466028": 63, "046612": 70, "046640": 71, "046728": 99, "046788": 93, "046844": 111, "047": 94, "047014": 71, "047057": [112, 116], "047239": 88, "047548": 69, "047589": 86, "047873": 88, "047954": 91, "048365": 111, "048476": 88, "048490": 72, "048699": 101, "048723": [112, 118], "049042e": 72, "04932316": 139, "049622": 70, "049714": 82, "05": [16, 17, 19, 56, 60, 62, 63, 64, 65, 66, 69, 72, 76, 81, 82, 83, 84, 91, 92, 93, 94, 96, 97, 100, 112, 117, 118, 120, 121, 122, 123, 124, 125, 127, 129, 139, 140, 156, 170, 173], "050": 94, "050090": 71, "05039": 99, "050396": 82, "050630": 71, "050919": 70, "050950": 82, "051": 65, "051186": 96, "051341": 95, "051369": 71, "051651": 97, "051666": 69, "051870e": 83, "051939": [124, 129], "052": 97, "052023": 88, "0520233166790431": 90, "052192": 71, "052233": 81, "052274e": 82, "052305": 70, "0525": 70, "052502": 96, "052582": 81, "052745": 83, "052811": 93, "053": [65, 124], "053018": 81, "053029": [157, 163], "053176e": 139, "053222": 69, "053224": 70, "05329204": 139, "0533": 62, "053331": 83, "053337": [124, 129], "053389": 156, "053541": 96, "053558": 83, "053659": 67, "053740": 70, "054": [65, 97], "05401512": 139, "054064": 90, "054068": 91, "054162": 91, "05425976": 139, "054279": 76, "054339": 156, "054370": 83, "054443": [124, 129], "05447527": 139, "054529": 156, "054554": 69, "054700": 70, "054771e": 96, "055": 94, "055165": 99, "05524180e": 139, "055398": 72, "055439": 93, "055493": 100, "055648": 81, "055680": 156, "055723": 82, "055762": 70, "055921": 82, "056": 97, "056172": 77, "0564": 70, "056413": 71, "056469": 69, "056499": 86, "056552": 70, "056602": 71, "0567": 139, "056731": 70, "056915": 88, "05696211e": 139, "057": 120, "057371": 93, "057388": 70, "057455": 72, "057506": 81, "0576": [64, 92], "057762": 96, "057796": 71, "057958": 173, "057962": 83, "058042": 156, "058096": 81, "058175": 96, "058216": 72, "058375": 78, "058463": 96, "058472e": 81, "058508": 101, "058538": 25, "058706": 70, "0588": 70, "058827": 69, "058903541934281406": 76, "05891": 124, "059": 120, "0590": 62, "059384": 96, "059630": 68, "059685": 96, "059705": 71, "05972558e": 139, "059898": 70, "0599": 62, "06": [18, 31, 40, 69, 72, 78, 81, 82, 83, 92, 93, 96, 112, 121], "060": 94, "060016": 78, "060091": 70, "0601": 70, "060201": 96, "060212": [92, 93], "0603268864456956": 90, "06058120e": 139, "060658": 70, "060659": 70, "060672": [124, 127], "060706": 76, "060787": 70, "060838": 81, "060845": 156, "06098643": 124, "061": 94, "061090": 69, "06111111": 65, "06139975e": 139, "0615": 62, "061505": 69, "061953": 81, "062": 124, "0622": 62, "062346": 72, "062414": 93, "062546": [124, 127], "062570": 69, "062594": 81, "06269": 98, "06270135": 75, "06278811": [124, 127], "062794": 71, "062813": 82, "0629": 62, "06295": 124, "062964": 156, "062977": 69, "063017": 78, "0631": 62, "063115": 70, "063172": 71, "063221": 70, "0633": 62, "063312": 93, "063327": 88, "0634": 62, "063428e": 93, "0635": 62, "063590": 96, "0638": 62, "06397789": 96, "063978": [81, 96], "063995": 71, "064157": 81, "064164": 93, "06428": 92, "064280": 92, "0643": 62, "064563": 71, "0646": 62, "0646222": 64, "064673": 77, "064738": 69, "065": 100, "0651": 62, "0652": 62, "065223": 95, "065277": 111, "065356": [85, 86], "065453": 76, "065522": 71, "065535": 30, "065670": 71, "065725": 83, "0658": 62, "065892": 82, "065934": 71, "065969": 124, "065976": 88, "066": 97, "0660": 62, "066026": 71, "0661": 70, "066180": 72, "066295": 88, "0663": 62, "066367": 69, "066389": 70, "0664": 70, "066424": 71, "066464": 99, "066478": 90, "066654": 81, "0670": 62, "067212": 88, "067260": 70, "067289": 72, "067436": 90, "0675": 62, "067528": 100, "067564": 71, "067639": 93, "067721": 156, "06796976e": 139, "0680": 62, "06800344": 139, "06811": 124, "068148": 96, "068377": 93, "068437": 71, "068505": 69, "068669": 69, "068700": 111, "068780": 82, "068912": 71, "068934": 78, "06895837": 63, "068960": 69, "069": 60, "0690": 62, "0691": 62, "069144": 90, "069178": 70, "069384": 81, "069443": 78, "0695854": 63, "069589": 88, "069727": 81, "0698": 62, "069861": 72, "07": [72, 81, 82, 92, 93, 96, 97, 100, 111, 124, 139], "070020": 96, "0702": 62, "0702127": 63, "070393": 93, "0704": [62, 70], "070418": 82, "070433": 88, "070484": 71, "070497": 100, "070797": 93, "07085301": 124, "070884": 96, "071051": 69, "071279": 156, "0713": 62, "07136": [63, 91], "071543e": 83, "071676": 69, "07168291": 63, "071731": 93, "071777": [112, 118], "071782": 29, "0718": 62, "071871333": 139, "07196666": 139, "07202564": [85, 86], "072069": 26, "07222222": 65, "072293": 95, "072307": 72, "072448": 72, "072516": 88, "072605": 78, "0727": 124, "072945": 69, "073013": 96, "073207": 91, "0732109605604835": 76, "073232": 70, "073384": 88, "073404": 69, "07347676": 63, "07350015": [36, 43, 63, 91], "073518": 83, "0735183279373635": 83, "073520": 83, "073592": 82, "07366": [65, 112, 123], "073706": 70, "073736": 70, "073808": 81, "073948": 70, "074": 97, "074303": 76, "074304": 156, "074426": 96, "07456127": 63, "074700e": 93, "074708": 70, "07479278": 99, "074891": 70, "074927": 78, "07493193": 139, "075041": 69, "075097": 72, "075261": 68, "075384": 96, "07538443": 96, "075420": 72, "075540": 72, "07561": 170, "0758": 100, "075809": 78, "075869": [112, 118], "076019": 92, "076119": 93, "076150": 156, "076179312": 156, "0761793120": 156, "07627968": 139, "07628202": 139, "076322": 96, "076347": 83, "076403": 70, "0765": 65, "076656": 70, "076684": 170, "076869": 117, "07689": 65, "076953": [85, 86], "076971": 73, "077090": 90, "077211": 112, "07723": 70, "077319": 96, "077527e": 93, "07752960e": 139, "077592": 88, "077676": 70, "077702": 78, "0777777777777778": [112, 123], "07777778": [65, 112, 123], "077883": 96, "077920": [105, 109], "07796": 124, "078090": 156, "078095": 16, "078138007883929": 76, "078207": 73, "07828372": 156, "078426": 111, "078474": 156, "078810": 96, "079085": 73, "079109": 71, "07915": 65, "079214": 82, "079272": [124, 129], "07932084": 139, "07942v3": 171, "079458e": 92, "079485": 81, "07954276e": 139, "079564": 70, "07961": 99, "079761": [105, 109], "08": [83, 93, 96, 100], "080121": 93, "080193": 81, "08021288": 139, "08032865": 139, "08034998": 139, "080601": 71, "080633": 76, "080716": 72, "080737": [124, 127], "08075788": 139, "0808": 70, "080854": 93, "080938": 86, "080947": 73, "080958": 81, "080994e": 81, "080998": 70, "081": [65, 94], "081100": 96, "081230": [81, 82], "081488": 91, "081494": 71, "081919e": 82, "0820": 62, "082188": 69, "082197": 88, "0823523": 139, "082354": 70, "082405": 81, "0826": 70, "082643": 16, "082804": 68, "0829122": 139, "082973": 91, "083079": 93, "083101": 82, "083142": 139, "083226": 72, "083258": 156, "083312": 156, "08333333": 65, "083449": 82, "0835771416": 63, "0836": 88, "083601": 82, "08364": 88, "083706": 100, "083940": 70, "083949": 100, "083950": 72, "084": 63, "084007": 87, "084247": 72, "084269": 93, "084401": 69, "084461": 69, "08455772": 139, "08462303e": 139, "084863": 81, "0853505": 63, "085465": 81, "085566": 83, "085584": 70, "085592": 88, "085697": 76, "085730": 77, "085962": 69, "086004": 88, "0862": 168, "086264": 83, "08629404e": 139, "086401570476133": 83, "086402": 83, "086679": [112, 118], "086871": 85, "086950": 70, "087184": 38, "087373": [124, 127], "08754027": 139, "087566": 90, "08762143": 70, "087634": 81, "087677": 72, "087897": 71, "0879": 62, "087924": 81, "087947": 96, "088034": 86, "088048": 96, "088082": 90, "088114": 69, "088288": [105, 109], "088357": 96, "08848": [112, 123], "088482": 29, "088604": 99, "08860402": 99, "088792": 90, "088836": 90, "088868": 81, "08888889": 65, "0889": [62, 77], "088928": 69, "089229": 77, "089277": 69, "089461": 124, "089647": 76, "08968939": 63, "089784": 71, "089854": 124, "08987313": 70, "08e": 64, "09": [38, 81, 82, 83, 92, 93, 96], "09000000000000001": [112, 118], "09015": 62, "090255": 96, "090363": 95, "090433": 69, "090598": 157, "090728": 89, "091072": 70, "091262": 69, "091308": 111, "091391": 156, "091465": 124, "09174584": [124, 127], "091952": 69, "091992": 95, "092162": [124, 129], "092170": 71, "092171": 69, "09218894": [124, 127], "0922": 62, "092229": 100, "092365": 156, "092453": 96, "09245337": 96, "09255324e": 139, "092590": 81, "092598": 158, "0926": 124, "092646": 96, "092872": 69, "0929369228758206": 90, "092959": 69, "092989405": 139, "093043": 96, "093056": 69, "09310496": 156, "093153": 96, "0932": 62, "093512": 72, "093548": 69, "093740": 156, "093950": 91, "094026": 91, "094118": 96, "094381": 91, "094383": 99, "094420": 93, "09444444": 65, "094643": 81, "094755": 69, "094766": 95, "094829": 124, "09495145": 124, "094999": 96, "095074": 94, "095087": 72, "095104": 78, "095492": 70, "095785": 78, "096": 173, "09603": 168, "096337": 91, "096339": [124, 127], "096418": 78, "096616": 24, "096890e": 111, "096915": 100, "097": 97, "097004": 69, "097009": 88, "097157": 100, "097208": 95, "097468": 83, "097509": 85, "09756": 98, "097721": 94, "09779675": 156, "097796750": 156, "097918": 82, "098": [60, 64], "09801463544687879": 76, "098256": 96, "098319": 96, "09838932": 139, "09853586": 124, "098712": 96, "098719": 81, "098728": 72, "098901": 88, "099": 97, "09914088": 70, "099169": 70, "09957868943595276": 76, "099647": 95, "099670": 93, "099731": [81, 82], "09980311": 156, "09988": 171, "099933": 86, "0_": 42, "0ff823b17d45": 65, "0x": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "0x1747bdd4520": 73, "0x1747bdd6b90": 73, "0x7f028a1bb4a0": 76, "0x7f029668ba10": 76, "0x7f4980e55070": 95, "0x7fcbfa0629f0": 173, "0x7fcc0923ab40": 114, "0x7fcc0923eb10": 124, "0x7fcc0923fb90": 113, "0x7fcc0943f440": 163, "0x7fcc096202f0": 117, "0x7fcc09622b10": 112, "0x7fcc09657050": 156, "0x7fcc096ccf50": 114, "0x7fcc0984a2d0": 157, "0x7fcc09c00f20": 156, "0x7fcc09c03440": 156, "0x7fcc09cefc20": 124, "0x7fcc0a4a1070": 156, "0x7fcc0a782ed0": 112, "0x7fcc0a804950": 112, "0x7fcc0a81e750": 124, "0x7fcc0a829640": 112, "0x7fcc0abc5820": 124, "0x7fcc0ac83f80": 125, "0x7fcc0af2fe30": 124, "0x7fcc0af38860": 124, "0x7fcc0b273620": 125, "0x7feb36129610": 100, "1": [8, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 53, 54, 55, 56, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172], "10": [10, 11, 12, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 35, 36, 37, 38, 39, 40, 41, 43, 44, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 106, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 168, 170, 171, 172, 173], "100": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 43, 45, 63, 65, 66, 67, 75, 76, 81, 82, 84, 87, 88, 91, 94, 98, 100, 101, 102, 104, 109, 111, 112, 114, 117, 120, 122, 123, 124, 126, 139, 140, 156, 157, 163, 170, 172], "1000": [17, 19, 25, 27, 61, 67, 68, 75, 76, 77, 79, 80, 85, 86, 87, 89, 90, 92, 93, 97, 99, 100, 103, 124], "10000": [60, 68, 72, 81, 82, 92, 96], "100000e": 93, "100084": 81, "10018": 72, "100225e": 82, "100356": 83, "10038": 99, "10039862": [101, 124], "100477": 72, "100510": 156, "1006074": 139, "10065": 72, "10074": 93, "100779": 70, "10079785": 124, "1008": 70, "100807": [81, 82], "100848": 72, "10089588": 96, "100896": 96, "100923": 96, "100_000": 94, "101": [18, 31, 40, 62, 97, 111, 171, 172], "101092": 111, "10126": 93, "101272": 82, "10127930": 156, "101279300": 156, "101332": 111, "101417e": 139, "1015": [64, 92], "10152": 72, "1016": [17, 18, 19, 31, 40, 62], "1016010": 64, "101616": [124, 127], "1016338581630878": 90, "10169881": 70, "1018": 93, "101955": 139, "101998": 78, "102": [104, 109, 111, 170, 172], "102172": 70, "102282": 16, "10235": 93, "102499": 82, "102616": 83, "1027": 139, "10277": 93, "102775": 83, "102863": 76, "10299": 92, "103": [81, 91, 97, 104, 109, 111, 120, 172], "10307": 156, "1030891095588866": 90, "1031": 93, "103179163001313": 90, "103215": 16, "10348": 92, "103497": 96, "103606e": 82, "103806": 83, "103931": 82, "10396": 92, "104": [64, 92, 111, 172], "1040": 93, "10406": 93, "104097": 72, "1041": 62, "10414": 93, "104162": 69, "10449": 98, "104492": 88, "1045303": 63, "104562": 69, "104683": 72, "104787": 91, "104913": [124, 127], "104917": [124, 127], "105": [42, 63, 88, 91, 111, 172], "105086": 70, "105103": 81, "105206": 139, "105318": 96, "105384": 82, "1054": 65, "1055": [62, 98], "105548": 111, "10557165": 70, "1056": 62, "10585": 69, "1059136": 139, "106": [65, 111, 172], "10607": [73, 104, 109, 170], "106116": 93, "10627": 93, "1063": 77, "106385": 156, "1065": [75, 84, 90], "106595": [12, 124, 126], "106616": 72, "106635": 69, "106715": 87, "10673335": 70, "106872": 69, "106975": 71, "107": [65, 70, 100, 111, 172], "107073": 83, "107133": 72, "107156": 88, "107290": 156, "1073": 93, "107357": 72, "1074246": 139, "107467": [105, 109], "10747": [73, 104, 109, 170], "107490": 81, "107697": 69, "107746": 96, "10791": 93, "107935": 96, "108": [111, 168, 171, 172], "1080": [36, 43, 62, 63, 91], "10824": [73, 104, 109, 170], "108257e": 93, "108259": 78, "10829": 93, "1082972": 139, "10831": [73, 104, 109, 170], "108310": 69, "108603": [157, 163], "108742": 16, "108783": 83, "108783087402629": 83, "10878571": 96, "108786": 96, "108813": 69, "109": [76, 81, 111], "10903": 92, "109069": 156, "109079e": 96, "1091216": 139, "10922588": 139, "109273": 91, "109277": 88, "10928": 93, "1093": [41, 84], "109430": 70, "109454": 93, "1095": 77, "1096": 98, "10967": 92, "109861": 170, "1099472942084532": 79, "109955": 69, "10e": [83, 96], "11": [39, 41, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 91, 92, 93, 94, 96, 97, 99, 100, 101, 102, 104, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 124, 125, 126, 127, 139, 140, 156, 157, 163, 170, 172, 173], "110": [76, 94, 111, 172], "110077": 139, "110081": 88, "1101": 93, "11019365749799062": 100, "110194": 100, "110359": 91, "110365": 100, "110557": 90, "1106": 62, "110681": 99, "11071087": [101, 124], "110717": 156, "110742": 90, "110756": 82, "110831": 72, "1109": 93, "110e": 124, "111": [82, 111, 172], "1111": [10, 11, 44, 61, 63, 80, 84, 91, 100, 103, 124, 157, 163, 168], "111167": 85, "11120": 93, "111249": 72, "11131819333637394550535665688385868899100": 139, "111352344760325": 90, "111526": 76, "111577": 93, "1117": [75, 84, 90], "111783": 93, "1118": 64, "111908": 72, "112": [65, 111, 172], "1120": 92, "11203426": 70, "112078": 111, "112135": 83, "1121351274811793": 83, "11218701": 70, "1122": [62, 93], "112216": 83, "112244": [124, 127], "112397": 69, "112415": 76, "112536": 71, "1129812": 139, "113": [10, 111, 172], "113005": 93, "113022": 88, "11311": 92, "113149": 88, "113207": 96, "113270": 83, "11376": [93, 173], "113780": 91, "113858": 72, "113943": 69, "113952": 88, "11399": 92, "114": [111, 172], "114017": 139, "114026": 90, "11423": 173, "1144": 93, "1144500": 63, "11447": 99, "114530": 85, "1145370": 63, "114570": 82, "11458": 93, "114647": 83, "114719": 71, "1148": 139, "114834": 93, "114863": 69, "114989": 78, "115": [111, 172], "11500": 92, "115060e": 96, "1151": 93, "115297e": [92, 93], "11530": 93, "11542": 173, "115605": 69, "11570": 92, "115754": 82, "115785": 16, "11588": 93, "1158875": 139, "115888": 82, "115901": 78, "116": [111, 172], "116103e": 81, "116274": 83, "116304": 70, "11633": 173, "11638738": 99, "1166": [92, 171], "116664": 69, "1167": 92, "11673": 93, "11676": 93, "1169": 70, "11691319": [102, 140], "117": [81, 111], "117194": 95, "11720": 93, "117242": 96, "11724226": 96, "117313": 81, "117366": 96, "11750": 93, "117710": 83, "11789998": 96, "117900": 96, "11792": 64, "11796": 93, "117970e": 81, "118": 111, "11812623": 139, "118150": 70, "1182": 64, "11820": 93, "11823404": 100, "118255": 96, "11850": 93, "1185482": 139, "118596": 83, "1186": 64, "118601": 91, "11861": 64, "1187": 99, "118708e": 91, "1187339840850312": 91, "118799": 93, "1188": 62, "11888309": [124, 127], "118952": 91, "119": [100, 111, 172], "11935": 99, "119409": 90, "119468": 72, "119763e": 82, "119766": 96, "1198": [63, 91], "119808": 72, "11992667e": 139, "11e": 139, "12": [4, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 55, 56, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 99, 100, 102, 104, 109, 111, 112, 113, 114, 116, 117, 118, 121, 123, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 168, 170, 171, 172, 173], "120": [66, 67, 90, 101, 111, 172], "12002": 92, "120098e": 81, "1200x600": [69, 70, 72], "1200x800": [69, 70, 72], "1202": 171, "120347e": 139, "120567": [85, 86], "120721": 91, "12097": [10, 11, 44, 63, 84, 91, 93, 103, 168], "121": [93, 111, 172], "1210": 93, "12105472": 156, "121054720": 156, "1211": 93, "121120": 71, "121297": 93, "1213405": 63, "1214": 156, "121584e": 96, "121774": 87, "121801": 82, "121878": 69, "122": [18, 31, 40, 62, 69, 97, 104, 109, 111, 171, 172], "12214": 64, "122408": 83, "122421": 88, "122749": 69, "122777": 156, "123": [46, 64, 65, 69, 72, 92, 100, 111, 172, 173], "1230": 93, "123192": 100, "1232": 98, "12323": 93, "1234": [60, 61, 62, 73, 79, 80, 97, 98, 103, 112, 113, 116, 118, 122, 123, 139, 156], "12348": 100, "123501e": 93, "123562": 99, "123911": 72, "123940": 82, "124": 111, "1240": 98, "12410": 93, "124700": 69, "1247297280098136": 76, "124805": 92, "125": [111, 172], "12500": 92, "125059": 156, "12539340": 156, "125649": 111, "12572": 93, "125738": 72, "1258": [63, 93], "126": [111, 172], "126023": 76, "12612": 93, "12616": 93, "126229": 72, "126494": 16, "12661776": 70, "126771": 156, "126802": 93, "12685966": [140, 156], "12689": 93, "127": [31, 60, 111, 172], "12707800": 63, "1273": 62, "127337": 88, "127379": 69, "1275": 93, "12752825": 156, "127563": 99, "1277": 94, "127984": 69, "128": [64, 111, 172], "12802": 64, "128061": 76, "12814": 93, "128158": 81, "128229": 88, "128312": 96, "128393": 93, "128408": 91, "128412": [105, 109], "128443": 69, "12861": 93, "12869788": 70, "128720": 71, "128758": 71, "128762": 69, "128902e": 81, "129": [91, 111, 172], "129071e": 124, "12908637942868845": 112, "12945": 171, "1295": [62, 93], "12955": 92, "129674": 72, "1297": 93, "129881": 76, "13": [12, 14, 15, 17, 18, 19, 22, 24, 25, 26, 27, 28, 30, 33, 35, 37, 38, 39, 40, 41, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 76, 77, 78, 81, 82, 83, 85, 86, 87, 88, 90, 91, 92, 93, 96, 97, 98, 99, 100, 102, 104, 109, 111, 112, 113, 114, 116, 117, 118, 121, 124, 126, 127, 139, 140, 156, 157, 163, 170, 173], "130": [65, 91, 111, 172], "13018227582500236": 117, "1302728": 124, "130370": 83, "130481": 69, "1306": 99, "130686": 111, "13069934": 139, "130829": 96, "13084": 124, "13091": 93, "131": [111, 172], "13101273": 124, "131034": 81, "131051": 85, "131071": 76, "13115662": 69, "13119": 99, "1312": 62, "1313": 64, "131483": 88, "131535": 70, "13154134": 70, "1316": 70, "131842": 88, "131914": 82, "131928": [105, 109], "132": [65, 81, 91, 111, 172], "1321": 92, "13229": 173, "132354": 70, "132366": 72, "1324": [64, 92], "132454": 68, "132481": 111, "1325": 64, "13257": 92, "1326": 173, "132671": 83, "132774": 69, "1328": 99, "132916": 69, "132941": 93, "132998": 71, "133": [65, 104, 109, 111, 171, 172], "133115": 69, "133202": 93, "133365e": 82, "133421": 93, "1335": 173, "13357": 93, "133596": 96, "133832": 70, "133839": 88, "13386353": [124, 127], "13398": 100, "133f5a": 94, "134": [91, 101, 111, 139, 172], "134037": 71, "1340371": 62, "1341": 64, "134211": 96, "1343": [92, 93], "134412": 69, "134510": 82, "134567": 93, "1346035": 64, "134687": 93, "13474": 93, "134765": 93, "1348": 92, "13489679": 70, "1349": 62, "13490": 93, "135": [65, 111, 124, 172], "13505272": 63, "1351": 139, "135135": 72, "135234": 124, "135375": 78, "135378": 156, "135567": 81, "135596": 93, "135707": [112, 118], "135742": 81, "135856": 96, "13585644": 96, "135871": 91, "1359": 93, "136": [73, 91, 100, 111, 172], "1360": [64, 77], "13602": 100, "136089": 91, "13642": 93, "136442": 91, "1366": 94, "136836": 91, "136885": 93, "137": [31, 65, 73, 111, 172], "1371": 93, "137165": 124, "1372": 70, "137230": 90, "137234": 70, "137396": 96, "137436": 69, "137601": 69, "1378": 93, "137999": 124, "138": [111, 172], "1380": 92, "13809": 93, "138215": 72, "138229": 71, "138249": 72, "138264": 100, "138346": 69, "138378": 83, "1384": 92, "138556": [124, 129], "138595": 82, "138641": 85, "13868238": 156, "138682380": 156, "138698": 156, "138730": 81, "138828": 69, "1389": 62, "13893": 93, "139": [100, 111, 170], "1391": 62, "139279": 70, "13933403": 70, "139403": 69, "139491": 156, "139508": 88, "13951499": 139, "139548": 71, "13956": 99, "139622": 93, "139757": [124, 127], "1398": 93, "139811": 76, "13984011": 70, "13993416": [124, 127], "14": [61, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 76, 77, 78, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 96, 97, 99, 100, 102, 104, 109, 111, 112, 113, 114, 116, 117, 118, 120, 124, 126, 127, 139, 140, 156, 157, 163, 170, 171, 173], "140": [66, 67, 88, 93, 101, 111, 172], "1400": 93, "14001451": 70, "1401": 62, "140234": 70, "140299": 72, "1404": 70, "14040138e": 139, "140619": [124, 129], "140770": [81, 82], "140833": 83, "140861": 63, "141": [93, 111, 172], "141101e": 93, "14112647": 70, "14114": 99, "141232": 69, "141247": 78, "141371": 69, "14141": 93, "141426": 82, "141546": 156, "141646": 82, "141682": 69, "141784": 124, "141820": 83, "141927": 69, "142": [111, 172], "14200098": 156, "142045e": 91, "142063": 81, "142270": 68, "1424": [112, 122, 123], "142444": 82, "142482": 96, "142542": 81, "14268": 124, "14281403493938022": [112, 118], "14289": 93, "143": [104, 109, 111, 172], "1435": 93, "143541": 81, "14368145": 156, "143943": 76, "144": [72, 111, 172], "14400": 92, "14405": 93, "14406": 93, "144081": [124, 127], "144084": 83, "1443": 93, "14444229e": 139, "144669": 96, "14477189e": 139, "144800": 83, "144813": 78, "144846": 72, "144908": 95, "144971": 92, "145": [111, 172], "145245": 96, "14530": 93, "14532650": 156, "145625": 96, "145737": 156, "14588": 93, "146": [111, 172], "146037": 96, "146069": 70, "146087": 170, "14616": 72, "146214": [105, 109], "14625": 93, "1465": 64, "146530": 72, "146641": 156, "1468115": 63, "146861": 111, "147": [111, 172], "14702": 73, "147094": 69, "147121": 96, "147378": 81, "14744": 93, "147464": 111, "147575": 82, "14772": 93, "1479": 93, "14790924": 156, "147909240": 156, "147927": 73, "14795": 93, "148": [111, 172], "148005": 78, "14803": 93, "148125": 69, "148134": [81, 82], "148161": 96, "148177": 82, "148210": 83, "1482102407485826": 83, "14845": 73, "148455": 83, "1484554868601506": 83, "1485": 93, "148750e": [92, 93], "148835": 93, "149": [111, 172], "1492": 60, "149228": 100, "149285": 96, "14935": 72, "149375": 82, "149446427": 81, "149472": 100, "149714": 91, "149858": 28, "149898": 96, "15": [11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 61, 63, 64, 65, 67, 69, 72, 75, 76, 77, 78, 81, 82, 83, 87, 88, 89, 90, 91, 92, 93, 96, 97, 98, 99, 100, 104, 109, 111, 112, 113, 114, 116, 117, 118, 124, 126, 127, 139, 140, 156, 157, 163, 170, 173], "150": [42, 65, 100, 111, 172], "15000": [64, 92], "150000": 64, "15000000000000002": [83, 93, 96, 112, 118], "150165e": 81, "1502": 63, "150200": 91, "150234": 93, "150370": 69, "150372e": 111, "150408": 63, "150435": 93, "150614": 73, "1507": 62, "150719e": 92, "151": [111, 172], "15101587": 70, "15108": 93, "15122148": 70, "151447": 90, "15158817": 70, "151636": 83, "151646": 69, "151819": 96, "15190249": 70, "15194": 92, "152": [72, 94, 111, 172], "15211693": 70, "152148": [81, 82], "152149": 72, "15229": 72, "152456": 69, "152552": 82, "152615": 82, "152678": 71, "152706": 124, "152742": 99, "15287": 93, "152891": 81, "152926": 68, "153": [72, 100, 111, 120, 172], "153034": [124, 129], "153119": 83, "153293": 93, "15339": 93, "153398": 93, "15354": 99, "153587": 91, "153633": 73, "153728": 82, "15375": 72, "153983": 83, "1539831483813536": 83, "154": [77, 111], "15417171": 70, "154415": 156, "1545": 93, "154557": 96, "154707": 78, "15475": 173, "154752": 156, "154828": 83, "155": [77, 111, 172], "155000": 92, "155025": 96, "155120": 96, "155160": 78, "1554": 93, "155423": 78, "15549": 93, "15556": 93, "1557093": 63, "156": [111, 172], "156021": 96, "156159": 72, "1562": 62, "156202": [81, 82], "156205": 76, "156317": [81, 82], "156328": 90, "1564": 156, "156496": 82, "156540": 156, "156704": 93, "1569": 93, "156969": 83, "157": [111, 172], "157080": 156, "157470": 95, "157498": 69, "1576": 93, "157619": 72, "1577657": 63, "158": [60, 82, 111, 172], "158007": 96, "158115": 82, "15815035": 64, "158178": 83, "158198": 93, "1582": 93, "158288": 93, "15832529": 70, "1586": [62, 93], "158682": 90, "158697": 156, "158726": 111, "1589": 93, "159": 172, "159124": [112, 116], "15915": 71, "15916": [62, 71], "159466": 96, "15946647": 96, "159509": 94, "15954621": 139, "159573e": 82, "1596": 65, "159826": 93, "159904": 70, "159959": 93, "16": [24, 60, 61, 63, 64, 65, 66, 67, 69, 71, 72, 76, 77, 78, 81, 82, 83, 86, 87, 88, 91, 92, 93, 96, 97, 99, 100, 104, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 126, 127, 139, 140, 156, 157, 163, 170, 173], "160": [66, 67, 94, 101, 172], "1604": 64, "160654": 69, "160675": 82, "16068348": 70, "160762": 85, "16082404": 70, "160836": 88, "160932": 83, "160973": 69, "161": [65, 77, 171, 172], "16107377": 70, "161141": 91, "16118746": [124, 127], "161236": 96, "161243": 96, "161322": 81, "161441": 67, "1619": 64, "16199455": 70, "162": [77, 172], "16201": 93, "162072e": 82, "16211": 92, "162153": 96, "1622": 93, "16241": 93, "162436": 100, "16249": 93, "1626685": 63, "162683": 100, "162710": 83, "162752": 90, "1628": [70, 92], "162909": 91, "162930": 93, "163": 172, "163013": 16, "16315429": 70, "163194": 96, "163564e": 81, "163566": 93, "1636": 69, "163797": 69, "163895": 83, "163936": 71, "163992e": 81, "164": [78, 93, 97, 124, 172], "164034": 156, "164166": 81, "164222": 69, "16430437": 70, "16438436": 139, "164467": 92, "164550": 69, "164608": 96, "164698": 87, "164801": 96, "164805": 83, "164864": 91, "164928": 69, "164941": 16, "164943": 90, "165": [77, 100, 172], "16500": 92, "16504132": 70, "165178": 96, "1653": 93, "16536299": 156, "165362990": 156, "165419": 96, "16553": 92, "165549": 170, "165707": 78, "165822": 81, "16590": 93, "166": [77, 172], "166088": 77, "1661": 92, "16618": 93, "166322": [105, 109], "166517": 88, "166700": 72, "166904": 93, "167": [64, 92, 172], "167035": 93, "167224": 72, "16747866": [124, 127], "167547": 96, "1676": 93, "167650": 72, "1676705013704926": 76, "167930": 93, "167958": 70, "167981": 156, "168": 172, "1680": 70, "16803512": 156, "168092": 156, "1681": 92, "168614": 96, "1687": 62, "168792": 111, "168931": 96, "169": [65, 172], "1690": 62, "1691": 93, "16910": 93, "169117": 100, "169196": 96, "169220e": 83, "169312": 69, "16951": 93, "169677": 89, "169712": 82, "16984": 93, "169858": 90, "17": [61, 63, 64, 65, 67, 69, 72, 76, 77, 78, 81, 82, 87, 88, 90, 91, 92, 93, 96, 97, 99, 100, 104, 109, 111, 112, 114, 116, 117, 118, 123, 124, 126, 127, 139, 140, 156, 157, 163, 170, 173], "170": 172, "1705": 93, "170585": 69, "170705": 111, "17083": 93, "170918": 71, "171": [76, 77, 172], "171140": 70, "17116448": 70, "1712": 171, "171255": 111, "1714": 64, "171575": 96, "17164011": 70, "171815": [112, 118], "1719": [69, 70, 71, 72], "172": [97, 172], "172022": 156, "172083": 82, "172165": 82, "17219029": 70, "17228016": 70, "172400": 72, "172659": 82, "172672": 37, "172793": 96, "173": [77, 172], "173070": 69, "17311164": 70, "173191": 70, "173267": 86, "1736": 62, "17372": 93, "173829e": 82, "17385088": 70, "173941": 69, "173964": 156, "173e": 97, "174": 172, "174043": 69, "174177": 96, "174185": 96, "174499": 156, "174516e": 96, "17453": 93, "1746": 93, "17469": 93, "174968": 92, "174e": 124, "175": [72, 77, 172], "17500": 93, "1751": 92, "175254": 92, "175284": 83, "17536": 93, "175606": 82, "175635027": 63, "17576": 93, "175894": 100, "17600288": [112, 113], "176484": 72, "176495": 96, "17655394": 96, "176554": 96, "176855": 71, "176929": 156, "177": [72, 171, 172], "177007": 96, "17700723": 96, "177015": 76, "177043": [81, 82], "177138": 69, "177145": 72, "1773": 93, "177397": 93, "177496": [69, 96], "177611": 96, "177751": 96, "17781": 72, "177995": 96, "178": [93, 172], "17800": 93, "17807": 93, "178169": 85, "17823": 65, "178326": 72, "178661": 156, "178751": 77, "178763": 96, "178934": 156, "179": [89, 93, 105, 109, 172], "179101": 111, "179137": 82, "179355": 72, "179548": [105, 109], "1795850": 63, "179588e": 96, "179775": 69, "179842": 69, "1798913180930109556": 94, "18": [61, 63, 64, 65, 67, 69, 72, 73, 75, 76, 77, 81, 82, 85, 87, 88, 90, 91, 92, 93, 96, 97, 99, 100, 104, 109, 111, 112, 114, 117, 118, 124, 126, 127, 139, 156, 170, 173], "180": [66, 67, 76, 94, 101, 172], "180014": 82, "180023": 82, "180271": 88, "1803": 62, "18030": 93, "180453": 70, "180496": 82, "180575": [85, 86], "1807": [62, 93], "180890": 71, "1809": 171, "180951": 96, "181": 172, "1812": 93, "18129362": 70, "181330": 78, "1814": 62, "18141": 93, "181432": 156, "181939": [111, 112, 114, 124], "182": [72, 172], "182058": 72, "182151": 111, "182157e": 72, "18229279": 70, "182399": 16, "18251761e": 139, "182633": 96, "182849": 96, "183": [65, 77, 124, 172], "1832": 93, "183308": 72, "183373": 124, "18338952": 124, "183526": 83, "183553": 97, "183654": 82, "18368": 93, "183814": 93, "18383644e": 139, "183855": [112, 118], "183879": 81, "183888": 91, "183988": 117, "184": [65, 69, 171, 172], "184224": 78, "184376": 112, "184497": 82, "184728": 69, "184806": 77, "184818": 72, "185": [64, 65, 72, 76], "185098": 81, "185130": 97, "18519134e": 139, "185207e": 94, "185496": 82, "185845": 111, "186": [72, 172], "18604": 93, "186128": 81, "18622": 93, "18637": 168, "186507": 71, "186589": 78, "18666": 93, "186689": 124, "186735": 96, "186836": 96, "186864": 90, "186868": 67, "187": [69, 172], "187148": 156, "187690": 96, "18773": 93, "187895": [124, 129], "1879": 77, "188": [72, 172], "188175": 96, "1881752": 96, "188223": 96, "1882668518": 139, "188465": 82, "188625": 72, "188643": [111, 112, 114, 124], "188760": 88, "1888": 93, "189": [65, 93, 97, 172, 173], "189023": 156, "189069": 81, "18911423": 173, "189195": 93, "189220": 72, "1895815": [43, 63, 91], "189662": 69, "189737": 96, "189739": 78, "18976": 93, "189893": 71, "18990286": 99, "189903": 99, "189998": 96, "19": [15, 61, 63, 64, 65, 67, 69, 72, 76, 77, 81, 82, 88, 90, 91, 92, 93, 96, 97, 99, 100, 104, 109, 111, 112, 117, 118, 124, 126, 127, 139, 156, 170, 173], "190": [65, 172], "190000e": 93, "190090": 156, "190231": 93, "19031969": 96, "190320": 96, "19033538": 63, "190809": 96, "190892": 100, "1909": [43, 63, 91], "190915": 83, "190921": 86, "190976": 97, "190982": 96, "191": [65, 72, 171, 172], "191038": 72, "1912": 171, "1912705": 103, "191451": 69, "191534": 92, "191578": 25, "191653": 82, "191670": 81, "191718": 71, "19184386e": 139, "192": [69, 72, 76, 172], "192082": 72, "192234": 156, "1923": 93, "192418": 81, "19242831324042475161626975798081919298": 139, "192428313240424751616269757980819192982368212223252755575864677073777889961113181933363739455053566568838586889910045141516172026303435384344486366749395": 139, "192428313240424751616269757980819192987101229414649525459607172768284879094971113181933363739455053566568838586889910045141516172026303435384344486366749395": 139, "1924283132404247516162697579808191929871012294146495254596071727682848790949723682122232527555758646770737778899611131819333637394550535665688385868899100": 139, "1924283132404247516162697579808191929871012294146495254596071727682848790949723682122232527555758646770737778899645141516172026303435384344486366749395": 139, "192539": 29, "19290": 72, "192952": 78, "193": [69, 172], "193060": 96, "19314": 69, "193308": 29, "193314": 81, "193375": 88, "19360754e": 139, "193644": 93, "193802": 82, "19382": 93, "193833": 93, "193849": 97, "19385": 93, "193938": 82, "193f0d909729": 65, "194": [72, 75, 172], "1941": 64, "19413": [92, 93], "194172": 81, "1946": 62, "194696": 69, "194723": 69, "194945": 111, "195": [72, 76, 106, 109, 172], "195004": 72, "195044": 72, "195057": 81, "19508": 100, "19508031003642456": 100, "195377": 96, "195396": 96, "19550": 93, "195564": 91, "195573": 99, "19559": [64, 92], "195618": 69, "195621e": 139, "195894": 81, "1959": 171, "195920": 77, "195963": 88, "196": [72, 76, 172], "196189": 96, "196374": 69, "196469": 70, "196655": 88, "19680840": 156, "196824": 96, "196835": 90, "19685": 72, "197": [69, 72, 76, 105, 109, 172], "1970": 93, "19705": 93, "197199": 69, "197225": [73, 104, 109, 170], "1972250000001000100001": [65, 104, 109, 170], "197329": 95, "197424": [112, 118], "197484": 156, "1975": 93, "19756": 93, "19758": 93, "197600": 68, "197722": 69, "19793": 93, "197943": 72, "19798": 93, "198": [69, 72, 76, 172], "198085": 69, "198221": 93, "19824": 93, "198493": 88, "198503": 97, "198549": 73, "198556e": 82, "198612": 72, "198687": 64, "1988": [61, 80, 103, 124], "199": [76, 92, 172], "1990": [64, 92, 93], "199014": 70, "1991": [64, 92, 93, 173], "199281e": 96, "199427": 81, "199445": 156, "1995": [63, 91], "19952361": 124, "199575": 69, "1998": 94, "19983954": 101, "1999": 94, "1_": [83, 96], "1d": 13, "1e": [12, 15, 55, 56, 76, 88, 93], "1f77b4": 68, "1m": [112, 120, 121], "1mnon": [112, 120], "1x_4x_3": 68, "2": [10, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 45, 46, 47, 49, 52, 54, 55, 56, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 133, 139, 140, 156, 157, 158, 160, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172], "20": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 37, 38, 39, 40, 41, 43, 44, 45, 61, 63, 64, 65, 66, 67, 68, 69, 72, 75, 76, 77, 81, 82, 83, 87, 88, 89, 90, 91, 92, 93, 96, 99, 100, 101, 102, 103, 104, 109, 111, 112, 117, 118, 123, 124, 126, 127, 139, 140, 156, 157, 163, 170, 173], "200": [17, 19, 32, 35, 42, 62, 66, 67, 69, 72, 75, 76, 83, 84, 95, 96, 101, 103, 108, 109, 112, 117, 118, 122, 123, 171, 172, 173], "2000": [11, 30, 64, 66, 78, 81, 82, 83, 88, 92, 93, 96, 98, 101, 111, 124], "20000": [64, 92], "20000000000000004": [83, 93, 96], "200000e": 93, "200076": 81, "2001": 70, "20010": 93, "200110": 93, "2002": 62, "200225": 77, "2003": [10, 70, 171], "200303": 170, "2004": [70, 124, 128], "2005": [67, 70], "20055": 93, "2006": [70, 93, 124, 128], "2007": [70, 124, 128], "20074": 93, "200834": [124, 125], "200907": 85, "200935": 82, "20094485": 70, "201": [65, 172], "2010": [63, 91], "2011": [63, 91, 168, 170], "2013": [84, 156, 171], "201302": 82, "2014": [156, 171], "2015": [42, 171], "201528": [81, 82], "20158": 93, "201599": 72, "2016": 94, "20166799": 70, "20167273": 75, "2017": [34, 171], "20172972": 70, "201768": 91, "2017904": 70, "2018": [10, 11, 44, 45, 61, 63, 64, 67, 75, 80, 84, 91, 92, 93, 98, 99, 103, 124, 130, 139, 140, 148, 156, 168, 171, 172], "2019": [32, 65, 81, 82, 83, 85, 86, 93, 96, 99, 112, 122, 140, 146, 149, 150, 168, 170, 171], "201939": 86, "201957e": 139, "201e": 97, "202": [69, 72, 172], "2020": [12, 14, 15, 17, 18, 19, 31, 33, 35, 40, 62, 65, 67, 69, 72, 89, 100, 112, 122, 124, 126, 130, 157, 158, 171], "2020435": 63, "2021": [17, 19, 41, 43, 62, 63, 65, 69, 70, 71, 72, 81, 82, 91, 124, 125, 128, 130, 140, 151, 171, 172], "20219609": 63, "2022": [99, 100, 124, 126, 130, 157, 158, 167, 168, 171], "2023": [36, 66, 98, 101, 124, 138, 140, 154, 155, 171], "2024": [60, 75, 79, 84, 90, 94, 97, 98, 100, 124, 168, 171], "2025": [16, 17, 19, 69, 72, 76, 98, 124, 125, 127, 129], "202650e": 83, "202660": 69, "20269": 93, "2029554862": 68, "203": [64, 72, 81, 92, 172], "203082": 111, "2032": 77, "203284": 83, "20329": 93, "20333353": 72, "203828": 93, "204": [69, 172], "204007": 96, "20400735": 96, "204362": 100, "204482": 96, "204794": 96, "20479494": [124, 127], "204893": 78, "20497662e": 139, "205": [97, 99, 172], "205187": 83, "205224": 99, "205245": 93, "205333e": 92, "20592555": 70, "205938": 91, "206": [76, 77, 172], "20620632": 70, "206253": [92, 93], "206256": 88, "206752": 77, "206802": 26, "207": [69, 76, 97, 172], "207236": 70, "207347": 72, "2075": 62, "207815": 69, "20783816": 63, "207885": 92, "207899": 72, "207912": 156, "208": [69, 72, 76, 77, 78, 172], "2080787": 63, "208080": 70, "2081": 70, "20823898": 63, "208300": 96, "208342": 69, "208403486": 139, "208408e": 82, "208480": 82, "208485": 77, "2086": 93, "208792": 69, "209": [69, 72, 76, 77, 78, 94], "209380": 70, "209388": 72, "209448e": 111, "20956": 93, "209894": 96, "21": [10, 11, 44, 61, 63, 64, 65, 67, 69, 72, 76, 77, 81, 82, 84, 90, 91, 92, 93, 96, 98, 99, 100, 103, 104, 109, 111, 112, 117, 118, 122, 123, 124, 126, 139, 156, 168, 170, 171, 173], "210": [17, 18, 19, 35, 40, 69, 72, 76, 77, 78, 93], "2103": 168, "2103034": 63, "210319": [81, 82], "210323": 96, "2104": 172, "210554": 72, "2107": 171, "210789": 81, "21082": 93, "211": [69, 72, 76, 77, 78, 97, 172], "21105": [65, 112, 122, 168, 170], "211078": 72, "2112": 100, "2114026": 96, "211403": 96, "21142": 93, "211534": 83, "211921": 72, "212": [69, 72, 78, 172], "2121": 93, "212317": 96, "212466": 69, "212491": 96, "212811": 78, "212844": 91, "213": [69, 72, 76, 77, 78, 97, 171, 172], "213199": 124, "21342": 93, "21349216": 139, "21351": 93, "213555": 82, "2138": 77, "213876": 72, "2139": 33, "214": [69, 72, 76, 77, 94, 172], "214403": 69, "2147": 62, "214764": 99, "214769": 88, "214850": 69, "215": [69, 76, 78, 172], "215094": 69, "215159": 93, "215342": 96, "2155": 93, "21550": 93, "215511": 72, "21562": 93, "215958": 156, "216": [68, 72, 77, 78, 172], "216113": 96, "216147e": 81, "216207": [112, 118], "21624417": 63, "2163": 93, "216344": 96, "216604": 72, "2167": 93, "21673923e": 139, "216943": 111, "217": [69, 72, 76, 77, 78, 97, 171, 172], "21716": 93, "2171802": [63, 91], "217244": 27, "217412e": 82, "21744247": 139, "2175": 93, "217684": 88, "218": [69, 72, 76, 77, 78, 172], "21804": [64, 92], "218223": 90, "218383": 78, "218546": 96, "2189": 93, "218924": 93, "219": [18, 31, 40, 62, 69, 76, 77, 78, 105, 109, 171, 172], "2191274": 63, "219187": [124, 127], "219585": 78, "22": [60, 61, 63, 64, 65, 67, 69, 72, 76, 77, 81, 82, 90, 91, 92, 96, 97, 99, 100, 111, 112, 117, 118, 124, 126, 139, 156, 170, 173], "220": [69, 72, 76, 77, 78, 172], "220211": 93, "220446e": 72, "220587": 16, "22063493e": 139, "220721": 69, "220767": 72, "220772": 96, "220922": 81, "221": [77, 78, 172], "22103912e": 139, "221111": 81, "221250": 71, "2213": 91, "2214": 91, "221418": 82, "2215": [62, 91], "2216": 91, "221619e": 82, "221643": 82, "2217": [63, 91], "2218": 171, "221845": 86, "221888": 71, "222": [69, 72, 77, 172], "222168": 82, "2222": [61, 63, 80, 124], "222306": 90, "222749": 111, "222843": 96, "223": [69, 76, 94, 97, 172], "223095": 67, "22336235": 63, "223485956098174": [85, 86], "223625": 16, "22375856": 63, "223776": 69, "22390": 92, "2239684": 139, "224": [72, 97, 172], "224073": 77, "2244": 171, "224830": 69, "224897": [81, 82], "225": [17, 19, 62, 69, 101, 120, 171, 172], "22505965": 63, "225175": 96, "22528": 93, "225322": 69, "225425": 69, "225427": 78, "225574": 91, "2256": 93, "22562": 93, "225635": 96, "22563538": 96, "225764": 90, "225776": 100, "225917": 90, "226": [69, 72, 172], "226262": [124, 127], "226395": 173, "226479": 88, "226524": 96, "226598": 91, "226655": 99, "226769": 82, "227": [69, 72, 76, 105, 109, 172], "227018": 83, "227018245501943": 83, "227086": 90, "2271071": 36, "227249": 72, "227567": 111, "227765": 82, "2279": 93, "227932e": 92, "228": [72, 76, 172], "228035": 93, "2281": 93, "228648": 64, "228725": 93, "229": [64, 69, 72, 172], "22913274": 70, "22913377": 70, "22913858": 70, "22914202": 70, "22925": 93, "22934266": 70, "229443": 96, "229472": 92, "2295": [72, 93], "22959": 93, "229590": 72, "22965023": 70, "22970217": 70, "229726": 93, "229759": [112, 118], "229766": 82, "229816": [124, 125], "229961": [81, 82], "229994": [81, 82], "22m": [112, 120, 121], "23": [49, 54, 63, 64, 65, 67, 69, 72, 76, 81, 82, 89, 90, 91, 92, 96, 99, 100, 104, 109, 111, 112, 117, 118, 124, 139, 156, 168, 170, 171, 173], "230": [17, 19, 62, 171, 172], "230009": [85, 86], "2302": 98, "230518": [111, 112, 114, 124], "2307": [63, 91, 98, 103], "2308": 99, "230810": 72, "230821": 93, "230956": 68, "231": [10, 105, 109, 172], "231226": 124, "23124913": [124, 127], "231310": 96, "231430": 156, "231467": 124, "231501e": 81, "23153593": 70, "231879": 77, "231986": 96, "231e": 97, "232": 172, "232002": 124, "232119": 69, "232134": [81, 82], "232352": 124, "232360": 70, "232659": 76, "232959": [85, 86], "232e": 124, "233": [34, 172], "233087": [124, 127], "2331": 93, "233415": 93, "233550": 69, "233573": 76, "23368": 93, "233720": 82, "233835": 69, "23391813": 70, "233931": 76, "234": [171, 172], "2341": 62, "234137": 100, "234153": 100, "234315": 81, "234534": 83, "234605": 73, "234632": 69, "234910": 91, "235": [69, 72, 171, 172], "2350": 173, "2352": 62, "2353": 62, "235395": 71, "235419": 76, "236": [76, 172], "236008": 83, "23611": 93, "236411": 96, "236568": 71, "236821222325275557586467707377788996": 139, "2368268": 139, "23689448": [124, 127], "237": [65, 172], "237219": 69, "237223": 69, "237453": 81, "237704": 82, "237756": 77, "238": [63, 91, 172], "238012": 96, "23801203": 96, "238101": 96, "238213": 156, "238251": 83, "23856": 93, "238619": 78, "238974": 81, "239": 172, "239019": 88, "239154": 81, "239352": 93, "23968": 93, "239845": 93, "23e": 64, "24": [41, 63, 64, 65, 69, 72, 76, 81, 82, 90, 91, 92, 96, 99, 100, 101, 111, 112, 118, 124, 139, 156, 170, 171, 172, 173], "240": 90, "240127": [81, 82], "240160": 82, "240197": 81, "240295": 99, "2403": 98, "240419": 69, "240495": 111, "240529": 71, "240532": [81, 82], "240693": 111, "24080030a4d": 65, "240813": 87, "240830": 70, "241": [76, 172], "241049": 96, "241064": 82, "241288": 69, "241338": 69, "241443": 111, "241495e": 82, "241629": 72, "241793": 69, "241834e": 82, "241841": 95, "241935": 72, "241962": 100, "24197136": [124, 127], "241973": 93, "242": [171, 172], "242124": [92, 93], "242139": 156, "242158": [92, 93], "2424": 88, "242410": 72, "242427": 88, "242559": 67, "242604": 76, "242661": 81, "242728": 72, "242744": 72, "242815": 156, "242864": 93, "242902": 96, "242983": 72, "243": [93, 172], "243056": 71, "2430561": 62, "243180": 76, "243246": 96, "243279": [124, 127], "243457": 95, "243574": [69, 72], "243687": 72, "2438": 93, "244": [93, 172], "244455": 96, "2445": 93, "244622": 156, "244627": 76, "24469564": 170, "245": [171, 172], "245062": 96, "2451": 62, "24510393": 64, "245357": 72, "245370": 91, "245512": 96, "245704": 72, "245720": 68, "2458": 62, "246": 172, "246110": 72, "246274": 72, "2467506": 63, "246753": 96, "2468": 93, "246837": 99, "246879": 96, "247": [97, 172], "247020": 83, "247057e": 96, "2472": 93, "24774": [92, 93], "247826": 91, "247872": 72, "248": [93, 172], "248137": 81, "248170": 111, "248171": 96, "24835916": [124, 127], "248487": 90, "248638": 83, "248682": 111, "2489": 77, "249": [63, 91, 94, 172], "2491": 93, "249100": 91, "24917": 93, "249306": 90, "249873": 71, "2499": [70, 106, 109], "249970": 76, "25": [17, 18, 19, 29, 30, 31, 35, 40, 41, 42, 43, 44, 63, 64, 65, 68, 69, 72, 76, 81, 82, 83, 84, 90, 91, 92, 93, 96, 100, 111, 112, 118, 124, 139, 156, 170, 173], "250": [94, 172], "2500": [70, 93, 106, 109, 124, 129], "25000000000000006": [83, 93, 96], "250083": 93, "2501": 93, "250210": 83, "250425": 83, "250529": 90, "2506": 98, "250838": 90, "250885": 81, "250951": 69, "251": [92, 93, 99, 172], "251056": 82, "251152": 93, "251852": 81, "252": [93, 172], "252133": 93, "252253": 99, "252370": [124, 127], "25237186e": 139, "252524": 96, "252601": 156, "252667e": 86, "252685": 72, "252832e": 81, "253": [139, 172], "253026": [81, 82], "253030": 72, "253046": 69, "253056": 82, "25322406": 139, "25330855e": 139, "253504": 82, "253610": 16, "253675": 92, "253724": 96, "25374": 93, "253828": 77, "254": [60, 93, 172], "25401679": 63, "254038": 86, "2543": 93, "254324": 83, "254400": 156, "2545": 70, "254670": 72, "254858": 77, "255": [93, 172], "255088": 82, "255151e": 96, "255690": 72, "255728": 72, "256": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 93, 112, 118, 172], "2561": 70, "256416": 96, "25643": 93, "256567": 91, "25672": 93, "256944": 96, "256952": 72, "257": 172, "257037": 93, "257207": 63, "257311": 72, "257377": 68, "2575": [124, 127], "257508": 72, "257762": 96, "257783": 71, "257998e": 82, "258": 172, "258158": [81, 82], "25821083": 76, "258391": 69, "2584": 93, "258430": 111, "25855": 67, "258567": 81, "25865188": 70, "25871228": 70, "258809": 81, "259": 172, "259001": 82, "259184": 77, "259395": 87, "2594": [64, 92], "25943313": 70, "259666": [124, 129], "259828": [81, 82], "259961": 69, "25x_3": 68, "26": [63, 64, 65, 67, 69, 72, 73, 76, 77, 81, 82, 90, 91, 92, 104, 109, 111, 112, 118, 124, 139, 156, 170, 173], "260": 172, "26016": 93, "260161": 28, "260211": [81, 82], "260223": 81, "260356": 92, "260360": 96, "26073113": 70, "260762": 78, "261": 172, "2610": 93, "261034": 82, "261232": 139, "261493": 82, "261624": [92, 93], "261634": 111, "261685": 93, "261686": 90, "261700": 72, "261903": 91, "2619317": 63, "262": 172, "2620": 93, "262083": 90, "262204": 93, "262281e": [112, 113, 116], "263": [10, 93, 172], "263196": 86, "2633": 93, "264": [171, 172], "264086": 68, "264255": 16, "264426": 90, "265": [120, 172], "2651": 124, "265119": 95, "2652": [65, 92, 93], "265547": 93, "265888": 77, "265929": 97, "266": 172, "266015": 82, "266034": 72, "266147": 97, "2663": 77, "266724": 93, "266909": 156, "267": 94, "2670691": 63, "267164": 90, "267444": 76, "26748394": 72, "267500": 91, "267581": 93, "267945": 69, "267950": 96, "268": 172, "268275": 71, "268310": 82, "268406": 76, "268414": 72, "268942": 96, "268977": 93, "268986835": 139, "268998": 64, "269043": 96, "269308": 124, "2695162": 72, "26bd56a6": 65, "26e": 64, "27": [17, 18, 19, 35, 40, 61, 63, 64, 65, 66, 67, 69, 72, 73, 76, 81, 82, 90, 91, 92, 104, 109, 111, 112, 113, 116, 118, 124, 139, 156, 170, 171, 173], "270": 172, "2700": 65, "270000e": 93, "270311": 16, "27053": 69, "270644": [81, 82], "27066": 93, "27076058": 72, "270994": 70, "271": 172, "271004": [92, 93], "271183": 92, "2714838731": 91, "271867": 91, "272505": 90, "272643": 93, "273": 65, "273208": 93, "273356": 83, "273433": 76, "27371": [64, 92], "27372": [64, 92], "2739": [68, 69, 72, 97, 100], "273945": 69, "274": [65, 76, 93, 94], "2740991": 62, "274251e": 92, "27429763": [124, 126], "274397e": 82, "274619": 69, "2747": 62, "27472": 93, "274793": 96, "274825": 29, "275159342": 139, "275291": 72, "275341": 82, "275538": [105, 109], "275596": 156, "276": [65, 172], "2760": 93, "276002": 77, "276023": 72, "276043": 81, "276087": 69, "276148": 96, "276189e": 91, "2762": 93, "276311": 93, "2766091": 64, "276774": 76, "276882": 69, "27695": 93, "2772017": [124, 127], "277299": 73, "277626": 111, "277922": 72, "277987": 90, "277992": 81, "278": [85, 99, 172], "2780": 63, "278000": 91, "278035": 78, "278176": 82, "2783": 70, "278391": 93, "278434": 85, "278454": 88, "278539": 72, "27854362": 139, "2786": 156, "278907": 96, "279": 172, "279140": 77, "279241": 81, "27927": 93, "279333": 69, "279524": 96, "279595": 78, "27991": 93, "279925": 69, "28": [63, 64, 65, 69, 71, 72, 76, 81, 82, 84, 90, 91, 92, 111, 112, 118, 120, 124, 139, 156, 170, 172, 173], "280201": [124, 129], "280421": 82, "280454dd": 65, "2805": 168, "280501": 156, "280849": 86, "280963": 95, "281": [97, 172], "281024": 96, "28111364": 64, "281340": 81, "281360": 16, "281370": 81, "2815": 93, "2819": 156, "282": [97, 171, 172], "282308": 76, "282397": 69, "2825": [168, 170], "282502": 112, "282942": 72, "2830": [62, 168, 170], "28326": 93, "2836059": 63, "28367": 93, "283e": 97, "284": 172, "284073": 96, "28408045": 72, "284080e": 85, "28425026": 99, "284271": 87, "284366e": 81, "284407": 82, "28448202": [124, 127], "28452": [64, 92], "284763": 81, "2849": 93, "284953": 82, "284987": 93, "285": [97, 124, 172], "285001": 78, "285160": 91, "285276": 96, "285343": 71, "285483": 88, "285534": 69, "285580": 81, "285763": 82, "285e": 97, "286": 172, "2861": 77, "286203": 81, "2865": [62, 93], "286507": 83, "286563e": 93, "28696797": 72, "287": [172, 173], "287041": 96, "28712022561": 173, "287234": 111, "28748518": 72, "28760332": [102, 140], "287926": 96, "288": [94, 172], "288000": 72, "288013": 82, "288833": 76, "289": [171, 172], "2890": 62, "289006": 77, "289062": [92, 93], "289081": 96, "289170": 91, "289440": [81, 82], "289549": 77, "289555": 88, "28986014": [124, 129], "289983": 90, "29": [63, 64, 65, 69, 72, 76, 77, 81, 82, 90, 91, 92, 99, 111, 112, 116, 118, 120, 124, 139, 156, 170, 173], "290": 124, "29038": 82, "290901": 78, "290987": 92, "291": [97, 172], "2910": 93, "291011": [12, 124, 126], "291071": 96, "29107127": 96, "291318": [124, 129], "291406": 96, "291500e": [92, 93], "291517": [81, 82], "291855": 72, "291963": 96, "291994": 69, "292": [95, 172], "2920": 93, "292028": 83, "292046": 156, "2921": 77, "292105": 96, "292194": [124, 129], "2925": 65, "29259493": 72, "292923": 111, "292956": 69, "292997": 96, "29299726": 96, "293218": 96, "293669": 93, "293799": 82, "29388834e": 139, "294": 172, "294067": [81, 82], "294123": 111, "29428099": 72, "294302": 69, "2945": 77, "294862": 82, "295": [171, 172], "295005": 81, "295441": 72, "295837": [73, 104, 109, 170], "2958370000000100000100": [65, 104, 109, 170], "2958370001000010011100": [65, 104, 109, 170], "2958371000000010010100": [65, 104, 109, 170], "295855642811191": 83, "295856": 83, "295868": 93, "296077": 76, "296099": 78, "296142757": 139, "29622409": 72, "296506": 81, "296606": 82, "296728": 81, "296729": 91, "29675887": 96, "296759": 96, "297": 172, "297071": 72, "297139": 81, "297287": [81, 82], "297349": [85, 86], "29743279": 99, "297555": 82, "297682": 96, "2977": 93, "29774017e": 139, "297765e": 82, "298": [34, 65], "298120": 83, "298150": 90, "298235e": 93, "298327": 78, "298629": 111, "298995": 111, "299": [60, 65, 97], "299216": [72, 124], "299338": 81, "299485079": 139, "299524": 86, "299755": 90, "2999": 78, "29999": 69, "2_": [36, 66, 101, 157, 158, 167], "2_x": [36, 66, 101], "2d": [13, 140, 149], "2dx_5": [83, 96], "2e": [60, 62, 63, 64, 65, 66, 112, 120, 121, 122, 123, 124, 140, 156, 170], "2f": [77, 87], "2m": [157, 163, 167], "2n_t": 68, "2x": 96, "2x_0": [32, 81, 82, 85, 86], "2x_4": 68, "3": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 44, 47, 48, 49, 53, 54, 55, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 139, 140, 156, 157, 163, 168, 169, 170, 171, 172], "30": [32, 60, 61, 63, 65, 66, 67, 69, 71, 72, 76, 77, 78, 79, 80, 81, 82, 83, 89, 90, 91, 92, 93, 96, 97, 111, 112, 118, 120, 124, 139, 156, 170, 173], "300": [61, 80, 83, 93, 96, 103, 171], "3000": 78, "30000": 69, "30000000000000004": [83, 93, 96], "3002": 77, "300555": 81, "300566": 76, "30071009": 139, "300759": 69, "30093956": 99, "301": 65, "301222": 81, "301238e": 82, "301251": 69, "301366": 156, "301371": 96, "3016": 92, "301731": 81, "30175": 93, "30186": 93, "302212": 81, "30229388": 96, "302294": 96, "302392": 72, "302489": 69, "302648": 91, "30304559": 72, "303324": 91, "303733": 94, "303835": 91, "303f00f0bd62": 65, "304130": 96, "304159": 96, "304201": 68, "3044": 77, "304621": 124, "304794": 81, "305045": 69, "305067": 111, "305142": 90, "305272": 67, "30529": 93, "30530007": 72, "305341": 96, "305514": 69, "305612": 91, "305687": 96, "305775": 96, "305b": 65, "306": 120, "30628774": 96, "306288": 96, "306329e": 81, "306361": 72, "30645": 93, "30672815": 63, "306915": 91, "30695022": 72, "306963": 96, "307407": 96, "307444": 90, "307923": 16, "308": 93, "308180": 69, "308239": 93, "308410": 72, "308415": 16, "3087": 77, "30917769": [85, 86], "309391": 69, "309464": 93, "309772": 91, "309914": 72, "309952": 77, "31": [63, 64, 65, 66, 69, 71, 72, 76, 77, 81, 82, 90, 91, 92, 111, 112, 118, 120, 124, 139, 156, 170, 173], "310129": 69, "310482": 86, "310755": 81, "310899": 76, "310905": 72, "311": 97, "311449": 111, "311740": 90, "311793": 81, "312030": 93, "31214": 85, "3125": 93, "312652": 97, "313044": 156, "313209": 83, "313535": 96, "31378": 65, "313870": 88, "314": [76, 139], "3141": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 63, 65, 66, 73, 91, 102, 104, 109, 111, 112, 117, 118, 120, 121, 122, 123, 124, 140, 156, 170], "314102": 81, "314165": 72, "314247": 100, "314412": 72, "314543": 69, "3146": 30, "314625": 82, "31476": [92, 93], "314761": 81, "314959": 72, "315": 172, "315031": 100, "315155": 82, "315290": [85, 86], "315393": 85, "315804e": 82, "315883": 70, "316": [65, 172], "31607982": 72, "316193": 96, "316197": 77, "316286": [124, 129], "31633": 93, "316474": 81, "316717": [81, 82], "316718": 69, "317394": 68, "317487": 96, "317607": 96, "318": [65, 172], "318106": [124, 127], "318281": 82, "318571": 156, "318753": [85, 86], "319": [65, 172], "319100": [85, 86], "319151": 82, "319291": 76, "319420": 88, "319559": 77, "319634": 93, "319750": 16, "319759": 96, "319850": 96, "319903": 67, "319998": 81, "32": [39, 63, 64, 65, 69, 71, 72, 76, 81, 82, 90, 91, 92, 93, 94, 101, 111, 112, 118, 120, 124, 126, 139, 140, 156, 170], "320": 93, "320000e": 93, "320314": 92, "320633": 83, "320662547": 139, "320848": 72, "32087615": 72, "321": [94, 172], "321126": 72, "32165173": 69, "321673": 156, "32189133e": 139, "321959": 69, "322065": 81, "322404": 99, "322736": 37, "3235": 93, "323612931": 139, "323615": 82, "323636": 92, "323679": 91, "323831": 82, "324": [64, 172], "324078": 82, "324132": 82, "32458367": 63, "3245837": 96, "32459025": 72, "324910": 77, "324999": 81, "325": 97, "325046": 93, "325056": 96, "325171": 82, "325335": 72, "32540187e": 139, "325819": 93, "325825": 76, "325992": 81, "326": 97, "326543": 82, "326740": 96, "32674263": 71, "326871": 100, "3268714482135145": 100, "327": [93, 172], "327121": 16, "327303": 81, "327503": 82, "327564": 69, "32758462": 72, "327687": 69, "32789": 93, "327958": 78, "328141": 72, "328243": 72, "328367": 93, "328478": 76, "32875335": 71, "329715": 72, "33": [63, 64, 65, 69, 72, 76, 81, 82, 91, 92, 93, 111, 112, 118, 120, 121, 124, 139, 156, 170, 171], "330": 172, "3300": [64, 92], "330000e": 93, "330120": 72, "330285": [81, 82], "3304269": 63, "330582": 72, "330615": 96, "33065771": 96, "330658": 96, "330731": 29, "330828": 85, "330867": 72, "330868": 81, "330950": 82, "330962e": 72, "331": 172, "331015": 82, "331215": 90, "331415": 81, "331521": 96, "331538": 76, "3316": 62, "331602": 93, "33168943": 93, "33175566": 96, "331756": 96, "331991": 72, "332": 172, "332039": 76, "332085": 81, "33224667": 69, "332340": 69, "33243556": 72, "332522": 69, "332662": [124, 125], "332775": 94, "332782": 29, "3329": 93, "332996": 91, "333037": 69, "333115": 81, "333250": 93, "3333": [61, 63, 80, 111, 112, 114, 124], "3333333": 65, "33333333": [69, 71, 72], "3334": [77, 93], "3334743971": 139, "333581": 92, "334": 64, "3341": 77, "33437809e": 139, "334702": 72, "334750": 83, "334818": [124, 127], "334821": [124, 127], "335125": 76, "335162": 72, "335411": 71, "335446": 78, "335496": 76, "335609e": 96, "335846": 96, "335869": 111, "3359": 77, "336": [94, 172], "336064": [124, 127], "336461": 93, "336498": 96, "336612": 68, "336870": 70, "337": 172, "3371": 93, "337211": 69, "337343": 69, "338": [60, 99, 172], "338289": 69, "338449": [124, 125, 129], "33849": 93, "3386": 139, "338775": 83, "338855": 93, "3389": 77, "338908": 83, "339177": 77, "339261": 69, "339268": 96, "339269": 99, "339570": 96, "339762e": 93, "339875": [85, 86], "339940": 72, "34": [61, 62, 63, 64, 65, 66, 69, 71, 72, 76, 81, 82, 91, 92, 98, 99, 111, 112, 118, 120, 124, 139, 156, 173], "340": [64, 93], "340142": 124, "340144": 82, "340217": 70, "340235": 88, "340274": 99, "340692": 82, "340712": 93, "340844": 38, "341301": 95, "341336": 27, "341537": 69, "34164459e": 139, "34178188": 72, "341973": 72, "342117": 88, "3422": 93, "34222944": 72, "342307": 69, "342362": 78, "34246837": 99, "342651": 69, "342675": 63, "342693": 76, "342831": 72, "342938": 69, "342992": 91, "343": [97, 124, 172], "343073": 86, "343117": 76, "34335367": 72, "343509": 72, "343611": 111, "34375": 92, "344006": 81, "344054": 69, "344166": 71, "34427901": 72, "344300": 69, "344408": 81, "344465": 72, "34450402": 71, "344505": [92, 93], "344617": 111, "344640": 96, "344787": [81, 82], "344834": 68, "345": [120, 172], "345143": 82, "3454": 93, "345699": 69, "345903": 96, "346063": 72, "3461016": 69, "346198": 92, "346206": 96, "346238": 99, "346671": 72, "346678": 95, "346683": 83, "3466832975109777": 83, "3467": 62, "346836": 72, "347213": 91, "347310": 29, "347501": 89, "347547": 82, "347777": 72, "348": 97, "348047": 82, "34815089": 70, "348338": 93, "348617": 96, "348622": 97, "348697": 83, "3486970271639334": 83, "34870008": 70, "348724": 81, "34880724": 70, "349213": 71, "3492131": 62, "34937785": 70, "349379": 82, "349607": 82, "34967621": 63, "349771": 81, "349791": 72, "34980811": 75, "349900e": 93, "34m": [112, 120, 121], "34mglmnet": [112, 120], "34mmlr3": [112, 120, 121], "34mmlr3learner": [112, 120, 121], "34mmlr3pipelin": [112, 120], "34mranger": [112, 120, 121], "34mrpart": [112, 120], "35": [64, 65, 69, 72, 76, 81, 82, 83, 85, 91, 92, 93, 96, 111, 112, 118, 124, 139, 156, 157, 163, 173], "3500000000000001": [83, 93, 96], "3502": 139, "350323": 93, "350518": 96, "350533": 96, "35053317": 96, "350712": [85, 86], "35099879": 72, "35108603": [157, 163], "351182": 72, "35155": 93, "352": [64, 91, 172], "352021": [124, 127], "352246": 96, "352250e": [92, 93], "3522697": 63, "352573": 72, "35292": 93, "353": 173, "353064": 69, "353748e": 96, "354": [93, 172], "354188": 68, "354371": 96, "354375": 72, "355": 172, "355065": 78, "355112e": 82, "355251": 82, "355451": 72, "355617": 69, "355790": 81, "35596435": 70, "35596593": 70, "35596658": 70, "35596686": 70, "356": 172, "356136e": 93, "356167": 86, "3566": 93, "356796": 111, "3568": 124, "356899": 111, "357": [93, 172], "357586986897548": 67, "357654": 93, "35799809e": 139, "358": 172, "358158": 92, "358289": 91, "358395": 99, "358690e": 93, "35871235": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "358787": 156, "35881997": [124, 126], "359": [97, 172], "359229": 78, "3593": [75, 99], "359307": 82, "359622": 77, "359778e": 82, "359792": 81, "3598": 93, "35th": 171, "36": [64, 65, 69, 71, 72, 76, 77, 81, 82, 91, 92, 111, 112, 118, 124, 139, 156], "360": 172, "360004": 96, "360054": 156, "360249": 87, "360403": 76, "360475": [81, 82], "360683": 83, "360798": 69, "360801": 83, "360965": [105, 109], "361": [97, 172], "361211": 69, "361220": 111, "361406": 72, "361481": 72, "361623": 88, "361624e": 139, "36177231": 139, "3619201": 33, "362040": 81, "36213": 93, "362397": 81, "362398": 25, "362760": 93, "363": 172, "363059": 82, "363082": 81, "363103": 83, "3631031251500065": 83, "363144": 81, "363276": 63, "36355881": [124, 129], "363576": 90, "363770": 69, "363820": 70, "364276": 90, "3643": 156, "364456": 82, "364595": 63, "3647": 65, "364800": 96, "364860": 72, "364929": 72, "365": 172, "365527": 93, "365619": [124, 129], "365681": 81, "366188": [105, 109], "366620": 69, "366718627": 63, "36677424": 72, "367": [97, 172], "367017": 90, "367323": 96, "367366": 88, "36744004": 72, "367571": 83, "367600": 72, "367625": 96, "367827": 72, "368": [69, 71, 72, 93, 172], "368081": 69, "368152": 91, "3682": [64, 92, 93], "368324": 91, "368577": 111, "368592": 82, "368739": 71, "369": 172, "369020": 69, "369556": 83, "3696": 99, "369723": 111, "369796": 96, "369869": 92, "369981": 91, "36m": [112, 120, 121], "37": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 64, 71, 75, 77, 81, 82, 88, 91, 92, 94, 111, 112, 122, 123, 124, 139, 156], "370": 172, "370000e": 93, "370141": 81, "370165": 93, "370223": 72, "370254e": 92, "3702770": 63, "370736": 91, "3707775": 63, "370889": 76, "371": 172, "3711": 77, "3711317415516624": 83, "371132": 83, "3712": 93, "371254": [124, 125], "371357": [92, 93], "371429": 83, "371557": 69, "371972": 26, "372": [171, 172], "37200": [92, 93], "372097": 83, "3722": 93, "372329": 82, "372332": 82, "3724": 93, "372453": 69, "3727679": 63, "373034e": 38, "373065": 69, "373129": 69, "373235": 93, "373280": 72, "3738573": 63, "374": 172, "374080": 72, "374335": 96, "37433503": 96, "374426": 81, "3745": 93, "37458675": 72, "374812e": 93, "375066": 81, "375081": 93, "375106": 76, "375403": 67, "375465": 96, "375525": 69, "375897": 76, "375995": 90, "3763": 62, "3766": 88, "376617": 88, "376718": 82, "377": [60, 172], "377147": 111, "377246": 93, "377273": 72, "377311": 96, "37738347e": 139, "377955": 96, "378161": 70, "378367": 96, "378428": 82, "378457": 81, "378474": 81, "378596": 91, "378671": 81, "378734": 72, "378834": 96, "378869": 72, "3788859": 63, "378932": 81, "379": 171, "379614": 96, "379910e": 82, "38": [65, 76, 77, 81, 82, 92, 111, 124, 139, 156], "3800694": 63, "380126": 69, "380349": 72, "380538": 77, "380695": 77, "380837": [92, 93], "381": [97, 120], "381023": 69, "381072": 96, "381083": 82, "381247": 93, "381278": 93, "381684e": 92, "381685e": 93, "381689": 96, "382024": [105, 109], "382109": 69, "382285": 93, "382289": 72, "382570e": 72, "382872": 83, "382919": 16, "383019": 69, "383297": 96, "38348": 93, "384": 93, "38402797": [124, 127], "38405574": 71, "384189": 90, "384208": 90, "384232": 77, "384677": 78, "3848": 93, "384867e": 72, "384883": 93, "385226": 156, "385279e": 81, "385601": 81, "385917": 91, "386": [65, 93], "386102": 83, "386263": 69, "386502": 93, "386643": 86, "386831": 78, "387": 65, "387169": 69, "387268": 76, "387426": 96, "387780": 96, "388005": 93, "388071": 96, "3881069": 139, "388185": 78, "388216e": [112, 118], "388675": 69, "388807": 71, "38898864": 96, "388989": 96, "389": 65, "389126": 99, "389787": [124, 127], "38983785": 67, "39": [60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 78, 81, 82, 87, 89, 91, 92, 93, 99, 100, 101, 111, 112, 117, 124, 139, 156], "39007": 30, "390155": 90, "390244": 124, "390337": 82, "390379": 96, "390412": 72, "39070379": 139, "390965": 69, "391377": 100, "391816e": 81, "39186467": 75, "392": 120, "392242": 87, "392259": 81, "392472": 93, "392623": 82, "392713": 69, "392833": 99, "392864e": [92, 93], "393000": 81, "393207": 72, "39322719": 72, "393374": 82, "393463e": 72, "393604": 83, "393654": 78, "3937": 77, "39383671": 71, "393925": 72, "39425708": 63, "39462897": 139, "394757": 90, "39477123": [124, 127], "394848": 81, "394911": 69, "395076e": 93, "395195": 91, "395538": 81, "395671": 81, "3958": 124, "396": 124, "396073": [124, 125, 129], "3961": 93, "39611477": 64, "396272": 90, "396318": 111, "396531": 93, "396763": 82, "396985": 91, "396992": [81, 82], "397140": 83, "39727": 93, "397313": 62, "397578": 87, "3977": 77, "3979": 71, "398": [104, 109, 170], "398000e": 93, "398166": 78, "398367": 77, "3985": 93, "3985256": 71, "398544": 69, "398562": 69, "398770": 96, "398999": 24, "399": 64, "399052": [124, 127], "399056": 96, "399223": 68, "399355": 68, "399544": 72, "399679": 124, "399692": 96, "399858": 100, "399e": 97, "39m": [112, 120, 121], "3cd0": 65, "3dx_1": [83, 96], "3e1c": 65, "3ec2": 65, "3f5d93": 94, "3x_": 96, "3x_4": [83, 96], "4": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 36, 37, 38, 39, 40, 41, 47, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 117, 118, 123, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170, 172], "40": [63, 66, 67, 76, 81, 82, 83, 92, 93, 96, 101, 104, 109, 111, 112, 118, 124, 139, 156, 157, 163], "400": 91, "4000": [69, 72], "4000000000000001": [112, 118], "40000000000000013": [83, 93, 96], "400047": 82, "400113": 88, "400194": 82, "40053029": [157, 163], "400587": 90, "4006": 62, "400823": 96, "400905": 78, "401": [10, 173], "401009": 90, "401035": 76, "401037": 76, "401140e": 82, "401247": 39, "401560": 81, "40161852e": 139, "40164216": 139, "401768": [140, 156], "401931": [85, 86], "402": [104, 109], "4020": 77, "402077": 93, "402100": 156, "402505": 82, "40271116": 139, "40274824": 71, "402924": 76, "403": 97, "403113": 83, "403113188429014": 83, "403175e": 82, "403313e": 81, "403425": 96, "40359107": 67, "403626490670169": 102, "4036264906701690": 102, "403626491": 102, "403771948": 140, "40408972": 71, "40417126e": 139, "404300": 78, "404318": 62, "40452": 93, "404550": 95, "404629": 81, "404824": 93, "404997": 72, "405000": 93, "405203": 68, "4052364": 71, "405411": 82, "405476": 69, "405754": 69, "40583": 62, "405890": 29, "406085": 111, "406285": 96, "406446": 83, "406580": 81, "406589": 69, "40661598": 71, "40676": 62, "407": 97, "40732": 88, "407758": 16, "408": 120, "408448": [157, 163], "40844844": [157, 163], "408539": 96, "408565": 96, "408834": 71, "4091108": 139, "409154": 62, "4093": 99, "409359": 71, "409390": 93, "409395": 96, "409746": 83, "409846": [124, 125], "409848": [81, 82], "40998665": [124, 129], "409988e": 81, "41": [76, 81, 82, 92, 93, 111, 112, 123, 124, 139, 156], "410153": 82, "4101895": 76, "410204": 81, "4103": 77, "410393": 83, "410673": 71, "410681": 68, "410795": 91, "411190": [81, 82], "411291": 69, "411295": 96, "411304": [81, 82], "41132579": 72, "411374": [112, 113, 116], "411582": 96, "412": 93, "412127": 96, "41217": 94, "412304": 100, "412406": 93, "412477": 68, "412620": 76, "412653": 91, "412714": 83, "412919": 85, "413": 93, "413376": 124, "413404": 82, "41341040": 63, "413784": 93, "41383313": 71, "414": 97, "414845": 81, "41514": 173, "41514013": 173, "415294": 96, "415523": 81, "415956": 111, "416052": 78, "41613276e": 139, "41654757": 71, "4166": 93, "4166667": 65, "416737": 90, "416899": 81, "416e": 97, "41718791": 72, "417669": 93, "417727": 92, "417767": [85, 86], "417834": 78, "417988": 96, "418056": 96, "41805621": 96, "418277": 81, "418368": 82, "418400": 88, "418412": 82, "418591": 93, "418741": 78, "418806e": 83, "4192": 62, "41936": 71, "419371": 96, "419644": 111, "419871": 78, "4199952": 63, "41e5": 65, "42": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 66, 67, 68, 69, 71, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 93, 95, 96, 99, 100, 101, 106, 109, 111, 112, 118, 124, 125, 126, 127, 129, 139, 156, 171], "420259": 81, "420308e": 93, "42073312": 63, "420967": [83, 99], "42096717": 99, "421083": 62, "4211349413": 63, "421200": 100, "421357": [85, 86], "42143": 93, "421576e": 93, "421793": 99, "4218": 77, "421919": 93, "422004": 81, "422007": 99, "422116": 93, "422119": 93, "422142": 72, "422244": 76, "422325": 83, "422458": 69, "422496e": 72, "42279": 124, "422832": 69, "422933": 76, "422984": 16, "42308093": 71, "423092": 82, "4232": 77, "4234": 77, "423441": 72, "4235": 77, "4235839": [85, 86], "423951": 62, "424108": 83, "42412729": 63, "424188": 139, "4242": 89, "424328": 96, "424573": 90, "424607": 124, "4247": 77, "424717": 83, "424748": 100, "424879": 93, "42488405": 71, "424959": 81, "425": 91, "425000e": 93, "425103": 62, "42510305": 71, "425274": 69, "425325": 88, "425414": [105, 109], "425493": 62, "42550": 93, "42563256": 71, "425636": 88, "425947": [124, 129], "426055": 62, "426301": [124, 127], "426540": 91, "426540301": 63, "426734": 90, "426736": 93, "427": 93, "4270334": 71, "427050": 81, "427317e": 81, "42742131": 71, "427486": [81, 82], "427573": 91, "4276": 77, "427654": 88, "427725": 96, "427747e": 82, "428": 156, "428046": 95, "428255": 96, "428411": [92, 93], "428467": 96, "4284675": 96, "4286": 77, "428771": 29, "428811e": 72, "429083": [124, 129], "429133": 90, "429298": 90, "429309": 93, "42934105": 101, "42ba": 65, "43": [64, 71, 76, 78, 81, 82, 93, 111, 124, 139, 156], "430102": 72, "4302": [62, 77], "430239": 69, "430298e": [92, 93], "430305e": 82, "430345": 90, "430627": 124, "430872": 76, "430878": 76, "4311947070055128": [112, 118], "431268e": 139, "431306": 96, "431437": 99, "431512": 81, "431645": 81, "431701914": 168, "431825": [124, 129], "431929": 93, "431998": 78, "432130e": 92, "432300e": 96, "432466": 82, "432707": 97, "432881": [112, 116], "43294": 65, "432f": 65, "433": [65, 97], "4330": 93, "433221": 83, "433473": 93, "433509": 77, "43361991": 96, "433620": 96, "434": 94, "434054e": 88, "4344": 77, "43444488": 72, "434448": 81, "434451": 72, "434535": 96, "43453524": 96, "434565": 81, "434656": [124, 129], "434784": 72, "4348": 77, "43482": 93, "43489148": 71, "435": 65, "43503345": 124, "435306": 76, "435401": 91, "4357": 93, "435739": 86, "435927": 93, "435967": 91, "436": [65, 93], "436243e": 82, "436327": 93, "436408": [124, 129], "436764": 97, "436806": 93, "437134": 93, "43752328": 72, "437667": 92, "437924": 93, "437967e": 81, "438": 91, "438203": 82, "438219": 96, "43849868e": 139, "438569": 93, "438592": 69, "438933": [124, 129], "438960": 91, "43900941e": 139, "4391": 77, "439541": [92, 93], "439675": 97, "439699": 78, "439836": 72, "439860": 72, "43989": 24, "43f0": 65, "44": [71, 76, 78, 81, 82, 111, 124, 126, 139, 156], "4401": 77, "440101": 72, "440320": 93, "440574": 85, "440790": 77, "440867": 76, "440a": 65, "441004": 93, "441057": 72, "441153": 96, "441209": 96, "44124313": 124, "441493": [124, 127], "441611": 72, "4416552": 63, "4417": 77, "4420": 77, "442248": 72, "442737": 82, "442976": 76, "443016": 83, "443032": 92, "44312177": 64, "443191": 81, "44349955": 76, "443686": 96, "4438": 93, "44404522": [124, 127], "444046": 93, "44405401": 124, "444304": [124, 127], "444342": 71, "4444": [61, 63, 80, 124], "444500": [92, 93], "444805": 90, "445": 97, "4450": 77, "445059e": 82, "445107": 82, "445331": [124, 129], "4455": 77, "445587": 69, "4460": 77, "446023": 95, "4462": 65, "446203": 81, "4466": 70, "44688257": 139, "446927": 82, "44709129e": 139, "447375": 76, "44756165": 72, "447624": [81, 82], "447706": 83, "447863": 83, "447863428811719": 83, "448": 93, "4481": 77, "448245": 71, "44840084": 99, "448587": 83, "448677": 82, "4487": 93, "448745": 96, "448904": 69, "448923": 87, "449": 120, "449000": 72, "449150": 29, "449172": 76, "449677": 88, "449715": 76, "44978937": 72, "4499681662941313": 117, "449974": 69, "44fa97767be8": 65, "45": [67, 81, 82, 83, 87, 90, 93, 96, 111, 112, 120, 124, 139, 156], "4500": 92, "45000000000000007": [83, 93, 96, 112, 118], "450000e": 93, "450152": 91, "4501588297905004": 112, "45047161": 139, "4506": 77, "450786": [124, 127], "450870601": 63, "451158": 76, "4513": 77, "45141516172026303435384344486366749395": 139, "451664": 69, "452": 65, "452016": 69, "452091": 93, "452187e": 81, "452435": 69, "452488701": 63, "452489": 91, "452716": 81, "453": 65, "453189": 76, "453326e": 81, "4535": 93, "453558": 70, "4536": 93, "453696": [124, 125, 129], "4539": 65, "454129": 16, "454251": 111, "454305": 82, "454406": 78, "454426": 157, "45451": 93, "454793": [124, 129], "454991": 77, "455": 65, "455078": 83, "455107": 83, "4552": 65, "455293": 83, "4552b8af": 65, "455564": 16, "455653": 82, "455672": 93, "455683": 72, "455798": 158, "455813": 82, "45606783": 72, "456225": 96, "456370": 91, "456453": 16, "456585": 72, "4566031": 156, "45660310": 156, "456617": 96, "456662": 81, "4567": 99, "4568": 77, "456892": 83, "457088": 96, "457299": [124, 127], "457614": 99, "457637e": 82, "45774321": 72, "457829": 82, "45789302": 139, "458114": 93, "4584": 93, "458420": 93, "4584447": 63, "458517": 82, "458814": 88, "458855": 64, "4592": 63, "459200": 91, "459383": 83, "459448e": 82, "459541": 81, "45956884": 71, "459617": 16, "459760": 93, "459812": 83, "46": [71, 77, 81, 82, 87, 111, 124, 139, 156], "4601": 93, "460207": [81, 82], "460218": 83, "460289": 96, "460447": 81, "46083": 92, "460856": 117, "460888": 82, "460970": 72, "4611": 77, "461396": 90, "461469": 70, "461493": 90, "461615": 72, "461629": 100, "4619": 173, "461947": 111, "461e": 97, "462319": 93, "462451": 83, "462506e": [112, 116], "463057": 82, "463208": 93, "463325": 96, "4634": 93, "463418": 100, "463422": 69, "463508": 72, "463658": 72, "4637": 77, "463766": 86, "463b": 65, "464076": 83, "4642343": 72, "464282": 91, "464556": 72, "464668": 27, "4649": 77, "465": [78, 124], "465011": 72, "465160": 72, "4652": 77, "46531443": 139, "465649": 100, "465730": 100, "4659651": 102, "465965114589023": 102, "4659651145890230": 102, "465981": 96, "46598119": 96, "466047": 96, "466246": [124, 129], "4663": 77, "466440": 83, "466452": 76, "466684": 93, "466756": 96, "46684493": 139, "46747971": 99, "467613": 91, "467613401": 63, "467681": [81, 82], "46768664": 139, "467770": 83, "468001": 93, "468016": 76, "468072": 82, "468075": 96, "46807543": 96, "468406": 93, "468906": 82, "468907": 78, "468d": 65, "469": 65, "469170": 93, "469379": 93, "469474": 100, "469676": 78, "469769": 82, "469794": 72, "46982323": [124, 129], "469825": 83, "47": [64, 77, 78, 81, 82, 85, 92, 99, 111, 124, 139, 156, 172, 173], "470216": 82, "470365": 93, "470511": 82, "470748": 81, "47089388": 72, "471": 78, "471435": 97, "471454": [105, 109], "471666": [105, 109], "471819": 71, "471838e": 81, "472255": 93, "472407": 72, "472891": 96, "472968": 111, "472e": 65, "473054": 81, "473099": 83, "47319": 124, "473203": 72, "473975": 72, "47419634": 170, "474214": [85, 86], "474338": 69, "474557": 124, "47503882": 72, "47524987": 139, "475395": 93, "475688": 81, "475e": 97, "4766": 77, "476856": 83, "4770": 77, "477130": [81, 82], "477150": 96, "4772": 77, "477354": 93, "477357": 97, "477395": 93, "477474": 91, "47779154": 72, "47782": 93, "477833": [124, 129], "477892e": 139, "47805578": [124, 129], "4782": 77, "478399": 93, "478896": 93, "479288": 16, "479422": 71, "479532": 81, "479722": 82, "479750e": 81, "479876": [85, 86], "479882": 82, "479928": 96, "47be": 65, "48": [65, 78, 81, 82, 86, 92, 93, 111, 124, 139, 156], "480": 78, "480000e": 93, "480133e": 96, "480199": 88, "480246": 82, "48029755": 99, "480579": 82, "480691": 39, "480800e": 96, "480813": [124, 125, 129], "481172": 96, "48123964": [102, 140, 156], "48124": [140, 156], "481295": 81, "481399": [92, 93], "4814": 75, "481602": 86, "481713": 88, "481761e": 93, "481813": 72, "481986": [124, 127], "482": [65, 78], "482038": 83, "482304": [157, 163], "48230419": [157, 163], "482403": 85, "482483": 96, "482508": 90, "482707": 82, "482790": 68, "482902e": 82, "48296": 99, "483": 124, "483055": 72, "48315": 99, "483186": 68, "483192": [92, 93], "48331": 99, "483531": 96, "48353114": 96, "483642": 93, "48366857e": 139, "483711": 96, "483717": 83, "48377465e": 139, "483855": 93, "483908": 69, "484": 93, "48404": 63, "4842": 93, "484640": 96, "48478441e": 139, "48482291": 139, "4849": 65, "484999": 81, "485": [65, 76, 93], "485000": 76, "485138": 72, "485201": 94, "485468": [105, 109], "48550": 100, "485586": 69, "485617": [92, 93], "485759": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "48583": [92, 93], "486": [42, 93], "486137": 69, "486202": 83, "486342": 93, "486476": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "486477": 71, "486532": 96, "486692": 76, "487": [78, 93], "487151": 72, "487352": 70, "487467": 93, "487524": 88, "487641e": 96, "4876748": 72, "487872": 79, "487934": 16, "488455": 90, "488537": 81, "488551": 76, "488811": 96, "488909": [92, 93], "488982e": 83, "489": 120, "489054": 95, "48926817e": 139, "489292": 76, "489315": 82, "489455": 82, "489511": 76, "4895498": 96, "489550": 96, "489567": 98, "489699": 83, "49": [65, 76, 78, 81, 82, 94, 111, 139, 156], "490070931": 63, "49031859": 139, "4904": 76, "490437": 81, "490488e": [92, 93], "490523": [124, 125], "490689": 90, "490888": 76, "490896": 78, "490898": 93, "490941": 93, "490970e": 81, "49098": 99, "491245": 91, "491249": 90, "49137024": [124, 126], "49153740e": 139, "492": [60, 93], "49222603e": 139, "4923156": 102, "49231564722955": 102, "492315647229550": 102, "492410": 96, "492547": [124, 129], "492626": 87, "492685": 81, "492789": 82, "49289678e": 139, "493": [97, 124, 171], "4931": 93, "493144": 100, "493195": 88, "493219": 96, "493426": 97, "493442": 94, "493782": 72, "493792": 90, "494": 97, "494321": 82, "494324": 91, "494324401": 63, "495": 95, "49530782": 63, "495405": 96, "495657": 83, "495752": 96, "496": 95, "496096": 72, "496251": 94, "496300e": 93, "4965": 77, "49650883": 99, "496551": 96, "496714": 100, "49693": [112, 121], "497": [95, 120], "49756588e": 139, "497698": 94, "498": [93, 95], "498019": 111, "498048": 77, "498122": 88, "498572": 72, "498575": 76, "498727": 82, "498921": 96, "498979": 93, "498f": 65, "499": [76, 95, 104, 109, 170], "499000e": [92, 93], "49934441": 76, "4996": 77, "49d4": 65, "4a53": 65, "4b8f": 65, "4dba": 65, "4dd2": 65, "4e": [63, 64], "4ecd": 65, "4f": 76, "4fee": 65, "4x": 96, "4x_0": [32, 81, 82, 85, 86], "4x_1": [32, 81, 82], "5": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 55, 56, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 139, 140, 148, 156, 157, 163, 169, 170, 172], "50": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 63, 65, 66, 68, 72, 75, 76, 78, 83, 86, 90, 92, 93, 94, 96, 111, 139, 156, 173], "500": [7, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 37, 38, 39, 40, 41, 44, 51, 61, 65, 69, 70, 71, 72, 73, 75, 76, 77, 80, 81, 82, 85, 86, 92, 95, 97, 99, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 124, 125, 126, 127, 129, 140, 156, 157, 163, 170, 173], "5000": [47, 69, 70, 71, 72, 81, 82, 83, 96, 98], "50000": 91, "500000": [92, 93], "5000000000000001": [83, 93, 96], "500000e": 93, "500084": 96, "500229": 82, "500267": 87, "5003517412": 63, "500530": 82, "500635": 93, "500755": 72, "501021": 93, "5011": 77, "501203": 90, "501395": [124, 127], "501403": 96, "501645": [112, 113, 116], "501836": [124, 129], "502005": 111, "502084": 124, "502268": 93, "502302": 94, "502494": 83, "5025850": 63, "502612": 96, "502759": 72, "502995": 96, "503049": 81, "503089": 96, "503475": 81, "503700": 78, "503749": 94, "503859": 72, "504286": 91, "5042861": 63, "50444188e": 139, "504773": 81, "5050973": 63, "505385": 82, "505473": 82, "505712": 76, "506080": 77, "5065899": [124, 127], "506629": 72, "50672034": 63, "506900e": 96, "506903": 83, "507": 97, "507115": 111, "507285": 88, "507568": 81, "507630": 82, "50768b": 94, "508": 97, "508153": 95, "50828321": 76, "508406": 95, "508433": 81, "508459": 91, "508529": 82, "508641": 72, "508756": [105, 109], "5089": 93, "508947": 15, "509059": 93, "509091": 72, "509102": 93, "509196": 96, "509359": 69, "509461": 96, "509651": 90, "50967": 100, "5097": 100, "5098": [73, 104, 109, 170], "509853": 96, "509894": 76, "5099": [65, 73, 104, 109, 170], "509951": 83, "509958": 91, "51": [62, 64, 65, 71, 78, 85, 111, 139, 156, 172], "510000e": [92, 93], "510257e": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "510385": 91, "510555": 78, "510586": 96, "51063": [124, 126], "51079110": 63, "510971": 90, "5115547": 102, "5115547181877": 102, "51155471818770": 102, "511668": 100, "5116683753999614": 100, "511722": 38, "511833": 72, "511855": [124, 129], "512": 91, "512064": 81, "512108": 96, "512149": 96, "51214922": 96, "512313": 72, "51248": [124, 157, 163], "512519": 91, "512572": 96, "512657e": 93, "512885": 76, "51290781": 72, "513": 60, "5131": 92, "513222": 88, "513595": 69, "513658": 93, "513992": 96, "513994": 72, "514": 65, "514022": 81, "514199": 77, "514581": 81, "514950": 72, "515300": 72, "515358": 83, "5154": 93, "5154782807567244": 91, "5155": 65, "516": 65, "516043": 82, "516125": 83, "516145": 93, "516222": 96, "516255": 96, "516256": 96, "51633331": 72, "51635518": 72, "516528": 96, "517": [65, 91], "5175": 93, "517510812139451": 67, "51766939": [124, 127], "517798": 78, "51795656": 139, "518175": 91, "518478": 78, "518517": 30, "518610": 124, "518846": 91, "518854": 78, "518978": 90, "519170": 82, "519278": 81, "51966955": 63, "519697": [124, 129], "519710": 96, "52": [62, 65, 87, 90, 97, 111, 139, 156], "520": 93, "520096": 90, "520930": 83, "521002": 83, "5210215656500876": 76, "521044": 76, "521104": 93, "521118": 76, "521233": 78, "5212361": 139, "521644": 76, "521848": 72, "521990": 76, "521994": 76, "522005": 82, "522208": 81, "522398": 90, "522605": 93, "522835": 68, "523018": 72, "523030": 100, "523163": 83, "523315": 69, "523453": 76, "5236488": 139, "523794e": 96, "5238": 77, "523977545": 63, "524215": 93, "52424539": 63, "524338": 81, "524518": 69, "524554e": 82, "524619e": 82, "52465": [112, 120], "524657": 96, "524817": 90, "524934": [81, 82], "525029": 81, "525064": 78, "52510803": 64, "52511": [112, 116], "5251546891842576": 100, "525182": 70, "525316": 76, "525324": 89, "52545013": 124, "5255": 65, "52590": [64, 92], "526": 91, "526532": 93, "526582": 88, "526769": [81, 82], "52686240": 139, "5271": 139, "52714188": [124, 127], "527171": 72, "52732": [112, 121], "527644e": 96, "527660": 81, "527984": 81, "528000e": 99, "528075": 76, "528381e": 101, "528434": [124, 127], "528553": 76, "528725": 96, "528937": [85, 86], "528991": 76, "528996901": 63, "528997": 91, "529": 91, "529405": 62, "529545": 71, "529782": 62, "53": [62, 65, 76, 78, 87, 91, 104, 109, 111, 124, 126, 139, 156, 168, 171], "5301": 93, "5302": 62, "530597": 111, "530659": 87, "530830": 93, "530940": 96, "53094017": 96, "531": 65, "531223": 83, "53148135": [124, 127], "531594": 93, "531691": 82, "531999": 93, "532157": 82, "532202e": 30, "532266": 83, "532291": 82, "532492": 82, "532509": 81, "532738": 96, "53273833": 96, "5328": 93, "532868": [124, 127], "53291002": [124, 127], "532998": 111, "533": 60, "533276": 85, "533316": 93, "533489": 68, "5337": 77, "533871": 93, "533900": 96, "53393769e": 139, "534139": 78, "534347": 69, "534362": 124, "534487": 70, "53449013": 72, "5346": 65, "534934e": 82, "535179": 96, "535318": 96, "535919": 72, "53606675": 96, "536067": 96, "53677633": 72, "536792": 93, "536798e": [92, 93], "53693619": 72, "537681": 93, "537818": 72, "53791422": [101, 124], "538": 65, "5382": 99, "538282": 93, "53849791": 96, "538498": 96, "538513": [105, 109], "538937": [92, 93], "539228": 69, "539455": 96, "539471": 81, "539491": [85, 86], "539767": 83, "54": [62, 64, 65, 76, 77, 90, 92, 97, 103, 111, 139, 156, 172], "540101": 96, "54010127": 96, "5401499": 139, "540240": 93, "540299": 76, "540375": 88, "540429": 82, "5405": 88, "540549": 88, "540578": 90, "541": 97, "541010": 96, "541060": 88, "541159": 96, "54119805": 67, "54149": 69, "54163": 99, "5416844": 102, "541684435562712": 102, "54173552": 97, "541863": 76, "542": 97, "542203": 82, "54238616e": 139, "542451": 96, "542560": 100, "542647": 96, "54265599": [157, 163], "542656": [157, 163], "542671": 91, "542769": 96, "542989": 96, "543": [91, 93], "543014": 93, "543052": 78, "543075": 83, "5431258": 72, "543136": 83, "543287": 76, "543380": 91, "5434231": 102, "543423145188043": 102, "5436005": 63, "543665": 82, "5436876323961168": 83, "543688": 83, "543734": 93, "543764": 86, "54378": 99, "543929": 69, "543955": 111, "544058": 76, "544088": 72, "544097": 96, "544098": 111, "544127": 72, "544383": 100, "544555": 91, "544657": 76, "544678": 111, "544821": 82, "54483": [140, 156], "5448331": [140, 156], "5448416": 139, "545492": 78, "545504": 82, "545592": 72, "545605e": 96, "545720": 82, "546100": [124, 127], "546260": 90, "546438": 93, "546889": 81, "54716": 99, "547306": 83, "5473064979425033": 83, "5475": 93, "547749": 69, "547794": 16, "547834": 69, "5479": 93, "547909": 93, "548": 97, "548171": 71, "548928": 71, "549192": 76, "5494": 93, "549654": 86, "5496967": 139, "55": [64, 65, 76, 83, 90, 92, 93, 96, 111, 139, 156], "5500000000000002": [83, 93, 96], "550176": 90, "550196": 76, "550911": 72, "551010e": 93, "551257": 139, "551317": 82, "551686": 83, "55176": [112, 120], "552081": 90, "552396": 69, "552727": 91, "552776": 96, "5528173": 139, "553004": 78, "55307": [112, 123], "553087": 69, "55340296e": 139, "55346908": [124, 129], "553593": 71, "553672": 93, "553878": 28, "55390846": [124, 127], "554076": 83, "554140": 111, "554168": [124, 129], "554390": 72, "5548": 77, "55494725": 72, "554957": 69, "554986": 83, "554986206618521": 83, "555": 91, "555445": 95, "555498": 96, "5555": [61, 80], "555705": 81, "555821e": 72, "555954": 93, "556191": [81, 82], "556533e": 93, "556591": 173, "55659131": 173, "55662098": [124, 127], "556792": 96, "5572": 77, "5574dcd4": 65, "557595": 91, "557741": 90, "557999": 91, "558": 60, "558134": [81, 82], "55828259": 101, "5584": 91, "5585": 91, "55855122": 67, "55855638": 72, "55863386": 124, "558655": 83, "5589": 91, "558996": 93, "559": [41, 171, 173], "5590": 91, "559144": 83, "559179": 72, "559186": 83, "5592": 91, "559394": 96, "559522": 96, "559680": 93, "559712": [105, 109], "559992": 72, "55dc37e31fb1": 65, "55e": 64, "56": [65, 77, 103, 111, 139, 156, 168, 171], "560135": 39, "56018481": 96, "560185": 96, "560616": 76, "560689": 62, "560711": [140, 156], "560723": 87, "560855": 72, "560949": [124, 126], "561309": 93, "561538": 111, "5616": 92, "561785": 15, "561827": 82, "562013": 96, "56223": 99, "562288": 100, "562390": 111, "562556": 71, "5625561": 62, "562660": 82, "562712": [81, 82], "562749": 139, "562866": 93, "563067": 97, "563257": 69, "563374e": 83, "5633823": 139, "563503": 96, "563563": 88, "563673": 93, "563690": 93, "56374312": [124, 127], "563760": 93, "5638": 93, "56381489e": 139, "564": 124, "564045": 96, "564073": 93, "564232": [81, 82], "564262": 81, "564451": 82, "564512": 82, "564577": 93, "564617": 69, "564800e": 93, "564829": 83, "565066": 83, "565610": 81, "565915": 93, "566": 100, "566024": 96, "566068": 72, "566263e": 72, "56633984": [124, 127], "566600": 96, "56670073": 96, "566701": 96, "566803": 69, "566943e": 82, "567004": 99, "567206": 82, "567319": [124, 127], "56753233": [124, 129], "567661": 69, "567695": 81, "5678": 70, "567945": [85, 86], "568071": 93, "568116069": 139, "568287": 88, "56915009": [124, 127], "569153e": 81, "569381": 111, "569626": 81, "56964588": [124, 127], "569679": 72, "569828e": 139, "569911": 63, "5699994715": 63, "57": [65, 76, 97, 111, 139, 156], "57002385": [124, 127], "570109": [124, 127], "570351": 77, "570354": 81, "570486": 62, "570500": 82, "570562": 62, "570722": 170, "5708": 93, "57126191e": 139, "571429": 83, "5714294154804167": 83, "571778": 62, "5718": 93, "5722": 92, "57245066": 96, "572451": 96, "572717": 98, "573225": 72, "573679": 16, "573700": 68, "574": 65, "574160": 88, "57436": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "574714": 94, "574796": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "5748": [112, 121, 139], "57496671": 63, "575": 11, "575423": 16, "575462": 92, "57572422": 99, "57585824": 99, "57599221": 99, "575e": 97, "576": 65, "576041": 81, "57633822": 72, "5763996": 63, "57643609": 99, "576535": 81, "576666": 69, "576879": 91, "577": [60, 65], "5770": 92, "57715074": 63, "577271": 91, "5773": 139, "57751232": 93, "5776971": 99, "57775704": 99, "577807": [81, 82], "577884": 93, "5778949": 139, "5780788": [124, 127], "578081": 93, "578307": 96, "578557": 82, "578847e": 83, "579074": 69, "579080": 76, "579128": 92, "57914935": 64, "579197": 88, "579213": 100, "579237": 69, "579238": 83, "579322e": 92, "579521": 95, "57981566e": 139, "57e": 64, "58": [31, 64, 76, 92, 100, 111, 139, 156, 172], "5800": 93, "58000": 92, "580367": [124, 125], "5804": [65, 93], "580414": 100, "580751": 92, "580922": 85, "581675": 76, "581880": 90, "582": 97, "582031": 92, "582263e": 81, "582331": 93, "582386": 71, "582506": 81, "582679": 81, "582681": 99, "582687": 82, "582754": 98, "582761": 83, "582979": 86, "582991": 92, "583076": 93, "583195": [81, 82], "5833333": 65, "583534": 96, "584180": [124, 129], "584849": 83, "58491548": 72, "584943": 81, "5852": 93, "585402": 77, "585478": 81, "585479": 88, "585628": 82, "585793": 83, "585794": 82, "58631757": [124, 127], "586362": 96, "5865652": 139, "5867": 93, "5868472": 63, "587135": 82, "587144": 139, "587230": 82, "587366": [124, 127], "58749687e": 139, "5875": 62, "587594": 93, "588": 171, "58812": [112, 121], "5882": 92, "588364": 24, "588373": 82, "588396": 83, "5883964619856044": 83, "588513": 72, "58872294": 72, "588741": 81, "588785": 69, "589184": 76, "58930305": 72, "589619e": 81, "589832": 82, "589835": 82, "59": [69, 76, 82, 111, 124, 139, 156], "590": 93, "590098": 83, "590320": 68, "590467244372398": 76, "5905": 92, "5905226": 139, "590736": 96, "590738": 96, "590813": 96, "590880": 77, "590905": 85, "590911": 83, "590991": 83, "591080": 68, "591156": 72, "59123": 72, "591652": 92, "591678": 92, "59172611": [124, 127], "591917": [112, 113, 116], "591921": [124, 125], "592626": 82, "592631": [124, 129], "592681e": 83, "593004": 70, "59347351": 139, "593517": 82, "59352911": [124, 127], "593725": 69, "594": 11, "594316e": 96, "594374e": 81, "595353": 83, "595398": 72, "595422": 111, "596": 93, "596076e": 93, "596233": 69, "596270": [85, 86], "596504": 81, "597": 64, "597051": 111, "597263": 81, "597691": 81, "59769369": [124, 127], "597923": 93, "59798": 93, "598371": 93, "598457": 85, "59854797": 124, "5985730": 64, "5989": 124, "598918": 82, "599457": 70, "599586e": 90, "5cb31a99b9cc": 65, "5d": [83, 96], "5x_2": 68, "5x_3": 68, "5z_i": 96, "6": [11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 41, 42, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 104, 106, 108, 109, 111, 112, 113, 114, 116, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 168, 170, 171, 172], "60": [63, 66, 67, 77, 83, 93, 94, 96, 101, 111, 139, 156, 171], "600": 91, "6000": 93, "6000000000000002": [83, 93, 96], "600000e": 93, "60013741": [124, 129], "600252": [111, 112, 114, 124], "600254": 95, "600353": 124, "600445": 76, "600573e": 72, "600776": 78, "600864716": 139, "601": 64, "601061": 83, "601109": 77, "601158": 76, "601476": 111, "60152186e": 139, "60154": 72, "60155": 72, "601598": 91, "601654": 86, "601757": 93, "602168": 83, "602337e": 81, "602587": 96, "602628": 83, "603206": 82, "60337339": [124, 127], "603415": 72, "603824": 69, "604053e": 81, "604110": 93, "6043": 77, "604532": 93, "604841": [92, 93], "605": 93, "605195": 86, "60527433": 70, "605646e": 82, "605766": 72, "60582906": 70, "60600937": 70, "606034": 96, "606129": 96, "606342": 83, "606494": 72, "60655423": 70, "606658": 72, "606733": 93, "6068": 63, "606800": 91, "60690661": 72, "606954": 83, "60712561": [124, 127], "607618": 96, "60771207": 139, "608": [84, 97], "608177": 77, "608283": 86, "6083654": 72, "608442": 82, "60857": 62, "608818": 99, "60883043": [124, 127], "60883703": 70, "60884768": 70, "60885482": 70, "60887009": 70, "609194": 69, "609575": [12, 124, 126], "609947": 77, "61": [24, 69, 78, 94, 111, 139, 156, 172], "610318": 78, "610402": 94, "611269": 91, "61170069": 97, "611802": 69, "611995": 93, "612246": 88, "61250258": [124, 127], "612792": 93, "613": 97, "6133": 64, "613408": 96, "613498": 93, "613574": 87, "613691": [12, 124, 126], "613950": 83, "614": 97, "614188": 91, "614366": 72, "61448202": [124, 127], "61458483": 124, "614870": 81, "615": 78, "615180": 76, "615295": 72, "61575684": 97, "615849": 82, "615863": [85, 86], "616086": 93, "616116": 93, "61635078": [157, 163], "616351": [157, 163], "616372": 92, "616828": 93, "617277": 111, "6173": 65, "617339": [112, 116], "617379": 95, "617481": 93, "617780": 111, "618": 97, "6180": 77, "618069": 92, "61810738": 64, "618327": 82, "618722": 93, "618753": 93, "618823": 173, "619294": 78, "619351": [81, 82], "619390": [81, 82], "619454": 68, "619613": 92, "61e": 64, "62": [78, 86, 87, 111, 139, 156], "620026": 96, "620156": 96, "620874e": 124, "620995": 101, "621060": 77, "621094": [92, 93], "621359": 111, "6214": 173, "621490": 96, "6215": 92, "621953": 96, "62195343": 96, "622": [93, 97], "622153": 93, "622272": 78, "6224": [63, 173], "622502": 90, "62254809": 72, "622668": 69, "623024": 83, "623031": 81, "6232": 77, "623209": 72, "62355931": 72, "623693": 81, "6236967": [124, 127], "624": 91, "6240": 99, "6241": 93, "624320": 16, "624335": 95, "6243811": 63, "624798": 92, "624894": 111, "624919": 93, "625": [63, 91], "625002": 93, "625147": 72, "625159": 87, "625477": 96, "625888": 77, "625891": [85, 86], "626299": 69, "62642034": 72, "626433": 96, "626470": 82, "62651849": 139, "626765": 93, "62697608": 72, "6270": 173, "627031": 81, "627091": 69, "627319": 90, "627424e": 82, "627433": 72, "627505": [85, 86], "627560": 96, "627564": 83, "627656": 82, "627716": 70, "62799689": 72, "628": 97, "628069": 91, "628213": 16, "628361": 69, "628399": 72, "628538": 81, "628701": 71, "628834": 93, "628902": 82, "629129": 82, "62916": 76, "629319": 93, "629566": 81, "629657": 82, "629813": 72, "629835": 93, "629e": 97, "63": [63, 78, 91, 111, 139, 156, 171, 172], "630124": 76, "630150e": 96, "630545": [124, 127], "630548065": 139, "6306": 173, "630880": 97, "630914": 87, "63117311": 124, "631333": 96, "63138744e": 139, "6318": 92, "632058": 91, "632499": [124, 125], "632747e": 96, "633": 94, "6330631": 156, "633240": 85, "633433": 91, "633543": [124, 125], "634": 97, "634078": 92, "634347e": 85, "634587": 156, "634699": 82, "6349": 77, "63499": 93, "635000e": [92, 93], "635199": [92, 93], "635757": 93, "635889": 82, "63593298": [101, 124], "636048": 124, "636453": 27, "63662533": 72, "636639": 76, "637108": 83, "637240": 93, "637326": 96, "637372": 82, "6379": 92, "63817859": 96, "638179": 96, "638264": 96, "638482e": 82, "638488": 87, "638971": 82, "639": 92, "639135": 91, "63916605": 64, "639198": 82, "639586": 69, "639613e": 72, "64": [78, 86, 92, 93, 94, 97, 111, 139, 156, 170], "640": 93, "640903": 72, "64117907": [124, 127], "641355": 76, "641440e": 124, "641528": 96, "64176852": 139, "642016": 96, "642096": 95, "642329": 78, "64269": 99, "6427": 93, "64340": 99, "643512": 83, "643623": [96, 111], "64362337": 96, "643752": 96, "643939": 78, "644665": 83, "644799": 68, "645": 93, "64526147": 139, "645449": 69, "645583": 78, "64579": 62, "6458": 63, "645800": 91, "646318": 76, "646634": 82, "646937": 68, "646963": 30, "647": 124, "647196": 68, "64723": 99, "647689": 100, "647750": 93, "647873": 96, "64797": 99, "648": 92, "648000e": 93, "648070": 81, "648112": 72, "648267": 111, "648820e": 111, "649": [124, 171], "649146": 81, "649158": 96, "649177": 77, "649380e": 81, "649610": 71, "649897": 124, "649969": 93, "649973e": 81, "65": [75, 78, 83, 87, 93, 96, 97, 111, 139, 156], "650": 84, "6500000000000001": [83, 93, 96], "6502": 93, "65022913": 72, "650376": 82, "650483": [124, 129], "650867": 83, "650874": [124, 129], "65104917": 72, "651662": 93, "651694": 81, "651863": 69, "651919": 96, "652": [60, 94], "65211887": 72, "6522": 171, "652247": 82, "652324": 78, "652350": 91, "652450e": [92, 93], "652667": 85, "6527": 84, "652773": 69, "652778": 91, "652814": 81, "653": 173, "653008e": 92, "653320": 72, "65334211": 72, "65337084": 72, "653771": 82, "653829": 88, "653846": 83, "653900": 111, "653901": [81, 82], "654070e": 124, "654191": 77, "654755": 68, "654958": 82, "654964": 90, "6551": 70, "655284": 96, "6554": 171, "655462": 82, "655547": 81, "655858": 86, "655959": 88, "657": [65, 124], "657283": 99, "657310": 82, "657431": 76, "657857": [124, 126], "658267": 96, "65843157": 139, "6586": 62, "658612": [105, 109], "659": 65, "659245": [81, 82], "659269": 81, "659387": 71, "6593871": 62, "659423": [81, 82], "659473": 100, "659755": 97, "659799": [105, 109], "659842e": 81, "66": [78, 88, 94, 111, 139, 156, 170, 172], "660": [65, 124], "660128": 83, "660479": 15, "6607402": 97, "660776": 96, "660e": 97, "661145": 93, "661166": 37, "66133": 124, "661338e": 93, "661884": 81, "66199596": 71, "66208231": 71, "662109": 82, "66226424": 71, "6625": 93, "662616": 81, "662696": 82, "662961": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "663115": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "663177": 78, "663182": 83, "6634357241067572": 100, "663529": 96, "663533": 93, "663567": 82, "663672": 88, "664": 173, "66411161491": 173, "664846": 91, "665264": 96, "66529": 124, "665393": 72, "665464": 111, "665602": 90, "66584997": 72, "665868e": 93, "665974": 90, "66601815": [124, 126], "666062": 72, "666104": 96, "666307": 68, "6666667": 65, "666750": 93, "666959e": 88, "667": [91, 97], "667115": 70, "667196": 82, "667285944503316": 83, "667286": 83, "667294": [124, 129], "6674": 139, "66748884": 72, "667536": 96, "667682": 67, "667860": 82, "667985": 87, "66802851": 72, "668044": 72, "668446": 87, "66853233e": 139, "668584": 68, "66858989": 72, "66875978": 72, "669": 120, "669024": 81, "669130": 90, "669774": 85, "669809": 81, "67": [60, 65, 77, 92, 100, 111, 139, 156, 170], "670867": 29, "671066": 82, "671104": 111, "671162": 72, "671271": [81, 82], "671420": 90, "671633": 93, "67199": 93, "672": 60, "672014": 81, "6722": [65, 77], "672234": [81, 82], "672384": [81, 82], "67245350": 63, "672684": 69, "673092": [81, 82], "673302": 91, "6734628878523097": 83, "673463": 83, "673550": 69, "673581815999206": 83, "673582": 83, "673586": 69, "67410934": 63, "674181": 93, "6745349414": 63, "67456": 100, "674609": 83, "675011": 83, "675011328023803": 83, "675451": 76, "675987": 69, "67618978": 67, "6762884214": 139, "676405": 83, "6765": [64, 92], "676532": 82, "676536": 156, "676570e": 72, "676572": 81, "67674909": 67, "676756": 96, "676767": 81, "676807": 92, "677614": 96, "677739": 81, "677870": 69, "677980": 83, "678": 97, "678083": 72, "678104": 77, "678117": 93, "678540": 77, "678695": 72, "678781": 81, "678826": 83, "678872": 72, "678953": 77, "678954e": 82, "678972e": 81, "67913613": [124, 129], "679210": 72, "67923975": 71, "67936506": [124, 126], "679486": 77, "679539": 91, "67969173": 71, "67987456": 71, "67ad635a": 65, "68": [65, 78, 98, 99, 111, 139, 156], "680": 93, "680366": 93, "6807": 77, "680889": 82, "681": 91, "681029": 82, "6810775": 99, "68111838e": 139, "681176": 91, "681817dcfcda": 65, "68183347": [124, 127], "682315": 93, "6826": [38, 92], "683140": 81, "683331": 83, "683428": 82, "683487": 82, "683637e": 88, "683867e": 139, "683942": 96, "68410364": 64, "68411700": 64, "684502": 96, "685107": 96, "685143": [82, 90], "685807": 96, "686030": 81, "686346": 72, "687034377": 139, "687068": 124, "687265": 81, "687345": 96, "68742384e": 139, "687449": 82, "687647": 96, "687719": 124, "687854": 68, "687871": 91, "6878711": 63, "688": 171, "688207": 69, "688286": 82, "688537": 111, "688635": 111, "688886": 111, "688918": 93, "689088": [81, 82], "689188": 68, "689260": 81, "689392": 96, "689542": [105, 109], "689772": 70, "69": [87, 111, 139, 156, 172], "690501": 69, "690536": 93, "69070226": 71, "69094586": 71, "6909926": 71, "691085": 72, "691111e": 81, "691511": 92, "691539": 81, "691761": 93, "6921": 77, "692159": 82, "692662": 82, "692725": 96, "692907": [72, 93], "692953": 71, "6930": 77, "693082": 82, "693345": 93, "693359": 93, "69336100e": 139, "6934117220290754": 83, "693412": 83, "693495e": 93, "693796": 91, "693837": 77, "69402886": 69, "69404895": [124, 129], "694154": 83, "69434472": 69, "694356": 81, "694561": 97, "694755": 93, "694919": 91, "695": 60, "69505403": 124, "69520523": 67, "69537129": 69, "6955": 93, "695581": 87, "695582": 90, "695720": 81, "695867": 69, "696011": 28, "696289": [85, 86], "696357": 81, "696490": 82, "69684828": 124, "697": 91, "697000": 83, "697118": 96, "697420": [85, 86], "697486": 82, "697692": 72, "698223": 68, "698244": 68, "698450": 16, "69850969": [124, 127], "69859288": 139, "698694": 91, "698818": 81, "699035": 96, "699082": 83, "699097": 77, "69921": 65, "699256e": 82, "699259e": 96, "699333": 83, "699610": 72, "699646": 72, "6_design_1a": 84, "6_r2d_0": 84, "6_r2y_0": 84, "6b": 156, "6cea": 65, "7": [12, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 40, 41, 44, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 106, 108, 109, 111, 112, 113, 114, 116, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170, 171, 172], "70": [64, 66, 83, 92, 93, 96, 111, 139, 156, 172], "700": [81, 82, 84, 91], "7000000000000002": [83, 93, 96], "700015": 96, "70007159": 96, "700072": 96, "700102": 96, "700290": 82, "700314": 78, "700596": 96, "700651": 85, "701088": 92, "701106": 87, "701413": 93, "701632": 72, "701672e": 83, "70186108": 69, "70189057": 69, "702500": 93, "703049": 81, "703256": 72, "703325": 88, "703482": [124, 129], "703950": 82, "7040": 93, "704482": 88, "7045": 88, "704558": 88, "704848": 82, "704896": 88, "705074": 81, "705110": 81, "70521233e": 139, "705519": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "705581": 93, "7055958": 102, "705595810371231": 102, "7055958103712310": 102, "705641": 82, "705695": 81, "705784": 69, "70583": 99, "706208": 93, "706244": 81, "70631947": [124, 129], "706645": 83, "706686": 69, "706903": 81, "707101": 83, "707201": 82, "707237": 69, "707868": 96, "707963e": 92, "708190": 91, "708439": 81, "708459": 96, "708821": 78, "708837": 78, "708882": 82, "709026": 68, "709129": 81, "709606": 29, "71": [111, 139, 156, 172], "710059": 78, "710122941464952545960717276828487909497": 139, "7101229414649525459607172768284879094972368212223252755575864677073777889961113181933363739455053566568838586889910045141516172026303435384344486366749395": 139, "71028179": [124, 127], "7104": 69, "711328": 93, "711518": 93, "711589": [124, 127], "711713": 70, "711849": 82, "712072": 95, "712095": 78, "712197": 72, "712457": [124, 127], "712503": 99, "712592": 92, "712818": 81, "712846": 111, "712921": 93, "712960": 83, "713": 93, "713120": 85, "713294": 94, "713531": 81, "713582": 90, "714240": 91, "7142787": 96, "714279": 96, "714321": 92, "714350": 81, "714919": 82, "715": [60, 120], "715013": 93, "71517828": 69, "715179e": 93, "715407": 83, "715477": 69, "7155": 93, "7157": 93, "715764": 16, "7158581": 63, "716": 93, "716098": 78, "7161": 93, "716100": 76, "716110": 82, "716191": 70, "716357": 69, "716456": 96, "716520e": 81, "716615": 78, "716740": 111, "716762": 83, "716793": 83, "716799": 91, "7167991": 63, "71695182": 69, "717185": 96, "717747": 81, "717923": 82, "718219": 82, "718294": 67, "718686": 100, "718914413": 139, "719169": 70, "7193": 93, "719951": 72, "72": [66, 88, 111, 139, 156, 172], "72017804": [124, 129], "72022955": 69, "720333": 81, "72039958": 69, "720447": 77, "720571": 96, "720589": 97, "720664": 91, "721071": 96, "721097": 16, "721118": 90, "7215093d9089": 65, "721609": 93, "721742": 82, "721909": 81, "722": 91, "722316": 96, "7224": 93, "722634": 96, "72279758": 69, "722805": 111, "722848": 83, "723": 65, "72330647": [124, 129], "723314": 96, "723345e": 96, "723689": 93, "723825": 81, "723846": 78, "7239": 93, "723994": 81, "72413544e": 156, "724135e": [140, 156], "7241399": 63, "724174102": 139, "724338": 96, "724498": 77, "724634": 81, "724767": [85, 86], "724912": 81, "724918": 100, "725": 65, "725010": 78, "725031": 96, "725087": 93, "725166": 96, "725202": 81, "72521526": 69, "725232": 111, "725662": 76, "725715": 81, "725848": 69, "726": [65, 97], "726078": 77, "72675261": 69, "7268131": 63, "726995": 82, "7271188": [124, 129], "727324": 69, "72735557": 69, "7273837": 69, "727501": 93, "727543": 68, "727693": 93, "727734": 72, "727780": 93, "72787325": 69, "727976": 83, "728005": 82, "7282094": [124, 126], "728304": 81, "728710": 96, "728e": 97, "7297": 62, "72993015": [124, 129], "729985": 81, "73": [64, 75, 78, 111, 139, 156], "730241": 72, "730629": 90, "730715": 81, "730806": 69, "731297": 111, "73136027e": 139, "731711": 82, "731731": 81, "731754": 83, "731928": 90, "732": 97, "732196": 81, "732525": 82, "732594": 81, "7326": 93, "73285": 27, "732928": 82, "733404": 76, "7338": 69, "733818": 69, "734": 60, "7344": 77, "734580": 82, "734948": 96, "735": 120, "735360": [124, 127], "735587": 77, "735694": 96, "73569431": 96, "73579164": 76, "735901": 81, "735958": 69, "735964": 68, "7360": 70, "73602094": [124, 129], "736082": [81, 82], "73649108": [124, 129], "736593": 111, "7368": 62, "736956e": 82, "7375615": 64, "737796": 93, "737884": 25, "737951": [81, 82], "737955": 69, "738147": 93, "738315": 93, "73866809": 139, "739": 93, "739286": 81, "7393": 77, "739310": 72, "73949306": [124, 129], "739595": 97, "739720": 93, "739817": 87, "74": [31, 64, 92, 111, 139, 156, 172], "740": 91, "740180e": 96, "740339": 77, "740417": 92, "7405": 93, "740505": 78, "740511": [124, 125], "740542": 82, "740571": 69, "7407627053044026": 83, "740763": 83, "740869": 83, "741104": 83, "741380": 97, "741523": 78, "741702": 96, "74189": 65, "742080": 69, "742128": 96, "742365": 95, "742407": 95, "742636": 81, "742907": 96, "743248": 81, "74327678": [124, 129], "743279": 99, "743437": 82, "743529": 72, "743533": 86, "743591": 81, "7437": 93, "743746": 71, "74402577": 96, "744026": 96, "744211": 93, "744236": 99, "745046": 72, "745638": 92, "746359": 82, "746361": 96, "746689": 81, "747023": 76, "747057": 77, "747082": 94, "747089": 81, "747164": 96, "747679": 82, "747846e": 81, "747945": 63, "748241": 82, "748284": 90, "748377": 92, "748513": 93, "7486": 62, "748945": 93, "749370": 82, "74938952": [124, 126], "749854893": 140, "75": [18, 29, 31, 65, 68, 78, 82, 83, 88, 92, 93, 96, 111, 139, 156, 172], "75000": 100, "7500000000000002": [83, 93, 96], "750023": 69, "750275": 93, "75028276": 72, "750701": 78, "751013": 93, "751081": 93, "75114934e": 139, "751194": 77, "751633": 93, "75171": 92, "751710": [83, 92], "751712655588833": 102, "7517126555888330": 102, "751712656": 102, "751840": 81, "752091": 70, "752696": 78, "752909": 88, "7533": 92, "753523": 96, "754166": 81, "75466617": [124, 129], "754718": 81, "7548": 100, "754870": 91, "754953": 82, "754991": 111, "754996": 81, "755721": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "755885": 111, "755919": 81, "7560824": 63, "756200": 78, "756539": 77, "756585519864526": 83, "756586": 83, "756805": 91, "756867e": 93, "756969": 83, "757": [97, 171], "757136": 93, "757151": [81, 82], "757448": 72, "757559": 88, "757596": 83, "757690": 96, "757819": 91, "758027": 93, "75805060e": 139, "758340": 77, "75887": 65, "758890e": 81, "75899873": [124, 129], "759": 97, "759004": 85, "759087": 82, "76": [111, 139, 156, 171, 172], "760104": 96, "760155": 93, "76023347": [124, 129], "760386": [12, 124, 126], "760778": 91, "760868": 76, "760915": 68, "761": [63, 91], "761224": 78, "761317": 77, "761714": 83, "762049": 77, "762280": 69, "762284": 96, "76228406": 96, "76278669": 72, "763573": 82, "764093": [81, 82], "764315": 96, "7644": 69, "764859": 67, "764905": 69, "764953": 92, "765144": 71, "765363": [81, 82], "765500e": [92, 93], "7656": [62, 93], "765710e": 101, "765792": 96, "76591188": 63, "766005": 96, "76608187": 70, "766148": 82, "766499": 96, "766585": 93, "766741": 82, "7669": 93, "766940": 78, "767027": 77, "767188": [85, 86], "767435": 100, "76796798": 139, "768000e": 82, "768132": [124, 129], "768273": [85, 86], "768798": 78, "769063": 93, "769361": 96, "769402": 72, "769555e": 76, "769805": 96, "77": [97, 111, 139, 156], "770051": 69, "770508": 81, "770556": 93, "770576": 72, "770790": 82, "770944": [85, 86], "770981": 69, "7710": 99, "7711": 62, "771161e": 81, "771167": 156, "771179e": 81, "7712": 77, "771275": 93, "771383e": 93, "7714": 94, "771463014326052": 76, "7715": 62, "771529": 90, "7716982": 64, "77172088e": 139, "771844": 111, "771876": 93, "771965": 93, "771986": 67, "772157": 88, "772322": 173, "77255404e": 139, "772612": 96, "77261209": 96, "772791": 93, "772944": 82, "772998": 94, "773": 65, "773077": 81, "773177": 83, "773202e": 139, "773206": [124, 129], "7733": 62, "773339": 88, "773617": 69, "774526": 69, "774683e": 91, "774751": 72, "775": [65, 93], "775009": 124, "77507927": [124, 129], "775117": 69, "775167": 72, "775191": [81, 82], "775969": 99, "775995": 81, "77628228": 139, "7763": 92, "776385": 69, "776439": 82, "776880": 72, "776887": 92, "776901": 72, "777297": 26, "777435": 72, "7776071": 63, "777728": 24, "777867": 88, "777e": 97, "778403": 81, "778e": 97, "779": [97, 124], "779005e": 81, "779068": 88, "779105": 69, "779266e": 81, "779517": [81, 82], "779682": 83, "779727e": 93, "78": [85, 97, 111, 139, 156, 172], "780": 65, "780458": 96, "780487": 82, "780676": 69, "7808": 70, "780856": 92, "780943": 96, "781076": 93, "781129": 70, "781630": 82, "781681": 96, "781770": [124, 129], "782": 65, "782159": 93, "782252": 70, "782643": [81, 96], "782662": 72, "78279118e": 139, "783": [65, 93], "783276": 124, "783292": 94, "783573": 81, "784": 156, "784238": 91, "784364": 82, "78439014e": 139, "784814": 71, "784841": 93, "784872": 78, "784911e": 82, "785": 65, "7850": 62, "785107": 83, "785815": 78, "785911": 96, "785979": 90, "786": 65, "786106": 72, "786191": 87, "786396": 70, "786446": 81, "786538": 72, "786744": 83, "78682938": [124, 127], "786986": 78, "787010": 69, "78777": 99, "787795": 16, "787912": 69, "787959": 82, "788": 171, "788071": 81, "788169": 69, "78818": 65, "788267": 90, "788385": 111, "789": 124, "789227": 111, "78929066": 69, "789355e": 99, "789464e": 81, "7895685853": 139, "78980657": [124, 127], "79": [78, 111, 139, 172], "790": 92, "790000e": 93, "790115": 93, "790142": 90, "790395": 82, "790723": [85, 86], "7910908091075142": 83, "791091": 83, "791097": 92, "791241": 96, "791297": 29, "7914": 69, "791546e": 82, "791586": 82, "792011": 69, "792396": 78, "792529": 71, "792939": 83, "793030": 111, "793208": [124, 126], "793297e": 93, "793315": 111, "793570": 96, "793735": 96, "793818": [81, 82], "794": 124, "794119": 93, "794148": 71, "794366": 93, "794501": 72, "794755": 93, "794805": 85, "795": 97, "795321": 72, "795493": [124, 125], "795647": 96, "795932": [112, 118], "7963": 93, "796340": 93, "796574": 71, "7966": 70, "796646": 70, "796711": 72, "796e": 97, "797175": 81, "797280": 96, "797323": 91, "797340": 94, "797617": 16, "797720": 72, "797743": 156, "7978": 70, "797966": 77, "797971": 156, "798": 120, "798090": 82, "798159": 82, "798308": 92, "798573": [124, 129], "798685": 26, "798783": [85, 86], "798915": 94, "799403": 96, "799416": 124, "799466": 82, "799744": 81, "7b428990": 65, "7x": 96, "8": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 52, 55, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 104, 109, 111, 112, 113, 114, 116, 117, 118, 123, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170, 172], "80": [66, 67, 76, 77, 83, 93, 96, 101, 111, 139, 172], "800": 91, "8000": [36, 66, 101], "8000000000000002": [83, 93, 96], "800000e": 93, "8001": 70, "800189": 81, "800668": 76, "80103467": [124, 129], "801130": 93, "801186": 69, "802": 97, "802165": 69, "80269403e": 139, "803300": 81, "803492e": 96, "80386488": [124, 129], "803889e": 93, "804140": 111, "804219": 96, "804284": 99, "804316": 96, "804317": 173, "804549": 93, "8048": 64, "804828": 96, "80488332": 139, "805007": 91, "805153e": [92, 93], "805155": 71, "8055563": 63, "805663": 81, "805720": 96, "8059": 92, "806167": 93, "806220e": 93, "806356": 93, "806423": 81, "806732": 87, "807011": 82, "80715126e": 139, "80783734e": 139, "807877": 72, "807879": 96, "808": [64, 156], "808246": 92, "808347": 67, "808640": 93, "8087815": 69, "809": 93, "809278": 67, "809280": 81, "809356": 81, "80947286e": 139, "8095": 94, "809751": 82, "809913": [81, 82], "80a8": 65, "81": [63, 81, 84, 87, 111, 139, 172], "810044": 92, "810118": 76, "810134": 96, "8102": 92, "810306": 78, "810382": [92, 93], "810632": 81, "810650": 96, "81073717e": 139, "810916": 93, "810960": 76, "811095": 69, "811155": 87, "81128199": 96, "811282": 96, "811329": 76, "811458": 92, "8116912": 156, "811825": 91, "811901": 96, "81190107": 96, "811962": 72, "812": 97, "812278": 72, "812418": 71, "8132463": 63, "813285": 72, "813293": 96, "813342": 156, "813682": 93, "813894": 82, "814": 60, "814351": 83, "814438": 85, "814717": 83, "814913": 91, "815224": 156, "815226": 15, "815391": 81, "815993": 96, "8160": 93, "816176": 100, "816259": 70, "816318": 91, "8164": 62, "816432": 81, "816648": 71, "816773": 72, "81750838": [124, 129], "817705": 81, "817967": 124, "817977": 77, "818029": 93, "81827267": 96, "818273": 96, "818289": 96, "81828926": 96, "818380": [81, 82], "81856": 65, "819223": 111, "82": [77, 100, 111, 139, 172], "82018439e": 139, "8202": 64, "820366": 91, "820793": 94, "8208": 93, "8209": 64, "820963": 78, "820993": 93, "821": 171, "8210": 64, "821009": 93, "821021": 83, "821406": 72, "8215": 62, "821566": 96, "822289": 92, "822482": 83, "8226": 77, "8227": 93, "822822": 83, "822932": 69, "823247": 96, "823273": [81, 82], "82329138": 75, "82372418": [124, 127], "824": 120, "824126": 69, "824350": [81, 82], "824499": 69, "824536": 82, "824701": 83, "824750": 83, "824889": 83, "824940": 72, "824961e": 93, "825617": 91, "8260": 92, "826065": [81, 82], "826074e": 91, "82615777": 99, "826158": 99, "826366": 76, "826426": 15, "826492": 96, "826519": 29, "826666": 69, "826897": 96, "827381": 96, "827414": 77, "8274607": 72, "827735": 96, "827778e": 82, "827874854703913": 83, "827875": 83, "827938162750831": [85, 86], "828100": 81, "828157": 78, "828618": 88, "82865920e": 139, "82886598": [124, 129], "828915": [85, 86], "829038": 37, "829543": 83, "829616": 82, "829728": 111, "83": [76, 111, 139, 172], "830073": 72, "830134": 72, "830142": 89, "830273": 78, "830301": 95, "831": 97, "831019": 83, "831330": 81, "831996": 81, "832086": 96, "83227019": 99, "83272569": 69, "832875": 96, "83287529": 96, "832965": 93, "832971": 71, "832991": 16, "833018": 93, "833024": 91, "833065": 78, "833096": 93, "83313309e": 139, "833189": [124, 125], "833439": 69, "833563": 90, "8337": 124, "833898": 81, "833907": 91, "83392588": 72, "834133": 88, "834593": 81, "8350": [77, 93], "835155": 69, "835344": 78, "835750": 88, "835839": 93, "836234": 124, "836397": 81, "837022": 76, "837327": 72, "837971": 72, "83807851e": 139, "838114": 96, "838235": 94, "83854057": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "838883": 77, "83906353": [124, 127], "83922147e": 139, "839321": 69, "8397": 62, "839784": 82, "84": [65, 87, 97, 111, 124, 139, 172], "840041": 93, "840303": 96, "84030318": 96, "840718": [12, 69, 124, 126], "840836": 96, "840995e": 92, "841": [63, 91], "841132": 92, "8415": 64, "841712": 93, "841847": 93, "841910": 81, "84199217": 139, "842": 94, "842051": 82, "842205": 93, "842405": 83, "842625": 91, "842746": 96, "8428": 92, "842853": 96, "8429": 77, "843": 93, "843018": 111, "843296": 93, "843439": 124, "843515": 69, "843569": 81, "843706": 69, "843730": 91, "844002": 111, "844243": 70, "844247": 94, "844308": 96, "8445": 75, "844549": [85, 86], "844556": 82, "844663": 78, "844673": 156, "844707": 96, "844893": 91, "845166": [124, 126], "8452": 77, "845241": 97, "84538481": [124, 127], "8461": 77, "846602": 93, "846682e": 139, "846707": 93, "8470572": 72, "847136": 83, "847595": 28, "847948": 83, "848": 97, "848578": 93, "848659": 69, "848757e": 92, "848868": 83, "848997": 93, "84913138e": 139, "849257": 81, "849427": 111, "849747": 99, "849766": 93, "849797": 72, "8497f641": 65, "8498": 93, "84e": 139, "85": [34, 75, 83, 87, 93, 96, 101, 111, 139], "8500000000000002": [83, 93, 96], "850038": 78, "85013943e": 139, "850321": 91, "850575": [81, 82], "850656": 88, "851": 171, "851012": 90, "851100": 78, "8513": 65, "851366": 91, "851596": [124, 127], "851723": 86, "852012": 71, "852016": 96, "852592e": 90, "853303e": 82, "854": 60, "85402594": 75, "854593e": 82, "85479416": [124, 126], "854978": 81, "855": 93, "855137": 111, "855780": 96, "856298": 82, "856552": 78, "856797": 70, "857161": 96, "857494": 111, "857544": 91, "85795664": 72, "858073": 82, "858577": 95, "858635": 90, "858952": 78, "859": 93, "859112": 69, "859129": 39, "859171": 82, "859694e": 81, "8597": 92, "85c5": 65, "85e": 64, "86": [76, 111, 139, 172, 173], "860340376": 139, "860369": 82, "860378": 93, "860663": 156, "860804": 96, "861019": 78, "8611376": 173, "86130123": [124, 127], "861466e": 81, "861635": 71, "862043": [85, 86], "862083": 82, "862359": 83, "86236584": 69, "862966e": 82, "86343": 76, "863430": 76, "863772": 92, "863982270": 140, "864": 97, "86415573": 64, "8643": 62, "8644": 65, "864404": 93, "864541e": 81, "864550": 69, "864664": 78, "8648": 77, "864994575": 139, "865101": 77, "86516019e": 139, "865313": 93, "865442": 90, "865562": [81, 82], "865860": [92, 93], "866102": [81, 82], "86610467": 96, "866105": 96, "866179899731091": 102, "866179900": 102, "866579": 93, "866670e": 81, "866798": 93, "867187": 76, "867564": 82, "867565": 96, "8679": 93, "868": 65, "8681": 75, "868111": 81, "868211": 69, "868558": [140, 156], "86855811": 156, "86857726": [124, 127], "869": [65, 97], "869020": 83, "86924": 111, "869427": 38, "869586": 87, "869590": 111, "869738": 82, "869778": 95, "87": [64, 81, 87, 91, 111, 139, 172], "870": 71, "8700": 64, "870099": [85, 86], "870106": 82, "870239": 81, "870260": 96, "870291": 72, "870332": 96, "870444": 92, "870663": 81, "8707": 70, "870825e": 81, "871": 65, "871080": 77, "871972": 88, "872": 97, "872354": 96, "8725": 62, "872528": 69, "872768": 96, "872852": 96, "872862": 173, "87313366": [124, 127], "873198": 93, "873677": [85, 86], "873848": 71, "87384812361": 62, "87384812362": 62, "873941": 70, "87430335": 156, "874303353": 156, "874702": [85, 86], "87489418": [124, 129], "875044": 70, "8751": 93, "875404": 82, "8759": 93, "876": [97, 120], "876111": 69, "876233": 71, "87623301": 62, "87640238": [124, 127], "876431e": 83, "8771": 93, "877153": 93, "8772542231": 139, "877287": 16, "877446": 111, "877455": 95, "877833": [81, 82], "877903": 78, "878122": 90, "878281": 96, "878395e": 72, "878402": 81, "878746": 78, "878802": 93, "878895": 78, "879025": 71, "879103": 83, "879349": 81, "87948306": 139, "87969234e": 139, "879970": 71, "87e": [64, 173], "88": [41, 64, 76, 87, 97, 111, 139], "880106": 91, "880579": 96, "880591": 95, "8808108": 69, "880886": 92, "8810": 92, "881201": 93, "881465": 68, "881509": [124, 129], "881706": 82, "881714": 112, "88173062": 63, "88173585": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "881937": 78, "882": 60, "8823": 173, "8824": 173, "882470": 16, "882641": 92, "882666": 117, "88270861": [124, 129], "8828": 62, "882896": 96, "882928": 78, "883009": 82, "883050": 82, "883301": 83, "883326": 81, "883383e": 72, "883435": 81, "883622": 96, "883863": 72, "883914": 83, "8843": 99, "884667": 86, "884821": 111, "884996": 83, "8850": 64, "885065": 96, "885513": 93, "8855716": 69, "885832": 97, "885978": [85, 86], "88598693": 139, "886086": [81, 82], "886172": 69, "88629": 62, "886544": 81, "886548": 72, "886611": 93, "88664": 65, "887075": 82, "887162": 82, "887212": 93, "887307e": 139, "887556": 83, "887731": 67, "887778": 93, "887832": 82, "888146": 91, "8881461": 63, "888207": 72, "888217": 93, "888343": 69, "888352": 78, "8884": 70, "888423": 93, "888425": [124, 127], "888757": 96, "889293": 96, "8893407232": 139, "889516": 82, "889566": 99, "889638": 78, "889733": 96, "889913": [81, 82], "88ad": 65, "89": [64, 82, 111, 139, 171, 172], "890": [63, 91], "89027368": 156, "890273683": 156, "89035917": 87, "890372": [73, 104, 109, 170], "8903720000100010000010": [65, 104, 109, 170], "8904": 60, "890454": [112, 118], "890548": 76, "890665": 88, "890855": 78, "8909": [63, 92], "890993": 81, "89122132": 139, "891299": 77, "891547": 76, "891606": 92, "891649": 25, "891724": [112, 113, 116], "8918185700": 139, "892": 65, "892314": 69, "8924": 173, "892648": 96, "892796": [81, 82], "892828": 88, "893": 65, "8932105": 63, "893349": 81, "893461": 78, "8936459": [124, 129], "893649": [81, 82], "89372468": 139, "893851": 96, "893884": 93, "894": 65, "894143": 70, "894329e": 93, "8946549": 66, "894704": 70, "894804": 71, "895086": 69, "895106": [81, 82], "895275": 16, "895308": 93, "895333": 96, "895442": 92, "8955398237": 139, "895690": [81, 82], "895768e": 83, "896023": 96, "896182e": 88, "896263": 88, "896761": 70, "896820e": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "897220": 96, "8974": 92, "897641": 81, "8979": 77, "898183": 78, "898722": 96, "898839": 71, "898850": 69, "898993": 69, "899021": 111, "899250": 78, "899460": 96, "899548e": 72, "8996": 75, "899654": 78, "899878": 94, "8bdee1a1d83d": 65, "8da924c": 65, "8e3aa840": 65, "9": [12, 14, 15, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 104, 109, 111, 112, 113, 114, 116, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 169, 170, 172, 173], "90": [42, 64, 66, 67, 83, 93, 96, 101, 111, 139, 172], "9000000000000002": [83, 93, 96], "900021": [112, 118], "900525e": 72, "900654": [124, 125], "900829": 88, "901013": 82, "901148": 96, "90136": 92, "901360": 92, "901526": 87, "901563": 16, "901685e": 81, "901998": 90, "902": 156, "90238547e": 139, "902573": 83, "902920": 111, "903056e": 96, "903339": 83, "903366e": 83, "903418": 91, "903566": [124, 125], "903681": 96, "903731": 90, "903767": [81, 82], "90398686": 69, "904": 94, "9045": 93, "904926": 81, "905352": 81, "905494": 83, "905612": 77, "905637": 72, "905858": 100, "905951": 99, "905981": 72, "906": 139, "906051": 83, "906051023766621": 83, "906083": 81, "906346": 82, "9067": 93, "906716732639898": [85, 86], "906757": 79, "90678968": [124, 127], "90700068": [124, 129], "907115": 96, "90715914e": 139, "907176": 96, "907408": 82, "907491": 83, "90755156": 69, "907801": 91, "907879": 78, "90794478": 156, "907944783": 156, "908024": 100, "90824073e": 139, "90882487": 69, "909225": 72, "909304": [81, 82], "909571": 78, "909942e": 24, "909997": 92, "91": [76, 98, 111, 139, 172], "910": 60, "910059": 82, "910152": 82, "9109": 65, "911026": 82, "91102953": 96, "911030": 96, "9112": 92, "911464": 82, "911529": 72, "91172445": 69, "911844": 81, "911853": 72, "912033": 30, "91205901": 69, "91220693": 69, "912230": [81, 82], "912495": [124, 127], "9126": 64, "9127": 64, "912740": 81, "912803e": 139, "913": 65, "91315015": 63, "913280": 100, "91339343e": 139, "91345825e": 139, "913585": 88, "913774": 83, "914": 94, "914134": 71, "9142": 93, "9143065172164": 76, "91438767e": 139, "9145": 62, "914598": 78, "915": [64, 65, 92, 93], "915000e": [92, 93], "915195e": 82, "91524510": 139, "915256": 72, "915488": [85, 86], "915724": 82, "916171": 95, "9162": 77, "916236": 62, "916359": 78, "9166667": 65, "916913": 96, "916914": 96, "916921": 93, "916984": 96, "917": 65, "917038": 77, "91757958": 69, "917665": 85, "91771387": 96, "917714": 96, "9179": 93, "917958": 71, "918": 97, "918159": 82, "918227": 83, "919348": 72, "919879": 93, "91e": 64, "92": [69, 98, 111, 112, 121, 139, 172], "920205": 81, "920283": 93, "920337": 86, "920661": 76, "920695": 93, "9209": [62, 77], "920961": 69, "9210": 93, "921061": 100, "92108124": 69, "921184": 82, "921198": 88, "921372": 83, "92164883": 69, "921778": 88, "921913": 91, "921956": [81, 82], "921e4f0d": 65, "922619": 81, "922668": 88, "922996": 91, "923084e": 83, "923517": 101, "923607": 96, "92369755": 63, "923816": 72, "924002": 96, "924232e": 90, "9243": 93, "924396": [85, 86], "924443": 78, "924540": 90, "924634": 68, "924724": 83, "924732": 93, "9248": 65, "924821": 83, "924850": 82, "9251": 62, "925169": 81, "925248": [85, 86], "92566": 124, "925736": 83, "92585841": 69, "925994": 92, "926": 94, "926355": 72, "926621": 83, "926629": 85, "926685": 91, "927": 61, "927074": 96, "9271": 62, "927232": 93, "9274": 93, "927481": 81, "927956": 76, "928269": 93, "92827999": 124, "928482": 72, "928546": 81, "928608": 71, "92880779": [124, 127], "92905": 63, "929086": 71, "929607": 93, "929636": 76, "929729e": 39, "93": [64, 76, 88, 98, 111, 112, 120, 123, 139, 172], "930243": 71, "930357": 76, "9304028": 63, "930416": 82, "930497": 76, "931": 103, "931169": 69, "931345": 82, "931479": 96, "931825e": 82, "931978": 170, "932027": 83, "932233": 77, "932332": 81, "932404e": 93, "932651": 76, "93271035": 69, "932973": 96, "93300": 124, "933223": 71, "93352177": [124, 129], "933996": 83, "93414861": 69, "934270": 91, "934433": [81, 82], "9345": 65, "934511": 156, "93458": 99, "934992": 83, "935": 75, "935501": 82, "935571": 81, "935591": 96, "935730": 96, "935989": 91, "9359891": 63, "936124": 76, "93648": 101, "9365": 77, "936739": 96, "936946": 71, "937345": 72, "937586": 93, "937749": 82, "937794": 72, "937967": 77, "938": 156, "938179": 69, "9386744462704798": 90, "9388": 93, "938914": 72, "939068": [85, 86], "9395": 93, "939535": 81, "939707": 81, "939731": 72, "939770e": 124, "94": [75, 76, 111, 139, 172], "940": 94, "940253": 70, "9403": 70, "940375": 72, "940445": [124, 125], "941047": 16, "941251": 86, "941364": 77, "941419": 72, "941440": 81, "941709": 83, "9417090334740552": 83, "942": 60, "942062": 82, "942312": 96, "942460e": 96, "942550": 93, "942629": 93, "942661": 91, "94324016": 139, "943355": 82, "943548": 88, "943857": 82, "944045e": 96, "944253e": 96, "944266": [85, 86], "944280": 93, "944317": 93, "944630": 96, "944645": 69, "94473": 66, "944733": 82, "945258": [124, 127], "946165": 82, "946180": [81, 88], "94629": 101, "946297": 83, "946406": 86, "946433": 96, "946968": 83, "947391": 76, "947440": 95, "947855": 78, "9480": 93, "948112": 97, "94815546": 69, "9485": 62, "948526": 82, "948644": 76, "948975": 87, "949042": 72, "94906344": 63, "949071": 71, "94919837": 69, "949241": 156, "949456": 96, "94960336": 69, "9497455": [124, 127], "949773": 72, "949858": 69, "949866": 82, "95": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 51, 62, 64, 66, 67, 68, 69, 70, 71, 72, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 92, 93, 96, 97, 99, 100, 101, 111, 124, 139, 156, 157, 163, 172, 173], "9500": 93, "950144": 82, "95015854e": 139, "9502413": 69, "950280": 96, "950542": 81, "950545": 79, "950814": 72, "95091248": 139, "951176": 82, "951382": 124, "951532": 91, "951761842": 139, "951920": 95, "952": [64, 97], "9523": 62, "952456": [124, 129], "952572": 77, "952682": 82, "952839": 96, "95305": 66, "953175": 81, "9532": 93, "953683": 91, "953855": 81, "953884": 124, "953985": 72, "954": 156, "95408703": 139, "954307": 71, "9552": 62, "955541": 29, "955929": 82, "956047": 63, "9561": 62, "956272": 91, "956349e": 82, "956392": 82, "956574": 93, "956786": 71, "956904e": 81, "957": 94, "95724086": 69, "957339": 91, "957375": 91, "957417": 93, "957418": 173, "957479": 83, "9574793755564219": 83, "957745": 83, "9579": 64, "957996": 83, "958": 156, "9580": 64, "958072": 82, "958636": 87, "958670": 111, "95871461": [112, 113], "95916407e": 139, "959433": 69, "959701": 69, "95e": 64, "96": [64, 67, 81, 82, 94, 111, 139, 172], "960049": 78, "960808": 83, "960846": 93, "9609": 62, "961471": 124, "961538": 93, "96179218": 69, "96188988": 69, "961962": 83, "962364": 78, "9625": 93, "962948": 77, "964025e": 96, "964149": 82, "964261e": 91, "964317": 93, "9647": 62, "964914": 16, "965": 67, "965143891": 139, "965211": 72, "965454": 69, "965480": 81, "965531": 124, "965620e": 81, "965754": 99, "96582": [112, 118], "966015": 96, "9660801": [124, 129], "966936": 72, "967321": 82, "967869": 69, "967883": 72, "968030": 82, "968127": 78, "968134e": 96, "968288": 93, "968396e": 139, "968539": 94, "968973": 70, "969": 97, "969675": 81, "969702563412915": 83, "969703": 83, "9699": 92, "969986": [111, 112, 114, 124], "97": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 111, 112, 113, 114, 116, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 170, 172, 173], "970065": 96, "971058": [85, 86], "971132": 93, "971382": 71, "971617": 76, "972051": 93, "972114": 70, "97241289": [124, 129], "972875": 81, "97314470": 63, "973241": 96, "973874": 83, "974021": [124, 126], "974202": 83, "974213": 82, "97441062": [85, 86], "974414": 83, "974491": 77, "97470462e": 139, "97470872": 99, "9748910611": 63, "97499195": 96, "974992": 96, "974993": 82, "975": [75, 81, 82, 85, 86, 88, 94, 124, 127], "975228": 86, "975289": 78, "9753": 65, "975412": 72, "975646": 69, "976088": 96, "976368": 82, "976562": 96, "977189": 81, "977280": [81, 82], "977295": 93, "9773": 93, "977312": 82, "977489": 93, "978": 120, "978000e": 93, "978215": 82, "978305": 72, "978343": 69, "978346": 82, "978555": 71, "9787": 93, "978977": 96, "979": 97, "979013e": 82, "979029": 69, "979135": 111, "979384": 88, "979701": 81, "979966": 88, "97e": 139, "98": [81, 82, 111, 139, 172], "980066": [124, 127], "980150": 69, "9802393": 63, "980286": 82, "980643e": 83, "980855": 82, "981063": 82, "981104": 95, "981106": 72, "981295": 82, "981642": 82, "981672": 83, "982353e": 93, "982417": 83, "982456": 76, "982497": 81, "982815": 93, "983035": 81, "983141": 157, "983192": 96, "983482": 93, "983483": 78, "983809": 81, "98393441": 99, "984083": [85, 86], "984277": 81, "984368": 77, "984562": 96, "984866": 156, "984872": [81, 82], "984937": 83, "985207": [81, 82], "985279": 93, "985569": [69, 93], "986249": 92, "98673": 87, "986936": 81, "987": 94, "9870004": 65, "987175": 77, "987296e": 82, "9875": 62, "98750578": 67, "987528": 81, "988": 94, "988005": 81, "9880384": 65, "988177": 71, "988199": 81, "98824813": 69, "988421": [81, 82], "988463": 96, "989": 94, "989135": 94, "989374": 70, "989561": [124, 127], "989919": 94, "99": [55, 64, 76, 78, 81, 82, 93, 94, 111, 139, 172], "990034": 71, "990210": 93, "990437": 71, "990567e": 93, "991": [65, 94], "991053": 81, "9914": [92, 93, 99], "9915": [64, 92, 93, 99], "991512": 64, "99178825": 69, "991963": [81, 82], "991977": 93, "99232145": 99, "992359": 16, "992582": [81, 82], "992731": 82, "993113": 81, "993173": 82, "993416": 88, "993607": 69, "993660": 81, "993967": 72, "994": 97, "994168239": 63, "994208": 78, "994304": 93, "994332": 79, "9944": 15, "9946": 72, "994615": 77, "9948104": 66, "994864": 93, "994937": 86, "9951": 62, "995248": 96, "995297": 82, "99605792": 69, "9961": 92, "99612579": 69, "9961392": 63, "996310": 69, "996358": 69, "996413": 93, "99686170": 139, "996934": 91, "9970": 93, "997034": 101, "997494": 101, "997571": 91, "997596e": 82, "997621": 83, "9977": 72, "997920e": 81, "997934": [85, 86], "998050": 95, "998063": 79, "998074": 72, "9981": 77, "99883095": [124, 129], "998855": 93, "999": [55, 68, 77, 87, 89, 99], "999120": 95, "999207": 96, "9995": [68, 81, 82], "999596": 37, "9996": [68, 81, 82], "9996553": 64, "9997": [68, 72, 81, 82], "9998": [68, 81, 82], "999860": 81, "9999": [68, 81, 82], "99c8": 65, "A": [10, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 34, 35, 37, 38, 39, 40, 45, 46, 48, 49, 51, 53, 54, 60, 61, 62, 64, 65, 69, 70, 71, 72, 75, 77, 79, 80, 84, 90, 94, 95, 97, 98, 99, 100, 103, 104, 109, 111, 112, 113, 116, 117, 119, 122, 124, 125, 127, 129, 138, 156, 157, 158, 164, 165, 166, 167, 168, 170, 171, 173], "ATE": [26, 30, 31, 64, 73, 77, 78, 88, 92, 99, 100, 111, 124, 134, 138, 140, 148, 157, 165], "ATEs": [76, 77, 78, 94], "And": [66, 94, 101, 157, 159, 160], "As": [61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 75, 76, 77, 78, 80, 81, 82, 83, 84, 87, 88, 90, 91, 92, 93, 94, 96, 100, 101, 102, 112, 122, 123, 124, 127, 128, 129, 139, 140, 142, 144, 146, 156, 157, 158, 167, 173], "At": [18, 31, 40, 63, 67, 68, 70, 75, 76, 78, 87, 91, 93, 96, 173], "Being": 173, "But": [75, 88], "By": [62, 63, 91, 97, 100, 112, 123, 124, 127, 129, 157, 163], "For": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 49, 54, 60, 62, 63, 65, 67, 69, 70, 71, 72, 75, 76, 77, 78, 79, 87, 88, 90, 91, 93, 95, 97, 99, 100, 102, 103, 104, 105, 109, 111, 112, 113, 120, 123, 124, 125, 126, 127, 128, 129, 130, 131, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 167, 168, 169, 170, 173], "ITE": [35, 77, 78], "ITEs": [76, 77, 78], "If": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 53, 54, 55, 56, 61, 63, 67, 69, 70, 71, 72, 75, 76, 77, 80, 81, 82, 88, 91, 93, 97, 103, 104, 109, 111, 112, 123, 124, 126, 132, 140, 141, 142, 143, 144, 145, 148, 156, 157, 158, 159, 160, 161, 162, 163, 166, 167, 168, 173], "In": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 53, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 139, 140, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173], "It": [13, 62, 63, 64, 77, 81, 82, 84, 85, 86, 91, 92, 93, 97, 98, 100, 105, 108, 109, 112, 119, 121, 124, 125, 139, 168, 172], "No": [33, 56, 60, 62, 64, 65, 66, 67, 69, 70, 71, 72, 73, 76, 77, 78, 87, 89, 92, 93, 97, 99, 101, 104, 106, 107, 108, 109, 112, 113, 124, 126, 127, 129, 140, 156, 170, 171], "Not": [124, 130], "Of": [75, 156, 173], "On": [61, 76, 80, 90, 94, 98, 103, 171], "One": [41, 64, 69, 72, 92, 93, 100, 111, 156], "Or": 46, "Such": [100, 112], "That": [46, 173], "The": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 124, 125, 126, 128, 130, 133, 135, 136, 137, 138, 139, 140, 145, 148, 153, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 167, 168, 169, 171, 172, 173], "Then": [18, 83, 96, 98, 124, 156, 157, 167, 168, 169], "There": [64, 92, 100, 124, 130, 169, 173], "These": [25, 64, 65, 70, 74, 76, 90, 92, 95, 97, 99, 111, 124, 173], "To": [12, 14, 15, 16, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 61, 63, 64, 65, 66, 67, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 88, 91, 92, 93, 95, 96, 97, 98, 99, 100, 101, 103, 104, 109, 111, 112, 113, 114, 116, 117, 118, 119, 121, 123, 124, 128, 139, 156, 157, 158, 163, 167, 168, 169, 170, 173], "Will": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30], "With": [34, 81, 82, 112, 114, 171], "_": [9, 12, 14, 15, 17, 19, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 61, 63, 67, 68, 69, 71, 72, 80, 81, 82, 83, 84, 85, 86, 91, 92, 93, 95, 96, 97, 98, 102, 103, 108, 109, 111, 124, 127, 130, 139, 140, 142, 156, 157, 158, 163], "_0": [61, 63, 80, 84, 91, 102, 103, 139, 140, 154, 155, 156, 157, 167], "_1": [17, 18, 19, 31, 35, 40, 66, 94, 101, 140, 154, 155], "_2": [17, 18, 19, 31, 35, 40, 94], "_3": [17, 18, 19, 31, 35, 40], "_4": [17, 18, 19, 31, 35, 40], "_5": [31, 35], "__": [53, 54], "__init__": [90, 98], "__version__": 169, "_a": [140, 142, 144], "_all_coef": 139, "_all_s": 139, "_b": [140, 142, 144], "_compute_scor": 21, "_compute_score_deriv": 21, "_coordinate_desc": 91, "_d": [77, 97, 124], "_est_causal_pars_and_s": 172, "_estimator_typ": 90, "_h": [97, 124], "_i": [61, 66, 80, 96, 101, 103, 124, 130], "_id": 139, "_j": [17, 18, 19, 31, 35, 40, 43, 63, 91, 156], "_l": [112, 121, 123], "_lower_quantil": [69, 72], "_m": [112, 121, 123, 139], "_mean": [69, 72], "_n": [140, 141, 142, 143, 144, 148, 156, 157, 163, 166], "_n_folds_per_clust": 91, "_offset": [112, 123], "_pred": [112, 123], "_rmse": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "_upper_quantil": [69, 72], "_x": 47, "_y": [97, 124], "a0": 90, "a09a": 65, "a09b": 65, "a1": 90, "a3d9": 65, "a4a147": 94, "a5e6": 65, "a5e7": 65, "a6ba": 65, "a79359d2da46": 65, "a840": 65, "a_": [66, 101], "a_0": [41, 44, 140, 151], "a_1": 44, "a_j": [124, 131], "ab": [62, 98, 168], "ab71": 65, "abadi": [10, 67], "abb0fd28": 65, "abdt": [73, 104, 109, 170], "abl": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 61, 75, 80, 93, 94, 112, 115, 157, 158, 167], "about": [14, 16, 64, 67, 68, 75, 77, 92, 98, 124, 168, 170, 173], "abov": [61, 64, 70, 75, 78, 80, 81, 82, 85, 86, 90, 92, 94, 95, 96, 97, 100, 103, 111, 112, 122, 124, 128, 130, 169], "absolut": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 75, 112, 113], "abstract": [21, 62, 63, 91, 140, 168, 172], "abus": [69, 72, 124, 127, 129, 130], "acc": [23, 62], "acceler": 77, "accept": [17, 19, 105, 109, 111, 112, 116, 121], "access": [48, 49, 62, 64, 69, 70, 71, 72, 75, 76, 77, 85, 86, 87, 99, 107, 109, 112, 121, 157, 163, 173], "accompani": 168, "accord": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 64, 66, 67, 70, 76, 78, 80, 83, 92, 96, 97, 100, 101, 112, 123, 124, 125, 130, 156, 157, 159, 160, 161, 162, 164, 165, 173], "accordingli": [66, 67, 75, 90, 92, 97, 101], "account": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 63, 64, 69, 72, 91, 92, 93, 99, 100, 157, 163, 167, 173], "accross": 89, "accumul": [64, 92, 93, 99], "accur": 77, "accuraci": [48, 53, 62, 124], "acemoglu": 171, "achiev": [50, 63, 77, 88, 91, 95, 100, 124, 156], "acic_2024_post": 94, "acknowledg": [64, 65, 92], "acm": 171, "acov": 171, "across": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 64, 76, 77, 92, 94, 173], "action": 172, "activ": [4, 5, 6, 7, 8, 9, 169, 172], "actual": [46, 69, 72, 76, 87, 100], "acycl": [66, 101, 173], "ad": [5, 6, 7, 8, 9, 10, 11, 21, 48, 49, 53, 54, 87, 104, 109, 112, 114, 124, 156, 157, 158, 172], "adapt": [25, 89, 92, 172], "add": [62, 63, 66, 67, 68, 73, 76, 77, 78, 85, 86, 87, 94, 96, 97, 99, 100, 101, 112, 115, 119, 124, 171, 172], "add_trac": 100, "addit": [12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 43, 44, 45, 47, 51, 69, 70, 71, 72, 84, 89, 100, 105, 106, 107, 108, 109, 112, 115, 124, 125, 140, 149, 157, 164, 165, 167, 171, 172], "addition": [31, 40, 72, 76, 78, 83, 93, 99, 112, 115, 124, 139, 156, 157, 163, 170], "additional_inform": 13, "additional_paramet": 13, "address": 100, "adel": 171, "adj": [97, 100], "adj_coef_bench": 100, "adj_est": 100, "adj_vanderweelearah": 100, "adjust": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 52, 55, 63, 68, 88, 89, 91, 93, 99, 100, 111, 124, 130, 156, 157, 163, 171, 172, 173], "adjust_p": 55, "adopt": [67, 124, 126, 130], "advanc": [90, 110, 139, 171], "advantag": [61, 62, 64, 78, 80, 92, 93, 103, 169], "advers": [157, 158], "adversari": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 99, 157, 163, 167], "ae": [61, 63, 64, 66], "ae56": 65, "ae89": 65, "aesthet": 61, "aeturrel": 45, "afd9e4": 94, "affect": [77, 78, 84, 124, 172, 173], "after": [55, 56, 62, 64, 65, 66, 67, 84, 92, 93, 100, 101, 111, 112, 116, 119, 121, 124, 130, 157, 159, 163, 169, 173], "after_stat": 61, "afterward": 76, "ag": [64, 92, 93, 95, 99, 173], "again": [61, 62, 63, 64, 66, 67, 69, 72, 76, 78, 80, 87, 90, 91, 92, 97, 98, 99, 100, 101, 103, 157, 159, 160], "against": [67, 75, 77, 87, 95, 112, 114], "agebra": 111, "agegt54": [65, 73, 104, 109, 170], "agelt35": [65, 73, 104, 109, 170], "agg": [62, 69, 72, 98], "agg_df": [69, 72], "agg_df_anticip": [69, 72], "agg_dict": [69, 72], "agg_dictionari": [69, 72], "agg_did_obj": [124, 125], "aggrag": [124, 125], "aggreg": [13, 16, 62, 102, 125, 139, 172], "aggregate_over_split": 46, "aggregated_eventstudi": [69, 70, 71, 72], "aggregated_framework": [69, 70, 71, 72, 124, 125], "aggregated_group": [69, 72], "aggregated_tim": [69, 71, 72], "aggregation_0": 13, "aggregation_1": 13, "aggregation_color_idx": 13, "aggregation_method_nam": 13, "aggregation_nam": 13, "aggregation_weight": [13, 69, 70, 71, 72], "aggt": 62, "ai": [71, 98, 171], "aim": 97, "aipw": 94, "aipw_est_1": 94, "aipw_est_2": 94, "aipw_obj_1": 94, "aipw_obj_2": 94, "air": [63, 91], "al": [10, 11, 32, 34, 41, 43, 44, 61, 63, 64, 65, 67, 75, 80, 81, 82, 83, 84, 85, 86, 89, 91, 92, 93, 96, 99, 103, 124, 126, 130, 139, 140, 146, 148, 149, 150, 151, 156, 157, 158, 167, 168, 170, 172], "alexandr": [84, 171], "algebra": 124, "algorithm": [60, 62, 63, 65, 66, 67, 69, 70, 71, 72, 75, 78, 80, 83, 88, 91, 93, 96, 99, 101, 110, 112, 113, 122, 123, 124, 126, 127, 129, 139, 140, 156, 172, 173], "alia": [48, 49, 53, 54], "align": [41, 61, 63, 66, 68, 69, 72, 75, 80, 83, 89, 91, 92, 94, 95, 96, 101, 124, 127, 129, 130, 140, 142, 144, 172], "all": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 49, 53, 54, 57, 61, 62, 63, 64, 66, 67, 75, 76, 77, 78, 80, 87, 88, 89, 90, 91, 92, 93, 95, 97, 100, 101, 103, 104, 106, 109, 111, 112, 113, 115, 121, 123, 124, 125, 126, 128, 130, 139, 140, 142, 144, 156, 157, 167, 168, 169, 172], "all_coef": 139, "all_dml1_coef": 102, "all_s": 139, "all_smpl": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 88], "all_smpls_clust": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "all_z_col": [63, 91], "allow": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 53, 54, 64, 69, 72, 77, 78, 92, 93, 97, 109, 111, 112, 113, 124, 139, 140, 156, 168, 172, 173], "almqvist": 171, "along": [77, 112, 118], "alongsid": [105, 107, 109], "alpha": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 42, 44, 61, 63, 64, 66, 69, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 84, 88, 89, 90, 91, 92, 93, 96, 102, 103, 111, 112, 113, 116, 117, 118, 120, 121, 123, 124, 139, 140, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167], "alpha_": [43, 63, 91, 112, 118], "alpha_0": [157, 167], "alpha_ml_l": 84, "alpha_ml_m": 84, "alpha_x": [25, 33, 124], "alreadi": [18, 66, 67, 69, 72, 76, 98, 101, 112, 113, 124, 126], "also": [12, 14, 15, 16, 22, 23, 25, 26, 39, 60, 61, 62, 63, 64, 65, 67, 69, 70, 72, 75, 76, 77, 78, 79, 80, 81, 82, 85, 86, 87, 90, 91, 92, 93, 95, 97, 99, 100, 103, 111, 112, 113, 116, 118, 119, 120, 121, 122, 123, 124, 139, 140, 156, 157, 158, 169, 170, 172, 173], "alter": [63, 91], "altern": [37, 62, 64, 65, 70, 92, 95, 110, 112, 116, 121, 122, 123, 156, 168, 170], "although": 100, "alwai": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 49, 54, 62, 89, 97, 172], "always_tak": [25, 64, 92], "alyssa": 171, "amamb": 91, "american": [42, 94], "amgrem": 91, "amhorn": 91, "amit": [100, 171], "amjavl": 91, "ammata": 91, "among": [64, 84, 92, 93, 99, 100], "amount": [16, 64, 90, 92, 93, 173], "amp": [60, 63, 65, 66, 67, 69, 70, 71, 72, 91, 93, 99, 101], "an": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 41, 48, 49, 53, 54, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 75, 76, 78, 80, 81, 82, 84, 87, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108, 109, 111, 112, 114, 118, 119, 122, 123, 124, 125, 128, 130, 131, 139, 140, 156, 157, 158, 163, 168, 170, 171, 172, 173], "analog": [20, 21, 63, 69, 70, 72, 91, 93, 99, 111, 124, 126, 140, 141, 142, 143, 144, 156, 157, 163], "analys": [104, 109, 173], "analysi": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 47, 61, 63, 64, 80, 91, 92, 93, 98, 103, 110, 111, 124, 129, 158, 163, 167, 168, 172], "analyst": 98, "analyt": [94, 96], "analyz": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 64, 92, 93, 99, 173], "ancillari": 100, "andrea": 171, "angl": 64, "angrist": 94, "ani": [60, 61, 62, 65, 66, 67, 77, 79, 80, 98, 100, 101, 103, 124, 169, 173], "anna": [12, 14, 15, 17, 18, 19, 31, 35, 40, 62, 67, 69, 70, 71, 72, 124, 125, 126, 128, 130, 171], "annal": [156, 171], "anneal": [112, 123], "annot": 61, "annual": 171, "anoth": [61, 62, 63, 64, 75, 80, 90, 91, 98, 103, 112, 123, 124, 132], "anticip": [14, 16, 17, 19, 62, 70, 71, 124, 125, 127, 128, 129, 130], "anticipation_period": [14, 16, 17, 19, 69, 72], "anymor": [63, 91], "aos1161": 156, "aos1230": 156, "aos1671": 156, "ap": [64, 92], "ape_e401_uncond": 64, "ape_p401_uncond": 64, "api": [77, 89, 104, 109, 168, 172], "apo": [22, 23, 76, 77, 131, 145, 164], "apo_result": 77, "apoorva": 172, "apoorva__l": 94, "apoorval": 94, "app": 172, "appeal": 100, "append": [75, 77, 80, 98, 103], "appendix": [34, 36, 66, 69, 72, 99, 101, 157, 158], "appli": [11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 55, 56, 60, 61, 63, 64, 65, 66, 68, 69, 71, 72, 75, 80, 88, 91, 92, 93, 97, 98, 100, 101, 103, 120, 124, 139, 140, 156, 168, 170, 171, 172], "applic": [41, 61, 67, 77, 80, 94, 100, 103, 105, 109, 111, 139, 171, 173], "applicatoin": 70, "apply_along_axi": 95, "apply_cross_fit": [61, 139], "apply_crossfit": 172, "appreci": 168, "approach": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 62, 63, 77, 78, 91, 97, 99, 100, 110, 112, 122, 124, 139, 156, 157, 158, 169, 171, 173], "appropri": [64, 84, 92, 124, 139, 173], "approx": [111, 124, 127, 129], "approxim": [61, 72, 75, 80, 81, 82, 83, 96, 100, 103, 111, 124, 156, 172, 173], "april": 69, "apt": 169, "ar": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 37, 38, 39, 40, 41, 44, 45, 46, 48, 49, 51, 52, 53, 54, 55, 56, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 78, 80, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 109, 111, 112, 113, 114, 116, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 152, 153, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173], "arang": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 68, 80, 83, 89, 93, 95, 96, 99, 100, 104, 109, 112, 118], "arbitrarili": [49, 54], "architectur": [77, 140, 171], "arellano": 171, "arg": [90, 97, 111, 124], "argmax": 98, "argmin": 75, "argu": [61, 64, 80, 92, 93, 99, 103, 173], "argument": [12, 14, 15, 17, 18, 19, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 43, 44, 45, 46, 47, 51, 64, 67, 69, 70, 71, 72, 75, 76, 81, 82, 87, 91, 92, 93, 98, 102, 104, 111, 112, 113, 114, 117, 121, 123, 124, 125, 172, 173], "aris": [61, 62, 63, 70, 80, 91, 100, 103, 173], "aronow": 94, "around": [62, 64, 77, 92, 93, 97, 124, 140], "arr": 95, "arrai": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 48, 49, 51, 52, 53, 54, 55, 66, 67, 69, 70, 71, 72, 75, 76, 77, 78, 80, 81, 82, 83, 91, 94, 95, 98, 99, 100, 101, 102, 103, 105, 107, 108, 111, 112, 113, 114, 117, 118, 139, 156, 157, 163, 170, 172, 173], "arrang": 63, "array_lik": 29, "articl": [45, 168], "arxiv": [43, 62, 63, 91, 98, 100, 168, 171, 172], "as_learn": [65, 112, 120, 122], "asarrai": [81, 82], "aspect": [64, 92, 93], "assert": [112, 121], "assess": 62, "asset": [93, 99, 173], "assign": [4, 5, 6, 7, 8, 9, 17, 19, 55, 64, 69, 72, 76, 77, 89, 92, 97, 104, 109, 111, 112, 123, 124, 125, 138, 157, 173], "assmput": [124, 138], "associ": [64, 84, 92, 124, 156, 171], "assum": [60, 63, 67, 79, 91, 94, 95, 98, 100, 124, 125, 126, 128, 130, 140, 141, 142, 143, 144, 156, 157, 167, 173], "assumpt": [62, 63, 64, 66, 67, 68, 69, 70, 72, 75, 91, 92, 94, 97, 101, 124, 125, 126, 128, 130, 138, 156, 173], "assur": 172, "astyp": [69, 71, 79, 97, 100], "asymptot": [20, 21, 61, 63, 77, 80, 91, 103, 139, 156, 171], "ate": [77, 78], "ate_estim": [66, 101], "ates": [77, 78], "athei": 171, "att": [16, 17, 19, 26, 31, 62, 68, 87, 88, 95, 100, 111, 124, 125, 126, 127, 128, 129, 130, 134, 140, 148, 157, 165, 172], "att_": [124, 128], "att_gt": [62, 70, 71], "attach": 62, "atte_estim": 67, "attempt": [48, 49], "attenu": [64, 92], "attr": 64, "attribut": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 50, 51, 52, 53, 54, 55, 56, 75, 90, 102, 106, 109, 112, 113, 114, 139, 140, 156], "attributeerror": [48, 49], "attrict": 124, "attrit": [30, 66, 101, 124, 138], "au": [65, 112, 122, 168, 170], "auc": 62, "author": [62, 100, 168], "auto_ml": 90, "autodoubleml": 90, "autom": 90, "automat": [13, 16, 61, 69, 71, 72, 76, 80, 87, 103, 111, 157, 163], "automl": 172, "automl_l": 90, "automl_l_lesstim": 90, "automl_m": 90, "automl_m_lesstim": 90, "automobil": [63, 91], "autos": 84, "autosklearn": 90, "auxiliari": [37, 61, 80, 103], "avail": [33, 62, 64, 65, 67, 69, 70, 72, 75, 77, 78, 84, 92, 93, 94, 95, 97, 100, 103, 104, 109, 111, 112, 113, 120, 122, 124, 125, 126, 127, 129, 157, 167, 168, 169, 172, 173], "avaiv": 52, "aver": [76, 78], "averag": [17, 18, 19, 22, 23, 25, 26, 31, 39, 40, 50, 60, 62, 65, 66, 67, 68, 69, 71, 72, 76, 79, 87, 93, 94, 95, 97, 98, 99, 100, 101, 110, 125, 126, 130, 131, 132, 133, 134, 138, 145, 148, 156, 164, 165, 171, 172, 173], "average_it": [76, 77, 78], "avoid": [56, 61, 62, 80, 97, 124, 139, 169, 172], "awai": 99, "ax": [13, 16, 68, 69, 70, 71, 72, 75, 77, 78, 80, 81, 82, 83, 85, 86, 90, 91, 92, 93, 94, 96, 97], "ax1": [77, 78, 83, 88, 93, 96], "ax2": [77, 78, 83, 88, 93, 96], "axhlin": [68, 76, 77, 90, 97], "axi": [13, 16, 63, 64, 75, 77, 78, 84, 88, 91, 92, 94, 95, 97], "axvlin": [77, 78, 80], "b": [12, 14, 15, 16, 17, 19, 45, 61, 63, 65, 80, 81, 82, 91, 94, 96, 97, 100, 103, 111, 112, 122, 124, 130, 156, 157, 167, 168, 170, 171], "b208": 65, "b371": 65, "b5d34a6f42b": 65, "b5d7": 65, "b_": 124, "b_0": 44, "b_1": 44, "b_j": 45, "bach": [75, 84, 90, 100, 168, 171, 172], "back": 77, "backbon": 75, "backend": [5, 6, 7, 8, 9, 62, 93, 99, 100, 104, 106, 108, 110, 112, 117, 172], "backward": [4, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 104, 109, 172], "bad": 94, "balanc": [41, 64, 69, 70, 72, 89, 92, 93], "balanced_r0": 41, "band": [62, 110, 173], "bandwidth": [27, 28, 29, 46, 97, 124, 172], "bar": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 87, 90, 92, 111, 124, 140, 145, 148, 157, 164], "base": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 37, 38, 39, 47, 50, 52, 56, 61, 62, 63, 64, 66, 67, 68, 70, 71, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 107, 109, 111, 112, 115, 121, 122, 123, 124, 125, 128, 130, 138, 139, 140, 156, 157, 158, 163, 168, 170, 171, 172, 173], "base_classifi": 76, "base_estim": [53, 54, 97], "base_regressor": 76, "baseestim": 98, "baselin": [14, 35, 64, 90, 92], "basesampl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "basi": [22, 26, 39, 51, 72, 81, 82, 88, 111], "basic": [62, 63, 64, 67, 71, 91, 92, 93, 94, 97, 99, 100, 110, 112, 122], "basis_df": 88, "basis_matrix": 88, "batch": 65, "battocchi": 171, "batuhan": 172, "bay": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 156], "bayesian": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "bb2913dc": 65, "bbotk": [65, 112, 122, 123, 172, 173], "bbox_inch": 80, "bbox_to_anchor": [80, 89], "bcallaway11": [62, 70], "bd929a9e": 65, "bde4": 65, "becam": [64, 92, 93], "becaus": [49, 54, 60, 61, 62, 63, 79, 80, 87, 91, 94, 98, 100, 103, 173], "becker": [65, 112, 122], "becom": [63, 90, 91, 111, 139], "bee": 68, "been": [13, 16, 63, 64, 69, 71, 72, 90, 91, 92, 93, 99, 100, 111, 112, 119, 120, 124, 130, 168, 172], "befor": [17, 19, 55, 56, 62, 64, 68, 69, 71, 72, 76, 78, 87, 92, 96, 100, 124, 126, 173], "begin": [33, 41, 42, 43, 61, 63, 64, 65, 66, 68, 69, 72, 75, 80, 83, 91, 92, 94, 95, 96, 101, 102, 104, 109, 112, 121, 123, 124, 127, 129, 130, 139, 140, 142, 144, 156, 170, 173], "behav": [69, 71, 72, 98], "behavior": [64, 94, 112, 114], "behind": [70, 124], "being": [17, 19, 20, 21, 35, 36, 47, 50, 53, 54, 63, 72, 91, 97, 100, 124, 129, 130, 139, 140, 146, 156, 157, 163, 168], "belloni": [34, 84, 156, 171], "below": [55, 56, 60, 64, 70, 79, 92, 94, 98, 124, 169, 170], "bench_x1": 100, "bench_x2": 100, "benchmark": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 57, 77, 78, 87, 158, 172], "benchmark_dict": [57, 99], "benchmark_inc": 99, "benchmark_pira": 99, "benchmark_result": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39], "benchmark_twoearn": 99, "benchmarking_set": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 78, 87, 99, 100, 157, 158], "benchmarking_vari": 87, "benefit": [61, 64, 80, 92, 103], "bernoulli": [33, 41], "berri": [63, 91], "besid": 170, "best": [22, 26, 39, 49, 50, 51, 54, 69, 72, 76, 77, 81, 82, 85, 86, 90, 169], "best_estim": 50, "best_loss": 90, "best_param": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50], "best_scor": 50, "beta": [30, 33, 34, 36, 42, 64, 66, 92, 95, 97, 101, 124, 140, 151], "beta_": [66, 101], "beta_0": [32, 66, 89, 95, 101, 111, 124, 135, 140, 151], "beta_a": [31, 40, 100], "beta_j": [33, 34, 36, 42], "better": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 69, 72, 75, 77, 78, 100, 124, 130], "between": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 56, 60, 66, 68, 69, 72, 77, 78, 79, 83, 84, 90, 94, 96, 98, 99, 100, 101, 124, 132, 140, 141, 142, 143, 144, 148, 151, 152, 153, 156, 157, 167, 170, 172], "betwen": [60, 79], "beyond": 171, "bia": [36, 60, 66, 79, 84, 97, 100, 101, 110, 124, 138, 139, 140, 154, 155, 157, 167, 171, 172], "bias": [60, 64, 69, 72, 79, 92, 93, 99, 173], "bias_bench": 100, "bibtex": 168, "big": [84, 102, 139, 140, 141, 142, 149, 156, 157, 159, 160, 161, 162, 165, 166, 167], "bigg": [63, 91, 140, 147, 148, 157, 160, 165], "bilia": 11, "bilinski": 171, "bin": [61, 77, 78, 80, 169], "binari": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 32, 37, 39, 41, 47, 60, 62, 64, 65, 67, 75, 79, 87, 88, 89, 92, 94, 95, 100, 111, 112, 119, 123, 126, 130, 133, 134, 135, 138, 157, 164, 165, 172, 173], "binary_outcom": 47, "binary_treat": [32, 81, 85, 87], "binary_unbalanc": 41, "bind": 172, "binder": [65, 112, 122, 168, 170, 172], "binomi": [79, 94, 95, 96, 98], "bischl": [65, 112, 122, 168, 170], "black": [61, 65, 73, 104, 109, 170], "blob": 62, "blog": 45, "blondel": [168, 170], "blp": [51, 63, 91], "blp_data": [63, 91], "blp_model": [85, 86], "blue": [61, 63, 66, 91], "blueprint": 76, "bodori": 171, "bond": [64, 92, 93], "bonferroni": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 156], "bonu": [11, 65, 104, 109, 170], "book": [65, 98, 100, 112, 119, 122, 123, 171], "bool": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 41, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 87, 97], "boolean": [36, 85, 86, 104, 109, 139], "boost": [60, 64, 67, 69, 72, 75, 76, 77, 79, 92], "boost_class": [64, 92], "boost_summari": 92, "boostrap": [83, 172], "bootstrap": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 51, 69, 70, 71, 72, 78, 81, 82, 83, 85, 86, 93, 96, 110, 111, 124, 125, 139, 140, 168, 170, 172, 173], "both": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 40, 62, 64, 65, 67, 68, 75, 76, 77, 88, 89, 90, 92, 93, 95, 97, 98, 99, 100, 104, 105, 109, 112, 114, 123, 124, 127, 128, 129, 156, 157, 158, 163, 166, 167, 172, 173], "bottom": [63, 64, 75, 91, 92, 93], "bound": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 35, 37, 38, 39, 55, 56, 64, 69, 72, 77, 78, 87, 88, 92, 99, 100, 157, 158, 163, 167, 172, 173], "branch": 65, "brantli": [62, 171], "break": [61, 172], "breve": [140, 151], "breviti": 173, "brew": 169, "brewer": 63, "bridg": 100, "brief": 103, "briefli": 70, "bring": [60, 79], "brucher": [168, 170], "bsd": [168, 172], "bst": 92, "budget": [90, 112, 123], "bug": [168, 172], "build": [63, 75, 91, 95], "build_design_matric": [81, 82], "build_sim_dataset": 62, "built": [52, 90, 112, 123, 168], "bureau": [100, 139, 171], "busi": [36, 43, 63, 91, 100, 171], "b\u00fchlmann": 171, "c": [10, 11, 17, 18, 19, 34, 40, 42, 44, 60, 61, 62, 63, 64, 65, 68, 73, 79, 80, 84, 85, 86, 91, 92, 94, 97, 98, 103, 104, 109, 112, 122, 123, 124, 130, 140, 142, 144, 157, 160, 162, 168, 169, 170, 171, 173], "c1": [10, 11, 44, 63, 84, 91, 98, 103, 168, 171], "c68": [10, 11, 44, 63, 84, 91, 98, 103, 168, 171], "c895": 65, "c_": [16, 124, 127, 129, 130, 140, 142, 144, 156], "c_d": [34, 157, 165, 166, 167], "c_i": [124, 130], "c_y": [34, 157, 167], "ca1af7be64b2": 65, "caac5a95": 65, "calcualt": 95, "calcul": [22, 26, 39, 62, 64, 69, 72, 75, 77, 78, 81, 82, 83, 85, 86, 90, 92, 96, 99, 157, 163, 167], "calendar": [69, 70, 71, 72], "calibr": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 55, 56, 75, 90, 100], "calibration_method": [55, 56], "call": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 49, 53, 54, 60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 76, 77, 79, 81, 82, 83, 85, 86, 92, 93, 95, 96, 97, 99, 100, 101, 104, 109, 112, 114, 123, 139, 140, 156, 157, 163, 167, 170, 172, 173], "callabl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 61, 75, 76, 80, 81, 82, 110, 112, 113, 168], "callawai": [17, 19, 62, 69, 70, 71, 72, 124, 125, 128, 130, 171], "callback": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "camera": 84, "cameron": [63, 91], "can": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 46, 49, 52, 54, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 109, 111, 112, 113, 114, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 130, 131, 138, 139, 140, 141, 142, 143, 144, 145, 148, 151, 152, 153, 156, 157, 158, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173], "candid": 100, "cannot": [75, 97, 100, 124, 173], "capabl": [5, 6, 7, 8, 9, 60, 79], "capo": [22, 88], "capo0": 88, "capo1": 88, "capsiz": [76, 77, 78, 90, 94, 97], "capthick": [76, 77, 78, 97], "cardin": [63, 91], "care": [70, 98, 112, 114], "carlo": [31, 32, 35, 40, 81, 82, 85, 86, 100, 171], "casalicchio": [65, 112, 122, 168, 170], "case": [4, 5, 6, 7, 8, 9, 11, 14, 22, 23, 25, 26, 32, 46, 60, 63, 64, 69, 70, 72, 79, 81, 82, 83, 84, 87, 88, 89, 90, 91, 95, 96, 97, 98, 99, 100, 104, 109, 111, 112, 114, 115, 116, 118, 119, 121, 124, 126, 130, 138, 139, 140, 142, 144, 156, 157, 163, 170, 172, 173], "cat": [61, 172], "catboost": 75, "cate": [26, 39, 51, 88, 110, 172], "cate_obj": 111, "categor": [17, 19], "categori": 77, "cattaneo": [124, 171], "caus": [61, 80, 97, 103], "causal": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 60, 61, 63, 64, 65, 66, 75, 79, 80, 88, 90, 91, 92, 94, 98, 99, 101, 102, 103, 104, 107, 109, 110, 124, 130, 139, 156, 157, 163, 171], "causal_contrast": [23, 76, 77, 78, 88, 124], "causal_contrast_att": 88, "causal_contrast_c": 88, "causal_contrast_model": [77, 78, 124], "causal_contrast_result": 77, "causaldml": 171, "causalml": [98, 171], "causalweight": 171, "caution": 156, "caveat": 100, "cbind": 63, "cbook": [69, 70, 71, 72], "cc": 92, "ccp_alpha": [26, 52, 92], "cd": 169, "cd_fast": 91, "cda85647": 65, "cdf": 111, "cdid": [63, 91], "cdot": [17, 18, 19, 31, 35, 40, 41, 47, 63, 68, 69, 72, 83, 87, 91, 94, 96, 97, 100, 111, 124, 125, 127, 129, 130, 139, 140, 142, 144, 145, 148, 149, 151, 154, 155, 156, 157, 160, 162, 164], "cdot1": 87, "ce": 173, "cell": [76, 90], "center": [69, 70, 72, 77, 84, 89], "central": [139, 172], "certain": [124, 140, 142, 144], "cexcol": 63, "cexrow": 63, "cf_d": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 57, 69, 72, 78, 87, 88, 99, 100, 157, 158, 163, 164, 165, 166, 167, 173], "cf_y": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 57, 69, 72, 78, 87, 88, 99, 100, 157, 158, 163, 164, 165, 166, 167, 173], "cff": 172, "chad": 100, "challeng": [63, 91, 157, 158], "chanc": 72, "chang": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 53, 54, 64, 66, 67, 71, 89, 93, 99, 100, 101, 124, 129, 130, 140, 144, 148, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 169, 171, 172], "channel": 173, "chapter": [20, 21, 65, 98, 112, 119, 120, 122, 123, 157, 167], "charact": [64, 65, 112, 120, 121, 123, 172], "characterist": [99, 173], "chart": 90, "check": [48, 49, 53, 54, 61, 64, 67, 68, 69, 72, 75, 80, 90, 92, 93, 98, 102, 103, 124, 125, 168, 169, 172], "check_data": 172, "check_scor": 172, "checkmat": 172, "chernozhukov": [10, 11, 34, 42, 44, 61, 63, 64, 75, 80, 84, 90, 91, 92, 93, 98, 99, 103, 139, 140, 148, 156, 157, 158, 167, 168, 171, 172], "chetverikov": [10, 11, 44, 63, 84, 91, 98, 103, 156, 168, 171], "chiang": [43, 63, 91, 171], "chieh": 171, "choic": [5, 6, 7, 8, 9, 14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 64, 69, 70, 71, 72, 76, 84, 92, 95, 111, 112, 113, 121, 123, 124, 128, 140, 142, 144, 157, 158, 163, 167, 172], "cholecyst": 98, "choos": [60, 64, 70, 75, 76, 79, 80, 84, 92, 93, 102, 124, 128, 139, 140, 141, 142, 143, 144, 148, 151, 152, 153, 156, 170, 173], "chosen": [22, 35, 40, 75, 76, 112, 123, 124], "chou": 94, "chr": 64, "christian": [84, 171], "christoph": 171, "chunk": [112, 123], "ci": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 67, 68, 69, 70, 71, 72, 76, 77, 78, 81, 82, 83, 85, 86, 87, 88, 90, 92, 93, 96, 97, 99, 100, 111, 124, 157, 163, 172, 173], "ci_at": [77, 78], "ci_bound": 16, "ci_cvar": [83, 93], "ci_cvar_0": 83, "ci_cvar_1": 83, "ci_joint": [69, 70, 71, 72, 78], "ci_joint_cvar": 83, "ci_joint_lqt": 96, "ci_joint_qt": 96, "ci_length": 67, "ci_low": [76, 77, 78], "ci_lpq_0": 96, "ci_lpq_1": 96, "ci_lqt": [93, 96], "ci_pointwis": [77, 78], "ci_pq_0": [93, 96], "ci_pq_1": [93, 96], "ci_qt": [93, 96], "ci_tun": 76, "ci_tuned_pipelin": 76, "ci_untun": 76, "ci_untuned_pipelin": 76, "ci_upp": [76, 77, 78], "cinelli": [100, 157, 158, 171], "circumv": 173, "citat": 172, "cite": 168, "cl": 68, "claim": 65, "clarifi": [69, 70, 71, 72], "clash": 62, "class": [0, 4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 64, 65, 66, 67, 69, 70, 71, 72, 73, 76, 77, 78, 87, 88, 89, 90, 92, 93, 98, 99, 101, 102, 104, 105, 106, 107, 108, 109, 111, 112, 114, 115, 116, 119, 121, 123, 124, 125, 139, 140, 156, 168, 170, 172], "class_estim": 97, "class_learn": 93, "class_learner_1": 75, "class_learner_2": 75, "classes_": [90, 98], "classic": [62, 63, 70, 91, 173], "classif": [26, 48, 53, 60, 62, 64, 65, 66, 67, 69, 70, 71, 72, 75, 90, 93, 95, 99, 101, 111, 112, 113, 119, 123, 124, 126, 127, 129, 173], "classifavg": 65, "classifi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 39, 46, 48, 53, 65, 77, 78, 89, 90, 97, 98, 112, 115, 119, 172], "classifiermixin": 98, "classmethod": [4, 5, 6, 7, 8, 9, 55], "claudia": [171, 172], "claus": [168, 172], "clean": 172, "cleaner": 75, "cleanup": 172, "clear": [63, 77, 91], "clearli": [69, 72, 97], "clever": 75, "client": 77, "clip": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 55, 56], "clipping_threshold": [12, 15, 55, 56, 81], "clone": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 61, 65, 75, 80, 91, 93, 102, 112, 115, 121, 123, 124, 139, 140, 156, 157, 163, 169, 170], "close": [56, 62, 64, 92, 98, 100, 157, 158], "cluster": [5, 6, 7, 8, 9, 12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 43, 71, 104, 105, 107, 108, 109, 171, 172], "cluster_col": [4, 5, 6, 8, 9, 63, 91, 104, 105, 107, 108, 109], "cluster_var": [4, 5, 6, 7, 8, 9, 43, 104, 105, 109], "cluster_var_i": [63, 91], "cluster_var_j": [63, 91], "cmap": 91, "cmd": 172, "co": [41, 45, 68], "codaci": 172, "code": [22, 26, 39, 45, 60, 62, 63, 64, 65, 66, 79, 84, 92, 103, 111, 112, 120, 121, 122, 123, 124, 139, 140, 156, 169, 170, 172, 173], "codecov": 172, "coef": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 99, 100, 101, 102, 103, 111, 112, 113, 114, 116, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 170, 173], "coef_": 100, "coef_df": 63, "coef_valu": 90, "coeffici": [16, 31, 32, 40, 49, 51, 54, 64, 66, 75, 76, 85, 86, 89, 92, 94, 95, 97, 100, 101, 111, 156, 157, 163, 173], "coefs_t": 95, "coefs_w": 95, "coffici": [157, 163], "cofid": 51, "coincid": [68, 88, 93], "col": [61, 63, 92], "col_nam": [69, 72], "collect": [65, 66, 67, 76, 77, 91, 101], "colnam": [63, 75], "color": [13, 16, 64, 66, 68, 72, 76, 77, 78, 80, 81, 82, 83, 90, 91, 92, 93, 94, 96, 97, 100], "color_palett": [13, 16, 69, 72, 76, 77, 78, 80, 91, 92, 93], "colorbar": 91, "colorblind": [13, 16, 69, 72, 76, 77, 78], "colorramppalett": 63, "colorscal": [81, 82], "colour": [61, 63], "column": [5, 6, 7, 8, 9, 16, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 97, 99, 100, 101, 104, 105, 106, 107, 108, 109, 111, 112, 114, 124, 127, 129, 139, 170, 172, 173], "column_stack": [8, 68, 76, 77, 78, 85, 86, 97, 99, 100, 124], "colv": 63, "com": [45, 62, 64, 65, 70, 71, 84, 94, 100, 112, 122, 168, 169], "comb": 84, "combin": [14, 16, 62, 63, 65, 67, 70, 71, 75, 76, 77, 78, 88, 90, 91, 100, 112, 120, 124, 128, 139, 157, 163, 172], "combind": 93, "combined_loss": 84, "come": [102, 112, 115, 140, 157, 158, 168, 173], "command": [169, 172], "comment": [104, 109], "commit": 172, "common": [75, 99, 100, 111, 124, 171], "commonli": 77, "companion": 171, "compar": [61, 63, 68, 69, 72, 76, 77, 80, 81, 82, 83, 85, 86, 88, 89, 94, 96, 97, 98, 100, 103, 112, 113, 124, 157, 158, 172], "comparevers": 64, "comparison": [69, 72, 75, 78, 94], "comparison_data": 76, "compat": [4, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 60, 62, 79, 104, 109, 172], "complement": 100, "complet": [50, 76, 90, 103, 157, 163, 169], "complex": [26, 62, 76, 90], "compli": [97, 124], "complianc": [96, 97, 124, 140, 149], "complic": [65, 76, 173], "complier": [64, 92, 93, 96, 97, 111, 124], "compon": [17, 19, 53, 54, 62, 64, 70, 75, 77, 84, 90, 92, 95, 111, 112, 123, 139, 140, 141, 142, 143, 144, 145, 147, 148, 149, 152, 153, 172], "compont": 62, "composit": 171, "comprehens": 77, "compris": 156, "comput": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 51, 57, 61, 62, 64, 65, 69, 70, 71, 72, 77, 80, 92, 93, 98, 99, 100, 139, 140, 151, 157, 158, 159, 160, 161, 162, 163, 164, 165, 168, 171, 172, 173], "computation": [157, 158], "concat": [77, 90, 91, 92, 95, 156], "concaten": [68, 76, 92, 156], "concentr": 156, "concern": 100, "conclud": [97, 100, 173], "cond": [124, 126, 138], "conda": [171, 172], "condit": [20, 21, 22, 24, 26, 31, 32, 39, 40, 61, 63, 64, 66, 67, 68, 69, 72, 78, 80, 87, 88, 91, 92, 95, 97, 100, 101, 103, 110, 124, 127, 128, 129, 130, 156, 157, 164, 165, 167, 170, 171, 172, 173], "conduct": [111, 124, 126, 127, 129, 173], "conf": [62, 96], "confer": 171, "confid": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 51, 62, 63, 64, 66, 67, 69, 70, 71, 72, 76, 77, 78, 81, 82, 83, 85, 86, 88, 91, 93, 96, 97, 99, 101, 110, 111, 124, 139, 140, 157, 163, 170, 171, 172, 173], "confidenceband": 83, "confidenti": 100, "config": [55, 56, 94], "configur": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 56, 65, 90], "confint": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 51, 64, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 81, 82, 83, 85, 86, 88, 93, 95, 96, 97, 98, 99, 101, 111, 139, 156, 168, 170, 173], "conflict": 169, "confound": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 37, 38, 39, 40, 57, 60, 64, 69, 70, 72, 79, 87, 89, 92, 96, 99, 100, 104, 109, 124, 135, 136, 137, 156, 157, 158, 163, 166, 167, 170, 171, 172, 173], "congress": 171, "connect": [64, 92, 93], "consequ": [31, 40, 63, 76, 87, 91, 99, 111, 124, 126, 157, 158, 164, 165, 167], "conserv": [99, 100, 157, 167], "consid": [24, 25, 26, 27, 28, 47, 55, 56, 61, 63, 64, 66, 67, 68, 69, 70, 71, 72, 75, 80, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 111, 112, 116, 121, 124, 125, 130, 133, 134, 139, 140, 156, 157, 158, 168, 173], "consider": [100, 124], "consist": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 49, 54, 64, 67, 89, 90, 92, 93, 94, 100, 103, 104, 109, 124, 130, 135, 136, 137, 170, 172], "consol": [61, 172], "constant": [17, 19, 34, 49, 54, 69, 72, 77, 84, 89, 95, 111, 124, 156], "constrained_layout": 80, "construct": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 51, 65, 68, 69, 72, 76, 81, 82, 83, 88, 93, 98, 99, 102, 111, 119, 120, 140, 146, 155, 156, 172, 173], "construct_framework": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "construct_iv": 91, "constructiv": 63, "constructor": [65, 105, 109], "consum": [63, 91], "cont": 35, "cont_d": [76, 77, 78], "contain": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 47, 48, 49, 50, 53, 54, 61, 63, 64, 69, 70, 72, 75, 76, 78, 80, 81, 82, 85, 86, 91, 92, 98, 103, 105, 106, 108, 109, 111, 112, 123, 124, 125, 127, 129, 156, 157, 158, 163, 172], "context": [100, 124, 138, 173], "contin": [35, 90], "continu": [35, 37, 41, 60, 65, 76, 77, 78, 79, 84, 89, 94, 97, 124, 157, 167, 172, 173], "contour": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 84, 87, 99, 100, 157, 163], "contour_plot": 100, "contours_z": [81, 82], "contrast": [23, 67, 77, 83, 88, 124, 132], "contribut": [169, 172], "contributor": 172, "control": [14, 16, 17, 19, 42, 47, 55, 62, 70, 71, 76, 84, 88, 93, 95, 97, 98, 100, 104, 105, 109, 124, 125, 127, 128, 129, 130, 140, 142, 144, 157, 160, 162, 173], "control_group": [14, 16, 69, 70, 71, 72, 124, 125, 127, 129], "conveni": [104, 107, 109], "convent": [64, 70, 92, 93, 97, 124, 130], "converg": [37, 61, 75, 80, 91, 103], "convergencewarn": 91, "convers": 91, "convert": [17, 19, 77, 83, 91, 96], "convex": 94, "cooper": 172, "coor": [65, 112, 122, 168, 170], "coordin": [77, 100], "copi": [72, 76, 90, 92, 95, 100], "cor": [157, 167], "core": [17, 19, 69, 70, 71, 72, 73, 76, 77, 78, 83, 87, 89, 91, 92, 93, 96, 99, 104, 106, 109, 112, 119, 170, 172], "cores_us": [83, 93, 96], "correct": [87, 88, 89, 100, 111, 156, 172], "correctli": [48, 53, 67, 76, 94, 99, 157, 167], "correl": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 63, 66, 84, 91, 98, 99, 101, 124, 157, 158, 167], "correpond": [69, 72, 124], "correspond": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 61, 63, 64, 65, 67, 68, 69, 70, 71, 72, 75, 76, 78, 80, 81, 82, 84, 88, 91, 92, 93, 95, 96, 99, 100, 103, 111, 112, 113, 114, 121, 123, 124, 126, 128, 130, 131, 138, 139, 140, 142, 144, 156, 157, 158, 160, 162, 163, 165, 167, 172, 173], "correspondingli": [69, 72], "cosh": 45, "coul": 63, "could": [60, 65, 69, 70, 72, 76, 79, 81, 82, 90, 100, 172, 173], "counfound": [31, 40, 96, 99, 111, 157, 167], "count": [76, 77, 78, 92, 93], "counti": 70, "countour": [157, 163], "countyr": 70, "coupl": [64, 92, 93], "cournapeau": [168, 170], "cours": [64, 75, 92, 100, 156, 173], "cov": [30, 31, 47, 97], "cov_nam": [97, 124], "cov_typ": [22, 26, 39, 51, 172], "covari": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 19, 26, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 51, 52, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 73, 76, 77, 78, 80, 81, 82, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 124, 126, 127, 129, 135, 136, 137, 138, 140, 141, 142, 143, 144, 156, 157, 158, 170, 171, 172], "cover": [62, 84, 99, 108, 109], "coverag": [69, 72, 75, 97, 98, 111, 172], "cp": [64, 65, 112, 120], "cpu": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 77], "cpu_count": [83, 93, 96], "cran": [65, 171, 172], "creat": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 55, 60, 63, 65, 69, 72, 76, 78, 79, 80, 81, 82, 83, 85, 86, 89, 91, 93, 95, 96, 100, 104, 109, 112, 120, 157, 158, 163, 167, 169, 172], "create_default_for_vers": 77, "create_synthetic_group_data": 95, "creation": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "crictial": 139, "critic": [100, 173], "cross": [12, 14, 15, 16, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 55, 56, 60, 61, 62, 64, 65, 66, 70, 75, 77, 80, 90, 92, 93, 97, 100, 103, 105, 109, 110, 112, 113, 114, 123, 127, 128, 130, 143, 155, 156, 159, 160, 163, 172, 173], "cross_sectional_data": [15, 18, 67, 124, 126], "crossfit": [75, 124], "crosstab": 94, "crucial": [84, 124, 173], "csail": [168, 170], "csdid": 71, "csv": [71, 84], "cuda": 77, "cumul": 124, "current": [52, 56, 67, 68, 69, 72, 88, 140, 157, 167, 168, 169, 173], "custom": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 62, 70, 76, 80, 100, 112, 113], "custom_measur": 62, "cut": 95, "cutoff": [46, 47, 97, 124], "cv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 55, 65, 92, 112, 122, 123, 139], "cv_calibr": [55, 56], "cv_glmnet": [63, 64, 65, 66, 112, 120, 123, 124, 156, 170], "cvar": [24, 29, 110, 146, 172], "cvar_0": 83, "cvar_1": 83, "d": [4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 51, 52, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 163, 164, 165, 166, 167, 168, 170, 171, 173], "d0": [83, 96, 156], "d0_true": 96, "d0cdb0ea4795": 65, "d1": [83, 94, 96, 156], "d10": 156, "d1_true": 96, "d2": [94, 156], "d21ee5775b5f": 65, "d2cml": 71, "d3": 156, "d4": 156, "d5": 156, "d5a0c70f1d98": 65, "d6": 156, "d7": 156, "d8": 156, "d9": 156, "d_": [35, 43, 63, 68, 78, 91, 124, 126, 130, 156], "d_0": [124, 131], "d_1": [94, 156], "d_2": 94, "d_col": [4, 5, 6, 7, 8, 9, 16, 60, 61, 63, 64, 65, 66, 69, 70, 71, 72, 79, 81, 82, 85, 86, 88, 91, 92, 93, 95, 97, 98, 99, 102, 103, 104, 105, 106, 108, 109, 112, 120, 121, 124, 125, 127, 129, 139, 140, 170, 172, 173], "d_i": [32, 33, 34, 36, 41, 42, 44, 45, 61, 66, 67, 78, 80, 83, 94, 96, 97, 101, 103, 124, 126, 138], "d_j": [78, 124, 131, 132, 156], "d_k": [124, 132, 156], "d_l": [124, 131], "d_w": 95, "da1440": 94, "dag": [66, 100, 101, 173], "dai": [69, 76], "dark": [61, 80], "darkblu": 63, "darkr": 63, "dash": [66, 69, 72, 77, 78], "dat": [104, 109], "data": [0, 10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 51, 53, 54, 62, 68, 75, 84, 94, 98, 102, 104, 106, 107, 108, 110, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 125, 127, 128, 129, 130, 139, 156, 160, 161, 162, 163, 171, 172], "data_apo": [76, 77, 78], "data_cvar": 93, "data_dict": [46, 81, 82, 85, 86, 87, 97, 124], "data_dml": 99, "data_dml_bas": [64, 81, 82, 85, 86, 92, 93, 95], "data_dml_base_iv": [64, 92, 93], "data_dml_flex": [64, 92], "data_dml_flex_iv": 64, "data_dml_iv_flex": 92, "data_dml_new": 95, "data_fram": 173, "data_lqt": 93, "data_pq": 93, "data_qt": 93, "data_transf": [63, 91, 92], "dataclass": [50, 56], "datafram": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 51, 52, 63, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 105, 106, 107, 108, 111, 112, 113, 114, 116, 124, 126, 140, 156, 157, 158, 163, 170, 173], "dataset": [0, 5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 50, 61, 62, 66, 67, 69, 72, 75, 76, 77, 78, 80, 81, 82, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170, 172, 173], "datatyp": [71, 172], "date": [16, 17, 19, 112, 120], "date_format": 16, "datetim": [7, 16, 17, 19, 69, 70, 72, 106, 109], "datetime64": [69, 106, 109], "datetime_complet": 76, "datetime_start": 76, "datetime_unit": [7, 16, 69, 72, 106, 109, 124, 125, 127, 129], "david": 172, "db": [64, 92, 93, 99, 173], "dbl": [62, 63, 64, 65, 104, 109, 156, 170, 173], "dc13a11076b3": 65, "ddc9": 65, "de": [60, 79, 171], "deal": [60, 79], "debias": [10, 11, 41, 43, 44, 63, 84, 91, 98, 110, 112, 139, 168, 171, 172], "debt": [64, 92, 93], "decai": [66, 101], "decid": [64, 92, 98], "decis": [26, 60, 64, 79, 92, 93, 111, 124, 171, 173], "decision_effect": 60, "decision_impact": [60, 79], "decisiontreeclassifi": [26, 52, 92], "decisiontreeregressor": 92, "declar": 173, "decomposit": [124, 125], "decreas": 97, "dedic": [104, 108, 109], "deep": [48, 49, 53, 54, 90], "deeper": 26, "def": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 69, 72, 75, 76, 80, 83, 90, 91, 94, 95, 96, 98, 100, 112, 113, 117, 140, 173], "default": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 51, 52, 53, 54, 55, 56, 62, 63, 66, 67, 69, 70, 71, 72, 75, 76, 85, 86, 91, 95, 97, 99, 100, 101, 102, 105, 109, 111, 112, 113, 116, 121, 123, 124, 139, 156, 157, 163, 164, 170, 173], "default_arg": [69, 72], "default_convert": 91, "default_jitt": 16, "defier": [97, 124], "defin": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 54, 61, 64, 65, 67, 70, 75, 77, 78, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 92, 93, 95, 96, 97, 98, 99, 100, 111, 112, 113, 117, 120, 122, 124, 126, 127, 129, 130, 138, 140, 141, 143, 144, 157, 158, 163, 167, 172, 173], "definit": [45, 69, 72, 76, 85, 86, 88, 124, 128, 157, 164, 165], "degre": [47, 64, 81, 82, 88, 91, 92, 97, 111, 157, 158], "dekel": 171, "delete_origin": 65, "deliber": 94, "delta": [16, 42, 62, 67, 69, 72, 100, 124, 126, 127, 128, 129, 130, 140, 142, 144], "delta_bench": 100, "delta_i": 62, "delta_j": 42, "delta_t": [17, 19, 69, 72], "delta_theta": [57, 78, 87, 99, 100, 157, 158], "delta_v": 100, "demand": [63, 91, 157, 158], "demir": [10, 11, 44, 63, 84, 91, 98, 103, 139, 168, 171], "demo": [70, 100], "demonstr": [61, 62, 63, 70, 77, 80, 91, 97, 100, 104, 109, 124, 156, 168, 170], "deni": 171, "denomin": [157, 158, 164, 165], "denot": [19, 38, 63, 64, 66, 67, 68, 72, 91, 92, 97, 100, 101, 111, 124, 126, 127, 129, 130, 131, 136, 140, 157, 158, 163, 165, 167], "dens_net_tfa": 64, "densiti": [27, 28, 29, 61, 66, 77, 78, 80], "dep": 73, "dep1": [65, 73, 104, 109, 170], "dep2": [65, 73, 104, 109, 170], "depend": [17, 19, 22, 23, 24, 26, 27, 29, 32, 41, 65, 67, 69, 72, 75, 81, 82, 85, 86, 87, 89, 90, 95, 97, 102, 111, 112, 121, 124, 127, 128, 129, 140, 149, 150, 157, 158, 164, 167, 170, 171], "deprec": [4, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 67, 68, 69, 70, 71, 72, 81, 102, 104, 109, 112, 124, 139, 140, 157], "deprecationwarn": [68, 81], "depreci": 172, "depth": [26, 52, 64, 65, 95, 102, 111, 112, 123, 124, 139, 140, 156, 170, 173], "deriv": [12, 14, 15, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 124, 156], "describ": [13, 62, 63, 69, 72, 91, 92, 93, 100, 112, 139, 140, 151, 169, 172], "descript": [19, 64, 71, 73, 76, 99, 112, 117, 121, 123, 139, 140, 142, 144, 157, 158, 160, 162], "deserv": 124, "design": [8, 17, 19, 41, 46, 47, 76, 77, 78, 90, 107, 109, 110, 171, 172], "design_info": [81, 82], "design_matrix": [81, 82, 111], "desir": [40, 65, 95, 124, 169], "detail": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 64, 65, 67, 68, 77, 78, 80, 84, 90, 93, 97, 99, 100, 103, 104, 109, 111, 112, 113, 117, 120, 122, 125, 126, 127, 128, 129, 130, 138, 140, 146, 148, 149, 150, 154, 155, 156, 157, 158, 160, 162, 167, 168, 169, 170, 172, 173], "determin": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 41, 49, 54, 64, 72, 83, 92, 93, 96, 97, 99, 124, 156, 157, 167], "determinist": [95, 97, 111, 124], "deutsch": 168, "dev": [169, 172], "develop": [62, 63, 65, 91, 100, 124, 126, 130, 172], "deviat": [75, 124, 157, 167], "devic": 77, "dezeur": 171, "df": [5, 6, 7, 8, 9, 16, 60, 61, 63, 66, 68, 69, 72, 76, 77, 78, 79, 81, 82, 83, 88, 91, 94, 96, 97, 99, 100, 101, 103, 105, 106, 108, 109, 111, 124, 125, 127, 129], "df_agg": 84, "df_all_apo": 77, "df_all_at": 77, "df_anticip": [69, 72], "df_apo": [76, 77, 78], "df_apo_ci": 78, "df_apo_plot": 76, "df_apos_ci": 78, "df_ate": [77, 78], "df_bench": 100, "df_binari": 100, "df_bonu": [65, 104, 109, 170], "df_capo0": 88, "df_capo1": 88, "df_cate": [81, 82, 88], "df_causal_contrast_c": 88, "df_ci": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 51], "df_coef": 75, "df_comparison": 76, "df_cvar": 93, "df_fuzzi": 97, "df_lqte": 93, "df_ml_g0": 75, "df_ml_g1": 75, "df_ml_m": 75, "df_pa": [67, 101], "df_perform": 77, "df_plot": 63, "df_post_treat": [69, 72], "df_pq": 93, "df_qte": 93, "df_result": 84, "df_sharp": 97, "df_sort": [77, 78], "df_summari": 92, "df_treat": 72, "df_wide": 91, "dfg": 168, "dgp": [17, 18, 19, 63, 66, 68, 69, 72, 83, 84, 91, 94, 95, 96, 100, 101], "dgp1": [17, 18, 19], "dgp2": [17, 18, 19], "dgp3": [17, 18, 19], "dgp4": [17, 18, 19], "dgp5": [17, 18, 19], "dgp6": [17, 18, 19], "dgp_confounded_irm_data": 100, "dgp_dict": 100, "dgp_tpye": 67, "dgp_type": [17, 18, 19, 67, 69, 72], "diagnost": 70, "diagon": 100, "diagram": [60, 79, 124], "dichotom": [60, 79], "dict": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 50, 51, 52, 53, 54, 57, 69, 72, 81, 82, 84, 90, 100, 112, 116], "dict_kei": [76, 157, 163], "dict_rdd": [107, 109], "dictionari": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 47, 57, 69, 72, 76, 81, 82, 85, 86, 99, 111, 112, 114, 117, 124, 125, 157, 163], "dictonari": [64, 92], "did": [0, 5, 7, 61, 67, 68, 69, 70, 71, 72, 76, 77, 91, 105, 106, 109, 110, 125, 126, 127, 129, 130, 140, 142, 144, 172, 173], "did_aggreg": [69, 71, 72], "did_data": 68, "did_multi": [124, 125], "diff": 92, "differ": [5, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 37, 38, 39, 40, 60, 61, 63, 64, 65, 66, 69, 71, 72, 76, 77, 78, 79, 80, 83, 85, 86, 87, 88, 90, 91, 92, 93, 94, 96, 97, 99, 100, 101, 102, 105, 109, 110, 111, 112, 113, 116, 120, 121, 125, 126, 127, 129, 130, 139, 141, 142, 143, 144, 169, 170, 171, 172, 173], "differenti": 124, "difficult": 100, "dillon": 171, "dim": [47, 64], "dim_x": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 47, 61, 63, 65, 75, 80, 88, 89, 90, 91, 103, 111, 112, 114, 124, 157, 163], "dim_z": [38, 42, 124], "dimens": [17, 19, 32, 43, 63, 91, 95, 139], "dimension": [13, 32, 34, 37, 38, 39, 84, 98, 111, 124, 135, 136, 137, 139, 156, 157, 163, 170, 171], "direct": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 66, 68, 80, 88, 101, 103, 124, 173], "directli": [46, 61, 62, 64, 75, 76, 78, 80, 99, 103, 104, 109, 157, 163, 170, 173], "discontinu": [8, 46, 47, 107, 109, 110, 171, 172], "discret": [23, 35, 76, 77, 78, 91, 124, 131, 172], "discretis": 93, "discuss": [33, 63, 64, 91, 92, 124, 125, 130, 171, 172, 173], "disjoint": [63, 85, 86, 91], "displai": [13, 63, 69, 70, 71, 72, 76, 77, 78, 91, 100, 111, 112, 113, 157, 163], "displot": 92, "disproportion": [64, 92], "disregard": [49, 54], "dist": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "distinct": [104, 109], "distinguish": [16, 69, 72], "distr": [112, 120], "distribut": [17, 19, 47, 61, 67, 75, 77, 78, 80, 100, 103, 124, 126, 130, 157, 165, 169, 171, 172], "diverg": [46, 61, 80, 103], "divid": [69, 72, 124], "dmatrix": [81, 82, 111], "dml": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 60, 61, 65, 66, 70, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 95, 96, 97, 98, 99, 100, 101, 102, 103, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117, 118, 124, 125, 126, 127, 129, 140, 156, 157, 163, 169], "dml1": [110, 170, 172, 173], "dml2": [60, 63, 65, 66, 73, 91, 110, 124, 140, 156, 170, 172, 173], "dml_apo": 88, "dml_apo_obj": 124, "dml_apos_att": 88, "dml_apos_obj": 124, "dml_apos_tun": 76, "dml_apos_untun": 76, "dml_combin": 156, "dml_cover": 98, "dml_cv_predict": 172, "dml_cvar": [83, 93], "dml_cvar_0": 83, "dml_cvar_1": 83, "dml_cvar_obj": [24, 111], "dml_data": [7, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 63, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 87, 88, 91, 94, 98, 99, 100, 101, 105, 106, 107, 108, 109, 111, 112, 117, 118, 122, 123, 124, 125, 127, 129, 156, 173], "dml_data_anticip": [69, 72], "dml_data_arrai": [108, 109], "dml_data_bench": 100, "dml_data_bonu": [65, 170], "dml_data_df": 173, "dml_data_fuzzi": 97, "dml_data_lasso": 73, "dml_data_sharp": 97, "dml_data_sim": [65, 170], "dml_df": [63, 91], "dml_did": [67, 68], "dml_did_obj": [12, 15, 16, 124, 125, 126, 127, 129], "dml_iivm": 98, "dml_iivm_boost": [64, 92], "dml_iivm_forest": [64, 92], "dml_iivm_lasso": [64, 92], "dml_iivm_obj": [25, 79, 124], "dml_iivm_tre": [64, 92], "dml_irm": [75, 81, 85, 88, 95], "dml_irm_at": 87, "dml_irm_att": 88, "dml_irm_boost": [64, 92], "dml_irm_forest": [64, 92], "dml_irm_gat": 87, "dml_irm_gatet": 87, "dml_irm_lasso": [64, 73, 92], "dml_irm_new": 95, "dml_irm_obj": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 99, 111, 112, 114, 124], "dml_irm_obj_ext": [112, 114], "dml_irm_rf": 73, "dml_irm_tre": [64, 92], "dml_irm_weighted_att": 88, "dml_kwarg": 88, "dml_length": 98, "dml_long": 57, "dml_lplr": 89, "dml_lplr_obj": [37, 124], "dml_lpq_0": 96, "dml_lpq_1": 96, "dml_lpq_obj": [27, 111], "dml_lqte": [93, 96], "dml_obj": [62, 69, 70, 71, 72, 77, 78, 99, 100], "dml_obj_al": [69, 72], "dml_obj_anticip": [69, 72], "dml_obj_bench": 100, "dml_obj_lasso": 70, "dml_obj_linear": [69, 72], "dml_obj_linear_logist": 70, "dml_obj_nyt": [69, 72], "dml_obj_tun": 76, "dml_obj_tuned_pipelin": 76, "dml_obj_univers": [69, 72], "dml_obj_untun": 76, "dml_obj_untuned_pipelin": 76, "dml_pliv": [63, 91], "dml_pliv_obj": [38, 63, 91, 124], "dml_plr": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 82, 86, 156], "dml_plr_1": 156, "dml_plr_2": 156, "dml_plr_boost": [64, 92], "dml_plr_forest": [64, 92, 173], "dml_plr_lasso": [64, 73, 92], "dml_plr_no_split": 139, "dml_plr_obj": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 99, 102, 111, 112, 113, 116, 117, 118, 120, 121, 122, 123, 124, 139, 140, 156, 157, 158, 163], "dml_plr_obj_extern": 139, "dml_plr_obj_intern": 139, "dml_plr_obj_onfold": 90, "dml_plr_obj_untun": 90, "dml_plr_rf": 73, "dml_plr_tree": [64, 92, 173], "dml_pq_0": [93, 96], "dml_pq_1": [93, 96], "dml_pq_obj": [28, 111], "dml_procedur": [73, 102, 170, 172, 173], "dml_qte": [93, 96], "dml_qte_obj": [29, 111], "dml_robust_confset": 98, "dml_robust_length": 98, "dml_short": 57, "dml_ssm": [66, 101, 124], "dml_standard_ci": 98, "dml_tune": 172, "dmldummyclassifi": [112, 114], "dmldummyregressor": [112, 114], "dmlmt": 171, "dmloptunaresult": 76, "dnorm": 61, "do": [62, 63, 64, 65, 69, 70, 72, 75, 88, 91, 92, 93, 94, 100, 111, 112, 114, 116, 139, 157, 167, 170, 173], "doabl": 140, "doc": [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 168, 172], "docu": 172, "documen": 76, "document": [68, 69, 70, 71, 72, 74, 76, 77, 81, 82, 85, 86, 88, 89, 90, 100, 124, 130, 140, 157, 168, 172, 173], "doe": [16, 23, 29, 62, 63, 64, 68, 69, 72, 76, 77, 78, 88, 91, 92, 94, 97, 99, 100, 124, 140, 144, 157, 167, 173], "doesn": [60, 79], "doi": [10, 11, 17, 18, 19, 31, 33, 36, 40, 41, 43, 44, 62, 63, 65, 84, 91, 100, 103, 112, 122, 139, 156, 168, 170, 172], "domain": 95, "don": [62, 90], "done": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 70, 90, 93, 112, 117, 118, 123, 139, 157, 158], "dosag": [77, 78], "dot": [30, 68, 69, 72, 95, 104, 109, 111, 112, 117, 118, 124, 128, 130, 131, 156, 170], "doubl": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 41, 43, 44, 45, 46, 64, 70, 75, 84, 90, 92, 94, 98, 110, 112, 139, 140, 156, 157, 158, 172], "double_ml": [69, 98], "double_ml_bonus_data": 73, "double_ml_data_from_data_fram": [61, 103, 104, 109, 173], "double_ml_data_from_matrix": [62, 65, 104, 109, 112, 122, 123, 156, 170], "double_ml_framework": [124, 125], "double_ml_irm": 73, "double_ml_score_mixin": 0, "doubleiivm": 168, "doubleml": [0, 61, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 93, 94, 95, 96, 97, 98, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 139, 140, 157, 163, 170, 171, 172], "doubleml2022python": 168, "doubleml2024r": 168, "doubleml_did_eval_linear": 62, "doubleml_did_eval_rf": 62, "doubleml_did_linear": 62, "doubleml_did_rf": 62, "doubleml_framework": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "doublemlapo": [77, 78, 88, 124, 140, 145, 172], "doublemlblp": [22, 26, 39, 81, 82, 88, 111, 172], "doublemlclusterdata": [0, 104, 109], "doublemlcvar": [83, 111, 140, 146, 172], "doublemldata": [0, 4, 7, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 60, 63, 65, 66, 67, 68, 73, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 105, 106, 107, 108, 110, 111, 112, 113, 114, 116, 117, 118, 120, 121, 124, 126, 139, 140, 156, 157, 163, 172, 173], "doublemldid": [67, 68, 124, 126, 140, 143, 172], "doublemldidaggreg": [69, 70, 71, 72, 124, 125], "doublemldidbinari": 68, "doublemldidc": [67, 124, 126, 140, 141, 172], "doublemldiddata": [0, 12, 15, 18, 67, 68, 105, 124, 126], "doublemldidmulti": [69, 70, 71, 72, 76, 124, 125, 127, 128, 129, 140, 142, 144, 172], "doublemlframework": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 69, 70, 71, 72, 124, 125, 139, 156, 172], "doublemlframwork": 23, "doublemlidid": [124, 126], "doublemlididc": [124, 126], "doublemliivm": [60, 64, 79, 92, 98, 112, 115, 116, 119, 121, 124, 139, 140, 147, 172], "doublemlirm": [12, 14, 15, 22, 24, 25, 27, 28, 30, 37, 38, 39, 62, 64, 73, 75, 76, 78, 81, 85, 87, 88, 92, 94, 95, 99, 100, 111, 112, 114, 115, 116, 119, 121, 124, 139, 140, 148, 168, 172], "doublemllplr": [89, 124, 140, 151, 172], "doublemllpq": [96, 111, 140, 149, 172], "doublemlpaneldata": [0, 14, 16, 68, 70, 71, 106, 124, 125, 127, 128, 129, 172], "doublemlpliv": [112, 116, 121, 124, 139, 140, 152, 168, 172], "doublemlplr": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 61, 64, 65, 73, 80, 82, 86, 90, 92, 94, 99, 102, 103, 111, 112, 113, 116, 117, 118, 119, 120, 121, 122, 123, 124, 139, 140, 153, 156, 157, 163, 168, 170, 172, 173], "doublemlpolicytre": [26, 111], "doublemlpq": [93, 96, 111, 140, 150, 172], "doublemlqt": [83, 93, 96, 111, 156, 172], "doublemlrdddata": [0, 46, 97, 107, 124, 172], "doublemlresampl": [75, 88], "doublemlsmm": 172, "doublemlssm": [66, 101, 124, 140, 154, 155, 172], "doublemlssmdata": [0, 30, 36, 101, 108, 124, 172], "doubli": [18, 31, 40, 62, 70, 98, 171], "doudou": [41, 171], "down": [89, 100], "download": [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 104, 109, 169, 170], "download_fil": 70, "downward": 100, "dpg_dict": 99, "dpi": [61, 80, 94], "dr": [124, 128], "dramat": 62, "draw": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 100, 139, 172], "draw_sample_split": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 75, 88, 139], "drawn": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 47, 64, 69, 72, 92, 93, 95, 139], "drive": [61, 80, 103], "driven": [100, 173], "drop": [62, 90, 91, 94, 104, 109, 112, 122, 123, 140, 141, 142, 143, 144, 156], "dropna": [69, 72], "dt": [69, 140, 141, 157, 159], "dt_bonu": [104, 109], "dta": [62, 71], "dtrain": 92, "dtype": [69, 70, 71, 72, 73, 76, 77, 78, 85, 86, 87, 89, 91, 92, 93, 98, 99, 104, 106, 109, 111, 170], "dualiti": 91, "dubourg": [168, 170], "duchesnai": [168, 170], "due": [61, 62, 70, 80, 81, 82, 87, 89, 99, 100, 103, 124, 138, 157, 158, 172, 173], "duflo": [10, 11, 44, 63, 84, 91, 98, 103, 139, 168, 171], "dummi": [22, 26, 39, 48, 49, 51, 70, 90, 100, 111, 112, 114, 124, 126, 172], "dummyclassifi": [48, 70], "dummyregressor": [49, 70], "duplic": 172, "durabl": [65, 73, 104, 109, 170], "durat": [11, 76], "dure": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 62, 63, 64, 65, 66, 90, 91, 92, 112, 116, 121, 123, 139, 170, 172, 173], "dx": 33, "dynam": [62, 171], "e": [6, 7, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 36, 37, 38, 39, 40, 44, 46, 48, 49, 50, 53, 54, 61, 62, 63, 64, 66, 67, 69, 70, 71, 72, 75, 76, 77, 78, 80, 81, 82, 84, 87, 89, 90, 91, 92, 93, 94, 97, 98, 99, 100, 101, 103, 106, 109, 111, 112, 113, 114, 116, 121, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173], "e20ea26": 65, "e401": [64, 92, 93, 99, 173], "e4016224": 173, "e4016270": 173, "e45228": 94, "e57c": 65, "each": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 48, 49, 53, 54, 63, 65, 68, 69, 70, 71, 72, 75, 76, 77, 78, 85, 86, 89, 90, 91, 93, 94, 95, 98, 99, 100, 102, 104, 106, 109, 112, 113, 116, 121, 123, 124, 130, 139, 156, 157, 163, 173], "earlier": [69, 72, 173], "earn": [64, 92, 93], "earner": [64, 92, 99], "easi": [65, 98, 140], "easier": 90, "easili": [65, 75, 90, 93, 172], "ec973f": 94, "ecolor": [68, 76, 77, 78, 92, 94], "econ": 171, "econml": 171, "econom": [36, 42, 43, 45, 63, 84, 91, 94, 100, 139, 171], "econometr": [10, 11, 17, 18, 19, 31, 40, 41, 44, 45, 62, 63, 84, 91, 98, 103, 168, 171], "econometrica": [34, 63, 91, 94, 98, 103, 171], "ecosystem": [168, 173], "ectj": [10, 11, 41, 44, 63, 84, 91, 103, 168], "ed": 171, "edge_color": 80, "edgecolor": 80, "edit": [169, 171], "edu": [168, 170], "educ": [64, 92, 93, 99, 173], "ee97bda7": 65, "effect": [12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 47, 48, 49, 53, 54, 60, 61, 62, 63, 65, 66, 67, 68, 76, 78, 79, 80, 84, 87, 91, 95, 97, 98, 101, 103, 110, 112, 120, 121, 122, 123, 125, 126, 128, 130, 132, 133, 134, 138, 139, 140, 148, 156, 157, 158, 170, 171, 172, 173], "effici": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 77, 124, 171], "effort": 140, "eight": [63, 91], "either": [12, 14, 15, 16, 17, 19, 22, 24, 25, 26, 27, 28, 30, 32, 37, 38, 39, 65, 68, 69, 72, 84, 95, 97, 111, 112, 113, 118, 119, 123, 124, 127, 130, 173], "elapsed_tim": 77, "eleanor": 171, "element": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 63, 66, 67, 69, 70, 71, 72, 75, 81, 82, 83, 91, 93, 96, 99, 101, 124, 127, 128, 129, 140, 141, 142, 143, 144, 145, 151, 157, 160, 162, 163, 166, 167, 172], "element_text": [63, 64], "elementari": 171, "elif": [85, 86, 95], "elig": [93, 99, 173], "eligibl": [64, 92, 99], "ell": [61, 63, 80, 84, 91, 103, 140, 152, 153, 170], "ell_0": [25, 38, 39, 61, 80, 84, 90, 103, 124, 133, 140, 153], "ell_1": 70, "ell_2": 75, "els": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 51, 62, 63, 64, 68, 76, 77, 85, 86, 91, 95, 100], "em": 171, "emphas": [63, 91], "empir": [20, 21, 61, 63, 70, 80, 91, 94, 100, 103, 124, 127, 128, 129, 139, 140, 156], "emploi": [63, 76, 84, 91, 100, 140, 147], "employ": [64, 70, 92, 93], "employe": 173, "empti": 91, "emul": [157, 158], "enabl": [13, 76, 77, 78, 95, 99, 111, 157, 158, 172], "enable_metadata_rout": [48, 49, 53, 54], "encapsul": [48, 49, 53, 54, 112, 120, 121], "encod": 94, "encount": [72, 77], "end": [33, 41, 42, 43, 61, 62, 63, 64, 66, 68, 69, 72, 75, 80, 83, 84, 91, 92, 94, 95, 96, 101, 102, 104, 109, 112, 121, 123, 124, 127, 129, 130, 139, 140, 142, 144, 156, 170, 173], "end_tim": 77, "endogen": [64, 92, 93, 173], "enet_coordinate_descent_gram": 91, "enforc": 70, "engin": [65, 171], "enrol": [64, 92, 93], "ensembl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 73, 75, 76, 77, 78, 80, 81, 82, 85, 86, 87, 89, 92, 95, 99, 100, 102, 111, 112, 113, 114, 115, 116, 120, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170, 173], "ensemble_learner_pipelin": [112, 120], "ensemble_pipe_classif": 65, "ensemble_pipe_regr": 65, "ensur": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 53, 54, 63, 77, 88, 90, 91, 95, 98, 107, 109], "enter": 124, "entir": [16, 61, 64, 80, 92, 103, 157, 158], "entri": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 47, 61, 63, 69, 70, 71, 72, 73, 76, 77, 78, 80, 87, 89, 91, 92, 93, 99, 103, 104, 106, 109, 112, 121, 123, 168, 170, 172], "enumer": [68, 75, 76, 77, 78, 83, 85, 86, 91, 92, 93, 96, 102, 112, 121, 123, 139], "env": 169, "environ": [76, 169], "ep": 94, "epanechnikov": 46, "epsilon": [64, 67, 68, 83, 92, 96, 111, 124, 126, 130], "epsilon_": [63, 68, 69, 72, 91], "epsilon_0": 47, "epsilon_1": 47, "epsilon_i": [32, 83, 94, 95, 96], "epsilon_sampl": 95, "epsilon_tru": [83, 96], "eqnarrai": 64, "equal": [13, 17, 19, 22, 23, 26, 63, 66, 69, 72, 91, 94, 98, 101, 111, 112, 123, 124, 157, 165], "equat": [47, 63, 64, 91, 92, 100, 102, 156, 173], "equilibrium": [63, 91], "equiv": [124, 130, 140, 142, 144], "equival": [84, 88, 139], "err": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 67, 69, 70, 71, 72, 73, 76, 77, 78, 79, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 99, 100, 101, 111, 112, 113, 114, 116, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 170, 173], "error": [5, 6, 7, 8, 9, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 53, 54, 60, 61, 62, 64, 65, 66, 68, 75, 76, 77, 80, 84, 85, 86, 90, 92, 97, 100, 103, 112, 113, 114, 120, 121, 122, 123, 124, 136, 137, 139, 140, 156, 157, 163, 170, 172, 173], "error_on_convergence_failur": 37, "errorbar": [68, 76, 77, 78, 85, 86, 90, 92, 94, 97], "erstellt": [63, 64, 65], "es_linear_logist": 70, "es_rf": 70, "escap": 91, "esim": 97, "especi": [75, 90, 104, 109], "essenti": 100, "est": 97, "est_bound": 16, "est_method": 62, "esther": [139, 171], "estim": [10, 12, 13, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 46, 48, 49, 50, 51, 52, 53, 54, 61, 62, 63, 65, 68, 75, 76, 78, 80, 81, 82, 83, 85, 86, 88, 89, 91, 95, 97, 98, 102, 103, 110, 111, 112, 114, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 132, 134, 141, 142, 143, 146, 149, 150, 151, 155, 157, 158, 163, 168, 171, 172], "estimand": 98, "estimator_list": 90, "et": [10, 11, 32, 34, 41, 43, 44, 61, 63, 64, 65, 67, 75, 80, 81, 82, 83, 84, 85, 86, 89, 91, 92, 93, 96, 99, 103, 124, 126, 130, 139, 140, 146, 148, 149, 150, 151, 156, 157, 158, 167, 168, 170, 172], "eta": [20, 21, 61, 63, 64, 68, 90, 91, 92, 96, 97, 102, 111, 124, 127, 128, 129, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 167, 170, 173], "eta1": 94, "eta2": 94, "eta_": [156, 157, 167], "eta_0": [46, 102, 124, 127, 128, 129, 140, 156], "eta_d": [97, 124], "eta_i": [17, 19, 32, 68, 69, 72, 95, 96, 97, 124], "eta_sampl": 95, "eta_tru": 96, "etc": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 63, 75, 90, 91, 172], "ev": [61, 80, 103], "eval": [16, 65, 69, 70, 71, 72, 112, 122, 123, 124, 127, 128, 129, 140, 142, 144, 173], "eval_metr": [64, 92, 173], "eval_pr": 62, "eval_predict": 62, "evalu": [12, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 34, 37, 38, 39, 62, 65, 68, 69, 70, 71, 72, 81, 82, 83, 87, 88, 93, 96, 99, 102, 113, 114, 123, 124, 128, 130, 171, 172], "evaluate_learn": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 75, 76, 90, 112, 113, 172], "evalut": [112, 113], "even": [64, 65, 69, 72, 76, 92, 94, 97, 112, 120, 124, 173], "event": [124, 125], "eventstudi": [16, 69, 70, 71, 72, 124, 125], "eventu": [63, 91], "everi": [63, 70, 91], "everyth": 168, "evid": [87, 90], "exact": [88, 100], "exactli": [97, 100, 124], "examin": 77, "exampl": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 46, 55, 56, 60, 61, 64, 65, 66, 67, 68, 69, 71, 72, 73, 75, 77, 78, 79, 80, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 103, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 127, 128, 129, 138, 139, 140, 156, 157, 163, 168, 170, 172, 173], "example_attgt": 62, "example_attgt_dml_eval_linear": 62, "example_attgt_dml_eval_rf": 62, "example_attgt_dml_linear": 62, "example_attgt_dml_rf": 62, "except": [49, 54, 84, 100, 172], "excess": 75, "exclud": 57, "exclus": [22, 26, 39, 85, 86, 111], "execut": [65, 173], "exemplarili": 170, "exemplatori": 95, "exhaust": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 112, 118], "exhibit": [63, 91], "exist": [48, 49, 53, 54, 88, 124, 126, 130, 157, 167], "exogen": [64, 92, 93, 124, 173], "exp": [17, 18, 19, 31, 32, 34, 35, 40, 41, 44, 61, 68, 80, 81, 82, 85, 86, 94, 95, 103], "expect": [31, 37, 40, 49, 54, 62, 66, 67, 69, 70, 72, 75, 78, 87, 89, 90, 97, 100, 101, 107, 108, 109, 111, 124, 139, 156, 157, 164, 170], "experi": [11, 33, 34, 61, 64, 80, 92, 98, 100, 103, 104, 109, 139, 170, 171], "experiment": [12, 14, 15, 16, 17, 18, 19, 140, 141, 142, 143, 144, 157, 159, 160, 161, 162], "expertis": 100, "expit": [37, 89, 124, 135, 140, 151], "explain": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 76, 99, 157, 158, 166, 167], "explan": [63, 67, 91, 99, 157, 166, 168, 173], "explanatori": [100, 156], "explicit": 100, "explicitli": [69, 71, 72, 87, 173], "exploit": [61, 80, 103, 124, 173], "explor": [76, 90], "exponenti": 156, "export": [90, 172], "expos": [104, 105, 107, 108, 109, 124, 130], "exposur": [7, 17, 19, 68, 69, 70, 71, 72], "express": [63, 84, 97, 157, 167], "ext": [16, 17, 19], "extend": [89, 100, 105, 108, 109, 112, 119, 168, 172], "extendend": [157, 167], "extens": [112, 114, 119, 140, 168, 171, 172], "extent": 84, "extern": [61, 80, 88, 90, 110, 114, 118, 121, 123, 157, 158, 172], "external_predict": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 80, 112, 114], "externalptr": 64, "extra": 65, "extract": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 77, 90], "extralearn": 65, "extrem": [55, 56, 64, 92], "extreme_threshold": [55, 56], "ey": 84, "ezequiel": 172, "f": [64, 65, 67, 68, 69, 72, 75, 76, 77, 78, 80, 83, 84, 91, 92, 93, 95, 96, 98, 99, 100, 101, 112, 122, 157, 167, 168, 170], "f00584a57972": 65, "f1718fdeb9b0": 65, "f2e7": 65, "f3d24993": 65, "f6ebc": 94, "f_": [17, 18, 19, 31, 68, 111], "f_loc": [83, 96], "f_p": 68, "f_scale": [83, 96], "f_t": [69, 72], "f_x": 124, "face_color": 80, "facet_wrap": 64, "facilit": 90, "fact": [64, 92, 93], "factor": [47, 61, 62, 63, 64, 65, 75, 80, 103, 112, 120, 121, 157, 160, 162, 173], "faculti": 171, "fail": 172, "failur": 37, "fair": 75, "fake": [60, 79, 98], "fall": 77, "fallback": [112, 120, 121], "fals": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 41, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 61, 64, 65, 66, 67, 69, 72, 75, 76, 77, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 95, 96, 97, 98, 100, 101, 104, 109, 112, 121, 122, 123, 124, 127, 139, 140, 141, 142, 143, 144, 156, 157, 159, 160, 161, 162, 173], "famili": [64, 92, 112, 120, 123], "familiar": 98, "fanci": 62, "far": [64, 92], "farbmach": 33, "fast": [75, 76, 95, 112, 119], "faster": 84, "fb5c25fa": 65, "fc9e": 65, "fd8a": 65, "featur": [10, 11, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 49, 52, 54, 62, 68, 69, 72, 73, 75, 77, 87, 88, 92, 95, 97, 100, 107, 109, 111, 112, 114, 120, 121, 124], "featureless": [65, 112, 120], "features_bas": [64, 92, 93, 99], "features_flex": 64, "featureunion": 65, "februari": [69, 100], "feder": 70, "femal": [65, 73, 104, 109, 170], "fern\u00e1ndez": [34, 139, 171], "fetch": [64, 91, 92, 93, 104, 109], "fetch_401k": [64, 92, 93, 99, 173], "fetch_bonu": [65, 73, 104, 109, 170], "few": [64, 92, 93], "ff7f0e": 68, "field": [63, 91, 112, 121, 173], "fifteenth": 171, "fifth": [63, 69, 72], "fig": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 68, 69, 70, 71, 72, 75, 76, 77, 78, 81, 82, 83, 84, 88, 90, 93, 94, 96, 97, 100], "fig_al": 80, "fig_dml": 80, "fig_non_orth": 80, "fig_orth_nosplit": 80, "fig_po_al": 80, "fig_po_dml": 80, "fig_po_nosplit": 80, "figsiz": [13, 16, 68, 69, 72, 73, 75, 76, 77, 78, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 96, 97], "figur": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 44, 61, 63, 68, 69, 70, 72, 73, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 96, 100, 103], "figure_format": 94, "file": [10, 11, 70, 84, 94, 171, 172], "filenam": 61, "fill": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 61, 63, 64, 66, 67, 75, 92, 101], "fill_between": [81, 82, 83, 88, 93, 96], "fill_valu": 75, "fillna": 69, "filter": 65, "filterwarn": [75, 76, 77, 80], "final": [61, 65, 66, 68, 69, 71, 72, 76, 78, 80, 81, 82, 83, 85, 86, 87, 93, 96, 97, 101, 103, 124, 138, 140, 142, 144, 173], "final_estim": [76, 97], "final_estimatorridg": 76, "financi": [10, 99, 173], "find": [64, 67, 68, 77, 92, 100, 111, 112, 122, 123, 173], "finish": 65, "finit": [61, 64], "firm": [63, 91, 99], "firmid": 91, "first": [7, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 43, 46, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 78, 79, 80, 81, 82, 85, 86, 87, 88, 89, 91, 92, 93, 95, 96, 97, 100, 101, 103, 111, 124, 125, 128, 130, 139, 156, 157, 163, 169, 170, 172, 173], "fit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 50, 51, 52, 53, 54, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 140, 143, 155, 156, 157, 158, 163, 168, 172, 173], "fit_arg": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39], "fit_transform": [88, 91, 92], "fittedpipelin": 76, "five": 91, "fix": [68, 69, 72, 75, 76, 172], "flag": [18, 104, 109, 139, 169], "flake8": 172, "flamlclassifierdoubleml": 90, "flamlregressordoubleml": 90, "flatten": [90, 94], "flexibl": [46, 60, 62, 64, 65, 67, 79, 92, 124, 168, 171, 172, 173], "flexibli": [64, 70, 92, 99], "float": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 41, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 69, 70, 71, 72, 106, 109], "float32": [92, 93, 99], "float64": [69, 70, 71, 72, 73, 76, 77, 78, 83, 85, 86, 87, 89, 91, 92, 99, 104, 106, 109, 112, 118, 170], "floor": 65, "floor_divid": 91, "flt": 65, "flush": 61, "fmt": [68, 76, 77, 78, 85, 86, 90, 92, 94, 97], "fobj": 92, "focu": [63, 64, 70, 76, 88, 91, 92, 93, 100, 111, 124, 126, 130, 138, 173], "focus": [76, 93, 99, 100, 173], "fold": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 60, 63, 64, 65, 66, 67, 69, 70, 71, 72, 75, 76, 91, 92, 93, 99, 101, 102, 110, 112, 113, 116, 121, 122, 123, 124, 126, 127, 129, 140, 143, 156, 170, 173], "follow": [17, 18, 19, 31, 32, 35, 40, 41, 61, 63, 64, 66, 67, 68, 69, 70, 71, 72, 76, 80, 81, 82, 83, 85, 86, 89, 90, 91, 92, 93, 96, 97, 98, 99, 100, 101, 103, 104, 109, 111, 112, 113, 114, 120, 121, 123, 124, 125, 127, 128, 129, 130, 139, 140, 142, 144, 151, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173], "font_scal": [91, 92, 93], "fontsiz": [69, 72, 83, 89, 93, 96], "force_all_d_finit": [5, 6, 7, 8, 9, 104, 105, 109], "force_all_x_finit": [4, 5, 6, 7, 8, 9, 104, 109], "forest": [33, 60, 61, 62, 64, 65, 67, 75, 77, 79, 80, 87, 92, 99, 103, 112, 120, 121, 123, 170, 173], "forest_summari": 92, "forg": [169, 171, 172], "form": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 40, 51, 53, 54, 64, 66, 67, 68, 75, 81, 82, 83, 85, 86, 87, 92, 96, 97, 99, 101, 111, 124, 125, 126, 130, 133, 134, 135, 136, 137, 140, 142, 144, 145, 148, 157, 158, 163, 164, 165, 166, 167, 169, 170], "format": [7, 16, 41, 70, 77, 80, 87, 157, 163, 172], "former": [70, 98], "formula": [63, 64, 91, 92, 97, 100, 172], "formula_flex": 64, "forschungsgemeinschaft": 168, "forthcom": [100, 171], "forum": 172, "forward": [26, 52], "found": [50, 71, 81, 82, 84, 85, 86, 89, 90, 103, 104, 109, 112, 117, 119, 124, 138, 170], "foundat": [77, 168, 171], "four": [64, 75, 77, 92, 124, 127, 172], "fourth": [63, 91], "frac": [17, 18, 19, 21, 25, 31, 33, 34, 36, 40, 41, 42, 44, 45, 49, 54, 61, 63, 65, 68, 72, 80, 84, 87, 89, 91, 94, 97, 102, 103, 111, 124, 127, 128, 129, 133, 135, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167], "fraction": [19, 65], "frame": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 51, 60, 61, 63, 64, 66, 69, 70, 71, 72, 73, 76, 77, 78, 81, 82, 85, 86, 87, 89, 91, 92, 93, 94, 95, 99, 103, 104, 106, 109, 170, 173], "framealpha": [69, 72], "frameon": [69, 72, 89], "framework": [13, 16, 21, 61, 63, 65, 75, 77, 80, 90, 91, 94, 100, 103, 112, 122, 156, 168, 170, 172, 173], "freez": 169, "fribourg": 171, "friendli": [69, 72, 77, 78], "from": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 51, 54, 55, 56, 60, 61, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 105, 106, 107, 108, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 139, 140, 156, 157, 163, 170, 172, 173], "from_arrai": [4, 5, 6, 7, 8, 9, 30, 46, 67, 68, 80, 83, 96, 103, 104, 105, 107, 108, 109, 112, 117, 118, 156, 170], "from_config": [55, 56], "from_product": 91, "front": 78, "fr\u00e9chet": [157, 167], "fs_kernel": [46, 124], "fs_specif": [46, 124], "fsize": [64, 92, 93, 99, 173], "full": [50, 67, 68, 75, 76, 77, 78, 80, 83, 85, 86, 92, 93, 96, 97, 98, 101, 103, 124], "fulli": [26, 64, 74, 90, 92, 98, 124, 134], "fun": 61, "func": 62, "function": [0, 4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 44, 45, 46, 60, 61, 64, 65, 66, 67, 69, 70, 71, 72, 75, 77, 79, 80, 81, 82, 83, 88, 89, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 110, 112, 113, 114, 115, 116, 117, 118, 119, 121, 123, 124, 125, 126, 127, 128, 129, 130, 133, 135, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 162, 167, 168, 171, 172, 173], "fund": [64, 92, 93, 168], "further": [13, 16, 17, 18, 19, 31, 32, 35, 40, 43, 63, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 78, 81, 82, 83, 87, 88, 89, 91, 93, 95, 96, 97, 99, 100, 101, 112, 113, 114, 115, 116, 121, 124, 126, 129, 138, 140, 146, 149, 150, 151, 154, 155, 156, 157, 158, 163, 166, 167, 168, 170, 172, 173], "furthermor": [80, 106, 109, 140, 145, 148], "futur": [4, 14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 69, 70, 71, 72, 81, 112, 124, 140, 157], "futurewarn": [68, 69, 70, 71, 72], "fuzzi": [46, 47], "g": [6, 7, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 46, 48, 49, 50, 53, 54, 61, 62, 65, 67, 68, 69, 70, 71, 72, 73, 75, 80, 81, 82, 84, 87, 93, 94, 95, 98, 99, 101, 103, 106, 109, 111, 112, 113, 114, 122, 124, 125, 126, 127, 128, 129, 130, 140, 141, 142, 143, 144, 145, 147, 148, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173], "g0": 77, "g1": 77, "g_": [47, 78, 124, 129, 140, 141, 143, 144, 146, 149, 150, 156], "g_0": [12, 14, 15, 16, 22, 23, 25, 26, 28, 38, 39, 44, 45, 46, 47, 61, 63, 64, 75, 77, 80, 91, 92, 103, 111, 112, 116, 121, 124, 131, 132, 133, 134, 136, 137, 140, 142, 144, 145, 153, 154, 155, 157, 164, 165, 167, 170, 173], "g_1": [47, 75], "g_all": [61, 64], "g_all_po": 61, "g_ci": 64, "g_d": [140, 146, 150], "g_dml": 61, "g_dml_po": 61, "g_hat": [38, 39, 61, 80, 140], "g_hat0": [25, 26], "g_hat1": [25, 26], "g_i": [17, 19, 124, 127, 129, 130, 140, 142, 144], "g_k": 111, "g_nonorth": 61, "g_nosplit": 61, "g_nosplit_po": 61, "g_valu": 14, "g_x": 68, "gain": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 57, 75, 157, 158, 165, 172], "gain_statist": 172, "galleri": [75, 76, 103, 111, 112, 113, 117, 120, 124, 125, 127, 129, 138, 168, 172], "gama": 90, "gamma": [36, 42, 45, 63, 91, 94, 95, 97, 100, 124, 140, 146, 149], "gamma_0": [32, 66, 95, 101, 140, 146, 149], "gamma_a": [31, 40, 100], "gamma_bench": 100, "gamma_v": 100, "gap": [91, 100], "gapo": 22, "gate": [22, 26, 39, 51, 94, 95, 110, 172], "gate_obj": 111, "gatet": 111, "gaussian": [27, 28, 29, 61, 80, 103, 111, 112, 120, 123, 156, 171], "ge": [18, 31, 32, 87, 95, 111, 124, 128, 130], "geer": 171, "gelbach": [63, 91], "gener": [0, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 49, 54, 55, 60, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 75, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 102, 104, 109, 110, 111, 112, 114, 117, 118, 120, 122, 123, 124, 127, 129, 130, 131, 139, 140, 142, 144, 145, 148, 156, 158, 159, 160, 161, 162, 164, 165, 167, 171, 172, 173], "generate_treat": 96, "generate_weakiv_data": 98, "geom_bar": 64, "geom_dens": [64, 66], "geom_errorbar": 64, "geom_funct": 61, "geom_histogram": 61, "geom_hlin": 64, "geom_point": 64, "geom_til": 63, "geom_vlin": [61, 66], "geq": [17, 19, 97, 124], "german": 168, "get": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 53, 54, 65, 69, 71, 72, 75, 76, 77, 78, 94, 99, 100, 157, 158, 168, 169], "get_dummi": 94, "get_feature_names_out": [88, 91, 92], "get_legend_handles_label": [77, 78], "get_level_valu": 90, "get_logg": [61, 62, 63, 64, 65, 66, 102, 112, 121, 122, 123, 124, 139, 140, 156, 170, 173], "get_metadata_rout": [48, 49, 53, 54], "get_n_split": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "get_param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 53, 54, 90, 112, 115], "get_ylim": 88, "ggdid": 62, "ggplot": [61, 63, 64, 66], "ggplot2": [61, 63, 64, 66], "ggsave": 61, "ggtitl": 64, "gh": 172, "git": 169, "github": [62, 64, 70, 76, 84, 90, 94, 168, 171, 172], "githubusercont": [71, 84], "give": [64, 88, 92], "given": [12, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 34, 37, 38, 39, 40, 44, 45, 46, 47, 48, 49, 53, 54, 61, 63, 66, 68, 70, 71, 76, 78, 80, 85, 86, 91, 93, 94, 97, 100, 101, 103, 111, 124, 127, 128, 129, 140, 145, 156, 157, 163, 164, 165, 166, 167, 170, 172], "glmnet": [64, 65, 112, 122, 123, 172], "global": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 53, 54, 112, 121, 123, 124], "globalclassifi": 97, "globallearn": 97, "globalregressor": 97, "glrn": 65, "glrn_lasso": 65, "gmm": 98, "gname": 62, "go": [81, 82, 84, 88, 90, 97, 100], "goal": [76, 78, 85, 86, 124], "goe": 124, "goldman": 171, "good": [84, 89, 157, 158, 173], "gpu": 77, "gradient": [64, 92], "gradientboostingclassifi": 75, "gradientboostingregressor": 75, "gradual": 100, "gramfort": [168, 170], "grant": 168, "graph": [65, 66, 101, 173], "graph_ensemble_classif": 65, "graph_ensemble_regr": 65, "graph_obj": 97, "graph_object": [81, 82, 84, 100], "graphlearn": [65, 112, 120], "grasp": [78, 157, 158], "great": [68, 173], "greater": 173, "green": [61, 81, 82, 83, 96], "greg": 171, "grei": [64, 77, 78], "grenand": 171, "grey50": 63, "grid": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 63, 65, 69, 72, 76, 77, 78, 81, 82, 83, 84, 88, 89, 93, 94, 96, 100, 122, 123, 157, 163], "grid_arrai": [81, 82], "grid_basi": 88, "grid_bound": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 100], "grid_search": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 65, 112, 118, 122, 123, 173], "grid_siz": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 81, 82], "gridextra": 63, "gridsearchcv": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 112, 118], "grisel": [168, 170], "grk": 168, "grob": 63, "group": [7, 14, 16, 17, 19, 22, 26, 39, 60, 62, 70, 76, 77, 78, 79, 87, 88, 93, 94, 95, 100, 110, 124, 125, 127, 128, 129, 130, 140, 142, 144, 157, 160, 162], "group_0": 111, "group_1": [85, 86, 111], "group_2": [85, 86, 111], "group_3": [85, 86], "group_effect": 95, "group_ind": 87, "group_treat": 87, "groupbi": [69, 72, 77, 84, 92, 98], "gruber": 33, "gt": [60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 76, 77, 78, 79, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 104, 109, 170], "gt_combin": [16, 69, 70, 72, 124, 125, 127, 128, 129], "gt_dict": [69, 72], "guarante": [63, 91], "guber": 33, "guess": [99, 157, 158], "guid": [20, 21, 48, 49, 53, 54, 61, 62, 63, 65, 68, 69, 70, 71, 72, 78, 80, 87, 88, 91, 97, 99, 112, 119, 168, 170, 172], "guidelin": 172, "gunion": [65, 112, 120], "gxidclusterperiodytreat": 62, "h": [17, 18, 19, 31, 33, 40, 43, 62, 63, 91, 97, 98, 106, 109, 124, 171], "h20": 90, "h_0": [69, 72, 78, 87, 88, 99, 100, 157, 163, 173], "h_f": [46, 124], "ha": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 51, 52, 53, 54, 61, 62, 63, 64, 70, 72, 75, 76, 80, 84, 90, 91, 92, 93, 94, 97, 98, 99, 100, 104, 109, 111, 112, 113, 114, 124, 126, 130, 157, 158, 163, 164, 165, 166, 167, 168, 173], "had": 70, "half": [61, 80, 94, 103, 139], "hand": [46, 75, 90, 94, 98, 173], "handbook": 94, "handl": [62, 70, 75, 76, 77, 78, 105, 106, 109, 112, 113, 114, 172], "hansen": [10, 11, 34, 42, 44, 63, 84, 91, 98, 103, 168, 171], "happend": 75, "hard": [99, 157, 158], "harold": 171, "harsh": [48, 53], "hasn": [13, 16, 69, 71, 72], "hat": [61, 63, 80, 84, 87, 91, 94, 97, 102, 103, 111, 124, 139, 140, 156, 157, 158, 163, 166], "have": [16, 22, 23, 26, 29, 32, 35, 39, 51, 52, 53, 54, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 81, 82, 87, 88, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 104, 109, 111, 112, 114, 119, 120, 121, 124, 128, 140, 142, 144, 156, 157, 158, 164, 167, 169, 170, 172, 173], "hazlett": [100, 157, 158], "hc": [62, 171], "hc0": [51, 172], "hdm": [63, 91], "he": [66, 101], "head": [62, 63, 65, 69, 70, 71, 72, 73, 81, 82, 85, 86, 88, 90, 91, 92, 94, 97, 100, 104, 105, 109, 111, 170], "heat": [63, 91], "heatmap": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 63, 91, 100], "heavili": 75, "hei": 171, "height": [13, 16, 61, 63, 84, 90], "help": [62, 64, 70, 75, 83, 93, 95, 100, 124, 125, 139, 173], "helper": [105, 109, 172], "henc": [62, 64, 65, 92, 100, 112, 123, 140, 173], "here": [27, 28, 29, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 81, 82, 83, 85, 86, 87, 89, 91, 92, 93, 95, 96, 97, 100, 101, 104, 109, 112, 114, 117, 124, 127, 169], "herzig": 172, "heterogen": [17, 19, 26, 32, 41, 64, 87, 92, 93, 95, 110, 124, 134, 139, 171, 172, 173], "heteroskedast": [85, 86], "heurist": [61, 80, 103], "high": [34, 37, 38, 39, 64, 68, 84, 92, 93, 98, 102, 124, 135, 136, 137, 156, 168, 170, 171], "higher": [62, 64, 84, 92, 93, 94, 97, 98, 172, 173], "highli": [64, 92, 168], "highlight": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 67, 75, 76, 88, 90, 100, 172], "highlightcolor": [81, 82], "hint": 90, "hispan": 73, "hist": [77, 78], "hist_e401": 64, "hist_p401": 64, "histogram": [77, 78], "histori": 50, "histplot": 80, "hjust": 64, "hline": [104, 109, 156, 170, 173], "hold": [36, 50, 56, 63, 64, 66, 89, 90, 91, 92, 101, 111, 112, 123, 124, 128], "holdout": [112, 123, 139], "holm": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "home": [64, 68, 69, 71, 72, 92, 98, 100], "homogen": 124, "hook": 172, "hopefulli": 93, "horizont": [63, 68, 76, 77, 91], "hostedtoolcach": [68, 69, 70, 71, 72, 91, 92, 97, 100], "hot": 94, "hotstart_backward": [65, 112, 120, 121], "hotstart_forward": [65, 112, 120], "household": [64, 92, 93, 99], "how": [13, 17, 19, 41, 48, 49, 53, 54, 60, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 95, 96, 97, 98, 99, 100, 101, 103, 105, 109, 112, 119, 120, 121, 123, 124, 168, 169], "howev": [61, 64, 66, 80, 89, 90, 92, 97, 100, 101, 103, 124, 173], "hown": [64, 92, 93, 99, 173], "hpwt": [63, 91], "hpwt0": 63, "hpwtairmpdspac": 63, "href": 168, "hspace": 75, "hstack": [30, 68], "html": [65, 76, 168, 170, 172], "http": [33, 45, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 112, 122, 168, 169, 170, 172], "huber": [36, 66, 101, 124, 138, 140, 154, 155, 171], "hue": [69, 72, 92], "huge": 75, "hugo": 171, "husd": [65, 73, 104, 109, 170], "hyperparamet": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 64, 65, 73, 75, 77, 84, 90, 92, 110, 114, 116, 117, 118, 119, 120, 121, 123, 170, 172], "hypothes": [156, 171], "hypothesi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 64, 92, 99, 157, 163, 171], "hypothet": 100, "i": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 52, 53, 54, 55, 56, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 155, 156, 157, 158, 160, 162, 163, 164, 165, 167, 168, 169, 170, 172, 173], "i0": [67, 68, 124, 126], "i03": 168, "i1": [67, 124, 126], "i_": [42, 91, 95], "i_1": [63, 91], "i_2": [63, 91], "i_3": [63, 91], "i_4": 68, "i_est": 80, "i_fold": 63, "i_k": [63, 91, 102, 139, 156], "i_learn": 75, "i_level": 78, "i_rep": [61, 66, 67, 75, 80, 101, 103], "i_split": 91, "i_train": 80, "icp": 171, "id": [7, 16, 62, 63, 65, 69, 70, 71, 72, 91, 106, 109, 124, 125, 127, 129, 157, 162], "id_col": [7, 16, 69, 70, 71, 72, 106, 109, 124, 125, 127, 129], "id_var": 91, "idea": [64, 65, 76, 92, 93, 100, 112, 122, 124, 157, 158, 173], "ideal": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "ident": [17, 18, 19, 31, 32, 35, 40, 42, 52, 65, 70, 76, 78, 88, 90, 97, 112, 114, 124, 130, 140, 148, 157, 163], "identfi": 100, "identif": [69, 70, 71, 72, 97, 98, 124, 173], "identifi": [7, 63, 64, 67, 70, 77, 87, 91, 92, 93, 97, 100, 105, 106, 107, 108, 109, 111, 124, 126, 128, 130, 138, 157, 167, 172], "identifii": 111, "idnam": 62, "idx_gt_att": 16, "idx_learn": 77, "idx_tau": [83, 93, 96], "idx_treat": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 77, 78, 157, 163], "ieee": 171, "ifels": 62, "ignor": [48, 49, 53, 54, 70, 75, 76, 77, 80, 97], "ignore_index": 77, "ii": [63, 91], "iid": [69, 72, 124, 126], "iivm": [20, 21, 25, 33, 93, 102, 111, 133, 147, 168, 172], "iivm_summari": 92, "iivmglmnet": 64, "iivmrang": 64, "iivmrpart": 64, "iivmxgboost11861": 64, "ij": [43, 63, 66, 78, 91, 101], "ilia": 171, "illustr": [61, 63, 64, 65, 66, 67, 68, 70, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 95, 96, 99, 100, 101, 103, 112, 113, 114, 117, 118, 121, 123, 173], "iloc": [67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 91, 94, 98], "immedi": 169, "immun": [139, 171], "impact": [60, 75, 79, 94, 99], "implement": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 53, 54, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 80, 84, 88, 91, 92, 94, 97, 99, 100, 101, 103, 110, 111, 112, 115, 118, 119, 123, 125, 126, 128, 130, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 170, 171, 172, 173], "impli": [31, 40, 63, 64, 69, 72, 91, 92, 93, 97, 111, 124, 130, 157, 159, 160, 161, 162, 164, 165], "implicitli": [124, 128], "implment": [68, 76, 124, 125], "import": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 55, 56, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 169, 170, 172, 173], "importlib": 84, "impos": 100, "improv": [67, 69, 72, 75, 95, 124, 172], "in_sample_norm": [12, 14, 15, 16, 67, 140, 141, 142, 143, 144, 157, 159, 160, 161, 162], "inbuild": 75, "inbuilt": 75, "inc": [64, 92, 93, 99, 173], "includ": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 50, 57, 62, 64, 68, 69, 70, 71, 72, 76, 77, 78, 85, 86, 88, 92, 97, 99, 100, 111, 124, 156, 157, 163, 164, 165, 167, 169, 172, 173], "include_bia": [88, 91, 92], "include_never_tr": [17, 19, 72], "include_scenario": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 100], "incom": [64, 92, 93, 95, 99, 173], "incorpor": [65, 99, 157, 163], "increas": [19, 69, 72, 75, 87, 89, 91, 100, 173], "increment": 172, "ind": 92, "independ": [12, 14, 15, 16, 18, 31, 32, 40, 47, 63, 65, 68, 87, 89, 91, 95, 124, 130, 138, 140, 141, 142, 143, 144, 172], "index": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 61, 63, 68, 73, 76, 77, 80, 84, 85, 86, 90, 91, 92, 94, 95, 103, 104, 109, 139, 140, 141, 142, 143, 144, 168, 170], "index_col": 84, "india": [139, 171], "indic": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 36, 37, 38, 39, 46, 50, 51, 63, 64, 66, 68, 69, 70, 71, 72, 87, 91, 92, 93, 97, 98, 100, 101, 102, 104, 105, 108, 109, 111, 124, 126, 130, 131, 138, 139], "individu": [17, 19, 22, 23, 26, 53, 54, 62, 64, 68, 69, 72, 76, 77, 78, 85, 86, 87, 90, 92, 93, 97, 99, 111, 124, 173], "individual_df": 68, "induc": [110, 139], "industri": [63, 91], "inf": [5, 6, 7, 8, 9, 62, 69, 70, 71, 72, 98], "inf_model": 140, "infer": [34, 42, 60, 61, 63, 70, 75, 77, 79, 80, 84, 90, 91, 98, 103, 110, 124, 139, 168, 170, 171, 172], "inferenti": 173, "infinit": [5, 6, 7, 8, 9, 98, 104, 105, 109, 172], "influenc": [25, 49, 54, 77, 124], "info": [60, 65, 69, 70, 71, 72, 73, 76, 77, 78, 87, 89, 90, 91, 92, 93, 99, 104, 106, 109, 120, 170, 172], "inform": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 51, 53, 54, 60, 65, 69, 70, 71, 72, 75, 79, 81, 82, 97, 98, 99, 100, 124, 125, 157, 158, 171], "infti": [61, 80, 103, 124, 130], "inher": 100, "inherit": [94, 105, 106, 107, 108, 109, 172], "initi": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 64, 65, 66, 67, 69, 70, 71, 72, 76, 77, 83, 92, 93, 96, 97, 99, 100, 101, 104, 106, 109, 111, 112, 114, 116, 121, 123, 124, 139, 170, 172, 173], "inlin": [73, 94], "inlinebackend": 94, "inner": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 112, 114, 116, 121], "innermost": [112, 121], "input": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 49, 54, 65, 70, 76, 99, 102, 112, 117, 120, 124, 128, 156, 157, 158, 163], "insensit": 124, "insid": [48, 49, 53, 54], "insight": [84, 100], "insignific": 99, "inspect": [76, 170], "inspir": [31, 33, 34, 36, 69, 72, 100], "instabl": 56, "instal": [64, 77, 90, 97, 124, 172], "install_github": 169, "instanc": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 53, 54, 64, 65, 77, 92, 112, 120], "instanti": [63, 64, 91, 92, 112, 119, 122, 139], "instead": [4, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 53, 54, 60, 62, 64, 68, 69, 70, 71, 72, 76, 77, 78, 79, 81, 87, 90, 92, 93, 104, 109, 111, 112, 115, 124, 140, 144, 157, 159, 160, 161, 162, 165, 166, 172], "instruct": [169, 172], "instrument": [5, 6, 7, 8, 9, 10, 25, 33, 37, 38, 42, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 76, 77, 78, 87, 89, 91, 92, 93, 96, 99, 101, 104, 106, 107, 108, 109, 112, 113, 124, 126, 127, 129, 133, 136, 140, 149, 151, 156, 170, 173], "instrument_effect": 60, "instrument_impact": 79, "instrument_strength": 98, "insuffienct": 90, "int": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 41, 46, 47, 51, 52, 55, 62, 63, 64, 67, 70, 76, 77, 79, 83, 95, 96, 98, 100, 101, 106, 109], "int32": 70, "int64": [69, 71, 72, 73, 91, 104, 106, 109, 170], "int8": [92, 93, 99], "integ": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 65, 112, 120, 121], "integr": [77, 90, 100, 157, 167, 172], "intend": [46, 65, 100, 173], "intent": [124, 173], "inter": [112, 114], "interact": [22, 23, 25, 26, 31, 33, 34, 35, 46, 47, 76, 78, 100, 110, 112, 119, 133, 134, 164, 165, 168, 172, 173], "interchang": 156, "interest": [25, 26, 31, 37, 38, 39, 40, 61, 64, 66, 67, 80, 84, 89, 92, 93, 97, 98, 101, 103, 111, 124, 126, 128, 131, 132, 133, 134, 135, 136, 137, 138, 140, 156, 170, 173], "interfac": [37, 62, 64, 65, 77, 104, 109, 112, 123, 139, 170], "intermedi": 100, "intern": [19, 37, 62, 64, 65, 78, 90, 93, 112, 114, 117, 123, 124, 125, 171], "internet": [64, 92, 93], "interpret": [69, 70, 71, 72, 85, 86, 89, 98, 100, 111, 157, 158, 164, 165, 166, 167, 169, 173], "intersect": [100, 157, 163, 172], "interv": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 51, 62, 63, 64, 66, 67, 69, 70, 71, 72, 76, 77, 78, 81, 82, 83, 85, 86, 88, 91, 93, 96, 97, 99, 101, 110, 111, 139, 140, 157, 163, 170, 171, 172, 173], "intial": 97, "introduc": [61, 80, 103, 104, 109, 156, 172, 173], "introduct": [61, 63, 65, 80, 91, 93, 99, 112, 119, 120, 123, 124, 126, 130, 157, 158], "introductori": [62, 100], "intrument": [66, 101], "intspecifi": 46, "intuit": 100, "inuidur1": [65, 73, 104, 109, 170], "inuidur1femaleblackothracedep1dep2q2q3q4q5q6agelt35agegt54durablelusdhusdtg": [65, 104, 109, 170], "inuidur2": [73, 104, 109, 170], "inv_sigmoid": 94, "invalid": [61, 72, 80, 91, 98, 103], "invari": [124, 126, 130], "invers": [22, 23, 24, 25, 26, 27, 28, 29, 30, 66, 101, 157, 164, 165], "invert": [25, 98], "invert_yaxi": 91, "investig": [84, 90, 100], "involv": [111, 112, 122, 140, 173], "io": [76, 94, 172], "ipw_norm": 172, "ipykernel_14826": 68, "ipykernel_17666": 81, "ipykernel_19233": 91, "ipynb": [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101], "ira": [64, 92, 93], "irm": [0, 9, 12, 14, 15, 16, 20, 21, 37, 38, 39, 51, 52, 75, 76, 77, 78, 82, 86, 87, 100, 101, 102, 108, 109, 110, 112, 113, 114, 127, 129, 134, 148, 164, 165, 168, 172, 173], "irm_summari": 92, "irmglmnet": 64, "irmrang": 64, "irmrpart": 64, "irmxgboost8047": 64, "irrespect": 100, "irrevers": [124, 130], "is_classifi": [12, 14, 15, 22, 23, 25, 26, 39], "is_gat": [22, 26, 39, 51], "isfinit": [69, 70, 71, 72], "isnan": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 75, 112, 113], "isoton": [55, 56, 100], "isotonicregress": 100, "issn": 84, "issu": [13, 16, 100, 168, 171, 172], "ite": [69, 72, 76, 77, 78, 85, 86, 87], "ite_lower_quantil": [69, 72], "ite_mean": [69, 72], "ite_upper_quantil": [69, 72], "item": [25, 77, 92, 102, 112, 121, 123, 139], "iter": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 60, 66, 67, 91, 92, 97, 101, 112, 118, 120, 156], "itertool": 84, "its": [48, 49, 77, 100, 102, 111, 112, 119, 124, 126, 127, 129, 139, 140, 156], "iv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 37, 38, 39, 42, 43, 61, 63, 80, 91, 103, 104, 109, 133, 136, 152, 153, 157, 166, 168, 172, 173], "iv_2": 60, "iv_var": [63, 91], "iv\u00e1n": [139, 171], "j": [10, 11, 17, 18, 19, 31, 33, 34, 36, 40, 41, 42, 43, 44, 45, 61, 62, 63, 65, 66, 78, 80, 84, 91, 94, 98, 101, 103, 112, 122, 124, 131, 156, 168, 170], "j_": [63, 91], "j_0": 156, "j_1": [63, 91], "j_2": [63, 91], "j_3": [63, 91], "j_k": [63, 91], "jame": 171, "jan": 172, "janari": [69, 72], "janni": [64, 92], "januari": 69, "jasenakova": 172, "javanmard": 171, "jbe": [63, 91], "jeconom": [17, 18, 19, 31, 40, 62], "jerzi": 171, "jia": 100, "jitter": [16, 76, 77], "jitter_strength": [76, 77], "jitter_valu": 16, "jk": [124, 132], "jmlr": [65, 168, 170, 172], "job": [64, 92, 93], "john": 171, "joint": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 51, 69, 70, 71, 72, 78, 81, 82, 83, 85, 86, 88, 93, 96, 98, 124, 126, 156, 172, 173], "joint_prob": 89, "jointli": [96, 111], "jonathan": 171, "joss": [65, 112, 122, 168, 170], "journal": [10, 11, 17, 18, 19, 31, 36, 40, 41, 43, 44, 62, 63, 65, 84, 91, 94, 98, 100, 103, 112, 122, 168, 170, 171, 172], "jss": 168, "juliu": 172, "jump": [95, 97, 124], "jun": [62, 171], "jupyt": [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101], "juraj": 171, "just": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 65, 67, 68, 69, 72, 76, 78, 83, 85, 86, 87, 88, 95, 96, 124, 140, 141, 142, 143, 144, 157, 158], "justif": [139, 157, 158], "k": [10, 17, 18, 19, 33, 34, 36, 40, 41, 42, 43, 44, 61, 63, 65, 75, 80, 90, 91, 97, 98, 102, 103, 110, 111, 124, 156, 173], "k_h": [97, 124], "kaggl": [64, 92], "kallu": [83, 93, 96, 98, 99, 140, 146, 149, 150, 171], "kappa": 124, "kato": [43, 63, 91, 156, 171], "kb": [70, 71, 76, 77, 78, 87, 89, 91, 92, 93, 99, 104, 106, 109, 170], "kde": [27, 28, 29, 92], "kdeplot": [67, 75, 101], "kdeunivari": [27, 28, 29], "kecsk\u00e9sov\u00e1": 172, "keel": 98, "keep": [49, 54, 62, 76, 88, 100, 108, 109, 173], "kei": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 52, 63, 64, 76, 81, 82, 85, 86, 90, 91, 92, 93, 97, 100, 112, 114, 123, 124, 140, 157, 163, 172], "keith": 171, "kelz": 98, "kengo": 171, "kennedi": 98, "kept": [107, 109], "kernel": [27, 28, 29, 46, 49, 54, 97, 124], "kernel_regress": 97, "kernelreg": 97, "keyword": [17, 18, 19, 22, 26, 39, 43, 44, 45, 47, 51], "kf": 139, "kfold": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 91, 139], "kind": [60, 79, 92], "kj": [17, 18, 19, 33, 34, 36, 40, 41, 42, 43, 44, 61, 63, 80, 91, 103], "klaassen": [33, 75, 84, 90, 100, 168, 171], "klaa\u00dfen": 33, "kluge": 172, "knau": 171, "know": [67, 95, 98], "knowledg": [60, 75, 79, 94, 95], "known": [75, 87, 97, 98, 100, 112, 113, 124, 130], "kohei": 171, "kotthof": 65, "kotthoff": [65, 112, 122, 168, 170], "krueger": 94, "kueck": [64, 92], "kurz": [168, 171, 172], "kwarg": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 43, 44, 45, 46, 47, 48, 51, 90, 124], "l": [63, 65, 66, 73, 81, 82, 91, 98, 100, 101, 112, 122, 157, 166, 168, 170], "l1": [92, 101, 124], "l_hat": [38, 39, 61, 80, 140], "lab": 66, "label": [13, 16, 48, 53, 68, 76, 77, 78, 80, 81, 82, 83, 85, 86, 88, 89, 90, 93, 94, 96, 97], "labor": 94, "laffer": 171, "laff\u00e9r": [36, 66, 101, 124, 138, 140, 154, 155], "lal": [94, 172], "lambda": [63, 64, 65, 66, 69, 72, 92, 94, 95, 112, 120, 122, 123, 124, 140, 141, 142, 156, 170], "lambda_": 84, "lambda_0": [140, 141, 142], "lambda_l1": [76, 77], "lambda_l2": [76, 77], "lambda_t": [18, 19, 72], "land": 95, "lang": [65, 112, 122, 168, 170], "langl": [32, 95], "lanni": 98, "lappli": 139, "larg": [61, 75, 80, 87, 90, 94, 100, 124], "larger": [26, 41, 62, 97, 100, 157, 163], "largest": 75, "largli": 75, "lasso": [63, 64, 65, 66, 70, 92, 101, 112, 117, 118, 122, 123, 170, 171], "lasso_class": [64, 92], "lasso_pip": [65, 112, 122], "lasso_summari": 92, "lassocv": [30, 70, 84, 91, 92, 101, 112, 118, 124, 156, 170], "last": [18, 65, 169], "late": [25, 60, 64, 92, 98, 124, 133, 140, 147], "latent": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 99, 157, 166, 167], "later": [64, 65, 76, 97, 100, 112, 123, 173], "latest": 168, "latter": [53, 54, 98, 124], "layout": 84, "lbrace": [25, 26, 33, 34, 36, 63, 91, 102, 124, 131, 133, 134, 139, 140, 145, 156, 157, 164], "ldot": [37, 38, 39, 63, 66, 89, 91, 101, 102, 124, 135, 136, 137, 139, 156, 170], "le": [18, 67, 95, 111, 124, 126, 140, 149, 150], "lead": [62, 100, 124], "leadsto": 156, "lear": [65, 112, 122, 168, 170], "learn": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 41, 43, 44, 45, 46, 60, 64, 65, 70, 73, 75, 76, 78, 79, 83, 84, 88, 90, 92, 93, 94, 96, 97, 98, 100, 104, 109, 110, 112, 115, 139, 140, 156, 157, 158, 172, 173], "learner": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 50, 55, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 72, 77, 80, 81, 82, 84, 89, 91, 92, 93, 98, 99, 100, 101, 102, 103, 110, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 126, 127, 129, 139, 140, 156, 157, 163, 172, 173], "learner_class": [30, 172], "learner_cv": 65, "learner_dict": 77, "learner_forest_classif": 65, "learner_forest_regr": 65, "learner_l": 99, "learner_lasso": 65, "learner_list": 75, "learner_m": 99, "learner_nam": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 55, 76, 77, 112, 123], "learner_pair": 77, "learner_param_v": 65, "learner_rf": 156, "learnerclassif": 65, "learnerregr": 65, "learnerregrcvglmnet": 65, "learnerregrrang": [65, 112, 121], "learning_r": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 69, 72, 76, 77, 80, 83, 93, 96, 97, 100, 103], "least": [60, 64, 79, 92, 93, 99, 124, 128, 139], "leav": [66, 100, 101], "left": [17, 19, 33, 34, 36, 42, 43, 61, 63, 75, 77, 78, 80, 91, 92, 93, 94, 96, 97, 103, 124, 140, 141, 142, 143, 144, 156, 157, 159, 160, 161, 162, 164, 165], "legend": [64, 68, 69, 72, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 88, 89, 93, 94, 96], "lemp": 70, "len": [75, 76, 77, 78, 83, 90, 91, 93, 96, 98], "length": [12, 13, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 65, 67, 69, 72, 98, 112, 116, 121], "leq": [63, 91], "less": [62, 64, 92, 93, 97, 100], "lester": 171, "let": [17, 18, 19, 31, 35, 40, 61, 62, 64, 65, 66, 67, 69, 72, 75, 76, 77, 78, 80, 83, 85, 86, 88, 92, 93, 96, 100, 101, 102, 103, 112, 116, 121, 124, 126, 130, 138, 157, 158, 167, 173], "level": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 46, 51, 63, 64, 66, 67, 68, 69, 70, 71, 72, 76, 77, 78, 81, 82, 83, 85, 86, 87, 88, 91, 92, 93, 96, 98, 99, 100, 101, 112, 114, 131, 132, 140, 145, 157, 163, 164, 173], "level_0": [65, 91], "level_1": 91, "level_bound": [76, 77, 78], "leverag": 77, "levi": 98, "levinsohn": [63, 91], "lewi": 171, "lgbm": [76, 77], "lgbm_argument": 77, "lgbmclassifi": [67, 68, 69, 72, 75, 76, 77, 83, 93, 96, 97, 100], "lgbmlgbmregressorlgbmregressor": 76, "lgbmregressor": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 67, 68, 69, 72, 75, 76, 77, 80, 83, 93, 97, 100, 103], "lgr": [61, 62, 63, 64, 65, 66, 102, 112, 121, 122, 123, 124, 139, 140, 156, 170, 173], "lib": [68, 69, 70, 71, 72, 91, 92, 97, 100], "liblinear": [92, 101, 124], "librari": [60, 61, 62, 63, 64, 65, 66, 77, 102, 103, 104, 109, 112, 120, 121, 122, 123, 124, 139, 140, 156, 169, 170, 173], "licens": [168, 172], "lie": 171, "lightgbm": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 67, 68, 69, 72, 75, 76, 77, 80, 83, 93, 96, 97, 100], "like": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 53, 54, 62, 64, 65, 69, 70, 71, 72, 76, 84, 92, 93, 100, 112, 115, 116, 118, 124, 125, 139, 170, 173], "lim": 94, "lim_": [97, 124], "limegreen": [81, 82], "limit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 94, 124, 130, 171], "limits_": 111, "lin": [97, 100, 124], "line": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 68, 70, 76, 77, 78, 89, 100], "line_width": [76, 77], "linear": [17, 19, 20, 21, 22, 26, 31, 35, 38, 39, 40, 41, 42, 43, 44, 45, 51, 60, 61, 62, 63, 65, 67, 68, 70, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 88, 89, 90, 91, 98, 99, 100, 102, 103, 110, 111, 112, 116, 117, 118, 121, 123, 127, 128, 129, 135, 136, 137, 139, 141, 142, 143, 144, 145, 147, 148, 152, 153, 156, 163, 165, 166, 167, 168, 170, 171, 172, 173], "linear_learn": [69, 72], "linear_model": [22, 26, 30, 39, 51, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 84, 88, 91, 92, 97, 98, 100, 101, 112, 117, 118, 124, 156, 170], "linear_regress": 76, "linear_regressionlinearregress": 76, "linearli": [97, 124], "linearregress": [60, 69, 70, 71, 72, 75, 76, 77, 78, 79, 88, 97, 98, 100], "linearregressionlinearregress": 76, "linearscoremixin": [0, 140], "lineplot": [69, 72, 77, 78], "linestyl": [68, 69, 72, 76, 77, 78, 89, 90, 97], "linetyp": 66, "linewidth": [68, 69, 72, 76, 77], "link": [89, 100, 124, 135, 172], "linspac": [81, 82, 88, 100], "lint": 172, "linux": 169, "list": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 49, 54, 55, 61, 62, 63, 64, 65, 69, 70, 72, 76, 80, 81, 82, 91, 93, 95, 103, 112, 116, 119, 120, 121, 122, 123, 139, 140, 169, 172, 173], "list_confset": 25, "listedcolormap": 91, "literatur": [100, 124, 126, 130], "littl": [87, 98], "liu": [41, 89, 140, 151, 171], "ll": [65, 156, 173], "lllllllllllllllll": [104, 109, 170], "lm": [60, 62, 100], "ln_alpha_ml_l": 84, "ln_alpha_ml_m": 84, "load": [60, 62, 64, 65, 76, 84, 92, 93, 104, 109, 169, 170], "loader": 0, "loc": [68, 69, 70, 71, 72, 76, 77, 78, 80, 83, 84, 85, 86, 89, 91, 94, 96, 99, 100], "local": [25, 27, 76, 98, 111, 124, 133, 171, 172], "localconvert": 91, "locat": [83, 96, 124], "log": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 63, 67, 69, 70, 71, 72, 75, 76, 77, 84, 91, 93, 94, 99, 101, 112, 113, 117, 124, 126, 127, 129, 173], "log_odd": 95, "log_p": [63, 91], "log_reg": [60, 62], "logarithm": [77, 84], "logic": [25, 65, 112, 120, 121], "logical_not": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 75, 112, 113], "logist": [31, 37, 41, 47, 60, 62, 64, 66, 70, 78, 79, 92, 98, 100, 101, 135, 171, 172, 173], "logistic_regress": 76, "logisticregress": [60, 69, 70, 71, 72, 73, 76, 77, 78, 79, 88, 97, 98, 100], "logisticregressioncv": [30, 70, 75, 92, 101, 124], "logit": [17, 19, 37, 75, 94, 140, 151], "loglik": 65, "logloss": [64, 77, 92, 173], "logloss_m": 77, "logo": 172, "logspac": 92, "long": [7, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 61, 70, 75, 80, 99, 100, 157, 158, 167, 171], "longer": [69, 72, 104, 109, 124, 128], "look": [62, 64, 65, 67, 68, 69, 70, 71, 72, 75, 76, 83, 89, 92, 93, 96, 97, 99], "loop": [78, 98], "loss": [67, 69, 70, 71, 72, 75, 76, 77, 90, 93, 97, 99, 101, 112, 113, 124, 126, 127, 129], "loss_ml_g0": 75, "loss_ml_g1": 75, "loss_ml_m": 75, "low": [68, 87, 98, 111, 171], "lower": [25, 64, 65, 68, 69, 72, 77, 78, 83, 84, 87, 88, 93, 94, 96, 97, 98, 99, 100, 112, 122, 123, 157, 163, 167, 173], "lower_bound": [81, 82], "lplr": [89, 135, 151], "lpop": 70, "lpq": [27, 29, 93, 111, 149, 172], "lpq_0": 96, "lpq_1": 96, "lqte": 111, "lr": 97, "lrn": [60, 61, 62, 63, 64, 65, 66, 102, 112, 119, 120, 121, 122, 123, 124, 139, 140, 156, 170, 173], "lrn_0": 65, "lt": [60, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 76, 77, 78, 87, 89, 91, 92, 93, 95, 99, 100, 104, 109, 170], "lucien": 172, "luka": 171, "luk\u00e1\u0161": 36, "lusd": [65, 73, 104, 109, 170], "lvert": 84, "m": [7, 10, 11, 16, 30, 31, 37, 42, 43, 44, 61, 63, 65, 69, 72, 73, 75, 77, 80, 84, 87, 90, 91, 94, 98, 103, 106, 109, 110, 111, 112, 122, 124, 125, 127, 129, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 157, 159, 160, 161, 162, 164, 165, 166, 167, 168, 169, 170, 171, 172], "m_": [77, 78, 124, 127, 129, 131, 140, 142, 144, 145, 149, 156], "m_0": [12, 14, 15, 16, 22, 23, 24, 25, 26, 28, 37, 38, 39, 44, 45, 46, 61, 63, 64, 80, 84, 87, 90, 91, 92, 103, 111, 112, 116, 121, 124, 133, 134, 136, 137, 140, 141, 142, 143, 144, 146, 149, 150, 151, 153, 154, 155, 170, 173], "m_hat": [25, 26, 38, 39, 61, 80, 88, 140], "m_i": [97, 124], "ma": [43, 63, 91, 98, 124, 125, 171], "mac": 169, "machin": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 41, 43, 44, 45, 46, 60, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 78, 79, 83, 84, 88, 90, 92, 93, 94, 96, 97, 98, 99, 100, 101, 110, 112, 113, 122, 124, 126, 127, 129, 139, 140, 156, 157, 158, 172, 173], "machineri": [84, 171], "mackei": 171, "maco": 169, "made": [124, 138, 173], "mae": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 75, 112, 113], "maggi": 171, "magnitud": [41, 157, 158], "mai": [49, 54, 66, 67, 101], "main": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 71, 76, 84, 93, 100, 124, 156, 157, 158, 171, 173], "mainli": [75, 76, 100], "maintain": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 62, 168, 172], "mainten": 172, "major": [65, 100, 172], "make": [60, 75, 77, 78, 79, 90, 100, 111, 112, 119, 172, 173], "make_confounded_irm_data": [100, 172], "make_confounded_plr_data": 99, "make_did_cs2021": [7, 16, 69, 106, 109, 124, 125, 129], "make_did_cs_cs2021": [72, 124, 127], "make_did_sz2020": [5, 12, 15, 67, 105, 109, 124, 126], "make_heterogeneous_data": [81, 82, 85, 86, 87], "make_iivm_data": [25, 27, 111, 124], "make_irm_data": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 75, 88, 111, 112, 114, 124], "make_irm_data_discrete_treat": [76, 77, 78], "make_lplr_lzz2020": [37, 89, 124], "make_pipelin": 92, "make_pliv_chs2015": [38, 124], "make_pliv_multiway_cluster_ckms2021": [63, 91], "make_plr_ccddhnr2018": [6, 7, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 80, 90, 102, 103, 111, 112, 113, 116, 120, 121, 123, 124, 139, 140, 156, 157, 163, 172], "make_simple_rdd_data": [8, 46, 97, 107, 109, 124], "make_spd_matrix": 45, "make_ssm_data": [9, 66, 101, 108, 109, 124], "malt": [168, 171], "man": [60, 79], "manag": [112, 123, 169], "mandatori": [107, 109], "mani": [20, 21, 42, 61, 62, 63, 65, 67, 80, 90, 91, 103, 140, 156, 173], "manili": 51, "manipul": [64, 65, 97, 124], "manual": [64, 88, 90, 99, 173], "mao": 171, "map": [25, 48, 49, 53, 54, 62, 63, 91, 111, 124, 133], "mapsto": [102, 111], "mar": [36, 124], "march": [75, 84, 90], "margin": [81, 82, 100], "marit": [64, 92], "marker": [69, 72, 78, 100], "markers": [76, 94], "market": 94, "markettwo": 63, "markov": [45, 171], "marr": [64, 92, 93, 99, 173], "marshal": [112, 120], "martin": [36, 100, 168, 171, 172], "masatoshi": 171, "masip": [98, 172], "mask": 16, "maskedarrai": [124, 125], "master": [62, 70], "mat": 63, "match": [112, 123, 157, 166], "math": [30, 69, 70, 71, 72], "mathbb": [17, 18, 19, 20, 21, 25, 26, 31, 35, 38, 39, 40, 41, 63, 66, 67, 68, 69, 72, 75, 76, 77, 78, 87, 89, 90, 91, 94, 97, 101, 111, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166, 167, 170, 173], "mathcal": [17, 18, 19, 31, 32, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 47, 61, 63, 66, 68, 80, 83, 91, 95, 96, 101, 103, 124, 125, 128, 130], "mathop": 111, "mathrm": [31, 40, 69, 70, 71, 72, 97, 124, 125, 127, 128, 129, 130, 140, 142, 144, 157, 160, 162], "matia": 171, "matplotlib": [13, 16, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 96, 97, 100, 101], "matric": [95, 172], "matrix": [17, 18, 19, 31, 33, 34, 35, 36, 40, 42, 43, 44, 45, 49, 54, 61, 63, 64, 65, 66, 80, 91, 101, 103, 104, 109, 112, 122, 123, 156, 170, 172, 173], "matt": 171, "matter": [75, 94], "max": [17, 19, 64, 65, 76, 77, 88, 92, 93, 98, 102, 111, 112, 123, 124, 139, 140, 142, 144, 146, 156, 157, 160, 162, 170, 173], "max_depth": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 73, 76, 77, 92, 99, 102, 111, 112, 114, 124, 126, 139, 140, 156, 157, 163, 170, 173], "max_featur": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 73, 92, 99, 102, 111, 112, 114, 124, 139, 140, 156, 157, 163, 170, 173], "max_it": [76, 77, 91, 92, 100], "maxim": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 95, 111, 124], "maxima": 156, "maximum": [55, 56, 111, 112, 123], "mb": [69, 72, 73, 104, 109, 170], "mb706": 172, "mea": 33, "mean": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 48, 49, 53, 54, 60, 61, 63, 64, 67, 69, 72, 75, 76, 77, 78, 79, 80, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 96, 98, 99, 100, 103, 112, 113, 123, 124, 156, 173], "mean_absolute_error": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 75, 112, 113], "meant": [70, 111, 172], "measir": 99, "measur": [62, 65, 70, 84, 90, 98, 99, 100, 112, 122, 123, 124, 125, 138, 157, 158, 164, 165, 166, 167, 173], "measure_col": 84, "measure_func": 62, "measure_pr": 62, "measures_r": 62, "mechan": [48, 49, 53, 54, 100, 124, 130], "median": [98, 100, 139], "medium": 77, "melt": 63, "membership": 100, "memori": [69, 70, 71, 72, 73, 76, 77, 78, 87, 89, 91, 92, 93, 99, 104, 106, 109, 170], "mention": [87, 111], "merg": [64, 92], "mert": [139, 171], "meshgrid": [81, 82, 100], "messag": [61, 62, 63, 64, 65, 66, 75, 77, 170, 172], "meta": [48, 49, 53, 54, 112, 170], "metadata": [48, 49, 53, 54], "metadata_rout": [48, 49, 53, 54], "metadatarequest": [48, 49, 53, 54], "method": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 76, 78, 79, 80, 81, 82, 83, 85, 86, 88, 91, 92, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 123, 124, 125, 126, 127, 129, 139, 140, 156, 157, 158, 163, 168, 170, 172], "methodolog": 171, "methodologi": 100, "metric": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 53, 77, 112, 113], "michael": 171, "michaela": 172, "michel": [168, 170], "michela": [36, 171], "mid": [64, 92, 94, 97, 124, 140, 153], "mid_point": [76, 77, 78], "might": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 63, 70, 75, 83, 88, 91, 95, 97, 99, 100, 112, 114, 124], "mild": [61, 80, 103], "militari": 94, "miller": [63, 91], "mimic": 100, "min": [17, 19, 63, 64, 65, 66, 69, 70, 71, 72, 76, 77, 83, 88, 91, 92, 93, 96, 97, 102, 112, 120, 123, 124, 128, 139, 140, 156, 170, 173], "min_": 111, "min_child_sampl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76], "min_data_in_leaf": 77, "min_samples_leaf": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 52, 77, 87, 89, 92, 99, 102, 111, 112, 114, 124, 126, 139, 140, 156, 157, 163, 173], "min_samples_split": [92, 124, 125, 127, 129], "minim": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 52, 64, 75, 92, 97, 124], "minimum": [55, 56, 70, 115, 119, 123], "minor": [61, 70, 80, 103, 140, 172], "minsplit": 64, "minut": 90, "mirror": [104, 109], "miruna": 171, "mislead": 172, "miss": [5, 6, 7, 8, 9, 30, 65, 104, 105, 109, 112, 120, 121, 124, 140, 154, 172], "missing": [36, 66, 101], "misspecif": 67, "misspecifi": 67, "mit": [168, 170], "mix": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "mixin": [0, 20, 21, 140], "ml": [45, 63, 64, 65, 70, 84, 90, 91, 92, 97, 98, 102, 110, 112, 121, 123, 124, 139, 168, 171, 172], "ml_a": 37, "ml_g": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 50, 60, 61, 62, 64, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 83, 85, 87, 88, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 111, 112, 114, 120, 122, 124, 125, 126, 127, 129, 172], "ml_g0": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 62, 64, 67, 69, 70, 71, 73, 75, 92, 99, 112, 114, 124, 126, 129], "ml_g1": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 64, 67, 69, 70, 71, 73, 75, 92, 99, 112, 114, 124, 126, 129], "ml_g_d0": [101, 124], "ml_g_d0_t0": [67, 72, 124, 126, 127], "ml_g_d0_t1": [67, 72, 124, 126, 127], "ml_g_d1": [101, 124], "ml_g_d1_t0": [67, 72, 124, 126, 127], "ml_g_d1_t1": [67, 72, 124, 126, 127], "ml_g_d_lvl0": [76, 77, 124], "ml_g_d_lvl1": [76, 77, 124], "ml_g_param": 76, "ml_g_params_pipelin": 76, "ml_g_pipelin": 76, "ml_g_sim": 30, "ml_l": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 63, 64, 65, 73, 80, 82, 86, 90, 91, 92, 94, 99, 102, 103, 112, 113, 116, 117, 118, 120, 121, 123, 124, 139, 140, 156, 157, 163, 170, 172, 173], "ml_l_bonu": 170, "ml_l_forest": 65, "ml_l_forest_pip": 65, "ml_l_lasso": 65, "ml_l_lasso_pip": 65, "ml_l_param": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 112, 117, 173], "ml_l_rf": 173, "ml_l_sim": 170, "ml_l_tune": [112, 118], "ml_l_xgb": 173, "ml_m": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170, 172, 173], "ml_m_bench_control": 100, "ml_m_bench_treat": 100, "ml_m_bonu": 170, "ml_m_forest": 65, "ml_m_forest_pip": 65, "ml_m_lasso": 65, "ml_m_lasso_pip": 65, "ml_m_param": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76, 112, 117, 173], "ml_m_params_pipelin": 76, "ml_m_pipelin": 76, "ml_m_rf": 173, "ml_m_sim": [30, 170], "ml_m_studi": 76, "ml_m_tune": [112, 118], "ml_m_xgb": 173, "ml_param_spac": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76, 112, 117, 173], "ml_pi": [30, 66, 101, 124], "ml_pi_sim": 30, "ml_r": [25, 38, 60, 63, 64, 79, 91, 92, 98, 124, 172], "ml_r0": 124, "ml_r1": [64, 92, 124], "ml_t": [37, 89, 124], "mlr": [65, 112, 122], "mlr3": [60, 61, 62, 63, 64, 66, 102, 112, 119, 120, 121, 122, 123, 124, 139, 140, 156, 168, 170, 172, 173], "mlr3book": [65, 112, 120, 122, 123], "mlr3extralearn": [64, 112, 119], "mlr3filter": 65, "mlr3learner": [60, 61, 62, 63, 64, 102, 112, 119, 120, 121, 123, 124, 139, 140, 156, 170, 173], "mlr3measur": 62, "mlr3pipelin": [112, 119, 120, 122, 172], "mlr3tune": [65, 112, 122, 123, 172, 173], "mlr3vers": 64, "mlrmeasur": 62, "mode": [100, 169], "model": [0, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 49, 51, 52, 54, 57, 60, 61, 62, 63, 65, 67, 68, 69, 70, 71, 72, 75, 79, 80, 83, 84, 87, 91, 93, 96, 99, 102, 103, 104, 106, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 123, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 141, 142, 143, 144, 145, 147, 148, 151, 152, 153, 158, 163, 164, 165, 166, 167, 168, 171, 172], "model_data": [64, 92], "model_label": 90, "model_list": 77, "model_select": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 80, 91, 112, 118, 139], "modellist": [77, 88], "modelmlestimatelowerupp": 64, "modelvers": 77, "modern": [65, 112, 122, 168, 170], "modul": [77, 97, 109, 124, 169], "molei": [41, 171], "moment": [20, 21, 63, 91, 124, 127, 128, 129, 140, 156, 157, 158, 167, 170], "monoton": 124, "mont": [31, 32, 35, 40, 81, 82, 85, 86], "montanari": 171, "month": [69, 72], "more": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 41, 51, 60, 62, 64, 69, 70, 71, 72, 75, 76, 77, 78, 79, 81, 82, 84, 88, 90, 92, 93, 97, 99, 100, 102, 111, 112, 113, 117, 118, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 138, 140, 148, 156, 157, 158, 163, 167, 170, 173], "moreov": [64, 65, 70, 84, 112, 120, 121, 156, 173], "mortgag": [64, 92, 93], "most": [64, 75, 83, 92, 93, 96, 100, 111, 112, 115, 117, 124, 157, 163, 169], "motiv": [100, 103], "motivation_example_bch": 84, "mp": 62, "mpd": [63, 91], "mpdta": 70, "mpg": 91, "mse": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 65, 84, 112, 122, 123, 173], "mserd": 97, "msg": 69, "msr": [65, 112, 122, 123, 173], "mtry": [64, 65, 102, 112, 123, 124, 139, 140, 156, 173], "mu": 68, "mu_": 68, "mu_0": 124, "mu_mean": 68, "much": [64, 65, 76, 77, 92, 97, 98, 100, 173], "muld": [73, 104, 109, 170], "multi": [16, 48, 53, 62, 63, 81, 82, 91, 140, 142, 144, 172], "multiclass": [65, 90], "multiindex": 91, "multioutput": [49, 54], "multioutputregressor": [49, 54], "multipl": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 62, 63, 64, 66, 67, 70, 71, 76, 77, 88, 91, 92, 99, 100, 101, 104, 109, 112, 116, 121, 122, 128, 132, 136, 139, 156, 157, 158, 171, 172, 173], "multipletest": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "multipli": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 80, 110, 111, 140, 173], "multiprocess": [83, 93, 96], "multitest": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "multivariate_norm": 30, "multiwai": [43, 63, 91, 171], "music": 171, "must": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 56, 89, 104, 105, 109, 112, 121, 123, 124], "mutat": 65, "mutual": [22, 26, 39, 64, 85, 86, 92, 93, 111], "my_sampl": 139, "my_task": 139, "n": [12, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 60, 61, 63, 65, 66, 68, 69, 72, 76, 77, 78, 79, 80, 83, 84, 87, 91, 94, 95, 96, 97, 98, 101, 102, 103, 111, 112, 122, 124, 130, 139, 156, 168, 169], "n_": [35, 68, 72, 157, 160, 162], "n_aggreg": 13, "n_coef": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 157, 163], "n_color": [69, 72], "n_complier": 96, "n_core": [83, 93, 96], "n_estim": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 67, 68, 69, 72, 73, 76, 77, 80, 81, 82, 83, 85, 86, 87, 92, 93, 95, 96, 97, 99, 100, 102, 103, 111, 112, 114, 116, 124, 126, 139, 140, 156, 157, 163, 170, 173], "n_eval": [65, 112, 122, 123, 173], "n_featur": [48, 49, 53, 54], "n_fold": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 61, 62, 63, 64, 67, 69, 70, 72, 73, 75, 80, 81, 82, 83, 85, 86, 88, 89, 91, 92, 93, 94, 95, 96, 97, 99, 100, 103, 112, 116, 120, 121, 123, 139, 170, 173], "n_folds_inn": 37, "n_folds_per_clust": [63, 91], "n_folds_tun": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "n_framework": 13, "n_iter": [46, 97, 124], "n_iter_randomized_search": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 112, 118], "n_job": [83, 92, 93, 96], "n_jobs_cv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 75], "n_jobs_model": [16, 23, 29, 83, 93, 96], "n_jobs_optuna": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "n_learner": 77, "n_level": [35, 76, 77, 78], "n_model": 76, "n_ob": [7, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 47, 51, 52, 61, 65, 66, 67, 68, 69, 72, 75, 76, 77, 78, 80, 81, 82, 85, 86, 87, 88, 89, 90, 97, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 111, 112, 114, 117, 118, 122, 123, 124, 125, 126, 127, 129, 139, 156, 157, 163, 170], "n_output": [48, 49, 53, 54], "n_period": [17, 19, 69, 72], "n_pre_treat_period": [17, 19, 69, 72], "n_rep": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 61, 62, 63, 66, 67, 69, 72, 73, 75, 77, 78, 80, 87, 88, 91, 97, 99, 100, 101, 103, 112, 113, 114, 116, 120, 121, 139, 157, 163, 170, 173], "n_rep_boot": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 51, 69, 70, 71, 72, 78, 81, 82, 83, 85, 86, 88, 93, 96, 156], "n_sampl": [48, 49, 53, 54, 95, 98], "n_samples_fit": [49, 54], "n_split": 139, "n_t": 68, "n_target": [53, 54], "n_theta": 13, "n_time_period": 68, "n_trial": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76, 112, 117, 173], "n_true": [83, 96], "n_var": [61, 65, 80, 103, 104, 109, 112, 117, 118, 122, 123, 156, 170], "n_w": 95, "n_x": [32, 81, 82, 85, 86, 87], "na": [5, 6, 7, 8, 9, 61, 63, 66, 103, 172], "na_real_": [63, 172], "naiv": [61, 80, 103], "name": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 50, 53, 54, 55, 61, 62, 63, 68, 69, 70, 72, 76, 77, 85, 86, 87, 90, 91, 97, 99, 100, 112, 114, 121, 123, 169, 172], "namespac": 62, "nan": [5, 6, 7, 8, 9, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 67, 68, 75, 76, 77, 78, 80, 83, 85, 86, 90, 92, 93, 96, 101, 103, 112, 113], "nanmean": 80, "narita": 171, "nat": [69, 72], "nathan": 171, "nation": [100, 139, 171], "nativ": 62, "natt": 95, "natur": 100, "nbest": 77, "nbviewer": 76, "ncol": [63, 64, 65, 89, 97, 104, 109, 112, 122, 123, 156, 170], "ncoverag": 75, "ndarrai": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 55, 104, 109], "nearli": 75, "necess": [63, 91], "necessari": [62, 63, 77, 90, 91, 97, 124, 169], "need": [12, 14, 15, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 61, 62, 64, 66, 77, 79, 80, 90, 93, 98, 101, 112, 115, 116, 121, 139, 140, 151, 157, 167, 172, 173], "neg": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 49, 54], "neg_log_loss": 76, "neg_mean_squared_error": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "neighborhood": [97, 156], "neither": [5, 6, 7, 8, 9, 63, 91, 98, 104, 109], "neng": 171, "neq": [77, 97, 124], "nest": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 53, 54, 112, 114, 116, 121, 140, 155, 157, 163], "net": [93, 99, 173], "net_tfa": [64, 92, 93, 99, 173], "network": 77, "nev": [124, 127, 129, 130], "never": [17, 19, 25, 62, 63, 69, 70, 71, 72, 91, 124, 130, 172], "never_tak": [25, 64, 92], "never_tr": [14, 16, 69, 70, 71, 72, 124, 125, 127, 129], "nevertheless": 88, "new": [60, 61, 62, 63, 64, 65, 66, 81, 82, 90, 92, 95, 102, 103, 104, 109, 111, 112, 120, 121, 122, 123, 124, 139, 140, 156, 168, 170, 171, 172, 173], "new_data": [81, 82, 95], "newei": [10, 11, 44, 63, 84, 91, 100, 103, 168, 171], "newest": 172, "next": [62, 64, 65, 75, 81, 82, 83, 87, 89, 92, 93, 95, 96, 98, 100, 172], "neyman": [63, 91, 102, 110, 157, 167, 168, 171], "nfold": [63, 64, 66, 124], "nh": 124, "nice": [62, 77], "nifa": [92, 93, 99], "nil": 100, "nine": [63, 91], "nlogloss": 77, "nn": 97, "noack": [97, 124, 171, 172], "node": [64, 65, 102, 124, 139, 140, 156, 170, 173], "nois": [47, 94, 95], "nomin": 98, "non": [14, 17, 18, 19, 25, 43, 44, 45, 46, 60, 61, 64, 68, 69, 72, 76, 79, 80, 92, 93, 95, 97, 112, 123, 139, 140, 142, 144, 156, 157, 160, 162], "non_orth_scor": [61, 80, 140], "nondur": 73, "none": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 46, 48, 49, 51, 53, 54, 55, 56, 63, 64, 67, 69, 70, 71, 72, 73, 76, 77, 78, 79, 87, 89, 92, 93, 98, 99, 100, 101, 104, 106, 107, 108, 109, 112, 113, 120, 121, 124, 126, 127, 129, 140, 156, 169, 170], "nonignor": [30, 155], "nonlinear": [17, 19, 21, 64, 69, 72, 92, 97, 124, 140, 149, 150, 172], "nonlinearscoremixin": [0, 140], "nonparametr": [27, 28, 29, 97, 100, 140, 157, 158, 164, 165, 166, 167, 171], "nop": 65, "nor": [5, 6, 7, 8, 9, 63, 91, 98, 104, 109], "norm": 80, "normal": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 66, 67, 68, 69, 71, 72, 79, 80, 83, 87, 93, 94, 95, 96, 97, 98, 101, 103, 104, 109, 112, 117, 118, 124, 140, 141, 142, 143, 144, 156, 170], "normalize_ipw": [22, 23, 24, 25, 26, 27, 28, 29, 30, 66, 88, 93, 101], "not_yet_tr": [14, 16, 69, 72], "notat": [63, 66, 67, 91, 101, 124, 126, 127, 128, 129, 130, 138, 140, 142, 144], "note": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 52, 53, 54, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 139, 140, 168, 170], "notebook": [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 111, 112, 113, 120, 124, 172, 173], "notic": [60, 79, 98], "now": [62, 63, 64, 66, 70, 71, 75, 76, 77, 81, 82, 91, 92, 95, 98, 100, 101, 109, 170, 172], "np": [5, 6, 7, 8, 9, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 55, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 109, 111, 112, 113, 114, 116, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170, 173], "nrmse": 77, "nround": [61, 64, 173], "nrow": [62, 63, 65, 97, 104, 109, 112, 122, 123, 156, 170], "ntrue": 76, "nu": [18, 25, 45, 66, 101, 124, 133, 157, 158, 160, 162, 163, 166, 167], "nu2": [69, 75, 157, 163], "nu_0": [157, 167], "nu_i": [66, 101], "nuis_g0": 60, "nuis_g1": 60, "nuis_l": 120, "nuis_m": [60, 120], "nuis_r0": 60, "nuis_r1": 60, "nuis_rmse_ml_l": 84, "nuis_rmse_ml_m": 84, "nuisanc": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 44, 45, 46, 50, 61, 62, 63, 64, 65, 66, 67, 70, 75, 77, 80, 81, 82, 83, 84, 87, 89, 91, 92, 93, 96, 98, 99, 100, 101, 102, 103, 112, 114, 115, 116, 117, 118, 119, 121, 123, 124, 127, 128, 129, 139, 140, 141, 142, 143, 144, 145, 149, 151, 156, 157, 160, 162, 167, 168, 172, 173], "nuisance_el": [157, 159, 160, 161, 162, 164, 165, 166], "nuisance_loss": [75, 77, 112, 113, 172], "nuisance_spac": [37, 140, 151], "nuisance_target": 75, "null": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 62, 99, 112, 120, 157, 163, 172], "null_hypothesi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 99, 157, 163], "num": [64, 65, 102, 112, 120, 121, 123, 124, 139, 140, 156, 170], "num_leav": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 68, 83, 93, 96], "number": [12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 51, 52, 54, 61, 63, 68, 69, 70, 72, 75, 76, 80, 81, 82, 83, 84, 85, 86, 91, 93, 95, 96, 97, 100, 124, 128, 139, 156, 168, 170, 173], "numer": [17, 19, 21, 56, 60, 65, 88, 94, 112, 120, 121, 140, 157, 164, 165, 172], "numeric_onli": 84, "numpi": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 46, 51, 52, 55, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 106, 109, 111, 112, 114, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170], "nuniqu": [69, 72], "ny": 171, "nyt": [124, 127, 129, 130], "o": [68, 75, 76, 77, 78, 84, 85, 86, 90, 92, 94, 97, 168, 170], "ob": [62, 64, 68, 72, 97, 157, 160], "obei": 140, "obj": 92, "obj_dml_data": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 60, 61, 63, 70, 71, 79, 80, 83, 88, 90, 91, 96, 102, 103, 111, 112, 113, 114, 116, 120, 121, 123, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 172], "obj_dml_data_bonu": [104, 109], "obj_dml_data_bonus_df": [104, 109], "obj_dml_data_from_arrai": [5, 6, 7, 8, 9], "obj_dml_data_from_df": [5, 6, 8, 9], "obj_dml_data_sim": [104, 109], "obj_dml_data_sim_clust": [104, 109], "obj_dml_plr": [61, 80, 103], "obj_dml_plr_bonu": [65, 170], "obj_dml_plr_bonus_pip": 65, "obj_dml_plr_bonus_pipe2": 65, "obj_dml_plr_bonus_pipe3": 65, "obj_dml_plr_bonus_pipe_ensembl": 65, "obj_dml_plr_fullsampl": 90, "obj_dml_plr_lesstim": 90, "obj_dml_plr_nonorth": [61, 80], "obj_dml_plr_orth_nosplit": [61, 80], "obj_dml_plr_sim": [65, 170], "obj_dml_plr_sim_pip": 65, "obj_dml_plr_sim_pipe_ensembl": 65, "obj_dml_plr_sim_pipe_tun": 65, "obj_dml_sim": 30, "object": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 60, 64, 65, 66, 67, 69, 70, 71, 72, 73, 76, 78, 81, 82, 83, 87, 88, 89, 90, 92, 93, 96, 97, 101, 104, 106, 107, 108, 109, 111, 112, 113, 117, 119, 122, 123, 124, 125, 126, 127, 129, 139, 140, 156, 168, 170, 171, 172, 173], "obs_confound": [60, 79], "observ": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 51, 52, 57, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 87, 89, 90, 91, 92, 93, 96, 97, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 111, 112, 113, 123, 124, 125, 126, 127, 129, 130, 138, 139, 140, 141, 142, 143, 144, 156, 157, 158, 159, 160, 161, 162, 170, 171, 173], "obtain": [19, 25, 40, 60, 61, 62, 63, 66, 67, 69, 70, 71, 72, 75, 79, 80, 81, 82, 83, 84, 91, 96, 98, 100, 101, 102, 103, 111, 112, 123, 139, 140, 151, 156, 157, 158, 163, 169, 170], "obvious": [69, 72], "occur": [17, 19, 69, 72, 90, 172], "odd": 37, "off": [95, 171], "offer": [17, 19, 62, 64, 92, 93, 100, 173], "offici": 169, "offset": [112, 120], "often": 96, "oka": 171, "ol": [22, 26, 39, 51], "olma": [97, 124, 171, 172], "omega": [87, 111, 124, 125, 140, 145, 148, 157, 164, 165], "omega_": [43, 63, 91], "omega_1": [43, 63, 91], "omega_2": [43, 63, 91], "omega_epsilon": [63, 91], "omega_v": [43, 63, 91], "omega_x": [43, 63, 91], "omit": [69, 72, 99, 100, 124, 128, 140, 142, 144, 157, 158, 167, 171, 172, 173], "ommit": 100, "onc": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 90, 100, 124, 130, 173], "one": [13, 16, 38, 57, 60, 61, 62, 63, 64, 65, 69, 70, 71, 72, 75, 78, 79, 80, 81, 82, 89, 91, 93, 94, 97, 98, 99, 100, 103, 104, 109, 111, 112, 113, 122, 123, 124, 125, 128, 130, 136, 139, 140, 141, 142, 143, 144, 148, 151, 152, 153, 156, 157, 158, 163, 164, 165, 166, 170, 172], "ones": [65, 68, 83, 90, 96, 99, 111], "ones_lik": [77, 78, 96], "onli": [5, 6, 7, 8, 9, 12, 14, 15, 16, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 51, 53, 54, 55, 56, 62, 63, 64, 69, 72, 75, 76, 81, 82, 85, 86, 87, 89, 90, 91, 92, 93, 97, 102, 111, 112, 121, 122, 124, 127, 129, 138, 140, 142, 144, 146, 149, 150, 156, 157, 158, 160, 162, 164, 165, 167, 172], "onlin": 173, "onto": 75, "oo": 90, "oob_error": [65, 112, 120, 121], "oop": 172, "opac": [81, 82], "open": [65, 112, 122, 168, 170], "oper": [65, 172], "opposit": [95, 97, 124], "oprescu": [32, 81, 82, 85, 86, 171], "opt": [68, 69, 70, 71, 72, 91, 92, 97, 100], "optim": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 65, 81, 82, 90, 95, 111, 112, 122, 123, 171], "optimize_kwarg": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "option": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 53, 54, 55, 56, 60, 61, 63, 64, 66, 69, 70, 72, 75, 76, 78, 81, 82, 85, 86, 87, 91, 92, 93, 101, 104, 105, 107, 108, 109, 112, 113, 121, 123, 124, 139, 140, 146, 149, 150, 156, 172, 173], "optuna": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 112, 117, 172, 173], "optuna_set": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76, 112, 117, 173], "optuna_sett": 76, "optuna_settings_pipelin": 76, "oracl": [35, 47, 69, 72, 76, 77, 78], "oracle_valu": [31, 35, 40, 47, 76, 77, 78], "orang": 61, "orcal": [31, 40], "order": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 53, 62, 63, 64, 65, 70, 88, 91, 92, 97, 112, 115, 120, 121, 124, 139, 140], "org": [33, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 112, 122, 168, 169, 171, 172], "orient": [65, 112, 122, 140, 168, 170, 171, 172], "origin": [48, 49, 52, 53, 54, 62, 65, 69, 70, 71, 72, 95, 99, 100, 111, 140, 148], "orign": [64, 92], "orth_sign": [51, 52, 88], "orthogon": [51, 52, 63, 64, 69, 91, 92, 102, 110, 124, 156, 157, 167, 168, 171], "orthongon": [157, 167], "osx": 169, "other": [5, 6, 7, 8, 9, 37, 38, 39, 48, 49, 53, 54, 61, 63, 64, 65, 66, 67, 69, 70, 71, 72, 75, 76, 78, 80, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 104, 107, 109, 111, 112, 120, 121, 122, 124, 136, 137, 139, 140, 148, 156, 157, 167, 168, 169, 170, 171, 172, 173], "other_ind": 91, "otherwis": [12, 14, 15, 22, 23, 25, 26, 39, 48, 49, 53, 54, 64, 92, 93, 95, 124, 126, 140, 142, 144], "othrac": [65, 73, 104, 109, 170], "our": [61, 62, 64, 65, 67, 69, 72, 75, 76, 77, 80, 81, 82, 83, 90, 92, 93, 96, 97, 99, 100, 103, 124, 168, 170, 172, 173], "ourselv": 75, "out": [38, 39, 63, 65, 67, 68, 69, 70, 71, 72, 73, 75, 84, 90, 91, 93, 99, 100, 101, 102, 104, 109, 110, 111, 112, 113, 114, 116, 117, 123, 124, 125, 126, 127, 129, 140, 152, 153, 156, 157, 158, 163, 166, 168, 170, 172, 173], "outcom": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 35, 37, 38, 39, 40, 41, 47, 60, 62, 63, 64, 65, 68, 69, 70, 71, 72, 73, 76, 79, 84, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 104, 106, 107, 108, 109, 112, 113, 123, 126, 127, 129, 130, 131, 135, 136, 137, 138, 142, 144, 145, 156, 158, 163, 164, 166, 167, 170, 172, 173], "outcome_0": 79, "outcome_1": 79, "outer": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 112, 114, 116, 121], "outperform": 77, "output": [62, 69, 71, 72, 75, 98, 102, 112, 120, 124, 127, 129, 156, 173], "output_list": 98, "outshr": 91, "outsid": 61, "over": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 61, 69, 70, 72, 75, 78, 80, 84, 103, 110, 112, 117, 118, 123, 124, 125, 157, 163, 172], "overal": [13, 69, 70, 71, 72, 89, 95, 100, 124, 125], "overall_aggregation_weight": [13, 69, 70, 71, 72], "overcom": [110, 140], "overfit": [90, 110, 139], "overlap": [67, 89, 100, 124, 126, 130], "overrid": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76, 112, 121, 172], "overridden": 124, "overst": [64, 92, 93], "overview": [75, 156, 157, 163, 171], "overwrit": 172, "overwritten": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "ownership": [64, 92], "p": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 37, 38, 39, 40, 41, 47, 55, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 103, 111, 112, 113, 114, 116, 117, 118, 122, 123, 124, 125, 126, 127, 129, 130, 135, 139, 140, 141, 142, 143, 144, 145, 146, 149, 150, 151, 154, 155, 156, 157, 164, 165, 168, 169, 170, 172, 173], "p401": [64, 92, 93], "p_": [17, 19], "p_0": [140, 141, 142, 143], "p_1": 156, "p_adjust": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 139, 156, 168, 170], "p_dbl": [65, 112, 122, 123], "p_hat": 88, "p_i": 41, "p_int": [112, 123, 173], "p_n": 42, "p_val": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "p_x": [43, 63, 91], "p_x0": 94, "p_x1": 94, "packag": [60, 61, 63, 65, 66, 67, 68, 69, 70, 71, 72, 74, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 89, 90, 91, 93, 95, 96, 97, 98, 99, 100, 101, 102, 103, 111, 112, 115, 119, 120, 121, 122, 123, 124, 126, 130, 138, 139, 140, 156, 157, 158, 168, 170, 171, 172, 173], "packagedata": 91, "packagevers": 64, "pad": 89, "page": [76, 100, 168, 171], "pair": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 79], "pake": [63, 91], "paket": [63, 64, 65], "pal": 63, "palett": [13, 16, 69, 72, 76, 77, 78], "pand": [69, 72], "panda": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 41, 51, 52, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 104, 106, 109, 111, 124, 157, 158, 170], "pandas2ri": 91, "panel": [6, 7, 12, 14, 16, 17, 18, 19, 70, 72, 104, 105, 106, 109, 127, 128, 130, 161, 162, 171, 172], "paper": [33, 42, 65, 90, 94, 97, 99, 100, 157, 167, 168, 170, 171, 172], "par": 73, "par_grid": [65, 112, 118, 122, 123], "paradox": [65, 112, 123, 172, 173], "parallel": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 67, 68, 69, 72, 75, 78, 83, 96, 124, 126, 128, 130], "param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 53, 54, 76, 90, 112, 116, 117, 118, 121, 123], "param_grid": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 112, 123, 173], "param_nam": [62, 76], "param_set": [65, 112, 122, 123, 173], "param_spac": [76, 112, 117, 173], "param_space_pipelin": 76, "param_v": 65, "paramet": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 61, 62, 63, 64, 66, 67, 69, 70, 72, 75, 76, 77, 78, 80, 81, 82, 83, 84, 87, 88, 91, 92, 95, 96, 97, 98, 99, 100, 101, 102, 103, 109, 110, 111, 112, 113, 116, 117, 118, 120, 121, 122, 123, 126, 127, 128, 129, 130, 131, 132, 133, 134, 138, 139, 149, 150, 156, 157, 158, 163, 165, 167, 168, 170, 171, 172, 173], "parametr": [25, 62, 76, 100, 103, 112, 116, 121, 173], "params_exact": [112, 121], "params_nam": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 62, 76], "params_stacking__final_estimator__c": 76, "params_stacking__lgbm__lambda_l1": 76, "params_stacking__lgbm__lambda_l2": 76, "params_stacking__lgbm__learning_r": 76, "params_stacking__lgbm__min_child_sampl": 76, "parenttoc": 168, "part": [45, 61, 63, 64, 65, 71, 75, 80, 90, 91, 92, 103, 112, 123, 139, 157, 167, 172, 173], "parti": 45, "partial": [21, 37, 38, 39, 40, 41, 42, 43, 44, 45, 63, 65, 73, 84, 89, 90, 91, 99, 102, 110, 112, 113, 116, 117, 118, 121, 123, 135, 136, 137, 139, 152, 153, 156, 163, 164, 165, 166, 167, 168, 170, 171, 172, 173], "partial_": [140, 156], "partiallli": 99, "particip": [10, 93, 99, 173], "particular": [124, 168], "particularli": [77, 90], "partion": [63, 91], "partit": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 63, 91, 102, 110], "partli": 173, "pass": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 50, 51, 53, 54, 62, 65, 70, 76, 90, 112, 114, 119, 121, 123, 173], "passo": [168, 170], "past": 63, "paste0": [63, 66], "pastel": 80, "path": [112, 118, 123, 124], "path_to_r": 84, "patsi": [81, 82, 111], "pattern": 100, "paul": 171, "pd": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 51, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 111, 124], "pdf": [80, 94], "pedregosa": [168, 170], "pedregosa11a": [168, 170], "pedro": [62, 171], "penal": [66, 70, 101], "penalti": [64, 65, 70, 79, 92, 98, 100, 101, 112, 123, 124], "pennsylvania": [11, 104, 109, 170], "pension": [64, 92, 93, 173], "peopl": [64, 92, 93], "pep8": 172, "per": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 63, 69, 70, 71, 72, 76, 91], "percent": [112, 123], "percentag": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40], "perf_count": 75, "perfectli": [89, 97, 124], "perform": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 52, 61, 63, 65, 67, 69, 70, 71, 72, 75, 76, 80, 84, 87, 90, 91, 93, 99, 100, 101, 103, 112, 113, 117, 118, 120, 122, 123, 124, 126, 127, 129, 139, 140, 156, 168, 170, 171, 173], "performance_result": 77, "perfrom": 87, "perhap": 173, "period": [12, 14, 16, 17, 19, 62, 67, 68, 71, 106, 109, 125, 126, 127, 128, 129, 130, 142, 144, 171, 172], "perp": [124, 138], "perrot": [168, 170], "person": 173, "pessimist": 100, "peter": 171, "petra": 172, "petronelaj": 172, "pfister": [65, 112, 122, 168, 170], "phi": [63, 91, 111, 156], "philipp": [100, 168, 171], "philippbach": [168, 172], "pi": [30, 34, 42, 45, 111, 124, 140, 154, 155], "pi_": [43, 63, 91], "pi_0": [140, 154, 155], "pi_i": [66, 101, 124], "pick": [97, 173], "pip": [97, 124], "pip3": 169, "pipe": 65, "pipe_forest_classif": 65, "pipe_forest_regr": 65, "pipe_lasso": 65, "pipelin": [48, 49, 53, 54, 65, 92, 119, 120, 122, 172], "pipelineinot": 76, "pipeop": 65, "pira": [64, 92, 93, 99, 173], "pivot": [77, 84, 91, 171], "pivot_logloss": 77, "pivot_rmse_g0": 77, "pivot_rmse_g1": 77, "place": 172, "plai": [90, 173], "plan": [10, 64, 92, 93, 173], "plausibl": [100, 157], "pleas": [48, 49, 53, 54, 62, 67, 68, 70, 76, 78, 90, 100, 112, 139, 168, 169, 173], "plim": 94, "pliv": [20, 21, 38, 63, 91, 102, 111, 136, 152, 168, 172], "plm": [0, 6, 7, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 80, 89, 90, 91, 99, 102, 103, 110, 111, 112, 113, 116, 117, 139, 156, 163, 172, 173], "plot": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 52, 61, 62, 64, 65, 66, 68, 69, 70, 71, 72, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 96, 97, 99, 100, 101, 111, 157, 163], "plot_data": [69, 72], "plot_effect": [13, 16, 69, 70, 71, 72], "plot_optimization_histori": 76, "plot_parallel_coordin": 76, "plot_param_import": 76, "plot_tre": [52, 95, 111], "plotli": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 76, 81, 82, 84, 97, 100], "plr": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 65, 90, 94, 99, 102, 112, 113, 117, 137, 139, 153, 156, 163, 165, 166, 167, 168, 170, 172, 173], "plr_est": 94, "plr_est1": 94, "plr_est2": 94, "plr_obj": 94, "plr_obj_1": 94, "plr_obj_2": 94, "plr_summari": 92, "plrglmnet": 64, "plrranger": 64, "plrrpart": 64, "plrxgboost8700": 64, "plt": [67, 68, 69, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 96, 97, 100, 101], "plt_smpl": [63, 91], "plt_smpls_cluster": [63, 91], "plug": [87, 157, 159, 160, 161, 162, 163, 164, 165], "pm": [46, 63, 91, 156, 157, 163, 167], "pmatrix": [66, 101], "pmlr": [75, 84, 90], "po": [65, 112, 120, 122], "poe": 171, "point": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 62, 63, 77, 85, 86, 91, 100, 111, 124, 173], "pointwis": [51, 83, 85, 86, 96], "poli": [64, 88, 91, 92], "polici": [26, 37, 38, 39, 52, 89, 110, 124, 135, 136, 137, 170, 171, 172], "policy_tre": [26, 95, 111], "policy_tree_2": 95, "policy_tree_obj": 111, "policytre": 95, "polit": 94, "poly_dict": 92, "polynomi": [10, 11, 47, 64, 73, 88, 92, 97], "polynomial_featur": [10, 11, 64, 73], "polynomialfeatur": [88, 91, 92], "poor": 98, "pop": [140, 142], "popul": [100, 124, 127, 129, 140, 144], "popular": [75, 124, 157, 158], "porport": 99, "posit": [45, 64, 69, 71, 72, 75, 77, 94, 100, 173], "posixct": [65, 112, 120], "possibl": [5, 6, 7, 8, 9, 49, 53, 54, 62, 65, 69, 71, 72, 75, 76, 81, 82, 85, 86, 87, 88, 90, 95, 97, 98, 99, 100, 112, 113, 114, 117, 119, 122, 124, 128, 131, 132, 156, 157, 158, 172, 173], "possibli": [98, 157, 158], "post": [16, 42, 45, 124, 126, 128, 130, 156, 171], "postdoubl": 171, "poster": 94, "potenti": [17, 19, 22, 23, 24, 27, 28, 30, 31, 35, 37, 47, 66, 67, 69, 72, 76, 88, 94, 97, 101, 126, 130, 131, 138, 145, 146, 156, 164, 169, 172, 173], "potential_level": [76, 77, 78], "power": [65, 90, 98, 100, 112, 123, 171], "pp": [62, 75, 84, 90], "pq": [27, 28, 29, 93, 150, 172], "pq_0": [93, 96], "pq_1": [93, 96], "pr": [30, 60, 63, 64, 65, 66, 112, 120, 121, 122, 123, 124, 139, 140, 156, 170, 173], "practic": [75, 100, 171], "pre": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 66, 67, 69, 70, 71, 72, 101, 112, 116, 121, 124, 126, 127, 128, 129, 140, 142, 144, 172], "precis": [62, 124, 157, 165, 173], "precomput": [49, 54], "pred": [62, 90], "pred_df": 95, "pred_dict": [112, 114], "pred_treat": 95, "predict": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 51, 52, 53, 54, 55, 61, 63, 64, 65, 70, 75, 76, 77, 80, 83, 84, 88, 90, 91, 92, 95, 98, 100, 103, 111, 114, 115, 120, 121, 123, 139, 157, 158, 163, 165, 172, 173], "predict_proba": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 39, 46, 48, 53, 90, 98, 112, 115], "predictor": [22, 26, 39, 51, 52, 81, 82, 85, 86, 100, 102], "prefer": [64, 92, 93, 173], "preliminari": [24, 61, 80, 97, 140, 146, 149, 150, 151, 155], "prepar": [62, 63, 76, 91, 172], "preprint": [98, 171], "preprocess": [64, 70, 76, 88, 91, 92, 93, 112, 120, 122], "presenc": [64, 92, 93], "present": [23, 62, 70, 100, 112, 122, 140, 148, 173], "prespecifi": 99, "pretreat": [12, 14, 15, 16, 67], "prettenhof": [168, 170], "preval": 100, "prevent": [139, 172], "previou": [68, 87, 88, 94, 169, 173], "previous": [70, 98, 112, 121, 122, 173], "price": [63, 91], "priliminari": [27, 29], "primari": [77, 78], "principl": [76, 157, 158], "print": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 55, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 169, 170, 172, 173], "print_detail": 62, "print_period": [14, 16], "prior": [75, 77, 124, 138], "privat": 172, "prob": 65, "prob_dist": 98, "prob_dist_": 98, "probabilit": [87, 89], "probability_from_treat": 89, "probabl": [17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 53, 61, 62, 66, 67, 69, 72, 78, 80, 87, 89, 94, 96, 97, 98, 100, 101, 103, 124, 140, 141, 142, 143, 144, 149, 171], "problem": [64, 70, 92, 93, 111, 112, 123], "proce": 76, "procedur": [61, 63, 64, 75, 76, 80, 91, 92, 99, 100, 112, 114, 156, 169, 172], "proceed": [42, 171], "process": [14, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 47, 56, 62, 66, 67, 68, 71, 72, 75, 81, 82, 83, 84, 85, 86, 89, 90, 95, 96, 100, 101, 110, 156, 157, 158, 171, 172], "processor": [55, 56], "produc": 94, "product": [75, 81, 82, 84, 100, 157, 167], "producton": 63, "program": [34, 64, 92, 93, 171, 173], "progress": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 74], "project": [65, 81, 82, 111, 168, 172], "project_z": [81, 82], "prone": 140, "pronounc": 97, "propens": [14, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 40, 41, 55, 56, 64, 66, 67, 69, 70, 72, 75, 76, 77, 87, 88, 92, 93, 100, 101, 111, 124, 127, 129, 131, 140, 142, 144, 157, 164, 172], "propensity_scor": 55, "proper": 77, "properli": [77, 90, 173], "properti": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 64, 65, 69, 70, 71, 72, 75, 76, 92, 93, 94, 98, 99, 104, 105, 107, 108, 109, 112, 120, 121, 124, 157, 163, 170, 172], "proport": [99, 157, 158, 166, 167], "propos": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 63, 65, 91, 97, 157, 158, 171, 172], "provid": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 53, 54, 55, 56, 62, 63, 64, 65, 70, 77, 81, 82, 85, 86, 88, 90, 91, 92, 97, 98, 100, 102, 103, 104, 106, 109, 110, 112, 114, 116, 119, 120, 121, 122, 123, 156, 168, 170, 172, 173], "prune": [26, 52], "ps911c": 91, "ps944": 91, "ps_processor_config": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 81], "pscore1": 94, "pscore2": 94, "psi": [20, 21, 61, 62, 63, 91, 102, 124, 127, 128, 129, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 167, 170], "psi_": [156, 157, 160, 162, 163, 166, 167], "psi_a": [20, 25, 26, 38, 39, 61, 63, 80, 91, 124, 127, 128, 129, 139, 140, 141, 142, 143, 144, 145, 147, 148, 152, 153, 156], "psi_b": [20, 25, 26, 38, 39, 61, 80, 111, 124, 127, 128, 129, 139, 140, 141, 142, 143, 144, 145, 147, 148, 152, 153], "psi_el": [89, 139, 140], "psi_j": 156, "psi_nu2": [157, 163], "psi_sigma2": [157, 163], "psprocessor": [56, 172], "psprocessorconfig": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 55], "public": [60, 79, 172], "publish": [100, 168, 172], "pull": [64, 172], "purchas": 100, "pure": 100, "purp": [81, 82], "purpos": [61, 80, 87, 99, 100, 140, 142, 144, 157, 158, 170, 173], "pval": 156, "px": [84, 97], "py": [68, 69, 70, 71, 72, 81, 91, 92, 97, 98, 100, 168, 169, 172], "py3": 169, "py_al": 80, "py_did": 67, "py_did_pretest": 68, "py_dml": 80, "py_dml_nosplit": 80, "py_dml_po": 80, "py_dml_po_nosplit": 80, "py_double_ml_apo": 78, "py_double_ml_bas": 80, "py_double_ml_basic_iv": 79, "py_double_ml_c": 81, "py_double_ml_cate_plr": 82, "py_double_ml_cvar": 83, "py_double_ml_firststag": 84, "py_double_ml_g": 85, "py_double_ml_gate_plr": 86, "py_double_ml_gate_sensit": 87, "py_double_ml_irm_vs_apo": 88, "py_double_ml_lplr": 89, "py_double_ml_meets_flaml": 90, "py_double_ml_multiway_clust": 91, "py_double_ml_pens": 92, "py_double_ml_pension_qt": 93, "py_double_ml_plm_irm_hetfx": 94, "py_double_ml_policy_tre": 95, "py_double_ml_pq": 96, "py_double_ml_rdflex": 97, "py_double_ml_robust_iv": 98, "py_double_ml_sensit": 99, "py_double_ml_sensitivity_book": 100, "py_double_ml_ssm": 101, "py_learn": 75, "py_non_orthogon": 80, "py_optuna": 76, "py_panel": 69, "py_panel_data_exampl": 70, "py_panel_simpl": 71, "py_po_al": 80, "py_rep_c": 72, "py_tabpfn": 77, "pypi": [171, 172], "pyplot": [67, 68, 69, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 96, 97, 100, 101], "pyproject": 172, "pyreadr": 70, "python": [45, 62, 90, 97, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 116, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 157, 158, 163, 168, 170, 171, 172, 173], "python3": [68, 69, 70, 71, 72, 91, 92, 97, 100, 169], "pytorch": 77, "q": [65, 83, 96, 97, 112, 122, 168, 170], "q2": [65, 73, 104, 109, 170], "q3": [65, 73, 104, 109, 170], "q4": [65, 73, 104, 109, 170], "q5": [65, 73, 104, 109, 170], "q6": [65, 73, 104, 109, 170], "q_i": [97, 124], "qquad": 34, "qte": [83, 93, 172], "quad": [17, 18, 19, 41, 64, 66, 67, 92, 95, 97, 101, 111, 124, 126, 130, 138, 140, 149, 156, 157, 159, 160], "quadrat": [66, 101], "qualiti": [99, 102, 172], "quanitl": 93, "quant": 83, "quantifi": [100, 124, 130], "quantil": [16, 23, 24, 27, 28, 29, 35, 69, 72, 76, 78, 83, 88, 99, 110, 112, 120, 121, 146, 149, 150, 171, 172], "quantiti": [60, 79, 100], "queri": 92, "question": [100, 173], "quick": 93, "quit": [75, 76, 77, 95, 99, 157, 158], "r": [25, 33, 48, 49, 53, 54, 68, 69, 70, 71, 72, 80, 81, 82, 84, 91, 94, 97, 98, 100, 102, 103, 104, 109, 110, 120, 121, 122, 123, 124, 133, 139, 140, 147, 151, 152, 156, 157, 158, 164, 165, 166, 167, 168, 170, 171, 172, 173], "r2_d": [34, 75], "r2_score": [49, 54], "r2_y": [34, 75], "r6": [65, 172], "r_0": [25, 37, 38, 41, 64, 89, 92, 124, 133, 135, 140, 151], "r_all": 61, "r_d": 34, "r_df": 91, "r_dml": 61, "r_dml_nosplit": 61, "r_dml_po": 61, "r_dml_po_nosplit": 61, "r_double_ml_bas": 61, "r_double_ml_basic_iv": 60, "r_double_ml_did": 62, "r_double_ml_multiway_clust": 63, "r_double_ml_pens": 64, "r_double_ml_pipelin": 65, "r_double_ml_ssm": 66, "r_hat": [38, 89], "r_hat0": 25, "r_hat1": 25, "r_non_orthogon": 61, "r_po_al": 61, "r_y": 34, "rais": [5, 6, 7, 8, 9, 37, 48, 49, 53, 54, 69, 70, 71, 72, 112, 114], "randint": 94, "randn": 30, "random": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 36, 37, 38, 39, 45, 46, 47, 60, 61, 62, 64, 65, 67, 68, 69, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 106, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 123, 125, 126, 127, 129, 130, 139, 154, 156, 157, 163, 167, 170, 171, 173], "random_search": [112, 123], "random_st": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 69, 72, 76, 77, 80, 87, 88, 95], "randomforest": [64, 75, 77, 92], "randomforest_class": [64, 81, 89, 92, 95], "randomforest_reg": [81, 89, 95], "randomforestclassifi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 73, 75, 77, 78, 81, 82, 85, 86, 87, 89, 92, 95, 97, 99, 100, 111, 112, 114, 115, 124, 125, 126, 127, 129, 173], "randomforestregressor": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 73, 75, 77, 78, 80, 81, 82, 85, 86, 87, 89, 92, 95, 97, 99, 100, 102, 111, 112, 113, 114, 115, 116, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170, 173], "randomized_search": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 112, 118], "randomizedsearchcv": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 112, 118], "randomli": [61, 63, 80, 91, 103, 139, 173], "rang": [8, 17, 19, 61, 67, 68, 75, 76, 77, 78, 80, 83, 85, 86, 89, 90, 91, 93, 95, 96, 97, 98, 100, 101, 103, 112, 123, 124], "rangeindex": [69, 70, 71, 72, 73, 76, 77, 78, 87, 89, 91, 92, 93, 99, 104, 106, 109, 170], "ranger": [62, 64, 65, 102, 112, 120, 121, 123, 124, 139, 140, 156, 170, 173], "rangl": [32, 95], "rank": 172, "rate": [75, 84, 124], "rather": [97, 100, 124], "ratio": [112, 123, 139, 157, 158], "rational": 70, "ravel": [81, 82], "raw": [55, 64, 70, 71, 84, 92], "raw_res_manual_lasso_r_100_n_100_p_200_rho_0": 84, "rbind": 64, "rbindlist": 64, "rbinom": 60, "rbrace": [25, 26, 33, 34, 36, 63, 91, 102, 124, 131, 133, 134, 139, 140, 145, 156, 157, 164], "rcolorbrew": 63, "rcparam": [68, 73, 81, 82, 83, 85, 86, 88, 91, 92, 93, 96], "rd": [124, 172], "rda": 70, "rdbu": 63, "rdbu_r": 91, "rdbwselect": 124, "rdd": [0, 8, 107, 109, 110, 169], "rdflex": [97, 124, 172], "rdflex_fuzzi": 97, "rdflex_fuzzy_stack": 97, "rdflex_obj": [46, 124], "rdflex_sharp": 97, "rdflex_sharp_stack": 97, "rdrobust": [46, 97, 124, 169, 172], "rdrobust_fuzzi": 97, "rdrobust_fuzzy_noadj": 97, "rdrobust_sharp": 97, "rdrobust_sharp_noadj": 97, "rdt044": 84, "re": [69, 91, 100, 169], "read": [70, 169], "read_csv": [71, 84], "read_r": 70, "readabl": [77, 172], "reader": [70, 98], "readili": 168, "real": [64, 92, 93, 99, 157, 158], "realat": 124, "realiz": [97, 124, 138], "reason": [5, 6, 7, 8, 9, 60, 75, 77, 79, 84, 90, 99, 100, 157, 158, 173], "recal": [73, 157, 167], "receiv": [17, 19, 69, 72, 78, 97, 124, 126, 128], "recent": [90, 124, 126, 130, 171], "recogn": [64, 92, 93], "recommend": [65, 69, 72, 75, 76, 77, 97, 100, 102, 124, 139, 157, 169, 171, 172], "recov": [60, 62, 79, 94], "recsi": 171, "red": [63, 66, 76, 77, 85, 86, 90, 91], "reduc": [64, 76, 87, 90, 92, 97, 99, 100, 124, 172], "redund": 172, "reemploy": [11, 104, 109, 170], "ref": 70, "refactor": 172, "refer": [10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 64, 68, 69, 72, 77, 78, 87, 89, 92, 93, 97, 99, 104, 109, 110, 111, 120, 124, 125, 127, 129, 130, 140, 157, 158, 163, 171, 172, 173], "reference_level": [23, 76, 77, 78, 88, 124], "refin": 172, "refit": [157, 158], "reflect": [95, 100, 111], "reg": [17, 18, 19, 64, 92, 173], "reg_estim": 97, "reg_learn": 93, "reg_learner_1": 75, "reg_learner_2": 75, "regard": [100, 168], "regener": 172, "region": [63, 83, 91, 156, 171], "regr": [60, 61, 62, 63, 64, 65, 66, 102, 112, 120, 121, 122, 123, 124, 139, 140, 142, 156, 170, 173], "regravg": [65, 112, 120], "regress": [8, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 51, 56, 60, 62, 63, 65, 66, 67, 69, 70, 71, 72, 76, 78, 79, 84, 89, 90, 91, 94, 98, 99, 100, 101, 102, 103, 107, 109, 110, 111, 112, 113, 116, 117, 118, 119, 121, 123, 126, 127, 129, 133, 134, 135, 136, 137, 139, 144, 156, 158, 163, 164, 165, 166, 167, 168, 170, 171, 172, 173], "regressor": [37, 49, 54, 61, 64, 75, 77, 78, 80, 83, 89, 90, 92, 98, 103], "regular": [42, 110, 112, 118, 140, 156, 171], "reich": [65, 112, 122], "reinforc": 171, "reject": [64, 92], "rel": [64, 70, 76, 77, 92, 98, 124, 125, 157, 158, 164, 165], "relat": [88, 100, 173], "relationship": [60, 77, 79, 84, 100, 156], "relev": [12, 14, 15, 16, 32, 48, 49, 51, 53, 54, 69, 72, 83, 95, 96, 104, 109, 124, 157, 173], "reli": [16, 67, 68, 69, 72, 81, 82, 87, 88, 111, 112, 114, 115, 124, 126, 140, 144, 157, 158, 173], "reload": 64, "remain": [62, 104, 109, 156, 173], "remark": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 61, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 93, 99, 111, 112, 114, 124, 125, 126, 127, 129, 139, 140, 141, 142, 143, 144, 149, 150, 156, 157, 160, 162, 165], "remot": 169, "remov": [4, 14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 64, 68, 81, 88, 100, 104, 109, 110, 112, 124, 139, 140, 157, 172], "renam": [69, 72, 92, 172], "render": [76, 99, 100], "renorm": [17, 19], "reorgan": 172, "rep": [61, 66, 103, 112, 121, 156], "repeat": [12, 14, 15, 16, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 61, 63, 64, 65, 66, 69, 70, 71, 76, 80, 87, 91, 92, 93, 94, 97, 99, 101, 103, 105, 109, 110, 112, 113, 122, 127, 128, 129, 130, 156, 159, 160, 170, 172, 173], "repeatedkfold": 91, "repet": 99, "repetit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 51, 70, 75, 81, 82, 84, 85, 86, 87, 110, 112, 113, 156, 170, 172, 173], "replac": [95, 100, 172], "replic": [10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 64, 70, 71, 80, 84, 98, 100], "repo": 172, "report": [64, 90, 92, 168, 172], "repositori": [69, 72, 84, 97, 172], "repr": [61, 63], "repres": [17, 19, 77, 94, 100, 124], "represent": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 69, 76, 99, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 170, 172], "reproduc": 35, "request": [48, 49, 53, 54, 172], "requir": [38, 39, 48, 53, 56, 60, 64, 65, 69, 70, 71, 72, 76, 77, 78, 87, 92, 93, 99, 104, 109, 114, 115, 119, 121, 124, 125, 127, 128, 129, 140, 142, 144, 156, 157, 158, 163, 169, 172, 173], "requirenamespac": 62, "rerun": [70, 76], "res_df": 91, "res_dict": [31, 32, 35, 40, 47], "resampl": [37, 60, 63, 65, 66, 67, 69, 70, 71, 72, 91, 93, 99, 101, 112, 113, 123, 124, 126, 127, 129, 139, 140, 156, 168, 170, 173], "resdat": 72, "research": [63, 65, 91, 94, 100, 139, 168, 170, 171, 173], "resembl": [66, 101], "reset": 62, "reset_index": [69, 72, 84, 91, 92], "reshap": [68, 80, 81, 82, 88, 104, 109], "reshape2": 63, "residu": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 49, 54, 99, 157, 158, 166, 167], "resolut": [65, 112, 122, 123, 173], "resourc": 75, "resourcewis": 75, "respect": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 64, 69, 70, 71, 72, 76, 78, 92, 93, 97, 111, 124, 128, 133, 138, 139, 157, 167, 173], "respons": [10, 65, 76, 112, 120, 121], "rest": 124, "restart": 169, "restrict": 75, "restructur": 172, "restud": 84, "result": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 60, 61, 62, 65, 66, 67, 68, 69, 70, 71, 72, 75, 77, 78, 80, 81, 82, 84, 87, 88, 95, 97, 98, 99, 100, 101, 103, 112, 123, 139, 140, 141, 142, 143, 144, 157, 158, 163, 170, 172], "result_iivm": 64, "result_irm": 64, "result_plr": 64, "result_typ": 16, "results_df": 98, "retain": [48, 49, 53, 54], "retina": 94, "retir": [64, 92, 93, 99], "return": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 52, 53, 54, 55, 57, 61, 62, 63, 65, 66, 68, 69, 70, 71, 72, 75, 76, 80, 83, 90, 91, 94, 95, 96, 98, 99, 100, 101, 102, 112, 113, 117, 140, 157, 158, 172, 173], "return_count": [75, 76, 77, 78], "return_tune_r": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76], "return_typ": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 61, 64, 65, 66, 67, 75, 80, 88, 90, 92, 93, 99, 101, 102, 103, 104, 105, 107, 108, 109, 111, 112, 113, 114, 116, 120, 121, 124, 126, 139, 140, 156, 157, 163, 170, 173], "rev": 63, "reveal": 87, "review": [42, 84, 171], "revist": [63, 91], "reweight": [124, 125], "rf": 97, "rf_argument": 77, "rho": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 57, 69, 72, 78, 87, 88, 97, 99, 100, 157, 158, 163, 167, 173], "rho_val": 100, "richter": [65, 112, 122, 168, 170], "ridg": 76, "ridgeridg": 76, "riesz": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 69, 99, 157, 158, 159, 160, 161, 162, 163, 166, 167], "riesz_rep": [157, 163], "right": [17, 19, 33, 34, 36, 42, 43, 61, 63, 75, 76, 80, 91, 92, 93, 94, 96, 97, 100, 103, 124, 140, 141, 142, 143, 144, 156, 157, 159, 160, 161, 162, 164, 165], "rightarrow_": [61, 80, 103], "risk": [24, 110, 172], "ritov": 171, "rival": 91, "rival_ind": 91, "rmd": 62, "rmse": [62, 67, 69, 70, 71, 72, 75, 77, 90, 99, 101, 112, 113, 124, 126, 127, 129, 140, 156, 170, 172], "rmse_dml_ml_l_fullsampl": 90, "rmse_dml_ml_l_lesstim": 90, "rmse_dml_ml_l_onfold": 90, "rmse_dml_ml_l_untun": 90, "rmse_dml_ml_m_fullsampl": 90, "rmse_dml_ml_m_lesstim": 90, "rmse_dml_ml_m_onfold": 90, "rmse_dml_ml_m_untun": 90, "rmse_g0": 77, "rmse_g1": 77, "rmse_oos_ml_l": 90, "rmse_oos_ml_m": 90, "rmse_oos_onfolds_ml_l": 90, "rmse_oos_onfolds_ml_m": 90, "rnorm": [60, 65, 104, 109, 112, 122, 123, 156, 170], "robin": [10, 11, 44, 63, 84, 91, 103, 168, 171], "robinson": [61, 80, 103], "robject": 91, "robu": [85, 86], "robust": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 43, 62, 69, 70, 72, 78, 87, 88, 97, 99, 100, 104, 109, 124, 157, 163, 171, 172, 173], "robust_confset": [25, 98, 172], "robust_cov": 98, "robust_length": 98, "robustscal": 76, "robustscalerrobustscal": 76, "roc\u00edo": 171, "role": [4, 5, 6, 7, 8, 9, 61, 80, 90, 103, 107, 109, 173], "romano": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 156], "root": [12, 14, 15, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 77, 84, 103, 112, 113, 140, 171], "rotat": [76, 90, 97], "roth": [97, 124, 126, 130, 171], "rough": [100, 173], "roughli": [69, 72, 100], "round": [55, 64, 70, 75, 76, 77, 78, 88, 94, 100], "rout": [48, 49, 53, 54], "row": [13, 16, 61, 64, 68, 70, 73, 76, 81, 82, 90, 91, 95, 104, 109, 124, 127, 129, 139, 170, 173], "rownam": 63, "rowv": 63, "roxygen2": 172, "royal": [100, 171], "rpart": [64, 65, 112, 120], "rpart_cv": 65, "rprocess": 75, "rpy2": 91, "rpy2pi": 91, "rskf": 88, "rsmp": [65, 112, 122, 123, 139], "rsmp_tune": [65, 112, 122, 123], "rssb": 100, "rtype": 23, "ruben": 171, "ruiz": [60, 79], "rule": [62, 111], "run": [8, 62, 70, 77, 97, 107, 109, 124, 169, 172], "runif": 60, "runner": [68, 69, 71, 72, 98, 100], "runtime_learn": 65, "runtimewarn": 72, "rv": [16, 69, 72, 78, 87, 88, 99, 100, 157, 163, 173], "rva": [69, 72, 78, 87, 88, 99, 100, 157, 163, 173], "rvert": 84, "rvert_": 84, "s1": 90, "s2": 90, "s_": [43, 63, 91, 124, 138], "s_1": 44, "s_2": 44, "s_col": [4, 9, 66, 97, 101, 108, 109, 124], "s_i": [36, 66, 97, 101, 124], "s_x": [43, 63, 91], "safeguard": [67, 112, 114], "sake": [64, 92, 100, 173], "same": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 51, 61, 63, 66, 69, 70, 72, 75, 76, 80, 81, 82, 87, 88, 91, 93, 95, 97, 98, 99, 100, 101, 112, 116, 121, 124, 127, 129, 140, 143, 144, 156, 157, 165, 172], "samii": 94, "sampl": [9, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 36, 37, 38, 39, 40, 43, 46, 48, 49, 53, 54, 60, 62, 63, 65, 67, 69, 70, 71, 72, 75, 79, 85, 86, 88, 91, 93, 95, 98, 99, 108, 109, 110, 112, 113, 123, 126, 127, 129, 130, 138, 141, 156, 157, 160, 162, 170, 171, 172], "sample_weight": [46, 48, 49, 53, 54, 97], "sampler": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76], "sant": [12, 14, 15, 17, 18, 19, 31, 35, 40, 62, 67, 69, 70, 71, 72, 124, 125, 126, 128, 130, 171], "sara": 171, "sasaki": [43, 63, 91, 171], "satisfi": [66, 70, 101, 112, 115, 119, 140, 156], "save": [61, 64, 70, 75, 76, 80, 85, 86, 90, 92, 93, 112, 114, 157, 163, 173], "savefig": 80, "saveguard": 75, "saver": [64, 92, 93], "sc": [69, 72], "scalar": 124, "scale": [17, 19, 61, 63, 68, 83, 88, 94, 96, 100, 156, 157, 160, 162, 167], "scale_color_manu": 61, "scale_fill_manu": [61, 63], "scaled_psi": 88, "scaler": 76, "scatter": [68, 77, 78, 85, 86, 89, 94, 97, 100], "scatterplot": [77, 78], "scenario": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 69, 72, 78, 87, 88, 99, 100, 124, 157, 163, 173], "scene": [81, 82, 84], "scene_camera": 84, "schacht": [75, 84, 90], "schaefer": 94, "schedul": [104, 109, 172], "scheme": [63, 91, 112, 123, 124, 125, 139, 168], "schneider": 65, "schratz": [65, 112, 122, 168, 170], "scienc": [45, 60, 79, 94, 171], "scikit": [70, 75, 92, 112, 115, 168, 170, 172, 173], "scipi": [80, 89], "score": [0, 8, 12, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 41, 46, 47, 48, 49, 50, 53, 54, 55, 56, 60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 81, 82, 83, 84, 87, 88, 90, 91, 92, 93, 96, 97, 98, 99, 100, 101, 102, 107, 109, 110, 111, 112, 113, 124, 125, 126, 127, 128, 129, 130, 131, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 165, 166, 167, 168, 172, 173], "score_col": [8, 97, 107, 109, 124], "scorer": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "scoring_method": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50], "script": 169, "sd": 60, "se": [61, 63, 76, 80, 99, 103, 112, 120, 121, 139, 156, 157, 163, 171, 173], "se_aggr": [112, 120], "se_df": 63, "se_dml": [61, 80, 103], "se_dml_po": [61, 80, 103], "se_nonorth": [61, 80], "se_orth_nosplit": [61, 80], "se_orth_po_nosplit": [61, 80], "seaborn": [13, 16, 67, 69, 72, 73, 75, 76, 77, 78, 80, 91, 92, 93, 100, 101], "seamlessli": 77, "search": [12, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 76, 117, 118, 123, 140, 173], "search_mod": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 112, 118], "searchabl": 64, "second": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 43, 61, 63, 65, 75, 76, 77, 80, 90, 91, 102, 103, 139, 156, 157, 158, 167, 170], "secondari": [77, 78], "section": [15, 16, 18, 19, 62, 63, 64, 65, 68, 76, 87, 90, 91, 93, 100, 105, 109, 125, 127, 128, 129, 130, 148, 159, 160, 172, 173], "secur": 94, "see": [10, 11, 12, 14, 15, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 34, 36, 37, 38, 39, 48, 49, 51, 53, 54, 60, 62, 63, 64, 65, 67, 69, 70, 71, 72, 75, 76, 77, 78, 79, 81, 82, 88, 89, 90, 91, 93, 94, 95, 97, 98, 99, 100, 112, 115, 123, 124, 126, 128, 130, 139, 140, 146, 148, 149, 150, 154, 155, 157, 158, 160, 162, 163, 167, 169, 170, 172], "seed": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 46, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 91, 92, 93, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170, 173], "seek": 94, "seem": [62, 64, 87, 92, 93, 157, 173], "seen": [85, 86, 88], "sel_cols_chiang": 91, "select": [9, 17, 19, 30, 35, 36, 42, 60, 75, 84, 97, 100, 102, 104, 108, 109, 110, 112, 120, 138, 156, 170, 171, 172, 173], "selected_coef": 75, "selected_featur": [65, 112, 120, 121], "selected_learn": 75, "self": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 51, 52, 53, 54, 75, 90, 98, 173], "selfref": 64, "semenova": [81, 82, 171], "semi": 103, "semiparametr": 10, "sens": [99, 100], "sensemakr": [157, 158], "sensit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 57, 110, 111, 124, 129, 158, 163, 167, 172], "sensitivity_analysi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 69, 72, 78, 87, 88, 99, 100, 157, 163, 173], "sensitivity_benchmark": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 78, 87, 99, 100, 157, 158], "sensitivity_el": [157, 163], "sensitivity_param": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 99, 100, 157, 158, 163], "sensitivity_plot": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 78, 87, 99, 100, 157, 163], "sensitivity_summari": [69, 72, 78, 87, 88, 99, 100, 157, 163, 173], "sensitv": 88, "sensitvity_benchmark": 78, "sensiv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39], "senstiv": [157, 166], "sep": 61, "separ": [13, 76, 77, 94, 99, 107, 108, 109, 112, 121, 124, 139, 172], "seper": [90, 97, 99, 156, 157, 158], "seq_len": [61, 66, 103], "sequenc": 91, "sequenti": [11, 112, 120], "ser": [69, 70, 71, 72], "seri": [69, 70, 71, 72, 100, 171], "serv": [17, 19, 104, 106, 109, 170, 172], "serverless": [171, 172], "servic": 94, "set": [4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 43, 44, 45, 48, 50, 52, 53, 54, 56, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 85, 86, 87, 88, 90, 91, 92, 93, 94, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 109, 111, 116, 118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 130, 139, 140, 141, 142, 143, 144, 145, 148, 156, 157, 158, 164, 165, 166, 169, 170, 172, 173], "set_as_param": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "set_config": [48, 49, 53, 54], "set_fit_request": [53, 54], "set_fold_specif": [112, 121], "set_index": 92, "set_ml_nuisance_param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 64, 73, 92, 112, 115, 116, 118, 119, 121, 123, 172], "set_param": [48, 49, 53, 54, 90, 112, 115], "set_sample_split": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 75, 88, 139, 172], "set_score_request": [48, 49, 53, 54], "set_styl": [92, 93], "set_text": 75, "set_threshold": [61, 62, 63, 64, 65, 66, 102, 112, 121, 122, 123, 124, 139, 140, 156, 170, 173], "set_tick": 91, "set_ticklabel": 91, "set_titl": [77, 78, 88, 90, 91, 97], "set_x_d": [4, 5, 6, 7, 8, 9], "set_xlabel": [77, 78, 80, 88, 90, 91, 97], "set_xlim": 80, "set_xtick": 94, "set_xticklabel": 94, "set_ylabel": [77, 78, 88, 90, 91, 94, 97], "set_ylim": [83, 88, 90, 91, 96], "setdiff": 172, "setdiff1d": 91, "setminu": [63, 91, 156], "settings_l": 90, "settings_m": 90, "setup": [169, 172], "seven": [63, 91], "sever": [57, 64, 65, 69, 70, 71, 72, 75, 76, 77, 90, 92, 93, 99, 100, 103, 112, 173], "shadow": 89, "shape": [8, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 51, 52, 53, 54, 68, 69, 72, 75, 76, 77, 78, 81, 82, 85, 86, 91, 92, 95, 97, 99, 100, 112, 114, 124], "share": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 63, 64, 91, 92], "sharma": [100, 171], "sharp": 46, "shift": [69, 72, 89], "shock": [63, 91], "short": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 99, 100, 124, 128, 157, 158, 171, 172, 173], "shortcut": 64, "shortli": [63, 65, 91, 112, 122], "shota": 171, "should": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 53, 54, 64, 66, 69, 70, 72, 75, 76, 78, 85, 86, 92, 97, 98, 99, 101, 104, 109, 111, 112, 116, 118, 121, 124, 125, 132, 156, 157, 158, 168], "show": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 61, 63, 66, 67, 69, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 84, 87, 88, 90, 91, 94, 97, 98, 100, 101, 103, 157, 166, 169], "show_progress_bar": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76, 173], "showcas": 95, "showlabel": 100, "showlegend": 100, "shown": [60, 79, 94, 170], "showscal": [81, 82, 84], "shrink": 97, "shuffl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 139], "side": [97, 124, 157, 163], "sigma": [17, 18, 19, 30, 31, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 61, 63, 66, 80, 91, 101, 103, 111, 139, 156, 157, 158, 160, 162, 163, 166, 167], "sigma2": [112, 120, 121, 123, 157, 163], "sigma_": [17, 18, 19, 33, 34, 36, 40, 41, 42, 43, 44, 61, 63, 80, 91, 103], "sigma_0": [157, 167], "sigma_j": 156, "sigmoid": [41, 94], "sign": [98, 100], "signal": [51, 52], "signatur": [25, 26, 27, 28, 29, 38, 39, 140], "signif": [60, 62, 63, 64, 65, 66, 112, 120, 121, 122, 123, 124, 139, 140, 156, 170, 173], "signific": [60, 63, 64, 65, 66, 69, 72, 78, 87, 88, 92, 95, 97, 99, 100, 112, 120, 121, 122, 123, 124, 139, 140, 156, 157, 163, 170, 173], "significantli": 77, "silverman": [27, 28, 29], "sim": [17, 18, 19, 31, 32, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 47, 61, 62, 63, 66, 68, 80, 83, 91, 95, 96, 101, 103, 124], "sim_data": 71, "similar": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 40, 62, 65, 70, 81, 82, 87, 90, 93, 97, 98, 99, 100, 124, 140, 141, 142], "similarli": [70, 90, 98], "simpl": [32, 53, 54, 62, 65, 76, 81, 82, 85, 86, 87, 88, 89, 95, 100, 110, 124, 157, 158], "simplest": 111, "simpli": [65, 67, 173], "simplic": [64, 75, 92, 95, 100], "simplif": [157, 159], "simplifi": [69, 72, 88, 94, 100, 111, 157, 166], "simul": [17, 18, 19, 31, 32, 33, 34, 35, 36, 40, 41, 42, 44, 45, 61, 65, 66, 69, 71, 72, 75, 80, 81, 82, 83, 84, 85, 86, 90, 96, 97, 100, 101, 103, 112, 116, 121, 123, 156, 170], "simul_data": 30, "simulaten": [124, 132], "simulation_run": 84, "simult": 62, "simultan": [70, 110, 173], "sin": [32, 35, 41, 45, 68, 81, 82, 85, 86], "sinc": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 48, 53, 64, 66, 67, 68, 75, 76, 78, 85, 86, 87, 89, 90, 92, 94, 101, 112, 114, 124, 126, 157, 163, 165, 169, 172], "singl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 67, 69, 70, 71, 72, 76, 85, 86, 93, 94, 112, 120, 156], "single_learner_pipelin": [112, 120], "singleton": 139, "sinh": 45, "sipp": [64, 92, 93], "site": [68, 69, 70, 71, 72, 91, 92, 97, 100], "situat": [63, 91], "six": [17, 19, 63], "sixth": 91, "size": [13, 16, 30, 61, 63, 64, 65, 68, 69, 70, 71, 72, 75, 77, 80, 83, 84, 87, 90, 92, 94, 95, 96, 98, 100, 102, 104, 109, 112, 117, 118, 124, 125, 139, 140, 156, 157, 160, 162, 170, 173], "sizeabl": 100, "skill": 171, "skip": 50, "sklearn": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 45, 46, 48, 49, 52, 53, 54, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 97, 98, 99, 100, 101, 102, 111, 112, 113, 114, 115, 116, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170, 173], "skotara": 100, "slide": 94, "slight": [124, 127, 129], "slightli": [12, 14, 15, 68, 69, 72, 75, 85, 86, 87, 111, 124, 128, 130, 140, 141, 142, 143, 144, 157, 158], "slow": [61, 80, 103], "slower": [61, 80, 103], "small": [32, 66, 67, 68, 76, 77, 88, 95, 101, 124, 157, 158, 165, 173], "smaller": [41, 64, 67, 85, 86, 87, 90, 92, 97, 100, 124, 173], "smallest": [14, 75, 77], "smpl": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 63, 75, 80, 91, 139, 140], "smpls_cluster": [63, 91], "smucler": [98, 172], "sn": [67, 69, 72, 73, 75, 76, 77, 78, 80, 91, 92, 93, 100, 101], "so": [17, 19, 53, 54, 60, 64, 65, 66, 67, 69, 70, 72, 79, 90, 92, 94, 100, 101, 112, 116, 156, 173], "social": [94, 171], "societi": [63, 91, 100, 171], "softmax": [17, 19], "softwar": [65, 112, 122, 168, 170, 171, 172], "solari": 172, "sole": [70, 100], "solut": [102, 111, 140], "solv": [20, 63, 91, 111, 112, 123, 124, 127, 128, 129, 156], "solver": [92, 101, 124], "some": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 53, 54, 64, 65, 66, 67, 68, 70, 73, 75, 76, 90, 92, 93, 97, 98, 99, 101, 111, 112, 113, 114, 115, 116, 119, 121, 124, 133, 169, 172], "sometim": [75, 124, 130], "sonabend": [65, 112, 122], "sophist": [112, 118, 123], "sort": [13, 77, 92, 98, 124], "sort_bi": 13, "sort_valu": [77, 78], "sourc": [65, 112, 122, 170, 172], "sourcefileload": 84, "sp": 62, "space": [63, 76, 77, 91, 112, 117, 123, 173], "spars": [84, 112, 117, 118, 123, 156, 170, 171], "sparsiti": 171, "spec": 171, "special": [63, 89, 91, 110, 124], "specialis": [107, 109], "specif": [12, 14, 15, 16, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 46, 63, 64, 69, 70, 71, 72, 75, 76, 77, 78, 88, 91, 92, 100, 104, 109, 110, 111, 112, 114, 116, 119, 121, 124, 127, 129, 139, 140, 148, 156, 163, 167, 168, 170, 172], "specifi": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 46, 55, 56, 60, 63, 64, 65, 66, 67, 69, 70, 71, 72, 76, 77, 78, 79, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 95, 96, 97, 99, 100, 101, 102, 104, 106, 109, 110, 111, 116, 118, 119, 120, 121, 123, 124, 145, 148, 169, 170, 172, 173], "specifii": 93, "speed": [16, 23, 29, 75], "speedup": 75, "spefici": 25, "spindler": [42, 75, 84, 90, 98, 100, 168, 171, 172], "spine": [92, 93], "spline": [81, 82, 111], "spline_basi": [81, 82, 111], "spline_grid": [81, 82], "split": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 60, 63, 65, 66, 67, 69, 70, 71, 72, 75, 88, 91, 93, 95, 99, 101, 110, 111, 112, 113, 123, 124, 126, 127, 129, 140, 156, 170, 172], "split_sampl": [75, 88], "splitter": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "sponsor": [64, 92, 93], "sprintf": 61, "sq_error": 84, "sqrt": [17, 18, 19, 31, 34, 35, 40, 61, 63, 65, 73, 80, 83, 91, 96, 103, 139, 156, 157, 158, 170], "squar": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 49, 54, 64, 77, 84, 92, 112, 113, 123, 124, 157, 167, 171], "squarederror": [64, 92, 173], "squeez": [67, 83, 89, 96, 101], "src": 92, "ssm": [9, 36, 110, 138], "ssrn": 33, "stabil": [17, 19, 87], "stabl": [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 124, 125, 168], "stack": [65, 76, 112, 120], "stacking__final_estimator__alpha": 76, "stacking__final_estimator__c": 76, "stacking__final_estimator__max_it": 76, "stacking__lgbm__lambda_l1": 76, "stacking__lgbm__lambda_l2": 76, "stacking__lgbm__learning_r": 76, "stacking__lgbm__max_depth": 76, "stacking__lgbm__min_child_sampl": 76, "stacking__lgbm__n_estim": 76, "stacking_classifi": 76, "stacking_regressor": 76, "stackingclassifi": [76, 97], "stackingregressor": [76, 97], "stackingregressorstackingregressor": 76, "stacklrn": 65, "stackrel": 124, "stage": [46, 70, 81, 82, 85, 86, 89, 95, 97, 112, 123, 124, 172, 173], "stagger": [124, 130], "stai": [124, 130], "standard": [16, 18, 62, 65, 69, 70, 71, 72, 76, 83, 85, 86, 97, 98, 105, 107, 109, 124, 125, 127, 128, 129, 139, 140, 156, 157, 163, 167, 172, 173], "standard_norm": [104, 109, 112, 117, 118, 156, 170], "standardscal": 92, "star": 124, "start": [17, 19, 62, 64, 65, 69, 70, 71, 72, 75, 76, 77, 81, 82, 84, 87, 90, 91, 92, 96, 100, 124, 126, 168, 173], "start_dat": [17, 19], "start_tim": 77, "stat": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 80, 97, 104, 109, 112, 122, 123, 124, 156, 168, 171], "stat_bin": 61, "stat_dens": 64, "state": [76, 173], "stationar": 67, "stationari": [124, 126], "statist": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 36, 37, 38, 39, 43, 57, 63, 91, 98, 99, 100, 156, 157, 163, 168, 170, 171, 172, 173], "statsmodel": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 51, 97], "statu": [62, 64, 66, 67, 92, 94, 97, 101, 124, 130], "std": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 76, 77, 78, 79, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 99, 100, 101, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 139, 140, 156, 170, 173], "stefan": 171, "step": [12, 14, 15, 16, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 64, 65, 76, 80, 85, 86, 87, 92, 95, 103, 112, 123, 124, 156, 168, 173], "stepdown": 156, "stick": [64, 92], "still": [66, 67, 81, 82, 85, 86, 87, 93, 97, 99, 101, 112, 113, 124, 128], "stochast": [38, 39, 124, 136, 137, 170], "stock": [64, 92, 93, 98], "store": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 77, 98, 102, 112, 113, 139, 140, 156, 157, 163, 172], "store_model": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 90], "store_predict": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 92, 95], "stori": [100, 171], "str": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 46, 48, 49, 50, 51, 53, 54, 55, 56, 64, 69, 72, 76, 77, 78, 85, 86, 96, 97, 98, 111, 124, 172], "straightforward": [69, 72, 75, 85, 86, 111], "strategi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 55, 94, 100, 124, 173], "stratifi": [75, 88], "stratum": 94, "strength": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 98, 99, 100, 157, 158, 163, 166], "strftime": 69, "strictli": 124, "string": [12, 13, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 111, 156, 157, 163, 170, 172], "string_label": 94, "strong": [66, 98, 101, 157, 158], "stronger": [98, 124, 128, 156, 173], "structur": [10, 11, 16, 17, 19, 44, 63, 64, 66, 70, 76, 84, 91, 92, 98, 101, 103, 112, 121, 168, 171, 173], "student": 171, "studi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 50, 63, 64, 75, 76, 84, 90, 91, 92, 93, 98, 99, 124, 125, 170, 173], "study_kwarg": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "style": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 69, 72, 90, 172], "styler": 172, "styliz": 100, "sub": [48, 49, 53, 54, 63, 91], "subclass": [106, 109, 172], "subfold": [112, 123], "subgroup": [25, 64, 92, 172], "subject": [63, 91], "submiss": 172, "submit": [69, 72], "submodel": 76, "submodul": [76, 172], "subobject": [48, 49, 53, 54], "subplot": [63, 68, 75, 77, 78, 80, 81, 82, 83, 85, 86, 88, 90, 91, 92, 93, 94, 96, 97], "subplots_adjust": 75, "subpopul": [124, 138], "subsampl": [37, 65, 75, 140, 142], "subscript": [124, 128, 140, 142, 144, 157, 158], "subsequ": [63, 77, 91], "subset": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 53, 63, 75, 76, 91, 95, 102, 111, 112, 113, 157, 158, 160, 162], "subseteq": 111, "substanti": [64, 92, 94], "substract": 156, "subtract": 156, "sudo": 169, "suffic": 100, "suffici": [75, 90, 100], "suggest": [63, 64, 76, 91, 92, 100, 172], "suggest_float": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76, 112, 117], "suggest_int": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76, 173], "suitabl": [66, 81, 82, 101, 112, 117, 124, 128], "sum": [17, 19, 49, 54, 63, 64, 91, 92, 93, 96, 97, 111, 156], "sum_": [17, 19, 47, 61, 63, 80, 91, 97, 102, 103, 111, 124, 125, 130, 156], "sum_i": 94, "sum_oth": 91, "sum_riv": 91, "summar": [13, 62, 69, 70, 71, 72, 77, 78, 94, 100, 102, 157, 163], "summari": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 62, 63, 65, 66, 67, 69, 70, 71, 72, 73, 76, 78, 79, 81, 82, 83, 85, 86, 87, 88, 89, 91, 93, 96, 98, 99, 100, 101, 103, 104, 106, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 139, 140, 156, 157, 170, 172, 173], "summary_df": 98, "summary_result": 64, "summary_stat": 77, "superior": 77, "suppli": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 76, 81, 82, 85, 86, 87, 95, 104, 109, 111, 157, 158, 163, 164], "support": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 46, 56, 62, 63, 69, 70, 71, 72, 75, 89, 91, 95, 97, 108, 109, 112, 114, 116, 121, 123, 124, 133, 173], "support_s": [32, 81, 82, 85, 86, 95], "support_t": 95, "support_w": 95, "suppos": 100, "suppress": [62, 64, 65, 66, 76, 173], "suppresswarn": 61, "suprema": 156, "suptitl": [75, 83, 90, 93, 96], "supxlabel": [83, 93, 96], "supylabel": [83, 93, 96], "sure": [77, 78, 112, 119, 172], "surfac": [76, 81, 82, 84], "surgic": 98, "surpress": [63, 170], "survei": [64, 92, 93, 173], "susan": 171, "sven": [100, 168, 171], "svenklaassen": [168, 172], "svg": [61, 80], "switch": [61, 80, 100, 103], "symbol": 100, "symmetr": 45, "syntax": [97, 124], "syntaxwarn": 91, "synthesi": 171, "synthet": [17, 19, 32, 41, 47, 60, 77, 79, 81, 82, 83, 85, 86, 89, 90, 95, 96, 98], "syrgkani": [98, 100, 171], "system": 171, "szita": 171, "t": [4, 5, 7, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 48, 49, 53, 54, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 76, 77, 78, 79, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 105, 106, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 139, 140, 141, 142, 156, 157, 159, 160, 170, 173], "t_": [16, 69, 70, 71, 72, 124, 127, 128, 129, 140, 142, 144], "t_0": [37, 140, 151], "t_1_start": 75, "t_1_stop": 75, "t_2_start": 75, "t_2_stop": 75, "t_3_start": 75, "t_3_stop": 75, "t_col": [4, 5, 7, 15, 16, 69, 70, 71, 72, 105, 106, 109, 124, 125, 126, 127, 129], "t_df": 95, "t_diff": 68, "t_dml": 61, "t_g": [17, 19], "t_i": [67, 95, 97, 124, 126, 127, 130, 140, 142], "t_idx": 68, "t_nonorth": 61, "t_orth_nosplit": 61, "t_sigmoid": 95, "t_stat": 156, "t_value_ev": 14, "t_value_pr": 14, "tabl": [61, 63, 64, 65, 66, 77, 78, 98, 102, 104, 109, 112, 120, 121, 123, 124, 139, 140, 156, 170, 173], "tabpfn": 172, "tabpfnclassifi": 77, "tabpfnregressor": 77, "tabular": [75, 77, 104, 109, 156, 170, 173], "taddi": 171, "tailor": [105, 109], "takatsu": 98, "take": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 40, 66, 67, 68, 69, 70, 71, 72, 75, 76, 81, 82, 83, 84, 85, 86, 93, 96, 97, 98, 99, 101, 102, 111, 112, 114, 124, 125, 130, 133, 134, 135, 136, 137, 140, 145, 148, 157, 164, 165, 166, 170], "taken": [64, 92, 93, 173], "taker": [25, 172], "talk": 173, "target": [12, 14, 15, 20, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 53, 54, 60, 63, 64, 65, 66, 75, 81, 82, 91, 111, 112, 113, 120, 121, 122, 123, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 138, 139, 140, 149, 150, 156, 157, 165, 167, 168, 170, 172, 173], "task": [60, 77, 90, 104, 109, 120, 139], "task_typ": 172, "tau": [47, 68, 83, 93, 94, 96, 97, 111, 124, 140, 146, 149, 150], "tau_": [94, 97, 124], "tau_0": [97, 124], "tau_1": 94, "tau_2": 94, "tau_vec": [83, 93, 96], "tax": [64, 92, 93], "te": [62, 81, 82, 95], "techniqu": [61, 80, 103, 139, 173], "teen": 70, "teichert": 172, "templat": 172, "ten": 90, "tend": [64, 92, 93, 124], "tensor": [81, 82], "tenth": 171, "term": [7, 14, 61, 63, 64, 65, 68, 80, 84, 91, 92, 94, 100, 103, 124, 130, 168, 173], "termin": [65, 112, 122, 123, 173], "terminatorev": 65, "test": [11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 37, 38, 39, 48, 49, 53, 54, 60, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 80, 87, 91, 98, 100, 103, 112, 120, 121, 122, 123, 124, 139, 140, 156, 170, 171, 172, 173], "test_id": [63, 139], "test_ind": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "test_indic": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "test_set": 139, "test_siz": 80, "text": [16, 17, 18, 19, 31, 33, 35, 37, 40, 41, 46, 47, 63, 64, 69, 70, 71, 72, 77, 83, 84, 89, 94, 95, 96, 97, 100, 111, 124, 127, 128, 129, 130, 135, 139, 140, 142, 144, 151, 157, 160, 162], "textbf": [102, 112, 121, 123, 173], "textposit": 100, "textrm": [157, 158, 164, 165, 166, 167], "tg": [65, 73, 104, 109, 170], "th": [63, 91], "than": [26, 61, 62, 64, 75, 80, 84, 88, 92, 93, 94, 97, 98, 99, 100, 103, 124, 128, 157, 163, 173], "thank": [62, 64, 65, 92, 172], "thatw": 68, "thei": [17, 19, 62, 64, 68, 85, 86, 92, 94, 98, 104, 109, 124, 157, 167], "them": [13, 64, 65, 81, 82, 83, 87, 90, 92, 96, 124], "theme": [63, 64], "theme_minim": [61, 64, 66], "theorem": [124, 130, 157, 167], "theoret": [75, 100, 139, 171], "theori": [111, 171], "therebi": [63, 65, 91, 173], "therefor": [69, 71, 72, 78, 94, 97, 99, 139, 140, 157, 166], "thereof": 41, "theta": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 43, 45, 61, 63, 65, 66, 67, 68, 69, 72, 75, 76, 77, 78, 80, 84, 87, 88, 91, 97, 99, 100, 101, 102, 103, 104, 109, 111, 112, 114, 117, 118, 122, 123, 124, 125, 127, 128, 129, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 152, 153, 154, 155, 156, 157, 158, 163, 166, 167, 170, 173], "theta_": [17, 19, 69, 72, 78, 97, 100, 111, 124, 131, 132, 156, 157, 167], "theta_0": [25, 26, 32, 37, 38, 39, 61, 63, 64, 66, 76, 78, 80, 81, 82, 84, 85, 86, 91, 92, 100, 101, 103, 111, 124, 126, 133, 134, 136, 137, 138, 140, 149, 150, 153, 156, 157, 164, 165, 167, 170], "theta_d": 77, "theta_dml": [61, 80, 103], "theta_dml_po": [61, 80, 103], "theta_initi": 80, "theta_nonorth": [61, 80], "theta_orth_nosplit": [61, 80], "theta_orth_po_nosplit": [61, 80], "theta_resc": 61, "theta_t": 68, "thi": [4, 12, 13, 14, 15, 16, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 48, 49, 50, 52, 53, 54, 56, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 111, 112, 114, 117, 118, 122, 123, 124, 128, 130, 131, 132, 133, 134, 139, 140, 141, 142, 143, 144, 146, 148, 149, 150, 156, 157, 158, 163, 164, 165, 168, 169, 170, 171, 172, 173], "think": 65, "third": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 70, 71, 80, 91, 103, 139], "thirion": [168, 170], "this_df": [84, 92], "this_split_ind": 91, "those": [62, 64, 70, 92, 93, 98], "though": [60, 79, 94], "thread": [94, 112, 120, 121, 123], "three": [63, 65, 76, 85, 86, 169, 172], "threshold": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 55, 56, 97, 100, 112, 120, 121, 123, 124], "through": [62, 70, 83, 85, 86, 96, 97, 104, 109, 112, 123, 124], "throughout": [87, 98], "thu": [90, 97, 111, 124], "tibbl": 62, "tick": 16, "tick_param": 97, "tight": 80, "tight_layout": [69, 72, 76, 89, 90, 91, 97], "tighter": [76, 97], "tild": [17, 18, 19, 31, 35, 40, 63, 91, 94, 102, 111, 139, 140, 142, 144, 149, 150, 154, 155, 156, 157, 166, 167], "tile": [76, 98], "time": [5, 7, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 42, 43, 61, 62, 63, 64, 66, 67, 68, 70, 76, 77, 80, 84, 85, 86, 91, 92, 93, 97, 99, 100, 101, 105, 106, 109, 124, 125, 126, 127, 128, 129, 130, 140, 157, 171, 172, 173], "time_budget": 90, "time_df": 68, "time_period": 68, "time_typ": [17, 19, 69, 72], "timeout": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "titiunik": [124, 171], "titl": [13, 16, 63, 64, 66, 69, 70, 72, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 93, 94, 96, 97, 100, 168], "title_fonts": [69, 72], "tmp": [68, 81, 91], "tname": 62, "tnr": [65, 112, 122, 123, 173], "to_datetim": 69, "to_fram": 95, "to_numpi": [83, 87, 93, 96], "to_str": [76, 77], "todo": [63, 73], "toeplitz": 84, "togeth": [85, 86, 108, 109, 156], "toler": 91, "tomasz": [171, 172], "toml": 172, "tongarlak": 172, "too": [56, 75], "tool": [62, 65, 99, 173], "top": [63, 75, 91, 92, 93, 97, 100, 124, 168], "total": [49, 54, 64, 90, 92, 124, 125], "total_width": [76, 77], "tpe": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "tpesampl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76], "tpot": 90, "track": [105, 107, 109], "tracker": 168, "tradit": [77, 156], "train": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 53, 54, 61, 63, 65, 75, 77, 80, 81, 82, 83, 85, 86, 88, 89, 91, 92, 95, 96, 102, 103, 139], "train_id": [63, 139], "train_ind": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "train_indic": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "train_set": 139, "train_test_split": 80, "transact": 171, "transform": [31, 40, 41, 47, 77, 88, 94, 100, 112, 117, 173], "translat": [77, 84], "transpos": 68, "treament": 95, "treat": [16, 17, 18, 19, 26, 55, 62, 67, 68, 69, 70, 71, 72, 76, 78, 87, 89, 95, 97, 100, 111, 124, 126, 127, 129, 130, 134, 140, 142, 144, 156, 173], "treat1_param": 94, "treat2_param": 94, "treat_var": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 112, 116, 121], "treatment": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 41, 44, 46, 55, 60, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 78, 79, 84, 87, 89, 90, 91, 95, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 112, 113, 114, 116, 119, 121, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 138, 139, 141, 142, 143, 144, 145, 146, 148, 149, 150, 156, 160, 162, 163, 164, 166, 168, 170, 171, 172, 173], "treatment_df": 68, "treatment_effect": [32, 81, 82], "treatment_level": [22, 23, 76, 77, 78, 88, 124], "treatment_levels_plot": [76, 77], "treatment_lvl": 76, "treatment_var": [4, 5, 6, 7, 8, 9], "tree": [26, 52, 64, 65, 67, 68, 69, 72, 75, 76, 77, 92, 102, 110, 112, 120, 121, 124, 139, 140, 156, 170, 172], "tree_param": [26, 52], "tree_summari": 92, "trees_class": [64, 92], "trend": [62, 67, 68, 69, 72, 91, 124, 126, 128, 130, 171], "tri": [84, 157, 158], "trial": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76, 112, 117, 173], "trials_datafram": 76, "triangular": [46, 97, 124], "trim": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 64, 92, 93, 100], "trimming_rul": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 93], "trimming_threshold": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 64, 81, 88, 92, 93, 95, 96, 100], "trm": [65, 112, 122, 123, 173], "true": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 36, 37, 38, 39, 41, 46, 47, 48, 49, 50, 53, 54, 56, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 109, 112, 120, 121, 122, 123, 124, 126, 139, 140, 145, 146, 149, 150, 154, 155, 156, 157, 159, 160, 161, 162, 167, 170, 173], "true_effect": [68, 81, 82, 85, 86, 98], "true_gatet_effect": 87, "true_group_effect": 87, "true_tau": 97, "truemfunct": 98, "truncat": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 93], "trust": 76, "try": [75, 76, 99], "tune": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 75, 77, 84, 97, 110, 113, 114, 117, 118, 120, 122, 123, 124, 168, 170, 172], "tune_ml_model": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76, 112, 117, 172, 173], "tune_on_fold": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 112, 122, 123, 173], "tune_r": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "tune_set": [65, 112, 122, 123, 173], "tuned_model": 90, "tuner": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 112, 123], "tunergridsearch": 65, "tuning_result": 76, "tupl": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 69, 72, 124, 127, 128, 129], "turn": 100, "turrel": 45, "tutori": 64, "tw": [92, 93], "twice": 124, "twinx": [77, 78], "two": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 60, 61, 64, 65, 67, 69, 70, 71, 72, 75, 76, 79, 80, 83, 88, 90, 92, 93, 94, 95, 96, 98, 99, 100, 102, 103, 111, 112, 120, 123, 126, 129, 139, 149, 156, 173], "twoclass": 65, "twoearn": [64, 92, 93, 99, 173], "type": [7, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 41, 46, 47, 48, 49, 51, 52, 53, 54, 55, 57, 61, 62, 63, 64, 65, 69, 72, 75, 80, 89, 90, 91, 97, 100, 103, 110, 112, 120, 121, 124, 140, 152, 153, 156, 157, 166, 172, 173], "typeerror": [69, 70, 71, 72], "typic": [76, 77, 124, 130, 168], "u": [18, 25, 26, 27, 28, 29, 31, 32, 34, 36, 40, 49, 54, 61, 62, 63, 64, 67, 68, 69, 70, 72, 75, 76, 77, 78, 80, 83, 85, 86, 88, 91, 92, 93, 95, 96, 98, 99, 100, 103, 124, 131, 133, 134, 157, 158, 169, 173], "u_hat": [61, 80, 140], "u_i": [33, 36, 42, 45], "u_t": 18, "uehara": 171, "uhash": 65, "ulf": 171, "unabl": 76, "unambigu": 100, "unbalanc": 41, "uncap": [17, 19], "uncertainti": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 85, 86, 88, 97, 99, 157, 163, 173], "unchang": [48, 49, 53, 54], "uncondit": [64, 69, 72, 92, 173], "unconfounded": [100, 171], "under": [25, 30, 61, 64, 67, 70, 80, 92, 95, 97, 100, 103, 124, 128, 130, 138, 156, 171], "underbrac": [61, 68, 80, 103, 111], "underfit": 90, "underli": [31, 35, 64, 65, 69, 72, 76, 77, 78, 85, 86, 94, 95, 124, 130, 157, 158, 173], "underlin": [63, 91], "underset": [97, 124], "understand": [69, 70, 72, 77, 100], "undesir": [112, 114], "unevenli": 139, "unifi": 109, "uniform": [18, 46, 47, 68, 79, 81, 82, 83, 95, 96, 156], "uniform_averag": [49, 54], "uniformli": [25, 69, 72, 83, 93, 156], "union": [25, 41], "uniqu": [7, 60, 69, 70, 71, 72, 75, 76, 77, 78, 79, 97, 105, 106, 109, 124, 127, 129, 140, 157, 167], "unique_label": 90, "unit": [7, 17, 19, 61, 62, 66, 67, 68, 69, 70, 71, 72, 87, 89, 97, 101, 106, 109, 124, 126, 127, 128, 129, 130, 140, 141, 142, 143, 144, 157, 160, 162, 172], "univari": [32, 81, 82], "univers": [16, 171], "unknown": 124, "unlik": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 64, 92, 93, 100], "unobserv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 60, 64, 69, 72, 79, 92, 93, 99, 100, 124, 157, 158, 167, 173], "unpen": 62, "unstabl": [157, 158], "unter": [63, 64, 65], "untest": 100, "until": [124, 126, 172], "untreat": [89, 100, 124, 126], "untun": 77, "up": [16, 23, 29, 64, 75, 84, 89, 90, 92, 93, 99, 100, 112, 121, 123, 124, 126, 139, 157, 158, 169, 172, 173], "upcom": 172, "updat": [48, 49, 53, 54, 63, 91, 92, 171, 172], "update_layout": [81, 82, 84, 97, 100], "update_trac": [81, 82], "upload": 172, "upon": [140, 172], "upper": [25, 64, 65, 68, 69, 72, 77, 78, 80, 83, 87, 88, 89, 93, 96, 97, 99, 100, 112, 122, 123, 157, 163, 167, 173], "upper_bound": [81, 82], "upsilon": [66, 101], "upsilon_i": [66, 101], "upward": [64, 92, 93, 100], "upweight": 94, "url": [70, 84, 168, 171], "us": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 53, 54, 55, 56, 61, 63, 64, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 109, 111, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 129, 139, 140, 141, 142, 143, 144, 156, 157, 158, 163, 165, 166, 167, 168, 169, 170, 172, 173], "usa": 171, "usabl": 75, "usag": [62, 69, 70, 71, 72, 73, 76, 77, 78, 87, 89, 91, 92, 93, 99, 104, 170, 172], "use_label_encod": [92, 173], "use_other_treat_as_covari": [4, 5, 6, 7, 8, 9, 104, 109], "use_pred_offset": [112, 120, 123], "use_weight": [112, 120, 121], "usecolormap": [81, 82], "user": [20, 21, 48, 49, 53, 54, 61, 62, 63, 64, 65, 70, 75, 78, 80, 87, 88, 91, 92, 97, 99, 111, 112, 114, 119, 120, 123, 124, 140, 156, 168, 169, 170, 172, 173], "userwarn": [68, 69, 71, 72, 77, 92, 97, 98, 100], "usual": [63, 67, 69, 70, 71, 72, 75, 76, 77, 81, 82, 91, 97, 99, 100, 111, 112, 114, 139, 157, 167], "utab019": 41, "util": [0, 21, 68, 69, 72, 75, 88, 90, 94, 97, 100, 112, 114, 124, 172], "v": [10, 11, 25, 26, 34, 36, 38, 39, 42, 43, 44, 49, 54, 61, 63, 64, 69, 72, 75, 76, 77, 78, 80, 87, 88, 90, 91, 92, 94, 97, 98, 102, 103, 111, 124, 131, 133, 134, 136, 137, 156, 168, 170, 171, 172, 173], "v108": 168, "v12": [168, 170], "v2": 77, "v22": 65, "v23": 168, "v_": [43, 63, 91, 124], "v_i": [33, 34, 36, 44, 45, 61, 80, 103, 124], "v_j": 156, "val": [34, 69, 70, 71, 72, 139, 171], "val_list": 84, "valid": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 37, 38, 39, 50, 55, 56, 61, 62, 63, 64, 67, 68, 69, 70, 72, 75, 77, 80, 83, 90, 91, 92, 93, 96, 97, 100, 103, 110, 111, 112, 120, 123, 139, 140, 146, 149, 150, 157, 158, 171, 173], "valu": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 53, 54, 56, 57, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 97, 98, 99, 100, 102, 104, 105, 106, 109, 110, 112, 113, 114, 116, 118, 120, 121, 122, 123, 124, 125, 130, 131, 138, 139, 146, 149, 150, 154, 155, 156, 157, 158, 163, 167, 170, 172, 173], "value_count": 92, "van": 171, "vanderpla": [168, 170], "vanish": [61, 80, 103], "var": [17, 18, 19, 31, 35, 40, 63, 91, 94, 97, 157, 158, 164, 165, 166, 167], "var_ep": 100, "varepsilon": [25, 31, 40, 43, 63, 66, 91, 101, 111, 124, 133], "varepsilon_": [17, 19, 43, 63, 69, 72, 91], "varepsilon_0": 18, "varepsilon_1": 18, "varepsilon_d": [35, 40], "varepsilon_i": [35, 42, 66, 83, 96, 101], "vari": [17, 19, 64, 68, 69, 72, 75, 92, 94, 100], "variabl": [4, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 41, 46, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 76, 77, 78, 84, 87, 89, 90, 91, 92, 93, 97, 99, 100, 101, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 119, 120, 121, 122, 123, 124, 126, 127, 129, 130, 131, 133, 134, 135, 136, 137, 139, 140, 156, 157, 158, 163, 167, 170, 171, 172, 173], "varianc": [20, 21, 63, 65, 91, 97, 99, 100, 110, 124, 139, 157, 158, 163, 165, 166, 167, 170, 172], "variant": [62, 70, 88], "variat": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 88, 99, 157, 158, 167], "variou": [62, 90, 100, 112, 123, 173], "varoquaux": [168, 170], "vasili": [100, 171], "vast": 77, "vector": [12, 14, 15, 17, 19, 22, 24, 25, 26, 27, 28, 30, 32, 33, 34, 36, 37, 38, 39, 42, 43, 45, 60, 63, 64, 66, 67, 79, 85, 86, 87, 89, 91, 92, 95, 98, 101, 124, 126, 135, 136, 137, 138, 156, 170, 172], "vee": [140, 142, 144], "venv": 169, "verbos": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 64, 67, 68, 69, 72, 75, 76, 77, 80, 83, 90, 93, 96, 97, 100, 112, 117, 173], "veri": [62, 63, 65, 70, 75, 76, 87, 91, 98, 100, 140, 168], "verifi": 94, "versa": [75, 94, 157, 163], "version": [4, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 48, 49, 53, 54, 63, 64, 65, 67, 68, 81, 100, 102, 104, 109, 111, 112, 124, 127, 128, 129, 140, 156, 157, 159, 160, 161, 162, 164, 165, 168, 172, 173], "versoin": 100, "versu": 89, "vertic": [63, 77, 78, 91], "via": [12, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 55, 62, 66, 67, 68, 69, 70, 71, 72, 75, 76, 83, 84, 85, 86, 87, 88, 91, 97, 99, 101, 102, 104, 109, 110, 111, 112, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 139, 146, 155, 156, 157, 158, 163, 167, 168, 169, 170, 171, 172, 173], "viabl": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "vice": [75, 94, 157, 163], "victor": [84, 100, 139, 168, 171], "vignett": [62, 172], "villa": [60, 79], "violat": [69, 72], "violet": [83, 93, 96], "vira": 171, "virtual": [70, 169], "virtualenv": 169, "visibl": [93, 97, 100], "visit": [168, 173], "visual": [13, 63, 69, 70, 71, 72, 76, 87, 88, 90, 91, 97], "vmax": 72, "vmin": 72, "vol": 62, "volum": [100, 168], "voluntari": 94, "vv740": 91, "vv760g": 91, "w": [10, 11, 17, 18, 19, 20, 21, 31, 37, 40, 44, 48, 49, 53, 54, 63, 84, 91, 94, 95, 98, 102, 103, 124, 127, 128, 129, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170], "w24678": 139, "w30302": 171, "w_": [17, 18, 19, 63, 91, 95, 124], "w_1": [17, 18, 19, 95], "w_2": [17, 18, 19, 95], "w_3": [17, 18, 19], "w_4": [17, 18, 19], "w_df": 95, "w_i": [36, 67, 95, 97, 102, 111, 124, 139, 140, 142, 144, 156], "wa": [50, 63, 68, 69, 72, 90, 91, 97, 100, 172], "wage": 70, "wager": 171, "wai": [64, 75, 90, 92, 98, 100, 112, 119, 140, 169], "wander": 45, "wang": 171, "want": [60, 63, 64, 65, 67, 75, 76, 79, 83, 91, 96, 97, 112, 114, 124, 168, 169, 171], "warn": [13, 16, 55, 60, 61, 62, 63, 64, 65, 66, 68, 69, 71, 72, 75, 76, 77, 80, 92, 98, 100, 102, 112, 117, 121, 122, 123, 124, 139, 140, 156, 170, 172, 173], "warn_msg_prefix": 98, "wayon": 63, "we": [26, 52, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 109, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 134, 139, 140, 142, 144, 147, 151, 156, 157, 158, 167, 169, 170, 172, 173], "weak": [25, 157, 158, 171, 172], "weakest": [69, 72], "wealth": [10, 99], "websit": [64, 65, 70, 112, 119, 168], "wedg": [63, 91], "week": 172, "wei": 156, "weight": [13, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 48, 49, 53, 54, 63, 64, 65, 66, 69, 70, 71, 72, 77, 78, 87, 88, 91, 92, 97, 101, 110, 112, 120, 121, 124, 125, 140, 145, 148, 156, 157, 164, 165, 172], "weights_bar": [22, 23, 26, 88], "weights_dict": 88, "weiss": [168, 170], "well": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 53, 54, 61, 63, 75, 76, 80, 84, 89, 90, 91, 98, 102, 103, 139, 169, 170], "were": [64, 66, 76, 92, 93, 101, 173], "what": [62, 75, 84, 171], "when": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 53, 54, 56, 64, 67, 70, 77, 88, 92, 94, 98, 124, 134, 138, 140, 156, 168, 169, 170, 172], "whenev": [64, 92], "whera": [157, 165], "where": [12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 49, 51, 52, 54, 60, 61, 63, 64, 66, 67, 68, 69, 70, 71, 72, 78, 79, 80, 83, 87, 89, 91, 92, 94, 95, 96, 97, 98, 100, 101, 102, 103, 111, 112, 114, 116, 124, 125, 126, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 163, 164, 165, 167, 169, 170, 172, 173], "wherea": [32, 66, 67, 69, 72, 78, 98, 100, 101, 124, 127, 129, 140, 148, 157, 164, 173], "whether": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 36, 37, 38, 39, 46, 50, 51, 55, 56, 64, 68, 75, 92, 93, 97, 98, 100, 104, 109, 112, 121, 124, 157, 158, 172], "which": [5, 6, 7, 8, 9, 12, 14, 15, 16, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 48, 53, 55, 56, 60, 61, 62, 64, 65, 66, 67, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 84, 87, 90, 92, 93, 95, 97, 99, 100, 101, 103, 104, 109, 111, 112, 114, 115, 122, 123, 124, 128, 140, 156, 157, 158, 163, 164, 165, 167, 169, 172, 173], "while": [60, 79, 124], "white": [63, 85, 86, 91, 100], "whitegrid": [92, 93], "whitnei": [100, 171], "who": [62, 64, 92, 100], "whole": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 67, 80, 97, 103, 112, 123, 140, 141, 157, 158], "whom": 124, "why": [70, 77], "widehat": [69, 70, 71, 72, 124], "width": [13, 16, 61, 63, 77, 81, 82, 84], "wiki": 172, "wiksel": 171, "wild": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 156], "window": 169, "wise": [85, 86], "wish": 169, "within": [46, 63, 69, 70, 71, 72, 77, 85, 86, 91, 95, 97], "without": [25, 35, 46, 60, 61, 69, 70, 71, 72, 75, 76, 77, 79, 80, 90, 100, 103, 110, 112, 116, 121, 124, 157, 158, 169, 172], "wolf": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 156], "won": 100, "word": [46, 97, 124, 172, 173], "work": [48, 49, 53, 54, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 87, 94, 98, 99, 100, 112, 113, 114, 124, 156, 169, 171], "workflow": [76, 168, 172], "workspac": 92, "world": 171, "worri": 100, "wors": [49, 54], "would": [49, 54, 62, 64, 65, 69, 70, 71, 72, 75, 76, 81, 82, 84, 92, 93, 97, 99, 100, 111, 112, 123, 157, 167, 173], "wrapper": [4, 62, 97, 104, 109, 112, 123], "wright": 98, "write": [61, 62, 66, 67, 80, 101, 103, 157, 167], "written": [124, 140, 157, 164, 165], "wrong": [75, 94], "wspace": 75, "wurd": [63, 64, 65], "www": [168, 169], "x": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 53, 54, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 107, 108, 109, 111, 112, 116, 117, 118, 121, 122, 123, 124, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 162, 164, 165, 166, 167, 170, 173], "x0": [76, 77, 78, 94, 97], "x1": [63, 65, 66, 67, 76, 77, 78, 88, 89, 90, 91, 94, 97, 99, 100, 101, 104, 107, 108, 109, 111, 112, 113, 124, 140, 156, 157, 158, 170], "x10": [63, 65, 66, 88, 89, 90, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x100": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x11": [63, 65, 66, 89, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x12": [63, 65, 66, 89, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x13": [63, 65, 66, 89, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x14": [63, 65, 66, 89, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x15": [63, 65, 66, 89, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x16": [63, 65, 66, 89, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x17": [63, 65, 66, 89, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x18": [63, 65, 66, 89, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x19": [63, 65, 66, 89, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x1x2x3x4x5x6x7x8x9x10": 63, "x2": [63, 65, 66, 67, 76, 77, 78, 88, 89, 90, 91, 97, 99, 100, 101, 104, 107, 108, 109, 111, 112, 113, 124, 140, 156, 170], "x20": [63, 65, 66, 89, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x21": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x22": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x23": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x24": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x25": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x26": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x27": [63, 65, 66, 76, 91, 101, 104, 108, 109, 124, 170], "x28": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x29": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x2_dummi": 100, "x2_preds_control": 100, "x2_preds_treat": 100, "x3": [63, 65, 66, 67, 76, 77, 78, 88, 89, 90, 91, 99, 100, 101, 104, 107, 108, 109, 111, 112, 113, 124, 140, 156, 170], "x30": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x31": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x32": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x33": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x34": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x35": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x36": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x37": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x38": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x39": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x4": [63, 65, 66, 67, 76, 77, 78, 88, 89, 90, 91, 99, 100, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x40": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x41": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x42": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x43": [63, 65, 66, 90, 91, 101, 104, 108, 109, 124, 170], "x44": [63, 65, 66, 90, 91, 101, 104, 108, 109, 124, 170], "x45": [63, 65, 66, 90, 91, 101, 104, 108, 109, 124, 170], "x46": [63, 65, 66, 90, 91, 101, 104, 108, 109, 124, 170], "x47": [63, 65, 66, 90, 91, 101, 104, 108, 109, 124, 170], "x48": [63, 65, 66, 90, 91, 101, 104, 108, 109, 124, 170], "x49": [63, 65, 66, 90, 91, 101, 104, 108, 109, 124, 170], "x5": [63, 65, 66, 88, 89, 90, 91, 100, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x50": [63, 65, 66, 90, 91, 101, 104, 108, 109, 124, 170], "x51": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x52": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x53": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x54": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x55": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x56": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x57": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x58": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x59": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x6": [63, 65, 66, 88, 89, 90, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x60": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x61": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x62": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x63": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x64": [63, 65, 66, 68, 69, 70, 71, 72, 91, 92, 97, 100, 101, 104, 108, 109, 124, 170], "x65": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x66": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x67": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x68": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x69": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x7": [63, 65, 66, 88, 89, 90, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x70": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x71": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x72": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x73": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x74": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x75": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x76": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x77": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x78": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x79": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x8": [63, 65, 66, 88, 89, 90, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x80": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x81": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x82": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x83": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x84": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x85": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x86": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x87": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x88": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x89": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x9": [63, 65, 66, 88, 89, 90, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x90": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x91": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x92": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x93": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x94": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x95": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x96": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x96x97x98x99x100ydcluster_var_icluster_var_jz": 63, "x97": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x98": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x99": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x_": [17, 19, 41, 43, 44, 61, 63, 68, 80, 91, 100, 103], "x_0": [68, 81, 82, 85, 86, 87], "x_1": [17, 18, 19, 31, 35, 37, 38, 39, 40, 68, 81, 82, 83, 85, 86, 87, 89, 96, 100, 124, 135, 136, 137, 157, 158, 170], "x_1x_3": [83, 96], "x_2": [17, 18, 19, 31, 35, 40, 68, 81, 82, 83, 85, 86, 87, 96, 100, 157, 158], "x_3": [17, 18, 19, 31, 35, 40, 68, 81, 82, 85, 86, 87, 157, 158], "x_4": [17, 18, 19, 31, 35, 40, 81, 82, 83, 85, 86, 87, 96], "x_5": [31, 35, 40, 81, 82, 85, 86], "x_6": [81, 82, 85, 86], "x_7": [81, 82, 85, 86], "x_8": [81, 82, 85, 86], "x_9": [81, 82, 85, 86], "x_binary_control": 100, "x_binary_tr": 100, "x_center": [76, 77], "x_col": [4, 5, 6, 7, 8, 9, 16, 60, 63, 64, 65, 69, 70, 71, 72, 79, 84, 91, 92, 93, 95, 97, 98, 99, 100, 104, 105, 106, 109, 112, 114, 124, 125, 127, 129, 170, 172, 173], "x_cols_bench": 100, "x_cols_binari": 100, "x_cols_poli": 91, "x_conf": 96, "x_conf_tru": 96, "x_df": 68, "x_domain": 65, "x_end": [76, 77], "x_i": [32, 33, 34, 36, 41, 42, 44, 45, 47, 61, 66, 67, 80, 83, 85, 86, 94, 96, 97, 101, 103, 111, 124, 126, 127, 129, 130, 138, 140, 142, 144], "x_jitter": [76, 77], "x_p": [37, 38, 39, 89, 124, 135, 136, 137, 170], "x_rang": [76, 77], "x_start": [76, 77], "x_train": 90, "x_true": [83, 96], "x_var": 65, "xaxis_titl": [81, 82, 84, 97, 100], "xformla": 62, "xgb": 90, "xgb_untuned_l": 90, "xgb_untuned_m": 90, "xgbclassifi": [75, 92, 94, 173], "xgboost": [61, 64, 75, 92, 94, 173], "xgbregressor": [75, 90, 92, 94, 173], "xi": [17, 18, 19, 35, 124], "xi_": 156, "xi_0": [43, 63, 91], "xi_i": [66, 101], "xiaoji": 171, "xintercept": [61, 66], "xlab": [61, 63, 64], "xlabel": [68, 69, 70, 72, 76, 77, 78, 81, 82, 83, 85, 86, 89, 90, 92, 93, 96], "xlim": [61, 64, 76, 77], "xmax": [76, 77], "xmax_rel": [76, 77], "xmin": [76, 77], "xmin_rel": [76, 77], "xtick": [76, 77, 78, 90], "xval": [65, 112, 120], "xx": 80, "y": [4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 53, 54, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 131, 133, 134, 135, 136, 137, 139, 140, 141, 142, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 163, 164, 165, 166, 167, 170, 173], "y0": [62, 69, 72, 76, 77, 78, 83, 96], "y0_cvar": 83, "y0_quant": [83, 96], "y1": [62, 69, 72, 83, 96], "y1_cvar": 83, "y1_quant": [83, 96], "y_": [16, 17, 19, 43, 63, 66, 67, 68, 69, 72, 91, 101, 124, 126, 127, 129, 130, 138, 140, 142, 144], "y_0": [12, 14, 18, 47, 140, 143], "y_1": [12, 14, 18, 47, 140, 143], "y_col": [4, 5, 6, 7, 8, 9, 16, 60, 61, 63, 64, 65, 66, 69, 70, 71, 72, 79, 81, 82, 84, 85, 86, 88, 91, 92, 93, 95, 97, 98, 99, 102, 103, 104, 105, 106, 108, 109, 112, 120, 121, 124, 125, 127, 129, 139, 140, 170, 172, 173], "y_df": [68, 95], "y_diff": 68, "y_i": [32, 33, 34, 36, 41, 42, 44, 45, 61, 66, 67, 80, 83, 94, 95, 96, 97, 101, 103, 124, 126, 138], "y_label": [13, 16], "y_lower_quantil": [69, 72], "y_mean": [69, 72], "y_pred": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 49, 54, 75, 112, 113], "y_train": 90, "y_true": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 49, 54, 75, 112, 113], "y_upper_quantil": [69, 72], "ya": 171, "yasui": 171, "yata": 171, "yaxis_titl": [81, 82, 84, 97, 100], "year": [70, 168], "yerr": [68, 76, 77, 78, 85, 86, 90, 92, 94, 97], "yet": [63, 69, 71, 72, 74, 124, 127, 129, 130], "yggvpl": 91, "yi": [41, 171], "yield": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 124], "yintercept": 64, "ylab": [61, 63, 64], "ylabel": [68, 69, 70, 72, 76, 77, 78, 81, 82, 83, 85, 86, 89, 90, 92, 93, 96], "ylim": 92, "ymax": 64, "ymin": 64, "yname": 62, "york": 171, "you": [48, 49, 53, 54, 60, 61, 68, 69, 70, 71, 72, 76, 77, 79, 91, 99, 124, 168, 169, 173], "your": [75, 169], "ython": 168, "yukun": 171, "yusuk": 171, "yuya": 171, "yy": 80, "z": [4, 5, 6, 7, 8, 9, 17, 18, 19, 25, 27, 30, 31, 33, 35, 36, 38, 40, 42, 43, 60, 63, 64, 66, 69, 72, 79, 81, 82, 84, 91, 92, 96, 98, 100, 101, 111, 124, 133, 136, 140, 147, 149, 152, 155, 156, 172], "z1": [7, 16, 38, 69, 72, 105, 106, 109, 124, 125, 126, 127, 129], "z2": [7, 16, 69, 72, 105, 106, 109, 124, 125, 126, 127, 129], "z3": [7, 16, 69, 72, 105, 106, 109, 124, 125, 126, 127, 129], "z4": [7, 16, 69, 72, 105, 106, 109, 124, 125, 126, 127, 129], "z_": [43, 63, 91], "z_0": [140, 151], "z_1": [31, 35, 40, 69, 72], "z_2": [31, 35, 40], "z_3": [31, 35, 40], "z_4": [31, 35, 40, 69, 72], "z_5": 31, "z_col": [4, 5, 6, 7, 8, 9, 25, 27, 38, 60, 63, 64, 66, 79, 91, 92, 93, 98, 101, 104, 105, 109, 111, 124, 172], "z_i": [36, 42, 66, 96, 101, 124], "z_j": [17, 18, 19, 31, 35, 40], "z_true": 96, "zadik": 171, "zaxis_titl": [81, 82, 84], "zero": [14, 18, 47, 67, 68, 69, 72, 75, 83, 88, 95, 96, 99, 100, 124, 140, 142, 144, 156, 157, 160, 162], "zeros_lik": 96, "zeta": [25, 38, 39, 64, 92, 111, 124, 133, 136, 137, 170], "zeta_": [43, 63, 91], "zeta_0": [43, 63, 91], "zeta_i": [34, 42, 44, 61, 80, 103], "zeta_j": 156, "zhang": [41, 171], "zhao": [12, 14, 15, 18, 31, 35, 40, 62, 67, 69, 72, 124, 126, 130, 171], "zhou": [41, 171], "zimmert": [67, 124, 130, 171], "zip": [81, 82], "zorder": [76, 77, 78], "\u03c4_x0": 94, "\u03c4_x1": 94, "\u2139": 61}, "titles": ["API Reference", "<span class=\"section-number\">1. </span>DoubleML Data Class", "<span class=\"section-number\">3. </span>Datasets", "<span class=\"section-number\">2. </span>DoubleML Models", "<span class=\"section-number\">1.2. </span>doubleml.data.DoubleMLClusterData", "<span class=\"section-number\">1.6. </span>doubleml.data.DoubleMLDIDData", "<span class=\"section-number\">1.1. </span>doubleml.data.DoubleMLData", "<span class=\"section-number\">1.3. </span>doubleml.data.DoubleMLPanelData", "<span class=\"section-number\">1.5. </span>doubleml.data.DoubleMLRDDData", "<span class=\"section-number\">1.4. </span>doubleml.data.DoubleMLSSMData", "<span class=\"section-number\">3.1.1. </span>doubleml.datasets.fetch_401K", "<span class=\"section-number\">3.1.2. </span>doubleml.datasets.fetch_bonus", "<span class=\"section-number\">2.3.4. </span>doubleml.did.DoubleMLDID", "<span class=\"section-number\">2.3.2. </span>doubleml.did.DoubleMLDIDAggregation", "<span class=\"section-number\">2.3.3. </span>doubleml.did.DoubleMLDIDBinary", "<span class=\"section-number\">2.3.5. </span>doubleml.did.DoubleMLDIDCS", "<span class=\"section-number\">2.3.1. </span>doubleml.did.DoubleMLDIDMulti", "<span class=\"section-number\">3.2.14. </span>doubleml.did.datasets.make_did_CS2021", "<span class=\"section-number\">3.2.13. </span>doubleml.did.datasets.make_did_SZ2020", "<span class=\"section-number\">3.2.15. </span>doubleml.did.datasets.make_did_cs_CS2021", "<span class=\"section-number\">5.1. </span>doubleml.double_ml_score_mixins.LinearScoreMixin", "<span class=\"section-number\">5.2. </span>doubleml.double_ml_score_mixins.NonLinearScoreMixin", "<span class=\"section-number\">2.2.2. </span>doubleml.irm.DoubleMLAPO", "<span class=\"section-number\">2.2.3. </span>doubleml.irm.DoubleMLAPOS", "<span class=\"section-number\">2.2.7. </span>doubleml.irm.DoubleMLCVAR", "<span class=\"section-number\">2.2.4. </span>doubleml.irm.DoubleMLIIVM", "<span class=\"section-number\">2.2.1. </span>doubleml.irm.DoubleMLIRM", "<span class=\"section-number\">2.2.6. </span>doubleml.irm.DoubleMLLPQ", "<span class=\"section-number\">2.2.5. </span>doubleml.irm.DoubleMLPQ", "<span class=\"section-number\">2.2.8. </span>doubleml.irm.DoubleMLQTE", "<span class=\"section-number\">2.2.9. </span>doubleml.irm.DoubleMLSSM", "<span class=\"section-number\">3.2.5. </span>doubleml.irm.datasets.make_confounded_irm_data", "<span class=\"section-number\">3.2.3. </span>doubleml.irm.datasets.make_heterogeneous_data", "<span class=\"section-number\">3.2.2. </span>doubleml.irm.datasets.make_iivm_data", "<span class=\"section-number\">3.2.1. </span>doubleml.irm.datasets.make_irm_data", "<span class=\"section-number\">3.2.4. </span>doubleml.irm.datasets.make_irm_data_discrete_treatments", "<span class=\"section-number\">3.2.6. </span>doubleml.irm.datasets.make_ssm_data", "<span class=\"section-number\">2.1.2. </span>doubleml.plm.DoubleMLLPLR", "<span class=\"section-number\">2.1.3. </span>doubleml.plm.DoubleMLPLIV", "<span class=\"section-number\">2.1.1. </span>doubleml.plm.DoubleMLPLR", "<span class=\"section-number\">3.2.12. </span>doubleml.plm.datasets.make_confounded_plr_data", "<span class=\"section-number\">3.2.9. </span>doubleml.plm.datasets.make_lplr_LZZ2020", "<span class=\"section-number\">3.2.10. </span>doubleml.plm.datasets.make_pliv_CHS2015", "<span class=\"section-number\">3.2.11. </span>doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021", "<span class=\"section-number\">3.2.7. </span>doubleml.plm.datasets.make_plr_CCDDHNR2018", "<span class=\"section-number\">3.2.8. </span>doubleml.plm.datasets.make_plr_turrell2018", "<span class=\"section-number\">2.4.1. </span>doubleml.rdd.RDFlex", "<span class=\"section-number\">3.2.16. </span>doubleml.rdd.datasets.make_simple_rdd_data", "<span class=\"section-number\">4.1.2. </span>doubleml.utils.DMLDummyClassifier", "<span class=\"section-number\">4.1.1. </span>doubleml.utils.DMLDummyRegressor", "<span class=\"section-number\">4.1.3. </span>doubleml.utils.DMLOptunaResult", "<span class=\"section-number\">4.1.4. </span>doubleml.utils.DoubleMLBLP", "<span class=\"section-number\">4.1.5. </span>doubleml.utils.DoubleMLPolicyTree", "<span class=\"section-number\">4.1.7. </span>doubleml.utils.GlobalClassifier", "<span class=\"section-number\">4.1.6. </span>doubleml.utils.GlobalRegressor", "<span class=\"section-number\">4.1.9. </span>doubleml.utils.PSProcessor", "<span class=\"section-number\">4.1.8. </span>doubleml.utils.PSProcessorConfig", "<span class=\"section-number\">4.2.1. </span>doubleml.utils.gain_statistics", "<span class=\"section-number\">5. </span>Score Mixin Classes for DoubleML Models", "<span class=\"section-number\">4. </span>Utility Classes and Functions", "R: Basic Instrumental Variables Calculation", "R: Basics of Double Machine Learning", "R: DoubleML for Difference-in-Differences", "R: Cluster Robust Double Machine Learning", "R: Impact of 401(k) on Financial Wealth", "R: Ensemble Learners and More with <code class=\"docutils literal notranslate\"><span class=\"pre\">mlr3pipelines</span></code>", "R: Sample Selection Models", "Python: Difference-in-Differences", "Python: Difference-in-Differences Pre-Testing", "Python: Panel Data with Multiple Time Periods", "Python: Real-Data Example for Multi-Period Difference-in-Differences", "Python: Panel Data Introduction", "Python: Repeated Cross-Sectional Data with Multiple Time Periods", "DML: Bonus Data", "Examples", "Python: Choice of learners", "Python: Hyperparametertuning with Optuna", "Python: Causal Machine Learning with TabPFN", "Python: Average Potential Outcome (APO) Models", "Python: Basic Instrumental Variables calculation", "Python: Basics of Double Machine Learning", "Python: Conditional Average Treatment Effects (CATEs) for IRM models", "Python: Conditional Average Treatment Effects (CATEs) for PLR models", "Python: Conditional Value at Risk of potential outcomes", "Python: First Stage and Causal Estimation", "Python: Group Average Treatment Effects (GATEs) for IRM models", "Python: Group Average Treatment Effects (GATEs) for PLR models", "Python: GATE Sensitivity Analysis", "Python: IRM and APO Model Comparison", "Python: Log-Odds Effects for Logistic PLR models", "DoubleML meets FLAML - How to tune learners automatically within <code class=\"docutils literal notranslate\"><span class=\"pre\">DoubleML</span></code>", "Python: Cluster Robust Double Machine Learning", "Python: Impact of 401(k) on Financial Wealth", "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)", "Python: PLM and IRM for Multiple Treatments", "Python: Policy Learning with Trees", "Python: Potential Quantiles and Quantile Treatment Effects", "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)", "Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments", "Python: Sensitivity Analysis", "Python: Sensitivity Analysis for Causal ML", "Python: Sample Selection Models", "<span class=\"section-number\">6. </span>Double machine learning algorithms", "<span class=\"section-number\">1. </span>The basics of double/debiased machine learning", "DoubleMLData from dataframes", "Key arguments", "Key arguments", "Key arguments", "Key arguments", "<span class=\"section-number\">2. </span>Data Backend", "User Guide", "<span class=\"section-number\">4. </span>Heterogeneous treatment effects", "<span class=\"section-number\">7. </span>Learners, hyperparameters and hyperparameter tuning", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "References", "&lt;no title&gt;", "<span class=\"section-number\">3. </span>Models", "&lt;no title&gt;", "Panel Data", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">9. </span>Sample-splitting, cross-fitting and repeated cross-fitting", "<span class=\"section-number\">5. </span>Score functions", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">8. </span>Variance estimation and confidence intervals", "<span class=\"section-number\">10. </span>Sensitivity analysis", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "DoubleML", "Installing DoubleML", "Getting Started", "Double Machine Learning Literature", "Release Notes", "DoubleML Workflow"], "titleterms": {"": 90, "0": 173, "1": [90, 100, 173], "2": [90, 100, 173], "2011": 100, "2023": 100, "3": [90, 100, 173], "4": [100, 173], "401": [64, 92, 93, 99], "5": [100, 173], "6": 173, "7": 173, "8": 173, "95": 90, "A": [63, 91], "ATE": [66, 87, 94, 101], "No": [63, 91], "One": [63, 81, 82, 91], "That": 98, "The": [64, 92, 94, 103, 170], "acknowledg": [62, 168], "acycl": [60, 79], "addit": 94, "adjust": [69, 72, 97], "advanc": [97, 112, 156], "aggreg": [69, 70, 71, 72, 124], "al": 100, "algorithm": [102, 157, 168, 170], "all": [69, 72], "altern": 140, "analysi": [69, 72, 76, 78, 87, 88, 99, 100, 157, 173], "anticip": [69, 72], "api": [0, 90], "apo": [78, 88, 124, 140, 157], "applic": [63, 91, 99], "approach": [61, 75, 80, 103], "ar": 98, "arah": 100, "arbitrari": 94, "archiv": 74, "argument": [105, 106, 107, 108, 109], "arrai": [104, 109], "asset": [64, 92], "assumpt": 100, "att": [67, 69, 70, 71, 72], "augment": 94, "automat": 90, "automl": 90, "averag": [64, 77, 78, 81, 82, 85, 86, 88, 92, 111, 124, 140, 157], "backend": [63, 64, 91, 92, 109, 170, 173], "band": 156, "base": [65, 69, 72], "basic": [60, 61, 69, 72, 76, 79, 80, 103], "benchmark": [99, 100, 157], "bia": [61, 80, 103], "binari": [124, 140], "bonu": 73, "bootstrap": 156, "build": 169, "calcul": [60, 79], "call": 90, "callabl": 140, "case": 74, "cate": [81, 82, 94, 111], "causal": [70, 73, 77, 78, 84, 100, 140, 170, 173], "chernozhukov": 100, "choic": 75, "citat": 168, "class": [1, 58, 59, 63, 91], "cluster": [63, 91], "code": 168, "coeffici": 90, "combin": [69, 72, 84], "compar": [75, 90], "comparison": [62, 76, 77, 88, 90], "comput": [75, 90], "conclus": [90, 100], "conda": 169, "condit": [70, 81, 82, 83, 93, 111, 140], "confid": [90, 98, 156], "construct": 112, "contrast": 78, "control": [69, 72], "covari": [69, 72, 97], "coverag": [67, 84], "cran": 169, "creat": [77, 90], "cross": [63, 67, 72, 91, 124, 126, 139, 140, 157, 170], "custom": [75, 90], "cvar": [83, 93, 111, 140], "dag": [60, 79], "data": [1, 4, 5, 6, 7, 8, 9, 60, 61, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 99, 100, 101, 103, 109, 124, 126, 140, 157, 170, 173], "datafram": [104, 109], "dataset": [2, 10, 11, 17, 18, 19, 31, 32, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 47, 73], "debias": [61, 80, 103, 170], "default": 90, "defin": [63, 76, 91], "demo": 62, "depend": 169, "descript": [69, 72], "design": [97, 124], "detail": [62, 69, 70, 71, 72, 76, 124], "develop": 169, "dgp": [61, 76, 77, 78, 80], "did": [3, 12, 13, 14, 15, 16, 17, 18, 19, 62, 124], "differ": [62, 67, 68, 70, 74, 75, 124, 140, 156, 157], "dimension": [81, 82], "direct": [60, 79], "disclaim": 100, "discontinu": [97, 124], "distribut": [66, 101], "dml": [63, 73, 91, 139, 170, 173], "dml1": 102, "dml2": 102, "dmldummyclassifi": 48, "dmldummyregressor": 49, "dmloptunaresult": 50, "doubl": [61, 63, 80, 91, 102, 103, 168, 170, 171], "double_ml_score_mixin": [20, 21], "doubleml": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 62, 64, 65, 77, 79, 90, 92, 99, 100, 156, 168, 169, 173], "doublemlapo": [22, 23, 76], "doublemlblp": 51, "doublemlclusterdata": [4, 63], "doublemlcvar": 24, "doublemldata": [6, 64, 77, 91, 92, 104, 109, 170], "doublemldid": 12, "doublemldidaggreg": 13, "doublemldidbinari": 14, "doublemldidc": 15, "doublemldiddata": [5, 109], "doublemldidmulti": 16, "doublemliivm": 25, "doublemlirm": 26, "doublemllplr": 37, "doublemllpq": 27, "doublemlpaneldata": [7, 69, 72, 109], "doublemlpliv": [38, 63, 91], "doublemlplr": 39, "doublemlpolicytre": 52, "doublemlpq": 28, "doublemlqt": 29, "doublemlrdddata": [8, 109], "doublemlssm": 30, "doublemlssmdata": [9, 109], "effect": [64, 69, 70, 71, 72, 74, 77, 81, 82, 83, 85, 86, 88, 89, 92, 93, 94, 96, 99, 100, 111, 124], "elig": [64, 92], "empir": 84, "ensembl": [65, 97], "error": [63, 91], "estim": [60, 64, 66, 67, 69, 70, 71, 72, 73, 77, 79, 84, 87, 90, 92, 93, 94, 96, 99, 100, 101, 139, 140, 156, 170, 173], "et": 100, "evalu": [75, 77, 90, 112], "event": [69, 70, 71, 72], "exampl": [62, 63, 70, 74, 76, 81, 82, 91, 99, 105, 106, 107, 108, 109], "exploit": [62, 65], "extern": [112, 139], "featur": [65, 168], "fetch_401k": 10, "fetch_bonu": 11, "figur": 94, "file": 169, "final": 62, "financi": [64, 92, 93], "first": 84, "fit": [63, 90, 91, 139, 170], "flaml": 90, "flexibl": 97, "fold": [90, 139], "forest": 73, "formul": [100, 173], "from": [62, 65, 104, 109, 169], "full": 90, "function": [59, 62, 63, 91, 140, 170], "fuzzi": [97, 124], "gain_statist": 57, "gate": [85, 86, 87, 111], "gatet": 87, "gener": [2, 61, 74, 76, 77, 78, 80, 90, 97, 103, 157], "get": 170, "github": 169, "global": 97, "globalclassifi": 53, "globalregressor": 54, "graph": [60, 79], "grid": 112, "group": [69, 71, 72, 85, 86, 111], "guid": [76, 110], "helper": [63, 91], "heterogen": [74, 88, 94, 111], "how": [65, 90], "hyperparamet": [76, 88, 112, 173], "hyperparametertun": 76, "identif": 100, "iivm": [64, 92, 124, 140], "impact": [64, 92, 93], "implement": [102, 124, 140, 157], "import": 77, "induc": [61, 80, 103], "infer": [156, 173], "initi": [63, 90, 91], "insight": 77, "instal": 169, "instrument": [60, 79, 98], "integr": 62, "interact": [64, 85, 92, 95, 124, 140, 157], "interv": [90, 98, 156], "introduct": 71, "invers": 94, "irm": [3, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 64, 73, 81, 85, 88, 92, 94, 95, 99, 111, 124, 140, 157], "iv": [60, 64, 79, 92, 124, 140], "k": [64, 92, 93, 99, 139], "kei": [77, 105, 106, 107, 108, 109], "lambda": 84, "lasso": [73, 84], "latest": 169, "lear": [63, 91], "learn": [61, 63, 77, 80, 91, 95, 102, 103, 111, 168, 170, 171], "learner": [65, 73, 75, 76, 88, 90, 97, 112, 170], "less": 90, "level": 124, "linear": [64, 69, 72, 86, 92, 94, 97, 124, 140, 157], "linearscoremixin": 20, "literatur": 171, "load": [63, 73, 91, 100], "loader": 2, "local": [64, 92, 93, 96, 97, 140], "log": 89, "logist": [89, 124, 140], "loss": 84, "lplr": [124, 140], "lpq": [96, 140], "lqte": [93, 96], "m": 139, "machin": [61, 63, 77, 80, 91, 102, 103, 168, 170, 171], "main": 168, "mainten": 168, "make_confounded_irm_data": 31, "make_confounded_plr_data": 40, "make_did_cs2021": 17, "make_did_cs_cs2021": 19, "make_did_sz2020": 18, "make_heterogeneous_data": 32, "make_iivm_data": 33, "make_irm_data": 34, "make_irm_data_discrete_treat": 35, "make_lplr_lzz2020": 41, "make_pliv_chs2015": 42, "make_pliv_multiway_cluster_ckms2021": 43, "make_plr_ccddhnr2018": 44, "make_plr_turrell2018": 45, "make_simple_rdd_data": 47, "make_ssm_data": 36, "mar": [66, 101], "market": [63, 91], "matric": [104, 109], "meet": 90, "method": [77, 90, 173], "metric": [75, 90], "minimum": 112, "miss": [66, 101], "missing": [124, 140], "mixin": 58, "ml": [61, 62, 80, 100, 103, 173], "mlr3": 65, "mlr3extralearn": 65, "mlr3learner": 65, "mlr3pipelin": 65, "model": [3, 58, 64, 66, 73, 76, 77, 78, 81, 82, 85, 86, 88, 89, 90, 92, 94, 95, 98, 100, 101, 111, 124, 139, 140, 156, 157, 170, 173], "modul": 73, "more": 65, "motiv": [63, 91], "multi": 70, "multipl": [69, 72, 78, 94, 124], "multipli": 156, "naiv": [60, 79], "net": [64, 92], "neyman": [140, 170], "nonignor": [66, 101, 124, 140], "nonlinearscoremixin": 21, "nonrespons": [66, 101, 124, 140], "note": 172, "nuisanc": [76, 90, 170], "object": [63, 77, 91, 99], "odd": 89, "option": 169, "optuna": 76, "orthogon": [61, 80, 103, 140, 170], "out": [61, 80, 103], "outcom": [66, 67, 77, 78, 83, 101, 111, 124, 140, 157], "over": 156, "overcom": [61, 80, 103], "overfit": [61, 80, 103], "overlap": 94, "packag": [62, 64, 92, 169], "panel": [67, 69, 71, 124, 126, 140, 157], "parallel": 70, "paramet": [65, 73, 90, 124, 140], "partial": [61, 64, 80, 86, 92, 94, 103, 124, 140, 157], "particip": [64, 92], "partit": 139, "penalti": 84, "perform": [62, 77, 94], "period": [69, 70, 72, 124, 140, 157], "pip": 169, "pipelin": [76, 112], "pliv": [124, 140], "plm": [3, 37, 38, 39, 40, 41, 42, 43, 44, 45, 94, 124, 140, 157], "plot": [63, 90, 91], "plr": [64, 73, 82, 86, 89, 92, 111, 124, 140, 157], "polici": [95, 111], "potenti": [77, 78, 83, 93, 96, 111, 124, 140, 157], "pq": [96, 111, 140], "pre": 68, "predict": [62, 112], "preprocess": 65, "problem": 173, "process": [61, 63, 76, 77, 78, 80, 91, 103], "product": [63, 91], "propens": 94, "provid": 139, "psprocessor": 55, "psprocessorconfig": 56, "python": [67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 112, 169], "qte": [96, 111], "qualiti": 84, "quantil": [93, 96, 111, 140], "question": 70, "r": [60, 61, 62, 63, 64, 65, 66, 74, 112, 169], "random": [66, 73, 101, 124, 140], "rank": 94, "rdd": [3, 46, 47, 97, 124], "rdflex": 46, "real": [63, 70, 91], "refer": [0, 60, 62, 63, 65, 75, 79, 84, 90, 91, 94, 98, 100, 103, 112, 122, 139, 156, 168, 170], "regress": [64, 85, 86, 92, 95, 97, 124, 140, 157], "regular": [61, 80, 103], "releas": [169, 172], "remark": 62, "remov": [61, 80, 103], "repeat": [67, 72, 124, 126, 139, 140, 157], "repetit": 139, "requir": 112, "research": 70, "respect": [63, 91], "result": [63, 64, 76, 91, 92, 94], "risk": [83, 93, 111, 140], "robust": [63, 91, 98], "run": 98, "sampl": [61, 66, 80, 90, 101, 103, 124, 139, 140], "sandbox": 74, "score": [58, 61, 80, 94, 103, 140, 170], "search": 112, "section": [67, 72, 124, 126, 140, 157], "select": [66, 69, 72, 101, 124, 140], "sensit": [69, 72, 78, 87, 88, 99, 100, 157, 173], "set": [65, 112], "setup": 77, "sharp": [97, 124], "simpl": [61, 80, 103], "simul": [60, 63, 67, 79, 91, 98, 99], "simultan": 156, "singl": 78, "small": 98, "sourc": [168, 169], "special": 109, "specif": [157, 173], "specifi": [73, 112, 140], "split": [61, 80, 103, 139], "ssm": 124, "stack": 97, "stage": 84, "standard": [63, 75, 91], "start": 170, "step": 90, "structur": 77, "studi": [69, 70, 71, 72, 74], "summari": [64, 77, 90, 92, 94], "tabpfn": 77, "takeawai": 77, "test": 68, "theori": 157, "time": [69, 71, 72, 75, 90], "train": 90, "treat": 88, "treatment": [64, 77, 81, 82, 83, 85, 86, 88, 92, 93, 94, 96, 111, 124, 140, 157], "tree": [95, 111], "trend": 70, "tune": [65, 76, 90, 112, 173], "two": [63, 81, 82, 91, 124, 140, 157], "type": 109, "uncondit": 70, "under": [66, 94, 101], "univers": [69, 72], "untun": [76, 90], "up": 65, "us": [60, 62, 65, 73, 79, 90, 112], "usag": [105, 106, 107, 108, 109], "user": 110, "util": [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59], "v": 84, "valid": 156, "valu": [83, 93, 111, 140], "vanderweel": 100, "variabl": [60, 79, 98], "varianc": 156, "version": 169, "via": 140, "visual": [77, 89], "wai": [63, 91], "weak": 98, "wealth": [64, 92, 93], "weight": [94, 111], "when": 90, "whl": 169, "within": 90, "without": [97, 139], "workflow": 173, "xgboost": 90, "zero": [63, 91]}})