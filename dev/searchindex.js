Search.setIndex({"alltitles": {"(One-Way) Cluster Robust Double Machine Learing": [[54, "(One-Way)-Cluster-Robust-Double-Machine-Learing"], [77, "(One-Way)-Cluster-Robust-Double-Machine-Learing"]], "0. Problem Formulation": [[142, "problem-formulation"]], "1. Data Backend": [[142, "data-backend"]], "1. Formulation of Causal Model & Identification Assumptions": [[86, "1.-Formulation-of-Causal-Model-&-Identification-Assumptions"]], "2. Causal Model": [[142, "causal-model"]], "2. Estimation of Causal Effect": [[86, "2.-Estimation-of-Causal-Effect"]], "3. ML Methods": [[142, "ml-methods"]], "3. Sensitivity Analysis": [[86, "3.-Sensitivity-Analysis"]], "4. Benchmarking Analysis": [[86, "4.-Benchmarking-Analysis"]], "4. DML Specifications": [[142, "dml-specifications"]], "5. Conclusion": [[86, "5.-Conclusion"]], "5. Estimation": [[142, "estimation"]], "6. Inference": [[142, "inference"]], "7. Sensitivity Analysis": [[142, "sensitivity-analysis"]], "A Motivating Example: Two-Way Cluster Robust DML": [[54, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"], [77, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"]], "API Reference": [[0, null]], "ATE Estimation and Sensitivity": [[73, "ATE-Estimation-and-Sensitivity"]], "ATE estimates distribution": [[57, "ATE-estimates-distribution"], [57, "id3"], [87, "ATE-estimates-distribution"], [87, "id3"]], "ATT Estimation": [[60, "ATT-Estimation"], [60, "id1"], [61, "ATT-Estimation"]], "ATTE Estimation": [[58, "ATTE-Estimation"], [58, "id2"]], "Acknowledgements": [[137, "acknowledgements"]], "Acknowledgements and Final Remarks": [[53, "Acknowledgements-and-Final-Remarks"]], "Additional Results: CATE estimates": [[80, "Additional-Results:-CATE-estimates"]], "Advanced: External Predictions": [[95, "advanced-external-predictions"]], "Advanced: Global and Local Learners, Stacked Ensembles": [[83, "Advanced:-Global-and-Local-Learners,-Stacked-Ensembles"]], "Aggregated Effects": [[60, "Aggregated-Effects"]], "Aggregation Details": [[60, "Aggregation-Details"], [61, "Aggregation-Details"]], "Algorithm DML1": [[88, "algorithm-dml1"]], "Algorithm DML2": [[88, "algorithm-dml2"]], "All combinations": [[60, "All-combinations"]], "Anticipation": [[60, "Anticipation"]], "Application Results": [[54, "Application-Results"], [77, "Application-Results"]], "Application: 401(k)": [[85, "Application:-401(k)"]], "AutoML with less Computation time": [[76, "AutoML-with-less-Computation-time"]], "Average Potential Outcome (APOs)": [[64, "Average-Potential-Outcome-(APOs)"]], "Average Potential Outcomes (APOs)": [[96, "average-potential-outcomes-apos"], [110, "average-potential-outcomes-apos"], [126, "average-potential-outcomes-apos"]], "Average Potential Outcomes (APOs) for Multiple Treatment Levels": [[96, "average-potential-outcomes-apos-for-multiple-treatment-levels"]], "Average Treatment Effect": [[74, "Average-Treatment-Effect"]], "Average Treatment Effect on the Treated": [[74, "Average-Treatment-Effect-on-the-Treated"]], "Basics": [[60, "Basics"]], "Benchmarking": [[126, "benchmarking"]], "Benchmarking Analysis": [[85, "Benchmarking-Analysis"]], "Binary Interactive Regression Model (IRM)": [[96, "binary-interactive-regression-model-irm"], [110, "binary-interactive-regression-model-irm"]], "CATEs for IRM models": [[94, "cates-for-irm-models"]], "CATEs for PLR models": [[94, "cates-for-plr-models"]], "CVaR Treatment Effects": [[69, "CVaR-Treatment-Effects"]], "CVaR of potential outcomes": [[94, "cvar-of-potential-outcomes"]], "CVaR treatment effects": [[94, "cvar-treatment-effects"]], "Causal Analysis with DoubleML": [[86, "Causal-Analysis-with-DoubleML"]], "Causal Contrasts": [[64, "Causal-Contrasts"]], "Causal estimation vs. lasso penalty \\lambda": [[70, "Causal-estimation-vs.-lasso-penalty-\\lambda"]], "Chernozhukov et al. (2023): Benchmarking": [[86, "Chernozhukov-et-al.-(2023):-Benchmarking"]], "Citation": [[137, "citation"]], "Cluster Robust Cross Fitting": [[54, "Cluster-Robust-Cross-Fitting"], [77, "Cluster-Robust-Cross-Fitting"]], "Cluster Robust Standard Errors": [[54, "Cluster-Robust-Standard-Errors"], [77, "Cluster-Robust-Standard-Errors"]], "Clustering and double machine learning": [[54, "Clustering-and-double-machine-learning"], [77, "Clustering-and-double-machine-learning"]], "Combined loss vs. lasso penalty \\lambda": [[70, "Combined-loss-vs.-lasso-penalty-\\lambda"]], "Compare Metrics for Nuisance Estimation": [[76, "Compare-Metrics-for-Nuisance-Estimation"]], "Comparing different learners": [[75, "Comparing-different-learners"]], "Comparison and summary": [[76, "Comparison-and-summary"]], "Comparison to AutoML with less Computation time and Untuned XGBoost Learners": [[76, "Comparison-to-AutoML-with-less-Computation-time-and-Untuned-XGBoost-Learners"]], "Comparison to did package": [[53, "Comparison-to-did-package"]], "Computation time": [[75, "Computation-time"]], "Conclusion": [[76, "Conclusion"]], "Conditional Value at Risk (CVaR)": [[69, "Conditional-Value-at-Risk-(CVaR)"]], "Conditional average treatment effects (CATEs)": [[94, "conditional-average-treatment-effects-cates"]], "Conditional value at risk (CVaR)": [[94, "conditional-value-at-risk-cvar"], [110, "conditional-value-at-risk-cvar"]], "Confidence bands and multiplier bootstrap for valid simultaneous inference": [[125, "confidence-bands-and-multiplier-bootstrap-for-valid-simultaneous-inference"]], "Control Groups": [[60, "Control-Groups"]], "Coverage Simulation": [[58, "Coverage-Simulation"], [58, "id3"]], "Cross-fitting with K folds": [[109, "cross-fitting-with-k-folds"]], "Cross-fitting, DML algorithms and Neyman-orthogonal score functions": [[139, "cross-fitting-dml-algorithms-and-neyman-orthogonal-score-functions"]], "Custom evaluation metrics": [[75, "Custom-evaluation-metrics"]], "DML: Bonus Data": [[62, null]], "Data": [[55, "Data"], [57, "Data"], [57, "id1"], [58, "Data"], [58, "id1"], [60, "Data"], [61, "Data"], [67, "Data"], [68, "Data"], [69, "Data"], [71, "Data"], [72, "Data"], [73, "Data"], [74, "Data"], [78, "Data"], [79, "Data"], [81, "Data"], [82, "Data"], [82, "id1"], [85, "Data"], [87, "Data"], [87, "id1"], [139, "data"]], "Data Backend": [[92, null]], "Data Description": [[60, "Data-Description"]], "Data Details": [[60, "Data-Details"]], "Data Generating Process (DGP)": [[52, "Data-Generating-Process-(DGP)"], [64, "Data-Generating-Process-(DGP)"], [66, "Data-Generating-Process-(DGP)"]], "Data Generation": [[76, "Data-Generation"]], "Data Simulation": [[51, "Data-Simulation"], [65, "Data-Simulation"]], "Data and Effect Estimation": [[85, "Data-and-Effect-Estimation"]], "Data generating process": [[89, "data-generating-process"]], "Data preprocessing": [[56, "Data-preprocessing"]], "Data with Anticipation": [[60, "Data-with-Anticipation"]], "Data-Backend for Cluster Data": [[54, "Data-Backend-for-Cluster-Data"], [77, "Data-Backend-for-Cluster-Data"]], "Dataset Generators": [[2, "dataset-generators"]], "Dataset Loaders": [[2, "dataset-loaders"]], "Datasets": [[2, null]], "Define Helper Functions for Plotting": [[54, "Define-Helper-Functions-for-Plotting"], [77, "Define-Helper-Functions-for-Plotting"]], "Demo Example from did": [[53, "Demo-Example-from-did"]], "Details on Predictive Performance": [[53, "Details-on-Predictive-Performance"]], "Difference-in-Differences": [[63, "difference-in-differences"]], "Difference-in-Differences Models": [[110, "difference-in-differences-models"], [126, "difference-in-differences-models"]], "Difference-in-Differences Models (DID)": [[96, "difference-in-differences-models-did"]], "Difference-in-Differences for Panel Data": [[126, "difference-in-differences-for-panel-data"]], "Difference-in-Differences for repeated cross-sections": [[126, "difference-in-differences-for-repeated-cross-sections"]], "Disclaimer": [[86, "Disclaimer"]], "Double Machine Learning Algorithm": [[137, "double-machine-learning-algorithm"]], "Double Machine Learning Literature": [[140, null]], "Double machine learning algorithms": [[88, null]], "Double/debiased machine learning": [[52, "Double/debiased-machine-learning"], [66, "Double/debiased-machine-learning"], [89, "double-debiased-machine-learning"]], "DoubleML": [[137, null]], "DoubleML Data Class": [[1, null]], "DoubleML Models": [[3, null]], "DoubleML Object": [[85, "DoubleML-Object"]], "DoubleML Workflow": [[142, null]], "DoubleML meets FLAML - How to tune learners automatically within DoubleML": [[76, null]], "DoubleMLData": [[92, "doublemldata"]], "DoubleMLData from arrays and matrices": [[90, "doublemldata-from-arrays-and-matrices"], [92, "doublemldata-from-arrays-and-matrices"]], "DoubleMLData from dataframes": [[90, null], [92, "doublemldata-from-dataframes"]], "DoubleMLPanelData": [[60, "DoubleMLPanelData"], [92, "doublemlpaneldata"]], "Effect Aggregation": [[61, "Effect-Aggregation"], [96, "effect-aggregation"]], "Effect Heterogeneity": [[63, "effect-heterogeneity"], [74, "Effect-Heterogeneity"]], "Empirical coverage vs. lasso penalty \\lambda": [[70, "Empirical-coverage-vs.-lasso-penalty-\\lambda"]], "Estimate DML models without sample-splitting": [[109, "estimate-dml-models-without-sample-splitting"]], "Estimate double/debiased machine learning models": [[139, "estimate-double-debiased-machine-learning-models"]], "Estimating Potential Quantiles and Quantile Treatment Effects": [[79, "Estimating-Potential-Quantiles-and-Quantile-Treatment-Effects"]], "Estimating local quantile treatment effects (LQTEs)": [[79, "Estimating-local-quantile-treatment-effects-(LQTEs)"]], "Estimating the Average Treatment Effect of 401(k) Eligibility on Net Financial Assets": [[55, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"], [78, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"]], "Estimating the treatment effect on the Conditional Value a Risk (CVaR)": [[79, "Estimating-the-treatment-effect-on-the-Conditional-Value-a-Risk-(CVaR)"]], "Estimation": [[57, "Estimation"], [57, "id2"], [87, "Estimation"], [87, "id2"]], "Estimation quality vs. \\lambda": [[70, "Estimation-quality-vs.-\\lambda"]], "Evaluate learners": [[95, "evaluate-learners"]], "Event Study Aggregation": [[60, "Event-Study-Aggregation"], [61, "Event-Study-Aggregation"]], "Example: Sensitivity Analysis for Causal ML": [[86, null]], "Examples": [[63, null]], "Exploiting the Functionalities of did": [[53, "Exploiting-the-Functionalities-of-did"]], "Externally provide a sample splitting / partition": [[109, "externally-provide-a-sample-splitting-partition"]], "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)": [[83, null]], "Fuzzy RDD": [[83, "Fuzzy-RDD"]], "Fuzzy RDD Without Adjustment": [[83, "Fuzzy-RDD-Without-Adjustment"]], "Fuzzy RDD with Flexible Adjustment": [[83, "Fuzzy-RDD-with-Flexible-Adjustment"]], "Fuzzy RDD with Linear Adjustment": [[83, "Fuzzy-RDD-with-Linear-Adjustment"]], "Fuzzy Regression Discontinuity Design": [[96, "fuzzy-regression-discontinuity-design"]], "GATE Estimation and Sensitivity": [[73, "GATE-Estimation-and-Sensitivity"]], "GATET Estimation and Sensitivity": [[73, "GATET-Estimation-and-Sensitivity"]], "GATEs for IRM models": [[94, "gates-for-irm-models"]], "GATEs for PLR models": [[94, "gates-for-plr-models"]], "General Examples": [[63, "general-examples"]], "General algorithm": [[126, "general-algorithm"]], "Generate Fuzzy Data": [[83, "Generate-Fuzzy-Data"]], "Generate Sharp Data": [[83, "Generate-Sharp-Data"]], "Getting Started": [[139, null]], "Group Aggregation": [[60, "Group-Aggregation"], [61, "Group-Aggregation"]], "Group Average Treatment Effects (GATEs)": [[71, "Group-Average-Treatment-Effects-(GATEs)"], [72, "Group-Average-Treatment-Effects-(GATEs)"]], "Group average treatment effects (GATEs)": [[94, "group-average-treatment-effects-gates"]], "Group-Time Combinations": [[60, "Group-Time-Combinations"]], "Heterogeneous treatment effects": [[94, null]], "How to exploit more features of mlr3pipelines in DoubleML": [[56, "How-to-exploit-more-features-of-mlr3pipelines-in-DoubleML"]], "Hyperparameter tuning": [[95, "hyperparameter-tuning"], [95, "id16"]], "Hyperparameter tuning with pipelines": [[95, "hyperparameter-tuning-with-pipelines"]], "Implementation": [[126, "implementation"]], "Implementation Details": [[96, "implementation-details"]], "Implementation of the double machine learning algorithms": [[88, "implementation-of-the-double-machine-learning-algorithms"]], "Implementation of the score function and the estimate of the causal parameter": [[110, "implementation-of-the-score-function-and-the-estimate-of-the-causal-parameter"]], "Implemented Neyman orthogonal score functions": [[110, "implemented-neyman-orthogonal-score-functions"]], "Initialize DoubleMLClusterData object": [[54, "Initialize-DoubleMLClusterData-object"], [77, "Initialize-DoubleMLClusterData-object"]], "Initialize the objects of class DoubleMLPLIV": [[54, "Initialize-the-objects-of-class-DoubleMLPLIV"], [77, "Initialize-the-objects-of-class-DoubleMLPLIV"]], "Installing DoubleML": [[138, null]], "Instrumental Variables Directed Acyclic Graph (IV - DAG)": [[51, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"], [65, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"]], "Interactive IV Model (IIVM)": [[55, "Interactive-IV-Model-(IIVM)"], [78, "Interactive-IV-Model-(IIVM)"]], "Interactive IV model (IIVM)": [[96, "interactive-iv-model-iivm"], [110, "interactive-iv-model-iivm"]], "Interactive Regression Model (IRM)": [[55, "Interactive-Regression-Model-(IRM)"], [71, "Interactive-Regression-Model-(IRM)"], [78, "Interactive-Regression-Model-(IRM)"], [81, "Interactive-Regression-Model-(IRM)"]], "Interactive regression model (IRM)": [[126, "interactive-regression-model-irm"]], "Interactive regression models (IRM)": [[96, "interactive-regression-models-irm"], [110, "interactive-regression-models-irm"], [126, "interactive-regression-models-irm"]], "Learners and Hyperparameters": [[74, "Learners-and-Hyperparameters"]], "Learners to estimate the nuisance models": [[139, "learners-to-estimate-the-nuisance-models"]], "Learners, hyperparameters and hyperparameter tuning": [[95, null]], "Linear Covariate Adjustment": [[60, "Linear-Covariate-Adjustment"]], "Load Data": [[86, "Load-Data"]], "Load and Process Data": [[54, "Load-and-Process-Data"], [77, "Load-and-Process-Data"]], "Load bonus data using the dml datasets module": [[62, "Load-bonus-data-using-the-dml-datasets-module"]], "Local Average Treatment Effects of 401(k) Participation on Net Financial Assets": [[55, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"], [78, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"]], "Local Potential Quantile Estimation": [[82, "Local-Potential-Quantile-Estimation"]], "Local Potential Quantiles (LPQs)": [[82, "Local-Potential-Quantiles-(LPQs)"]], "Local Quantile Treatment Effects (LQTEs)": [[82, "Local-Quantile-Treatment-Effects-(LQTEs)"]], "Local potential quantiles (LPQs)": [[110, "local-potential-quantiles-lpqs"]], "Main Features": [[137, "main-features"]], "Minimum requirements for learners": [[95, "minimum-requirements-for-learners"], [95, "id2"]], "Missingness at Random": [[96, "missingness-at-random"], [110, "missingness-at-random"]], "Model-specific implementations": [[126, "model-specific-implementations"]], "Models": [[96, null]], "Motivation": [[54, "Motivation"], [77, "Motivation"]], "Multiple Average Potential Outcome Models (APOS)": [[64, "Multiple-Average-Potential-Outcome-Models-(APOS)"]], "Naive estimation": [[51, "Naive-estimation"], [65, "Naive-estimation"]], "No Clustering / Zero-Way Clustering": [[54, "No-Clustering-/-Zero-Way-Clustering"], [77, "No-Clustering-/-Zero-Way-Clustering"]], "Nonignorable Nonresponse": [[96, "nonignorable-nonresponse"], [110, "nonignorable-nonresponse"]], "One-Way Clustering with Respect to the Market": [[54, "One-Way-Clustering-with-Respect-to-the-Market"], [77, "One-Way-Clustering-with-Respect-to-the-Market"]], "One-Way Clustering with Respect to the Product": [[54, "One-Way-Clustering-with-Respect-to-the-Product"], [77, "One-Way-Clustering-with-Respect-to-the-Product"]], "One-dimensional Example": [[67, "One-dimensional-Example"], [68, "One-dimensional-Example"]], "Outcome missing at random (MAR)": [[57, "Outcome-missing-at-random-(MAR)"], [87, "Outcome-missing-at-random-(MAR)"]], "Outcome missing under nonignorable nonresponse": [[57, "Outcome-missing-under-nonignorable-nonresponse"], [87, "Outcome-missing-under-nonignorable-nonresponse"]], "Overcoming regularization bias by orthogonalization": [[52, "Overcoming-regularization-bias-by-orthogonalization"], [66, "Overcoming-regularization-bias-by-orthogonalization"], [89, "overcoming-regularization-bias-by-orthogonalization"]], "Panel Data": [[96, "id9"], [98, null], [110, "panel-data"], [110, "id3"], [126, "panel-data"]], "Panel Data (Repeated Outcomes)": [[58, "Panel-Data-(Repeated-Outcomes)"]], "Panel data": [[96, "panel-data"]], "Parameter tuning": [[56, "Parameter-tuning"]], "Partialling out score": [[52, "Partialling-out-score"], [66, "Partialling-out-score"], [89, "partialling-out-score"]], "Partially Linear Regression Model (PLR)": [[55, "Partially-Linear-Regression-Model-(PLR)"], [72, "Partially-Linear-Regression-Model-(PLR)"], [78, "Partially-Linear-Regression-Model-(PLR)"]], "Partially linear IV regression model (PLIV)": [[96, "partially-linear-iv-regression-model-pliv"], [110, "partially-linear-iv-regression-model-pliv"]], "Partially linear models (PLM)": [[96, "partially-linear-models-plm"], [110, "partially-linear-models-plm"], [126, "partially-linear-models-plm"]], "Partially linear regression model (PLR)": [[96, "partially-linear-regression-model-plr"], [110, "partially-linear-regression-model-plr"], [126, "partially-linear-regression-model-plr"]], "Plot Coefficients and 95% Confidence Intervals": [[76, "Plot-Coefficients-and-95%-Confidence-Intervals"]], "Policy Learning with Trees": [[81, "Policy-Learning-with-Trees"], [94, "policy-learning-with-trees"]], "Potential Quantile Estimation": [[82, "Potential-Quantile-Estimation"]], "Potential Quantiles (PQs)": [[82, "Potential-Quantiles-(PQs)"]], "Potential quantiles (PQs)": [[94, "potential-quantiles-pqs"], [110, "potential-quantiles-pqs"]], "Python: Average Potential Outcome (APO) Models": [[64, null]], "Python: Basic Instrumental Variables calculation": [[65, null]], "Python: Basics of Double Machine Learning": [[66, null]], "Python: Building the package from source": [[138, "python-building-the-package-from-source"]], "Python: Case studies": [[63, "python-case-studies"]], "Python: Choice of learners": [[75, null]], "Python: Cluster Robust Double Machine Learning": [[77, null]], "Python: Conditional Average Treatment Effects (CATEs) for IRM models": [[67, null]], "Python: Conditional Average Treatment Effects (CATEs) for PLR models": [[68, null]], "Python: Conditional Value at Risk of potential outcomes": [[69, null]], "Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments": [[84, null]], "Python: Difference-in-Differences": [[58, null]], "Python: Difference-in-Differences Pre-Testing": [[59, null]], "Python: First Stage and Causal Estimation": [[70, null]], "Python: GATE Sensitivity Analysis": [[73, null]], "Python: Group Average Treatment Effects (GATEs) for IRM models": [[71, null]], "Python: Group Average Treatment Effects (GATEs) for PLR models": [[72, null]], "Python: IRM and APO Model Comparison": [[74, null]], "Python: Impact of 401(k) on Financial Wealth": [[78, null]], "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)": [[79, null]], "Python: Installing DoubleML": [[138, "python-installing-doubleml"]], "Python: Installing a released version from a .whl file": [[138, "python-installing-a-released-version-from-a-whl-file"]], "Python: Installing the latest release from pip or conda": [[138, "python-installing-the-latest-release-from-pip-or-conda"]], "Python: Learners and hyperparameters": [[95, "python-learners-and-hyperparameters"]], "Python: Optional Dependencies": [[138, "python-optional-dependencies"]], "Python: PLM and IRM for Multiple Treatments": [[80, null]], "Python: Panel Data Introduction": [[61, null]], "Python: Panel Data with Multiple Time Periods": [[60, null]], "Python: Policy Learning with Trees": [[81, null]], "Python: Potential Quantiles and Quantile Treatment Effects": [[82, null]], "Python: Sample Selection Models": [[87, null]], "Python: Sensitivity Analysis": [[85, null]], "Quantile Treatment Effects (QTEs)": [[82, "Quantile-Treatment-Effects-(QTEs)"]], "Quantile treatment effects (QTEs)": [[94, "quantile-treatment-effects-qtes"]], "Quantiles": [[94, "quantiles"]], "R: Basic Instrumental Variables Calculation": [[51, null]], "R: Basics of Double Machine Learning": [[52, null]], "R: Case studies": [[63, "r-case-studies"]], "R: Cluster Robust Double Machine Learning": [[54, null]], "R: DoubleML for Difference-in-Differences": [[53, null]], "R: Ensemble Learners and More with mlr3pipelines": [[56, null]], "R: Impact of 401(k) on Financial Wealth": [[55, null]], "R: Installing DoubleML": [[138, "r-installing-doubleml"]], "R: Installing the development version from GitHub": [[138, "r-installing-the-development-version-from-github"]], "R: Installing the latest release from CRAN": [[138, "r-installing-the-latest-release-from-cran"]], "R: Learners and hyperparameters": [[95, "r-learners-and-hyperparameters"]], "R: Sample Selection Models": [[57, null]], "Ranking Treatment Effects under Treatment Propensity and Treatment Effect Heterogeneity": [[80, "Ranking-Treatment-Effects-under-Treatment-Propensity-and-Treatment-Effect-Heterogeneity"]], "Real-Data Application": [[54, "Real-Data-Application"], [77, "Real-Data-Application"]], "References": [[51, "References"], [53, "References"], [54, "References"], [56, "References"], [65, "References"], [70, "References"], [75, "References"], [76, "References"], [77, "References"], [80, "References"], [84, "References"], [86, "References"], [89, "references"], [95, "references"], [109, "references"], [125, "references"], [137, "references"], [139, "references"]], "Regression Discontinuity Designs (RDD)": [[96, "regression-discontinuity-designs-rdd"]], "Regularization Bias in Simple ML-Approaches": [[52, "Regularization-Bias-in-Simple-ML-Approaches"], [66, "Regularization-Bias-in-Simple-ML-Approaches"]], "Regularization bias in simple ML-approaches": [[89, "regularization-bias-in-simple-ml-approaches"]], "Release Notes": [[141, null]], "Repeated Cross-Sectional Data": [[58, "Repeated-Cross-Sectional-Data"], [110, "repeated-cross-sectional-data"], [110, "id4"], [126, "repeated-cross-sectional-data"]], "Repeated cross-fitting with K folds and M repetitions": [[109, "repeated-cross-fitting-with-k-folds-and-m-repetitions"]], "Repeated cross-sections": [[96, "repeated-cross-sections"], [96, "id10"], [98, "repeated-cross-sections"]], "Running a small simulation": [[84, "Running-a-small-simulation"]], "Sample Selection Models": [[110, "sample-selection-models"]], "Sample Selection Models (SSM)": [[96, "sample-selection-models-ssm"]], "Sample splitting to remove bias induced by overfitting": [[52, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [66, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [89, "sample-splitting-to-remove-bias-induced-by-overfitting"]], "Sample-splitting without cross-fitting": [[109, "sample-splitting-without-cross-fitting"]], "Sample-splitting, cross-fitting and repeated cross-fitting": [[109, null]], "Sandbox": [[63, "sandbox"]], "Score Mixin Classes for DoubleML Models": [[49, null]], "Score functions": [[110, null]], "Selected Combinations": [[60, "Selected-Combinations"]], "Sensitivity Analysis": [[60, "Sensitivity-Analysis"], [64, "Sensitivity-Analysis"], [74, "Sensitivity-Analysis"], [85, "Sensitivity-Analysis"], [85, "id1"]], "Sensitivity Analysis with IRM": [[85, "Sensitivity-Analysis-with-IRM"]], "Sensitivity analysis": [[126, null]], "Set up learners based on mlr3pipelines": [[56, "Set-up-learners-based-on-mlr3pipelines"]], "Sharp RDD": [[83, "Sharp-RDD"]], "Sharp RDD Without Adjustment": [[83, "Sharp-RDD-Without-Adjustment"]], "Sharp RDD with Flexible Adjustment": [[83, "Sharp-RDD-with-Flexible-Adjustment"]], "Sharp RDD with Linear Adjustment": [[83, "Sharp-RDD-with-Linear-Adjustment"]], "Sharp Regression Discontinuity Design": [[96, "sharp-regression-discontinuity-design"]], "Simulate two-way cluster data": [[54, "Simulate-two-way-cluster-data"], [77, "Simulate-two-way-cluster-data"]], "Simulation Example": [[85, "Simulation-Example"]], "Simultaneous inference over different DoubleML models (advanced)": [[125, "simultaneous-inference-over-different-doubleml-models-advanced"]], "Single Average Potential Outcome Models (APO)": [[64, "Single-Average-Potential-Outcome-Models-(APO)"]], "Source code and maintenance": [[137, "source-code-and-maintenance"]], "Special Data Types": [[92, "special-data-types"]], "Specify learner and estimate causal parameter: IRM model with Lasso as learner": [[62, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: IRM model with random forest as learner": [[62, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-random-forest-as-learner"]], "Specify learner and estimate causal parameter: PLR model with Lasso as learner": [[62, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: PLR model with random forest as learner": [[62, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-random-forest-as-learner"]], "Specifying alternative score functions via callables": [[110, "specifying-alternative-score-functions-via-callables"]], "Specifying learners and set hyperparameters": [[95, "specifying-learners-and-set-hyperparameters"], [95, "id9"]], "Standard approach": [[75, "Standard-approach"]], "Step 1: Custom API for FLAML Models within DoubleML": [[76, "Step-1:-Custom-API-for-FLAML-Models-within-DoubleML"]], "Step 1: Initialize and Train the AutoML Models:": [[76, "Step-1:-Initialize-and-Train-the-AutoML-Models:"]], "Step 2: Evaluate the Tuned Models": [[76, "Step-2:-Evaluate-the-Tuned-Models"]], "Step 2: Using the API when calling DoubleML\u2019s .fit() Method": [[76, "Step-2:-Using-the-API-when-calling-DoubleML's-.fit()-Method"]], "Step 3: Create and Fit DoubleML Model": [[76, "Step-3:-Create-and-Fit-DoubleML-Model"]], "Summary Figure": [[80, "Summary-Figure"]], "Summary of Results": [[55, "Summary-of-Results"], [78, "Summary-of-Results"]], "The Augmented Inverse Propensity Weighting Model estimates the ATE under arbitrary effect and propensity score heterogeneity": [[80, "The-Augmented-Inverse-Propensity-Weighting-Model-estimates-the-ATE-under-arbitrary-effect-and-propensity-score-heterogeneity"]], "The Data Backend: DoubleMLData": [[55, "The-Data-Backend:-DoubleMLData"], [78, "The-Data-Backend:-DoubleMLData"]], "The DoubleML package": [[55, "The-DoubleML-package"], [78, "The-DoubleML-package"]], "The Partially Linear Model performs overlap weighting": [[80, "The-Partially-Linear-Model-performs-overlap-weighting"]], "The basics of double/debiased machine learning": [[89, null]], "The causal model": [[139, "the-causal-model"]], "The data-backend DoubleMLData": [[139, "the-data-backend-doublemldata"]], "Theory": [[126, "theory"]], "Time Aggregation": [[60, "Time-Aggregation"], [61, "Time-Aggregation"]], "Tuning on the Folds": [[76, "Tuning-on-the-Folds"]], "Tuning on the full Sample": [[76, "Tuning-on-the-full-Sample"]], "Two treatment periods": [[96, "two-treatment-periods"], [110, "two-treatment-periods"], [126, "two-treatment-periods"]], "Two-Dimensional Example": [[67, "Two-Dimensional-Example"], [68, "Two-Dimensional-Example"]], "Two-Way Clustering with Respect to Product and Market": [[54, "Two-Way-Clustering-with-Respect-to-Product-and-Market"], [77, "Two-Way-Clustering-with-Respect-to-Product-and-Market"]], "Untuned (default parameter) XGBoost": [[76, "Untuned-(default-parameter)-XGBoost"]], "Use ensemble learners based on mlr3pipelines": [[56, "Use-ensemble-learners-based-on-mlr3pipelines"]], "User Guide": [[93, null]], "Using DoubleML": [[51, "Using-DoubleML"], [65, "Using-DoubleML"]], "Using ML for DiD: Integrating DoubleML in did": [[53, "Using-ML-for-DiD:-Integrating-DoubleML-in-did"]], "Using learners from mlr3, mlr3learners and mlr3extralearners": [[56, "Using-learners-from-mlr3,-mlr3learners-and-mlr3extralearners"]], "Using pipelines to construct learners": [[95, "using-pipelines-to-construct-learners"]], "Utility Classes": [[50, "utility-classes"]], "Utility Classes and Functions": [[50, null]], "Utility Functions": [[50, "utility-functions"]], "VanderWeele and Arah (2011): Benchmarking": [[86, "VanderWeele-and-Arah-(2011):-Benchmarking"]], "Variance estimation": [[125, "variance-estimation"]], "Variance estimation and confidence intervals": [[125, null]], "Weighted Average Treatment Effects": [[94, "weighted-average-treatment-effects"]], "doubleml.data.DoubleMLClusterData": [[4, null]], "doubleml.data.DoubleMLData": [[5, null]], "doubleml.data.DoubleMLPanelData": [[6, null]], "doubleml.datasets.fetch_401K": [[7, null]], "doubleml.datasets.fetch_bonus": [[8, null]], "doubleml.datasets.make_confounded_irm_data": [[9, null]], "doubleml.datasets.make_confounded_plr_data": [[10, null]], "doubleml.datasets.make_heterogeneous_data": [[11, null]], "doubleml.datasets.make_iivm_data": [[12, null]], "doubleml.datasets.make_irm_data": [[13, null]], "doubleml.datasets.make_irm_data_discrete_treatments": [[14, null]], "doubleml.datasets.make_pliv_CHS2015": [[15, null]], "doubleml.datasets.make_pliv_multiway_cluster_CKMS2021": [[16, null]], "doubleml.datasets.make_plr_CCDDHNR2018": [[17, null]], "doubleml.datasets.make_plr_turrell2018": [[18, null]], "doubleml.datasets.make_ssm_data": [[19, null]], "doubleml.did": [[3, "doubleml-did"]], "doubleml.did.DoubleMLDID": [[20, null]], "doubleml.did.DoubleMLDIDAggregation": [[21, null]], "doubleml.did.DoubleMLDIDBinary": [[22, null]], "doubleml.did.DoubleMLDIDCS": [[23, null]], "doubleml.did.DoubleMLDIDMulti": [[24, null]], "doubleml.did.datasets.make_did_CS2021": [[25, null]], "doubleml.did.datasets.make_did_SZ2020": [[26, null]], "doubleml.double_ml_score_mixins.LinearScoreMixin": [[27, null]], "doubleml.double_ml_score_mixins.NonLinearScoreMixin": [[28, null]], "doubleml.irm": [[3, "doubleml-irm"]], "doubleml.irm.DoubleMLAPO": [[29, null]], "doubleml.irm.DoubleMLAPOS": [[30, null]], "doubleml.irm.DoubleMLCVAR": [[31, null]], "doubleml.irm.DoubleMLIIVM": [[32, null]], "doubleml.irm.DoubleMLIRM": [[33, null]], "doubleml.irm.DoubleMLLPQ": [[34, null]], "doubleml.irm.DoubleMLPQ": [[35, null]], "doubleml.irm.DoubleMLQTE": [[36, null]], "doubleml.irm.DoubleMLSSM": [[37, null]], "doubleml.plm": [[3, "doubleml-plm"]], "doubleml.plm.DoubleMLPLIV": [[38, null]], "doubleml.plm.DoubleMLPLR": [[39, null]], "doubleml.rdd": [[3, "doubleml-rdd"]], "doubleml.rdd.RDFlex": [[40, null]], "doubleml.rdd.datasets.make_simple_rdd_data": [[41, null]], "doubleml.utils.DMLDummyClassifier": [[42, null]], "doubleml.utils.DMLDummyRegressor": [[43, null]], "doubleml.utils.DoubleMLBLP": [[44, null]], "doubleml.utils.DoubleMLPolicyTree": [[45, null]], "doubleml.utils.GlobalClassifier": [[46, null]], "doubleml.utils.GlobalRegressor": [[47, null]], "doubleml.utils.gain_statistics": [[48, null]]}, "docnames": ["api/api", "api/data_class", "api/datasets", "api/dml_models", "api/generated/doubleml.data.DoubleMLClusterData", "api/generated/doubleml.data.DoubleMLData", "api/generated/doubleml.data.DoubleMLPanelData", "api/generated/doubleml.datasets.fetch_401K", "api/generated/doubleml.datasets.fetch_bonus", "api/generated/doubleml.datasets.make_confounded_irm_data", "api/generated/doubleml.datasets.make_confounded_plr_data", "api/generated/doubleml.datasets.make_heterogeneous_data", "api/generated/doubleml.datasets.make_iivm_data", "api/generated/doubleml.datasets.make_irm_data", "api/generated/doubleml.datasets.make_irm_data_discrete_treatments", "api/generated/doubleml.datasets.make_pliv_CHS2015", "api/generated/doubleml.datasets.make_pliv_multiway_cluster_CKMS2021", "api/generated/doubleml.datasets.make_plr_CCDDHNR2018", "api/generated/doubleml.datasets.make_plr_turrell2018", "api/generated/doubleml.datasets.make_ssm_data", "api/generated/doubleml.did.DoubleMLDID", "api/generated/doubleml.did.DoubleMLDIDAggregation", "api/generated/doubleml.did.DoubleMLDIDBinary", "api/generated/doubleml.did.DoubleMLDIDCS", "api/generated/doubleml.did.DoubleMLDIDMulti", "api/generated/doubleml.did.datasets.make_did_CS2021", "api/generated/doubleml.did.datasets.make_did_SZ2020", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin", "api/generated/doubleml.irm.DoubleMLAPO", "api/generated/doubleml.irm.DoubleMLAPOS", "api/generated/doubleml.irm.DoubleMLCVAR", "api/generated/doubleml.irm.DoubleMLIIVM", "api/generated/doubleml.irm.DoubleMLIRM", "api/generated/doubleml.irm.DoubleMLLPQ", "api/generated/doubleml.irm.DoubleMLPQ", "api/generated/doubleml.irm.DoubleMLQTE", "api/generated/doubleml.irm.DoubleMLSSM", "api/generated/doubleml.plm.DoubleMLPLIV", "api/generated/doubleml.plm.DoubleMLPLR", "api/generated/doubleml.rdd.RDFlex", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data", "api/generated/doubleml.utils.DMLDummyClassifier", "api/generated/doubleml.utils.DMLDummyRegressor", "api/generated/doubleml.utils.DoubleMLBLP", "api/generated/doubleml.utils.DoubleMLPolicyTree", "api/generated/doubleml.utils.GlobalClassifier", "api/generated/doubleml.utils.GlobalRegressor", "api/generated/doubleml.utils.gain_statistics", "api/mixins", "api/utility", "examples/R_double_ml_basic_iv", "examples/R_double_ml_basics", "examples/R_double_ml_did", "examples/R_double_ml_multiway_cluster", "examples/R_double_ml_pension", "examples/R_double_ml_pipeline", "examples/R_double_ml_ssm", "examples/did/py_did", "examples/did/py_did_pretest", "examples/did/py_panel", "examples/did/py_panel_simple", "examples/double_ml_bonus_data", "examples/index", "examples/py_double_ml_apo", "examples/py_double_ml_basic_iv", "examples/py_double_ml_basics", "examples/py_double_ml_cate", "examples/py_double_ml_cate_plr", "examples/py_double_ml_cvar", "examples/py_double_ml_firststage", "examples/py_double_ml_gate", "examples/py_double_ml_gate_plr", "examples/py_double_ml_gate_sensitivity", "examples/py_double_ml_irm_vs_apo", "examples/py_double_ml_learner", "examples/py_double_ml_meets_flaml", "examples/py_double_ml_multiway_cluster", "examples/py_double_ml_pension", "examples/py_double_ml_pension_qte", "examples/py_double_ml_plm_irm_hetfx", "examples/py_double_ml_policy_tree", "examples/py_double_ml_pq", "examples/py_double_ml_rdflex", "examples/py_double_ml_robust_iv", "examples/py_double_ml_sensitivity", "examples/py_double_ml_sensitivity_booking", "examples/py_double_ml_ssm", "guide/algorithms", "guide/basics", "guide/data/base_data", "guide/data/panel_data", "guide/data_backend", "guide/guide", "guide/heterogeneity", "guide/learners", "guide/models", "guide/models/did/did_aggregation", "guide/models/did/did_binary", "guide/models/did/did_cs", "guide/models/did/did_pa", "guide/models/did/did_setup", "guide/models/irm/apo", "guide/models/irm/apos", "guide/models/irm/iivm", "guide/models/irm/irm", "guide/models/plm/pliv", "guide/models/plm/plr", "guide/models/ssm/ssm", "guide/resampling", "guide/scores", "guide/scores/did/did_cs_binary_score", "guide/scores/did/did_cs_score", "guide/scores/did/did_pa_binary_score", "guide/scores/did/did_pa_score", "guide/scores/irm/apo_score", "guide/scores/irm/cvar_score", "guide/scores/irm/iivm_score", "guide/scores/irm/irm_score", "guide/scores/irm/lpq_score", "guide/scores/irm/pq_score", "guide/scores/plm/pliv_score", "guide/scores/plm/plr_score", "guide/scores/ssm/mar_score", "guide/scores/ssm/nr_score", "guide/se_confint", "guide/sensitivity", "guide/sensitivity/benchmarking", "guide/sensitivity/did/did_cs_binary_sensitivity", "guide/sensitivity/did/did_cs_sensitivity", "guide/sensitivity/did/did_pa_binary_sensitivity", "guide/sensitivity/did/did_pa_sensitivity", "guide/sensitivity/implementation", "guide/sensitivity/irm/apo_sensitivity", "guide/sensitivity/irm/irm_sensitivity", "guide/sensitivity/plm/plr_sensitivity", "guide/sensitivity/theory", "index", "intro/install", "intro/intro", "literature/literature", "release/release", "workflow/workflow"], "envversion": {"nbsphinx": 4, "sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1}, "filenames": ["api/api.rst", "api/data_class.rst", "api/datasets.rst", "api/dml_models.rst", "api/generated/doubleml.data.DoubleMLClusterData.rst", "api/generated/doubleml.data.DoubleMLData.rst", "api/generated/doubleml.data.DoubleMLPanelData.rst", "api/generated/doubleml.datasets.fetch_401K.rst", "api/generated/doubleml.datasets.fetch_bonus.rst", "api/generated/doubleml.datasets.make_confounded_irm_data.rst", "api/generated/doubleml.datasets.make_confounded_plr_data.rst", "api/generated/doubleml.datasets.make_heterogeneous_data.rst", "api/generated/doubleml.datasets.make_iivm_data.rst", "api/generated/doubleml.datasets.make_irm_data.rst", "api/generated/doubleml.datasets.make_irm_data_discrete_treatments.rst", "api/generated/doubleml.datasets.make_pliv_CHS2015.rst", "api/generated/doubleml.datasets.make_pliv_multiway_cluster_CKMS2021.rst", "api/generated/doubleml.datasets.make_plr_CCDDHNR2018.rst", "api/generated/doubleml.datasets.make_plr_turrell2018.rst", "api/generated/doubleml.datasets.make_ssm_data.rst", "api/generated/doubleml.did.DoubleMLDID.rst", "api/generated/doubleml.did.DoubleMLDIDAggregation.rst", "api/generated/doubleml.did.DoubleMLDIDBinary.rst", "api/generated/doubleml.did.DoubleMLDIDCS.rst", "api/generated/doubleml.did.DoubleMLDIDMulti.rst", "api/generated/doubleml.did.datasets.make_did_CS2021.rst", "api/generated/doubleml.did.datasets.make_did_SZ2020.rst", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin.rst", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin.rst", "api/generated/doubleml.irm.DoubleMLAPO.rst", "api/generated/doubleml.irm.DoubleMLAPOS.rst", "api/generated/doubleml.irm.DoubleMLCVAR.rst", "api/generated/doubleml.irm.DoubleMLIIVM.rst", "api/generated/doubleml.irm.DoubleMLIRM.rst", "api/generated/doubleml.irm.DoubleMLLPQ.rst", "api/generated/doubleml.irm.DoubleMLPQ.rst", "api/generated/doubleml.irm.DoubleMLQTE.rst", "api/generated/doubleml.irm.DoubleMLSSM.rst", "api/generated/doubleml.plm.DoubleMLPLIV.rst", "api/generated/doubleml.plm.DoubleMLPLR.rst", "api/generated/doubleml.rdd.RDFlex.rst", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data.rst", "api/generated/doubleml.utils.DMLDummyClassifier.rst", "api/generated/doubleml.utils.DMLDummyRegressor.rst", "api/generated/doubleml.utils.DoubleMLBLP.rst", "api/generated/doubleml.utils.DoubleMLPolicyTree.rst", "api/generated/doubleml.utils.GlobalClassifier.rst", "api/generated/doubleml.utils.GlobalRegressor.rst", "api/generated/doubleml.utils.gain_statistics.rst", "api/mixins.rst", "api/utility.rst", "examples/R_double_ml_basic_iv.ipynb", "examples/R_double_ml_basics.ipynb", "examples/R_double_ml_did.ipynb", "examples/R_double_ml_multiway_cluster.ipynb", "examples/R_double_ml_pension.ipynb", "examples/R_double_ml_pipeline.ipynb", "examples/R_double_ml_ssm.ipynb", "examples/did/py_did.ipynb", "examples/did/py_did_pretest.ipynb", "examples/did/py_panel.ipynb", "examples/did/py_panel_simple.ipynb", "examples/double_ml_bonus_data.ipynb", "examples/index.rst", "examples/py_double_ml_apo.ipynb", "examples/py_double_ml_basic_iv.ipynb", "examples/py_double_ml_basics.ipynb", "examples/py_double_ml_cate.ipynb", "examples/py_double_ml_cate_plr.ipynb", "examples/py_double_ml_cvar.ipynb", "examples/py_double_ml_firststage.ipynb", "examples/py_double_ml_gate.ipynb", "examples/py_double_ml_gate_plr.ipynb", "examples/py_double_ml_gate_sensitivity.ipynb", "examples/py_double_ml_irm_vs_apo.ipynb", "examples/py_double_ml_learner.ipynb", "examples/py_double_ml_meets_flaml.ipynb", "examples/py_double_ml_multiway_cluster.ipynb", "examples/py_double_ml_pension.ipynb", "examples/py_double_ml_pension_qte.ipynb", "examples/py_double_ml_plm_irm_hetfx.ipynb", "examples/py_double_ml_policy_tree.ipynb", "examples/py_double_ml_pq.ipynb", "examples/py_double_ml_rdflex.ipynb", "examples/py_double_ml_robust_iv.ipynb", "examples/py_double_ml_sensitivity.ipynb", "examples/py_double_ml_sensitivity_booking.ipynb", "examples/py_double_ml_ssm.ipynb", "guide/algorithms.rst", "guide/basics.rst", "guide/data/base_data.rst", "guide/data/panel_data.rst", "guide/data_backend.rst", "guide/guide.rst", "guide/heterogeneity.rst", "guide/learners.rst", "guide/models.rst", "guide/models/did/did_aggregation.rst", "guide/models/did/did_binary.rst", "guide/models/did/did_cs.rst", "guide/models/did/did_pa.rst", "guide/models/did/did_setup.rst", "guide/models/irm/apo.rst", "guide/models/irm/apos.rst", "guide/models/irm/iivm.rst", "guide/models/irm/irm.rst", "guide/models/plm/pliv.rst", "guide/models/plm/plr.rst", "guide/models/ssm/ssm.rst", "guide/resampling.rst", "guide/scores.rst", "guide/scores/did/did_cs_binary_score.rst", "guide/scores/did/did_cs_score.rst", "guide/scores/did/did_pa_binary_score.rst", "guide/scores/did/did_pa_score.rst", "guide/scores/irm/apo_score.rst", "guide/scores/irm/cvar_score.rst", "guide/scores/irm/iivm_score.rst", "guide/scores/irm/irm_score.rst", "guide/scores/irm/lpq_score.rst", "guide/scores/irm/pq_score.rst", "guide/scores/plm/pliv_score.rst", "guide/scores/plm/plr_score.rst", "guide/scores/ssm/mar_score.rst", "guide/scores/ssm/nr_score.rst", "guide/se_confint.rst", "guide/sensitivity.rst", "guide/sensitivity/benchmarking.rst", "guide/sensitivity/did/did_cs_binary_sensitivity.rst", "guide/sensitivity/did/did_cs_sensitivity.rst", "guide/sensitivity/did/did_pa_binary_sensitivity.rst", "guide/sensitivity/did/did_pa_sensitivity.rst", "guide/sensitivity/implementation.rst", "guide/sensitivity/irm/apo_sensitivity.rst", "guide/sensitivity/irm/irm_sensitivity.rst", "guide/sensitivity/plm/plr_sensitivity.rst", "guide/sensitivity/theory.rst", "index.rst", "intro/install.rst", "intro/intro.rst", "literature/literature.rst", "release/release.rst", "workflow/workflow.rst"], "indexentries": {"aggregate() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.aggregate", false]], "aggregate_over_splits() (doubleml.rdd.rdflex method)": [[40, "doubleml.rdd.RDFlex.aggregate_over_splits", false]], "bootstrap() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.bootstrap", false]], "bootstrap() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.bootstrap", false]], "capo() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.capo", false]], "cate() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.cate", false]], "cate() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.cate", false]], "causal_contrast() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.causal_contrast", false]], "confint() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.confint", false]], "confint() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.confint", false]], "confint() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.confint", false]], "confint() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.confint", false]], "confint() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.confint", false]], "confint() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.confint", false]], "confint() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.confint", false]], "confint() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.confint", false]], "confint() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.confint", false]], "confint() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.confint", false]], "confint() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.confint", false]], "confint() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.confint", false]], "confint() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.confint", false]], "confint() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.confint", false]], "confint() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.confint", false]], "confint() (doubleml.rdd.rdflex method)": [[40, "doubleml.rdd.RDFlex.confint", false]], "confint() (doubleml.utils.doublemlblp method)": [[44, "doubleml.utils.DoubleMLBLP.confint", false]], "construct_framework() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.construct_framework", false]], "construct_framework() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.construct_framework", false]], "dmldummyclassifier (class in doubleml.utils)": [[42, "doubleml.utils.DMLDummyClassifier", false]], "dmldummyregressor (class in doubleml.utils)": [[43, "doubleml.utils.DMLDummyRegressor", false]], "doublemlapo (class in doubleml.irm)": [[29, "doubleml.irm.DoubleMLAPO", false]], "doublemlapos (class in doubleml.irm)": [[30, "doubleml.irm.DoubleMLAPOS", false]], "doublemlblp (class in doubleml.utils)": [[44, "doubleml.utils.DoubleMLBLP", false]], "doublemlclusterdata (class in doubleml.data)": [[4, "doubleml.data.DoubleMLClusterData", false]], "doublemlcvar (class in doubleml.irm)": [[31, "doubleml.irm.DoubleMLCVAR", false]], "doublemldata (class in doubleml.data)": [[5, "doubleml.data.DoubleMLData", false]], "doublemldid (class in doubleml.did)": [[20, "doubleml.did.DoubleMLDID", false]], "doublemldidaggregation (class in doubleml.did)": [[21, "doubleml.did.DoubleMLDIDAggregation", false]], "doublemldidbinary (class in doubleml.did)": [[22, "doubleml.did.DoubleMLDIDBinary", false]], "doublemldidcs (class in doubleml.did)": [[23, "doubleml.did.DoubleMLDIDCS", false]], "doublemldidmulti (class in doubleml.did)": [[24, "doubleml.did.DoubleMLDIDMulti", false]], "doublemliivm (class in doubleml.irm)": [[32, "doubleml.irm.DoubleMLIIVM", false]], "doublemlirm (class in doubleml.irm)": [[33, "doubleml.irm.DoubleMLIRM", false]], "doublemllpq (class in doubleml.irm)": [[34, "doubleml.irm.DoubleMLLPQ", false]], "doublemlpaneldata (class in doubleml.data)": [[6, "doubleml.data.DoubleMLPanelData", false]], "doublemlpliv (class in doubleml.plm)": [[38, "doubleml.plm.DoubleMLPLIV", false]], "doublemlplr (class in doubleml.plm)": [[39, "doubleml.plm.DoubleMLPLR", false]], "doublemlpolicytree (class in doubleml.utils)": [[45, "doubleml.utils.DoubleMLPolicyTree", false]], "doublemlpq (class in doubleml.irm)": [[35, "doubleml.irm.DoubleMLPQ", false]], "doublemlqte (class in doubleml.irm)": [[36, "doubleml.irm.DoubleMLQTE", false]], "doublemlssm (class in doubleml.irm)": [[37, "doubleml.irm.DoubleMLSSM", false]], "draw_sample_splitting() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.draw_sample_splitting", false]], "evaluate_learners() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.evaluate_learners", false]], "fetch_401k() (in module doubleml.datasets)": [[7, "doubleml.datasets.fetch_401K", false]], "fetch_bonus() (in module doubleml.datasets)": [[8, "doubleml.datasets.fetch_bonus", false]], "fit() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.fit", false]], "fit() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.fit", false]], "fit() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.fit", false]], "fit() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.fit", false]], "fit() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.fit", false]], "fit() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.fit", false]], "fit() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.fit", false]], "fit() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.fit", false]], "fit() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.fit", false]], "fit() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.fit", false]], "fit() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.fit", false]], "fit() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.fit", false]], "fit() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.fit", false]], "fit() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.fit", false]], "fit() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.fit", false]], "fit() (doubleml.rdd.rdflex method)": [[40, "doubleml.rdd.RDFlex.fit", false]], "fit() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.fit", false]], "fit() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.fit", false]], "fit() (doubleml.utils.doublemlblp method)": [[44, "doubleml.utils.DoubleMLBLP.fit", false]], "fit() (doubleml.utils.doublemlpolicytree method)": [[45, "doubleml.utils.DoubleMLPolicyTree.fit", false]], "fit() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.fit", false]], "fit() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.fit", false]], "from_arrays() (doubleml.data.doublemlclusterdata class method)": [[4, "doubleml.data.DoubleMLClusterData.from_arrays", false]], "from_arrays() (doubleml.data.doublemldata class method)": [[5, "doubleml.data.DoubleMLData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlpaneldata class method)": [[6, "doubleml.data.DoubleMLPanelData.from_arrays", false]], "gain_statistics() (in module doubleml.utils)": [[48, "doubleml.utils.gain_statistics", false]], "gapo() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.gapo", false]], "gate() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.gate", false]], "gate() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.gate", false]], "get_metadata_routing() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.get_metadata_routing", false]], "get_params() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.get_params", false]], "get_params() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.get_params", false]], "get_params() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.get_params", false]], "get_params() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.get_params", false]], "get_params() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.get_params", false]], "get_params() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.get_params", false]], "get_params() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.get_params", false]], "get_params() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.get_params", false]], "get_params() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.get_params", false]], "get_params() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.get_params", false]], "get_params() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.get_params", false]], "get_params() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.get_params", false]], "get_params() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.get_params", false]], "get_params() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.get_params", false]], "globalclassifier (class in doubleml.utils)": [[46, "doubleml.utils.GlobalClassifier", false]], "globalregressor (class in doubleml.utils)": [[47, "doubleml.utils.GlobalRegressor", false]], "linearscoremixin (class in doubleml.double_ml_score_mixins)": [[27, "doubleml.double_ml_score_mixins.LinearScoreMixin", false]], "make_confounded_irm_data() (in module doubleml.datasets)": [[9, "doubleml.datasets.make_confounded_irm_data", false]], "make_confounded_plr_data() (in module doubleml.datasets)": [[10, "doubleml.datasets.make_confounded_plr_data", false]], "make_did_cs2021() (in module doubleml.did.datasets)": [[25, "doubleml.did.datasets.make_did_CS2021", false]], "make_did_sz2020() (in module doubleml.did.datasets)": [[26, "doubleml.did.datasets.make_did_SZ2020", false]], "make_heterogeneous_data() (in module doubleml.datasets)": [[11, "doubleml.datasets.make_heterogeneous_data", false]], "make_iivm_data() (in module doubleml.datasets)": [[12, "doubleml.datasets.make_iivm_data", false]], "make_irm_data() (in module doubleml.datasets)": [[13, "doubleml.datasets.make_irm_data", false]], "make_irm_data_discrete_treatments() (in module doubleml.datasets)": [[14, "doubleml.datasets.make_irm_data_discrete_treatments", false]], "make_pliv_chs2015() (in module doubleml.datasets)": [[15, "doubleml.datasets.make_pliv_CHS2015", false]], "make_pliv_multiway_cluster_ckms2021() (in module doubleml.datasets)": [[16, "doubleml.datasets.make_pliv_multiway_cluster_CKMS2021", false]], "make_plr_ccddhnr2018() (in module doubleml.datasets)": [[17, "doubleml.datasets.make_plr_CCDDHNR2018", false]], "make_plr_turrell2018() (in module doubleml.datasets)": [[18, "doubleml.datasets.make_plr_turrell2018", false]], "make_simple_rdd_data() (in module doubleml.rdd.datasets)": [[41, "doubleml.rdd.datasets.make_simple_rdd_data", false]], "make_ssm_data() (in module doubleml.datasets)": [[19, "doubleml.datasets.make_ssm_data", false]], "nonlinearscoremixin (class in doubleml.double_ml_score_mixins)": [[28, "doubleml.double_ml_score_mixins.NonLinearScoreMixin", false]], "p_adjust() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.p_adjust", false]], "p_adjust() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.p_adjust", false]], "plot_effects() (doubleml.did.doublemldidaggregation method)": [[21, "doubleml.did.DoubleMLDIDAggregation.plot_effects", false]], "plot_effects() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.plot_effects", false]], "plot_tree() (doubleml.utils.doublemlpolicytree method)": [[45, "doubleml.utils.DoubleMLPolicyTree.plot_tree", false]], "policy_tree() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.policy_tree", false]], "predict() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.predict", false]], "predict() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.predict", false]], "predict() (doubleml.utils.doublemlpolicytree method)": [[45, "doubleml.utils.DoubleMLPolicyTree.predict", false]], "predict() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.predict", false]], "predict() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.predict", false]], "predict_proba() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.predict_proba", false]], "predict_proba() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.predict_proba", false]], "rdflex (class in doubleml.rdd)": [[40, "doubleml.rdd.RDFlex", false]], "robust_confset() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.robust_confset", false]], "score() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.score", false]], "score() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.score", false]], "score() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.score", false]], "score() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.score", false]], "sensitivity_analysis() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_analysis", false]], "sensitivity_benchmark() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_benchmark", false]], "sensitivity_plot() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_plot", false]], "set_fit_request() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.set_fit_request", false]], "set_fit_request() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.set_fit_request", false]], "set_ml_nuisance_params() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.set_ml_nuisance_params", false]], "set_params() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.set_params", false]], "set_params() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.set_params", false]], "set_params() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.set_params", false]], "set_params() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.set_params", false]], "set_sample_splitting() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.set_sample_splitting", false]], "set_score_request() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.set_score_request", false]], "set_score_request() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.set_score_request", false]], "set_x_d() (doubleml.data.doublemlclusterdata method)": [[4, "doubleml.data.DoubleMLClusterData.set_x_d", false]], "set_x_d() (doubleml.data.doublemldata method)": [[5, "doubleml.data.DoubleMLData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlpaneldata method)": [[6, "doubleml.data.DoubleMLPanelData.set_x_d", false]], "tune() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.tune", false]], "tune() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.tune", false]], "tune() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.tune", false]], "tune() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.tune", false]], "tune() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.tune", false]], "tune() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.tune", false]], "tune() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.tune", false]], "tune() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.tune", false]], "tune() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.tune", false]], "tune() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.tune", false]], "tune() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.tune", false]], "tune() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.tune", false]]}, "objects": {"doubleml.data": [[4, 0, 1, "", "DoubleMLClusterData"], [5, 0, 1, "", "DoubleMLData"], [6, 0, 1, "", "DoubleMLPanelData"]], "doubleml.data.DoubleMLClusterData": [[4, 1, 1, "", "from_arrays"], [4, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLData": [[5, 1, 1, "", "from_arrays"], [5, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLPanelData": [[6, 1, 1, "", "from_arrays"], [6, 1, 1, "", "set_x_d"]], "doubleml.datasets": [[7, 2, 1, "", "fetch_401K"], [8, 2, 1, "", "fetch_bonus"], [9, 2, 1, "", "make_confounded_irm_data"], [10, 2, 1, "", "make_confounded_plr_data"], [11, 2, 1, "", "make_heterogeneous_data"], [12, 2, 1, "", "make_iivm_data"], [13, 2, 1, "", "make_irm_data"], [14, 2, 1, "", "make_irm_data_discrete_treatments"], [15, 2, 1, "", "make_pliv_CHS2015"], [16, 2, 1, "", "make_pliv_multiway_cluster_CKMS2021"], [17, 2, 1, "", "make_plr_CCDDHNR2018"], [18, 2, 1, "", "make_plr_turrell2018"], [19, 2, 1, "", "make_ssm_data"]], "doubleml.did": [[20, 0, 1, "", "DoubleMLDID"], [21, 0, 1, "", "DoubleMLDIDAggregation"], [22, 0, 1, "", "DoubleMLDIDBinary"], [23, 0, 1, "", "DoubleMLDIDCS"], [24, 0, 1, "", "DoubleMLDIDMulti"]], "doubleml.did.DoubleMLDID": [[20, 1, 1, "", "bootstrap"], [20, 1, 1, "", "confint"], [20, 1, 1, "", "construct_framework"], [20, 1, 1, "", "draw_sample_splitting"], [20, 1, 1, "", "evaluate_learners"], [20, 1, 1, "", "fit"], [20, 1, 1, "", "get_params"], [20, 1, 1, "", "p_adjust"], [20, 1, 1, "", "sensitivity_analysis"], [20, 1, 1, "", "sensitivity_benchmark"], [20, 1, 1, "", "sensitivity_plot"], [20, 1, 1, "", "set_ml_nuisance_params"], [20, 1, 1, "", "set_sample_splitting"], [20, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDAggregation": [[21, 1, 1, "", "plot_effects"]], "doubleml.did.DoubleMLDIDBinary": [[22, 1, 1, "", "bootstrap"], [22, 1, 1, "", "confint"], [22, 1, 1, "", "construct_framework"], [22, 1, 1, "", "draw_sample_splitting"], [22, 1, 1, "", "evaluate_learners"], [22, 1, 1, "", "fit"], [22, 1, 1, "", "get_params"], [22, 1, 1, "", "p_adjust"], [22, 1, 1, "", "sensitivity_analysis"], [22, 1, 1, "", "sensitivity_benchmark"], [22, 1, 1, "", "sensitivity_plot"], [22, 1, 1, "", "set_ml_nuisance_params"], [22, 1, 1, "", "set_sample_splitting"], [22, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDCS": [[23, 1, 1, "", "bootstrap"], [23, 1, 1, "", "confint"], [23, 1, 1, "", "construct_framework"], [23, 1, 1, "", "draw_sample_splitting"], [23, 1, 1, "", "evaluate_learners"], [23, 1, 1, "", "fit"], [23, 1, 1, "", "get_params"], [23, 1, 1, "", "p_adjust"], [23, 1, 1, "", "sensitivity_analysis"], [23, 1, 1, "", "sensitivity_benchmark"], [23, 1, 1, "", "sensitivity_plot"], [23, 1, 1, "", "set_ml_nuisance_params"], [23, 1, 1, "", "set_sample_splitting"], [23, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDMulti": [[24, 1, 1, "", "aggregate"], [24, 1, 1, "", "bootstrap"], [24, 1, 1, "", "confint"], [24, 1, 1, "", "fit"], [24, 1, 1, "", "p_adjust"], [24, 1, 1, "", "plot_effects"], [24, 1, 1, "", "sensitivity_analysis"], [24, 1, 1, "", "sensitivity_benchmark"], [24, 1, 1, "", "sensitivity_plot"]], "doubleml.did.datasets": [[25, 2, 1, "", "make_did_CS2021"], [26, 2, 1, "", "make_did_SZ2020"]], "doubleml.double_ml_score_mixins": [[27, 0, 1, "", "LinearScoreMixin"], [28, 0, 1, "", "NonLinearScoreMixin"]], "doubleml.irm": [[29, 0, 1, "", "DoubleMLAPO"], [30, 0, 1, "", "DoubleMLAPOS"], [31, 0, 1, "", "DoubleMLCVAR"], [32, 0, 1, "", "DoubleMLIIVM"], [33, 0, 1, "", "DoubleMLIRM"], [34, 0, 1, "", "DoubleMLLPQ"], [35, 0, 1, "", "DoubleMLPQ"], [36, 0, 1, "", "DoubleMLQTE"], [37, 0, 1, "", "DoubleMLSSM"]], "doubleml.irm.DoubleMLAPO": [[29, 1, 1, "", "bootstrap"], [29, 1, 1, "", "capo"], [29, 1, 1, "", "confint"], [29, 1, 1, "", "construct_framework"], [29, 1, 1, "", "draw_sample_splitting"], [29, 1, 1, "", "evaluate_learners"], [29, 1, 1, "", "fit"], [29, 1, 1, "", "gapo"], [29, 1, 1, "", "get_params"], [29, 1, 1, "", "p_adjust"], [29, 1, 1, "", "sensitivity_analysis"], [29, 1, 1, "", "sensitivity_benchmark"], [29, 1, 1, "", "sensitivity_plot"], [29, 1, 1, "", "set_ml_nuisance_params"], [29, 1, 1, "", "set_sample_splitting"], [29, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLAPOS": [[30, 1, 1, "", "bootstrap"], [30, 1, 1, "", "causal_contrast"], [30, 1, 1, "", "confint"], [30, 1, 1, "", "draw_sample_splitting"], [30, 1, 1, "", "fit"], [30, 1, 1, "", "sensitivity_analysis"], [30, 1, 1, "", "sensitivity_benchmark"], [30, 1, 1, "", "sensitivity_plot"], [30, 1, 1, "", "set_sample_splitting"]], "doubleml.irm.DoubleMLCVAR": [[31, 1, 1, "", "bootstrap"], [31, 1, 1, "", "confint"], [31, 1, 1, "", "construct_framework"], [31, 1, 1, "", "draw_sample_splitting"], [31, 1, 1, "", "evaluate_learners"], [31, 1, 1, "", "fit"], [31, 1, 1, "", "get_params"], [31, 1, 1, "", "p_adjust"], [31, 1, 1, "", "sensitivity_analysis"], [31, 1, 1, "", "sensitivity_benchmark"], [31, 1, 1, "", "sensitivity_plot"], [31, 1, 1, "", "set_ml_nuisance_params"], [31, 1, 1, "", "set_sample_splitting"], [31, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLIIVM": [[32, 1, 1, "", "bootstrap"], [32, 1, 1, "", "confint"], [32, 1, 1, "", "construct_framework"], [32, 1, 1, "", "draw_sample_splitting"], [32, 1, 1, "", "evaluate_learners"], [32, 1, 1, "", "fit"], [32, 1, 1, "", "get_params"], [32, 1, 1, "", "p_adjust"], [32, 1, 1, "", "robust_confset"], [32, 1, 1, "", "sensitivity_analysis"], [32, 1, 1, "", "sensitivity_benchmark"], [32, 1, 1, "", "sensitivity_plot"], [32, 1, 1, "", "set_ml_nuisance_params"], [32, 1, 1, "", "set_sample_splitting"], [32, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLIRM": [[33, 1, 1, "", "bootstrap"], [33, 1, 1, "", "cate"], [33, 1, 1, "", "confint"], [33, 1, 1, "", "construct_framework"], [33, 1, 1, "", "draw_sample_splitting"], [33, 1, 1, "", "evaluate_learners"], [33, 1, 1, "", "fit"], [33, 1, 1, "", "gate"], [33, 1, 1, "", "get_params"], [33, 1, 1, "", "p_adjust"], [33, 1, 1, "", "policy_tree"], [33, 1, 1, "", "sensitivity_analysis"], [33, 1, 1, "", "sensitivity_benchmark"], [33, 1, 1, "", "sensitivity_plot"], [33, 1, 1, "", "set_ml_nuisance_params"], [33, 1, 1, "", "set_sample_splitting"], [33, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLLPQ": [[34, 1, 1, "", "bootstrap"], [34, 1, 1, "", "confint"], [34, 1, 1, "", "construct_framework"], [34, 1, 1, "", "draw_sample_splitting"], [34, 1, 1, "", "evaluate_learners"], [34, 1, 1, "", "fit"], [34, 1, 1, "", "get_params"], [34, 1, 1, "", "p_adjust"], [34, 1, 1, "", "sensitivity_analysis"], [34, 1, 1, "", "sensitivity_benchmark"], [34, 1, 1, "", "sensitivity_plot"], [34, 1, 1, "", "set_ml_nuisance_params"], [34, 1, 1, "", "set_sample_splitting"], [34, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLPQ": [[35, 1, 1, "", "bootstrap"], [35, 1, 1, "", "confint"], [35, 1, 1, "", "construct_framework"], [35, 1, 1, "", "draw_sample_splitting"], [35, 1, 1, "", "evaluate_learners"], [35, 1, 1, "", "fit"], [35, 1, 1, "", "get_params"], [35, 1, 1, "", "p_adjust"], [35, 1, 1, "", "sensitivity_analysis"], [35, 1, 1, "", "sensitivity_benchmark"], [35, 1, 1, "", "sensitivity_plot"], [35, 1, 1, "", "set_ml_nuisance_params"], [35, 1, 1, "", "set_sample_splitting"], [35, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLQTE": [[36, 1, 1, "", "bootstrap"], [36, 1, 1, "", "confint"], [36, 1, 1, "", "draw_sample_splitting"], [36, 1, 1, "", "fit"], [36, 1, 1, "", "p_adjust"], [36, 1, 1, "", "set_sample_splitting"]], "doubleml.irm.DoubleMLSSM": [[37, 1, 1, "", "bootstrap"], [37, 1, 1, "", "confint"], [37, 1, 1, "", "construct_framework"], [37, 1, 1, "", "draw_sample_splitting"], [37, 1, 1, "", "evaluate_learners"], [37, 1, 1, "", "fit"], [37, 1, 1, "", "get_params"], [37, 1, 1, "", "p_adjust"], [37, 1, 1, "", "sensitivity_analysis"], [37, 1, 1, "", "sensitivity_benchmark"], [37, 1, 1, "", "sensitivity_plot"], [37, 1, 1, "", "set_ml_nuisance_params"], [37, 1, 1, "", "set_sample_splitting"], [37, 1, 1, "", "tune"]], "doubleml.plm": [[38, 0, 1, "", "DoubleMLPLIV"], [39, 0, 1, "", "DoubleMLPLR"]], "doubleml.plm.DoubleMLPLIV": [[38, 1, 1, "", "bootstrap"], [38, 1, 1, "", "confint"], [38, 1, 1, "", "construct_framework"], [38, 1, 1, "", "draw_sample_splitting"], [38, 1, 1, "", "evaluate_learners"], [38, 1, 1, "", "fit"], [38, 1, 1, "", "get_params"], [38, 1, 1, "", "p_adjust"], [38, 1, 1, "", "sensitivity_analysis"], [38, 1, 1, "", "sensitivity_benchmark"], [38, 1, 1, "", "sensitivity_plot"], [38, 1, 1, "", "set_ml_nuisance_params"], [38, 1, 1, "", "set_sample_splitting"], [38, 1, 1, "", "tune"]], "doubleml.plm.DoubleMLPLR": [[39, 1, 1, "", "bootstrap"], [39, 1, 1, "", "cate"], [39, 1, 1, "", "confint"], [39, 1, 1, "", "construct_framework"], [39, 1, 1, "", "draw_sample_splitting"], [39, 1, 1, "", "evaluate_learners"], [39, 1, 1, "", "fit"], [39, 1, 1, "", "gate"], [39, 1, 1, "", "get_params"], [39, 1, 1, "", "p_adjust"], [39, 1, 1, "", "sensitivity_analysis"], [39, 1, 1, "", "sensitivity_benchmark"], [39, 1, 1, "", "sensitivity_plot"], [39, 1, 1, "", "set_ml_nuisance_params"], [39, 1, 1, "", "set_sample_splitting"], [39, 1, 1, "", "tune"]], "doubleml.rdd": [[40, 0, 1, "", "RDFlex"]], "doubleml.rdd.RDFlex": [[40, 1, 1, "", "aggregate_over_splits"], [40, 1, 1, "", "confint"], [40, 1, 1, "", "fit"]], "doubleml.rdd.datasets": [[41, 2, 1, "", "make_simple_rdd_data"]], "doubleml.utils": [[42, 0, 1, "", "DMLDummyClassifier"], [43, 0, 1, "", "DMLDummyRegressor"], [44, 0, 1, "", "DoubleMLBLP"], [45, 0, 1, "", "DoubleMLPolicyTree"], [46, 0, 1, "", "GlobalClassifier"], [47, 0, 1, "", "GlobalRegressor"], [48, 2, 1, "", "gain_statistics"]], "doubleml.utils.DMLDummyClassifier": [[42, 1, 1, "", "fit"], [42, 1, 1, "", "get_metadata_routing"], [42, 1, 1, "", "get_params"], [42, 1, 1, "", "predict"], [42, 1, 1, "", "predict_proba"], [42, 1, 1, "", "score"], [42, 1, 1, "", "set_params"], [42, 1, 1, "", "set_score_request"]], "doubleml.utils.DMLDummyRegressor": [[43, 1, 1, "", "fit"], [43, 1, 1, "", "get_metadata_routing"], [43, 1, 1, "", "get_params"], [43, 1, 1, "", "predict"], [43, 1, 1, "", "score"], [43, 1, 1, "", "set_params"], [43, 1, 1, "", "set_score_request"]], "doubleml.utils.DoubleMLBLP": [[44, 1, 1, "", "confint"], [44, 1, 1, "", "fit"]], "doubleml.utils.DoubleMLPolicyTree": [[45, 1, 1, "", "fit"], [45, 1, 1, "", "plot_tree"], [45, 1, 1, "", "predict"]], "doubleml.utils.GlobalClassifier": [[46, 1, 1, "", "fit"], [46, 1, 1, "", "get_metadata_routing"], [46, 1, 1, "", "get_params"], [46, 1, 1, "", "predict"], [46, 1, 1, "", "predict_proba"], [46, 1, 1, "", "score"], [46, 1, 1, "", "set_fit_request"], [46, 1, 1, "", "set_params"], [46, 1, 1, "", "set_score_request"]], "doubleml.utils.GlobalRegressor": [[47, 1, 1, "", "fit"], [47, 1, 1, "", "get_metadata_routing"], [47, 1, 1, "", "get_params"], [47, 1, 1, "", "predict"], [47, 1, 1, "", "score"], [47, 1, 1, "", "set_fit_request"], [47, 1, 1, "", "set_params"], [47, 1, 1, "", "set_score_request"]]}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function"}, "terms": {"": [4, 5, 6, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 46, 47, 51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 64, 70, 71, 72, 73, 75, 77, 78, 79, 83, 85, 86, 87, 88, 90, 91, 92, 95, 96, 98, 100, 101, 108, 110, 123, 124, 125, 126, 127, 137, 139, 140, 141, 142], "0": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 130, 131, 132, 134, 138, 139, 141], "00": [60, 72, 74, 78, 79, 109], "000": [84, 125, 142], "00000": 74, "000000": [60, 61, 62, 64, 74, 78, 79, 90, 92, 94, 139], "0000000": 125, "0000000000000010000100": [56, 90, 92, 139], "000000e": [60, 72, 74, 78, 79], "00000591": 82, "000006": [64, 82], "000017": 82, "000025": 77, "000034": 78, "000039": 77, "000064": 65, "000067": 77, "000076": 96, "000091": 77, "0001": [62, 78], "000219": [35, 94], "000242": [36, 94], "000341": 77, "000360": 60, "000373": 60, "000435": 60, "000442": 77, "00047580260495": 51, "000488": 77, "000494": 73, "0005": 62, "000522": 77, "000529": 60, "000542": 60, "000569": 60, "0005a80b528f": 56, "000610": 60, "000631": 60, "000640": 60, "000649": 60, "000670": 77, "000743": 85, "000915799": 125, "0009157990": 125, "000943": [67, 68], "001": [25, 51, 53, 54, 55, 56, 57, 66, 95, 96, 109, 110, 125, 139, 142], "001049": [96, 100], "001051": 77, "001234": 79, "00133": 56, "00138944": [88, 110], "001403": 83, "001471": 74, "001494": [94, 95, 96], "0016": [55, 78], "001603": [96, 100], "001684": 60, "001698": 74, "001714": 94, "001773": 60, "0018": [55, 78], "0019": 62, "001907": 74, "002169338": 125, "0021693380": 125, "0021693381": 125, "002277": 67, "002290": 59, "0023": 53, "002388": 76, "002390": 60, "002436": 73, "0026": 62, "002601": [96, 97], "002779": 85, "0028": [53, 55, 78], "002821": 86, "0028213335041910427": 86, "002921": 60, "002983": 77, "003": [9, 10, 26, 84], "003045": 74, "003074": 96, "003134": 82, "003187": 67, "003220": 64, "003328": 82, "0034": 70, "003404": 64, "003415": 64, "003427": 77, "003607": 68, "00365784": 109, "003724": 60, "003779": 73, "003836": 82, "003924": 73, "003944": 67, "003975": 67, "00409412": [88, 110], "00412": 109, "0042": [55, 78], "004253": 64, "004264": 60, "004392": 73, "004526": 64, "004542": 74, "004556": 60, "004688": 32, "0047": [55, 78], "004846": 86, "005": 80, "00518448": [96, 100], "005339": [67, 68], "005356": 60, "005428": 60, "005556": 61, "005857": 77, "005e": 96, "006055": 64, "006267": 68, "006425": 79, "0068101213851626": 76, "006922": 62, "006958": [67, 68], "007210e": 79, "00728": 139, "0073": 62, "007332": 69, "007332393760465": 69, "007421": 94, "00778625": 109, "0078540263583833": 76, "008": 86, "008023": 79, "008223": [67, 68], "008266e": 79, "008487": 62, "0084871742256079": 76, "008642": 94, "008883698": 110, "00888458890362062": 88, "008884589": 88, "008dbd": 80, "008e80": 80, "009": [80, 86], "009122": 82, "009255": 67, "009329847": 110, "009368": 61, "009428": 69, "00944171905420782": 86, "00950122695463054": 88, "009501226954630540": 88, "009501227": 88, "009645422": 54, "009656": 82, "00972": 62, "009727": [96, 97], "009790": 79, "009882": 80, "009986": 82, "01": [20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 45, 51, 54, 55, 56, 57, 60, 67, 68, 74, 78, 79, 80, 81, 82, 83, 95, 96, 100, 109, 110, 125, 139, 142], "010213": 85, "010269": 77, "010346": 60, "010378": 80, "010450": 54, "010940": 77, "011131": 82, "0112": 53, "011204": 74, "01128": 62, "011323": [96, 97], "011471": 60, "011486": 60, "011577": 60, "011598": 82, "0118095": 54, "011823": 85, "011988e": 82, "012": 80, "012175": 80, "01219": 56, "01274": 86, "012780": 79, "012831": 86, "013": 80, "013034": 86, "013128": 67, "013278": 80, "013313": 76, "013450": 74, "01351638": 54, "013593": 85, "013617": 79, "013665": 60, "013677": 83, "013712": 67, "01398951": 54, "013990": 125, "014": [80, 83], "01403089": 54, "014080": [67, 68], "014265": 60, "014432": 59, "014637": 77, "014681": 85, "014873e": 67, "015": 56, "015038": 69, "015424": 60, "015552": 67, "015565": 82, "0156853566737638": 76, "015698": 82, "01574297": 82, "015743": 82, "015831": 67, "016011": 68, "016154": 77, "016200": [67, 68], "016315": 71, "01643": 140, "016793": 60, "017": 56, "017140": 67, "01727": 80, "017393e": 125, "017660": 74, "01772": 127, "017777e": 68, "017800092": 125, "0178000920": 125, "018": 56, "018023": 81, "018092": 94, "018148": 82, "018508": 67, "01903": [56, 95, 137, 139], "019156": 60, "01916030e": 109, "01925597": 54, "019439633": 125, "0194396330": 125, "0194396331": 125, "019596": 69, "019660": [36, 94], "01990373": 87, "019974": 79, "02": [60, 67, 68, 78, 79, 82, 94, 96, 100, 109], "020127": 60, "02016117": 139, "020166": 82, "020271": 77, "020272": 74, "020360838": 125, "0203608380": 125, "0203608381": 125, "020367": 60, "020511": 60, "02052929": [88, 110], "02079162e": 109, "020819": 94, "02092": 139, "021269": [71, 72], "021307": 61, "021484": 60, "02163217": 54, "021690": 72, "021823": 76, "021866": 81, "021926": 69, "022181": 67, "022258": 74, "022295e": 67, "02247976": 54, "02260304": [96, 100], "022768": 62, "022783": 85, "022853": 60, "022915": 77, "022969": 79, "023020e": [78, 79], "023052": 68, "023256": 82, "023417": 60, "023537": 76, "023563": 125, "023955": 79, "024": 80, "024266": 74, "024346": 67, "024355": 59, "024364": 126, "024401": [71, 72], "024604": 77, "024782": 82, "024926": 59, "025": [67, 68, 71, 72, 74, 80], "025077": [68, 125], "02528067": 75, "0253": 56, "025300e": 68, "025443": 62, "025496": 67, "0257": 53, "025813114": 125, "0258131140": 125, "02584": 56, "025841": 74, "025964": 74, "026": 80, "026099": 60, "026669": 79, "026723": 69, "026822": 76, "026966": 74, "02791": 62, "0281": 56, "028186": 60, "028520": [67, 68], "028630": [96, 97], "028731": 94, "02897287": 58, "029": 80, "02900983": 82, "029010": 82, "029022": 68, "029209": 142, "029364": [126, 132], "029831": 82, "029910e": [78, 79], "02e": 55, "03": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 57, 60, 64, 67, 68, 69, 73, 74, 78, 79, 82, 83, 85, 86, 96, 97, 100, 109, 126, 132, 142], "030059": 94, "0301": 56, "03018": 34, "030346": 139, "03045": 57, "0307": 56, "030720": 61, "030934": 82, "030962": 82, "031": 80, "031007": 76, "03113": 87, "031134": 95, "031156": 68, "031269": 62, "031639": 82, "031820": 68, "03191": [80, 140], "03220": 141, "0323": 53, "03244552": 95, "0325": 139, "03258": 76, "032580": 76, "032738": 76, "032941": 67, "032953": 85, "033265": 76, "033756": 69, "033946": [71, 72], "034065": 68, "03411": 139, "03421": 61, "034226": 79, "03438": 57, "034690": 69, "03477609": 109, "034812763": 125, "0348127630": 125, "0348127631": 125, "034846": 78, "03489": [16, 54, 77], "035083": 61, "035119185": 125, "0351191850": 125, "0351191851": 125, "035133": 60, "035264": 68, "03536": 139, "03538": 56, "03539": 56, "035391": 62, "0354": 56, "035411": 139, "035441": 68, "03545": 56, "035545": 62, "035572": 62, "035689": [96, 100], "035730": 82, "03574": 62, "035762": 82, "035785": 68, "0359": 56, "036129015": 125, "0361290150": 125, "0361290151": 125, "036143": 82, "036147": 82, "036240": 64, "036729": 77, "0368": 53, "03692208": 60, "036945": 79, "03698487": 82, "036985": 82, "037008": [71, 72], "037114": 74, "037316": 60, "0374": 56, "037504": 68, "037509": 87, "037529": 74, "037577": [96, 97], "037747": [67, 68], "03780846": 109, "038103": 74, "0382525": 109, "038725": 61, "038845": 67, "039036": 67, "039141": 64, "03917696": [110, 125], "03920960e": 109, "039310e": 69, "039661": 74, "039895": 74, "039991e": 67, "04": [10, 40, 55, 60, 64, 67, 68, 78, 79, 82, 83, 85, 96, 97, 100, 109, 142], "040010": 74, "040079": 68, "040112": 125, "040139": [67, 68], "040449": 61, "040533": [110, 125], "04053339": 125, "040562": 67, "040581": 60, "040629": 39, "040688": 67, "040742": 80, "040784": 64, "0408": 67, "040912": 68, "040919": 68, "041071": 60, "041147": 69, "041284": 69, "041387": 69, "041459": 79, "041491e": 69, "04165": 96, "0418": 53, "041831": 69, "041925": 67, "042": 80, "042034": 86, "042249": 68, "042265": 69, "0425": 95, "0428": 87, "042804": 74, "042822e": 79, "042844e": 82, "042879": 60, "043082": [96, 100], "043108": 79, "0433": 53, "0434e374": 56, "043752": 60, "043774": 60, "04387": 95, "043998": 68, "044": 80, "044113": 69, "04415": 56, "044176": 74, "044205": 61, "044239": 74, "04424": 56, "04444978": 125, "044449780": 125, "04448849": 109, "0445": 95, "044535": 60, "04465": 54, "044680": 61, "044704": 68, "04478940": 109, "04486": 139, "04487585": [126, 132], "044895": 60, "04491": 96, "044929": 74, "04497975": [126, 132], "04501612": 125, "04502": [95, 110, 125], "045144": 77, "045172": 74, "045272": 80, "045313": 67, "045379": 139, "045510": 60, "04552": 77, "045553": 69, "045624": 59, "04563": 95, "045638": 67, "045690": 61, "045754": 82, "04586": 95, "045932": 82, "045984": 74, "045993": 95, "04625": 95, "046451": 74, "046507": [96, 100], "046525": 94, "046527": 69, "04653976": 82, "046540": 82, "046579": 61, "046587": 68, "0466028": 54, "046728": 85, "04682310e": 109, "046922": 95, "047194": 32, "047239": 74, "047288": 94, "04738504": 109, "047652e": 68, "047724": 67, "047873": 74, "0479343": 109, "047954": 77, "048220": 76, "048308": 72, "048476": 74, "048699": 87, "048723": 95, "048853": 68, "049264": 64, "049573": [96, 100], "04973": 68, "05": [40, 51, 53, 54, 55, 56, 57, 60, 67, 68, 69, 70, 75, 77, 78, 79, 80, 82, 83, 86, 95, 96, 97, 100, 109, 110, 125, 139, 142], "05039": 85, "050494e": 68, "050538": 68, "050623": 60, "051": 56, "051245": 60, "051355": 61, "051651": 83, "051867e": 69, "052": 83, "052000e": 79, "052023": 74, "052167": 60, "052212": 61, "052298": 82, "052380": 67, "052488": 72, "052502": 82, "052745": 69, "053": [56, 96], "053049": 68, "0533": 53, "053331": 69, "053342": 79, "053375": 61, "053389": 125, "053436": 33, "053541": 82, "053558": 69, "053849e": 67, "054": [56, 83], "054068": 77, "054162": 77, "054177": 60, "054348": 125, "054370": 69, "054416": 60, "054529": 125, "054576": 60, "054771e": 82, "055165": 85, "055171": 68, "055439": [76, 79], "055493": 86, "055576": 61, "055680": 125, "056": [80, 83], "056294": 61, "056499": 72, "056745": 67, "056764": 67, "056915": 74, "057095": 82, "0576": [55, 78], "057762": 82, "057792": 67, "057865": 61, "057962": 69, "058042": 125, "058276": 79, "058375": 64, "058398": 60, "058463": 82, "058508": 87, "058595": 67, "058598": 60, "05891": 96, "0590": 53, "0590176": 109, "059128": 67, "059384": 82, "059627": 79, "059630": 59, "059685": 82, "06": [9, 10, 26, 60, 64, 67, 68, 69, 78, 79, 82, 94, 95, 109], "060": 80, "060016": 64, "06008533": 96, "060179": 61, "060201": 82, "060212": [78, 79], "060417": 67, "060581": 75, "060710": 60, "060845": 125, "060893": 60, "060933": 67, "0611": 53, "06111111": 56, "0615": 53, "06161": 96, "061963": 60, "062": [83, 96], "062414": 79, "062507": 82, "06269": 84, "0628": 53, "062964": 125, "062988": 67, "063017": 64, "0632": 53, "063234e": 68, "063253": 61, "063327": 74, "0635": 53, "063593": 68, "0636": 53, "063700": 67, "0638": 53, "063881": 96, "0640": 53, "064007": 61, "064161": 79, "064181": 60, "064213": 68, "06428": 78, "064280": 78, "0645": 53, "064615": 61, "0646222": 55, "0647": 53, "0649": 53, "064957": 61, "065": 86, "0653": 53, "0653166": 109, "065356": [71, 72], "065368": 76, "0654": 53, "065409": 61, "065451": 79, "065455": 61, "0655": 53, "065725": 69, "065790": 61, "0659": [53, 61], "065969": 96, "065976": 74, "0662": 53, "066295": 74, "066374": 61, "066464": 85, "066889": 82, "0669": 53, "06692492": 109, "066964": 60, "067046e": 67, "0671": 53, "067212": 74, "067240": 82, "06724028": 82, "0673": 53, "067444": 61, "0675": 53, "067528": 86, "067721": 125, "068073": 68, "06827": 85, "06834315": 58, "068377": 79, "068514": 67, "068655": 61, "068700": 94, "068934": 64, "06895837": 54, "069443": 64, "0695854": 54, "069589": 74, "069600": 79, "069882e": 67, "069910": 61, "069913": 61, "07": [67, 68, 79, 82, 83, 86, 109], "070020": 82, "070196": 69, "0701961897676835": 69, "0702127": 54, "0704": 53, "070433": 74, "070471e": 60, "070497": 86, "070534": 37, "070552": 67, "070574e": 79, "0707": 53, "070751": 67, "070783": 60, "07085301": 96, "070884": 82, "0711": 53, "071285": 125, "07136": [54, 77], "071362": 67, "071488e": 69, "0716": 53, "07168291": 54, "071777": 95, "071782": [36, 94], "0719": 53, "07202564": [71, 72], "072148": 60, "07222222": 56, "072293": 81, "07229774": [96, 100], "072516": 74, "072605": 64, "0727": 96, "073": 83, "073013": 82, "073207": 77, "073275": 67, "073384": 74, "073447": [96, 100], "07347676": 54, "07350015": [16, 19, 54, 77], "073520": 69, "0736": 53, "07366": [56, 95], "073694": 68, "0739130271918385": 76, "073929": 76, "074255": 60, "0743": 53, "074304": 125, "07436521": 109, "074426": 82, "07456127": 54, "074617": 68, "07479278": 85, "074927": 64, "075261": 59, "075384": 82, "07538443": 82, "07544271e": 109, "07561": 139, "07564554e": 109, "0758": 86, "075809": 64, "075869": 95, "075942": 76, "076019": 78, "076156": 125, "076179312": 125, "0761793120": 125, "076322": 82, "076347": 69, "0765": 56, "076596": 67, "076653": [96, 100], "0766795": 109, "076684": 139, "07685043": 109, "07689": 56, "07691847": 109, "076953": [71, 72], "076971": 62, "077144e": 68, "077161": 79, "07727773e": 109, "077319": 82, "077502": [126, 132], "077555": 67, "077592": 74, "077702": 64, "0777777777777778": 95, "07777778": [56, 95], "077840": 79, "077883": 82, "07788588": [96, 100], "077923e": 67, "07796": 96, "078017": 67, "078096": 125, "078207": 62, "07828372": 125, "078426": 94, "078439": 60, "078474": 125, "078603": 60, "078709": 68, "078810": 82, "079": 80, "079085": 62, "07915": 56, "07919896": 109, "07942v3": 140, "079458e": 78, "079500": 61, "079500e": 67, "07961": 85, "07978296": 109, "079812": 80, "08": [69, 79, 82, 86, 96, 109], "08005229": 109, "08031571": 109, "080854": 79, "08091581": 109, "080930": 61, "080947": 62, "081": 56, "081100": 82, "081230": [67, 68], "081396": 72, "081488": 77, "08154161": 109, "081791": 60, "08181827e": 109, "0820": 53, "082103": 60, "082197": 74, "082263": 38, "082297": 109, "082400e": 67, "082574": 33, "082804": 59, "082858": 68, "082905": [96, 100], "082934": 79, "082973": 77, "0831993": 109, "083258": 125, "083318": 125, "08333333": 56, "08333617": 109, "083474": 60, "0835771416": 54, "0836": 74, "08364": 74, "083706": 86, "083750": 79, "08384324": 109, "083949": 86, "084": [51, 54], "084156": 68, "084184": 69, "0841842065698133": 69, "084212": 73, "084269": 79, "084323": 67, "084337": 96, "08440247": 109, "084430": 60, "084633": 71, "084740": 61, "08533179": 109, "0853505": 54, "085395": 67, "085566": 69, "085592": 74, "085644": 80, "085671": 67, "085965": 79, "086004": 74, "08602774e": 109, "0862": 137, "086264": 69, "08664208": 109, "086679": 95, "08681019": 109, "086889": 71, "0872": 53, "087222": 68, "087561": 67, "087634": 67, "087745": 68, "087947": 82, "088048": 82, "088282": 72, "088357": 82, "08848": 95, "088482": [36, 94], "088504e": 38, "088646": 61, "0886876": 109, "08876317": 109, "08888889": 56, "08898004": 109, "089018": 61, "08904241": 109, "089044": 60, "0894": 53, "08968939": 54, "089964": 76, "08e": 55, "09": [51, 67, 68, 69, 78, 79, 82, 94, 109], "09000000000000001": 95, "090025": 79, "09015": 53, "090255": 82, "090436": 68, "090895": 60, "091179e": 67, "091263": 76, "091391": 125, "09140174": 109, "091406": 126, "091535": 67, "0916": 53, "091824": 68, "091992": 81, "092229": 86, "092247": 82, "092263": 96, "092365": 125, "092919": 127, "092935": 67, "092963": 61, "093006": 60, "093043": 82, "09310496": 125, "093153": 82, "093257": 60, "093316": 60, "093474": 82, "09347419": 82, "0935": 96, "09351167": 96, "093746": 125, "093950": 77, "094026": 77, "094118": 82, "09422": 109, "09427234": 109, "094378e": 67, "094381": 77, "09444444": 56, "094581e": 68, "094829": 96, "094999": 82, "095104": 64, "095654": 67, "095781": 31, "095785": 64, "09603": 137, "096337": 77, "096418": 64, "09642999": 109, "096550": 71, "096576": 60, "096616": 94, "096685": 60, "096688": 68, "096741": 58, "09682314": 96, "096915": 86, "097": 83, "097009": 74, "097157": 86, "0973132": 109, "097468": 69, "09756": 84, "097694": 60, "09779675": 125, "097796750": 125, "098": 55, "098256": 82, "09830758": 85, "098308": 85, "098317": 79, "098319": 82, "0986": 53, "098712": 82, "09879814e": 109, "098901": 74, "099": 83, "099001": 68, "099307": 68, "099485": [96, 97], "099647": 81, "099670": 79, "099731": [67, 68], "09980311": 125, "09988": 140, "099965": 61, "0_": 15, "0ff823b17d45": 56, "0x1747bdd4520": 62, "0x1747bdd6b90": 62, "0x2920d7b7150": 81, "0x7f1e99aa9ee0": 86, "0x7fc81e2c5670": 142, "0x7fc81eb35010": 95, "0x7fc81eb46690": 97, "0x7fc81eb69b80": 95, "0x7fc81ecc41d0": 96, "0x7fc81ecc71d0": 96, "0x7fc81ece4bf0": 96, "0x7fc81ece5a30": 96, "0x7fc81eced7f0": 97, "0x7fc81ecf01d0": 96, "0x7fc81ecf0230": 96, "0x7fc81f039cd0": 95, "0x7fc81f85c0b0": 125, "0x7fc81f85ce60": 125, "0x7fc81fd45d30": 125, "0x7fc81fd47560": 125, "0x7fc81fe0eed0": 126, "0x7fc825696d50": 132, "1": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 101, 102, 104, 105, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141], "10": [7, 8, 9, 10, 12, 14, 16, 17, 19, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 132, 137, 139, 140, 142], "100": [16, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 56, 57, 58, 67, 68, 70, 73, 74, 75, 77, 80, 84, 86, 87, 88, 90, 92, 94, 95, 96, 98, 109, 110, 125, 126, 132, 139, 141], "1000": [25, 32, 34, 52, 58, 59, 65, 66, 71, 72, 73, 75, 76, 78, 79, 83, 85, 86, 89, 96], "10000": [51, 59, 67, 68, 78, 79, 82], "100000e": 79, "100044": 72, "100154": 76, "1001922": 109, "100208": 94, "100356": 69, "10038": 85, "100385": 76, "10039862": [87, 96], "100517": 125, "100715": 67, "10079785": 96, "100807": [67, 68], "100858": 85, "10089588": 82, "100896": 82, "10092": 79, "100923": 82, "100934": 60, "100_000": 80, "101": [9, 10, 26, 53, 83, 94, 109, 140, 141], "10126": 79, "10127930": 125, "101279300": 125, "101324": 60, "1015": [55, 78], "1016": [9, 10, 25, 26, 53], "1016010": 55, "1018": [79, 86], "101998": 64, "102": [90, 92, 94, 139, 141], "10235": 79, "10258": 79, "102616": 69, "102775": 69, "10299": 78, "103": [67, 77, 83, 87, 94, 141], "10307": 125, "1031": 79, "103189": 79, "103293": 60, "10348": 78, "103497": 82, "1038": 79, "103806": 69, "103951906910721": 69, "103952": 69, "10396": 78, "104": [51, 55, 78, 87, 94, 141], "10406": 79, "104087": 67, "1041": 53, "10414": 79, "104237": 60, "104492": 74, "1045303": 54, "104787": 77, "104849": 67, "104957": 60, "105": [15, 54, 74, 77, 94, 141], "105037": 60, "105086": 60, "105318": 82, "1054": 56, "105461": 94, "1055": [53, 84], "105701": 60, "105721": 60, "105869": 61, "106": [56, 94, 141], "10607": [62, 90, 92, 139], "10618": 79, "10637173e": 109, "106391": 125, "1065": [70, 75, 76], "106595": [96, 98], "106746": 82, "106952": 73, "107": [56, 86, 94, 141], "107073": 69, "107156": 74, "107295": 125, "1073": 79, "107413": 67, "10747": [62, 90, 92, 139], "10799": 79, "108": [94, 137, 140, 141], "1080": [16, 19, 53, 54, 77], "10809319": 109, "10824": [62, 90, 92, 139], "108257e": 79, "108259": 64, "10831": [62, 90, 92, 139], "108605": 60, "10878571": 82, "108786": 82, "108870": [96, 100], "109": [67, 94], "109005": 82, "10903": 78, "109069": 125, "109079e": 82, "109273": 77, "109277": 74, "10928": 79, "1093": 70, "109413": 60, "109454": 79, "109470": 68, "1096": [53, 84], "10967": 78, "109853": 60, "109861": 139, "1099472942084532": 65, "10e": [69, 82], "11": [39, 54, 55, 56, 57, 58, 60, 61, 62, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 82, 83, 85, 86, 88, 90, 92, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 132, 139, 142], "110": [94, 141], "110081": 74, "1101": 79, "11019365749799062": 86, "110194": 86, "110359": 77, "110365": 86, "110681": 85, "1107": 79, "11071087": [87, 96], "110717": 125, "110857": 61, "1109": 79, "110902": 69, "110902411746278": 69, "111": [68, 94, 96, 141], "1111": [7, 8, 17, 52, 54, 66, 70, 77, 86, 89, 96, 126, 132, 137], "111164": 81, "11120": 79, "1115212226283039404146505153626871879499": 109, "111521222628303940414650515362687187949916813323742474852585969727478808489962310121416182024353843666776838591929545171925293133364445545660636475889398": 109, "1115212226283039404146505153626871879499168133237424748525859697274788084899679232734495557616570737779818286909710023101214161820243538436667768385919295": 109, "1115212226283039404146505153626871879499168133237424748525859697274788084899679232734495557616570737779818286909710045171925293133364445545660636475889398": 109, "11152122262830394041465051536268718794997923273449555761657073777981828690971002310121416182024353843666776838591929545171925293133364445545660636475889398": 109, "111614": 61, "1117": [70, 75, 76], "1118": 55, "11199615e": 109, "112": [56, 94, 141], "1120": 78, "112036": 60, "112078": 94, "11208236": [88, 110], "1122": 79, "112216": 69, "1129": 79, "113": [7, 94, 141], "113022": 74, "11311": 78, "113149": 74, "113207": 82, "113270": 69, "113415": 79, "113686": 60, "11375": 79, "113780": 77, "113952": 74, "11399": 78, "114": [94, 141], "1144500": 54, "11447": 85, "114530": 71, "1145370": 54, "114570": 68, "11458": 79, "114647": 69, "1147": 53, "1148": 79, "114834": 79, "11488": 79, "11495": 79, "114989": 64, "115": [94, 141], "11500": [78, 142], "115060e": 82, "1151610541568202": 76, "115296e": 79, "115297e": 78, "115326": 60, "1155142425200442": 76, "11552911": 85, "11559": 79, "115636": 68, "11570": 78, "115792e": 79, "115901": 64, "115972": 67, "116": [94, 141], "116027": 69, "116137": 60, "11617": 79, "116191": 60, "116274": 69, "116293": 60, "116569": 79, "1166": [78, 140], "1167": 78, "11673": 79, "11675": 79, "117": [67, 94], "1170": 85, "11700": 142, "117072": 71, "117112": 68, "117242": 82, "11724226": 82, "117366": 82, "11743": 142, "11750": 79, "1176": 53, "1177": 53, "117710": 69, "11792": 55, "11796": 79, "118": 94, "11802": 79, "1182": 55, "11823404": 86, "118255": 82, "1186": 55, "118601": 77, "118604": 60, "11861": 55, "1187245": 109, "1187339840850312": 77, "11879": 79, "118799": 79, "118938e": 96, "118952": 77, "119": [86, 94, 141], "119028": 61, "11932": 79, "11935": 85, "11942111": [96, 100], "119512": 60, "119554": 60, "1197": 109, "119766": 82, "1198": [54, 77], "119981": 61, "12": [21, 24, 25, 37, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 64, 66, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 80, 82, 83, 85, 86, 88, 90, 92, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 132, 137, 139, 140, 141, 142], "120": [57, 58, 76, 87, 94, 141], "12002": 78, "1200x600": 60, "1200x800": 60, "1202": 140, "120468": 82, "12046836": 82, "120567": [71, 72], "1206": 109, "120721": 77, "12097": [7, 8, 17, 54, 70, 77, 89, 137], "121": [79, 94, 141], "1210": 79, "12101": 79, "12105472": 125, "121054720": 125, "1211": 79, "1213405": 54, "121399": 79, "1214": 125, "121584e": 82, "121711": 79, "121774": 73, "121824": 68, "12196389e": 109, "122": [9, 10, 26, 51, 53, 60, 83, 90, 92, 94, 140, 141], "12214": 55, "12223182e": 109, "122408": 69, "122421": 74, "122657": 60, "122777": 125, "122801": 60, "123": [40, 55, 56, 60, 78, 86, 94, 141, 142], "1230": 79, "123192": 86, "12323": 79, "123366": 60, "1234": [51, 52, 53, 62, 65, 66, 83, 84, 89, 95, 109, 125], "12348": 86, "1238": 79, "123917": 79, "123950": [96, 97, 100], "124": 94, "12410": 79, "124306": 76, "124480": 76, "124805": 78, "124825": 68, "125": [94, 141], "12500": 78, "125065": 125, "12539340": 125, "1255": 79, "12579": 79, "1258": 54, "126": [94, 141], "12606": 79, "12612": 79, "126777": 125, "126802": 79, "12689": 79, "127": [9, 94, 141], "127006": 79, "12705095": [110, 125], "12707800": 54, "127214": 61, "1272404618426184": 76, "127337": 74, "12736236": 109, "12752825": 125, "127563": 85, "1277": 80, "127778": 79, "127889": 94, "128": [55, 94, 141], "12802": 55, "12814": 79, "128229": 74, "128300e": 68, "128312": 82, "128408": 77, "1285": 53, "12861": 79, "128651": 68, "129": [77, 94, 141], "129005": 61, "12945": 140, "1295": [53, 79], "129514": 79, "12955": 78, "129606": 67, "129798": 67, "1298": 79, "12980769e": 109, "12983057": 96, "1298310": 109, "13": [10, 12, 14, 25, 26, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 66, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 82, 83, 85, 86, 88, 90, 92, 94, 95, 96, 98, 109, 110, 125, 126, 132, 139, 142], "130": [56, 71, 77, 94, 141], "130122": 85, "13034980e": 109, "130370": 69, "1306": 85, "130795": 60, "130829": 82, "13084": 96, "130863": 60, "13091": 79, "1309844442144665": 76, "131": [94, 141], "13102231": 96, "131024": 76, "13119": 85, "1312": 142, "131211": 79, "1313": [55, 142], "13137893e": 109, "131483": 74, "1318": 53, "131842": 74, "132": [56, 67, 77, 94, 141], "13208": 142, "1321": [78, 142], "132248": 94, "1324": [55, 78], "132454": 59, "132481": 94, "1325": 55, "13257": 78, "132671": 69, "132903": 79, "132982": 67, "133": [56, 90, 92, 94, 140, 141], "13300": 79, "133202": 79, "133421": 79, "13356": 79, "133596": 82, "133813": 60, "133839": 74, "13398": 86, "133f5a": 80, "134": [77, 87, 94, 141], "134037": 61, "1340371": 53, "1341": 55, "134146": 79, "1342": 79, "134211": 82, "1343": 78, "134367": 61, "134542": 67, "134567": 79, "1346035": 55, "134687": 79, "13474": 79, "134765": 79, "134784": 96, "134784e": 67, "1348": 78, "1349": 85, "13490": 79, "135": [56, 94, 96, 141], "13505272": 54, "135142": 68, "135344": 64, "135352": 20, "135379": 125, "135665": 68, "135707": 95, "135856": 82, "13585644": 82, "135871": 77, "135877": 60, "135915": 60, "136": [62, 77, 86, 94, 141], "1360": 55, "13602": 86, "136089": 77, "1361": 79, "136102": 67, "1362430723104844": 76, "13642": 79, "136442": 77, "1366": 80, "136836": 77, "137": [9, 56, 62, 94, 141], "1371": 79, "137165": 96, "137213": 68, "137396": 82, "137427": 60, "137754": 60, "1378": 79, "137999": 96, "138": [94, 141], "1380": 78, "138068": 71, "1380847": 109, "13809": 79, "138264": 86, "138378": 69, "1384": 78, "138473": 60, "1386": 53, "13868238": 125, "138682380": 125, "138698": 125, "1387": 53, "138851": 71, "13893": 79, "139": [86, 94, 139], "139117e": 67, "139491": 125, "139508": 74, "13956": 85, "139759": 60, "1398": 79, "139830": [96, 100], "139893": 61, "1399": 53, "14": [52, 54, 55, 56, 57, 58, 60, 61, 62, 64, 66, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 82, 83, 85, 86, 88, 90, 92, 94, 95, 96, 98, 109, 110, 125, 126, 132, 139, 140, 142], "140": [57, 58, 74, 79, 87, 94, 141], "1400": 79, "14000073": 109, "140073": 67, "140081": 76, "1401": 53, "140118": 61, "14018": 96, "140770": [67, 68], "140833": 69, "140861": 54, "140926": 82, "141": [51, 79, 94, 141], "141002": 68, "141098e": 79, "14114": 85, "14141": 79, "141460": 64, "141546": 125, "141820": 69, "141864": 60, "142": [94, 141], "14200098": 125, "142119": 67, "142270": 59, "142382": 67, "1424": 95, "14268": 96, "14281403493938022": 95, "14289": 79, "143": [90, 92, 94, 141], "143342": 68, "143495": 94, "1435": 79, "143534": 67, "14368145": 125, "144": [94, 141], "14400": 78, "14405": 79, "14406": 79, "144084": 69, "1441": 53, "144137": 58, "144241": 71, "1443": 79, "144500e": 79, "144669": 82, "1447": 79, "144800": 69, "144908": 81, "144971": 78, "145": [94, 141], "145027": 64, "1451070": 109, "145245": 82, "14532650": 125, "145625": 82, "145748": 125, "14587": 79, "146": [94, 141], "146037": 82, "146087": 139, "1461020": 109, "146142808990006": 69, "146143": 69, "14625": 79, "146278": 60, "146435": 68, "1465": 55, "146641": 125, "14667": 79, "1468115": 54, "146973": 69, "1469734445741286": 69, "147": [80, 94, 141], "147015e": 79, "14702": 62, "147121": 82, "14744": 79, "14772": 79, "1479": 79, "14790924": 125, "147909240": 125, "147927": 62, "147928": 60, "14798": 79, "148": [94, 141], "148005": 64, "14803": 79, "148134": [67, 68], "148161": 82, "14845": 62, "1485": 79, "148664": 60, "148750e": [78, 79], "148790": 79, "148802": 79, "149": [94, 141], "1492": 51, "149215e": 68, "149228": 86, "149285": 82, "149364": 60, "149472": 86, "149714": 77, "1497426": 109, "14984": 79, "149858": [35, 94], "149898": 82, "15": [8, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 54, 55, 56, 58, 60, 64, 66, 67, 68, 69, 71, 73, 74, 75, 76, 77, 78, 79, 82, 83, 84, 85, 86, 90, 92, 94, 95, 96, 98, 109, 110, 125, 126, 132, 139, 142], "150": [15, 56, 86, 94, 141], "15000": [55, 78], "150000": 55, "15000000000000002": [69, 79, 82, 95], "150000e": 79, "1502": 54, "150200": 77, "150334": 79, "150408": 54, "150614": 62, "150719e": 78, "151": [94, 141], "151033": 60, "151047e": 71, "151063": 67, "15113": 79, "151636": 69, "151819": 82, "15194": 78, "152": [94, 141], "152034": 79, "152148": [67, 68], "152353": 68, "152706": 96, "15285": 79, "152896": 76, "152926": 59, "153": [86, 94, 141], "1530959776797396": 69, "153096": 69, "153119": 69, "153314": 68, "153332": 60, "15347": 79, "15354": 85, "153587": 77, "153633": 62, "153639": 109, "153935": 68, "154": 94, "15430": 142, "154421": 125, "1545": 79, "154557": 82, "154758": 125, "154828": 69, "154890": 76, "155": [94, 141], "155000": 78, "155025": 82, "155120": 82, "155160": 64, "155174": 64, "155423": 64, "155516": 81, "15556": 79, "1556863": 109, "1557093": 54, "156": [94, 141], "1560": 79, "156021": 82, "156169": 68, "156202": [67, 68], "156317": [67, 68], "156326": 60, "1564": 125, "156545": 125, "156684": 68, "1569": 79, "156969": 69, "157": [68, 94, 141], "157091": 125, "157154": 67, "1576": 79, "157733": 76, "1577657": 54, "157e": 96, "158": [94, 141], "158007": 82, "15815035": 55, "158178": 69, "1582": 79, "1586": 79, "158697": 125, "158726": 94, "1589": 79, "15891559": 82, "158916": 82, "159": 141, "159051": 60, "15915": 61, "15916": [53, 61], "159235": 60, "159386": 85, "1596": 56, "159633e": 68, "159841": 67, "159959": 79, "16": [31, 51, 52, 54, 55, 56, 57, 60, 61, 64, 67, 68, 69, 72, 73, 74, 77, 78, 79, 82, 83, 85, 86, 90, 92, 94, 95, 96, 98, 109, 110, 125, 126, 132, 139, 142], "160": [57, 58, 87, 141], "160354": 61, "1604": 55, "160836": 74, "160932": 69, "161": [56, 140, 141], "161049": 68, "161141": 77, "161198": 81, "161236": 82, "161243": 82, "161269": 67, "161288": [68, 76], "161543": 79, "1619": 55, "162": 141, "16201": 79, "16211": 78, "162153": 82, "1622": 79, "16241": 79, "162436": 86, "162593": 76, "162603": 60, "1626685": 54, "162683": 86, "162710": 69, "1628": 78, "162930": 79, "163": [79, 141], "163194": 82, "163200": 60, "163433": 60, "163566": 79, "163895": 69, "164": [64, 83, 141], "164034": 125, "164467": 78, "164482": 60, "164605": 60, "164608": 82, "164698": 73, "1648": 53, "164801": 82, "164805": 69, "164864": 77, "164963": 60, "165": [51, 141], "16500": 78, "165178": 82, "16536299": 125, "165362990": 125, "16539906e": 109, "1654": 79, "165419": 82, "16553": 78, "165549": 139, "165561": 60, "165707": 64, "16590": 79, "16597": 79, "166": 141, "1661": 78, "166238": 76, "166375": 94, "166517": 74, "167": [55, 78, 141], "167141": 60, "16725": 79, "167547": 82, "167581e": 67, "1676": 79, "167765": 79, "167993": 125, "168": 141, "16803512": 125, "168089": 76, "168092": 125, "1681": [53, 78], "1681332374247485258596972747880848996": 109, "16813323742474852585969727478808489967923273449555761657073777981828690971002310121416182024353843666776838591929545171925293133364445545660636475889398": 109, "168162": 60, "168182": 60, "168195": 85, "168614": 82, "168931": 82, "169": [56, 141], "1691": [53, 79], "16910": 79, "169117": 86, "169196": 82, "169230e": 69, "16951": 79, "16984": 79, "169910": 60, "17": [51, 52, 54, 55, 56, 60, 64, 67, 68, 73, 74, 77, 78, 79, 82, 83, 85, 86, 90, 92, 94, 95, 96, 98, 109, 110, 125, 126, 132, 139, 142], "170": 141, "1704": 79, "170705": 94, "170709e": 68, "17083": 79, "170933": [96, 100], "171": 141, "1712": 140, "1714": 55, "171575": [60, 82], "171606": 60, "171696": 94, "171815": 95, "171833": 68, "171848e": 67, "1719": [60, 61], "171942": 79, "172": [83, 141], "172022": 125, "172083": 68, "172534": 60, "172628": 76, "172793": 82, "173": 141, "173150": 61, "173504": 72, "17372": 79, "1738": 79, "17385178": 95, "173969": 125, "173e": 83, "174": 141, "174106": 85, "174185": 82, "174499": 125, "174516e": 82, "17453": 79, "1746": 79, "174835": 68, "174968": 78, "17499": 79, "175": 141, "1751": 78, "175176": 82, "17522": 79, "175254": 78, "175284": 69, "175369": 68, "175635027": 54, "17576": 79, "175894": 86, "175931": [94, 95, 96], "176495": 82, "17655394": 82, "176554": 82, "176588": 60, "176807": 60, "176929": 125, "177": [140, 141], "177007": 82, "17700723": 82, "177043": [67, 68], "1773": 79, "1774": 53, "177463": 81, "177496": 82, "177611": 82, "177740": 67, "177751": 82, "17778": 79, "177830": 68, "17799": 79, "177995": 82, "178": 141, "178169": 71, "178218": 68, "17823": 56, "178704": 125, "178763": 82, "178934": 125, "179": [71, 141], "179026": 68, "179101": 94, "1795850": 54, "179588e": 82, "179777": 68, "1798913180930109556": 80, "18": [52, 54, 55, 56, 60, 62, 67, 68, 73, 74, 75, 77, 78, 79, 82, 83, 85, 86, 90, 92, 94, 95, 96, 98, 109, 125, 139, 142], "180": [57, 58, 87, 141], "180143": 64, "18015": 79, "180176e": 79, "180262": 68, "180271": 74, "1803": 53, "18030": 79, "180575": [71, 72], "1807": [53, 79], "180794": 61, "1809": 140, "180951": 82, "181": 141, "1812": 79, "1814": 53, "18141": 79, "181446": 125, "181754": 60, "1818210": 109, "182": 141, "1820": 53, "182348": 60, "182427": 68, "182528": 61, "182633": 82, "182849": 82, "183": [56, 60, 78, 96, 141], "183339": 67, "183373": 96, "183526": 69, "183553": 83, "18356413": 96, "18368": 79, "183855": 95, "183888": 77, "184": [56, 140, 141], "184224": 64, "184247": 67, "184303": [96, 100], "184347": 68, "184699": 60, "185": [55, 56], "18500": 79, "185130": 83, "18516129": [96, 100], "1855": 79, "185984": 67, "186": [79, 141], "18604": 79, "186066": 61, "1862": 53, "186237": 68, "18631": 79, "18637": 137, "186589": 64, "18666": 79, "186689": 96, "186735": 82, "186775": [96, 97, 100], "18678094e": 109, "186836": 82, "187": 141, "187019": 61, "187153": 125, "187664": 67, "187682": 60, "187690": 82, "18789": 79, "188": [60, 141], "188175": 82, "1881752": 82, "188223": 82, "188400": 68, "188412": 60, "188541": [96, 97], "1887": [94, 95, 96], "188760": 74, "188870": 60, "18888149e": 109, "188882": 68, "188991": 125, "189": [56, 83, 141], "189138": 60, "189195": 79, "189248": 67, "189293": 79, "1895815": [16, 54, 77], "189737": 82, "189739": 64, "189858": 60, "189927": 79, "189998": 82, "19": [52, 54, 55, 56, 60, 67, 68, 74, 76, 77, 78, 79, 82, 83, 85, 86, 94, 95, 96, 98, 109, 125, 139, 142], "190": [56, 141], "19000": 79, "190096": 125, "190270": 61, "19031969": 82, "190320": 82, "19033538": 54, "190648": 32, "19073905e": 109, "190809": 82, "190892": 86, "1909": [16, 54, 77], "190910": 60, "190915": 69, "19092023": 109, "190921": 72, "190976": 83, "190982": 82, "191": [56, 60, 140, 141], "191192": 67, "1912": 140, "191223": 68, "1912705": 89, "191294": 68, "191534": 78, "191716": 79, "1918": 53, "191906": 61, "192": [60, 141], "1922": 79, "192240": 125, "192505": 81, "192526": 85, "19252647": 85, "192539": [36, 94], "1925785": 109, "192587": 82, "192952": 64, "193": [60, 141], "193060": 82, "193253": 67, "193285": 67, "193308": [36, 94], "193341": 68, "193375": 74, "19338": 60, "19374710e": 109, "19382": 79, "193849": 83, "19385": 79, "193f0d909729": 56, "194": [75, 79, 141], "194092": 67, "1941": 55, "19413": [78, 79], "194303": 68, "194601": 58, "194786": [96, 97], "195": [60, 91, 92, 141], "19508": 86, "19508031003642462": 86, "19509680e": 109, "195377": 82, "195396": 82, "195547": 79, "195564": 77, "19559": [55, 78], "195649": 60, "195761": 82, "195781": 67, "1959": 140, "195963": 74, "196": [60, 141], "196189": 82, "196437": 79, "196478e": 68, "196655": 74, "19680840": 125, "196e": 40, "197": 141, "1970": 79, "197000e": 79, "19705": 79, "197225": [62, 90, 92, 139], "1972250000001000100001": [56, 90, 92, 139], "1974": 79, "197424": 95, "197484": 125, "19756": 79, "19758": 79, "197600": 59, "197711": 79, "197920": 67, "19793": 79, "19794": 79, "198": [60, 141], "198218": 77, "19824": 79, "198351": 82, "198493": 74, "198503": 83, "198549": 62, "198687": 55, "1988": [52, 66, 89, 96], "199": [60, 141], "1990": [55, 78, 79], "1991": [55, 78, 79, 142], "199281e": 82, "199282e": 79, "199412": 68, "199458": 125, "1995": [54, 77], "1998": 80, "19983954": 87, "199893": 71, "1999": [80, 87], "1_": [69, 82], "1d": 21, "1e": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 74, 79], "1f77b4": 59, "1x_4x_3": 59, "2": [7, 9, 10, 11, 12, 13, 14, 15, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 45, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 101, 104, 109, 110, 125, 126, 127, 132, 133, 134, 135, 136, 138, 139, 140, 141], "20": [9, 10, 12, 13, 14, 16, 17, 18, 20, 22, 23, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 55, 56, 57, 58, 60, 67, 68, 69, 71, 73, 74, 75, 77, 78, 79, 82, 85, 86, 87, 88, 89, 94, 95, 96, 98, 109, 110, 125, 126, 132, 139, 142], "200": [11, 14, 15, 25, 53, 57, 58, 60, 69, 70, 75, 81, 82, 87, 89, 95, 140, 141], "2000": [8, 37, 55, 57, 64, 67, 68, 69, 74, 78, 79, 82, 84, 87, 94, 96], "20000": [55, 78], "20000000000000004": [69, 79, 82], "200000e": 79, "200049": 67, "20010": 79, "200110": 79, "2003": [7, 140], "200303": 139, "2005": 58, "20055": 79, "2006": 79, "20073763": 75, "20074": 79, "200840": 60, "201": [56, 60, 79, 141], "2010": [54, 77], "2011": [54, 77, 137, 139], "2012695": 109, "2013": [70, 125, 140], "2014": [125, 140], "2015": [15, 140], "201528": [67, 68], "20158": 79, "2016": 80, "2017": [13, 140], "201700": 60, "201768": 77, "201788e": 79, "2018": [7, 8, 17, 18, 52, 54, 55, 58, 66, 70, 75, 77, 78, 79, 84, 85, 89, 96, 101, 109, 110, 118, 125, 137, 140, 141], "2019": [11, 56, 67, 68, 69, 71, 72, 79, 82, 85, 95, 110, 116, 119, 120, 137, 139, 140], "201e": 83, "202": [60, 141], "2020": [9, 10, 12, 14, 20, 22, 23, 25, 26, 53, 56, 58, 60, 86, 95, 96, 98, 101, 126, 127, 140], "2020435": 54, "2021": [16, 25, 53, 54, 56, 60, 61, 67, 68, 77, 96, 97, 100, 101, 140, 141], "20219609": 54, "2022": [85, 86, 96, 98, 101, 126, 127, 136, 137, 140], "2023": [19, 57, 84, 87, 96, 108, 110, 123, 124, 140], "2024": [51, 65, 70, 75, 76, 80, 83, 86, 96, 137, 140], "2025": [25, 60, 96, 97, 100], "202603": 68, "202650e": 69, "20269": 79, "20274": 79, "202846": 68, "203": [55, 67, 78, 141], "203284": 69, "20329": 79, "2036": 79, "203828": 79, "203832": 60, "204": [60, 141], "204007": 82, "20400735": 82, "204362": 86, "204455": 68, "204482": 82, "204794": 82, "204893": 64, "205": [60, 80, 83, 85, 141], "205187": 69, "205224": 85, "205333e": 78, "2057597": 109, "205938": 77, "206": 141, "206094": 60, "2061": 79, "206253": [78, 79], "206256": 74, "2064": 79, "206614": 82, "207": [83, 96, 141], "2075": 53, "207834": 68, "20783816": 54, "207840": 72, "207885": 78, "207912": 125, "208": [60, 64, 141], "208034e": 79, "2080787": 54, "20823898": 54, "2086": 79, "209": [60, 64], "209014": 82, "209219e": 85, "209257": 20, "209546e": 79, "209894": 82, "209967": 60, "21": [7, 8, 17, 52, 54, 55, 56, 60, 67, 68, 70, 77, 78, 79, 82, 84, 85, 86, 89, 94, 95, 96, 98, 109, 125, 137, 139, 140, 142], "210": [10, 14, 25, 26, 60, 64], "210229": 60, "2103": [79, 137], "2103034": 54, "210319": [67, 68], "210323": 82, "2104": 141, "2107": 140, "210761": 60, "21078": 79, "211": [60, 64, 83, 141], "211002": [96, 100], "21105": [56, 95, 137, 139], "2112": 86, "21142": 79, "211534": 69, "21155656": 82, "211557": 82, "211966": 60, "212": [60, 64, 141], "2122": 79, "212521": 60, "21257396e": 109, "212811": 64, "212844": 77, "212863": 67, "213": [60, 64, 83, 140, 141], "213026": 79, "213070": 68, "213135": 68, "213199": 96, "21361": 79, "213743e": 68, "2139": 12, "214": [60, 80], "214458": 76, "214764": 85, "214769": 74, "215": 64, "215069": 82, "215342": 82, "2155": 79, "21550": 79, "21562": 79, "21573": 79, "215967": 125, "216": 64, "216207": 95, "21624417": 54, "2163": 79, "216344": 82, "21669513e": 109, "216761": 81, "216943": 94, "216988": 60, "217": [60, 64, 83, 140], "21716": 79, "2171802": [54, 77], "217244": 34, "217684": 74, "2179004": 109, "218": [60, 64], "21804": [55, 78], "218383": 64, "218767": 79, "2189": 79, "218938": 79, "219": [9, 10, 26, 53, 64, 140], "2191274": 54, "219585": 64, "2197237644227434": 76, "22": [52, 54, 55, 56, 60, 67, 68, 76, 77, 78, 82, 83, 85, 86, 94, 95, 96, 98, 109, 125, 139, 142], "220": [40, 60, 64, 141], "220088": 79, "220398": 67, "220407": 76, "220772": 82, "220798": 60, "220988": 60, "221": [64, 141], "2213": 77, "2214": 77, "221419": 79, "2215": 77, "2216": 77, "2217": [54, 77], "2218": 140, "222": [60, 141], "2222": [52, 54, 66, 96], "22222": 79, "222261": 94, "22272803e": 109, "222777": 61, "222843": 82, "222882": [68, 76], "223": [80, 83, 141], "223158": 76, "22336235": 54, "223485956098176": [71, 72], "223617": 76, "22375856": 54, "22390": 78, "223928": 76, "224": [60, 83, 141], "2244": 140, "224842": 60, "224897": [67, 68], "225": [25, 53, 60, 87, 140, 141], "225034": 58, "22505965": 54, "22507006e": 109, "225175": 82, "225222": 82, "22522221": 82, "22528": 79, "225350": 68, "225427": 64, "225459760731946": 69, "225460": 69, "225574": 77, "2256": 79, "22562": 79, "225670": 76, "225776": 86, "225899": [96, 100], "226": 141, "2264": 53, "226479": 74, "226524": 82, "226598": 77, "226938": 72, "226969": 64, "227": [60, 79, 141], "2271071": 19, "2276": 53, "2279": 79, "227932e": 78, "228": 60, "228035": 79, "2281": 79, "228365": 60, "228404": 76, "228597e": 68, "228630": 68, "228648": 55, "229": [55, 60, 141], "22925": 79, "22937": 79, "229443": 82, "229452": [94, 95, 96], "229472": 78, "2295": 79, "229759": 95, "2298": 53, "229961": [67, 68], "229994": [67, 68], "23": [23, 43, 47, 54, 55, 56, 58, 60, 67, 68, 75, 77, 78, 79, 82, 85, 86, 90, 92, 94, 95, 96, 109, 125, 137, 139, 140, 142], "230": [25, 53, 140], "230009": [71, 72], "2302": 84, "2307": [54, 77, 84, 89], "2308": 85, "230842": 67, "230956": 59, "231": [7, 141], "23101214161820243538436667768385919295": 109, "23113": 96, "231153": 68, "231269": 60, "231310": 82, "2313356": 109, "231430": 125, "231467": 96, "231598": 61, "231734": 96, "231798": 96, "231986": 82, "231e": 83, "232134": [67, 68], "232157": 68, "2328": 79, "232868e": 68, "232959": [71, 72], "232e": 96, "233": [13, 60], "233029": 67, "233154": 142, "233429": 60, "2335": 53, "233705": 68, "23381687": 60, "234": [60, 140], "234137": 86, "234153": 86, "234157": 61, "234205": 79, "234431": 76, "234534": 69, "234605": 62, "234798": 79, "234910": 77, "235": [140, 141], "235291": 67, "235406": 61, "235641": 60, "2359": 142, "23590": 79, "236008": 69, "236015e": 67, "236309": 79, "236884": 76, "23690345e": 109, "237": 56, "237115": 68, "237200e": 67, "237252": 79, "237313": 60, "237341": 67, "237461": 85, "23748": 79, "23751359e": 109, "237896": 82, "23789633": 82, "238": [54, 77, 141], "238101": 82, "238225": 125, "238251": 69, "238529": 33, "23856": 79, "2385619": 109, "238619": 64, "238794": 82, "239": 141, "239019": 74, "239243": 67, "239267": 76, "239313": 68, "239571": 61, "23965": 79, "23e": 55, "24": [54, 55, 56, 60, 67, 68, 75, 76, 77, 78, 79, 82, 85, 86, 87, 94, 95, 96, 109, 125, 139, 140, 141, 142], "240": 51, "240127": [67, 68], "240146": 68, "240160": 60, "240295": 85, "240338": 61, "240532": [67, 68], "240660": 60, "2407": 53, "24080030a4d": 56, "240813": 73, "241049": 82, "241064": 68, "241503": 94, "241573": 60, "2416": 53, "241609": 79, "241645": 68, "241678": 67, "241827": 68, "241962": 86, "24199": 79, "241e": 83, "242": 140, "242000": 79, "242124": [78, 79], "242139": 125, "242158": [78, 79], "2424": 74, "242427": 74, "2424596822": 72, "242815": 125, "242902": 82, "243056": 61, "2430561": 53, "243246": 82, "243771": 60, "2438": 79, "2439": 79, "243e": 83, "244": [40, 79], "244090": 79, "244455": 82, "244622": 125, "24469564": 139, "245": [140, 141], "245062": 82, "2451": 53, "24510393": 55, "245370": 77, "245481": 60, "245512": 82, "245531": 67, "245720": 59, "245865": 60, "246": 141, "246624": 94, "2467506": 54, "246753": 82, "246879": 82, "247": [83, 141], "247020": 69, "247057e": 82, "2471": 79, "2472": 79, "247617": 94, "247717": 79, "24774": [78, 79], "247826": 77, "247977": 67, "248171": 82, "248441": 94, "248638": 69, "249": [54, 77, 80, 141], "2491": 79, "24917": 79, "249189": 60, "249601": [96, 97, 100], "2499": [91, 92], "249986": 76, "25": [9, 10, 14, 15, 16, 17, 25, 26, 36, 37, 54, 55, 56, 59, 60, 67, 68, 69, 70, 75, 76, 77, 78, 79, 80, 82, 86, 87, 94, 95, 96, 109, 125, 139, 142], "250": [80, 141], "2500": [79, 91, 92], "25000000000000006": [69, 79, 82], "250073": 79, "250158": 60, "250210": 69, "2503": 79, "250354": 82, "250425": 69, "250584": 60, "251": [78, 79, 85], "251412": 68, "251480": 68, "251953": 79, "252133": 79, "252253": 85, "25240463": 96, "252524": 82, "252554": 60, "252601": 125, "253026": [67, 68], "2532": 79, "253437": 81, "253675": 78, "253724": 82, "25374": 79, "254": [79, 141], "25401679": 54, "254035": 76, "254038": 72, "254083": 68, "2543": 79, "254324": 69, "254400": 125, "255": [79, 141], "255034e": 68, "255995": 67, "256": [79, 95], "256082": 94, "256416": 82, "256567": 77, "25672": 79, "256944": 82, "256992": 79, "257019": 68, "257207": 54, "257349": 60, "257377": 59, "257523": 67, "2576849": 109, "258083": 68, "258158": [67, 68], "258191": 60, "2583": 79, "258522": 67, "258541e": 37, "258951": 82, "258980": 60, "259": 51, "259164": 68, "259367": 94, "259395": 73, "2594": [55, 78], "259828": [67, 68], "259875": 68, "25x_3": 59, "26": [54, 55, 56, 58, 60, 62, 67, 68, 75, 77, 78, 79, 87, 90, 92, 94, 95, 96, 109, 125, 139], "26016": 79, "260161": [35, 94], "260211": [67, 68], "260356": 78, "260360": 82, "260738": [96, 100], "260762": 64, "261": 83, "2610": 79, "2613": 79, "261624": [78, 79], "261685": 79, "26175": 79, "261777": 79, "261903": 77, "2619317": 54, "261961": 60, "262423e": 79, "262621": 77, "262829": 109, "263": [7, 79, 141], "2633": 79, "263821": 60, "263942e": 68, "263974e": 82, "264": [140, 141], "264086": 59, "264274e": 79, "264790": 60, "264884": 79, "265": 141, "2651": 96, "265119": 81, "2652": [56, 78, 79], "265547": 79, "265744": 76, "2658": 72, "265820": 60, "265910": 60, "265929": 83, "266": 141, "266147": 83, "266686": 67, "266922": 125, "267": 80, "2670691": 54, "267500": 77, "267581": 79, "267607": 60, "267950": 82, "268": 141, "268055": 79, "268064": 60, "268343": 76, "268558": 60, "268628e": 68, "268942": 82, "268943": 67, "268998": 55, "269043": 82, "269112": 96, "269674": 60, "269977": 79, "26bd56a6": 56, "26e": 55, "27": [10, 14, 25, 26, 52, 54, 55, 56, 57, 58, 60, 62, 67, 68, 75, 77, 78, 79, 87, 90, 92, 94, 95, 96, 109, 125, 139, 140], "270": 141, "2700": 56, "270248": [96, 100], "270644": [67, 68], "271": 141, "271004": [78, 79], "271083": 79, "271183": 78, "271556": [96, 97], "272296": 79, "272332e": 67, "272408": 68, "272662": 79, "273": 56, "273087": 60, "273299": 68, "273356": 69, "27371": [55, 78], "27372": [55, 78], "274": [56, 79], "274066": 60, "2740991": 53, "274251e": 78, "274267": 77, "27429763": [96, 98], "27461519": [96, 100], "274637": 60, "274793": 82, "274825": [36, 94], "27487": 79, "2754": 53, "275469": 60, "275596": 125, "276": [56, 141], "27605804": 109, "276083": 60, "276148": 82, "276189": 60, "276189e": 77, "2764": 79, "2766091": 55, "27713": 79, "277188": 60, "277299": 62, "27751": 79, "277512": 68, "277561e": 77, "277968": 82, "278": [51, 85, 141], "2780": 54, "278000": 77, "278035": 64, "278391": 79, "278434": 71, "278454": 74, "2786": 125, "278804": 68, "279": 141, "27951256e": 109, "279595": 64, "27986": 79, "279933e": 68, "28": [54, 55, 56, 60, 61, 67, 68, 70, 75, 77, 78, 87, 94, 95, 96, 109, 125, 139, 141], "280081": 60, "280196": 72, "280454dd": 56, "280514": 125, "280963": 81, "281": [83, 141], "281024": 82, "28111364": 55, "2815": 79, "2818": 53, "2819": 125, "282": [83, 140, 141], "282200": 72, "2825": [137, 139], "28251": 79, "282870": 79, "2830": [137, 139], "283041": 67, "283207": 67, "28326": 79, "283386": 67, "283532": 61, "2836": 53, "2836059": 54, "283612": 60, "28382": 79, "283974": 82, "283992": 67, "283994": 82, "283e": 83, "284": 141, "28425026": 85, "284271": 73, "284397": 142, "284492": 60, "28452": [55, 78], "2847904": 109, "2849": 79, "284987": 79, "285": [83, 96, 141], "285001": 64, "285483": 74, "285e": 83, "286": 141, "286203": 67, "286371": 67, "2865": [53, 79], "286507": 69, "286563e": 79, "286593": 79, "287041": 82, "287123": 94, "287196": 67, "2873530": 109, "287815": 85, "287926": 82, "288": 80, "288976": 79, "289": 140, "289042": 60, "289062": 78, "289357": 68, "28943039": 109, "289440": [67, 68], "289555": 74, "289901": 60, "29": [54, 55, 56, 60, 67, 68, 75, 77, 78, 85, 87, 94, 95, 96, 109, 125, 139], "290": 96, "290565": 68, "290736e": 68, "290901": 64, "290987": 78, "291": [79, 83], "2910": 79, "291008": 67, "291011": [96, 98], "291071": 82, "29107127": 82, "291405": 82, "291406": 82, "291434": 68, "291500e": [78, 79], "291517": [67, 68], "29168951": [96, 100], "291963": 82, "292": 81, "292028": 69, "292047": 125, "292105": 82, "292302995303554": 69, "292303": 69, "2925": 56, "2927": 79, "292997": 82, "29299726": 82, "293218": 82, "293617e": 79, "294067": [67, 68], "294123": 94, "294449": 67, "295": 140, "295293": 60, "295307": 67, "295481": 82, "29548121": 82, "295532": 60, "295626": 60, "295837": [62, 90, 92, 139], "2958370000000100000100": [56, 90, 92, 139], "2958370001000010011100": [56, 90, 92, 139], "2958371000000010010100": [56, 90, 92, 139], "2959182": 109, "296099": 64, "296228": 79, "296729": 77, "29678199": [88, 110], "296901": 67, "297276": [96, 100], "297287": [67, 68], "2973": 79, "297349": [71, 72], "297682": 82, "297687": 79, "297705": 60, "297749": 79, "29784405": 85, "298": [13, 56], "298076": 67, "298120": 69, "298228e": 79, "299": [56, 83], "299090": 60, "2992278": 109, "299370": 60, "299537": 72, "299712": 71, "2999": 64, "29999": 60, "2_": [19, 57, 87, 126, 127, 136], "2_x": [19, 57, 87], "2d": [21, 110, 119], "2dx_5": [69, 82], "2e": [51, 53, 54, 55, 56, 57, 95, 96, 110, 125, 139], "2f": 73, "2m": [126, 132, 136], "2n_t": 59, "2x": 82, "2x_0": [11, 67, 68, 71, 72], "2x_4": 59, "3": [9, 10, 11, 12, 13, 14, 17, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 101, 109, 110, 125, 126, 132, 137, 138, 139, 140, 141], "30": [11, 51, 52, 54, 56, 57, 58, 60, 61, 64, 65, 66, 67, 68, 69, 75, 77, 78, 79, 82, 83, 87, 94, 95, 96, 109, 125, 139], "300": [51, 52, 66, 69, 79, 82, 89, 140], "3000": 64, "30000": 60, "30000000000000004": [69, 79, 82], "300031": 67, "30031116e": 109, "300892e": 68, "30093956": 85, "3009945": 109, "301": 56, "301366": 125, "301371": 82, "3015650": 109, "3016": 78, "301737": 67, "30189": 79, "302": 142, "302149": 67, "302357": 82, "302382": 76, "302648": 77, "303007": 67, "303146": 60, "303324": 77, "303489": 82, "303613": 82, "30361321": 82, "30383": 79, "303835": 77, "303f00f0bd62": 56, "304061": 60, "304113": 60, "304130": 82, "304159": 82, "304201": 59, "30527": 79, "305341": 82, "305612": 77, "305775": 82, "305b": 56, "306127": 60, "306297": 67, "306316": 60, "30645": 79, "30672815": 54, "306915": 77, "306963": 82, "307407": 82, "308": 79, "308568": 68, "308774": 67, "30917769": [71, 72], "309539": 76, "309772": 77, "309823e": 79, "30982972": 82, "309830": 82, "31": [54, 55, 56, 57, 60, 61, 67, 68, 75, 77, 78, 79, 87, 94, 95, 96, 109, 125, 139, 142], "310000e": 79, "310059": 60, "310145": 76, "310492e": 80, "310761": 81, "311": 83, "311253": 79, "311321": 68, "311667": 68, "311712": 71, "311869": 94, "3120": 79, "312135": 60, "312652": 83, "313": [96, 142], "313056": 125, "313209": 69, "313324": 79, "31337878": 79, "313535": 82, "31378": 56, "313870": 74, "314": 109, "3141": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 56, 57, 62, 77, 88, 90, 92, 94, 95, 96, 110, 125, 139], "314247": 86, "314341": 67, "3146": 37, "314625": 68, "314651": 71, "31476": [78, 79], "315": 142, "315031": 86, "315036": 67, "3151": 79, "315155": 68, "315290": [71, 72], "315291": 60, "315310": 67, "315769e": 67, "316": 56, "316193": 82, "31632": 79, "316407": 96, "316540": 77, "3166987": 109, "316717": [67, 68], "316826": 67, "316863": 68, "317231": 60, "317394": 59, "317487": 82, "317607": 82, "318": 56, "318000e": 79, "318438": 79, "318465": 60, "318552": 79, "318584": 125, "318753": [71, 72], "319": 56, "3190975": 60, "319100": [71, 72], "31910229": [96, 100], "319238": 60, "319420": 74, "319759": 82, "319850": 82, "32": [39, 54, 55, 56, 60, 61, 67, 68, 75, 76, 77, 78, 79, 87, 94, 95, 96, 98, 109, 110, 125, 139], "320": [51, 79], "320314": 78, "320633": 69, "320636": 60, "321686": 125, "322": 80, "322186e": 67, "32236455588136": 58, "322404": 85, "322751": 68, "3234": 79, "323636": 78, "323679": 77, "323810": 60, "324": [55, 79], "324518": 81, "32458367": 54, "3245837": 82, "325056": 82, "325090": 79, "325486": 67, "325599": 67, "326": 83, "326148": 68, "326510": 60, "326721": 68, "326740": 82, "32674263": 61, "326871": 86, "3268714482135234": 86, "327257": 67, "327803": 94, "327958": 64, "328322": 60, "328471": 67, "32875335": 61, "328763": 60, "32903572": 60, "329339": 58, "32950022e": 109, "329964": 61, "33": [54, 55, 56, 60, 67, 68, 71, 75, 76, 77, 78, 79, 87, 94, 95, 96, 109, 125, 139, 140], "3300": [55, 78], "330068": 67, "330100": 67, "330143": 82, "33014346": 82, "330163": 68, "330285": [67, 68], "3304269": 54, "330615": 82, "330731": [36, 94], "330921": 80, "331365": 71, "331521": 82, "331602": 79, "33175566": 82, "331756": 82, "332074": 60, "332502": 68, "332782": [36, 94], "3329": 79, "332996": 77, "333174": 60, "3333": [52, 54, 66, 94, 95, 96], "3333333": 56, "33333333": [60, 61], "33335939e": 109, "3335": 79, "333581": 78, "333655": 67, "333704": 68, "333955": 68, "334": 55, "334024": 60, "3342304": 109, "334750": 69, "33500": 79, "335176": 79, "335446": 64, "335609e": 82, "335846": 82, "335853": 79, "33613": [96, 97], "336382": 68, "336461": 79, "336612": 59, "336886": 60, "337": 80, "337380": 82, "3376": 53, "337619": 58, "338": 85, "33849": 79, "3386": 109, "338603": 67, "338775": 69, "338908": 69, "339269": 85, "33928": 79, "339443": 68, "339570": 82, "339875": [71, 72], "34": [52, 53, 54, 55, 56, 57, 60, 61, 67, 68, 72, 75, 77, 78, 79, 85, 87, 94, 95, 96, 109, 125, 142], "340": [55, 79], "340029": 68, "340142": 96, "340235": 74, "340274": 85, "340458": 60, "341336": 34, "341472": 76, "341755e": 67, "3420": 79, "342117": 74, "342362": 64, "342632": 76, "342675": 54, "342743": 80, "34287815": 85, "342989": 79, "342992": 77, "343": [79, 83], "343196": 60, "343639": 94, "343685": 67, "34375": 78, "343828": 67, "344147": 60, "344212": 142, "344440": 94, "34450402": 61, "344505": [78, 79], "344640": 82, "344787": [67, 68], "344834": 59, "345065e": 79, "345177": 60, "345381": 69, "3453813031813522": 69, "3454": 79, "345852": 68, "345903": 82, "345989": 67, "3460": 109, "346033": 60, "346107": 78, "346206": 82, "346238": 85, "346269": 68, "346678": 81, "346964": 67, "347310": [36, 94], "347696": 69, "34769649731686": 69, "347929": 79, "348": 83, "348319": 68, "34858240261807": 58, "348617": 82, "348622": 83, "348700": 68, "348980e": 68, "349213": 61, "3492131": 53, "349383": 77, "34943627": 75, "349638": 68, "34967621": 54, "349772": 72, "35": [51, 55, 56, 60, 67, 68, 69, 77, 78, 79, 80, 82, 94, 95, 96, 109, 125, 126, 132, 142], "3500000000000001": [69, 79, 82], "350165": 95, "350208": 67, "350518": 82, "350712": [71, 72], "35077502": [126, 132], "351220": 68, "351629": 79, "351766": 81, "35186677": 60, "352": [55, 77, 80], "352250e": 78, "352259e": 79, "3522697": 54, "35292": 79, "352990": 79, "352998": 79, "353105": 37, "353412": 82, "35341202": 82, "35365143": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "353748e": 82, "3538": 53, "354": 79, "354188": 59, "354371": 82, "354688": 38, "354915": 60, "355065": 64, "355209": 82, "355627": 94, "355651": 68, "355953": 60, "3560647": 109, "356136e": 79, "356167": 72, "356183": 79, "35620768e": 109, "3564": 79, "3565": 79, "356677": 60, "3568": 96, "357": 79, "357166": 60, "357170": 67, "35731523": [96, 98], "358158": [78, 142], "358185": 60, "358289": 77, "358395": 85, "358799": 125, "359": [83, 142], "359100": 79, "359161e": 67, "359229": 64, "3593": 85, "359307": 68, "35th": 140, "36": [51, 55, 56, 61, 67, 68, 77, 78, 94, 95, 96, 109, 125], "360004": 82, "360065": 125, "360249": 73, "360475": [67, 68], "360572": 68, "360655": 79, "360683": 69, "360801": 69, "361": 83, "361274": 61, "361518": 69, "361518457569366": 69, "361521": 38, "361623": 74, "3619201": 12, "362157": 67, "36231307e": 109, "362447": 60, "363276": 54, "364221": 67, "3643": 125, "364450": 60, "364591": 61, "364595": 54, "3647": 56, "364800": 82, "36501": 79, "365548": 60, "365551": 68, "36557195e": 109, "36566025e": 109, "366": 79, "36616": 79, "366310": 67, "366529": 81, "366718627": 54, "366950": 67, "36696349": [96, 100], "367": [40, 83], "367181": 67, "367323": 82, "367366": 74, "367398": 76, "367571": 69, "367625": 82, "368": [60, 61], "368152": 77, "3682": [55, 78, 79], "368324": 77, "368499": 69, "3684990272106954": 69, "368577": 94, "369556": 69, "3696": 85, "369796": 82, "369869": 78, "369981": 77, "37": [55, 61, 67, 68, 74, 77, 78, 79, 94, 95, 96, 109, 125], "370254e": 78, "3702770": 54, "370736": 77, "3707775": 54, "370908": 67, "3710": 79, "371357": [78, 79], "371429": 69, "371850e": 68, "372": 140, "37200": [78, 79], "372097": 69, "3722": 79, "37231324": 87, "372373": 60, "3724": 79, "372415": 80, "372427": 68, "372487": 61, "3727679": 54, "373218e": 76, "373343": 60, "373451": 94, "3738573": 54, "373966": 80, "374027": 60, "374364": 82, "37436439": 82, "3745": 79, "374821e": 79, "374862": 67, "374917e": 67, "375081": 79, "375274": 67, "375465": 82, "375578": 60, "375621": 76, "375844": 76, "376355": 60, "3766": 74, "376617": 74, "376760": 68, "376806": 68, "377060": 79, "377147": 94, "377311": 82, "377669": 68, "378351": 32, "378588": 67, "378596": 77, "378610": 60, "378688": 82, "378727": 67, "378834": 82, "378866": 60, "3788859": 54, "379": 140, "379038": 82, "37939": 79, "379614": 82, "379626": 67, "379981e": 68, "38": [56, 67, 68, 78, 94, 95, 96, 109, 125], "3800694": 54, "380837": [78, 79], "381": 83, "381072": 82, "381603": 67, "381685e": [78, 79], "381689": 82, "3817": 79, "382286": 79, "382582e": 31, "382872": 69, "38292749": 61, "383297": 82, "383531": 76, "3838090": 109, "384": 79, "384241": 60, "384443": 68, "384677": 64, "38470495": [96, 100], "384707": 60, "384777": 79, "384865": 68, "384928": 67, "3851": 79, "385160": 68, "385240": 125, "385917": 77, "385958": 60, "386": [56, 79], "386102": 69, "386502": 79, "386831": 64, "386988": 58, "387": 56, "3871": 53, "387374": 61, "387426": 82, "3876430": 109, "387780": 82, "388026": 67, "388071": 82, "388185": 64, "38818693": 109, "388216e": 95, "388668": 82, "38866808": 82, "388871": 79, "389": 56, "389126": 96, "389164": 76, "3893250": 109, "389489": [96, 97], "389566": 81, "38973512e": 109, "389755": 67, "38990574": 109, "39": [51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 64, 67, 68, 72, 73, 75, 77, 78, 79, 85, 86, 87, 94, 95, 96, 109, 125], "39010121e": 109, "390379": 82, "391377": 86, "392128": 68, "39215871": 109, "392242": 73, "392248": 80, "39236801": 75, "392400": 79, "392623": 68, "392752": 58, "392833": 85, "392864e": [78, 79], "392917": 67, "393604": 69, "393654": 64, "394226": 68, "39425708": 54, "394616": 60, "394886": 60, "395076e": 79, "395136": 77, "395268": 94, "395569": 67, "395603": 67, "395768": 60, "3958": 96, "395889": 79, "396": [40, 96], "39611477": 55, "3961673": 109, "396173": 71, "39621961e": 109, "396300": 71, "3964": 79, "396531": 79, "396985": 77, "396992": [67, 68], "397140": 69, "397155": 68, "39727": 79, "397313": 53, "397485": 60, "397536": 76, "397578": 73, "3975933": 61, "397811": 85, "3979": 61, "398": [51, 90, 92, 139], "398166": 64, "398272": 60, "3985": 79, "398770": 82, "398844": 60, "398999": 94, "399": 55, "399056": 82, "399223": 59, "399343e": 67, "399355": 59, "399494": 60, "399679": 96, "399692": 82, "3997933": 61, "399858": 86, "3cd0": 56, "3dx_1": [69, 82], "3e": 109, "3e1c": 56, "3ec2": 56, "3f5d93": 80, "3x_": 82, "3x_4": [69, 82], "4": [10, 14, 19, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 101, 109, 110, 125, 126, 132, 139, 141], "40": [54, 57, 58, 67, 68, 69, 76, 78, 79, 82, 87, 90, 92, 94, 95, 96, 109, 125, 126, 132], "400": 77, "4000": 60, "4000000000000001": 95, "40000000000000013": [69, 79, 82], "400113": 74, "4001458": 109, "40029364": [126, 132], "400587": 60, "400823": 82, "400855956463958": 69, "400856": 69, "400905": 64, "401": [7, 142], "401247": [110, 125], "40127723e": 109, "401690e": 68, "401931": [71, 72], "402077": 79, "402113": 125, "402301e": 95, "402619": 39, "402902": 79, "403": 83, "403425": 82, "403626490670169": 88, "4036264906701690": 88, "403626491": 88, "403715": 31, "403771948": 110, "4039": 53, "40391239": 61, "40402068": 61, "40403239": 61, "404267": 67, "404300": 64, "404318": 53, "404411": 67, "40452": 79, "404550": 81, "405050": 68, "405203": 59, "405374": 79, "40583": 53, "405890": [36, 94], "406285": 82, "406446": 69, "4065173": 109, "40676": 53, "407": 83, "407036": 60, "40732": 74, "407558": 67, "407565": 67, "4076865": 109, "408476": [126, 132], "40847623": [126, 132], "408479": 77, "408509": 68, "408539": 82, "408565": 82, "4089366": 109, "409154": 53, "4093": 85, "409328": 79, "409395": 82, "409592": 61, "409746": 69, "409848": [67, 68], "41": [67, 68, 78, 79, 94, 95, 96, 109, 125], "410100": 67, "410393": 69, "410667": 94, "410681": 59, "410682": 67, "410795": 77, "41093655": 109, "411125": 61, "411146e": 68, "411190": [67, 68], "411225": 60, "411291": 81, "411295": 82, "411304": [67, 68], "41143808": 61, "411447": 79, "411582": 82, "41168096": 61, "411768": 68, "412004": 71, "412127": 82, "412304": 86, "412477": 59, "412592": 60, "412653": 77, "412714": 69, "412726": 68, "412941e": 68, "413247e": 67, "41336": 95, "413376": 96, "41341040": 54, "413608": 82, "414": 83, "414073": 33, "414533": 68, "414714": 60, "41520223": 61, "41525168e": 109, "415375": 67, "41546114": 61, "415556": 94, "41566": 96, "415812": 142, "415988": 79, "416052": 64, "416132": 68, "4166": 79, "4166667": 56, "416757": 82, "416899": 67, "416919": 68, "416e": 83, "417": 80, "417640": 67, "417651": 60, "417727": 78, "417736": 76, "417767": [71, 72], "417834": 64, "41798768e": 109, "418": 40, "418056": 82, "41805621": 82, "418400": 74, "418741": 64, "418806e": 69, "418969": 94, "419093": 60, "41918406e": 109, "41926412": 61, "419371": 82, "419871": 64, "41989983e": 109, "4199952": 54, "41e5": 56, "42": [14, 20, 23, 24, 57, 58, 59, 61, 64, 66, 67, 68, 69, 71, 72, 73, 74, 75, 79, 81, 82, 85, 86, 87, 91, 92, 94, 95, 96, 97, 98, 100, 109, 125, 140], "4200": 79, "420316e": 79, "420608e": 85, "42073312": 54, "42094064": [96, 100], "420967": 69, "421083": 53, "4211349413": 54, "421163": 67, "421200": 86, "421297e": 68, "421357": [71, 72], "421576e": 79, "421581": 61, "421793": 85, "421919": 79, "422": 109, "422007": 85, "422266": 79, "422293e": 94, "422325": 69, "422591": 68, "422597": 80, "42338": 79, "42350776": 61, "4235839": [71, 72], "42367981": 61, "42386701": 61, "42388745": 87, "423921e": 94, "423951": 53, "424108": 69, "424127": 109, "42412729": 54, "424292": 67, "424328": 82, "424651": 96, "424717": 69, "424748": 86, "424995": 60, "425": 77, "425103": 53, "425208": 79, "425325": 74, "425493": 53, "42550": 79, "42563329": 61, "425636": 74, "426055": 53, "426453": 60, "426540": 77, "426540301": 54, "426736": 79, "427": 79, "427486": [67, 68], "42755087": 85, "427551": 85, "427573": 77, "427654": 74, "427725": 82, "428": [51, 125, 142], "428046": 81, "42811700": 142, "428255": 82, "428411": [78, 79], "428467": 82, "4284675": 82, "42848993": 61, "428771": [36, 94], "4290": 53, "429057": 68, "429230": 67, "429705": 67, "42ba": 56, "43": [55, 61, 64, 67, 68, 94, 95, 96, 109, 125], "430298e": [78, 79], "430465": [96, 100], "430595": 68, "430608": 67, "431061e": 67, "4311947070055128": 95, "431253": 76, "431306": 82, "431701914": 137, "431998": 64, "432130e": 78, "432300e": 82, "43231359e": 109, "4324876": 109, "432707": 83, "43294": 56, "432f": 56, "433": [56, 83], "433221": 69, "4336": 79, "43374433": 87, "433750": 67, "433753": 76, "4339": 53, "434054e": 74, "434121": 76, "434535": 82, "43453524": 82, "435": 56, "43503345": 96, "43511": 79, "435401": 77, "4357": 79, "435927": 79, "435967": 77, "43597565": 82, "435976": 82, "436": [56, 79], "436016": 76, "4360257": 109, "43627032": 58, "436327": 79, "436394": 76, "436764": 83, "436806": 79, "436817": 76, "437544": 60, "437667": 78, "437896": 60, "437924": 79, "438": 77, "438219": 82, "438289": 79, "438569": 79, "438578e": 79, "43883": 72, "438834": 68, "4389": 79, "438960": 77, "439401e": 68, "439541": [78, 79], "439675": 83, "439699": 64, "43989": 94, "43991427": 61, "439958": 76, "43f0": 56, "44": [58, 61, 64, 67, 68, 94, 95, 96, 98, 109, 125], "440320": 79, "440605": 95, "440747": 67, "440a": 56, "441153": 82, "441209": 82, "441219": 71, "44124313": 96, "441282": 67, "4416552": 54, "441849": 67, "442847": [96, 97], "443016": 69, "443032": 78, "44312177": 55, "443672": [96, 100], "443686": 82, "4437": 79, "443e": 83, "444040": 60, "444046": 79, "4444": [52, 54, 66, 96], "444500": [78, 79], "444850": 79, "4449272": 79, "445": 83, "445476": 67, "44563945e": 109, "446114": 61, "4461928741399595": 69, "446193": 69, "4462": 56, "44647451": 85, "44713577e": 109, "447266": 60, "447492": 79, "447624": [67, 68], "447706": 69, "447849": 58, "447909": 60, "448": 79, "448252": 68, "448456e": 68, "448569": 67, "448587": 69, "448745": 82, "448842": 68, "4489": 79, "44890536": 109, "448923": 73, "449107": 23, "449150": [36, 94], "44922": 109, "44950": 79, "449677": 74, "44fa97767be8": 56, "45": [67, 68, 69, 71, 73, 76, 79, 82, 94, 95, 96, 109, 125], "4500": 78, "45000000000000007": [69, 79, 82, 95], "450031": 76, "450152": 77, "450762": 60, "450812e": 68, "450870601": 54, "451051": 60, "451312e": 67, "4514296": 109, "45171925293133364445545660636475889398": 109, "452": 56, "452091": 79, "452114": 94, "452488701": 54, "452489": 77, "452623": 68, "453": 56, "453279": 67, "4535": 79, "4539": 56, "454081": 79, "454397": 82, "454406": 64, "45467447": 109, "455": 56, "45500": 79, "455078": 69, "455091": 68, "455107": 69, "455120": 82, "4552": 56, "455293": 69, "4552b8af": 56, "455448": 85, "455672": 79, "455981": 126, "4563551": 109, "456370": 77, "456458e": 67, "4566031": 125, "45660310": 125, "4567": 85, "456892": 69, "457": 109, "457088": 82, "457667": 79, "458114": 79, "458307": 127, "458368": 60, "458420": 79, "4584447": 54, "458784": 67, "458814": 74, "458855": 55, "4592": 54, "459200": 77, "45922123": 61, "459383": 69, "459418": 68, "459436": 76, "45957837": 109, "459760": 79, "459812": 69, "46": [61, 67, 68, 73, 75, 94, 95, 96, 109, 125], "460": [51, 79], "4601": 79, "460207": [67, 68], "460218": 69, "460289": 82, "460744": 78, "4610": 142, "461227e": 67, "461314": 61, "461412": 94, "461629": 86, "462451": 69, "462567": 68, "462979": 67, "463325": 82, "4634": 79, "463418": 86, "46364454": 109, "463668": 79, "463766": 72, "463816": [96, 100], "463857": 79, "463903": 68, "463b": 56, "464": 96, "464076": 69, "464284": 77, "46448227": 109, "464668": 34, "465": 64, "46507214": 85, "465424": 76, "465649": 86, "465730": 86, "4659651": 88, "465965114589023": 88, "4659651145890230": 88, "466047": 82, "46618738": 109, "466440": 69, "466756": 82, "4669655": 109, "467": 79, "46709481": 109, "46722576e": 109, "467613": 77, "467613401": 54, "467681": [67, 68], "467770": 69, "468072": 68, "468075": 82, "46807543": 82, "46811985": 82, "468120": 82, "468406": 79, "468449": [96, 100], "468907": 64, "468919": 79, "468d": 56, "469": 56, "469474": 86, "469630": 60, "469676": 64, "469825": 69, "469895": 68, "469905": 68, "47": [55, 58, 64, 67, 68, 78, 85, 94, 95, 96, 109, 125, 141], "470055": 68, "470904": 67, "471": 64, "4711822": 109, "471435": 83, "471532": 61, "471622": 67, "472": 79, "47222159": 87, "472255": 79, "472699": 68, "472891": 82, "472e": 56, "473099": 69, "47319": 96, "47419634": 139, "474214": [71, 72], "474731": 94, "475304": 79, "475517": 76, "475569": 67, "475e": 83, "476856": 69, "4770292": 109, "477130": [67, 68], "477150": 82, "477247": 68, "477357": 83, "477474": 77, "47759584": 109, "47761563": 58, "478032": 79, "478059": 68, "478064": 68, "4781": 79, "47857478": 109, "479270": 60, "4792861": 109, "479655": 76, "47966100e": 109, "479722": 68, "479860": 79, "479876": [71, 72], "479882": 68, "479887": 61, "479928": 82, "479959": [96, 100], "47be": 56, "48": [56, 64, 67, 68, 72, 78, 79, 94, 95, 96, 109, 125], "480": 64, "480133e": 82, "480199": 74, "48029755": 85, "480579": 68, "4806459": 109, "48069071": [88, 110, 125], "480691": [110, 125], "480800e": 82, "481172": 82, "481218": 79, "481399": [78, 79], "481705": 96, "481713": 74, "481761e": 79, "482": [56, 64], "482012": 71, "482038": 69, "48208358": 82, "482084": 82, "482179": 67, "482251": 39, "482461": [126, 132], "48246134": [126, 132], "482483": 82, "482616": 76, "482790": 59, "482898e": 68, "482903e": 60, "48296": 85, "483": [83, 96], "48315": 85, "483186": 59, "483192": [78, 79], "48331": 85, "483333": 60, "4835": 79, "483711": 82, "483717": 69, "48390784": 96, "48404": 54, "484303": 68, "4845": 79, "484640": 82, "484730": 60, "4849": 56, "485": [56, 79], "485197": 67, "48550": 86, "485617": [78, 79], "485812e": 79, "48583": [78, 79], "485871": 72, "485963e": 60, "486": [15, 79], "486178e": 67, "486202": 69, "486532": 82, "48661": 79, "487": [51, 64, 79], "4873901": 109, "487467": 79, "487524": 74, "487641e": 82, "487793": 68, "487872": 65, "488394": 67, "488460": 79, "488485": 79, "48863": 61, "48873663": 58, "488811": 82, "488909": [78, 79], "488982e": 69, "489429": 60, "489488": [96, 100], "4895498": 82, "489550": 82, "489567": 84, "489699": 69, "489951": 68, "49": [56, 64, 67, 68, 80, 94, 95, 96, 109, 125], "490000e": 79, "490070931": 54, "490488e": 78, "490504e": 79, "490700": 82, "490896": 64, "490941": 79, "491034": 67, "491245": 77, "49135": 37, "491455": 60, "4915707": [96, 98], "492": 79, "4923156": 88, "49231564722955": 88, "492315647229550": 88, "492417e": 96, "492637": 73, "492656": 68, "49270769e": 109, "4928938": 109, "493": [83, 96, 140], "493102e": 76, "493144": 86, "493195": 74, "493219": 82, "4932597": 109, "493313": 79, "493325": 20, "493426": 83, "494": 83, "494089": 68, "494129": 82, "494324": 77, "494324401": 54, "495": 81, "495108": 76, "49530782": 54, "495657": 69, "495752": 82, "49596416e": 109, "496": 81, "496466": 60, "49650883": 85, "496551": 82, "496591": [96, 100], "496714": 86, "496777": 142, "4968022": 109, "49693": 95, "496969": 60, "497": 81, "497025": 80, "497168": 76, "497422": 68, "497655": 23, "497674": 58, "497964": 94, "498": 81, "498122": 74, "498286": 76, "498921": 82, "498979": 79, "498992": 67, "498f": 56, "499": [79, 81, 90, 92, 139], "499000e": [78, 79], "499631": 80, "499776": 79, "49d4": 56, "4a53": 56, "4b8f": 56, "4dba": 56, "4dd2": 56, "4e": [54, 55], "4ecd": 56, "4fee": 56, "4x": 82, "4x_0": [11, 67, 68, 71, 72], "4x_1": [11, 67, 68], "5": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 109, 110, 118, 125, 126, 132, 138, 139, 141], "50": [36, 54, 56, 57, 59, 64, 69, 72, 75, 76, 78, 79, 80, 82, 94, 95, 109, 125], "500": [6, 9, 10, 12, 13, 17, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 52, 56, 60, 61, 62, 66, 67, 68, 71, 72, 75, 78, 81, 83, 85, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 110, 125, 126, 132, 139, 142], "5000": [41, 60, 61, 67, 68, 69, 82, 84], "50000": 77, "500000": [78, 79], "5000000000000001": [69, 79, 82], "500084": 82, "500267": 73, "5003517412": 54, "500369": 80, "500517": 82, "50093148e": 109, "501021": 79, "501047e": 67, "501638": 61, "501983": 82, "502005": 94, "502016": 76, "502084": 96, "502205": 67, "50245": 80, "502494": 69, "5025850": 54, "502595": 68, "502612": 82, "502901": 68, "502975": 80, "502995": 82, "503": 40, "503374": 76, "503504": 95, "503511": 79, "503700": 64, "5038921": 109, "50398782e": 109, "504286": 77, "5042861": 54, "5045174": 109, "504548e": 68, "5050973": 54, "505264": 67, "505353": 68, "506050": 67, "506644": 67, "506645": 60, "506659": 79, "506687": 79, "50672034": 54, "506900e": 82, "506903": 69, "507": 83, "507285": 74, "50768b": 80, "508084": 60, "508153": 81, "508433": 67, "508459": 77, "5085": 79, "508947": [96, 98], "509059": 79, "509196": 82, "509461": 82, "50967": 86, "5097": 86, "5098": [62, 90, 92, 139], "509853": 82, "5099": [56, 62, 90, 92, 139], "509951": 69, "509958": 77, "51": [53, 55, 56, 61, 64, 71, 76, 94, 95, 109, 125, 141], "510000e": [78, 79], "510121": 67, "510385": 77, "510555": 64, "51079110": 54, "51082": 109, "511257": 76, "511515": 79, "511540": 79, "5115547": 88, "5115547181877": 88, "51155471818770": 88, "511665": 67, "511668": 86, "5116683753999614": 86, "511862": 82, "512": 77, "512108": 82, "512149": 82, "51214922": 82, "51243406e": 109, "512519": 77, "512572": 82, "512672": [96, 126, 132], "5131": 78, "513222": 74, "513624": 94, "513992": 82, "514": 56, "514173": 68, "5144042": 109, "514545": 79, "51494845": [96, 100], "515031": 67, "515106": 61, "515338e": 67, "515358": 69, "5154": 79, "5154789948092002": 77, "5155": 56, "515672": 68, "515972": 60, "516": 56, "516125": 69, "516222": 82, "516242": 68, "516255": 82, "516256": 82, "516528": 82, "516797": 68, "517": [56, 77], "517279": 68, "5175": 79, "517753": 67, "517798": 64, "518175": 77, "518375": 68, "518446": 79, "518478": 64, "518610": 96, "518782": 79, "518846": 77, "518854": 64, "51966955": 54, "519710": 82, "52": [53, 56, 73, 76, 83, 94, 95, 109, 125], "520": 79, "520415": 67, "520641": 85, "520930": 69, "521002": 69, "521233": 64, "521608e": 60, "521611": 68, "521632": 67, "521781": 60, "521788": 67, "5218952": 109, "522753": 38, "522835": 59, "523030": 86, "523163": 69, "5232": 75, "52343523e": 109, "523794e": 82, "523807": 81, "523977545": 54, "52424539": 54, "524657": 82, "524934": [67, 68], "5250": 79, "525064": 64, "52510803": 55, "5251546891842586": 86, "5255": 56, "525722": 67, "52590": [55, 78], "526": 77, "526532": 79, "526582": 74, "526769": [67, 68], "526984": 68, "527226": 67, "52732": 95, "527452": 68, "527540": 67, "528": 51, "528381e": 87, "528580": 82, "528763": 68, "528937": [71, 72], "528996901": 54, "528997": 77, "529": 77, "529405": 53, "529468": 94, "529782": 53, "52e": 109, "53": [53, 56, 64, 73, 90, 92, 94, 95, 96, 98, 109, 125, 137, 140], "530659": 73, "530793": 67, "530940": 82, "53094017": 82, "531": 56, "531223": 69, "531594": 79, "531889": 60, "53209683": 96, "532266": 69, "53257": 95, "532738": 82, "53273833": 82, "532751": 71, "5329": 79, "533": 83, "533283": 96, "5333008": 109, "533489": 59, "5337": 109, "533900": 82, "534139": 64, "5346": 56, "535179": 82, "535318": 82, "535609": 79, "5356914": 109, "535718e": 79, "53606675": 82, "536067": 82, "536143": 79, "536522": 60, "536746": 82, "536778e": 68, "536798e": [78, 79], "537240": 82, "53724023": 82, "537438": [96, 97], "53791422": 96, "538": 56, "538013": 79, "538105": 68, "5382": 85, "538937": [78, 79], "53936025": 109, "539455": 82, "539475": 82, "53947541": 82, "539491": [71, 72], "539767": 69, "54": [53, 55, 56, 58, 83, 89, 94, 95, 109, 125, 141], "540240": 79, "540375": 74, "5405": 74, "540549": 74, "5408": 53, "541": 83, "541060": 74, "541159": 82, "541208": 80, "54163": 85, "5416844": 88, "541684435562712": 88, "541821": 79, "541990": 79, "542": 83, "542136": 67, "542159": 68, "542170": 68, "542268": [96, 97, 100], "542333": 79, "542446": 72, "542451": 82, "542560": 86, "542584": 69, "5425843074324594": 69, "542647": 82, "542648": 94, "542671": 77, "542883": [126, 132], "5428834": [126, 132], "542919": 94, "542989": 82, "543": [77, 79], "543052": 64, "543075": 69, "543136": 69, "543380": 77, "5434231": 88, "543423145188043": 88, "5436005": 54, "543691": 68, "543764": 72, "54378": 85, "543832": 82, "544097": 82, "544383": 86, "544555": 77, "544669": 71, "54483": [110, 125], "5448331": [110, 125], "54517706e": 109, "545492": 64, "54550506": 83, "545605e": 82, "545919": 79, "545930": 94, "546294": 79, "5467606094959261": 69, "546761": 69, "546768": 61, "546953": 68, "547039": 67, "54716": 85, "547324": 68, "547431": 81, "5476": 79, "5479": 79, "547909": 79, "548116": 60, "548648": 61, "5486895": 109, "549109e": 79, "55": [55, 56, 69, 78, 79, 82, 94, 95, 109, 125], "5500000000000002": [69, 79, 82], "550242": 76, "551317": 68, "551586928482123": 69, "551587": 69, "551686": 69, "55176": 95, "5518": 79, "552": 79, "552058": 85, "552508": 79, "552694e": 67, "552727": 77, "552776": 82, "553004": 64, "55307": 95, "553271": 60, "553522": 68, "553754": 94, "553878": [35, 94], "553916": 79, "554076": 69, "554793e": 109, "555": 77, "555137": 68, "555150": 79, "555445": 81, "555498": 82, "5555": [52, 66], "555536": 67, "55568": 61, "555949e": 79, "555954": 79, "556191": [67, 68], "556792": 82, "5574dcd4": 56, "557595": 77, "557731": 81, "557999": 77, "558134": [67, 68], "5584": 77, "5585": 77, "55863386": 96, "558655": 69, "5589": 77, "559": 142, "5590": 77, "559144": 69, "559186": 69, "5592": 77, "559394": 82, "559522": 82, "559592e": 67, "559680": 79, "55dc37e31fb1": 56, "55e": 55, "56": [56, 89, 94, 95, 109, 125, 137, 140], "560": 80, "560135": [110, 125], "56018481": 82, "560185": 82, "5602727": 58, "560530": 68, "560689": 53, "560723": 73, "561348": 68, "5616": 78, "561711": 79, "561785": [96, 98], "561883": 39, "56191635": 109, "562013": 82, "56223": 85, "562288": 86, "562390": 94, "562452": 76, "562518": 79, "562556": 61, "5625561": 53, "562712": [67, 68], "563067": 83, "563374e": 69, "563503": 82, "563528": 79, "563563": 74, "563673": 79, "56387280e": 109, "56390147e": 109, "564045": 82, "564073": 79, "5641": 79, "564142": 69, "564232": [67, 68], "564451": 68, "564577": 79, "565": 96, "565066": 69, "565373": 67, "566": 86, "566024": 82, "566091": 79, "566388": 67, "567004": 85, "567215": 76, "567343": 79, "567364": 68, "567529": 82, "567695": 67, "567945": [71, 72], "568287": 74, "568932": [96, 100], "569315e": 68, "569449": 94, "569540": 68, "569590": 76, "56965663": 82, "569657": 82, "569911": 54, "5699994715": 54, "57": [56, 83, 94, 95, 109, 125, 142], "570038": 69, "5700384030890744": 69, "570111": 81, "5702": 79, "570486": 53, "570562": 53, "57058876": 109, "570722": 139, "570740": 60, "570936": 67, "5716884": 109, "571778": 53, "5718": 79, "5722": 78, "572408e": 68, "57245066": 82, "572451": 82, "572717": 84, "572991": 68, "573700": 59, "574": 56, "574160": 74, "5748": 95, "57496671": 54, "575": 8, "575381": 78, "57572422": 85, "575810": 67, "57585824": 85, "57592948e": 109, "57599221": 85, "575e": 83, "576": 56, "576089": 60, "5763996": 54, "57643609": 85, "577": 56, "5770": 78, "57715074": 54, "577271": 77, "577273": 67, "5776971": 85, "57775704": 85, "577807": [67, 68], "577813": 67, "577e": 83, "578081": 79, "578307": 82, "578523": 77, "578557": 68, "578846e": 69, "579125": 78, "57914935": 55, "579197": 74, "579213": 86, "579238": 69, "579322e": 78, "579875e": 67, "57e": 55, "58": [9, 55, 78, 86, 94, 95, 109, 125, 141, 142], "5800": 79, "58000": 78, "5804": 56, "580414": 86, "580751": 78, "580853": 67, "580922": 71, "581655": 79, "581827": 76, "581849": 68, "581867": 61, "582031": 78, "582052": 60, "582146": 68, "58241568": 109, "582754": 84, "582761": 69, "582991": 78, "583034": 71, "583195": [67, 68], "583201": 68, "5833333": 56, "583534": 82, "583692": 76, "584012": 79, "584057e": 67, "5845274": 109, "584742": 72, "584849": 69, "584928": 67, "584942e": 77, "5852": 79, "585394": [96, 97], "585426": 109, "585479": 74, "585793": 69, "586362": 82, "5864": 53, "5866": 79, "586719": 69, "586719493648897": 69, "586794": 67, "5868472": 54, "586921": 76, "587135": 68, "587292": 79, "58794537": 109, "588": 79, "58812": 95, "5882": 78, "588233": 67, "588364": [80, 94], "588854": 67, "5890157": 109, "589147e": 76, "589248": 85, "589440": 69, "589958": 68, "59": [60, 68, 94, 95, 109, 125], "590320": 59, "590459": 60, "5905": 78, "590736": 82, "590813": 82, "590904": 68, "590911": 69, "590991": 69, "591080": 59, "591411": 71, "591441": 31, "591652": 78, "591678": 78, "591782": 82, "59199423e": 109, "592186": 68, "592681e": 69, "59307502e": 109, "593648": 95, "593981": 94, "594": 8, "594316e": 82, "595076e": 60, "595353": 69, "59563003": [96, 100], "596": 79, "596069e": 79, "596270": [71, 72], "5964": 75, "596460": 68, "596758": 67, "597": 55, "597098": 79, "597353": 60, "597923": 79, "598178": 79, "59854797": 96, "5985730": 55, "59861": 79, "598761e": 68, "599297": [94, 95, 96], "599334": [96, 100], "599770": 60, "5cb31a99b9cc": 56, "5d": [69, 82], "5x_2": 59, "5x_3": 59, "5z_i": 82, "6": [8, 9, 10, 14, 15, 20, 21, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 90, 92, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 132, 137, 139, 140, 141], "60": [54, 57, 58, 69, 79, 80, 82, 87, 94, 95, 109, 125, 140], "600": 77, "6000": 79, "6000000000000002": [69, 79, 82], "600000e": 79, "600195": 67, "600254": 81, "600694": 96, "600776": 64, "600855": 60, "601": 55, "601061": 69, "601598": 77, "601783e": 68, "601984": 68, "602": 109, "60205111": 109, "602079": 72, "602168": 69, "602322": 94, "602386e": 67, "602492": 67, "602587": 82, "602628": 69, "6029": 79, "604016": 79, "604111": 79, "604603": 23, "604825": 79, "604841": [78, 79], "605": 79, "6050630": 109, "605195": 72, "6056138": 109, "606034": 82, "606129": 82, "606342": 69, "606759": 79, "6068": 54, "606800": 77, "606937": 60, "606954": 69, "607156": 60, "607264": 67, "607387": 60, "607478": 60, "6075": 142, "607600": 79, "607900e": 68, "608": [70, 83], "608392": 82, "60857": 53, "608818": 85, "609": 83, "609522": 76, "609575": [96, 98], "61": [60, 64, 94, 95, 109, 125, 141], "610318": 64, "611": 142, "6110": 79, "611269": 77, "61170069": 83, "611859": 72, "612": 83, "612246": 74, "612792": 79, "613244": 68, "6133": 55, "613314": 69, "613408": 82, "613498": 79, "613574": 73, "613622": 68, "613691": [96, 98], "614": [51, 83], "61404894": 96, "614188": 77, "614678": 79, "615": 64, "615498": 31, "6156012": 109, "615863": [71, 72], "616372": 78, "616617": 68, "61669761": [126, 132], "616698": [126, 132], "616828": 79, "617": 77, "61728": 94, "617283": 79, "6173": 56, "61771229": 83, "617877": 82, "618069": 78, "61810738": 55, "618574": 68, "618776": 58, "618863": 80, "618881": 68, "619": 83, "619128": 68, "619294": 64, "619351": [67, 68], "619390": [67, 68], "619454": 59, "619613": 78, "619903": 68, "61e": [55, 142], "62": [31, 64, 72, 73, 94, 95, 109, 125], "620156": 82, "620311": 60, "620874e": 96, "620995": 87, "621094": [78, 79], "621318": 82, "62131806": 82, "621359": 94, "621490": 82, "6215": 78, "622": [79, 83], "622153": 79, "622272": 64, "6224": 54, "623024": 69, "623173": 67, "624": 77, "6240": 85, "62403053": 58, "6243811": 54, "624535": 95, "624764": 68, "624798": 78, "624818": 68, "624919": 79, "624988": 79, "625": [54, 77], "625159": 73, "625254": 60, "625477": 82, "625766": 71, "625767": 67, "625891": [71, 72], "626433": 82, "6266": 79, "626633": 68, "627505": [71, 72], "627560": 82, "627564": 69, "627588e": 79, "628": 83, "628069": 77, "62881269": 109, "629346": 79, "629549": 68, "629595": 37, "629740": 67, "629e": 83, "63": [54, 64, 77, 94, 95, 109, 125, 140, 141], "630150e": 82, "630880": 83, "630914": 73, "631083": 68, "63117637": 96, "631333": 82, "6318": [78, 142], "632058": 77, "632399": 60, "63245862e": 109, "632747e": 82, "632958": 81, "6330631": 125, "633083": 60, "633433": 77, "634": 83, "63407762": 142, "634078": [78, 142], "634577": 125, "634947": 60, "63499": 79, "635": 40, "635000e": [78, 79], "635199": [78, 79], "635768": 67, "63593298": 96, "636048": 96, "636453": 34, "636575": 69, "637326": 82, "6379": 78, "638264": 82, "638461": 76, "638488": 73, "639": 78, "639135": 77, "63916605": 55, "639345": 79, "639580": 68, "639603": 68, "64": [64, 72, 78, 79, 83, 94, 95, 109, 125, 139], "640": 79, "640498": 60, "640900": 79, "641528": 82, "641547": 82, "64154727": 82, "64197957": 82, "641980": 82, "642": 83, "6420": 79, "642016": 82, "642329": 64, "642648": [96, 97], "64269": 85, "643133": 79, "64340": 85, "643512": 69, "643679": [96, 100], "643752": 82, "643939": 64, "644113": 94, "644371": 68, "644665": 69, "64476745e": 109, "644799": 59, "644985": 67, "645": 79, "645551e": 60, "645583": 64, "6456867": 109, "64579": 53, "6458": 54, "645800": 77, "646117": 68, "646937": 59, "647": 51, "647002": 79, "647004": 96, "647010": 79, "647196": 59, "64723": 85, "647254e": 67, "647423": 61, "647689": 86, "647864": 94, "647873": 82, "64797": 85, "648": [51, 78], "648355": 67, "648690": 68, "648769": 68, "649": 140, "649158": 82, "649514": 67, "649738": 67, "65": [64, 69, 73, 79, 82, 83, 94, 95, 109, 125], "650": [70, 96], "6500000000000001": [69, 79, 82], "650000e": 79, "650234": 64, "650794": 60, "650810": 79, "650867": 69, "651127": 68, "652071": 79, "6522": 140, "652312": 71, "652324": 64, "652349": 82, "652350": 77, "652450e": [78, 79], "6527": 70, "652778": 77, "6528": 79, "6528937": 109, "6530": 79, "653008e": 78, "653829": 74, "653846": 69, "653901": [67, 68], "654070e": 96, "654219": 60, "654755": 59, "655284": 82, "6553": 142, "6554": 140, "655422": 79, "655547": 67, "65557405e": 109, "655959": 74, "65615782": 109, "657": 56, "65721031": 109, "658": 77, "658021": [96, 100], "658267": 82, "658455": 60, "658592": 68, "6586": 53, "658702": 68, "659": 56, "659245": [67, 68], "659339": 68, "659387": 61, "6593871": 53, "659423": [67, 68], "659473": 86, "659636": 69, "659735": 67, "659755": 83, "6598": 75, "659835": 68, "66": [64, 74, 75, 80, 94, 95, 109, 125, 139, 141], "660": [56, 96], "660073": 68, "660320": 72, "660479": [96, 98], "6607402": 83, "660776": 82, "66133": 96, "661369": 81, "661388": 67, "66242782": 61, "6625": 79, "66255828": 61, "66268609": 61, "662975": 76, "663081975281988": 69, "663082": 69, "663177": 64, "663182": 69, "6634357241067617": 86, "663443": 60, "663529": 82, "663533": 79, "663672": 74, "663765": 68, "664103e": 79, "664147": 79, "66431079": 60, "664409": 68, "664797": 67, "664824": 79, "664850": 77, "665071": 60, "665264": 82, "665653": [96, 100], "665826": 60, "666": 80, "66601815": [96, 98], "666104": 82, "666307": 59, "6666667": 56, "666742": 94, "666959e": 74, "667": 77, "6670246": 109, "667274": 73, "667492e": 79, "667536": 82, "667614": 69, "667614205604159": 69, "667981": 67, "667985": 73, "668337": 79, "668452": 73, "668584": 59, "6687553": 60, "668981": 71, "6689974": 109, "669579": 68, "66960193": 60, "66989604": 58, "67": [51, 56, 78, 86, 94, 95, 109, 125, 139], "670867": [36, 94], "671224": 68, "671271": [67, 68], "67136": 79, "6716717587835648": 69, "671672": 69, "671690": 67, "671914": 61, "6722": 56, "672234": [67, 68], "672368": 69, "6723684718264447": 69, "672384": [67, 68], "67245350": 54, "672511": 67, "673092": [67, 68], "673302": 77, "673330": 68, "67410934": 54, "6745349414": 54, "674552": 79, "67456": 86, "674609": 69, "674747": 76, "674949e": 85, "6750206": 109, "675233": 68, "675293": 81, "675625": 94, "675653": [96, 97], "675775": 76, "676298e": 60, "676405": 69, "6765": [55, 78], "676534": 125, "676641": 67, "676756": 82, "676807": 78, "677": 51, "677123": 67, "67732924": 60, "67752322": 60, "677614": 82, "677980": 69, "678": 83, "678117": 79, "678826": 69, "679066": 60, "67934627": 61, "67936506": [96, 98], "679539": 77, "67956273": 61, "679789e": 67, "67990691": 61, "67ad635a": 56, "68": [56, 64, 84, 85, 94, 95, 109, 125], "680": 79, "6800045": 109, "680896": 60, "6810775": 85, "681176": 77, "681246": 68, "681448": 79, "681521": 67, "681562": 79, "681817dcfcda": 56, "682": 96, "682122": 76, "682269": 79, "6826": 78, "682875": 69, "683487": 68, "683581": 96, "683637e": 74, "683687": 68, "683942": 82, "683984": 38, "684": 142, "68410364": 55, "68411700": [55, 142], "684128": 68, "684142": 67, "684502": 82, "685104": 20, "685107": 82, "68554404e": 109, "68562150e": 109, "685807": 82, "685989": 96, "686270": 68, "686627": 67, "687345": 82, "687612": 68, "687647": 82, "687854": 59, "687871": 77, "6878711": 54, "688": 140, "688540": 94, "688641": 76, "688747": 79, "688886": 94, "688918": 79, "688956": 67, "688999": 60, "689088": [67, 68], "689188": 59, "689392": 82, "689600": 76, "689932": 67, "69": [73, 94, 95, 109, 125, 141], "690334": 69, "6903344145051182": 69, "69067067": 61, "69074566": 61, "69079098": 61, "691097": 67, "691157": 58, "69140475e": 109, "691423": 67, "691511": 78, "691848e": 68, "691911": 94, "692202": 61, "692297": 68, "692460": 76, "692579": 68, "692725": 82, "692907": 79, "692959": 67, "693316": 79, "693497e": 79, "693690": 79, "693796": 77, "694": 80, "694154": 69, "694561": 83, "694845e": 79, "694919": 77, "6950": 79, "695045": 67, "69508862": 96, "69540165": 109, "695581": 73, "69562150e": 109, "695711": 76, "69572427": [96, 100], "695928": 67, "696011": [35, 94], "696224": 94, "696289": [71, 72], "69684828": 96, "696966": 68, "697": 77, "697000": 69, "697420": [71, 72], "697545": 82, "697616": 68, "697693": 67, "69803897": 60, "698223": 59, "698244": 59, "69840389e": 109, "698509": 67, "698642": [96, 100], "698694": 77, "698751": 76, "699035": 82, "699082": 69, "69921": 56, "699259e": 82, "699333": 69, "699616": 76, "699697": 68, "6_design_1a": 70, "6_r2d_0": 70, "6_r2y_0": 70, "6b": 125, "6cea": 56, "7": [10, 14, 17, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 132, 139, 140, 141], "70": [55, 57, 69, 71, 78, 79, 80, 82, 94, 95, 109, 125, 141], "700": [67, 68, 70, 77], "7000000000000002": [69, 79, 82], "700015": 82, "700102": 82, "7001263": 109, "700314": 64, "700458": 67, "70103685": 60, "701078": 82, "701088": 78, "701106": 73, "701265": 71, "70131513": 60, "701413": 79, "701672e": 69, "701841e": 72, "701866": 82, "7018663": 82, "701966": 79, "702489": 79, "703049": 67, "70305686": [96, 100], "703108e": 39, "703325": 74, "70344386": [96, 100], "703772": 79, "7040": 79, "70438583": 60, "704482": 74, "7045": 74, "704558": 74, "704634": 60, "704814": 67, "704896": 74, "70496355": 60, "705": 51, "705090": 68, "705354": 67, "70557077": [96, 100], "705581": 79, "7055958": 88, "705595810371231": 88, "7055958103712310": 88, "705794": 68, "70583": 85, "706056": 79, "706077": 68, "706122": 68, "706430": 67, "706645": 69, "706657": 69, "706862": 23, "707125": 68, "707441": 68, "707738": 68, "70774361": [96, 100], "707868": 82, "707963e": 78, "708190": 77, "708235": 67, "708459": 82, "708472": 68, "708821": 64, "708837": 64, "709025": 80, "709026": 59, "709596": 67, "709606": [36, 94], "71": [94, 95, 109, 125, 141], "710": 80, "710059": 64, "710319": 68, "710515": 67, "710586e": 77, "711024": 79, "711328": 79, "711383e": 67, "711518": 79, "711638": 96, "712064": 67, "712082": 79, "712095": 64, "712157": 81, "712268": 67, "712372e": 68, "712503": 85, "712592": 78, "712774": 71, "7128": 60, "712846": 94, "712960": 69, "713": 79, "713407": 79, "713457": 67, "7135003": 109, "713986": 79, "713993": 68, "714240": 77, "714250": 68, "714321": 78, "714534e": 68, "714651": 82, "71465114": 82, "714737": 60, "715013": 79, "715180e": 79, "7154": 79, "715407": 69, "7155": 79, "7158581": 54, "716013e": 67, "716065": 60, "716098": 64, "7161": 79, "716387": 67, "716427e": 68, "716456": 82, "716595e": 79, "716615": 64, "716762": 69, "716793": 69, "716799": 77, "7167991": 54, "716801": 76, "717": 79, "717130": 79, "717185": 82, "718686": 86, "71903749": 60, "719552": 68, "72": [57, 74, 94, 95, 109, 125, 141], "720153": 61, "7204309": [96, 100], "720559": 67, "720571": 82, "720573": 67, "720589": 83, "720662": 80, "720664": 77, "721018": 67, "721071": 82, "721245": 68, "7215093d9089": 56, "72155839e": 109, "721609": 79, "722316": 82, "722634": 82, "72269685": [96, 100], "72281632": 60, "722848": 69, "722881": 82, "7229": 79, "723": 56, "723314": 82, "723342": 94, "723345e": 82, "723657": 67, "723846": 64, "7239": 79, "72407565": 60, "7241399": 54, "72421675": 60, "724338": 82, "724767": [71, 72], "724918": 86, "72497392": 60, "725": 56, "725010": 64, "725061": 67, "725087": 79, "725166": 82, "725427": 61, "725565": 67, "725802": 23, "725820": 76, "725919": 67, "726": [56, 83], "726658": 76, "7268131": 54, "727159e": 68, "727543": 59, "727693": 79, "727704": 79, "727976": 69, "7282094": [96, 98], "728294": 81, "728710": 82, "72875815e": 109, "728798": 60, "728852": 79, "728e": 83, "729668": 94, "729867": 67, "72987186": [96, 100], "73": [55, 64, 94, 95, 109, 125], "730023": 79, "7308": 53, "730809": 67, "731174": 67, "731317": 69, "732": 83, "732067": 67, "732137": 67, "732150": 68, "7326": 79, "732638": 82, "73285": 34, "732918": 71, "733": 79, "733047": 68, "733644": 67, "734635": 67, "734770": 68, "734948": 82, "7350": 60, "735049": 80, "735369e": 94, "7357": 79, "735848": 94, "735941": 33, "735964": 59, "736082": [67, 68], "736084": 82, "73608412": 82, "736823": 68, "737": 51, "737052": 79, "7375529": 109, "7375615": 55, "73764317e": 109, "737951": [67, 68], "738": 78, "738065": 68, "738223": 79, "738315": 79, "738659e": 79, "738876": 68, "739": 79, "739063": 67, "739089": [96, 100], "7395359436844482": 69, "739536": 69, "739595": 83, "739720": 79, "739817": 73, "74": [9, 55, 68, 78, 94, 95, 109, 125, 141], "740": 77, "740180e": 82, "740367": 67, "740417": 78, "740505": 64, "740785": 67, "740869": 69, "741104": 69, "741380": 83, "741523": 64, "741702": 82, "7418": 53, "74189": 56, "742": 80, "742128": 82, "742375": 67, "742407": 81, "742411": 67, "742907": 82, "743026": 60, "7432": 53, "743247": 79, "743341": 68, "743609": 67, "7437": 79, "743919": 60, "74402577": 82, "744026": 82, "744236": 85, "74461783e": 109, "74475816": [96, 100], "745": 79, "745022": 64, "745444": 67, "745493": 61, "745638": 78, "745881": 67, "746361": 82, "74665995": 109, "746843": 72, "7470": 79, "747646": 79, "747945": 54, "747961": 79, "748084": 68, "748377": 78, "748513": 79, "748880": 79, "74938952": [96, 98], "749443": 79, "749854893": 110, "75": [9, 26, 36, 56, 59, 64, 69, 74, 78, 79, 82, 94, 95, 109, 125, 141], "75000": 86, "7500000000000002": [69, 79, 82], "750000e": 79, "750437": 60, "750597": 68, "750701": 64, "751013": 79, "751261": 79, "751633": 79, "75171": 78, "751710": [69, 78], "751712655588833": 88, "7517126555888330": 88, "751712656": 88, "752015": 32, "752283": 79, "752696": 64, "752909": 74, "7533": 78, "753323": 67, "753393": 67, "753523": 82, "753866": 68, "754469": 67, "754499": 68, "754678": 76, "754692": 94, "7548": 86, "754870": 77, "755688": 67, "755701e": 67, "755885": 94, "755910": 79, "7559417564883749": 69, "755942": 69, "7560824": 54, "756198": 60, "756200": 64, "75639736": 60, "756601": 60, "756805": 77, "756867e": 79, "756905": 23, "756969": 69, "757": [83, 140], "757151": [67, 68], "757183": 69, "757411": 82, "757559": 74, "757819": 77, "757917e": 82, "758391": 79, "758831": 68, "75887": 56, "759": 142, "759006": 58, "759054": 68, "759212": 60, "7592784": 109, "759833": 68, "76": [94, 95, 109, 125, 140, 141], "760104": 82, "7603": 53, "760386": [96, 98], "760778": 77, "760915": 59, "761": [54, 77], "761073": 80, "761224": 64, "761429": 68, "761714": 69, "762284": 82, "76228406": 82, "762299": [96, 100], "762748": 79, "763691": 79, "76378756": 109, "764093": [67, 68], "76419024e": 109, "764315": 82, "76444177e": 109, "764478": 81, "7646": 79, "764798": 82, "764953": 78, "765202": 79, "76535102": [96, 100], "765363": [67, 68], "765500e": [78, 79], "765710e": 87, "765792": 82, "765864": 85, "76591188": 54, "765960": 67, "7660": 53, "7662": 60, "7663": 79, "766499": 82, "766940": 64, "76702611e": 109, "767188": [71, 72], "767435": 86, "767549": 68, "768071": 82, "768273": [71, 72], "768763": 68, "768798": 64, "769361": 82, "769805": 82, "77": [83, 94, 95, 109, 125], "770556": 79, "770683": 80, "770944": [71, 72], "7710": 85, "771157": 125, "771390e": 79, "7714": 80, "7716982": 55, "771741": 79, "771965": 79, "772104": 67, "772157": 74, "77227783e": 109, "772396": 68, "772444": 94, "772791": 79, "77289874e": 109, "773": 56, "773177": 69, "773339": 74, "773488": 82, "77348822": 82, "773768": 61, "773769": 76, "77401500e": 109, "774271e": 79, "774373": 60, "775": [56, 79], "775191": [67, 68], "775285": 67, "7753031": 109, "775927": 60, "775969": 85, "776254e": 67, "7763": 78, "776500": 60, "776728e": 77, "776887": 78, "77746575": [96, 100], "7776071": 54, "777718": 76, "777728": 94, "777867": 74, "777e": 83, "778400": 67, "7786": 53, "779": 83, "779068": 74, "779108": 67, "779167": 31, "779517": [67, 68], "779682": 69, "7799": 75, "779912": 79, "78": [83, 94, 95, 109, 125, 141], "780": 56, "780068": 76, "780338": 67, "780458": 82, "780856": 78, "781": 79, "781233": 79, "781530": 82, "781681": 82, "781787": 60, "782": 56, "782050": 82, "782555": 79, "783": 56, "783276": 96, "7833": 53, "7838": 53, "783839": 61, "78386025": [96, 100], "784": 125, "784238": 77, "784405": 85, "784483": 77, "784624": 69, "784792": 76, "784872": 64, "785": 56, "785038": 68, "785153": 68, "785815": 64, "785911": 82, "785e": 40, "786": 56, "7860": 60, "786090": 76, "786132": 60, "786237": 67, "786563": 76, "786744": 69, "786986": 64, "78711285e": 109, "78777": 85, "788": 140, "78818": 56, "788196": 60, "788868": 68, "789032": 67, "789039": 68, "789330": 68, "789671": 69, "789671060840732": 69, "79": [64, 94, 95, 109, 141], "790039e": 67, "790115": 79, "790261": 94, "790723": [71, 72], "7908178": 109, "791097": 78, "791241": 82, "791297": [36, 94], "792327344955576165707377798182869097100": 109, "792396": 64, "792939": 69, "792972": 94, "79330022": [96, 100], "793315": 94, "79338596e": 109, "793570": 82, "793598": 68, "793735": 82, "793776": 61, "793818": [67, 68], "794": 96, "794366": 79, "794546": 80, "79458848e": 109, "794805": 71, "795": 83, "795647": 82, "7957": 79, "795932": 95, "796014": 68, "796384": 68, "796444": 79, "796596e": 67, "796e": 83, "797086": 67, "797189": [96, 100], "797280": 82, "797454": 96, "797737": 125, "7977573": 109, "797868": 68, "79792890e": 109, "797965": 125, "798071": 20, "798308": 78, "798783": [71, 72], "799403": 82, "79953099": [96, 100], "7999": 87, "7b428990": 56, "7x": 82, "8": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 66, 67, 68, 69, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 90, 92, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 132, 139, 141, 142], "80": [57, 58, 69, 79, 82, 87, 94, 95, 109, 141], "800": 77, "8000": [19, 57, 87], "8000000000000002": [69, 79, 82], "800143": 67, "800326e": 67, "800351": 67, "800729": 60, "801623": 79, "802": 83, "802289": 79, "802738": 94, "803112": 76, "803300": 67, "803492e": 82, "803563": 79, "80367242": 109, "803902e": 79, "804": 79, "804219": 82, "804284": 85, "804316": 82, "804484": 82, "8048": 55, "804828": 82, "804889": 79, "805007": 77, "805153e": [78, 79], "805293": 68, "8055563": 54, "805774": 67, "8059": 78, "806218e": 79, "806531": 79, "806554": 67, "806732": 73, "80696592e": 109, "8069796": 109, "80714504e": 109, "807879": 82, "808": [55, 125], "808008": 60, "8080317": 109, "808246": 78, "808284": 79, "808640": 79, "809125": 67, "8095": 80, "809589": 61, "809913": [67, 68], "80a8": 56, "81": [54, 67, 70, 73, 75, 94, 95, 109, 141], "810044": 78, "810134": 82, "8102": [53, 78], "810306": 64, "810322": 67, "810363": 79, "810382": [78, 79], "810419": 68, "810707": 79, "810895": 68, "811011": 68, "811155": 73, "811458": 78, "811513": 68, "8116912": 125, "811696": 67, "811825": 77, "811901": 82, "81190107": 82, "812": 83, "812028": [96, 97], "813120": 60, "8132463": 54, "813293": 82, "813342": 125, "813682": 79, "814136": 69, "814246e": 68, "814351": 69, "814913": 77, "8152": 79, "815213e": 68, "815224": 125, "815226": [96, 98], "81568484": 82, "815685": 82, "815993": 82, "816176": 86, "8162377": 109, "816318": 77, "816373": 67, "816752": 79, "816850": 61, "816982": 67, "817119": 67, "817291": 79, "8173602": 75, "817967": 96, "81827267": 82, "818273": 82, "818289": 82, "81828926": 82, "818380": [67, 68], "818409": 80, "81856": 56, "819223": 94, "819507": 76, "81981865": 109, "82": [86, 94, 95, 109, 141], "8202": 55, "820366": 77, "8209": 55, "820963": 64, "821": 140, "8210": 55, "821021": 69, "821457": 79, "821566": 82, "821855": 94, "821970": 76, "821995": 68, "8221": 53, "822289": [78, 142], "82228913": 142, "822482": 69, "822524": 60, "8227": 79, "822822": 69, "823247": 82, "823264": 60, "823273": [67, 68], "824350": [67, 68], "824643": 61, "824657": 76, "824701": 69, "824750": 69, "824889": 69, "824961e": 79, "8250": 53, "825587": 68, "825617": 77, "8258056": 109, "825862": 82, "825980": 69, "8259803249536914": 69, "8260": 78, "826065": [67, 68], "826426": [96, 98], "826467e": 67, "826492": 82, "826519": [36, 94], "82666866e": 109, "82684324": 85, "827375": 58, "827381": 82, "827735": 82, "827938162750831": [71, 72], "828058": 79, "828157": 64, "828618": 74, "828778e": 67, "828912": 67, "828915": [71, 72], "829512": 60, "829543": 69, "829730e": 68, "829764": 94, "82985": 73, "83": [94, 95, 109, 141], "830263": 76, "830273": 64, "830301": 81, "830442": 67, "830467": 67, "831": 83, "831019": 69, "831190": 68, "831278": 67, "831553": 61, "831741": 67, "831833": [96, 100], "832086": 82, "8324": 96, "832556": 60, "8326928": 85, "832693": 85, "832875": 82, "83287529": 82, "833024": 77, "833065": 64, "833227e": 95, "833464": 79, "83380202": 60, "833907": 77, "834133": 74, "834570": 60, "835": 83, "8350": 79, "835035": 76, "835239": [96, 100], "835344": 64, "835596": 79, "835750": 74, "835935": 68, "836234": 96, "8379987": 109, "838114": 82, "838235": 80, "838457": 79, "83905": 20, "839819e": 60, "839904": 60, "84": [56, 73, 83, 94, 95, 96, 109, 141], "840041": 79, "840303": 82, "84030318": 82, "840673": 67, "840718": [96, 98], "840836": 82, "840995e": 78, "841": [51, 54, 77], "841132": 78, "84116": 60, "8415": 55, "841847": 79, "842": 80, "842132": 96, "842405": 69, "842625": 77, "842725": 60, "842746": 82, "8428": 78, "842853": 82, "843018": 94, "843730": 77, "843796": 67, "8440": 79, "844107e": 68, "844308": 82, "844549": [71, 72], "844663": 64, "844667": 125, "844707": 82, "844889": 77, "845241": 83, "845534": 76, "846388": 69, "847029": 68, "847555": 67, "847595": [35, 94], "847948": 69, "847962": 67, "847966": 79, "848688e": 76, "848757e": 78, "848868": 69, "849": 80, "84930915e": 109, "849427": 94, "849747": 85, "8497f641": 56, "8499": 79, "85": [13, 69, 73, 79, 82, 87, 94, 95, 109], "8500000000000002": [69, 79, 82], "850038": 64, "850321": 77, "850439": 68, "850575": [67, 68], "850656": 74, "850794": 82, "850908": 61, "851": 140, "851198": 79, "8513": 56, "851366": 77, "852": [79, 142], "852212": 60, "85265193": 75, "85280376": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "85397773": [96, 98], "855035": 67, "855780": 82, "855862": 68, "856404": 72, "8566301": 109, "856758": 94, "8571": 53, "857161": 82, "857515": 94, "857544": 77, "857765": 79, "858212e": 68, "858329": 61, "858952": 64, "859": 79, "85911521e": 109, "85912862": 125, "859129": [110, 125], "8597": 78, "85974356": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "85c5": 56, "85e": 55, "86": [94, 95, 109, 141], "860663": 125, "860804": 82, "860992": 79, "861019": 64, "861210": [96, 100], "861519": 67, "862043": [71, 72], "862359": 69, "863": 80, "863772": 78, "863982270": 110, "864": 83, "86415573": 55, "86424193e": 109, "8644": 56, "864664": 64, "864741e": 79, "865313": 79, "865562": [67, 68], "865854": 79, "865860": [78, 79], "865914": 68, "866102": [67, 68], "866179899731091": 88, "866179900": 88, "866579": 79, "866798": 79, "867201": 76, "867565": 82, "8679": 79, "868": 56, "8685788": 82, "868579": 82, "86897905": [96, 100], "869": [56, 83, 109], "869020": 69, "869122": 60, "869133": 60, "869195": 68, "869398": 68, "869477": 67, "869585": 39, "869586": 73, "87": [55, 67, 73, 77, 94, 95, 109, 141], "870": 61, "8700": 55, "870099": [71, 72], "870260": 82, "870332": 82, "870444": 78, "870857": 82, "871": 56, "871519": 60, "871545e": 67, "871923": 68, "871972": 74, "872": 83, "872132": 68, "872222": 79, "872727": 67, "872768": 82, "872852": 82, "87290240e": 109, "872994": 79, "87309461": [96, 100], "873198": 79, "873677": [71, 72], "873848": 61, "87384812361": 53, "87384812362": 53, "87430335": 125, "874303353": 125, "874702": [71, 72], "8750": 79, "875047": 60, "8759": 79, "87598507": 60, "876": 83, "876083": 79, "876233": 61, "87623301": 53, "876431e": 69, "876549": 79, "87674597e": 109, "8768": 53, "8771": 79, "877153": 79, "877455": 81, "877673": 60, "877833": [67, 68], "877903": 64, "878281": 82, "878289": 79, "878402": 67, "878746": 64, "878847e": 79, "878895": 64, "878968e": 67, "879": 96, "879049": 79, "879058": 76, "879103": 69, "879509": 67, "87e": 55, "88": [55, 73, 83, 94, 109], "880106": 77, "880325": 61, "880579": 82, "880591": 81, "880808e": 79, "880880e": 79, "880886": 78, "8810": 78, "881201": 79, "88125046e": 109, "881465": 59, "881581": 33, "88173062": 54, "881937": 64, "882371": 61, "882475": 69, "882641": 78, "882928": 64, "883485": 68, "883622": 82, "883914": 69, "883953": 76, "884121e": 60, "884132": 82, "8843": 85, "884457": 60, "8845": 53, "884821": 94, "884996": 69, "8850": 55, "885065": 82, "885832": 83, "885956": 67, "885978": [71, 72], "886041": 68, "886086": [67, 68], "886266": 79, "88629": 53, "886314": 68, "88664": 56, "887182": [96, 97], "887345": 79, "887556": 69, "887648": 68, "887680": 67, "888146": 77, "8881461": 54, "888352": 64, "888445": 68, "888775": 72, "888804": 79, "889": 51, "889293": 82, "889326": 68, "889638": 64, "889733": 82, "889792": 68, "88988263e": 109, "889913": [67, 68], "889963": 82, "88ad": 56, "89": [55, 68, 94, 109, 140, 141], "890": [54, 77, 80], "890204": [96, 100], "89027368": 125, "890273683": 125, "890318": 67, "89035917": 73, "890372": [62, 90, 92, 139], "8903720000100010000010": [56, 90, 92, 139], "8904": 51, "890454": 95, "890665": 74, "890855": 64, "8909": [54, 78, 142], "891187": 60, "891527": [96, 100], "891606": 78, "891752": 68, "891997": 67, "892": 56, "8925006": 109, "892648": 82, "89273": 96, "892796": [67, 68], "892828": 74, "893": 56, "8932105": 54, "893461": 64, "89352855": 60, "893649": [67, 68], "893851": 82, "894": 56, "894307e": 79, "894448": 68, "8946549": 57, "89472978": [96, 100], "89482571": 60, "895106": [67, 68], "895201": 61, "895308": 79, "895333": 82, "895442": 78, "895690": [67, 68], "895768e": 69, "896023": 82, "896182e": 74, "896263": 74, "89710096": 60, "897220": 82, "897240": 79, "8974": 78, "897451": 67, "897495e": 68, "898183": 64, "898722": 82, "899021": 94, "899250": 64, "899460": 82, "899654": 64, "899662e": 67, "899716": 68, "8bdee1a1d83d": 56, "8da924c": 56, "8e3aa840": 56, "9": [20, 22, 23, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 88, 90, 92, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 132, 138, 139, 141, 142], "90": [15, 55, 57, 58, 69, 79, 82, 87, 94, 109, 141], "9000000000000002": [69, 79, 82], "900000e": 79, "900021": 95, "900127": [96, 100], "90074543": 60, "900829": 74, "901013": 68, "901148": 82, "90136": 78, "901360": 78, "90145324": [96, 100], "901526": 73, "901683": 79, "90169302": 60, "901705": 67, "902": 125, "902573": 69, "902920": 94, "903056e": 82, "903339": 69, "903351e": 69, "903418": 77, "903674": 67, "903681": 82, "903767": [67, 68], "904156": 69, "9041560442482157": 69, "904315": 67, "904396": 67, "90447803": 60, "905042": 68, "905494": 69, "905858": 86, "905951": 85, "906072": 94, "9061": 79, "906181": 60, "906653": 61, "906716732639898": [71, 72], "906757": 65, "907115": 82, "907176": 82, "9073": 79, "907491": 69, "90753679": 60, "907801": 77, "907879": 64, "90794478": 125, "907944783": 125, "907961": 79, "908024": 86, "908663": 68, "908767": 76, "909195": 60, "909304": [67, 68], "909571": 64, "90963122e": 109, "9096332": 109, "909942e": 94, "909975": 79, "909997": [78, 142], "91": [84, 94, 109, 141], "910000e": 79, "910895": 68, "9109": 56, "91102953": 82, "911030": 82, "9112": 78, "911277": 68, "911662": 71, "912230": [67, 68], "9123764": 60, "9126": [55, 142], "9127": [55, 142], "91289358": 60, "912903": 67, "913": 56, "91315015": 54, "913280": 86, "913371": 68, "913415e": 67, "913485": 79, "913585": 74, "913774": 69, "914099": 61, "9142": 79, "91438767e": 109, "9145": 53, "914598": 64, "915": [55, 56, 78, 79, 80], "915000e": [78, 79], "915260e": 67, "915327": 60, "915488": [71, 72], "9158080176561963": 76, "916236": 53, "916359": 64, "916528": 71, "9166667": 56, "916914": 82, "916930": 67, "917": 56, "917000": 68, "917066": 79, "917248": 82, "91724807": 82, "917436": 82, "918": 83, "91802702": 60, "918227": 69, "918293": [96, 100], "919228": 61, "919432": 82, "9197": 79, "919969": 67, "91e": [55, 109], "92": [60, 84, 94, 95, 109, 141], "920052": 68, "920335": 79, "920337": 72, "920645": 79, "9209": 53, "9210": 79, "921061": 86, "9211019": 109, "921198": 74, "921256e": 68, "921372": 69, "92171487": 60, "921778": 74, "921913": 77, "921956": [67, 68], "921e4f0d": 56, "922160": 79, "922251": 67, "9223": 79, "922668": 74, "922689": 60, "922996": 77, "923074e": 69, "923517": 87, "923607": 82, "92369755": 54, "923804": 69, "923943": 142, "923977": 79, "924002": 82, "9243": 79, "924396": [71, 72], "924443": 64, "924634": 59, "9248": 56, "924821": 69, "924843": 77, "924921": 94, "925": 58, "925248": [71, 72], "925660": 67, "925736": 69, "925957": 71, "925994": 78, "925995": 68, "926227": 68, "926621": 69, "926901": 76, "927": 52, "927074": 82, "927232": 79, "9274": 79, "92772551": 60, "927950": 79, "928": 80, "928206": 61, "92827999": 96, "92881435e": 109, "928947": 77, "92905": 54, "929309": 60, "929643": 67, "929665": 61, "92972925e": 125, "929729e": [110, 125], "93": [55, 74, 84, 94, 95, 109, 141], "9304028": 54, "93074943": [96, 100], "931": 89, "931479": 82, "931756": 61, "931978": 139, "932": 51, "932027": 69, "932404e": 79, "9325": 53, "9327": 53, "932956": 61, "932973": 82, "93300": 96, "93307839": 60, "933322": 68, "933671": 68, "933857": 68, "933996": 69, "934058": 67, "934243": 68, "934433": [67, 68], "9345": 56, "934500": 68, "934511": 125, "934549": 79, "93458": 85, "934963": 68, "934992": 69, "935": [40, 75, 96], "935220": [96, 97], "935498": 61, "935591": 82, "935730": 82, "935764": 68, "935989": 77, "9359891": 54, "93648": 87, "936494": 67, "936739": 82, "937116": 77, "937586": 79, "938": 125, "938263": [96, 100], "938692": 60, "938836": [96, 100], "939068": [71, 72], "9392": 79, "939250": 67, "939458": 67, "9395": 79, "93958082416": 142, "9398083": 60, "94": [58, 75, 94, 109, 141, 142], "940354721701296": 69, "940355": 69, "940373": 79, "941440": 67, "941724": 79, "941788": 71, "942139": 72, "942312": 82, "942460e": 82, "942489": 79, "9425": 53, "942550": 79, "94255163": 60, "942661": 77, "942823": 79, "94309994e": 109, "943291": 60, "943548": 74, "943693": [96, 97], "943938": 82, "943949e": 82, "944253e": 82, "944266": [71, 72], "94427158": [96, 100], "944280": 79, "94441007e": 109, "94473": 57, "944949": 60, "945126": 60, "945477": 60, "94579791": 109, "94587208": 60, "945881": 67, "946180": 74, "94629": 87, "946297": 69, "946406": 72, "946433": 82, "946533": 67, "946658": 79, "946968": 69, "947440": 81, "947466": 95, "947613": 68, "947855": 64, "9480": 79, "948112": 83, "948154e": 71, "948785e": 67, "948868": 79, "948975": 73, "94906344": 54, "9491496": 60, "949241": 125, "949456": 82, "949866": 68, "95": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 53, 55, 57, 58, 59, 60, 61, 64, 67, 68, 69, 70, 71, 72, 73, 74, 75, 78, 79, 82, 83, 85, 86, 87, 94, 96, 109, 125, 126, 132, 141, 142], "9500": 79, "950158": 67, "950545": 65, "95062986e": 109, "9509995": 109, "951502": 82, "951532": 77, "951920": 81, "952": [55, 83, 142], "952146": [96, 100], "9523": 53, "952506": 60, "952839": 82, "95305": 57, "95311164": [96, 100], "953242": 61, "9534": 79, "953683": 77, "953704": 67, "95372559e": 109, "953884": 96, "954": 125, "95401167e": 109, "955005e": 79, "9551": 79, "9552": 53, "955526": 61, "955541": [36, 94], "95559917": 95, "955701": 67, "955e": 96, "956047": 54, "9561": 53, "9561324": 109, "956476": 60, "956574": 79, "956588": 96, "956724": 69, "9567242535070148": 69, "956830": 61, "956877": 68, "956892": 79, "957229": 72, "957375": 77, "957745": 69, "9579": 55, "957996": 69, "958": 125, "9580": 55, "958105": 94, "958541": 79, "959132": 68, "959384": 68, "959613": 94, "95e": 55, "96": [55, 67, 68, 80, 94, 109, 141], "960236": 96, "9605": 79, "960808": 69, "960834": 68, "9609": 53, "961539": 79, "961962": 69, "96229524": 60, "962364": 64, "962373": 68, "962954": 68, "963055": 79, "963427e": 68, "964025e": 82, "964065e": 68, "964261e": 77, "964318": 79, "9647": 53, "965341": 68, "965531": 96, "965696": 67, "965774": 79, "96582": 95, "966015": 82, "966097": 37, "966659": 69, "9666592590622916": 69, "967092": 68, "967467": 85, "968127": 64, "968134e": 82, "968258e": 67, "968577": 58, "96890798": 60, "969141": [94, 95, 96], "9694843": 60, "9699": 78, "969925e": 68, "97": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 58, 59, 60, 61, 62, 64, 65, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 94, 95, 96, 97, 98, 100, 109, 110, 125, 139, 141, 142], "970065": 82, "970150": 68, "970286e": 60, "97081693": 60, "971058": [71, 72], "972509": 68, "972745": 67, "972748": 69, "9727504": 109, "97276281": 82, "972763": 82, "973": 80, "97314470": 54, "973156": 94, "973229": 68, "973241": 82, "973303": 60, "973331": 79, "973664": 61, "973741": 68, "973890": 68, "974202": 69, "974213": 68, "97441062": [71, 72], "974414": 69, "974487": 67, "97470872": 85, "9748910611": 54, "975": [51, 67, 68, 71, 72, 74, 75, 80], "975289": 64, "9753": 56, "975447": 58, "975450": 68, "975461": 77, "975592": 64, "976088": 82, "97619643": [96, 100], "976548e": 68, "976562": 82, "977202": 68, "977280": [67, 68], "977295": 79, "977507": 68, "977580": 61, "97778713": 60, "977820": 67, "97829943": 60, "978303": 76, "9786104": 109, "9787": 79, "978977": 82, "979": [80, 83], "979295": 60, "979384": 74, "979475": 67, "979702": 67, "979857": 67, "979966": 74, "979971e": 67, "98": [67, 68, 79, 94, 109, 141], "980": 142, "980026": 79, "9802393": 54, "980440": 68, "980643e": 69, "981104": 81, "981313": 60, "981403": 68, "981438": 67, "981672": 69, "981715": 67, "982": 80, "982019e": 68, "982353e": 79, "982417": 69, "982720": 67, "982797": 81, "9828796": 60, "983192": 82, "983253": 67, "983636": 60, "983759": 142, "98393441": 85, "984024": 81, "984083": [71, 72], "984551": 32, "984562": 82, "984866": 125, "984872": [67, 68], "984937": 69, "984971": 61, "9850067": 109, "985049": 80, "98505871e": 109, "985207": [67, 68], "98524840": 109, "985654": 68, "986249": 78, "986383": 79, "986417": 67, "98673": 73, "987": 80, "9870004": 56, "987220": 79, "987307": 76, "9875": 53, "987726": 68, "9880384": 56, "988421": [67, 68], "988463": 82, "988541": 67, "988709": 79, "988780": 79, "989622": 80, "99": [55, 61, 64, 67, 68, 80, 94, 109, 141], "990118": 80, "990210": 79, "990387": 60, "990903": 67, "991": [56, 80], "9914": [78, 79, 85], "991444e": 71, "9915": [55, 78, 79, 85], "991512": 55, "991963": [67, 68], "991977": 79, "991988": 67, "992": 83, "99232145": 85, "992582": [67, 68], "993166": 61, "993201": 68, "993416": 74, "993575": 79, "994": [80, 83], "994168239": 54, "994208": 64, "994214": 79, "994332": 65, "994377": 67, "9944": [75, 96, 98], "9948104": 57, "994851": 79, "994937": 72, "995015": 79, "9951": 53, "995138": 60, "995248": 82, "99549118e": 109, "99571372e": 109, "9961": 78, "9961392": 54, "996313": 67, "99669349": 60, "996892": 76, "996934": 77, "9970": 79, "997034": 87, "997494": 87, "997571": 77, "997621": 69, "997934": [71, 72], "998063": 65, "99864670889": 142, "998766": 79, "999": [58, 59, 73, 85, 142], "999207": 82, "9995": [59, 67, 68], "9996": [59, 67, 68], "9996553": 55, "9997": [59, 67, 68], "9998": [59, 67, 68], "9999": [59, 67, 68], "99c8": 56, "A": [7, 9, 10, 13, 14, 18, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 51, 52, 53, 55, 56, 60, 61, 65, 66, 70, 72, 75, 76, 80, 81, 83, 84, 85, 86, 89, 90, 92, 94, 95, 96, 97, 100, 108, 125, 126, 127, 133, 134, 135, 136, 137, 139, 140, 142], "ATE": [9, 33, 37, 55, 62, 64, 74, 78, 85, 86, 94, 96, 105, 108, 110, 118, 126, 134], "ATEs": [64, 80], "And": [57, 80, 87, 126, 128], "As": [52, 53, 54, 55, 56, 57, 58, 60, 61, 64, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 82, 86, 87, 88, 95, 96, 100, 109, 110, 114, 116, 125, 126, 127, 136, 142], "At": [9, 10, 26, 54, 58, 59, 64, 73, 75, 77, 79, 82, 142], "Being": 142, "But": [74, 75], "By": [53, 54, 77, 83, 86, 95, 96, 100, 126, 132], "For": [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 33, 39, 43, 47, 51, 53, 54, 56, 58, 60, 61, 64, 65, 73, 74, 75, 76, 77, 79, 81, 83, 85, 86, 88, 89, 90, 92, 94, 95, 96, 97, 98, 100, 101, 102, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 134, 136, 138, 139, 142], "ITE": [14, 64], "ITEs": 64, "If": [4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 52, 54, 58, 60, 61, 66, 67, 68, 74, 75, 77, 79, 83, 89, 90, 92, 94, 95, 96, 98, 103, 110, 111, 113, 114, 115, 118, 125, 126, 127, 128, 130, 131, 132, 135, 136, 137, 142], "In": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 46, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142], "It": [21, 53, 54, 55, 67, 68, 70, 71, 72, 77, 78, 79, 83, 84, 86, 95, 109, 137, 141], "No": [12, 51, 53, 55, 56, 57, 58, 60, 61, 62, 64, 73, 78, 79, 83, 85, 87, 90, 91, 92, 95, 96, 98, 100, 110, 125, 139, 140], "Not": [96, 101], "Of": [75, 125, 142], "On": [52, 66, 76, 80, 84, 89, 140], "One": [55, 60, 78, 79, 86, 94, 125], "Or": 40, "Such": [86, 95], "That": [40, 142], "The": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100, 101, 104, 106, 107, 108, 109, 110, 115, 118, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 136, 137, 138, 140, 141, 142], "Then": [26, 69, 82, 84, 96, 125, 126, 136, 137, 138], "There": [55, 78, 86, 96, 101, 138, 142], "These": [32, 55, 56, 63, 76, 78, 81, 83, 85, 94, 96, 142], "To": [28, 51, 52, 54, 55, 56, 57, 58, 60, 61, 64, 65, 66, 67, 68, 69, 71, 72, 74, 75, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 89, 90, 92, 94, 95, 96, 109, 125, 126, 127, 132, 136, 138, 139, 142], "Will": [96, 99, 110, 112, 126, 129], "With": [13, 67, 68, 95, 140], "_": [52, 54, 59, 60, 61, 66, 67, 68, 69, 70, 71, 72, 77, 78, 79, 81, 82, 83, 84, 88, 89, 94, 96, 101, 109, 110, 125, 126, 127, 132], "_0": [52, 54, 66, 70, 77, 88, 89, 109, 110, 123, 124, 125, 126, 136], "_1": [9, 10, 14, 25, 26, 57, 80, 87, 110, 123, 124], "_2": [9, 10, 14, 25, 26, 80], "_3": [9, 10, 14, 25, 26], "_4": [9, 10, 14, 25, 26], "_5": [9, 14], "__": [46, 47], "__init__": [76, 84], "__version__": 138, "_a": [110, 114], "_all_coef": 109, "_all_s": 109, "_b": [110, 114], "_compute_scor": 28, "_compute_score_deriv": 28, "_coordinate_desc": 77, "_d": [83, 96], "_est_causal_pars_and_s": 141, "_estimator_typ": 76, "_h": [83, 96], "_i": [52, 57, 66, 82, 87, 89, 96, 101], "_id": 109, "_j": [9, 10, 14, 16, 25, 26, 54, 77, 125], "_l": 95, "_lower_quantil": 60, "_m": [95, 109], "_mean": 60, "_n": [110, 111, 113, 114, 118, 125, 126, 132, 135], "_n_folds_per_clust": 77, "_offset": 95, "_pred": 95, "_rmse": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "_upper_quantil": 60, "_x": 41, "_y": [83, 96], "a0": 76, "a09a": 56, "a09b": 56, "a1": 76, "a3d9": 56, "a4a147": 80, "a5e6": 56, "a5e7": 56, "a6ba": 56, "a79359d2da46": 56, "a840": 56, "a_": [57, 87], "a_0": 17, "a_1": 17, "a_j": [96, 102], "ab": [53, 84, 137], "ab71": 56, "abadi": [7, 58], "abb0fd28": 56, "abdt": [62, 90, 92, 139], "abl": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 52, 66, 75, 79, 80, 95, 126, 127, 136], "about": [22, 24, 55, 75, 78, 84, 96, 137, 139, 142], "abov": [52, 55, 64, 66, 67, 68, 71, 72, 75, 76, 78, 80, 81, 82, 83, 86, 89, 94, 95, 96, 100, 101, 138], "absolut": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 75, 95], "abstract": [28, 53, 54, 77, 110, 137, 141], "abus": [60, 96, 100, 101], "acc": [30, 53], "accept": [25, 94, 95], "access": [42, 43, 53, 55, 60, 61, 71, 72, 73, 75, 85, 95, 126, 132, 142], "accord": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 55, 57, 58, 64, 66, 69, 78, 82, 83, 86, 87, 95, 96, 125, 126, 128, 130, 131, 133, 134, 142], "accordingli": [57, 58, 75, 76, 78, 83, 87], "account": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 54, 55, 77, 78, 79, 85, 86, 126, 132, 136, 142], "accumul": [55, 78, 79, 85], "accuraci": [42, 46, 53, 96], "acemoglu": 140, "achiev": [54, 74, 77, 81, 86, 96, 125], "acic_2024_post": 80, "acknowledg": [55, 56, 78], "acm": 140, "acov": 140, "across": [21, 25, 55, 78, 80, 142], "action": 141, "activ": [4, 5, 6, 138, 141], "actual": [40, 60, 73, 86], "acycl": [57, 87, 142], "ad": [4, 5, 6, 7, 8, 28, 42, 43, 46, 47, 73, 90, 92, 95, 96, 125, 126, 127, 141], "adapt": [32, 78, 141], "add": [53, 54, 57, 58, 59, 62, 64, 71, 72, 73, 80, 82, 83, 85, 86, 87, 95, 96, 140, 141], "add_trac": 86, "addit": [14, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 44, 60, 61, 70, 86, 91, 92, 95, 96, 97, 110, 119, 126, 133, 134, 136, 140, 141], "addition": [9, 10, 64, 69, 79, 85, 95, 96, 109, 125, 126, 132, 139], "additional_inform": 21, "additional_paramet": 21, "address": 86, "adel": 140, "adj": [83, 86], "adj_coef_bench": 86, "adj_est": 86, "adj_vanderweelearah": 86, "adjust": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 45, 54, 59, 74, 77, 79, 85, 86, 94, 96, 101, 125, 126, 132, 140, 141, 142], "adopt": [58, 96, 98, 101], "advanc": [76, 93, 109, 140], "advantag": [52, 53, 55, 64, 66, 78, 79, 89, 138], "advers": [126, 127], "adversari": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 85, 126, 132, 136], "ae": [52, 54, 55, 57], "ae56": 56, "ae89": 56, "aesthet": 52, "aeturrel": 18, "afd9e4": 80, "affect": [64, 70, 96, 141, 142], "after": [53, 55, 56, 57, 58, 70, 78, 79, 86, 87, 94, 95, 96, 101, 126, 128, 132, 138, 142], "after_stat": 52, "ag": [55, 78, 79, 81, 85, 142], "again": [52, 53, 54, 55, 57, 58, 60, 64, 66, 73, 76, 77, 78, 83, 84, 85, 86, 87, 89, 126, 128], "against": [58, 73, 75, 81, 95], "agebra": 94, "agegt54": [56, 62, 90, 92, 139], "agelt35": [56, 62, 90, 92, 139], "agg": [53, 60, 84], "agg_df": 60, "agg_df_anticip": 60, "agg_dict": 60, "agg_dictionari": 60, "agg_did_obj": [96, 97], "aggrag": [96, 97], "aggreg": [21, 24, 53, 88, 97, 109, 141], "aggregate_over_split": 40, "aggregated_eventstudi": [60, 61], "aggregated_framework": [60, 61, 96, 97], "aggregated_group": 60, "aggregated_tim": [60, 61], "aggregation_0": 21, "aggregation_1": 21, "aggregation_color_idx": 21, "aggregation_method_nam": 21, "aggregation_nam": 21, "aggregation_weight": [21, 60, 61], "aggt": 53, "ai": 61, "aim": 83, "aipw": 80, "aipw_est_1": 80, "aipw_est_2": 80, "aipw_obj_1": 80, "aipw_obj_2": 80, "air": [54, 77], "al": [7, 8, 11, 13, 16, 17, 52, 54, 55, 56, 58, 66, 67, 68, 69, 70, 71, 72, 75, 77, 78, 79, 82, 85, 89, 96, 98, 101, 109, 110, 116, 118, 119, 120, 125, 126, 127, 136, 137, 139, 141], "alexandr": [70, 140], "algebra": 96, "algorithm": [51, 53, 54, 56, 57, 58, 60, 61, 64, 66, 69, 74, 75, 77, 79, 82, 85, 87, 93, 95, 96, 98, 100, 109, 110, 125, 141, 142], "alia": [42, 43, 46, 47], "align": [52, 54, 57, 59, 60, 66, 69, 75, 77, 78, 80, 81, 82, 87, 96, 100, 101, 110, 114, 141], "all": [4, 5, 6, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 46, 47, 48, 52, 53, 54, 55, 57, 58, 64, 66, 73, 74, 75, 76, 77, 78, 79, 81, 83, 86, 87, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 101, 109, 110, 114, 125, 126, 136, 137, 138, 141], "all_coef": 109, "all_dml1_coef": 88, "all_s": 109, "all_smpl": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 74], "all_smpls_clust": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "all_z_col": [54, 77], "allow": [4, 5, 6, 20, 21, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 55, 60, 64, 78, 79, 83, 92, 94, 95, 96, 109, 110, 125, 137, 141, 142], "almqvist": 140, "along": 95, "alpha": [15, 17, 20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 55, 57, 60, 62, 64, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 82, 88, 89, 94, 95, 96, 109, 110, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136], "alpha_": [16, 54, 77, 95], "alpha_0": [126, 136], "alpha_ml_l": 70, "alpha_ml_m": 70, "alpha_x": [12, 32, 96], "alreadi": [26, 57, 58, 60, 84, 87, 95, 96, 98], "also": [20, 22, 23, 24, 29, 32, 33, 39, 51, 52, 53, 54, 55, 56, 58, 60, 64, 65, 66, 67, 68, 71, 72, 73, 75, 76, 77, 78, 79, 81, 83, 85, 86, 89, 94, 95, 96, 109, 110, 125, 126, 127, 138, 139, 141, 142], "alter": [54, 77], "altern": [53, 55, 56, 78, 81, 93, 95, 125, 137, 139], "although": 86, "alwai": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 47, 53, 83, 141], "always_tak": [32, 55, 78], "alyssa": 140, "amamb": 77, "american": [15, 80], "amgrem": 77, "amhorn": 77, "amit": [86, 140], "amjavl": 77, "ammata": 77, "among": [55, 70, 78, 79, 85, 86], "amount": [24, 55, 76, 78, 79, 142], "amp": [51, 54, 56, 57, 58, 60, 61, 77, 79, 85, 87], "an": [4, 5, 6, 9, 10, 14, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 52, 53, 54, 55, 56, 57, 60, 61, 64, 66, 67, 68, 70, 73, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 94, 95, 96, 97, 101, 102, 109, 110, 125, 126, 127, 132, 137, 139, 140, 141, 142], "analog": [27, 28, 54, 60, 77, 79, 85, 94, 96, 98, 110, 111, 113, 114, 125, 126, 132], "analys": 142, "analysi": [20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 52, 54, 55, 66, 77, 78, 79, 84, 89, 93, 94, 96, 100, 127, 132, 136, 137, 141], "analyst": 84, "analyt": [80, 82], "analyz": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 55, 78, 79, 85, 142], "ancillari": 86, "andrea": 140, "angl": 55, "angrist": 80, "ani": [51, 52, 53, 56, 57, 58, 65, 66, 84, 86, 87, 89, 96, 138, 142], "anna": [9, 10, 14, 20, 22, 23, 25, 26, 53, 58, 60, 61, 96, 97, 98, 100, 101, 140], "annal": [125, 140], "anneal": 95, "annot": 52, "annual": 140, "anoth": [52, 53, 54, 55, 66, 75, 76, 77, 84, 89, 95, 96, 103], "anticip": [22, 24, 25, 53, 61, 96, 97, 100, 101], "anticipation_period": [22, 24, 25, 60], "anymor": [54, 77], "aos1161": 125, "aos1230": 125, "aos1671": 125, "ap": [55, 78], "ape_e401_uncond": 55, "ape_p401_uncond": 55, "api": [90, 92, 137, 141], "apo": [29, 30, 102, 115, 133], "apoorva": 141, "apoorva__l": 80, "apoorval": 80, "app": 141, "appeal": 86, "append": [66, 75, 84, 89], "appendix": [13, 19, 57, 60, 85, 87, 126, 127], "appli": [8, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 66, 74, 75, 77, 78, 79, 83, 86, 87, 89, 96, 109, 110, 125, 137, 139, 141, 142], "applic": [52, 58, 66, 80, 86, 89, 94, 109, 140, 142], "apply_along_axi": 81, "apply_cross_fit": [52, 109], "apply_crossfit": 141, "appreci": 137, "approach": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 53, 54, 64, 77, 83, 85, 86, 93, 95, 109, 125, 126, 127, 138, 140, 142], "appropri": [55, 70, 78, 96, 109, 142], "approx": [94, 96, 100], "approxim": [52, 66, 67, 68, 69, 75, 82, 86, 89, 94, 96, 125, 141, 142], "april": 60, "apt": 138, "ar": [4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 66, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 92, 94, 95, 96, 97, 98, 100, 101, 102, 103, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 117, 118, 121, 122, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 142], "arang": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 59, 66, 69, 79, 81, 82, 85, 86, 95], "arbitrarili": [43, 47], "architectur": [110, 140], "arellano": 140, "arg": [76, 83, 94, 96], "argmax": 84, "argmin": 75, "argu": [52, 55, 66, 78, 79, 85, 89, 142], "argument": [16, 17, 18, 20, 22, 23, 25, 26, 29, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 44, 55, 58, 60, 61, 67, 68, 73, 75, 78, 79, 84, 88, 94, 95, 96, 97, 141, 142], "aris": [52, 53, 54, 66, 77, 86, 89, 142], "aronow": 80, "around": [53, 55, 78, 79, 83, 96, 110], "arr": 81, "arrai": [4, 5, 6, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 45, 46, 47, 57, 58, 60, 61, 64, 66, 67, 68, 69, 75, 77, 80, 81, 84, 85, 86, 87, 88, 89, 94, 95, 109, 125, 126, 132, 139, 141, 142], "arrang": 54, "array_lik": 36, "articl": [18, 137], "arxiv": [16, 53, 54, 77, 84, 86, 137, 140, 141], "as_learn": [56, 95], "asarrai": [67, 68], "aspect": [55, 78, 79], "assert": 95, "assess": 53, "asset": [79, 85, 142], "assign": [4, 5, 6, 25, 55, 60, 72, 78, 83, 94, 95, 96, 97, 108, 126, 142], "assmput": [96, 108], "associ": [55, 70, 78, 96, 125, 140], "assum": [51, 54, 58, 65, 77, 80, 81, 84, 86, 96, 98, 100, 101, 110, 111, 113, 114, 125, 126, 136, 142], "assumpt": [53, 54, 55, 57, 58, 59, 60, 75, 77, 78, 80, 83, 87, 96, 98, 100, 101, 108, 125, 142], "assur": 141, "astyp": [60, 61, 65, 83, 86], "asymptot": [27, 28, 52, 54, 66, 77, 89, 109, 125, 140], "ate": 64, "ate_estim": [57, 87], "ates": 64, "athei": 140, "att": [9, 24, 25, 33, 53, 59, 73, 74, 81, 86, 94, 96, 97, 98, 100, 101, 105, 110, 118, 126, 134, 141], "att_": [96, 100], "att_gt": [53, 61], "attach": 53, "atte_estim": 58, "attempt": [42, 43], "attenu": [55, 78], "attr": 55, "attribut": [4, 5, 6, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 45, 46, 47, 75, 76, 88, 91, 92, 95, 109, 110, 125], "attributeerror": [42, 43], "attrict": 96, "attrit": [37, 57, 87, 96, 108], "au": [56, 95, 137, 139], "auc": 53, "author": [53, 86, 137], "auto_ml": 76, "autodoubleml": 76, "autom": 76, "automat": [21, 24, 52, 60, 61, 66, 73, 89, 94, 126, 132], "automl": 141, "automl_l": 76, "automl_l_lesstim": 76, "automl_m": 76, "automl_m_lesstim": 76, "automobil": [54, 77], "autos": 70, "autosklearn": 76, "auxiliari": [52, 66, 89], "avail": [12, 53, 55, 56, 58, 60, 64, 70, 75, 78, 79, 80, 81, 83, 86, 89, 94, 95, 96, 97, 98, 100, 126, 136, 137, 138, 141, 142], "avaiv": 45, "aver": 64, "averag": [9, 10, 25, 26, 29, 30, 32, 33, 39, 51, 53, 56, 57, 58, 59, 60, 61, 65, 73, 79, 80, 81, 83, 85, 86, 87, 93, 97, 98, 101, 102, 103, 104, 105, 108, 115, 118, 125, 133, 134, 140, 141, 142], "average_it": 64, "avoid": [52, 53, 66, 83, 96, 109, 138, 141], "awai": 85, "ax": [21, 24, 59, 60, 61, 64, 66, 67, 68, 69, 71, 72, 75, 76, 77, 78, 79, 80, 82, 83], "ax1": [64, 69, 74, 79, 82], "ax2": [64, 69, 74, 79, 82], "axhlin": [59, 76, 83], "axi": [21, 24, 54, 55, 64, 70, 74, 75, 77, 78, 80, 81, 83], "axvlin": [64, 66], "b": [18, 20, 22, 23, 24, 25, 52, 54, 56, 66, 67, 68, 77, 80, 82, 83, 86, 89, 94, 95, 96, 101, 125, 126, 136, 137, 139, 140], "b208": 56, "b371": 56, "b5d34a6f42b": 56, "b5d7": 56, "b_": 96, "b_0": 17, "b_1": 17, "b_j": 18, "bach": [70, 75, 76, 86, 137, 140, 141], "backbon": 75, "backend": [4, 5, 6, 53, 79, 85, 86, 90, 91, 93, 141], "backward": 141, "bad": 80, "balanc": [55, 60, 78, 79], "band": [53, 93, 142], "bandwidth": [34, 35, 36, 40, 83, 96], "bar": [73, 76, 78, 94, 96, 110, 115, 118, 126, 133], "base": [11, 14, 20, 22, 23, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 45, 52, 53, 54, 55, 57, 58, 59, 60, 61, 64, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 94, 95, 96, 97, 101, 108, 109, 110, 125, 126, 127, 132, 137, 139, 140, 141, 142], "base_estim": [46, 47, 83], "baseestim": 84, "baselin": [14, 22, 55, 76, 78], "basi": [29, 33, 39, 44, 67, 68, 74, 94], "basic": [53, 54, 55, 58, 61, 77, 78, 79, 80, 83, 85, 86, 93, 95], "basis_df": 74, "basis_matrix": 74, "batch": 56, "battocchi": 140, "bay": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 125], "bb2913dc": 56, "bbotk": [56, 95, 141], "bbox_inch": 66, "bbox_to_anchor": 66, "bcallaway11": 53, "bd929a9e": 56, "bde4": 56, "becam": [55, 78, 79], "becaus": [43, 47, 51, 52, 53, 54, 65, 66, 72, 73, 77, 80, 84, 86, 89, 142], "becker": [56, 95], "becom": [54, 72, 76, 77, 94, 109], "bee": 59, "been": [21, 24, 54, 55, 60, 61, 76, 77, 78, 79, 85, 86, 94, 95, 96, 101, 141], "befor": [25, 53, 55, 59, 60, 61, 64, 73, 78, 82, 86, 96, 98, 142], "begin": [12, 15, 16, 52, 54, 55, 56, 57, 59, 60, 66, 69, 75, 77, 78, 80, 81, 82, 87, 88, 90, 92, 95, 96, 100, 101, 109, 110, 114, 125, 139, 142], "behav": [60, 61, 72, 84], "behavior": [55, 80, 95], "behaviour": 72, "behind": 96, "being": [14, 19, 25, 27, 28, 41, 46, 47, 54, 77, 83, 86, 96, 100, 101, 109, 110, 116, 125, 126, 132, 137], "belloni": [13, 70, 125, 140], "below": [51, 55, 65, 78, 80, 84, 96, 138, 139], "bench_x1": 86, "bench_x2": 86, "benchmark": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 64, 73, 127, 141], "benchmark_dict": [48, 85], "benchmark_inc": 85, "benchmark_pira": 85, "benchmark_result": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39], "benchmark_twoearn": 85, "benchmarking_set": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 64, 73, 85, 86, 126, 127], "benchmarking_vari": 73, "benefit": [52, 55, 66, 78, 89], "bernoulli": 12, "berri": [54, 77], "besid": 139, "best": [29, 33, 39, 43, 44, 47, 60, 67, 68, 71, 72, 76, 138], "best_loss": 76, "beta": [12, 13, 15, 19, 37, 55, 57, 78, 81, 83, 87, 96], "beta_": [57, 87], "beta_0": [11, 57, 81, 87, 94], "beta_a": [9, 10, 86], "beta_j": [12, 13, 15, 19], "better": [53, 60, 64, 75, 86, 96, 101], "between": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 57, 59, 60, 64, 65, 69, 70, 76, 80, 82, 84, 85, 86, 87, 96, 103, 110, 111, 113, 114, 118, 121, 122, 125, 126, 136, 139, 141], "betwen": [51, 65], "beyond": 140, "bia": [19, 51, 57, 65, 70, 83, 86, 87, 93, 96, 108, 109, 110, 123, 124, 126, 136, 140, 141], "bias": [51, 55, 60, 65, 78, 79, 85, 142], "bias_bench": 86, "bibtex": 137, "big": [70, 88, 109, 110, 111, 119, 125, 126, 128, 130, 131, 134, 135, 136], "bigg": [54, 77, 110, 117, 118, 126, 134], "bilia": 8, "bilinski": 140, "bin": [52, 64, 66, 138], "binari": [11, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 37, 39, 41, 51, 53, 55, 56, 58, 65, 73, 74, 75, 78, 80, 81, 86, 94, 95, 98, 101, 104, 105, 108, 126, 133, 134, 141, 142], "binary_outcom": 41, "binary_treat": [11, 67, 71, 73], "bind": 141, "binder": [56, 95, 137, 139, 141], "binomi": [65, 80, 81, 82, 84], "bischl": [56, 95, 137, 139], "black": [52, 56, 62, 90, 92, 139], "blob": 53, "blog": 18, "blondel": [137, 139], "blp": [44, 54, 77], "blp_data": [54, 77], "blp_model": [71, 72], "blue": [52, 54, 57, 77], "bodori": 140, "bond": [55, 78, 79], "bonferroni": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 125], "bonu": [8, 56, 90, 92, 139], "book": [56, 86, 95], "bool": [4, 5, 6, 9, 11, 14, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 73, 83], "boolean": [19, 71, 72, 90, 92, 109], "boost": [51, 55, 58, 60, 65, 75, 78], "boost_class": [55, 78], "boost_summari": 78, "boostrap": [69, 141], "bootstrap": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 60, 61, 64, 67, 68, 69, 71, 72, 79, 82, 93, 94, 96, 97, 109, 110, 137, 139, 141, 142], "both": [10, 11, 20, 21, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 55, 56, 58, 59, 74, 75, 76, 78, 79, 81, 83, 84, 85, 86, 90, 92, 95, 96, 100, 125, 126, 127, 132, 135, 136, 141, 142], "bottom": [54, 55, 75, 77, 78, 79], "bound": [14, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 60, 64, 73, 74, 78, 85, 86, 126, 127, 132, 136, 142], "branch": 56, "brantli": [53, 140], "break": [52, 141], "breviti": 142, "brew": 138, "brewer": 54, "bridg": 86, "brief": 89, "bring": [51, 65], "brucher": [137, 139], "bsd": 141, "bst": 78, "budget": [76, 95], "bug": [137, 141], "build": [54, 75, 77, 81], "build_design_matric": [67, 68], "build_sim_dataset": 53, "built": [45, 76, 95, 137], "bureau": [86, 109, 140], "busi": [16, 19, 54, 77, 86, 140], "b\u00fchlmann": 140, "c": [7, 8, 10, 13, 15, 17, 25, 26, 51, 52, 53, 54, 55, 56, 59, 62, 65, 66, 70, 71, 72, 77, 78, 80, 83, 84, 89, 90, 92, 95, 96, 101, 110, 114, 126, 131, 137, 138, 139, 140, 142], "c1": [7, 8, 17, 54, 70, 77, 84, 89, 137, 140], "c68": [7, 8, 17, 54, 70, 77, 84, 89, 137, 140], "c895": 56, "c_": [24, 96, 100, 101, 110, 114, 125, 126, 131], "c_d": [13, 126, 134, 135, 136], "c_y": [13, 126, 136], "ca1af7be64b2": 56, "caac5a95": 56, "calcualt": 81, "calcul": [29, 33, 39, 53, 55, 60, 64, 67, 68, 69, 71, 72, 75, 76, 78, 82, 85, 126, 132, 136], "calendar": [60, 61], "calibr": [75, 76, 86], "call": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 43, 46, 47, 51, 53, 54, 55, 56, 57, 58, 60, 61, 65, 67, 68, 69, 71, 72, 77, 78, 79, 81, 82, 83, 85, 86, 87, 90, 92, 95, 109, 110, 125, 126, 132, 136, 139, 141, 142], "callabl": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 66, 67, 68, 75, 93, 95, 137], "callawai": [25, 53, 60, 61, 96, 97, 100, 101, 140], "camera": 70, "cameron": [54, 77], "can": [4, 5, 6, 9, 11, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 45, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 98, 100, 101, 102, 108, 109, 110, 111, 113, 114, 115, 118, 121, 122, 125, 126, 127, 132, 133, 134, 135, 136, 137, 138, 139, 141, 142], "candid": 86, "cannot": [75, 83, 86, 96, 142], "capabl": [4, 5, 6, 51, 65], "capo": [29, 74], "capo0": 74, "capo1": 74, "capsiz": [64, 76, 80, 83], "capthick": [64, 83], "cardin": [54, 77], "care": [84, 95], "carlo": [9, 10, 11, 14, 67, 68, 71, 72, 86, 140], "casalicchio": [56, 95, 137, 139], "case": [4, 5, 6, 8, 11, 22, 29, 32, 33, 40, 51, 54, 55, 60, 65, 67, 68, 69, 70, 72, 73, 74, 76, 77, 81, 82, 83, 84, 85, 86, 90, 92, 94, 95, 96, 98, 101, 108, 109, 110, 114, 125, 126, 132, 139, 141, 142], "cat": [52, 141], "catboost": 75, "cate": [33, 39, 44, 74, 93, 141], "cate_obj": 94, "cattaneo": [96, 140], "caus": [52, 66, 83, 89], "causal": [12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 54, 55, 56, 57, 65, 66, 74, 75, 76, 77, 78, 80, 85, 87, 88, 89, 90, 92, 93, 96, 101, 109, 125, 126, 132, 140], "causal_contrast": [30, 64, 74, 96], "causal_contrast_att": 74, "causal_contrast_c": 74, "causal_contrast_model": [64, 96], "causaldml": 140, "causalweight": 140, "caution": 125, "caveat": [72, 86], "cbind": 54, "cbook": [60, 61], "cc": 78, "ccp_alpha": [33, 45, 78], "cd": 138, "cd_fast": 77, "cda85647": 56, "cdf": 94, "cdid": [54, 77], "cdot": [9, 10, 14, 25, 26, 41, 54, 59, 60, 69, 73, 77, 80, 82, 83, 86, 94, 96, 97, 100, 101, 110, 114, 115, 118, 119, 123, 124, 125, 126, 131, 133], "cdot1": 73, "cell": 76, "center": [60, 70], "central": [109, 141], "certain": [72, 96, 110, 114], "cexcol": 54, "cexrow": 54, "cf_d": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 60, 64, 73, 74, 85, 86, 126, 127, 132, 133, 134, 135, 136, 142], "cf_y": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 60, 64, 73, 74, 85, 86, 126, 127, 132, 133, 134, 135, 136, 142], "chad": 86, "chain": 72, "chainedassignmenterror": 72, "challeng": [54, 77, 126, 127], "chang": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 55, 57, 58, 61, 72, 79, 85, 86, 87, 96, 101, 110, 118, 125, 126, 127, 128, 130, 131, 132, 133, 134, 138, 140, 141], "channel": 142, "chapter": [27, 28, 56, 95, 126, 136], "charact": [55, 56, 95, 141], "characterist": [85, 142], "chart": 76, "check": [42, 43, 46, 47, 52, 55, 60, 66, 75, 76, 78, 79, 84, 88, 89, 137, 138, 141], "check_data": 141, "check_scor": 141, "checkmat": 141, "chernozhukov": [7, 8, 13, 15, 17, 52, 54, 55, 66, 70, 75, 76, 77, 78, 79, 84, 85, 89, 109, 110, 118, 125, 126, 127, 136, 137, 140, 141], "chetverikov": [7, 8, 17, 54, 70, 77, 84, 89, 125, 137, 140], "chiang": [16, 54, 77, 140], "chieh": 140, "choic": [4, 5, 6, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 55, 60, 61, 70, 78, 81, 94, 95, 96, 100, 110, 114, 126, 127, 132, 136, 141], "cholecyst": 84, "choos": [51, 55, 65, 66, 70, 75, 78, 79, 88, 96, 100, 109, 110, 111, 113, 114, 118, 121, 122, 125, 139, 142], "chosen": [10, 14, 29, 75, 95, 96], "chou": 80, "chr": 55, "christian": [70, 140], "christoph": 140, "chunk": 95, "ci": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 58, 59, 60, 61, 64, 67, 68, 69, 71, 72, 73, 74, 76, 78, 79, 82, 83, 85, 86, 94, 96, 126, 132, 141, 142], "ci_at": 64, "ci_cvar": [69, 79], "ci_cvar_0": 69, "ci_cvar_1": 69, "ci_joint": [60, 61, 64], "ci_joint_cvar": 69, "ci_joint_lqt": 82, "ci_joint_qt": 82, "ci_length": 58, "ci_low": 64, "ci_lpq_0": 82, "ci_lpq_1": 82, "ci_lqt": [79, 82], "ci_pointwis": 64, "ci_pq_0": [79, 82], "ci_pq_1": [79, 82], "ci_qt": [79, 82], "ci_upp": 64, "cinelli": [86, 126, 127, 140], "circumv": 142, "citat": 141, "claim": 56, "clarifi": [60, 61], "clash": 53, "class": [0, 4, 5, 6, 20, 21, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 55, 56, 57, 58, 60, 61, 62, 64, 73, 74, 76, 78, 79, 84, 85, 87, 88, 90, 91, 92, 94, 95, 96, 97, 109, 110, 125, 137, 139, 141], "class_estim": 83, "class_learn": 79, "class_learner_1": 75, "class_learner_2": 75, "classes_": [76, 84], "classic": [53, 54, 77, 142], "classif": [33, 42, 46, 51, 53, 55, 56, 57, 60, 61, 75, 76, 81, 85, 94, 95, 96, 98, 100, 142], "classifavg": 56, "classifi": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 39, 40, 42, 46, 56, 64, 76, 83, 95, 141], "classifiermixin": 84, "classmethod": [4, 5, 6], "claudia": [140, 141], "claus": 141, "clean": 141, "cleaner": 75, "cleanup": 141, "clear": [54, 77], "clearli": [60, 83], "clever": 75, "clone": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 56, 66, 75, 77, 79, 88, 95, 96, 109, 110, 125, 126, 132, 138, 139], "close": [53, 55, 78, 84, 86, 126, 127], "cluster": [4, 16, 20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 61, 140, 141], "cluster_col": [4, 54, 77], "cluster_var": [4, 16], "cluster_var_i": [4, 54, 77], "cluster_var_j": [4, 54, 77], "cmap": 77, "cmd": 141, "co": [18, 59], "codaci": 141, "code": [18, 29, 33, 39, 51, 53, 54, 55, 56, 57, 65, 70, 78, 89, 94, 95, 96, 109, 110, 125, 138, 139, 141, 142], "codecov": 141, "coef": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 100, 109, 110, 125, 139, 142], "coef_": 86, "coef_df": 54, "coef_valu": 76, "coeffici": [9, 10, 11, 24, 43, 44, 47, 55, 57, 71, 72, 75, 78, 80, 81, 83, 86, 87, 94, 125, 126, 132, 142], "coefs_t": 81, "coefs_w": 81, "coffici": [126, 132], "cofid": 44, "coincid": [59, 74, 79], "col": [52, 54, 72, 78], "col_nam": 60, "collect": [56, 57, 58, 77, 87], "colnam": [54, 75], "color": [21, 24, 55, 57, 59, 64, 66, 67, 68, 69, 76, 77, 78, 79, 80, 82, 83, 86], "color_palett": [21, 24, 60, 64, 66, 77, 78, 79], "colorbar": 77, "colorblind": [21, 24, 60, 64], "colorramppalett": 54, "colorscal": [67, 68], "colour": [52, 54], "column": [4, 5, 6, 58, 59, 60, 61, 62, 64, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 83, 85, 86, 87, 90, 91, 92, 94, 95, 96, 100, 109, 139, 141, 142], "column_stack": [59, 64, 71, 72, 83, 85, 86, 96], "colv": 54, "com": [18, 53, 55, 56, 61, 70, 80, 86, 95, 138], "comb": 70, "combin": [22, 24, 53, 54, 56, 58, 61, 64, 74, 75, 76, 77, 86, 95, 96, 100, 109, 126, 132, 141], "combind": 79, "combined_loss": 70, "come": [88, 95, 110, 126, 127, 137, 142], "command": [138, 141], "comment": [90, 92], "common": [75, 85, 86, 94, 96, 140], "companion": 140, "compar": [52, 54, 59, 60, 66, 67, 68, 69, 71, 72, 74, 77, 80, 82, 83, 84, 86, 89, 95, 96, 126, 127], "comparevers": 55, "comparison": [60, 64, 75, 80], "compat": [51, 53, 65, 141], "complement": 86, "complet": [76, 89, 126, 132, 138], "complex": [33, 53, 76], "compli": [83, 96], "complianc": [82, 83, 96, 110, 119], "complic": [56, 142], "complier": [55, 78, 79, 82, 83, 94, 96], "compon": [25, 46, 47, 53, 55, 70, 75, 76, 78, 81, 94, 95, 109, 110, 111, 113, 114, 115, 117, 118, 119, 121, 122, 141], "compont": 53, "composit": 140, "compris": 125, "comput": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 48, 52, 53, 55, 56, 60, 61, 66, 78, 79, 84, 85, 86, 109, 110, 126, 127, 128, 130, 131, 132, 133, 134, 137, 140, 141, 142], "computation": [126, 127], "concat": [76, 77, 78, 81, 125], "concaten": [59, 78, 125], "concentr": 125, "concern": 86, "conclud": [83, 86, 142], "cond": [96, 98, 108], "conda": [77, 140, 141], "condit": [9, 10, 11, 27, 28, 29, 31, 33, 39, 52, 54, 55, 57, 58, 59, 60, 64, 66, 73, 74, 77, 78, 81, 83, 86, 87, 89, 93, 96, 100, 101, 125, 126, 133, 134, 136, 139, 140, 141, 142], "conduct": [94, 96, 98, 100, 142], "conf": [53, 82], "confer": 140, "confid": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 53, 54, 55, 57, 58, 60, 61, 64, 67, 68, 69, 71, 72, 74, 77, 79, 82, 83, 85, 87, 93, 94, 96, 109, 110, 126, 132, 139, 140, 142], "confidenceband": 69, "confidenti": 86, "config": 80, "configur": [56, 76], "confint": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 55, 57, 58, 59, 60, 61, 64, 67, 68, 69, 71, 72, 74, 75, 79, 81, 82, 83, 84, 85, 87, 94, 109, 125, 137, 139, 142], "conflict": 138, "confound": [9, 10, 11, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 51, 55, 60, 65, 73, 78, 82, 85, 86, 90, 92, 96, 106, 107, 125, 126, 127, 132, 135, 136, 139, 140, 141, 142], "congress": 140, "connect": [55, 78, 79], "consequ": [9, 10, 54, 73, 77, 85, 94, 96, 98, 126, 127, 133, 134, 136], "conserv": [85, 86, 126, 136], "consid": [31, 32, 33, 34, 35, 41, 52, 54, 55, 57, 58, 59, 60, 61, 66, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 94, 95, 96, 97, 101, 104, 105, 109, 110, 125, 126, 127, 137, 142], "consider": [86, 96], "consist": [38, 39, 43, 47, 55, 58, 76, 78, 79, 80, 86, 89, 90, 92, 96, 101, 106, 107, 139, 141], "consol": [52, 141], "constant": [13, 25, 43, 47, 70, 81, 94, 96, 125], "constrained_layout": 66, "construct": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 44, 56, 59, 60, 67, 68, 69, 74, 79, 84, 85, 88, 94, 110, 116, 124, 125, 141, 142], "construct_framework": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "construct_iv": 77, "constructiv": 54, "constructor": 56, "consum": [54, 77], "cont": 14, "cont_d": 64, "contain": [14, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 46, 47, 52, 54, 55, 60, 64, 66, 67, 68, 71, 72, 75, 77, 78, 84, 89, 91, 92, 94, 95, 96, 97, 100, 125, 126, 127, 132, 141], "context": [86, 96, 108, 142], "contin": [14, 76], "continu": [14, 51, 56, 64, 65, 70, 80, 83, 96, 126, 136, 141, 142], "contour": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 70, 73, 85, 86, 126, 132], "contour_plot": 86, "contours_z": [67, 68], "contrast": [30, 58, 69, 74, 96, 103], "contribut": [138, 141], "contributor": 141, "control": [15, 22, 24, 25, 41, 53, 61, 70, 74, 79, 81, 83, 84, 86, 96, 97, 100, 101, 110, 114, 126, 131, 142], "control_group": [22, 24, 60, 61, 96, 97, 100], "convent": [40, 55, 78, 79, 83, 96, 101], "converg": [52, 66, 75, 77, 89], "convergencewarn": 77, "convers": 77, "convert": [69, 77, 82], "convex": 80, "cooper": 141, "coor": [56, 95, 137, 139], "coordin": 86, "copi": [72, 76, 78, 81, 86], "cor": [126, 136], "core": [25, 58, 60, 61, 62, 64, 69, 73, 77, 78, 79, 82, 85, 87, 90, 91, 92, 95, 139, 141], "cores_us": [69, 79, 82], "correct": [73, 74, 86, 94, 125, 141], "correctli": [42, 46, 58, 80, 85, 126, 136], "correl": [20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 54, 57, 70, 77, 84, 85, 87, 96, 126, 127, 136], "correpond": [60, 96], "correspond": [9, 10, 14, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 55, 56, 58, 59, 60, 61, 64, 66, 67, 68, 70, 74, 75, 77, 78, 79, 81, 82, 85, 86, 89, 94, 95, 96, 98, 100, 101, 102, 108, 109, 110, 114, 125, 126, 127, 131, 132, 134, 136, 141, 142], "correspondingli": 60, "cosh": 18, "coul": 54, "could": [51, 56, 60, 65, 67, 68, 76, 86, 141, 142], "counfound": [9, 10, 82, 85, 94, 126, 136], "count": [64, 78, 79], "countour": [126, 132], "coupl": [55, 78, 79], "cournapeau": [137, 139], "cours": [55, 75, 78, 86, 125, 142], "cov": [9, 37, 41, 83], "cov_nam": [83, 96], "cov_typ": [29, 33, 39, 44, 141], "covari": [4, 5, 6, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 33, 38, 39, 40, 41, 44, 45, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 64, 66, 67, 68, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 84, 85, 86, 87, 89, 90, 91, 92, 94, 95, 96, 98, 100, 106, 107, 108, 110, 111, 113, 114, 125, 126, 127, 139, 140, 141], "cover": [53, 70, 85], "coverag": [60, 75, 83, 84, 94, 141], "cp": [55, 56, 95], "cpu": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "cpu_count": [69, 79, 82], "cran": [56, 140, 141], "creat": [11, 21, 24, 25, 51, 54, 56, 60, 64, 65, 66, 67, 68, 69, 71, 72, 77, 79, 81, 82, 86, 95, 126, 127, 132, 136, 138, 141], "create_synthetic_group_data": 81, "critic": [86, 142], "cross": [20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 53, 55, 56, 57, 66, 75, 76, 78, 79, 83, 86, 89, 93, 95, 111, 113, 124, 125, 128, 132, 141, 142], "cross_sectional_data": [23, 26, 58, 96, 98], "crossfit": [75, 96], "crosstab": 80, "crucial": [70, 96, 142], "csail": [137, 139], "csdid": 61, "csv": [61, 70], "cumul": 96, "current": [45, 53, 60, 72, 74, 110, 126, 136, 137, 138, 142], "custom": [21, 24, 52, 53, 66, 86, 95], "custom_measur": 53, "cut": 81, "cutoff": [40, 41, 83, 96], "cv": [56, 78, 95, 109], "cv_glmnet": [54, 55, 56, 57, 95, 96, 125, 139], "cvar": [31, 36, 93, 116, 141], "cvar_0": 69, "cvar_1": 69, "d": [4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 102, 104, 105, 106, 107, 108, 109, 110, 111, 113, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 132, 133, 134, 135, 136, 137, 139, 140, 142], "d0": [69, 82, 125], "d0_true": 82, "d0cdb0ea4795": 56, "d1": [69, 80, 82, 125], "d10": 125, "d1_true": 82, "d2": [80, 125], "d21ee5775b5f": 56, "d2cml": 61, "d3": 125, "d4": 125, "d5": 125, "d5a0c70f1d98": 56, "d6": 125, "d7": 125, "d8": 125, "d9": 125, "d_": [14, 16, 54, 59, 64, 77, 96, 98, 101, 125], "d_0": [96, 102], "d_1": [80, 125], "d_2": 80, "d_col": [4, 5, 6, 24, 51, 52, 54, 55, 56, 57, 60, 61, 65, 67, 68, 71, 72, 74, 77, 78, 79, 81, 83, 84, 85, 88, 89, 90, 91, 92, 95, 96, 97, 100, 109, 110, 139, 141, 142], "d_i": [11, 12, 13, 15, 17, 18, 19, 52, 57, 58, 64, 66, 69, 80, 82, 83, 87, 89, 96, 98, 108], "d_j": [64, 96, 102, 103, 125], "d_k": [96, 103, 125], "d_l": [96, 102], "d_w": 81, "da1440": 80, "dag": [57, 86, 87, 142], "dai": 60, "dark": [52, 66], "darkblu": 54, "darkr": 54, "dash": [57, 60, 64], "dat": [90, 92], "data": [0, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 46, 47, 53, 59, 70, 75, 80, 84, 88, 90, 91, 93, 94, 95, 97, 100, 101, 109, 125, 130, 131, 132, 140, 141], "data_apo": 64, "data_cvar": 79, "data_dict": [40, 67, 68, 71, 72, 73, 83, 96], "data_dml": 85, "data_dml_bas": [55, 67, 68, 71, 72, 78, 79, 81], "data_dml_base_iv": [55, 78, 79], "data_dml_flex": [55, 78], "data_dml_flex_iv": 55, "data_dml_iv_flex": 78, "data_dml_new": 81, "data_fram": 142, "data_lqt": 79, "data_pq": 79, "data_qt": 79, "data_transf": [54, 77, 78], "datafram": [4, 5, 6, 7, 8, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 45, 54, 57, 58, 59, 60, 61, 62, 64, 65, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 91, 94, 95, 96, 98, 110, 125, 126, 127, 132, 139, 142], "dataset": [0, 4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 57, 58, 60, 64, 66, 67, 68, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 132, 139, 142], "datatyp": [61, 141], "date": [24, 25, 95], "date_format": 24, "datetim": [6, 24, 25, 60, 91, 92], "datetime64": [60, 91, 92], "datetime_unit": [6, 24, 60, 91, 92, 96, 97, 100], "db": [55, 78, 79, 85, 142], "dbl": [53, 54, 55, 56, 90, 92, 125, 139, 142], "dc13a11076b3": 56, "ddc9": 56, "de": [51, 65, 140], "deal": [51, 65], "debias": [7, 8, 16, 17, 54, 70, 77, 84, 93, 95, 109, 137, 140, 141], "debt": [55, 78, 79], "decai": [57, 87], "decid": [55, 78, 84], "decis": [33, 51, 55, 65, 78, 79, 94, 96, 140, 142], "decision_effect": 51, "decision_impact": [51, 65], "decisiontreeclassifi": [33, 45, 78], "decisiontreeregressor": 78, "declar": 142, "decreas": 83, "deep": [42, 43, 46, 47, 76], "deeper": 33, "def": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 60, 66, 69, 75, 76, 77, 80, 81, 82, 84, 86, 95, 110], "default": [4, 5, 6, 9, 10, 11, 14, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 53, 54, 57, 58, 60, 61, 71, 72, 75, 77, 81, 83, 85, 86, 87, 88, 94, 95, 96, 109, 125, 126, 132, 133, 139, 142], "default_arg": 60, "default_convert": 77, "default_jitt": 24, "defier": [83, 96], "defin": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 40, 41, 43, 47, 52, 55, 56, 58, 64, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 78, 79, 81, 82, 83, 84, 85, 86, 94, 95, 96, 98, 100, 101, 108, 110, 111, 113, 114, 126, 127, 132, 136], "definit": [18, 60, 71, 72, 74, 96, 100, 126, 133, 134], "defint": 126, "degre": [41, 55, 67, 68, 74, 77, 78, 83, 94, 126, 127], "dekel": 140, "delete_origin": 56, "deliber": 80, "delta": [15, 24, 53, 58, 60, 86, 96, 98, 100, 101, 110, 114, 126, 131], "delta_bench": 86, "delta_i": 53, "delta_j": 15, "delta_t": [25, 60], "delta_theta": [48, 64, 73, 85, 86, 126, 127], "delta_v": 86, "demand": [54, 77, 126, 127], "demir": [7, 8, 17, 54, 70, 77, 84, 89, 109, 137, 140], "demo": 86, "demonstr": [52, 53, 54, 66, 77, 83, 86, 90, 92, 96, 125, 137, 139], "deni": 140, "denomin": [126, 127, 133, 134], "denot": [38, 54, 55, 57, 58, 59, 77, 78, 83, 86, 87, 94, 96, 98, 100, 101, 102, 106, 110, 126, 127, 132, 134, 136], "dens_net_tfa": 55, "densiti": [34, 35, 36, 52, 57, 64, 66], "dep": 62, "dep1": [56, 62, 90, 92, 139], "dep2": [56, 62, 90, 92, 139], "depend": [11, 25, 29, 31, 33, 34, 36, 56, 58, 60, 67, 68, 71, 72, 73, 75, 76, 81, 83, 88, 94, 95, 96, 100, 110, 119, 120, 126, 127, 133, 136, 139, 140], "deprec": [60, 61, 88, 96, 109, 110, 126], "depreci": 141, "depth": [33, 45, 55, 56, 81, 88, 94, 95, 96, 109, 110, 125, 139, 142], "deriv": [20, 22, 23, 25, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 96, 125], "describ": [21, 53, 54, 60, 77, 78, 79, 86, 95, 109, 138, 141], "descript": [55, 61, 62, 85, 95, 109, 110, 114, 126, 127, 131], "deserv": 96, "design": [25, 40, 41, 64, 76, 93, 140, 141], "design_info": [67, 68], "design_matrix": [67, 68, 94], "desir": [10, 56, 81, 96, 138], "detail": [20, 22, 23, 27, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 52, 55, 56, 58, 59, 64, 66, 70, 76, 79, 83, 85, 86, 89, 90, 92, 94, 95, 97, 98, 100, 101, 108, 110, 116, 118, 119, 120, 123, 124, 125, 126, 127, 131, 136, 137, 138, 139, 141, 142], "determin": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 47, 55, 69, 78, 79, 82, 83, 85, 96, 125, 126, 136], "determinist": [81, 83, 94, 96], "deutsch": 137, "dev": [138, 141], "develop": [53, 54, 56, 77, 86, 96, 98, 101, 141], "deviat": [75, 96, 126, 136], "dezeur": 140, "df": [4, 5, 6, 24, 51, 52, 54, 57, 59, 60, 64, 65, 67, 68, 69, 72, 74, 77, 80, 82, 83, 85, 86, 87, 89, 91, 92, 94, 96, 97, 100], "df_agg": 70, "df_anticip": 60, "df_apo": 64, "df_apo_ci": 64, "df_apos_ci": 64, "df_ate": 64, "df_bench": 86, "df_binari": 86, "df_bonu": [56, 90, 92, 139], "df_capo0": 74, "df_capo1": 74, "df_cate": [67, 68, 74], "df_causal_contrast_c": 74, "df_ci": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44], "df_coef": 75, "df_cvar": 79, "df_fuzzi": 83, "df_lqte": 79, "df_ml_g0": 75, "df_ml_g1": 75, "df_ml_m": 75, "df_pa": [58, 87], "df_plot": 54, "df_post_treat": 60, "df_pq": 79, "df_qte": 79, "df_result": 70, "df_sharp": 83, "df_sort": 64, "df_summari": 78, "df_wide": 77, "dfg": 137, "dgp": [25, 26, 54, 57, 59, 60, 69, 70, 77, 80, 81, 82, 86, 87], "dgp1": [25, 26], "dgp2": [25, 26], "dgp3": [25, 26], "dgp4": [25, 26], "dgp5": [25, 26], "dgp6": [25, 26], "dgp_dict": 86, "dgp_tpye": 58, "dgp_type": [25, 26, 58, 60], "diagon": 86, "diagram": [51, 65, 96], "dichotom": [51, 65], "dict": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 45, 46, 47, 48, 60, 67, 68, 70, 76, 86, 95], "dict_kei": [126, 132], "dictionari": [9, 10, 11, 14, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 48, 60, 67, 68, 71, 72, 85, 94, 95, 96, 97, 126, 132], "dictonari": [55, 78], "did": [0, 4, 5, 6, 52, 58, 59, 60, 61, 77, 91, 92, 93, 97, 98, 100, 101, 110, 114, 141, 142], "did_aggreg": [60, 61], "did_multi": [96, 97], "diff": 78, "differ": [9, 10, 11, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 51, 52, 54, 55, 56, 57, 60, 61, 64, 65, 66, 69, 71, 72, 73, 74, 76, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 93, 94, 95, 97, 98, 100, 101, 109, 111, 113, 114, 138, 139, 140, 141, 142], "differenti": 96, "difficult": 86, "dillon": 140, "dim": [41, 55], "dim_x": [12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 25, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 52, 54, 56, 66, 74, 75, 76, 77, 89, 94, 95, 96, 126, 132], "dim_z": [15, 38, 96], "dimens": [11, 16, 25, 54, 77, 81, 109], "dimension": [11, 13, 21, 38, 39, 70, 84, 94, 96, 106, 107, 109, 125, 126, 132, 139, 140], "direct": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 57, 59, 66, 74, 87, 89, 96, 142], "directli": [40, 52, 53, 55, 64, 66, 75, 85, 89, 126, 132, 139, 142], "discontinu": [40, 41, 93, 140, 141], "discret": [14, 30, 64, 77, 96, 102, 141], "discretis": 79, "discuss": [12, 54, 55, 77, 78, 96, 97, 101, 140, 141, 142], "disjoint": [54, 71, 72, 77], "displai": [21, 54, 60, 61, 64, 77, 86, 94, 95, 126, 132], "displot": 78, "disproportion": [55, 78], "disregard": [43, 47], "dist": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "distinguish": [24, 60], "distr": 95, "distribut": [41, 52, 58, 64, 66, 75, 86, 89, 96, 98, 101, 126, 134, 138, 140, 141], "diverg": [40, 52, 66, 89], "divid": [60, 96], "dmatrix": [67, 68, 94], "dml": [20, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 56, 57, 58, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 100, 110, 125, 126, 132, 138], "dml1": [93, 139, 141, 142], "dml2": [51, 54, 56, 57, 58, 62, 77, 79, 93, 96, 110, 125, 139, 141, 142], "dml_apo": 74, "dml_apo_obj": 96, "dml_apos_att": 74, "dml_apos_obj": 96, "dml_base": 77, "dml_combin": 125, "dml_cover": 84, "dml_cv_predict": 141, "dml_cvar": [69, 79], "dml_cvar_0": 69, "dml_cvar_1": 69, "dml_cvar_obj": [31, 94], "dml_data": [6, 24, 53, 54, 57, 58, 59, 60, 61, 62, 64, 73, 74, 75, 77, 80, 84, 85, 86, 87, 91, 92, 94, 95, 96, 97, 100, 125, 142], "dml_data_anticip": 60, "dml_data_bench": 86, "dml_data_bonu": [56, 139], "dml_data_df": 142, "dml_data_fuzzi": 83, "dml_data_lasso": 62, "dml_data_sharp": 83, "dml_data_sim": [56, 139], "dml_df": [54, 77], "dml_did": [58, 59], "dml_did_obj": [20, 23, 24, 96, 97, 98, 100], "dml_iivm": 84, "dml_iivm_boost": [55, 78], "dml_iivm_forest": [55, 78], "dml_iivm_lasso": [55, 78], "dml_iivm_obj": [32, 65, 96], "dml_iivm_tre": [55, 78], "dml_irm": [67, 71, 74, 75, 81], "dml_irm_at": 73, "dml_irm_att": 74, "dml_irm_boost": [55, 78], "dml_irm_forest": [55, 78], "dml_irm_gat": 73, "dml_irm_gatet": 73, "dml_irm_lasso": [55, 62, 78], "dml_irm_new": 81, "dml_irm_obj": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 85, 94, 95, 96], "dml_irm_obj_ext": 95, "dml_irm_rf": 62, "dml_irm_tre": [55, 78], "dml_irm_weighted_att": 74, "dml_kwarg": 74, "dml_length": 84, "dml_long": 48, "dml_lpq_0": 82, "dml_lpq_1": 82, "dml_lpq_obj": [34, 94], "dml_lqte": [79, 82], "dml_obj": [53, 60, 61, 64, 85, 86], "dml_obj_al": 60, "dml_obj_anticip": 60, "dml_obj_bench": 86, "dml_obj_linear": 60, "dml_obj_nyt": 60, "dml_pliv": [54, 77], "dml_pliv_obj": [38, 54, 77, 96], "dml_plr": [68, 72, 125], "dml_plr_1": 125, "dml_plr_2": 125, "dml_plr_boost": [55, 78], "dml_plr_forest": [55, 78, 142], "dml_plr_lasso": [55, 62, 78], "dml_plr_no_split": 109, "dml_plr_obj": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 85, 88, 94, 95, 96, 109, 110, 125, 126, 127, 132], "dml_plr_obj_extern": 109, "dml_plr_obj_intern": 109, "dml_plr_obj_onfold": 76, "dml_plr_obj_untun": 76, "dml_plr_rf": 62, "dml_plr_tree": [55, 78, 142], "dml_pq_0": [79, 82], "dml_pq_1": [79, 82], "dml_pq_obj": [35, 94], "dml_procedur": [62, 88, 139, 141, 142], "dml_qte": [79, 82], "dml_qte_obj": [36, 94], "dml_robust_confset": 84, "dml_robust_length": 84, "dml_short": 48, "dml_ssm": [57, 87, 96], "dml_standard_ci": 84, "dml_tune": 141, "dmldummyclassifi": 95, "dmldummyregressor": 95, "dmlmt": 140, "dnorm": 52, "do": [53, 54, 55, 56, 74, 75, 77, 78, 79, 80, 86, 94, 95, 126, 136, 139, 142], "doabl": 110, "doc": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 137, 141], "doccument": 141, "docu": 141, "document": [59, 60, 61, 63, 67, 68, 71, 72, 74, 76, 86, 96, 101, 110, 126, 137, 141], "doe": [24, 30, 36, 53, 54, 55, 64, 74, 77, 78, 80, 85, 86, 110, 114, 126, 136, 142], "doesn": [51, 65], "doi": [7, 8, 9, 10, 12, 16, 17, 19, 25, 26, 53, 54, 56, 70, 77, 86, 89, 95, 109, 125, 137, 139, 141], "domain": 81, "don": [53, 76], "done": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 76, 79, 95, 109, 126, 127], "dosag": 64, "dot": [37, 59, 60, 81, 90, 92, 94, 95, 96, 100, 101, 102, 125, 139], "doubl": [4, 5, 6, 7, 8, 16, 17, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 55, 70, 75, 76, 78, 80, 84, 93, 95, 109, 110, 125, 126, 127, 141], "double_ml_bonus_data": 62, "double_ml_data_from_data_fram": [52, 89, 90, 92, 142], "double_ml_data_from_matrix": [53, 56, 90, 92, 95, 125, 139], "double_ml_framework": [96, 97], "double_ml_irm": [62, 81], "double_ml_score_mixin": 0, "doubleiivm": 137, "doubleml": [0, 52, 54, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 79, 80, 81, 82, 83, 84, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100, 109, 110, 126, 132, 139, 140, 141], "doubleml2022python": 137, "doubleml2024r": 137, "doubleml_did_eval_linear": 53, "doubleml_did_eval_rf": 53, "doubleml_did_linear": 53, "doubleml_did_rf": 53, "doubleml_framework": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "doublemlapo": [64, 74, 96, 110, 115, 141], "doublemlblp": [29, 33, 39, 67, 68, 74, 94, 141], "doublemlclusterdata": [0, 16], "doublemlcvar": [69, 94, 110, 116, 141], "doublemldata": [0, 6, 7, 8, 12, 13, 15, 17, 18, 19, 20, 22, 23, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 54, 56, 57, 58, 59, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 93, 94, 95, 96, 98, 109, 110, 125, 126, 132, 141, 142], "doublemldid": [58, 59, 96, 98, 110, 113, 141], "doublemldidaggreg": [60, 61, 96, 97], "doublemldidc": [58, 96, 98, 110, 111, 141], "doublemldidmulti": [60, 61, 96, 97, 100, 110, 114], "doublemlframework": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 61, 96, 97, 109, 125, 141], "doublemlframwork": 30, "doublemlidid": [96, 98], "doublemlididc": [96, 98], "doublemliivm": [51, 55, 65, 78, 84, 95, 96, 109, 110, 117, 141], "doublemlirm": [20, 22, 23, 29, 31, 32, 34, 35, 37, 38, 39, 53, 55, 62, 64, 67, 71, 73, 74, 75, 78, 80, 81, 85, 86, 94, 95, 96, 109, 110, 118, 137, 141], "doublemllpq": [82, 94, 110, 119, 141], "doublemlpaneldata": [0, 22, 24, 61, 91, 96, 97, 100], "doublemlpliv": [95, 96, 109, 110, 121, 137, 141], "doublemlplr": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 52, 55, 56, 62, 66, 68, 72, 76, 78, 80, 85, 88, 89, 94, 95, 96, 109, 110, 122, 125, 126, 132, 137, 139, 141, 142], "doublemlpolicytre": [33, 94], "doublemlpq": [79, 82, 94, 110, 120, 141], "doublemlqt": [69, 79, 82, 94, 125, 141], "doublemlresampl": [74, 75], "doublemlsmm": 141, "doublemlssm": [57, 87, 96, 110, 123, 124], "doubli": [9, 10, 26, 53, 84, 140], "down": 86, "download": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 90, 92, 138, 139], "downward": 86, "dpg_dict": 85, "dpi": [52, 66, 80], "dr": [96, 100], "dramat": 53, "draw": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 86, 109, 141], "draw_sample_split": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 74, 75, 109], "drawn": [11, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 55, 60, 78, 79, 81, 109], "drive": [52, 66, 89], "driven": [86, 142], "drop": [53, 76, 77, 80, 90, 92, 95, 110, 111, 113, 114, 125], "dropna": 60, "dt": [60, 110, 111, 126, 128], "dt_bonu": [90, 92], "dta": [53, 61], "dtrain": 78, "dtype": [58, 60, 61, 62, 64, 71, 72, 73, 75, 77, 78, 79, 84, 85, 87, 90, 91, 92, 94, 139], "dualiti": 77, "dubourg": [137, 139], "duchesnai": [137, 139], "due": [52, 53, 66, 67, 68, 73, 85, 86, 89, 96, 108, 126, 127, 141, 142], "duflo": [7, 8, 17, 54, 70, 77, 84, 89, 109, 137, 140], "dummi": [29, 33, 39, 42, 43, 44, 76, 86, 94, 95, 96, 98, 141], "dummyclassifi": 42, "dummyregressor": 43, "duplic": 141, "durabl": [56, 62, 90, 92, 139], "durat": 8, "dure": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 54, 55, 56, 57, 76, 77, 78, 95, 109, 139, 141, 142], "dx": 12, "dynam": [53, 140], "e": [5, 6, 7, 8, 9, 10, 14, 17, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 52, 53, 54, 55, 57, 58, 60, 61, 64, 66, 67, 68, 70, 73, 75, 76, 77, 78, 79, 80, 83, 84, 85, 86, 87, 89, 91, 92, 94, 95, 96, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 118, 119, 120, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 142], "e20ea26": 56, "e401": [55, 78, 79, 85, 142], "e4016553": 142, "e45228": 80, "e57c": 56, "each": [14, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 54, 56, 59, 60, 61, 64, 71, 72, 75, 76, 77, 79, 80, 81, 84, 85, 86, 88, 90, 91, 92, 95, 96, 101, 109, 125, 126, 132, 142], "earlier": [60, 142], "earn": [55, 78, 79], "earner": [55, 78, 85], "easi": [56, 84, 110], "easier": 76, "easili": [56, 75, 76, 79, 141], "ec973f": 80, "ecolor": [59, 64, 78, 80], "econ": 140, "econml": 140, "econom": [15, 16, 18, 19, 54, 70, 77, 80, 86, 109, 140], "econometr": [7, 8, 9, 10, 17, 18, 25, 26, 53, 54, 70, 77, 84, 89, 137, 140], "econometrica": [13, 54, 77, 80, 84, 89, 140], "ecosystem": [137, 142], "ectj": [7, 8, 17, 54, 70, 77, 89, 137], "ed": 140, "edge_color": 66, "edgecolor": 66, "edit": [138, 140], "edu": [137, 139], "educ": [55, 78, 79, 85, 142], "ee97bda7": 56, "effect": [9, 10, 11, 14, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 46, 47, 51, 52, 53, 54, 56, 57, 58, 59, 64, 65, 66, 70, 73, 77, 81, 83, 84, 87, 89, 93, 95, 97, 98, 100, 101, 103, 104, 105, 108, 109, 110, 118, 125, 126, 127, 139, 140, 141, 142], "effici": [96, 140], "effort": 110, "eight": [54, 77], "either": [11, 20, 22, 23, 24, 25, 29, 31, 32, 33, 34, 35, 37, 38, 39, 56, 59, 60, 70, 81, 83, 94, 95, 96, 101, 142], "eleanor": 140, "element": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 57, 58, 60, 61, 67, 68, 69, 75, 77, 79, 82, 85, 87, 96, 100, 110, 111, 113, 114, 115, 126, 131, 132, 135, 136, 141], "element_text": [54, 55], "elementari": 140, "elif": [71, 72, 81], "elig": [79, 85, 142], "eligibl": [55, 78, 85], "ell": [52, 54, 66, 70, 77, 89, 110, 121, 122, 139], "ell_0": [32, 38, 39, 52, 66, 70, 76, 89, 96, 104], "ell_2": 75, "els": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 44, 53, 54, 55, 59, 71, 72, 77, 81, 86], "em": 140, "emphas": [54, 77], "empir": [27, 28, 52, 54, 66, 77, 80, 86, 89, 96, 100, 109, 110, 125], "emploi": [54, 70, 77, 86, 110, 117], "employ": [55, 78, 79], "employe": 142, "empti": 77, "emul": [126, 127], "enabl": [21, 64, 81, 85, 94, 126, 127, 141], "enable_metadata_rout": [42, 43, 46, 47], "encapsul": [42, 43, 46, 47], "encod": 80, "end": [12, 15, 16, 52, 53, 54, 55, 57, 59, 60, 66, 69, 70, 75, 77, 78, 80, 81, 82, 87, 88, 90, 92, 95, 96, 100, 101, 109, 110, 114, 125, 139, 142], "endogen": [55, 78, 79, 142], "enet_coordinate_descent_gram": 77, "engin": [56, 140], "enrol": [55, 78, 79], "ensembl": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 62, 64, 66, 67, 68, 71, 72, 73, 75, 78, 81, 85, 86, 88, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 132, 139, 142], "ensemble_learner_pipelin": 95, "ensemble_pipe_classif": 56, "ensemble_pipe_regr": 56, "ensur": [46, 47, 54, 72, 74, 76, 77, 81, 84], "entir": [24, 52, 55, 66, 78, 89, 126, 127], "entri": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 52, 54, 58, 60, 61, 62, 64, 66, 73, 77, 78, 79, 85, 87, 89, 90, 91, 92, 95, 137, 139, 141], "enumer": [59, 64, 69, 71, 72, 75, 77, 78, 79, 82, 88, 95, 109], "env": [77, 138], "environ": 138, "ep": 80, "epanechnikov": 40, "epsilon": [55, 58, 59, 69, 78, 82, 94, 96, 98, 101], "epsilon_": [54, 59, 60, 77], "epsilon_0": 41, "epsilon_1": 41, "epsilon_i": [11, 69, 80, 81, 82], "epsilon_sampl": 81, "epsilon_tru": [69, 82], "eqnarrai": 55, "equal": [21, 25, 29, 33, 54, 57, 60, 77, 80, 84, 87, 94, 95, 96, 126, 134], "equat": [41, 54, 55, 77, 78, 86, 88, 125, 142], "equilibrium": [54, 77], "equiv": [96, 101, 110, 114], "equival": [70, 74, 109], "err": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 58, 60, 61, 62, 64, 65, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 80, 82, 85, 86, 87, 94, 95, 96, 97, 98, 100, 109, 110, 125, 139, 142], "error": [4, 5, 6, 20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 51, 52, 53, 55, 56, 57, 59, 66, 70, 71, 72, 75, 76, 78, 83, 86, 89, 95, 96, 106, 107, 109, 110, 125, 126, 132, 139, 141, 142], "errorbar": [59, 64, 71, 72, 76, 78, 80, 83], "erstellt": [54, 55, 56], "esim": 83, "especi": [75, 76], "essenti": 86, "est": 83, "est_method": 53, "esther": [109, 140], "estim": [4, 5, 7, 9, 10, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 52, 53, 54, 56, 59, 64, 66, 67, 68, 69, 71, 72, 74, 75, 77, 81, 83, 84, 88, 89, 93, 94, 95, 96, 97, 98, 100, 103, 105, 111, 113, 116, 119, 120, 124, 126, 127, 132, 137, 140, 141], "estimand": 84, "estimatior": [4, 5], "estimator_list": 76, "et": [7, 8, 11, 13, 16, 17, 52, 54, 55, 56, 58, 66, 67, 68, 69, 70, 71, 72, 75, 77, 78, 79, 82, 85, 89, 96, 98, 101, 109, 110, 116, 118, 119, 120, 125, 126, 127, 136, 137, 139, 141], "eta": [27, 28, 52, 54, 55, 59, 76, 77, 78, 82, 83, 88, 94, 96, 100, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 136, 139, 142], "eta1": 80, "eta2": 80, "eta_": [125, 126, 136], "eta_0": [40, 88, 96, 100, 110, 125], "eta_d": [83, 96], "eta_i": [11, 25, 59, 60, 81, 82, 83, 96], "eta_sampl": 81, "eta_tru": 82, "etc": [20, 21, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 75, 76, 77, 141], "ev": [52, 66, 89], "eval": [24, 56, 60, 61, 95, 96, 100, 110, 114, 126, 131], "eval_metr": [55, 78, 142], "eval_pr": 53, "eval_predict": 53, "evalu": [13, 20, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 56, 59, 60, 61, 67, 68, 69, 73, 74, 79, 82, 85, 88, 96, 100, 101, 140, 141], "evaluate_learn": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 75, 76, 95, 141], "evalut": 95, "even": [55, 56, 60, 78, 80, 83, 95, 96, 142], "event": [96, 97], "eventstudi": [24, 60, 61, 96, 97], "eventu": [54, 77], "everi": [54, 77], "everyth": 137, "evid": [73, 76], "exact": [74, 86], "exactli": [83, 86, 96], "exampl": [4, 5, 6, 11, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 87, 88, 89, 94, 95, 96, 97, 100, 108, 109, 110, 125, 126, 132, 137, 139, 141, 142], "example_attgt": 53, "example_attgt_dml_eval_linear": 53, "example_attgt_dml_eval_rf": 53, "example_attgt_dml_linear": 53, "example_attgt_dml_rf": 53, "except": [43, 47, 70, 86, 141], "excess": 75, "exclud": 48, "exclus": [29, 33, 39, 71, 72, 94], "execut": [56, 142], "exemplarili": 139, "exemplatori": 81, "exhaust": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 95], "exhibit": [54, 77], "exist": [42, 43, 46, 47, 74, 96, 98, 101, 126, 136], "exogen": [55, 78, 79, 96, 142], "exp": [9, 10, 11, 13, 14, 17, 25, 26, 52, 59, 66, 67, 68, 71, 72, 80, 81, 89], "expect": [9, 10, 43, 47, 53, 57, 58, 60, 64, 73, 75, 76, 83, 86, 87, 94, 96, 109, 125, 126, 133, 139], "experi": [8, 12, 13, 52, 55, 66, 78, 84, 86, 89, 90, 92, 109, 139, 140], "experiment": [20, 22, 23, 24, 25, 26, 110, 111, 113, 114, 126, 128, 130, 131], "expertis": 86, "explain": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 85, 126, 127, 135, 136], "explan": [54, 58, 77, 85, 126, 135, 137, 142], "explanatori": [86, 125], "explicit": 86, "explicitli": [60, 61, 73, 142], "exploit": [52, 66, 89, 96, 142], "explor": 76, "exponenti": 125, "export": [76, 141], "expos": [96, 101], "exposur": [6, 25, 59, 60, 61], "express": [54, 70, 83, 126, 136], "ext": 24, "extend": [86, 92, 95, 137, 141], "extendend": [126, 136], "extens": [95, 110, 137, 140, 141], "extent": 70, "extern": [52, 66, 74, 76, 93, 126, 127, 141], "external_predict": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 66, 95], "externalptr": 55, "extra": 56, "extract": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 76], "extralearn": 56, "extrem": [55, 78], "ey": 70, "f": [55, 56, 58, 59, 60, 64, 66, 69, 70, 75, 77, 78, 79, 81, 82, 85, 86, 87, 95, 126, 136, 137, 139], "f00584a57972": 56, "f1718fdeb9b0": 56, "f2e7": 56, "f3d24993": 56, "f6ebc": 80, "f_": [9, 25, 26, 59, 94], "f_loc": [69, 82], "f_p": 59, "f_scale": [69, 82], "f_t": 60, "f_x": 96, "face_color": 66, "facet_wrap": 55, "facilit": 76, "fact": [55, 78, 79], "factor": [41, 52, 53, 54, 55, 56, 66, 75, 89, 95, 142], "faculti": 140, "fail": 141, "fair": 75, "fake": [51, 65, 84], "fals": [4, 5, 6, 7, 8, 9, 11, 14, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 52, 55, 56, 57, 58, 60, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 86, 87, 90, 92, 95, 96, 109, 110, 111, 113, 114, 125, 126, 128, 130, 131, 142], "famili": [55, 78, 95], "familiar": 84, "fanci": 53, "far": [55, 78], "farbmach": 12, "fast": [75, 81, 95], "faster": 70, "fb5c25fa": 56, "fc9e": 56, "fd8a": 56, "featur": [7, 8, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 43, 45, 47, 53, 60, 62, 73, 74, 75, 78, 81, 94, 95, 96], "featureless": [56, 95], "features_bas": [55, 78, 79, 85], "features_flex": 55, "featureunion": 56, "februari": [60, 86], "femal": [56, 62, 90, 92, 139], "fern\u00e1ndez": [13, 109, 140], "fetch": [55, 77, 78, 79, 90, 92], "fetch_401k": [55, 78, 79, 85, 142], "fetch_bonu": [56, 62, 90, 92, 139], "few": [55, 78, 79], "ff7f0e": 59, "field": [54, 77, 95, 142], "fifteenth": 140, "fifth": [54, 60], "fig": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 59, 60, 61, 64, 67, 68, 69, 70, 74, 75, 76, 79, 80, 82, 83, 86], "fig_al": 66, "fig_dml": 66, "fig_non_orth": 66, "fig_orth_nosplit": 66, "fig_po_al": 66, "fig_po_dml": 66, "fig_po_nosplit": 66, "figsiz": [21, 24, 59, 60, 62, 64, 67, 68, 69, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83], "figur": [17, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 54, 59, 60, 62, 64, 66, 67, 68, 69, 70, 71, 72, 74, 76, 77, 78, 79, 82, 86, 89], "figure_format": 80, "file": [7, 8, 70, 80, 140, 141], "filenam": 52, "fill": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 54, 55, 57, 58, 75, 78, 87], "fill_between": [67, 68, 69, 74, 79, 82], "fill_valu": 75, "fillna": 60, "filter": 56, "filterwarn": 66, "final": [52, 56, 57, 59, 60, 61, 64, 66, 67, 68, 69, 71, 72, 73, 79, 82, 83, 87, 89, 96, 108, 110, 114, 142], "final_estim": 83, "financi": [7, 85, 142], "find": [55, 59, 78, 86, 94, 95, 142], "finish": 56, "finit": [52, 55], "firm": [54, 77, 85], "firmid": 77, "first": [6, 9, 10, 16, 20, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 71, 72, 73, 74, 75, 77, 78, 79, 81, 82, 83, 86, 87, 89, 94, 96, 97, 100, 101, 109, 125, 126, 132, 138, 139, 141, 142], "fit": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 93, 94, 95, 96, 97, 98, 100, 101, 110, 111, 113, 124, 125, 126, 127, 132, 137, 141, 142], "fit_arg": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39], "fit_transform": [74, 77, 78], "five": 77, "fix": [59, 60, 75, 141], "flag": [26, 109, 138], "flake8": 141, "flamlclassifierdoubleml": 76, "flamlregressordoubleml": 76, "flatten": [76, 80], "flexibl": [40, 51, 53, 55, 56, 58, 65, 78, 96, 137, 140, 141, 142], "flexibli": [55, 78, 85], "float": [9, 10, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 60, 61, 91, 92], "float32": [78, 79, 85], "float64": [58, 60, 61, 62, 64, 72, 73, 77, 78, 85, 87, 90, 91, 92, 95, 139], "floor": 56, "floor_divid": 77, "flt": 56, "flush": 52, "fmt": [59, 64, 71, 72, 76, 78, 80, 83], "fobj": 78, "focu": [54, 55, 74, 77, 78, 79, 86, 94, 96, 98, 101, 108, 142], "focus": [79, 85, 86, 142], "fold": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 54, 55, 56, 57, 58, 60, 61, 75, 77, 78, 79, 85, 87, 88, 93, 95, 96, 98, 100, 110, 111, 113, 125, 139, 142], "follow": [9, 10, 11, 14, 25, 26, 52, 54, 55, 57, 58, 59, 60, 61, 66, 67, 68, 69, 71, 72, 76, 77, 78, 79, 82, 83, 84, 85, 86, 87, 89, 90, 92, 94, 95, 96, 97, 100, 109, 110, 114, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 138, 139, 142], "font_scal": [77, 78, 79], "fontsiz": [60, 69, 79, 82], "force_all_d_finit": [5, 6], "force_all_x_finit": [4, 5, 6], "forest": [12, 51, 52, 53, 55, 56, 58, 65, 66, 73, 75, 78, 85, 89, 95, 139, 142], "forest_summari": 78, "forg": [138, 140, 141], "form": [9, 10, 11, 20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 46, 47, 55, 57, 58, 59, 67, 68, 69, 71, 72, 73, 75, 78, 82, 83, 85, 87, 94, 96, 97, 98, 101, 104, 105, 106, 107, 110, 114, 115, 118, 126, 127, 132, 133, 134, 135, 136, 138, 139], "format": [6, 24, 66, 73, 126, 132], "former": 84, "formula": [54, 55, 77, 78, 83, 86, 141], "formula_flex": 55, "forschungsgemeinschaft": 137, "forthcom": [86, 140], "forum": 141, "forward": [33, 45], "found": [61, 67, 68, 70, 71, 72, 76, 89, 90, 92, 95, 96, 108, 139], "foundat": [137, 140], "four": [55, 75, 78, 141], "fourth": [54, 77], "frac": [9, 10, 12, 13, 15, 17, 18, 19, 25, 26, 28, 32, 43, 47, 52, 54, 56, 59, 66, 70, 73, 77, 80, 83, 88, 89, 94, 96, 100, 104, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 130, 131, 133, 134, 135, 136], "fraction": 56, "frame": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 51, 52, 54, 55, 57, 58, 60, 61, 62, 64, 67, 68, 71, 72, 73, 77, 78, 79, 80, 81, 85, 87, 89, 90, 91, 92, 139, 142], "framealpha": 60, "frameon": 60, "framework": [21, 24, 28, 52, 54, 56, 66, 75, 76, 77, 80, 86, 89, 95, 125, 137, 139, 141, 142], "freez": 138, "fribourg": 140, "friendli": [60, 64], "from": [4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 47, 51, 52, 54, 55, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 132, 139, 141, 142], "from_arrai": [4, 5, 6, 37, 40, 58, 59, 66, 69, 82, 89, 90, 92, 95, 125, 139], "from_product": 77, "front": 64, "fr\u00e9chet": [126, 136], "fs_kernel": [40, 96], "fs_specif": [40, 96], "fsize": [55, 78, 79, 85, 142], "full": [58, 59, 64, 66, 69, 71, 72, 75, 78, 79, 82, 83, 84, 87, 89, 96], "fulli": [33, 55, 63, 76, 78, 84, 96, 105], "fun": 52, "func": 53, "function": [0, 4, 5, 6, 17, 18, 20, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 55, 56, 57, 58, 60, 61, 65, 66, 67, 68, 69, 74, 75, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 93, 95, 96, 97, 98, 100, 101, 104, 109, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 130, 131, 136, 137, 140, 141, 142], "fund": [55, 78, 79, 137], "further": [9, 10, 11, 14, 16, 21, 24, 25, 26, 54, 56, 57, 58, 59, 60, 61, 64, 67, 68, 69, 73, 74, 75, 77, 79, 81, 82, 83, 85, 86, 87, 95, 96, 98, 100, 108, 110, 116, 119, 120, 123, 124, 125, 126, 127, 132, 135, 136, 137, 139, 141, 142], "furthermor": [66, 91, 92, 110, 115, 118], "futur": [60, 61, 96, 110, 126], "futurewarn": [60, 61, 72], "fuzzi": [40, 41], "g": [5, 6, 9, 10, 20, 22, 23, 24, 25, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 52, 53, 56, 58, 59, 60, 61, 62, 66, 67, 68, 70, 73, 75, 79, 80, 81, 84, 85, 87, 89, 91, 92, 94, 95, 96, 97, 98, 100, 101, 110, 111, 113, 114, 115, 117, 118, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 142], "g_": [41, 64, 96, 100, 110, 111, 113, 114, 116, 119, 120, 125], "g_0": [17, 18, 20, 22, 23, 24, 29, 32, 33, 35, 38, 39, 40, 41, 52, 54, 55, 66, 75, 77, 78, 89, 94, 95, 96, 102, 103, 104, 105, 106, 107, 110, 114, 115, 123, 124, 126, 133, 134, 136, 139, 142], "g_1": [41, 75], "g_all": [52, 55], "g_all_po": 52, "g_ci": 55, "g_d": [110, 116, 120], "g_dml": 52, "g_dml_po": 52, "g_hat": [38, 39, 52, 66, 110], "g_hat0": [32, 33], "g_hat1": [32, 33], "g_i": [25, 96, 100, 101, 110, 114], "g_k": 94, "g_nonorth": 52, "g_nosplit": 52, "g_nosplit_po": 52, "g_valu": 22, "g_x": 59, "gain": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 75, 126, 127, 134, 141], "gain_statist": 141, "galleri": [89, 94, 95, 96, 97, 100, 108, 137, 141], "gama": 76, "gamma": [15, 18, 19, 54, 77, 80, 81, 83, 86, 96, 110, 116, 119], "gamma_0": [11, 57, 81, 87, 110, 116, 119], "gamma_a": [9, 10, 86], "gamma_bench": 86, "gamma_v": 86, "gap": [77, 86], "gapo": 29, "gate": [29, 33, 39, 44, 80, 81, 93, 141], "gate_obj": 94, "gatet": 94, "gaussian": [34, 35, 36, 52, 66, 89, 94, 95, 125, 140], "ge": [9, 11, 26, 73, 81, 94, 96, 100, 101], "geer": 140, "gelbach": [54, 77], "gener": [0, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 43, 47, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 90, 92, 93, 94, 95, 96, 100, 101, 102, 109, 110, 114, 115, 118, 125, 127, 128, 130, 131, 133, 134, 136, 140, 141, 142], "generate_treat": 82, "generate_weakiv_data": 84, "geom_bar": 55, "geom_dens": [55, 57], "geom_errorbar": 55, "geom_funct": 52, "geom_histogram": 52, "geom_hlin": 55, "geom_point": 55, "geom_til": 54, "geom_vlin": [52, 57], "geq": [25, 83, 96], "german": 137, "get": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 56, 60, 61, 64, 75, 80, 85, 86, 126, 127, 137, 138], "get_dummi": 80, "get_feature_names_out": [74, 77, 78], "get_legend_handles_label": 64, "get_level_valu": 76, "get_logg": [52, 53, 54, 55, 56, 57, 88, 95, 96, 109, 110, 125, 139], "get_metadata_rout": [42, 43, 46, 47], "get_param": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 76, 95], "get_ylim": 74, "ggdid": 53, "ggplot": [52, 54, 55, 57], "ggplot2": [52, 54, 55, 57], "ggsave": 52, "ggtitl": 55, "gh": 141, "git": 138, "github": [53, 55, 70, 76, 80, 137, 140, 141], "githubusercont": [61, 70], "give": [55, 74, 78], "given": [9, 10, 13, 17, 18, 20, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 46, 47, 52, 54, 57, 59, 61, 64, 66, 71, 72, 77, 79, 80, 83, 86, 87, 89, 94, 96, 100, 110, 115, 125, 126, 132, 133, 134, 135, 136, 139, 141], "glmnet": [55, 56, 95, 141], "global": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 46, 47, 95, 96], "globalclassifi": 83, "globallearn": 83, "globalregressor": 83, "glrn": 56, "glrn_lasso": 56, "gmm": 84, "gname": 53, "go": [67, 68, 70, 74, 76, 83, 86], "goal": [64, 71, 72, 96], "goe": 96, "goldman": 140, "good": [70, 126, 127, 142], "gradient": [55, 78], "gradientboostingclassifi": 75, "gradientboostingregressor": 75, "gradual": 86, "gramfort": [137, 139], "graph": [56, 57, 87, 142], "graph_ensemble_classif": 56, "graph_ensemble_regr": 56, "graph_obj": 83, "graph_object": [67, 68, 70, 86], "graphlearn": [56, 95], "grasp": [64, 126, 127], "great": [59, 142], "greater": 142, "green": [52, 67, 68, 69, 82], "greg": 140, "grei": [55, 64], "grenand": 140, "grey50": 54, "grid": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 54, 56, 60, 64, 67, 68, 69, 70, 74, 79, 80, 82, 86, 95, 126, 132], "grid_arrai": [67, 68], "grid_basi": 74, "grid_bound": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 86], "grid_search": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 56, 95], "grid_siz": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 67, 68], "gridextra": 54, "gridsearchcv": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 95], "grisel": [137, 139], "grob": 54, "group": [6, 22, 24, 25, 29, 33, 39, 51, 53, 64, 65, 73, 74, 79, 80, 81, 86, 93, 96, 97, 100, 101, 110, 114, 126, 131], "group_0": 94, "group_1": [71, 72, 94], "group_2": [71, 72, 94], "group_3": [71, 72], "group_effect": 81, "group_ind": 73, "group_treat": 73, "groupbi": [60, 70, 78, 84], "gruber": 12, "gt": [51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 64, 65, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 90, 92, 139], "gt_combin": [24, 60, 96, 97, 100], "gt_dict": 60, "guarante": [54, 77], "guber": 12, "guess": [85, 126, 127], "guid": [27, 28, 42, 43, 46, 47, 52, 53, 54, 56, 59, 60, 61, 64, 66, 73, 74, 77, 83, 85, 95, 137, 139, 141], "guidelin": 141, "gunion": [56, 95], "gxidclusterperiodytreat": 53, "h": [9, 10, 12, 16, 25, 26, 53, 54, 77, 83, 84, 91, 92, 96, 140], "h20": 76, "h_0": [60, 64, 73, 74, 85, 86, 126, 132, 142], "h_f": [40, 96], "ha": [20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 45, 46, 47, 52, 53, 54, 55, 66, 70, 75, 76, 77, 78, 79, 80, 83, 84, 85, 86, 94, 95, 96, 98, 101, 126, 127, 132, 133, 134, 135, 136, 142], "half": [52, 66, 80, 89, 109], "hand": [40, 75, 76, 80, 84, 142], "handbook": 80, "handl": [53, 64, 75, 91, 92, 95, 141], "hansen": [7, 8, 13, 15, 17, 54, 70, 77, 84, 89, 137, 140], "happend": 75, "hard": [85, 126, 127], "harold": 140, "harsh": [42, 46], "hasn": [21, 24, 60, 61], "hat": [52, 54, 66, 70, 73, 77, 80, 83, 88, 89, 94, 96, 109, 110, 125, 126, 127, 132, 135], "have": [11, 14, 24, 29, 30, 33, 36, 39, 44, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 67, 68, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 90, 92, 94, 95, 96, 110, 114, 125, 126, 127, 133, 136, 138, 139, 141, 142], "hazlett": [86, 126, 127], "hc": [53, 140], "hc0": [44, 141], "hdm": [54, 77], "he": [57, 87], "head": [53, 54, 56, 60, 61, 62, 67, 68, 71, 72, 74, 76, 77, 78, 80, 83, 86, 90, 92, 94, 139], "heat": [54, 77], "heatmap": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 54, 77, 86], "heavili": 75, "hei": 140, "height": [21, 24, 52, 54, 70, 76], "help": [53, 55, 69, 75, 79, 81, 86, 109, 142], "helper": 141, "henc": [53, 55, 56, 78, 86, 95, 110, 142], "here": [34, 35, 36, 53, 54, 55, 56, 57, 58, 59, 60, 61, 67, 68, 69, 71, 72, 73, 75, 77, 78, 79, 81, 82, 83, 86, 87, 90, 92, 95, 96, 138], "heterogen": [11, 25, 33, 55, 73, 78, 79, 81, 93, 96, 105, 109, 140, 141, 142], "heteroskedast": [71, 72], "heurist": [52, 66, 89], "high": [13, 38, 39, 55, 59, 70, 78, 79, 84, 88, 96, 106, 107, 125, 137, 139, 140], "higher": [53, 55, 70, 78, 79, 80, 83, 84, 141, 142], "highli": [55, 78, 137], "highlight": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 58, 74, 76, 86, 141], "highlightcolor": [67, 68], "hint": 76, "hispan": 62, "hist": 64, "hist_e401": 55, "hist_p401": 55, "histogram": 64, "histplot": 66, "hjust": 55, "hline": [90, 92, 125, 139, 142], "hold": [19, 54, 55, 57, 76, 77, 78, 87, 94, 95, 96, 100], "holdout": [95, 109], "holm": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39], "home": [55, 60, 61, 78, 86], "homogen": 96, "hopefulli": 79, "horizont": [54, 59, 77], "hostedtoolcach": [60, 61, 78], "hot": 80, "hotstart_backward": [56, 95], "hotstart_forward": [56, 95], "household": [55, 78, 79, 85], "how": [21, 25, 42, 43, 46, 47, 51, 53, 54, 55, 57, 58, 59, 60, 61, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 89, 95, 96, 137, 138], "howev": [52, 55, 57, 66, 76, 78, 83, 86, 87, 89, 96, 142], "hown": [55, 78, 79, 85, 142], "hpwt": [54, 77], "hpwt0": 54, "hpwtairmpdspac": 54, "href": 137, "hspace": 75, "hstack": [37, 59], "html": [56, 72, 137, 139, 141], "http": [12, 18, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 95, 137, 138, 139, 141], "huber": [19, 57, 87, 96, 108, 110, 123, 124, 140], "hue": [60, 78], "huge": 75, "hugo": 140, "husd": [56, 62, 90, 92, 139], "hyperparamet": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 55, 56, 62, 70, 75, 76, 78, 93, 139], "hypothes": [125, 140], "hypothesi": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 78, 85, 126, 132, 140], "hypothet": 86, "i": [4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 118, 119, 120, 124, 125, 126, 127, 132, 133, 134, 136, 137, 138, 139, 141, 142], "i0": [58, 59, 96, 98], "i03": 137, "i1": [58, 96, 98], "i_": [15, 77, 81], "i_1": [54, 77], "i_2": [54, 77], "i_3": [54, 77], "i_4": 59, "i_est": 66, "i_fold": 54, "i_k": [54, 77, 88, 109, 125], "i_learn": 75, "i_level": 64, "i_rep": [52, 57, 58, 66, 75, 87, 89], "i_split": 77, "i_train": 66, "icp": 140, "id": [6, 24, 53, 54, 56, 60, 61, 77, 91, 92, 96, 97, 100], "id_col": [6, 24, 60, 61, 91, 92, 96, 97, 100], "id_var": 77, "idea": [55, 56, 78, 79, 86, 95, 96, 126, 127, 142], "ident": [9, 10, 11, 14, 15, 25, 26, 45, 56, 64, 74, 76, 83, 95, 96, 101, 110, 118, 126, 132], "identfi": 86, "identif": [60, 61, 83, 84, 96, 142], "identifi": [6, 54, 55, 58, 73, 77, 78, 79, 83, 86, 91, 92, 94, 96, 98, 100, 101, 108, 126, 136, 141], "identifii": 94, "idnam": 53, "idx_gt_att": 24, "idx_tau": [69, 79, 82], "idx_treat": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 64, 126, 132], "ieee": 140, "ifels": 53, "ignor": [42, 43, 46, 47, 66, 83], "ii": [54, 77], "iid": [60, 96, 98], "iivm": [12, 27, 28, 32, 79, 88, 94, 104, 117, 137, 141], "iivm_summari": 78, "iivmglmnet": 55, "iivmrang": 55, "iivmrpart": 55, "iivmxgboost11861": 55, "ij": [16, 54, 57, 64, 77, 87], "ilia": 140, "illustr": [52, 54, 55, 56, 57, 58, 59, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 81, 82, 85, 86, 87, 89, 95, 142], "iloc": [58, 59, 60, 61, 64, 75, 77, 80, 84], "immedi": 138, "immun": [109, 140], "impact": [51, 65, 75, 80, 85], "implement": [20, 22, 23, 24, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 46, 47, 52, 53, 54, 55, 56, 57, 58, 60, 61, 66, 70, 74, 75, 77, 78, 80, 83, 85, 86, 87, 89, 93, 94, 95, 97, 98, 99, 100, 101, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 139, 140, 141, 142], "impli": [9, 10, 54, 55, 77, 78, 79, 83, 94, 96, 101, 126, 128, 130, 131, 133, 134], "implicitli": [96, 100], "implment": [59, 96, 97], "import": [4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 132, 138, 139, 141, 142], "importlib": 70, "impos": 86, "improv": [58, 60, 75, 81, 96, 141], "in_sample_norm": [20, 22, 23, 24, 58, 110, 111, 113, 114, 126, 128, 130, 131], "inbuild": 75, "inbuilt": 75, "inc": [55, 78, 79, 85, 142], "includ": [20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 53, 55, 59, 60, 61, 64, 71, 72, 74, 78, 83, 85, 86, 94, 96, 125, 126, 132, 133, 134, 136, 138, 141, 142], "include_bia": [74, 77, 78], "include_never_tr": 25, "include_scenario": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 86], "incom": [55, 78, 79, 81, 85, 142], "incorpor": [56, 85, 126, 132], "increas": [60, 73, 75, 77, 86, 142], "increment": 141, "ind": 78, "independ": [9, 10, 11, 20, 22, 23, 24, 26, 41, 54, 56, 59, 73, 77, 81, 96, 101, 108, 110, 111, 113, 114, 141], "index": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 54, 59, 62, 66, 70, 71, 72, 76, 77, 78, 80, 81, 89, 90, 92, 109, 110, 111, 113, 114, 139], "index_col": 70, "india": [109, 140], "indic": [4, 5, 6, 11, 14, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 54, 55, 57, 59, 60, 61, 73, 77, 78, 79, 83, 84, 86, 87, 88, 90, 92, 94, 96, 98, 101, 102, 108, 109], "individu": [25, 29, 33, 46, 47, 53, 55, 59, 60, 64, 71, 72, 73, 76, 78, 79, 83, 85, 94, 96, 142], "individual_df": 59, "induc": [93, 109], "industri": [54, 77], "inf": [4, 5, 6, 53, 60, 61, 84], "inf_model": 110, "infer": [13, 15, 51, 52, 54, 65, 66, 70, 75, 76, 77, 84, 89, 93, 96, 109, 137, 139, 140, 141], "inferenti": 142, "infinit": [4, 5, 6, 84, 141], "influenc": [32, 43, 47, 96], "info": [51, 56, 58, 60, 61, 62, 64, 73, 76, 77, 78, 79, 85, 87, 90, 91, 92, 139, 141, 142], "inform": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 46, 47, 51, 56, 60, 61, 65, 67, 68, 75, 83, 84, 85, 86, 96, 97, 126, 127, 140], "infti": [52, 66, 89, 96, 101], "inher": 86, "inherit": [80, 91, 92, 141], "initi": [4, 5, 6, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 55, 56, 57, 58, 60, 61, 69, 78, 79, 82, 83, 85, 86, 87, 90, 91, 92, 94, 95, 96, 109, 139, 141, 142], "inlin": [62, 80], "inlinebackend": 80, "inner": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 95], "innermost": 95, "input": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 56, 85, 88, 96, 100, 125, 126, 127, 132], "insensit": 96, "insid": [42, 43, 46, 47], "insight": [70, 86], "insignific": 85, "inspect": 139, "inspir": [9, 12, 13, 19, 60, 86], "instal": [55, 76, 83, 96, 141], "install_github": 138, "instanc": [46, 47, 55, 56, 78, 95], "instanti": [54, 55, 77, 78, 95, 109], "instead": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 51, 53, 55, 60, 61, 64, 65, 72, 73, 76, 78, 79, 94, 95, 96, 110, 114, 126, 128, 130, 131, 134, 135, 141], "instruct": [138, 141], "instrument": [4, 5, 6, 7, 12, 15, 32, 38, 54, 55, 56, 57, 58, 60, 61, 62, 64, 73, 77, 78, 79, 82, 85, 87, 90, 91, 92, 95, 96, 98, 100, 104, 106, 110, 119, 125, 139, 142], "instrument_effect": 51, "instrument_impact": 65, "instrument_strength": 84, "insuffienct": 76, "int": [9, 10, 11, 14, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 53, 54, 55, 58, 65, 69, 81, 82, 84, 86, 87, 91, 92], "int64": [60, 61, 62, 75, 77, 90, 91, 92, 139], "int8": [78, 79, 85], "integ": [26, 56, 95], "integr": [76, 86, 126, 136, 141], "intend": [40, 56, 86, 142], "intent": [96, 142], "inter": 95, "interact": [9, 12, 13, 14, 29, 30, 32, 33, 40, 41, 64, 86, 93, 95, 104, 105, 133, 134, 137, 141, 142], "interchang": 125, "interest": [9, 10, 32, 33, 38, 39, 52, 55, 57, 58, 66, 70, 78, 79, 83, 84, 87, 89, 94, 96, 98, 100, 102, 103, 104, 105, 106, 107, 108, 110, 125, 139, 142], "interfac": [53, 55, 56, 90, 92, 95, 109, 139], "intermedi": [72, 86], "intern": [53, 55, 56, 64, 76, 79, 95, 140], "internet": [55, 78, 79], "interpret": [60, 61, 71, 72, 84, 86, 94, 96, 100, 126, 127, 133, 134, 135, 136, 138, 142], "intersect": [86, 126, 132, 141], "interv": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 53, 54, 55, 57, 58, 60, 61, 64, 67, 68, 69, 71, 72, 74, 77, 79, 82, 83, 85, 87, 93, 94, 109, 110, 126, 132, 139, 140, 142], "intial": 83, "introduc": [52, 66, 89, 90, 92, 125, 141, 142], "introduct": [52, 54, 56, 66, 77, 79, 85, 95, 96, 98, 101, 126, 127], "introductori": [53, 86], "intrument": [57, 87], "intspecifi": 40, "intuit": 86, "inuidur1": [56, 62, 90, 92, 139], "inuidur1femaleblackothracedep1dep2q2q3q4q5q6agelt35agegt54durablelusdhusdtg": [56, 90, 92, 139], "inuidur2": [62, 90, 92, 139], "inv_sigmoid": 80, "invalid": [52, 66, 89], "invari": [96, 98], "invers": [29, 31, 32, 33, 34, 35, 36, 37, 57, 87, 126, 133, 134], "invert": 32, "invert_yaxi": 77, "investig": [70, 76, 86], "involv": [94, 95, 110, 142], "io": [80, 141], "ipw_norm": 141, "ipykernel_46295": 72, "ipynb": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "ira": [55, 78, 79], "irm": [0, 13, 14, 20, 22, 23, 27, 28, 38, 39, 44, 45, 75, 86, 88, 93, 95, 100, 105, 118, 133, 134, 137, 141, 142], "irm_summari": 78, "irmglmnet": 55, "irmrang": 55, "irmrpart": 55, "irmxgboost8047": 55, "irrespect": 86, "irrevers": [96, 101], "is_classifi": [20, 22, 23, 29, 32, 33, 39], "is_gat": [29, 33, 39, 44], "isfinit": [60, 61], "isnan": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 75, 95], "isoton": 86, "isotonicregress": 86, "issn": 70, "issu": [21, 24, 86, 137, 140, 141], "ite": [60, 64, 71, 72, 73], "ite_lower_quantil": 60, "ite_mean": 60, "ite_upper_quantil": 60, "item": [32, 78, 88, 95, 109], "iter": [40, 51, 57, 58, 77, 78, 83, 87, 95, 125, 142], "itertool": 70, "its": [42, 43, 86, 88, 94, 95, 96, 98, 100, 109, 110, 125], "iv": [12, 15, 16, 32, 38, 39, 52, 54, 66, 77, 89, 90, 92, 104, 106, 121, 122, 126, 135, 137, 141, 142], "iv_2": 51, "iv_var": [54, 77], "iv\u00e1n": [109, 140], "j": [7, 8, 9, 10, 12, 13, 15, 16, 17, 18, 19, 25, 26, 52, 53, 54, 56, 57, 64, 66, 70, 77, 80, 84, 87, 89, 95, 96, 102, 125, 137, 139], "j_": [54, 77], "j_0": 125, "j_1": [54, 77], "j_2": [54, 77], "j_3": [54, 77], "j_k": [54, 77], "jame": 140, "janari": 60, "janni": [55, 78], "januari": 60, "jasenakova": 141, "javanmard": 140, "jbe": [54, 77], "jeconom": [9, 10, 25, 26, 53], "jerzi": 140, "jia": 86, "jitter": 24, "jitter_valu": 24, "jk": [96, 103], "jmlr": [56, 137, 139, 141], "job": [55, 78, 79], "john": 140, "joint": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 60, 61, 64, 67, 68, 69, 71, 72, 74, 79, 82, 84, 96, 98, 125, 141, 142], "jointli": [82, 94], "jonathan": 140, "joss": [56, 95, 137, 139], "journal": [7, 8, 9, 10, 16, 17, 19, 25, 26, 53, 54, 56, 70, 77, 80, 84, 86, 89, 95, 137, 139, 140, 141], "jss": 137, "jump": [81, 83, 96], "jun": [53, 140], "jupyt": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "juraj": 140, "just": [53, 56, 58, 59, 60, 64, 69, 71, 72, 73, 74, 81, 82, 96, 110, 111, 113, 114, 126, 127], "justif": [109, 126, 127], "k": [7, 10, 12, 13, 15, 16, 17, 19, 25, 26, 52, 54, 56, 66, 75, 76, 77, 83, 84, 88, 89, 93, 94, 96, 125, 142], "k_h": [83, 96], "kaggl": [55, 78], "kallu": [69, 79, 82, 85, 110, 116, 119, 120, 140], "kappa": 96, "kato": [16, 54, 77, 125, 140], "kb": [58, 61, 64, 73, 77, 78, 79, 85, 90, 91, 92, 139], "kde": [34, 35, 36, 78], "kdeplot": [58, 75, 87], "kdeunivari": [34, 35, 36], "kecsk\u00e9sov\u00e1": 141, "keel": 84, "keep": [43, 47, 53, 72, 74, 86, 142], "kei": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 45, 54, 55, 67, 68, 71, 72, 76, 77, 78, 79, 83, 86, 95, 96, 110, 126, 132, 141], "keith": 140, "kelz": 84, "kengo": 140, "kennedi": 84, "kernel": [34, 35, 36, 40, 43, 47, 83, 96], "kernel_regress": 83, "kernelreg": 83, "keyword": [16, 17, 18, 25, 26, 29, 33, 39, 41, 44], "kf": 109, "kfold": [77, 109], "kind": [51, 65, 78], "kj": [10, 12, 13, 15, 16, 17, 19, 25, 26, 52, 54, 66, 77, 89], "klaassen": [12, 70, 75, 76, 86, 137, 140], "klaa\u00dfen": 12, "knau": 140, "know": [58, 81, 84], "knowledg": [51, 65, 75, 80, 81], "known": [73, 75, 83, 84, 86, 95, 96, 101], "kohei": 140, "kotthof": 56, "kotthoff": [56, 95, 137, 139], "krueger": 80, "kueck": [55, 78], "kurz": [137, 140, 141], "kwarg": [9, 10, 14, 16, 17, 18, 25, 26, 29, 33, 39, 40, 41, 42, 44, 76, 96], "l": [54, 56, 57, 62, 67, 68, 77, 84, 86, 87, 95, 126, 135, 137, 139], "l1": [78, 87, 96], "l_hat": [38, 39, 52, 66, 110], "lab": 57, "label": [21, 24, 42, 46, 59, 64, 66, 67, 68, 69, 71, 72, 74, 76, 79, 80, 82, 83], "labor": 80, "laffer": 140, "laff\u00e9r": [19, 57, 87, 96, 108, 110, 123, 124], "lal": [80, 141], "lambda": [54, 55, 56, 57, 60, 78, 80, 81, 95, 96, 110, 111, 125, 139], "lambda_": 70, "lambda_0": [110, 111], "lambda_t": 26, "land": 81, "lang": [56, 95, 137, 139], "langl": [11, 81], "lappli": 109, "larg": [52, 66, 73, 75, 76, 80, 86, 96], "larger": [33, 53, 83, 86, 126, 132], "largest": 75, "largli": 75, "lasso": [54, 55, 56, 57, 78, 87, 95, 139, 140], "lasso_class": [55, 78], "lasso_pip": [56, 95], "lasso_summari": 78, "lassocv": [37, 70, 77, 78, 87, 95, 96, 125, 139], "last": [26, 56, 138], "late": [32, 51, 55, 78, 84, 96, 104, 110, 117], "latent": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 85, 126, 135, 136], "later": [55, 56, 83, 86, 95, 142], "latter": [46, 47, 84, 96], "layout": 70, "lbrace": [12, 13, 19, 32, 33, 54, 77, 88, 96, 102, 104, 105, 109, 110, 115, 125, 126, 133], "ldot": [38, 39, 54, 57, 77, 87, 88, 96, 106, 107, 109, 125, 139], "le": [26, 58, 81, 94, 96, 98, 110, 119, 120], "lead": [53, 86, 96], "leadsto": 125, "lear": [56, 95, 137, 139], "learn": [4, 5, 6, 7, 8, 16, 17, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 55, 56, 62, 64, 65, 69, 70, 74, 75, 76, 78, 79, 80, 82, 83, 84, 86, 90, 92, 93, 95, 109, 110, 125, 126, 127, 141, 142], "learner": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55, 57, 58, 60, 61, 66, 67, 68, 70, 77, 78, 79, 85, 86, 87, 88, 89, 93, 96, 98, 100, 109, 110, 125, 126, 132, 141, 142], "learner_class": [37, 141], "learner_cv": 56, "learner_forest_classif": 56, "learner_forest_regr": 56, "learner_l": 85, "learner_lasso": 56, "learner_list": 75, "learner_m": 85, "learner_nam": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 95], "learner_param_v": 56, "learner_rf": 125, "learnerclassif": 56, "learnerregr": 56, "learnerregrcvglmnet": 56, "learnerregrrang": [56, 95], "learning_r": [60, 66, 69, 79, 82, 83, 86, 89], "least": [51, 55, 65, 78, 79, 85, 96, 100, 109], "leav": [57, 86, 87], "left": [12, 13, 15, 16, 19, 25, 52, 54, 64, 66, 75, 77, 78, 79, 80, 82, 83, 89, 96, 110, 111, 113, 114, 125, 126, 128, 130, 131, 133, 134], "legend": [55, 59, 60, 64, 66, 67, 68, 69, 71, 72, 74, 75, 79, 80, 82], "len": [64, 69, 75, 76, 77, 79, 82, 84], "length": [20, 21, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 56, 58, 60, 84, 95], "leq": [54, 77], "less": [53, 55, 78, 79, 83, 86], "lester": 140, "let": [9, 10, 14, 25, 26, 52, 53, 55, 56, 57, 58, 60, 64, 66, 69, 71, 72, 74, 75, 78, 79, 82, 86, 87, 88, 89, 95, 96, 98, 101, 108, 126, 127, 136, 142], "level": [14, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 54, 55, 57, 58, 59, 60, 61, 64, 67, 68, 69, 71, 72, 73, 74, 77, 78, 79, 82, 84, 85, 86, 87, 95, 102, 103, 110, 115, 126, 132, 133, 142], "level_0": [56, 77], "level_1": 77, "level_bound": 64, "levi": 84, "levinsohn": [54, 77], "lewi": 140, "lgbmclassifi": [58, 59, 60, 69, 75, 79, 82, 83, 86], "lgbmregressor": [58, 59, 60, 66, 69, 75, 79, 83, 86, 89], "lgr": [52, 53, 54, 55, 56, 57, 88, 95, 96, 109, 110, 125, 139], "lib": [60, 61, 77, 78], "liblinear": [78, 87, 96], "librari": [51, 52, 53, 54, 55, 56, 57, 88, 89, 90, 92, 95, 96, 109, 110, 125, 138, 139, 142], "licens": 141, "lie": 140, "lightgbm": [58, 59, 60, 66, 69, 75, 79, 82, 83, 86], "like": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 53, 55, 56, 60, 61, 70, 72, 78, 79, 86, 95, 96, 97, 109, 139, 142], "lim": 80, "lim_": [83, 96], "limegreen": [67, 68], "limit": [80, 96, 101, 140], "limits_": 94, "lin": [83, 86, 96], "line": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 59, 64, 86], "linear": [9, 10, 14, 15, 16, 17, 18, 25, 27, 28, 29, 33, 38, 39, 44, 51, 52, 53, 54, 56, 58, 59, 64, 65, 66, 67, 68, 70, 71, 74, 75, 76, 77, 84, 85, 86, 88, 89, 93, 94, 95, 100, 106, 107, 109, 111, 113, 114, 115, 117, 118, 121, 122, 125, 132, 134, 135, 136, 137, 139, 140, 141, 142], "linear_learn": 60, "linear_model": [29, 33, 37, 39, 44, 60, 61, 62, 64, 65, 70, 74, 75, 77, 78, 83, 84, 86, 87, 95, 96, 125, 139], "linearli": [83, 96], "linearregress": [51, 60, 61, 64, 65, 74, 75, 83, 84, 86], "linearscoremixin": [0, 110], "lineplot": [60, 64], "linestyl": [59, 60, 64, 76, 83], "linetyp": 57, "linewidth": [59, 60], "link": [86, 141], "linspac": [67, 68, 74, 86], "lint": 141, "linux": 138, "list": [4, 5, 6, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 47, 52, 53, 54, 55, 56, 60, 66, 67, 68, 77, 79, 81, 89, 95, 109, 110, 138, 141], "list_confset": 32, "listedcolormap": 77, "literatur": [86, 96, 98, 101], "littl": [73, 84], "ll": [56, 125, 142], "lllllllllllllllll": [90, 92, 139], "lm": [51, 53, 86], "ln_alpha_ml_l": 70, "ln_alpha_ml_m": 70, "load": [51, 53, 55, 56, 70, 78, 79, 90, 92, 138, 139], "loader": 0, "loc": [59, 60, 61, 64, 66, 69, 70, 72, 77, 80, 82, 85, 86], "local": [32, 34, 94, 96, 104, 140, 141], "localconvert": 77, "locat": [69, 82, 96], "log": [54, 60, 61, 70, 75, 77, 80, 85, 95, 96, 98, 100], "log_odd": 81, "log_p": [54, 77], "log_reg": [51, 53], "logarithm": 70, "logic": [32, 56, 95], "logical_not": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 75, 95], "logist": [9, 41, 51, 53, 55, 57, 64, 65, 78, 84, 86, 87, 142], "logisticregress": [51, 60, 61, 62, 64, 65, 74, 83, 84, 86], "logisticregressioncv": [37, 75, 78, 87, 96], "logit": [75, 80], "loglik": 56, "logloss": [55, 78, 142], "logo": 141, "logspac": 78, "long": [6, 9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 66, 75, 85, 86, 126, 127, 136, 140], "longer": 60, "look": [53, 55, 56, 58, 59, 60, 61, 69, 75, 78, 79, 82, 83, 85], "loop": [64, 84], "loss": [60, 61, 75, 76, 83, 85, 95, 96, 98, 100], "loss_ml_g0": 75, "loss_ml_g1": 75, "loss_ml_m": 75, "low": [59, 73, 84, 94, 140], "lower": [32, 55, 56, 59, 60, 64, 69, 70, 73, 74, 79, 80, 82, 83, 84, 85, 86, 95, 126, 132, 136, 142], "lower_bound": [67, 68], "lpq": [34, 36, 79, 94, 119, 141], "lpq_0": 82, "lpq_1": 82, "lqte": 94, "lr": 83, "lrn": [51, 52, 53, 54, 55, 56, 57, 88, 95, 96, 109, 110, 125, 139, 142], "lrn_0": 56, "lt": [51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 64, 73, 77, 78, 79, 81, 85, 86, 87, 90, 92, 139], "lucien": 141, "luka": 140, "luk\u00e1\u0161": 19, "lusd": [56, 62, 90, 92, 139], "lvert": 70, "m": [6, 7, 8, 9, 15, 16, 17, 24, 37, 52, 54, 56, 60, 62, 66, 70, 73, 75, 76, 77, 80, 84, 89, 91, 92, 93, 94, 95, 96, 97, 100, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 126, 128, 130, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141], "m_": [64, 96, 100, 102, 110, 114, 115, 119, 125], "m_0": [17, 18, 20, 22, 23, 24, 29, 31, 32, 33, 35, 38, 39, 40, 52, 54, 55, 66, 70, 73, 76, 77, 78, 89, 94, 95, 96, 104, 105, 106, 107, 110, 111, 113, 114, 116, 119, 120, 123, 124, 139, 142], "m_hat": [32, 33, 38, 39, 52, 66, 74, 110], "m_i": [83, 96], "ma": [16, 54, 77, 84, 96, 97, 140], "mac": 138, "machin": [4, 5, 6, 7, 8, 16, 17, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 55, 56, 57, 58, 60, 61, 62, 64, 65, 69, 70, 74, 75, 76, 78, 79, 80, 82, 83, 84, 85, 86, 87, 93, 95, 96, 98, 100, 109, 110, 125, 126, 127, 141, 142], "machineri": [70, 140], "mackei": 140, "maco": 138, "made": [96, 108, 142], "mae": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 75, 95], "maggi": 140, "magnitud": [126, 127], "mai": [43, 47, 57, 58, 87], "main": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 61, 70, 79, 86, 96, 125, 126, 127, 140, 142], "mainli": 86, "maintain": [53, 137, 141], "mainten": 141, "major": [56, 86, 141], "make": [51, 64, 65, 75, 76, 86, 94, 95, 141, 142], "make_confounded_irm_data": [86, 141], "make_confounded_plr_data": 85, "make_did_cs2021": [6, 24, 60, 91, 92, 96, 97, 100], "make_did_sz2020": [20, 23, 58, 96, 98], "make_heterogeneous_data": [67, 68, 71, 72, 73], "make_iivm_data": [32, 34, 94, 96], "make_irm_data": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 74, 75, 94, 95, 96], "make_irm_data_discrete_treat": 64, "make_pipelin": 78, "make_pliv_chs2015": [38, 96], "make_pliv_multiway_cluster_ckms2021": [4, 54, 77], "make_plr_ccddhnr2018": [5, 6, 20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 66, 76, 88, 89, 94, 95, 96, 109, 110, 125, 126, 132], "make_simple_rdd_data": [40, 83, 96], "make_spd_matrix": 18, "make_ssm_data": [57, 87, 96], "malt": [137, 140], "maltekurz": 137, "man": [51, 65], "manag": [95, 138], "mani": [15, 27, 28, 52, 53, 54, 56, 58, 66, 76, 77, 89, 110, 125, 142], "manili": 44, "manipul": [55, 56, 83, 96], "manual": [55, 74, 76, 85, 142], "mao": 140, "map": [32, 42, 43, 46, 47, 53, 54, 77, 94, 96, 104], "mapsto": [88, 94], "mar": [19, 96], "march": [70, 75, 76], "margin": [67, 68, 86], "marit": [55, 78], "marker": [60, 64, 86], "markers": 80, "market": 80, "markettwo": 54, "markov": [18, 140], "marr": [55, 78, 79, 85, 142], "marshal": 95, "martin": [19, 86, 137, 140, 141], "masatoshi": 140, "mask": 24, "maskedarrai": [96, 97], "master": 53, "mat": 54, "match": [95, 126, 135], "math": [37, 60, 61], "mathbb": [9, 10, 14, 25, 26, 27, 28, 32, 33, 38, 39, 54, 57, 58, 59, 60, 64, 73, 75, 76, 77, 80, 83, 87, 94, 96, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 111, 113, 114, 115, 116, 118, 119, 120, 123, 124, 125, 126, 128, 130, 131, 132, 133, 134, 135, 136, 139, 142], "mathcal": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 41, 52, 54, 57, 59, 66, 69, 77, 81, 82, 87, 89, 96, 97, 100, 101], "mathop": 94, "mathrm": [9, 10, 60, 61, 83, 96, 97, 100, 101, 110, 114, 126, 131], "matia": 140, "matplotlib": [21, 24, 58, 59, 60, 61, 62, 64, 66, 67, 68, 69, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 86, 87], "matric": [81, 141], "matrix": [9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 43, 47, 52, 54, 55, 56, 57, 66, 77, 87, 89, 90, 92, 95, 125, 139, 141, 142], "matt": 140, "matter": [75, 80], "max": [25, 55, 56, 74, 78, 79, 84, 88, 94, 95, 96, 109, 110, 114, 116, 125, 126, 131, 139, 142], "max_depth": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 62, 78, 85, 88, 94, 95, 96, 98, 109, 110, 125, 126, 132, 139, 142], "max_featur": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 62, 78, 85, 88, 94, 95, 96, 109, 110, 125, 126, 132, 139, 142], "max_it": [77, 78, 86], "maxim": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 81, 94, 96], "maxima": 125, "maximum": [94, 95], "mb": [60, 62, 87, 90, 92, 139], "mb706": 141, "mea": 12, "mean": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 51, 52, 54, 55, 58, 60, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 82, 84, 85, 86, 89, 95, 96, 125, 142], "mean_absolute_error": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 75, 95], "meant": [94, 141], "measir": 85, "measur": [53, 56, 70, 76, 84, 85, 86, 95, 96, 97, 108, 126, 127, 133, 134, 135, 136], "measure_col": 70, "measure_func": 53, "measure_pr": 53, "measures_r": 53, "mechan": [42, 43, 46, 47, 86], "median": [84, 86, 109], "melt": 54, "membership": 86, "memori": [58, 60, 61, 62, 64, 73, 77, 78, 79, 85, 87, 90, 91, 92, 139], "mention": [73, 94], "merg": [55, 78], "mert": [109, 140], "meshgrid": [67, 68, 86], "messag": [52, 53, 54, 55, 56, 57, 139, 141], "meta": [42, 43, 46, 47, 95, 139], "metadata": [42, 43, 46, 47], "metadata_rout": [42, 43, 46, 47], "metadatarequest": [42, 43, 46, 47], "method": [4, 5, 6, 20, 21, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 64, 65, 66, 67, 68, 69, 71, 72, 74, 75, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 127, 132, 137, 139, 141], "methodolog": 140, "methodologi": 86, "metric": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 46, 95], "michael": 140, "michaela": 141, "michel": [137, 139], "michela": [19, 140], "mid": [55, 78, 80, 83, 96], "mid_point": 64, "might": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 54, 69, 74, 75, 77, 81, 83, 85, 86, 95, 96], "mild": [52, 66, 89], "militari": 80, "miller": [54, 77], "mimic": 86, "min": [54, 55, 56, 57, 60, 61, 69, 74, 77, 78, 79, 82, 83, 88, 95, 96, 100, 109, 110, 125, 139, 142], "min_": 94, "min_samples_leaf": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 73, 78, 85, 88, 94, 95, 96, 98, 109, 110, 125, 126, 132, 142], "min_samples_split": [78, 96, 97, 100], "minim": [33, 45, 55, 75, 78, 83, 96], "minor": [52, 66, 89, 110, 141], "minsplit": 55, "minut": 76, "miruna": 140, "mislead": 141, "miss": [4, 5, 6, 37, 56, 95, 96, 110, 123, 141], "missing": [19, 57, 87], "misspecif": 58, "misspecifi": 58, "mit": [137, 139], "mixin": [0, 27, 28, 110], "ml": [18, 54, 55, 56, 70, 76, 77, 78, 83, 88, 93, 95, 96, 109, 137, 140, 141], "ml_g": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 55, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 69, 71, 73, 74, 75, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 94, 95, 96, 97, 98, 100, 141], "ml_g0": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 53, 55, 58, 60, 61, 62, 75, 78, 85, 95, 96, 98, 100], "ml_g1": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 53, 55, 58, 60, 61, 62, 75, 78, 85, 95, 96, 98, 100], "ml_g_d0": [87, 96], "ml_g_d0_t0": [58, 96, 98], "ml_g_d0_t1": [58, 96, 98], "ml_g_d1": [87, 96], "ml_g_d1_t0": [58, 96, 98], "ml_g_d1_t1": [58, 96, 98], "ml_g_d_lvl0": 96, "ml_g_d_lvl1": 96, "ml_g_sim": 37, "ml_l": [38, 39, 52, 54, 55, 56, 62, 66, 68, 72, 76, 77, 78, 80, 85, 88, 89, 95, 96, 109, 110, 125, 126, 132, 139, 141, 142], "ml_l_bonu": 139, "ml_l_forest": 56, "ml_l_forest_pip": 56, "ml_l_lasso": 56, "ml_l_lasso_pip": 56, "ml_l_rf": 142, "ml_l_sim": 139, "ml_l_tune": 95, "ml_l_xgb": 142, "ml_m": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 132, 139, 141, 142], "ml_m_bench_control": 86, "ml_m_bench_treat": 86, "ml_m_bonu": 139, "ml_m_forest": 56, "ml_m_forest_pip": 56, "ml_m_lasso": 56, "ml_m_lasso_pip": 56, "ml_m_rf": 142, "ml_m_sim": [37, 139], "ml_m_tune": 95, "ml_m_xgb": 142, "ml_pi": [37, 57, 87, 96], "ml_pi_sim": 37, "ml_r": [32, 38, 51, 54, 55, 65, 77, 78, 84, 96, 141], "ml_r0": 96, "ml_r1": [55, 78, 96], "mlr": [56, 95], "mlr3": [51, 52, 53, 54, 55, 57, 88, 95, 96, 109, 110, 125, 137, 139, 141, 142], "mlr3book": [56, 95], "mlr3extralearn": [55, 95], "mlr3filter": 56, "mlr3learner": [51, 52, 53, 54, 55, 88, 95, 96, 109, 110, 125, 139, 142], "mlr3measur": 53, "mlr3pipelin": [95, 141], "mlr3tune": [56, 95, 141], "mlr3vers": 55, "mlrmeasur": 53, "mode": [86, 138], "model": [0, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 47, 48, 51, 52, 53, 54, 56, 58, 59, 60, 61, 65, 66, 69, 70, 73, 75, 77, 79, 82, 85, 88, 89, 90, 91, 92, 93, 95, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 111, 113, 114, 115, 117, 118, 121, 122, 127, 132, 133, 134, 135, 136, 137, 140, 141], "model_data": [55, 78], "model_label": 76, "model_select": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 66, 77, 95, 109], "modellist": 74, "modelmlestimatelowerupp": 55, "modern": [56, 95, 137, 139], "modul": [83, 96, 138], "moment": [27, 28, 54, 77, 96, 100, 110, 125, 126, 127, 136, 139], "monoton": 96, "mont": [9, 10, 11, 14, 67, 68, 71, 72], "montanari": 140, "month": 60, "more": [33, 44, 51, 53, 55, 60, 61, 64, 65, 67, 68, 70, 74, 75, 76, 78, 79, 83, 85, 86, 88, 94, 95, 96, 97, 98, 100, 101, 108, 110, 118, 125, 126, 127, 132, 136, 139, 142], "moreov": [55, 56, 70, 95, 125, 142], "mortgag": [55, 78, 79], "most": [55, 69, 75, 78, 79, 82, 86, 94, 95, 96, 126, 132, 138], "motiv": [86, 89], "motivation_example_bch": 70, "mp": 53, "mpd": [54, 77], "mpg": 77, "mse": [56, 70, 95], "mserd": 83, "msr": [56, 95], "mtry": [55, 56, 88, 95, 96, 109, 110, 125, 142], "mu": 59, "mu_": 59, "mu_0": 96, "mu_mean": 59, "much": [55, 56, 78, 83, 84, 86, 142], "muld": [62, 90, 92, 139], "multi": [24, 42, 46, 53, 54, 67, 68, 77, 110, 114], "multiclass": [56, 76], "multiindex": 77, "multioutput": [43, 47], "multioutputregressor": [43, 47], "multipl": [4, 5, 6, 14, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 54, 55, 57, 58, 61, 74, 77, 78, 85, 86, 87, 90, 92, 95, 100, 103, 106, 109, 125, 126, 127, 140, 141, 142], "multipletest": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39], "multipli": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 66, 93, 94, 110, 142], "multiprocess": [69, 79, 82], "multitest": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39], "multivariate_norm": 37, "multiwai": [16, 54, 77, 140], "music": 140, "must": [20, 21, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 95, 96], "mutat": 56, "mutual": [29, 33, 39, 55, 71, 72, 78, 79, 94], "my_sampl": 109, "my_task": 109, "n": [9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 25, 26, 29, 31, 32, 33, 34, 35, 37, 38, 39, 41, 51, 52, 54, 56, 57, 59, 60, 64, 65, 66, 69, 70, 73, 77, 80, 81, 82, 83, 87, 88, 89, 94, 95, 96, 101, 109, 125, 137, 138], "n_": [14, 59], "n_aggreg": 21, "n_coef": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 126, 132], "n_color": 60, "n_complier": 82, "n_core": [69, 79, 82], "n_estim": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 58, 59, 60, 62, 66, 67, 68, 69, 71, 72, 73, 78, 79, 81, 82, 83, 85, 86, 88, 89, 94, 95, 96, 98, 109, 110, 125, 126, 132, 139, 142], "n_eval": [56, 95], "n_featur": [42, 43, 46, 47], "n_fold": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 54, 55, 58, 60, 62, 66, 67, 68, 69, 71, 72, 74, 75, 77, 78, 79, 80, 81, 82, 83, 85, 86, 89, 95, 109, 139, 142], "n_folds_per_clust": [54, 77], "n_folds_tun": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "n_framework": 21, "n_iter": [40, 83, 96], "n_iter_randomized_search": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 95], "n_job": 78, "n_jobs_cv": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 75], "n_jobs_model": [24, 30, 36, 69, 79, 82], "n_level": [14, 64], "n_ob": [6, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 44, 45, 52, 56, 57, 58, 59, 60, 64, 66, 67, 68, 71, 72, 73, 74, 75, 76, 83, 85, 86, 87, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 109, 125, 126, 132, 139], "n_output": [42, 43, 46, 47], "n_period": [25, 60], "n_pre_treat_period": [25, 60], "n_rep": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 54, 57, 58, 60, 62, 64, 66, 73, 74, 75, 77, 83, 85, 86, 87, 89, 95, 109, 126, 132, 139, 142], "n_rep_boot": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 60, 61, 64, 67, 68, 69, 71, 72, 74, 79, 82, 125], "n_sampl": [42, 43, 46, 47, 81, 84], "n_samples_fit": [43, 47], "n_split": 109, "n_t": 59, "n_target": [46, 47], "n_theta": 21, "n_time_period": 59, "n_true": [69, 82], "n_var": [52, 56, 66, 89, 90, 92, 95, 125, 139], "n_w": 81, "n_x": [11, 67, 68, 71, 72, 73], "na": [4, 5, 6, 52, 54, 57, 89, 141], "na_real_": [54, 141], "naiv": [52, 66, 89], "name": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 52, 53, 54, 60, 71, 72, 73, 76, 77, 83, 85, 86, 95, 138, 141], "namespac": 53, "nan": [4, 5, 6, 20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 58, 59, 64, 66, 69, 71, 72, 75, 76, 78, 79, 82, 87, 89, 95], "nanmean": 66, "narita": 140, "nat": 60, "nathan": 140, "nation": [86, 109, 140], "nativ": 53, "natt": 81, "natur": 86, "ncol": [54, 55, 56, 83, 90, 92, 95, 125, 139], "ncoverag": 75, "ndarrai": [4, 5, 6, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 90, 92], "nearli": 75, "necess": [54, 77], "necessari": [53, 54, 76, 77, 83, 96, 138], "need": [20, 22, 23, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 53, 55, 57, 65, 66, 76, 79, 84, 87, 95, 109, 126, 136, 141, 142], "neg": [43, 47], "neighborhood": [83, 125], "neither": [4, 5, 6, 54, 77, 90, 92], "neng": 140, "neq": [83, 96], "nest": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 46, 47, 95, 110, 124, 126, 132], "net": [79, 85, 142], "net_tfa": [55, 78, 79, 85, 142], "nev": [96, 100, 101], "never": [25, 32, 53, 54, 60, 61, 72, 77, 96, 101, 141], "never_tak": [32, 55, 78], "never_tr": [22, 24, 60, 61, 96, 97, 100], "nevertheless": 74, "new": [51, 52, 53, 54, 55, 56, 57, 67, 68, 76, 78, 81, 88, 89, 90, 92, 94, 95, 96, 109, 110, 125, 137, 139, 140, 141, 142], "new_data": [67, 68, 81], "newei": [7, 8, 17, 54, 70, 77, 86, 89, 137, 140], "newest": 141, "next": [53, 55, 56, 67, 68, 69, 73, 75, 78, 79, 81, 82, 84, 86, 141], "neyman": [54, 77, 88, 93, 126, 136, 137, 140], "nfold": [54, 55, 57, 96], "nh": 96, "nice": 53, "nifa": [78, 79, 85], "nil": 86, "nine": [54, 77], "nn": 83, "noack": [83, 96, 140, 141], "node": [55, 56, 88, 96, 109, 110, 125, 139, 142], "nois": [41, 80, 81], "nomin": 84, "non": [16, 17, 18, 22, 25, 26, 32, 40, 51, 52, 55, 59, 60, 65, 66, 78, 79, 81, 83, 95, 109, 110, 114, 125, 126, 131], "non_orth_scor": [52, 66, 110], "nondur": 62, "none": [4, 5, 6, 14, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 54, 55, 58, 60, 61, 62, 64, 65, 73, 78, 79, 84, 85, 86, 87, 90, 91, 92, 95, 96, 98, 100, 110, 125, 138, 139], "nonignor": [37, 124], "nonlinear": [25, 28, 55, 60, 78, 83, 96, 110, 119, 120, 141], "nonlinearscoremixin": [0, 110], "nonparametr": [34, 35, 36, 83, 86, 110, 126, 127, 133, 134, 135, 136, 140], "nop": 56, "nor": [4, 5, 6, 54, 77, 90, 92], "norm": 66, "normal": [20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 57, 58, 59, 60, 61, 65, 66, 69, 73, 79, 80, 81, 82, 83, 84, 87, 89, 90, 92, 95, 96, 110, 111, 113, 114, 125, 139], "normalize_ipw": [29, 30, 31, 32, 33, 34, 35, 36, 37, 57, 74, 79, 87], "not_yet_tr": [22, 24, 60], "notat": [54, 57, 58, 77, 87, 96, 98, 100, 101, 108, 110, 114], "note": [4, 5, 6, 21, 24, 27, 28, 32, 33, 37, 38, 39, 42, 43, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 109, 110, 137, 139], "notebook": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 94, 95, 96, 142], "notic": [51, 65, 84], "now": [53, 54, 55, 57, 61, 67, 68, 75, 77, 78, 81, 84, 86, 87, 139, 141], "np": [4, 5, 6, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 58, 59, 60, 61, 62, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 132, 139, 142], "nround": [52, 55, 142], "nrow": [53, 54, 56, 83, 90, 92, 95, 125, 139], "nu": [18, 26, 32, 57, 87, 96, 104, 126, 127, 132, 135, 136], "nu2": [126, 132], "nu_0": [126, 136], "nu_i": [57, 87], "nuis_g0": 51, "nuis_g1": 51, "nuis_l": 142, "nuis_m": [51, 142], "nuis_r0": 51, "nuis_r1": 51, "nuis_rmse_ml_l": 70, "nuis_rmse_ml_m": 70, "nuisanc": [4, 5, 6, 17, 18, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 54, 55, 56, 57, 58, 66, 67, 68, 69, 70, 73, 75, 77, 78, 79, 82, 84, 85, 86, 87, 88, 89, 95, 96, 100, 109, 110, 111, 113, 114, 115, 119, 125, 126, 131, 136, 137, 141, 142], "nuisance_el": [126, 128, 130, 131, 133, 134, 135], "nuisance_loss": [75, 95, 141], "nuisance_target": 75, "null": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 53, 85, 95, 126, 132, 141], "null_hypothesi": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 85, 126, 132], "num": [55, 56, 88, 95, 96, 109, 110, 125, 139], "num_leav": [59, 69, 79, 82], "number": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 47, 52, 54, 59, 60, 66, 67, 68, 69, 70, 71, 72, 75, 77, 79, 81, 82, 83, 86, 96, 100, 109, 125, 137, 139, 142], "numer": [28, 51, 56, 74, 80, 95, 110, 126, 133, 134, 141], "numeric_onli": 70, "numpi": [4, 5, 6, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 45, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 92, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 132, 139], "nuniqu": 60, "ny": 140, "nyt": [96, 100, 101], "o": [59, 64, 70, 71, 72, 75, 76, 78, 80, 83, 125, 137, 139], "ob": [53, 55, 59, 83], "obei": 110, "obj": 78, "obj_dml_data": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 54, 61, 65, 66, 69, 74, 76, 77, 82, 88, 89, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 132, 141], "obj_dml_data_bonu": [90, 92], "obj_dml_data_bonus_df": [90, 92], "obj_dml_data_from_arrai": [4, 5, 6], "obj_dml_data_from_df": [4, 5], "obj_dml_data_sim": [90, 92], "obj_dml_plr": [52, 66, 89], "obj_dml_plr_bonu": [56, 139], "obj_dml_plr_bonus_pip": 56, "obj_dml_plr_bonus_pipe2": 56, "obj_dml_plr_bonus_pipe3": 56, "obj_dml_plr_bonus_pipe_ensembl": 56, "obj_dml_plr_fullsampl": 76, "obj_dml_plr_lesstim": 76, "obj_dml_plr_nonorth": [52, 66], "obj_dml_plr_orth_nosplit": [52, 66], "obj_dml_plr_sim": [56, 139], "obj_dml_plr_sim_pip": 56, "obj_dml_plr_sim_pipe_ensembl": 56, "obj_dml_plr_sim_pipe_tun": 56, "obj_dml_sim": 37, "object": [4, 5, 6, 7, 8, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 51, 55, 56, 57, 58, 60, 61, 62, 64, 67, 68, 69, 72, 73, 74, 76, 78, 79, 82, 83, 87, 90, 91, 92, 94, 95, 96, 97, 98, 100, 109, 110, 125, 137, 139, 140, 141, 142], "obs_confound": [51, 65], "observ": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 37, 38, 39, 41, 44, 45, 48, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 73, 75, 76, 77, 78, 79, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 101, 108, 109, 110, 111, 113, 114, 125, 126, 127, 128, 130, 131, 139, 140, 142], "obtain": [10, 32, 51, 52, 53, 54, 57, 58, 60, 61, 65, 66, 67, 68, 69, 70, 75, 77, 82, 84, 86, 87, 88, 89, 94, 95, 109, 110, 125, 126, 127, 132, 138, 139], "obvious": 60, "occur": [25, 60, 76, 141], "off": [81, 140], "offer": [25, 53, 55, 78, 79, 86, 142], "offici": 138, "offset": 95, "often": 82, "oka": 140, "ol": [29, 33, 39, 44], "olma": [83, 96, 140, 141], "omega": [73, 94, 96, 97, 110, 115, 118, 126, 133, 134], "omega_": [16, 54, 77], "omega_1": [16, 54, 77], "omega_2": [16, 54, 77], "omega_epsilon": [54, 77], "omega_v": [16, 54, 77], "omega_x": [16, 54, 77], "omit": [60, 85, 86, 96, 100, 110, 114, 126, 127, 136, 140, 141, 142], "ommit": 86, "onc": [53, 76, 86, 96, 101, 142], "one": [21, 24, 38, 48, 51, 52, 53, 54, 55, 56, 60, 61, 64, 65, 66, 67, 68, 75, 77, 79, 80, 83, 84, 85, 86, 89, 94, 95, 96, 97, 100, 101, 106, 109, 110, 111, 113, 114, 118, 121, 122, 125, 126, 127, 132, 133, 134, 135, 139, 141], "ones": [56, 59, 69, 76, 82, 85, 94], "ones_lik": [64, 82], "onli": [4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 46, 47, 53, 54, 55, 60, 67, 68, 71, 72, 73, 75, 76, 77, 78, 79, 83, 88, 94, 95, 96, 100, 108, 110, 114, 116, 119, 120, 125, 126, 127, 131, 133, 134, 136, 141], "onlin": 142, "onto": 75, "oo": 76, "oob_error": [56, 95], "oop": 141, "opac": [67, 68], "open": [56, 95, 137, 139], "oper": 56, "opposit": [81, 83, 96], "oprescu": [11, 67, 68, 71, 72, 140], "opt": [60, 61, 78], "optim": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 40, 56, 67, 68, 76, 81, 94, 95, 140], "option": [20, 21, 22, 23, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 51, 52, 54, 55, 57, 60, 64, 67, 68, 71, 72, 73, 75, 77, 78, 79, 87, 95, 96, 109, 110, 116, 119, 120, 125, 141], "oracl": [14, 41, 60, 64], "oracle_valu": [9, 10, 14, 41, 64], "orang": 52, "orcal": [9, 10], "order": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 46, 53, 54, 55, 56, 74, 77, 78, 83, 95, 96, 109, 110], "org": [12, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 95, 137, 138, 141], "orient": [56, 95, 110, 137, 139, 140, 141], "origin": [42, 43, 45, 46, 47, 53, 56, 60, 61, 72, 81, 85, 86, 94, 110, 118], "orign": [55, 78], "orth_sign": [44, 45, 74], "orthogon": [44, 45, 54, 55, 77, 78, 88, 93, 96, 125, 126, 136, 137, 140], "orthongon": [126, 136], "osx": 138, "other": [4, 5, 6, 38, 39, 42, 43, 46, 47, 52, 54, 55, 56, 57, 58, 60, 61, 64, 66, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 89, 90, 92, 94, 95, 96, 106, 107, 109, 110, 118, 125, 126, 136, 137, 138, 139, 140, 141, 142], "other_ind": 77, "otherwis": [20, 22, 23, 29, 32, 33, 39, 42, 43, 46, 47, 55, 78, 79, 81, 96, 98, 110, 114], "othrac": [56, 62, 90, 92, 139], "our": [52, 53, 55, 56, 58, 60, 66, 67, 68, 69, 75, 76, 78, 79, 82, 83, 85, 86, 89, 96, 137, 139, 141, 142], "ourselv": 75, "out": [38, 39, 54, 56, 58, 60, 61, 62, 70, 75, 76, 77, 79, 85, 86, 87, 88, 90, 92, 93, 94, 95, 96, 97, 98, 100, 110, 121, 122, 125, 126, 127, 132, 135, 137, 139, 141, 142], "outcom": [4, 5, 6, 9, 10, 14, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 51, 53, 54, 55, 56, 59, 60, 61, 62, 65, 70, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 90, 91, 92, 95, 98, 100, 101, 102, 106, 107, 108, 114, 115, 125, 127, 132, 133, 135, 136, 139, 141, 142], "outcome_0": 65, "outcome_1": 65, "outer": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 95], "output": [53, 60, 61, 75, 84, 88, 96, 100, 125, 142], "output_list": 84, "outshr": 77, "outsid": 52, "over": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 37, 38, 39, 52, 60, 64, 66, 70, 75, 89, 93, 95, 126, 132], "overal": [21, 60, 61, 81, 86, 96, 97], "overall_aggregation_weight": [21, 60, 61], "overcom": [93, 110], "overfit": [76, 93, 109], "overlap": [58, 86, 96, 98, 101], "overrid": [95, 141], "overridden": 96, "overst": [55, 78, 79], "overview": [75, 125, 126, 132, 140], "overwrit": 141, "ownership": [55, 78], "p": [9, 10, 11, 13, 20, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 89, 94, 95, 96, 97, 98, 100, 101, 109, 110, 111, 113, 114, 115, 116, 119, 120, 123, 124, 125, 126, 133, 134, 137, 138, 139, 141], "p401": [55, 78, 79], "p_0": [110, 111, 113], "p_1": 125, "p_adjust": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 109, 125, 137, 139], "p_dbl": [56, 95], "p_hat": 74, "p_int": 95, "p_n": 15, "p_val": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39], "p_x": [16, 54, 77], "p_x0": 80, "p_x1": 80, "packag": [51, 52, 54, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 76, 77, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 94, 95, 96, 98, 101, 108, 109, 110, 125, 126, 127, 137, 139, 140, 141, 142], "packagedata": 77, "packagevers": 55, "page": [86, 137, 140], "pair": [51, 65], "pake": [54, 77], "paket": [54, 55, 56], "pal": 54, "palett": [21, 24, 60, 64], "pand": 60, "panda": [4, 5, 6, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 44, 45, 57, 58, 59, 60, 61, 62, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 90, 91, 92, 94, 96, 126, 127, 139], "pandas2ri": 77, "panel": [5, 6, 20, 22, 25, 26, 91, 92, 100, 101, 130, 131, 140, 141], "paper": [12, 15, 56, 76, 80, 83, 85, 86, 126, 136, 137, 139, 140, 141], "par": 62, "par_grid": [56, 95], "paradox": [56, 95, 141], "parallel": [53, 58, 59, 60, 64, 69, 75, 82, 96, 98, 100, 101], "param": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 76, 95], "param_grid": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 95], "param_nam": 53, "param_set": [56, 95], "param_v": 56, "paramet": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 55, 57, 58, 60, 64, 66, 67, 68, 69, 70, 73, 74, 75, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 98, 100, 101, 102, 103, 104, 105, 108, 109, 119, 120, 125, 126, 127, 132, 134, 136, 137, 139, 140, 141, 142], "parametr": [32, 53, 86, 89, 95, 142], "params_exact": 95, "params_nam": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 53], "parenttoc": 137, "part": [18, 52, 54, 55, 56, 61, 66, 75, 76, 77, 78, 89, 95, 109, 126, 136, 141, 142], "parti": 18, "partial": [10, 15, 16, 17, 18, 28, 38, 39, 54, 56, 62, 70, 76, 77, 85, 88, 93, 95, 106, 107, 109, 121, 122, 125, 132, 133, 134, 135, 136, 137, 139, 141, 142], "partial_": [110, 125], "partiallli": 85, "particip": [7, 79, 85, 142], "particular": [96, 137], "particularli": 76, "partion": [54, 77], "partit": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 77, 88, 93], "partli": 142, "pass": [29, 33, 39, 42, 43, 44, 46, 47, 53, 56, 76, 95, 142], "passo": [137, 139], "past": 54, "paste0": [54, 57], "pastel": 66, "path": [95, 96], "path_to_r": 70, "patsi": [67, 68, 94], "pattern": 86, "paul": 140, "pd": [4, 5, 6, 7, 8, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 58, 59, 60, 61, 64, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 96], "pdf": [66, 80], "pedregosa": [137, 139], "pedregosa11a": [137, 139], "pedro": [53, 140], "penal": [57, 87], "penalti": [55, 56, 65, 78, 84, 86, 87, 95, 96], "pennsylvania": [8, 90, 92, 139], "pension": [55, 78, 79, 142], "peopl": [55, 78, 79], "pep8": 141, "per": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 60, 61, 77], "percent": 95, "percentag": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39], "perf_count": 75, "perfectli": [83, 96], "perform": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 45, 52, 54, 56, 58, 60, 61, 66, 70, 72, 73, 75, 76, 77, 79, 85, 86, 87, 89, 95, 96, 98, 100, 109, 110, 125, 137, 139, 140, 142], "perfrom": 73, "perhap": 142, "period": [20, 22, 24, 25, 53, 58, 59, 61, 91, 92, 97, 98, 100, 101, 114, 140], "perp": [96, 108], "perrot": [137, 139], "person": 142, "pessimist": 86, "peter": 140, "petra": 141, "petronelaj": 141, "pfister": [56, 95, 137, 139], "phi": [54, 77, 94, 125], "philipp": [86, 137, 140], "philippbach": [137, 141], "pi": [13, 15, 18, 37, 94, 96, 110, 123, 124], "pi_": [16, 54, 77], "pi_0": [110, 123, 124], "pi_i": [57, 87, 96], "pick": [83, 142], "pip": [83, 96], "pip3": 138, "pipe": 56, "pipe_forest_classif": 56, "pipe_forest_regr": 56, "pipe_lasso": 56, "pipelin": [42, 43, 46, 47, 56, 78, 141], "pipeop": 56, "pira": [55, 78, 79, 85, 142], "pivot": [70, 77, 140], "plai": [76, 142], "plan": [7, 55, 78, 79, 142], "plausibl": [86, 126], "pleas": [42, 43, 46, 47, 53, 64, 76, 86, 109, 138], "plim": 80, "pliv": [27, 28, 38, 54, 77, 88, 94, 106, 121, 137, 141], "plm": [0, 93, 95, 125, 132, 142], "plot": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 45, 52, 53, 55, 56, 57, 59, 60, 61, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 78, 79, 80, 82, 83, 85, 86, 87, 94, 126, 132], "plot_data": 60, "plot_effect": [21, 24, 60, 61], "plot_tre": [45, 81, 94], "plotli": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 67, 68, 70, 83, 86], "plr": [27, 28, 39, 56, 76, 80, 85, 88, 95, 107, 109, 122, 125, 132, 134, 135, 136, 137, 139, 141, 142], "plr_est": 80, "plr_est1": 80, "plr_est2": 80, "plr_obj": 80, "plr_obj_1": 80, "plr_obj_2": 80, "plr_summari": 78, "plrglmnet": 55, "plrranger": 55, "plrrpart": 55, "plrxgboost8700": 55, "plt": [58, 59, 60, 62, 64, 66, 67, 68, 69, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 86, 87], "plt_smpl": [54, 77], "plt_smpls_cluster": [54, 77], "plug": [73, 126, 128, 130, 131, 132, 133, 134], "pm": [40, 54, 77, 125, 126, 132, 136], "pmatrix": [57, 87], "pmlr": [70, 75, 76], "po": [56, 95], "poe": 140, "point": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 53, 54, 71, 72, 77, 86, 94, 96, 142], "pointwis": [44, 69, 71, 72, 82], "poli": [55, 74, 77, 78], "polici": [33, 38, 39, 45, 93, 96, 106, 107, 139, 140, 141], "policy_tre": [33, 81, 94], "policy_tree_2": 81, "policy_tree_obj": 94, "policytre": 81, "polit": 80, "poly_dict": 78, "polynomi": [7, 8, 41, 55, 62, 74, 78, 83], "polynomial_featur": [7, 8, 55, 62], "polynomialfeatur": [74, 77, 78], "poor": 84, "popul": [86, 96, 100, 110, 114], "popular": [75, 96, 126, 127], "porport": 85, "posit": [18, 55, 60, 61, 80, 86, 142], "posixct": [56, 95], "possibl": [4, 5, 6, 43, 46, 47, 53, 56, 60, 61, 67, 68, 71, 72, 73, 74, 75, 76, 81, 83, 84, 85, 86, 95, 96, 100, 102, 103, 125, 126, 127, 141, 142], "possibli": [84, 126, 127], "post": [15, 18, 24, 96, 98, 100, 101, 125, 140], "postdoubl": 140, "poster": 80, "potenti": [9, 14, 25, 29, 30, 31, 34, 35, 37, 41, 57, 58, 60, 74, 80, 83, 87, 98, 101, 102, 108, 115, 116, 125, 133, 138, 141, 142], "potential_level": 64, "power": [56, 76, 86, 95, 140], "pp": [53, 70, 75, 76], "pq": [34, 35, 36, 79, 120, 141], "pq_0": [79, 82], "pq_1": [79, 82], "pr": [37, 51, 54, 55, 56, 57, 95, 96, 109, 110, 125, 139, 142], "practic": [75, 86, 140], "pre": [22, 24, 25, 53, 57, 58, 60, 61, 87, 95, 96, 98, 100, 110, 114], "precis": [53, 96, 126, 134, 142], "precomput": [43, 47], "pred": [53, 76], "pred_df": 81, "pred_dict": 95, "pred_treat": 81, "predict": [4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 52, 54, 55, 56, 66, 69, 70, 74, 75, 76, 77, 78, 81, 84, 86, 89, 94, 109, 126, 127, 132, 134, 141, 142], "predict_proba": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 39, 40, 42, 46, 76, 84, 95], "predictor": [29, 33, 39, 44, 45, 67, 68, 71, 72, 86, 88], "prefer": [55, 78, 79, 142], "preliminari": [31, 52, 66, 83, 110, 116, 119, 120, 124], "prepar": [53, 54, 77, 141], "preprint": [84, 140], "preprocess": [55, 74, 77, 78, 79, 95], "presenc": [55, 78, 79], "present": [53, 86, 95, 110, 118, 142], "prespecifi": 85, "pretest": 53, "pretreat": [20, 22, 23, 24, 53, 58], "prettenhof": [137, 139], "preval": 86, "prevent": [109, 141], "previou": [59, 73, 74, 80, 138, 142], "previous": [84, 95, 142], "price": [54, 77], "priliminari": [34, 36], "primari": 64, "principl": [126, 127], "print": [22, 24, 40, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 90, 91, 92, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 132, 138, 139, 141, 142], "print_detail": 53, "print_period": [22, 24], "prior": [75, 96, 108], "privat": 141, "prob": 56, "prob_dist": 84, "prob_dist_": 84, "probabilit": 73, "probabl": [14, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 46, 52, 53, 57, 58, 60, 64, 66, 73, 80, 82, 83, 86, 87, 89, 96, 110, 111, 113, 114, 119, 140], "problem": [55, 78, 79, 94, 95], "procedur": [52, 54, 55, 66, 75, 77, 78, 85, 86, 95, 125, 138, 141], "proceed": [15, 140], "process": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 41, 53, 57, 58, 59, 61, 67, 68, 69, 70, 71, 72, 75, 76, 81, 82, 86, 87, 93, 125, 126, 127, 140, 141], "produc": 80, "product": [67, 68, 70, 75, 86, 126, 136], "producton": 54, "program": [13, 55, 78, 79, 140, 142], "progress": 63, "project": [56, 67, 68, 94, 137, 141], "project_z": [67, 68], "prone": 110, "pronounc": 83, "propens": [9, 10, 25, 34, 36, 55, 57, 58, 60, 73, 74, 75, 78, 79, 86, 87, 94, 96, 100, 102, 110, 114, 126, 133], "properli": [76, 142], "properti": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 56, 60, 61, 75, 78, 79, 80, 84, 85, 95, 96, 126, 132, 139, 141], "proport": [85, 126, 127, 135, 136], "propos": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 54, 56, 77, 83, 126, 127, 140, 141], "provid": [4, 5, 6, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 53, 54, 55, 56, 67, 68, 71, 72, 74, 76, 77, 78, 83, 86, 88, 89, 90, 91, 92, 93, 95, 125, 137, 139, 141, 142], "prune": [33, 45], "ps911c": 77, "ps944": 77, "pscore1": 80, "pscore2": 80, "psi": [27, 28, 52, 53, 54, 77, 88, 96, 100, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 136, 139], "psi_": [125, 126, 132, 135, 136], "psi_a": [27, 32, 33, 38, 39, 52, 54, 66, 77, 96, 100, 109, 110, 111, 113, 114, 115, 117, 118, 121, 122, 125], "psi_b": [27, 32, 33, 38, 39, 52, 66, 94, 96, 100, 109, 110, 111, 113, 114, 115, 117, 118, 121, 122], "psi_el": [109, 110], "psi_j": 125, "psi_nu2": [126, 132], "psi_sigma2": [126, 132], "public": [51, 65, 141], "publish": [86, 141], "pull": [55, 141], "purchas": 86, "pure": 86, "purp": [67, 68], "purpos": [52, 66, 73, 85, 86, 110, 114, 126, 127, 139], "pval": 125, "px": [70, 83], "py": [60, 61, 72, 77, 78, 86, 137, 138, 141], "py3": 138, "py_al": 66, "py_did": 58, "py_did_pretest": 59, "py_dml": 66, "py_dml_nosplit": 66, "py_dml_po": 66, "py_dml_po_nosplit": 66, "py_double_ml_apo": 64, "py_double_ml_bas": 66, "py_double_ml_basic_iv": 65, "py_double_ml_c": 67, "py_double_ml_cate_plr": 68, "py_double_ml_cvar": 69, "py_double_ml_firststag": 70, "py_double_ml_g": 71, "py_double_ml_gate_plr": 72, "py_double_ml_gate_sensit": 73, "py_double_ml_irm_vs_apo": 74, "py_double_ml_learn": 75, "py_double_ml_meets_flaml": 76, "py_double_ml_multiway_clust": 77, "py_double_ml_pens": 78, "py_double_ml_pension_qt": 79, "py_double_ml_plm_irm_hetfx": 80, "py_double_ml_policy_tre": 81, "py_double_ml_pq": 82, "py_double_ml_rdflex": 83, "py_double_ml_robust_iv": 84, "py_double_ml_sensit": 85, "py_double_ml_sensitivity_book": 86, "py_double_ml_ssm": 87, "py_non_orthogon": 66, "py_panel": 60, "py_panel_simpl": 61, "py_po_al": 66, "pydata": 72, "pypi": [140, 141], "pyplot": [58, 59, 60, 62, 64, 66, 67, 68, 69, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 86, 87], "pyproject": 141, "python": [18, 53, 76, 86, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 100, 109, 110, 125, 126, 127, 132, 137, 139, 140, 141, 142], "python3": [60, 61, 78, 138], "q": [56, 69, 82, 83, 95, 137, 139], "q2": [56, 62, 90, 92, 139], "q3": [56, 62, 90, 92, 139], "q4": [56, 62, 90, 92, 139], "q5": [56, 62, 90, 92, 139], "q6": [56, 62, 90, 92, 139], "q_i": [83, 96], "qquad": 13, "qte": [69, 79, 141], "quad": [25, 26, 55, 57, 58, 78, 81, 83, 87, 94, 96, 98, 101, 108, 110, 119, 125, 126, 128], "quadrat": [57, 87], "qualiti": [85, 88, 141], "quanitl": 79, "quant": 69, "quantifi": [86, 96, 101], "quantil": [14, 24, 30, 31, 34, 35, 36, 60, 64, 69, 74, 85, 93, 95, 116, 119, 120, 140, 141], "quantiti": [51, 65, 86], "queri": 78, "question": [86, 142], "quick": 79, "quit": [75, 81, 85, 126, 127], "r": [12, 32, 42, 43, 46, 47, 59, 60, 61, 66, 67, 68, 70, 77, 80, 83, 84, 86, 88, 89, 90, 92, 93, 96, 104, 109, 110, 117, 121, 125, 126, 127, 133, 134, 135, 136, 137, 139, 140, 141, 142], "r2_d": [13, 75], "r2_score": [43, 47], "r2_y": [13, 75], "r6": [56, 141], "r_0": [32, 38, 55, 78, 96, 104], "r_all": 52, "r_d": 13, "r_df": 77, "r_dml": 52, "r_dml_nosplit": 52, "r_dml_po": 52, "r_dml_po_nosplit": 52, "r_double_ml_bas": 52, "r_double_ml_basic_iv": 51, "r_double_ml_did": 53, "r_double_ml_multiway_clust": 54, "r_double_ml_pens": 55, "r_double_ml_pipelin": 56, "r_double_ml_ssm": 57, "r_hat": 38, "r_hat0": 32, "r_hat1": 32, "r_non_orthogon": 52, "r_po_al": 52, "r_y": 13, "rais": [4, 5, 6, 42, 43, 46, 47, 60, 61, 95], "randint": 80, "randn": 37, "random": [11, 14, 18, 19, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 51, 52, 53, 55, 56, 58, 59, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 94, 95, 97, 98, 100, 101, 109, 123, 125, 126, 132, 136, 139, 140, 142], "random_search": 95, "random_st": [14, 60, 66, 73, 74, 81], "randomforest": [55, 75, 78], "randomforest_class": [55, 67, 78, 81], "randomforest_reg": [67, 81], "randomforestclassifi": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 62, 64, 67, 68, 71, 72, 73, 75, 78, 81, 83, 85, 86, 94, 95, 96, 97, 98, 100, 142], "randomforestregressor": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 62, 64, 66, 67, 68, 71, 72, 73, 75, 78, 81, 83, 85, 86, 88, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 132, 139, 142], "randomized_search": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 95], "randomizedsearchcv": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 95], "randomli": [52, 54, 66, 77, 89, 109, 142], "rang": [52, 58, 59, 64, 66, 69, 71, 72, 75, 76, 77, 79, 81, 82, 83, 84, 86, 87, 89, 95, 96], "rangeindex": [58, 60, 61, 62, 64, 73, 77, 78, 79, 85, 87, 90, 91, 92, 139], "ranger": [53, 55, 56, 88, 95, 96, 109, 110, 125, 139, 142], "rangl": [11, 81], "rank": 141, "rate": [70, 75, 96], "rather": [83, 86, 96], "ratio": [95, 109, 126, 127], "ravel": [67, 68], "raw": [55, 61, 70, 78], "raw_res_manual_lasso_r_100_n_100_p_200_rho_0": 70, "rbind": 55, "rbindlist": 55, "rbinom": 51, "rbrace": [12, 13, 19, 32, 33, 54, 77, 88, 96, 102, 104, 105, 109, 110, 115, 125, 126, 133], "rcolorbrew": 54, "rcparam": [59, 62, 67, 68, 69, 71, 72, 74, 77, 78, 79, 82], "rd": [96, 141], "rdbu": 54, "rdbu_r": 77, "rdbwselect": 96, "rdd": [0, 4, 5, 6, 93, 138], "rdflex": [83, 96, 141], "rdflex_fuzzi": 83, "rdflex_fuzzy_stack": 83, "rdflex_obj": [40, 96], "rdflex_sharp": 83, "rdflex_sharp_stack": 83, "rdrobust": [40, 83, 96, 138, 141], "rdrobust_fuzzi": 83, "rdrobust_fuzzy_noadj": 83, "rdrobust_sharp": 83, "rdrobust_sharp_noadj": 83, "rdt044": 70, "re": [77, 86, 138], "read": 138, "read_csv": [61, 70], "readabl": 141, "reader": 84, "readili": 137, "real": [55, 78, 79, 85, 126, 127], "realat": 96, "realiz": [83, 96, 108], "reason": [4, 5, 6, 51, 65, 70, 75, 76, 85, 86, 126, 127, 142], "recal": [62, 126, 136], "receiv": [25, 60, 64, 83, 96, 98], "recent": [76, 96, 98, 101, 140], "recogn": [55, 78, 79], "recommend": [56, 60, 75, 83, 86, 88, 96, 109, 126, 138, 140, 141], "recov": [51, 53, 65, 80], "recsi": 140, "red": [54, 57, 71, 72, 76, 77], "reduc": [55, 73, 76, 78, 83, 85, 86, 96, 141], "redund": 141, "reemploy": [8, 90, 92, 139], "refactor": 141, "refer": [7, 8, 9, 10, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 59, 60, 64, 73, 78, 79, 83, 85, 90, 92, 93, 94, 96, 97, 100, 101, 110, 126, 127, 132, 140, 141], "reference_level": [30, 64, 74, 96], "refin": 141, "refit": [126, 127], "reflect": [81, 86, 94], "reg": [25, 26, 55, 78, 142], "reg_estim": 83, "reg_learn": 79, "reg_learner_1": 75, "reg_learner_2": 75, "regard": [86, 137], "regener": 141, "region": [54, 69, 77, 125, 140], "regr": [51, 52, 53, 54, 55, 56, 57, 88, 95, 96, 109, 110, 125, 139, 142], "regravg": [56, 95], "regress": [9, 10, 12, 13, 14, 15, 16, 17, 18, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 44, 51, 53, 54, 56, 57, 60, 61, 64, 65, 70, 76, 77, 80, 84, 85, 86, 87, 88, 89, 93, 94, 95, 98, 100, 104, 105, 106, 107, 109, 114, 125, 127, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142], "regressor": [43, 47, 52, 55, 64, 66, 69, 75, 76, 78, 89], "regular": [15, 93, 95, 110, 125, 140], "reich": [56, 95], "reinforc": 140, "reject": [55, 78], "rel": [55, 78, 84, 126, 127, 133, 134], "relat": [74, 86, 142], "relationship": [51, 65, 70, 86, 125], "relev": [4, 5, 6, 11, 20, 22, 23, 24, 42, 43, 44, 46, 47, 60, 69, 81, 82, 96, 126, 142], "reli": [58, 59, 60, 67, 68, 73, 74, 94, 95, 96, 98, 110, 114, 126, 127, 142], "reload": 55, "remain": [53, 125, 142], "remark": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 52, 59, 60, 61, 64, 66, 67, 68, 69, 71, 72, 73, 74, 75, 79, 85, 94, 95, 96, 98, 100, 110, 111, 113, 114, 119, 120, 125, 126, 131, 134], "remot": 138, "remov": [55, 74, 86, 93, 96, 109, 110, 126, 141], "renam": [60, 78, 141], "render": [85, 86], "reorgan": 141, "rep": [52, 57, 89, 95, 125], "repeat": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 54, 55, 56, 57, 60, 61, 66, 73, 77, 78, 79, 80, 83, 85, 87, 89, 93, 95, 100, 125, 128, 139, 141, 142], "repeatedkfold": 77, "repet": 85, "repetit": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 67, 68, 70, 71, 72, 73, 75, 93, 95, 125, 139, 142], "repetiton": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40], "replac": [81, 86, 141], "replic": [7, 8, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 55, 61, 66, 70, 84, 86], "repo": 141, "report": [55, 76, 78, 137, 141], "repositori": [60, 70, 83, 141], "repr": [52, 54], "repres": [25, 80, 86, 96], "represent": [10, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 85, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 139, 141], "reproduc": 14, "request": [42, 43, 46, 47, 141], "requir": [38, 39, 42, 46, 51, 55, 56, 60, 61, 64, 73, 78, 79, 85, 96, 97, 100, 110, 114, 125, 126, 127, 132, 138, 141, 142], "requirenamespac": 53, "res_df": 77, "res_dict": [9, 10, 11, 14, 41], "resampl": [51, 54, 56, 57, 58, 60, 61, 77, 79, 85, 87, 95, 96, 98, 100, 109, 110, 125, 137, 139, 142], "research": [54, 56, 77, 80, 86, 109, 137, 139, 140, 142], "resembl": [57, 87], "reset": 53, "reset_index": [60, 70, 77, 78], "reshap": [59, 66, 67, 68, 74], "reshape2": 54, "residu": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 85, 126, 127, 135, 136], "resolut": [56, 95], "resourc": 75, "resourcewis": 75, "respect": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 60, 61, 64, 78, 79, 83, 94, 96, 100, 104, 108, 109, 126, 136, 142], "respons": [7, 56, 95], "rest": 96, "restart": 138, "restrict": 75, "restructur": 141, "restud": 70, "result": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 51, 52, 53, 56, 57, 58, 59, 60, 61, 64, 66, 67, 68, 70, 73, 74, 75, 81, 83, 84, 85, 86, 87, 89, 95, 109, 110, 111, 113, 114, 126, 127, 132, 139, 141], "result_iivm": 55, "result_irm": 55, "result_plr": 55, "results_df": 84, "retain": [42, 43, 46, 47], "retina": 80, "retir": [55, 78, 79, 85], "return": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 56, 57, 60, 61, 66, 69, 72, 75, 76, 77, 80, 81, 82, 84, 85, 86, 87, 88, 95, 110, 126, 127, 141], "return_count": [64, 75], "return_tune_r": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "return_typ": [4, 5, 6, 7, 8, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 55, 56, 57, 58, 66, 74, 75, 76, 78, 79, 85, 87, 88, 89, 90, 92, 94, 95, 96, 98, 109, 110, 125, 126, 132, 139, 142], "rev": 54, "reveal": 73, "review": [15, 70, 140], "revist": [54, 77], "rf": 83, "rho": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 60, 64, 73, 74, 83, 85, 86, 126, 127, 132, 136, 142], "rho_val": 86, "richter": [56, 95, 137, 139], "riesz": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 85, 126, 127, 128, 130, 131, 132, 135, 136], "riesz_rep": [126, 132], "right": [12, 13, 15, 16, 19, 25, 52, 54, 66, 75, 77, 78, 79, 80, 82, 83, 86, 89, 96, 110, 111, 113, 114, 125, 126, 128, 130, 131, 133, 134], "rightarrow_": [52, 66, 89], "risk": [31, 93, 141], "ritov": 140, "rival": 77, "rival_ind": 77, "rmd": 53, "rmse": [53, 58, 60, 61, 75, 76, 79, 85, 87, 95, 96, 98, 100, 110, 125, 139, 141], "rmse_dml_ml_l_fullsampl": 76, "rmse_dml_ml_l_lesstim": 76, "rmse_dml_ml_l_onfold": 76, "rmse_dml_ml_l_untun": 76, "rmse_dml_ml_m_fullsampl": 76, "rmse_dml_ml_m_lesstim": 76, "rmse_dml_ml_m_onfold": 76, "rmse_dml_ml_m_untun": 76, "rmse_oos_ml_l": 76, "rmse_oos_ml_m": 76, "rmse_oos_onfolds_ml_l": 76, "rmse_oos_onfolds_ml_m": 76, "rnorm": [51, 56, 90, 92, 95, 125, 139], "robin": [7, 8, 17, 54, 70, 77, 89, 137, 140], "robinson": [52, 66, 89], "robject": 77, "robu": [71, 72], "robust": [9, 10, 16, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 53, 60, 64, 73, 74, 83, 85, 86, 96, 126, 132, 140, 142], "robust_confset": [32, 84], "robust_cov": 84, "robust_length": 84, "roc\u00edo": 140, "role": [4, 5, 6, 52, 66, 76, 89, 142], "romano": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 125], "root": [20, 22, 23, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 70, 89, 95, 110, 140], "rotat": [76, 83], "roth": [83, 96, 98, 101, 140], "rough": [86, 142], "roughli": [60, 86], "round": [55, 64, 74, 75, 80, 86], "rout": [42, 43, 46, 47], "row": [21, 52, 55, 59, 62, 67, 68, 76, 77, 81, 90, 92, 96, 100, 109, 139, 142], "row_index": 72, "rownam": 54, "rowv": 54, "roxygen2": 141, "royal": [86, 140], "rpart": [55, 56, 95], "rpart_cv": 56, "rprocess": 75, "rpy2": 77, "rpy2pi": 77, "rskf": 74, "rsmp": [56, 95, 109], "rsmp_tune": [56, 95], "rssb": 86, "rtype": 30, "ruben": 140, "ruiz": [51, 65], "rule": [53, 94], "run": [53, 83, 96, 138, 141], "runif": 51, "runner": [60, 61, 86], "runtime_learn": 56, "rv": [60, 64, 73, 74, 85, 86, 126, 132, 142], "rva": [60, 64, 73, 74, 85, 86, 126, 132, 142], "rvert": 70, "rvert_": 70, "s1": 76, "s2": 76, "s_": [16, 54, 77, 96, 108], "s_1": 17, "s_2": 17, "s_col": [4, 5, 57, 83, 87, 96], "s_i": [19, 57, 83, 87, 96], "s_x": [16, 54, 77], "safeguard": [58, 95], "sake": [55, 78, 86, 142], "same": [14, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 44, 52, 54, 57, 60, 66, 67, 68, 73, 74, 75, 77, 79, 81, 83, 84, 85, 86, 87, 95, 96, 100, 110, 111, 113, 114, 125, 126, 134, 141], "samii": 80, "sampl": [9, 10, 16, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 51, 53, 54, 56, 58, 60, 61, 65, 71, 72, 74, 75, 77, 79, 81, 84, 85, 93, 95, 98, 100, 101, 108, 125, 139, 140, 141], "sample_weight": [40, 42, 43, 46, 47, 83], "sant": [9, 10, 14, 20, 22, 23, 25, 26, 53, 58, 60, 61, 96, 97, 98, 100, 101, 140], "sara": 140, "sasaki": [16, 54, 77, 140], "satisfi": [57, 87, 95, 110, 125], "save": [52, 55, 66, 71, 72, 75, 76, 78, 79, 95, 126, 132, 142], "savefig": 66, "saveguard": 75, "saver": [55, 78, 79], "sc": 60, "scalar": 96, "scale": [25, 52, 54, 59, 69, 74, 80, 82, 86, 125, 126, 136], "scale_color_manu": 52, "scale_fill_manu": [52, 54], "scaled_psi": 74, "scatter": [59, 64, 71, 72, 80, 83, 86], "scatterplot": 64, "scenario": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 64, 73, 74, 85, 86, 96, 126, 132, 142], "scene": [67, 68, 70], "scene_camera": 70, "schacht": [70, 75, 76], "schaefer": 80, "schedul": 141, "scheme": [54, 77, 95, 96, 97, 109, 137], "schneider": 56, "schratz": [56, 95, 137, 139], "scienc": [18, 51, 65, 80, 140], "scikit": [75, 78, 95, 137, 139, 141, 142], "scipi": 66, "score": [0, 4, 5, 6, 9, 10, 20, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 82, 83, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 100, 101, 102, 109, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 130, 131, 132, 134, 135, 136, 137, 141, 142], "scoring_method": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "script": 138, "sd": 51, "se": [52, 54, 66, 85, 89, 95, 109, 125, 126, 132, 140, 142], "se_df": 54, "se_dml": [52, 66, 89], "se_dml_po": [52, 66, 89], "se_nonorth": [52, 66], "se_orth_nosplit": [52, 66], "se_orth_po_nosplit": [52, 66], "seaborn": [21, 24, 58, 60, 62, 64, 66, 75, 77, 78, 79, 86, 87], "search": [20, 22, 23, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 95, 110], "search_mod": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 95], "searchabl": 55, "second": [16, 20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 56, 66, 75, 76, 77, 88, 89, 109, 125, 126, 127, 136, 139], "secondari": 64, "section": [23, 26, 53, 54, 55, 56, 73, 76, 77, 79, 86, 97, 100, 118, 128, 141], "secur": 80, "see": [7, 8, 13, 19, 20, 22, 23, 27, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 46, 47, 51, 53, 54, 55, 56, 58, 60, 61, 64, 65, 67, 68, 72, 74, 76, 77, 79, 80, 81, 83, 84, 85, 86, 95, 96, 98, 101, 109, 110, 116, 118, 119, 120, 123, 124, 126, 127, 131, 132, 136, 138, 139, 141], "seed": [14, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55, 56, 57, 58, 59, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 132, 139, 142], "seek": 80, "seem": [53, 55, 73, 78, 79, 126, 142], "seen": [71, 72, 74], "sel_cols_chiang": 77, "select": [4, 5, 6, 14, 15, 19, 25, 37, 51, 70, 75, 83, 86, 88, 90, 92, 93, 95, 108, 125, 139, 140, 141, 142], "selected_coef": 75, "selected_featur": [56, 95], "selected_learn": 75, "self": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 75, 76, 84, 142], "selfref": 55, "semenova": [67, 68, 140], "semi": 89, "semiparametr": 7, "sens": [85, 86], "sensemakr": [126, 127], "sensit": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 93, 94, 96, 100, 127, 132, 136, 141], "sensitivity_analysi": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 64, 73, 74, 85, 86, 126, 132, 142], "sensitivity_benchmark": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 64, 73, 85, 86, 126, 127], "sensitivity_el": [126, 132], "sensitivity_param": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 85, 86, 126, 127, 132], "sensitivity_plot": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 64, 73, 85, 86, 126, 132], "sensitivity_summari": [60, 64, 73, 74, 85, 86, 126, 132, 142], "sensitv": 74, "sensitvity_benchmark": 64, "sensiv": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39], "senstiv": [126, 135], "sep": 52, "separ": [21, 80, 85, 95, 96, 141], "seper": [76, 83, 85, 109, 125, 126, 127], "seq_len": [52, 57, 89], "sequenti": 8, "ser": [60, 61], "seri": [60, 61, 72, 86, 140], "serv": [25, 90, 91, 92, 139, 141], "serverless": [140, 141], "servic": 80, "set": [4, 5, 6, 7, 8, 9, 11, 16, 17, 18, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 45, 46, 47, 51, 52, 53, 54, 55, 57, 58, 60, 61, 62, 64, 65, 66, 67, 68, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 83, 84, 85, 86, 87, 88, 89, 90, 92, 94, 96, 97, 100, 101, 109, 110, 111, 113, 114, 115, 118, 125, 126, 127, 133, 134, 135, 138, 139, 141, 142], "set_as_param": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "set_config": [42, 43, 46, 47], "set_fit_request": [46, 47], "set_fold_specif": 95, "set_index": 78, "set_ml_nuisance_param": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 55, 62, 78, 95, 141], "set_param": [42, 43, 46, 47, 76, 95], "set_sample_split": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 74, 75, 109, 141], "set_score_request": [42, 43, 46, 47], "set_styl": [78, 79], "set_text": 75, "set_threshold": [52, 53, 54, 55, 56, 57, 88, 95, 96, 109, 110, 125, 139], "set_tick": 77, "set_ticklabel": 77, "set_titl": [64, 74, 76, 77, 83], "set_x_d": [4, 5, 6], "set_xlabel": [64, 66, 74, 76, 77, 83], "set_xlim": 66, "set_xtick": 80, "set_xticklabel": 80, "set_ylabel": [64, 74, 76, 77, 80, 83], "set_ylim": [69, 74, 76, 77, 82], "setdiff": 141, "setdiff1d": 77, "setminu": [54, 77, 125], "settings_l": 76, "settings_m": 76, "setup": [138, 141], "seven": [54, 77], "sever": [48, 55, 56, 60, 61, 75, 76, 78, 79, 85, 86, 89, 95, 142], "shape": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 45, 46, 47, 59, 60, 64, 67, 68, 71, 72, 75, 77, 78, 81, 83, 85, 86, 95, 96], "share": [54, 55, 77, 78], "sharma": [86, 140], "sharp": 40, "shift": 60, "shock": [54, 77], "short": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 85, 86, 96, 100, 126, 127, 140, 141, 142], "shortcut": 55, "shortli": [54, 56, 77, 95], "shota": 140, "should": [4, 5, 6, 20, 21, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 55, 57, 60, 64, 71, 72, 75, 78, 83, 84, 85, 87, 90, 92, 94, 95, 96, 103, 125, 126, 127, 137], "show": [51, 52, 54, 57, 58, 60, 62, 64, 65, 66, 67, 68, 70, 73, 74, 75, 76, 77, 80, 83, 84, 86, 87, 89, 126, 135, 138], "showcas": 81, "showlabel": 86, "showlegend": 86, "shown": [51, 65, 80, 139], "showscal": [67, 68, 70], "shrink": 83, "shuffl": 109, "side": [83, 96, 126, 132], "sigma": [9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 37, 52, 54, 57, 66, 77, 87, 89, 94, 109, 125, 126, 127, 132, 135, 136], "sigma2": [126, 132], "sigma_": [10, 12, 13, 15, 16, 17, 19, 25, 26, 52, 54, 66, 77, 89], "sigma_0": [126, 136], "sigma_j": 125, "sigmoid": 80, "sign": [84, 86], "signal": [44, 45], "signatur": [32, 33, 34, 35, 36, 38, 39, 110], "signif": [51, 53, 54, 55, 56, 57, 95, 96, 109, 110, 125, 139, 142], "signific": [51, 54, 55, 56, 57, 60, 64, 73, 74, 78, 81, 83, 85, 86, 95, 96, 109, 110, 125, 126, 132, 139, 142], "silverman": [34, 35, 36], "sim": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 41, 52, 53, 54, 57, 59, 66, 69, 77, 81, 82, 87, 89, 96], "sim_data": 61, "similar": [10, 14, 53, 56, 67, 68, 73, 76, 79, 83, 84, 85, 86, 96], "similarli": [76, 84], "simpl": [11, 20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 46, 47, 53, 56, 67, 68, 71, 72, 73, 74, 81, 86, 93, 96, 126, 127], "simplest": 94, "simpli": [56, 58, 142], "simplic": [55, 75, 78, 81, 86], "simplif": [126, 128], "simplifi": [60, 74, 80, 86, 94, 126, 135], "simul": [9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 25, 26, 52, 56, 57, 60, 61, 66, 67, 68, 69, 70, 71, 72, 75, 76, 82, 83, 86, 87, 89, 95, 125, 139], "simul_data": 37, "simulaten": [96, 103], "simulation_run": 70, "simult": 53, "simultan": [93, 142], "sin": [11, 14, 18, 59, 67, 68, 71, 72], "sinc": [9, 10, 42, 46, 55, 57, 58, 59, 64, 71, 72, 73, 75, 76, 78, 80, 87, 95, 96, 98, 126, 132, 134, 138, 141], "singl": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 58, 60, 61, 71, 72, 79, 80, 95, 125], "single_learner_pipelin": 95, "singleton": 109, "sinh": 18, "sipp": [55, 78, 79], "site": [60, 61, 77, 78], "situat": [54, 77], "six": [25, 54], "sixth": 77, "size": [21, 24, 37, 52, 54, 55, 56, 59, 60, 61, 66, 69, 70, 73, 75, 76, 78, 80, 81, 82, 84, 86, 88, 90, 92, 95, 96, 97, 109, 110, 125, 139, 142], "sizeabl": 86, "skill": 140, "sklearn": [18, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 45, 46, 47, 60, 61, 62, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 83, 84, 85, 86, 87, 88, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 132, 139, 142], "skotara": 86, "slide": 80, "slight": [96, 100], "slightli": [59, 60, 71, 72, 73, 75, 94, 96, 101, 110, 111, 113, 114, 126, 127], "sligthli": [20, 22, 23], "slow": [52, 66, 89], "slower": [52, 66, 89], "small": [11, 57, 58, 59, 74, 81, 87, 96, 126, 127, 134], "smaller": [55, 58, 71, 72, 73, 76, 78, 83, 86, 96, 142], "smallest": [22, 75], "smpl": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 66, 75, 77, 109, 110], "smpls_cluster": [54, 77], "sn": [58, 60, 62, 64, 66, 75, 77, 78, 79, 86, 87], "so": [46, 47, 51, 55, 56, 57, 58, 60, 65, 76, 78, 80, 86, 87, 95, 125, 142], "social": [80, 140], "societi": [54, 77, 86, 140], "softwar": [56, 95, 137, 139, 140, 141], "solari": 141, "sole": 86, "solut": [88, 94, 110], "solv": [27, 54, 77, 94, 95, 96, 100, 125], "solver": [78, 87, 96], "some": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 55, 56, 57, 58, 59, 62, 75, 76, 78, 79, 83, 84, 85, 87, 94, 95, 96, 104, 138, 141], "sometim": [75, 96, 101], "sonabend": [56, 95], "soon": [96, 99, 110, 112, 126, 129], "sophist": 95, "sort": [21, 78, 84, 96], "sort_bi": 21, "sort_valu": 64, "sourc": [56, 95, 139, 141], "sourcefileload": 70, "sp": 53, "space": [54, 77, 95], "spars": [70, 95, 125, 139, 140], "sparsiti": 140, "spec": 140, "special": [54, 77, 93, 96], "specif": [20, 22, 23, 25, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 40, 54, 55, 60, 61, 64, 74, 75, 77, 78, 86, 90, 92, 93, 94, 95, 96, 100, 109, 110, 118, 125, 132, 136, 137, 139], "specifi": [4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 54, 55, 56, 57, 58, 60, 61, 64, 65, 67, 68, 69, 71, 72, 74, 76, 77, 78, 79, 81, 82, 83, 85, 86, 87, 88, 90, 91, 92, 93, 94, 96, 115, 118, 138, 139, 141, 142], "specifii": 79, "speed": [24, 30, 36, 75], "speedup": 75, "spefici": 32, "spindler": [15, 70, 75, 76, 86, 137, 140, 141], "spine": [78, 79], "spline": [67, 68, 94], "spline_basi": [67, 68, 94], "spline_grid": [67, 68], "split": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 54, 56, 57, 58, 60, 61, 74, 75, 77, 79, 81, 85, 87, 93, 94, 95, 96, 98, 100, 110, 125, 139, 141], "split_sampl": [74, 75], "sponsor": [55, 78, 79], "sprintf": 52, "sq_error": 70, "sqrt": [9, 10, 13, 14, 25, 26, 52, 54, 56, 62, 66, 69, 77, 82, 89, 109, 125, 126, 127, 139], "squar": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 55, 70, 78, 95, 96, 126, 136, 140], "squarederror": [55, 78, 142], "squeez": [58, 69, 82, 87], "src": 78, "ssm": [4, 5, 6, 19, 93, 108], "ssrn": 12, "stabil": 73, "stabl": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 137], "stack": [56, 95], "stackingclassifi": 83, "stackingregressor": 83, "stacklrn": 56, "stackrel": 96, "stage": [40, 67, 68, 71, 72, 81, 83, 95, 96, 141, 142], "stagger": [96, 101], "stai": [96, 101], "standard": [24, 26, 53, 56, 60, 61, 69, 71, 72, 83, 84, 96, 97, 100, 109, 110, 125, 126, 132, 136, 141, 142], "standard_norm": [90, 92, 95, 125, 139], "standardscal": 78, "star": 96, "start": [25, 53, 55, 56, 60, 61, 67, 68, 70, 73, 75, 76, 77, 78, 82, 86, 96, 98, 137, 142], "start_dat": 25, "stat": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 66, 83, 90, 92, 95, 96, 125, 137, 140], "stat_bin": 52, "stat_dens": 55, "state": 142, "stationar": 58, "stationari": [96, 98], "statist": [16, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 54, 77, 84, 85, 86, 125, 126, 132, 137, 139, 140, 141, 142], "statsmodel": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 83], "statu": [53, 55, 57, 58, 78, 80, 83, 87, 96, 101], "std": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 64, 65, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 80, 82, 85, 86, 87, 94, 95, 96, 97, 98, 100, 109, 110, 125, 139, 142], "stefan": 140, "step": [52, 55, 56, 66, 71, 72, 73, 78, 81, 89, 95, 96, 125, 137, 142], "stepdown": 125, "stick": [55, 78], "still": [57, 58, 67, 68, 71, 72, 73, 79, 83, 85, 87, 95, 96, 100], "stochast": [38, 39, 96, 106, 107, 139], "stock": [55, 78, 79, 84], "store": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 84, 88, 95, 109, 110, 125, 126, 132, 141], "store_model": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 76], "store_predict": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 78, 81], "stori": [86, 140], "str": [4, 5, 6, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 55, 60, 64, 71, 72, 82, 83, 94, 96, 141], "straightforward": [71, 72, 75, 94], "strategi": [80, 86, 96, 142], "stratifi": [74, 75], "stratum": 80, "strength": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 84, 85, 86, 126, 127, 132, 135], "strftime": 60, "strictli": 96, "string": [20, 21, 22, 23, 24, 29, 31, 32, 33, 34, 35, 37, 38, 39, 94, 125, 126, 132, 139, 141], "string_label": 80, "strong": [57, 84, 87, 126, 127], "stronger": [84, 125, 142], "structur": [7, 8, 17, 25, 54, 55, 57, 70, 77, 78, 84, 87, 89, 95, 137, 140, 142], "student": 140, "studi": [19, 54, 55, 70, 75, 76, 77, 78, 79, 84, 85, 96, 97, 139, 142], "style": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 76, 141], "styler": 141, "styliz": 86, "sub": [42, 43, 46, 47, 54, 77], "subclass": [91, 92, 141], "subfold": 95, "subgroup": [32, 55, 78, 141], "subject": [54, 77], "submiss": 141, "submit": 60, "subobject": [42, 43, 46, 47], "subplot": [54, 59, 64, 66, 67, 68, 69, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83], "subplots_adjust": 75, "subpopul": [96, 108], "subsampl": [56, 75], "subscript": [96, 100, 110, 114, 126, 127], "subsequ": [54, 77], "subset": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 46, 54, 75, 77, 81, 88, 94, 95, 126, 127], "subseteq": 94, "substanti": [55, 78, 80], "substract": 125, "subtract": 125, "sudo": 138, "suffic": 86, "suffici": [75, 76, 86], "suggest": [54, 55, 77, 78, 86, 141], "suitabl": [57, 67, 68, 87, 96, 100], "sum": [43, 47, 54, 55, 77, 78, 79, 82, 83, 94, 125], "sum_": [25, 41, 52, 54, 66, 77, 83, 88, 89, 94, 96, 97, 101, 125], "sum_i": 80, "sum_oth": 77, "sum_riv": 77, "summar": [21, 53, 60, 61, 64, 80, 86, 88, 126, 132], "summari": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 53, 54, 56, 57, 58, 60, 61, 62, 64, 65, 67, 68, 69, 71, 72, 73, 74, 77, 79, 82, 84, 85, 86, 87, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 109, 110, 125, 126, 139, 141, 142], "summary_df": 84, "summary_result": 55, "suppli": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 67, 68, 71, 72, 73, 81, 94, 126, 127, 132, 133], "support": [11, 32, 40, 53, 54, 60, 61, 75, 77, 81, 83, 92, 95, 96, 104, 142], "support_s": [11, 67, 68, 71, 72, 81], "support_t": 81, "support_w": 81, "suppos": 86, "suppress": [53, 55, 56, 57], "suppresswarn": 52, "suprema": 125, "suptitl": [69, 75, 76, 79, 82], "supxlabel": [69, 79, 82], "supylabel": [69, 79, 82], "sure": [64, 95, 141], "surfac": [67, 68, 70], "surgic": 84, "surpress": [54, 139], "survei": [55, 78, 79, 142], "susan": 140, "sven": [86, 137, 140], "svenk": 77, "svenklaassen": [137, 141], "svg": [52, 66], "switch": [52, 66, 86, 89], "symbol": 86, "symmetr": 18, "syntax": [83, 96], "synthesi": 140, "synthet": [11, 25, 41, 51, 65, 67, 68, 69, 71, 72, 76, 81, 82, 84], "syrgkani": [86, 140], "system": 140, "szita": 140, "t": [4, 5, 6, 9, 10, 14, 20, 21, 23, 24, 25, 26, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 91, 92, 94, 95, 96, 97, 98, 100, 101, 109, 110, 111, 125, 126, 128, 139, 142], "t_": [24, 60, 61, 96, 100, 110, 114, 126, 131], "t_1_start": 75, "t_1_stop": 75, "t_2_start": 75, "t_2_stop": 75, "t_3_start": 75, "t_3_stop": 75, "t_col": [4, 5, 6, 23, 24, 60, 61, 91, 92, 96, 97, 98, 100], "t_df": 81, "t_diff": 59, "t_dml": 52, "t_g": 25, "t_i": [58, 81, 83, 96, 98], "t_idx": 59, "t_nonorth": 52, "t_orth_nosplit": 52, "t_sigmoid": 81, "t_stat": 125, "t_value_ev": 22, "t_value_pr": 22, "tabl": [52, 54, 55, 56, 57, 64, 84, 88, 90, 92, 95, 96, 109, 110, 125, 139, 142], "tabular": [75, 90, 92, 125, 139, 142], "taddi": 140, "takatsu": 84, "take": [9, 10, 11, 32, 33, 38, 39, 57, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 75, 79, 82, 83, 84, 85, 87, 88, 94, 95, 96, 97, 101, 104, 105, 106, 107, 110, 115, 118, 126, 133, 134, 135, 139], "taken": [55, 78, 79, 142], "taker": [32, 141], "talk": 142, "target": [20, 22, 23, 27, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 46, 47, 51, 54, 55, 56, 57, 67, 68, 75, 77, 94, 95, 96, 98, 100, 101, 102, 103, 104, 105, 108, 109, 110, 119, 120, 125, 126, 134, 136, 137, 139, 141, 142], "task": [51, 76, 90, 92, 109, 142], "task_typ": 141, "tau": [41, 59, 69, 79, 80, 82, 83, 94, 96, 110, 116, 119, 120], "tau_": [80, 83, 96], "tau_0": [83, 96], "tau_1": 80, "tau_2": 80, "tau_vec": [69, 79, 82], "tax": [55, 78, 79], "te": [53, 67, 68, 81], "techniqu": [52, 66, 89, 109, 142], "templat": 141, "ten": 76, "tend": [55, 78, 79, 96], "tensor": [67, 68], "tenth": 140, "term": [6, 22, 52, 54, 55, 56, 59, 66, 70, 77, 78, 80, 86, 89, 96, 101, 137, 142], "termin": [56, 95], "terminatorev": 56, "test": [8, 12, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 60, 61, 66, 73, 77, 84, 86, 89, 95, 96, 109, 110, 125, 139, 140, 141, 142], "test_id": [54, 109], "test_ind": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "test_set": 109, "test_siz": 66, "text": [9, 10, 12, 14, 24, 25, 26, 40, 41, 54, 55, 60, 61, 69, 70, 80, 81, 82, 83, 86, 94, 96, 100, 101, 109, 110, 114, 126, 131], "textbf": [88, 95, 142], "textposit": 86, "textrm": [126, 127, 133, 134, 135, 136], "tg": [56, 62, 90, 92, 139], "th": [54, 77], "than": [33, 52, 53, 55, 66, 70, 74, 75, 78, 79, 80, 83, 84, 85, 86, 89, 96, 100, 126, 132, 142], "thank": [53, 55, 56, 78, 141], "thatw": 59, "thei": [53, 55, 59, 71, 72, 78, 80, 84, 96, 126, 136], "them": [21, 55, 56, 67, 68, 69, 73, 76, 78, 82, 96], "theme": [54, 55], "theme_minim": [52, 55, 57], "theorem": [96, 101, 126, 136], "theoret": [75, 86, 109, 140], "theori": [94, 140], "therebi": [54, 56, 77, 142], "therefor": [61, 64, 80, 83, 85, 109, 110, 126, 135], "theta": [9, 10, 12, 13, 14, 16, 18, 19, 20, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 56, 57, 58, 59, 60, 64, 66, 70, 73, 74, 75, 77, 83, 85, 86, 87, 88, 89, 90, 92, 94, 95, 96, 97, 100, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 132, 135, 136, 139, 142], "theta_": [25, 60, 64, 83, 86, 94, 96, 102, 103, 125, 126, 136], "theta_0": [11, 32, 33, 38, 39, 52, 54, 55, 57, 64, 66, 67, 68, 70, 71, 72, 77, 78, 86, 87, 89, 94, 96, 98, 104, 105, 106, 107, 108, 110, 119, 120, 125, 126, 133, 134, 136, 139], "theta_dml": [52, 66, 89], "theta_dml_po": [52, 66, 89], "theta_initi": 66, "theta_nonorth": [52, 66], "theta_orth_nosplit": [52, 66], "theta_orth_po_nosplit": [52, 66], "theta_resc": 52, "theta_t": 59, "thi": [9, 10, 20, 21, 22, 23, 24, 25, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 94, 95, 96, 100, 101, 102, 103, 104, 105, 109, 110, 111, 113, 114, 116, 118, 119, 120, 125, 126, 127, 132, 133, 134, 137, 138, 139, 140, 141, 142], "think": 56, "third": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 61, 66, 77, 89, 109], "thirion": [137, 139], "this_df": [70, 78], "this_split_ind": 77, "those": [53, 55, 78, 79, 84], "though": [51, 65, 80], "thread": [80, 95], "three": [54, 56, 71, 72, 138, 141], "threshold": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 83, 86, 96], "through": [53, 69, 71, 72, 82, 83, 95, 96], "throughout": [73, 84], "thu": [76, 83, 94, 96], "tibbl": 53, "tick": 24, "tick_param": 83, "tight": 66, "tight_layout": [60, 76, 77, 83], "tighter": 83, "tild": [9, 10, 14, 25, 26, 54, 77, 80, 88, 94, 109, 110, 114, 119, 120, 123, 124, 125, 126, 135, 136], "tile": 84, "time": [4, 5, 6, 15, 16, 20, 22, 24, 25, 52, 53, 54, 55, 57, 58, 59, 66, 70, 71, 72, 77, 78, 79, 83, 85, 86, 87, 91, 92, 96, 97, 98, 100, 101, 110, 126, 140, 141, 142], "time_budget": 76, "time_df": 59, "time_period": 59, "time_typ": [25, 60], "titiunik": [96, 140], "titl": [21, 24, 54, 55, 57, 60, 64, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 82, 83, 86, 137], "title_fonts": 60, "tmp": 72, "tname": 53, "tnr": [56, 95], "to_datetim": 60, "to_fram": 81, "to_numpi": [69, 73, 79, 82], "todo": [54, 62], "toeplitz": 70, "togeth": [71, 72, 125], "toler": 77, "tomasz": [140, 141], "toml": 141, "too": 75, "tool": [53, 56, 85, 142], "top": [54, 75, 77, 78, 79, 83, 86, 96, 137], "total": [43, 47, 55, 76, 78], "tpot": 76, "tracker": 137, "tradit": 125, "train": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 46, 47, 52, 54, 56, 66, 67, 68, 69, 71, 72, 74, 75, 77, 78, 81, 82, 88, 89, 109], "train_id": [54, 109], "train_ind": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "train_set": 109, "train_test_split": 66, "transact": 140, "transform": [9, 10, 41, 74, 80, 86, 142], "translat": 70, "transpos": 59, "treament": 81, "treat": [24, 25, 26, 33, 53, 58, 59, 60, 61, 64, 73, 81, 83, 86, 94, 96, 98, 100, 101, 105, 110, 114, 125, 142], "treat1_param": 80, "treat2_param": 80, "treat_var": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 95], "treatment": [4, 5, 6, 7, 8, 9, 10, 11, 14, 17, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 53, 54, 56, 57, 58, 59, 60, 61, 62, 64, 65, 70, 73, 75, 76, 77, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 95, 97, 98, 100, 101, 102, 103, 104, 105, 108, 109, 111, 113, 114, 115, 116, 118, 119, 120, 125, 131, 132, 133, 135, 137, 139, 140, 141, 142], "treatment_df": 59, "treatment_effect": [11, 67, 68], "treatment_level": [29, 30, 64, 74, 96], "treatment_var": [4, 5, 6], "tree": [33, 45, 55, 56, 58, 59, 60, 75, 78, 88, 93, 95, 96, 109, 110, 125, 139, 141], "tree_param": [33, 45], "tree_summari": 78, "trees_class": [55, 78], "trend": [53, 58, 59, 60, 77, 96, 98, 100, 101, 140], "tri": [70, 126, 127], "triangular": [40, 83, 96], "trim": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 55, 78, 79, 86], "trimming_rul": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 79], "trimming_threshold": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 55, 67, 74, 78, 79, 81, 82, 86], "trm": [56, 95], "true": [4, 5, 6, 7, 8, 9, 14, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 92, 95, 96, 98, 109, 110, 115, 116, 119, 120, 123, 124, 125, 126, 128, 130, 131, 136, 139, 142], "true_effect": [59, 67, 68, 71, 72, 84], "true_gatet_effect": 73, "true_group_effect": 73, "true_tau": 83, "truemfunct": 84, "truncat": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 79], "try": [75, 85], "tune": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 70, 75, 83, 93, 96, 137, 139, 141], "tune_on_fold": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 95], "tune_r": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "tune_set": [56, 95], "tuned_model": 76, "tuner": 95, "tunergridsearch": 56, "tupl": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 60, 96, 100], "turn": 86, "turrel": 18, "tutori": 55, "tw": [78, 79], "twice": 96, "twinx": 64, "two": [11, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 55, 56, 58, 60, 61, 65, 66, 69, 74, 75, 76, 78, 79, 80, 81, 82, 84, 85, 86, 88, 89, 94, 95, 98, 100, 109, 119, 125, 142], "twoclass": 56, "twoearn": [55, 78, 79, 85, 142], "type": [6, 9, 10, 11, 14, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 55, 56, 60, 66, 75, 76, 77, 83, 86, 89, 93, 95, 96, 110, 121, 122, 125, 126, 135, 141, 142], "typeerror": [60, 61], "typic": [72, 96, 101, 137], "u": [9, 10, 11, 13, 19, 26, 32, 33, 34, 35, 36, 43, 47, 52, 53, 54, 55, 58, 59, 60, 64, 66, 69, 71, 72, 74, 75, 77, 78, 79, 81, 82, 84, 85, 86, 89, 96, 102, 104, 105, 126, 127, 138, 142], "u_hat": [52, 66, 110], "u_i": [12, 15, 18, 19], "u_t": 26, "uehara": 140, "uhash": 56, "ulf": 140, "unambigu": 86, "uncertainti": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 71, 72, 74, 83, 85, 126, 132, 142], "unchang": [42, 43, 46, 47], "uncondit": [55, 60, 78, 142], "unconfounded": [86, 140], "under": [32, 37, 52, 55, 58, 66, 78, 81, 83, 86, 89, 96, 100, 101, 108, 125, 140], "underbrac": [52, 59, 66, 89, 94], "underfit": 76, "underli": [9, 14, 55, 56, 60, 64, 71, 72, 80, 81, 96, 101, 126, 127, 142], "underlin": [54, 77], "underset": [83, 96], "understand": [60, 86], "undesir": 95, "unevenli": 109, "uniform": [26, 40, 41, 59, 65, 67, 68, 69, 81, 82, 125], "uniform_averag": [43, 47], "uniformli": [32, 60, 69, 79, 125], "union": 32, "uniqu": [6, 51, 60, 61, 64, 65, 75, 83, 91, 92, 110, 126, 136], "unique_label": 76, "unit": [6, 25, 52, 53, 57, 58, 59, 60, 61, 73, 83, 87, 91, 92, 96, 98, 100, 101, 110, 111, 113, 114, 126, 131, 141], "univari": [11, 67, 68], "univers": 140, "unknown": 96, "unlik": [55, 78, 79, 86], "unobserv": [9, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 51, 55, 60, 65, 78, 79, 85, 86, 96, 126, 127, 136, 142], "unpen": 53, "unstabl": [126, 127], "unter": [54, 55, 56], "untest": 86, "until": [96, 98, 141], "untreat": [86, 96, 98], "up": [24, 30, 36, 55, 70, 75, 76, 78, 79, 85, 86, 95, 96, 98, 109, 126, 127, 138, 141, 142], "upcom": 141, "updat": [42, 43, 46, 47, 54, 72, 77, 78, 140, 141], "update_layout": [67, 68, 70, 83, 86], "update_trac": [67, 68], "upload": 141, "upon": [110, 141], "upper": [32, 55, 56, 59, 60, 64, 66, 69, 73, 74, 79, 82, 83, 85, 86, 95, 126, 132, 136, 142], "upper_bound": [67, 68], "upsilon": [57, 87], "upsilon_i": [57, 87], "upward": [55, 78, 79, 86], "upweight": 80, "url": [70, 137, 140], "us": [4, 5, 6, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 52, 54, 55, 57, 58, 59, 60, 61, 64, 66, 67, 68, 69, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 92, 94, 96, 100, 109, 110, 111, 113, 114, 125, 126, 127, 132, 134, 135, 136, 137, 138, 139, 141, 142], "usa": 140, "usabl": 75, "usag": [53, 58, 60, 61, 62, 64, 73, 77, 78, 79, 85, 87, 90, 91, 92, 139, 141], "use_label_encod": [78, 142], "use_other_treat_as_covari": [4, 5, 6, 90, 92], "use_pred_offset": 95, "usecolormap": [67, 68], "user": [27, 28, 42, 43, 46, 47, 52, 53, 54, 55, 56, 64, 66, 73, 74, 75, 77, 78, 83, 85, 94, 95, 96, 110, 125, 137, 138, 139, 141, 142], "user_guid": 72, "userwarn": [60, 61, 78, 86], "usual": [54, 58, 60, 61, 67, 68, 75, 77, 83, 85, 86, 94, 95, 109, 126, 136], "util": [0, 28, 74, 75, 76, 80, 83, 95, 96, 141], "v": [7, 8, 13, 15, 16, 17, 19, 32, 33, 38, 39, 43, 47, 52, 54, 55, 60, 64, 66, 73, 74, 75, 76, 77, 78, 80, 83, 84, 88, 89, 94, 96, 102, 104, 105, 106, 107, 125, 137, 139, 140, 141, 142], "v108": 137, "v12": [137, 139], "v22": 56, "v23": 137, "v_": [16, 54, 77, 96], "v_i": [12, 13, 17, 18, 19, 52, 66, 89, 96], "v_j": 125, "val": [13, 60, 61, 109, 140], "val_list": 70, "valid": [4, 5, 6, 12, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 53, 54, 55, 58, 66, 69, 75, 76, 77, 78, 79, 82, 89, 93, 94, 95, 109, 110, 116, 119, 120, 126, 127, 140, 142], "valu": [4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 46, 47, 48, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 83, 84, 85, 86, 88, 91, 92, 93, 95, 96, 97, 101, 102, 108, 109, 116, 119, 120, 123, 124, 125, 126, 127, 132, 136, 139, 141, 142], "value_count": 78, "van": 140, "vanderpla": [137, 139], "vanish": [52, 66, 89], "var": [9, 10, 14, 25, 26, 54, 77, 80, 83, 126, 127, 133, 134, 135, 136], "var_ep": 86, "varepsilon": [9, 10, 16, 32, 54, 57, 77, 87, 94, 96, 104], "varepsilon_": [16, 25, 54, 60, 77], "varepsilon_0": 26, "varepsilon_1": 26, "varepsilon_d": [10, 14], "varepsilon_i": [14, 15, 57, 69, 82, 87], "vari": [25, 55, 59, 60, 75, 78, 80, 86], "variabl": [4, 5, 6, 7, 10, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 54, 55, 56, 57, 58, 60, 61, 62, 64, 70, 73, 76, 77, 78, 79, 83, 85, 86, 87, 90, 91, 92, 94, 95, 96, 98, 100, 101, 102, 104, 105, 106, 107, 109, 110, 125, 126, 127, 132, 136, 139, 140, 141, 142], "varianc": [27, 28, 54, 56, 77, 83, 85, 86, 93, 96, 109, 126, 127, 132, 134, 135, 136, 139], "variant": [53, 74], "variat": [10, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 74, 85, 126, 127, 136], "variou": [53, 76, 86, 95, 142], "varoquaux": [137, 139], "vasili": [86, 140], "vector": [11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 25, 29, 31, 32, 33, 34, 35, 37, 38, 39, 51, 54, 55, 57, 58, 65, 71, 72, 73, 77, 78, 81, 84, 87, 96, 98, 106, 107, 108, 125, 139, 141], "vee": [110, 114, 126, 131], "venv": 138, "verbos": [55, 59, 60, 66, 75, 76, 83, 86], "veri": [53, 54, 56, 73, 75, 77, 84, 86, 110, 137], "verifi": 80, "versa": [75, 80, 126, 132], "version": [9, 42, 43, 46, 47, 54, 55, 56, 86, 88, 94, 96, 100, 110, 125, 126, 128, 130, 131, 133, 134, 141], "versoin": 86, "versu": 72, "vertic": [54, 64, 77], "via": [9, 10, 20, 22, 23, 26, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 53, 57, 58, 59, 60, 61, 69, 70, 71, 72, 73, 74, 75, 83, 85, 87, 88, 90, 92, 93, 94, 95, 96, 97, 98, 100, 109, 116, 124, 125, 126, 127, 132, 136, 137, 138, 139, 140, 141, 142], "viabl": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "vice": [75, 80, 126, 132], "victor": [70, 86, 109, 137, 140], "view": 72, "vignett": [53, 141], "villa": [51, 65], "violat": 60, "violet": [69, 79, 82], "vira": 140, "virtual": 138, "virtualenv": 138, "visibl": [79, 83, 86], "visit": [137, 142], "visual": [21, 54, 60, 61, 73, 74, 76, 77, 83], "vol": 53, "volum": [86, 137], "voluntari": 80, "vv740": 77, "vv760g": 77, "w": [7, 8, 9, 10, 17, 25, 26, 27, 28, 42, 43, 46, 47, 54, 70, 77, 80, 81, 84, 88, 89, 96, 100, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 130, 131, 132, 133, 134, 135, 136, 137, 139], "w24678": 109, "w30302": 140, "w_": [25, 26, 54, 77, 81, 96], "w_1": [25, 26, 81], "w_2": [25, 26, 81], "w_3": [25, 26], "w_4": [25, 26], "w_df": 81, "w_i": [19, 58, 81, 83, 88, 94, 96, 109, 110, 114, 125], "wa": [54, 59, 76, 77, 86, 141], "wager": 140, "wai": [55, 75, 76, 78, 84, 86, 95, 110, 138], "wander": 18, "wang": 140, "want": [51, 54, 55, 56, 58, 65, 69, 75, 77, 82, 83, 95, 96, 137, 138, 140], "warn": [21, 24, 51, 52, 53, 54, 55, 56, 57, 60, 61, 66, 78, 86, 88, 95, 96, 109, 110, 125, 139, 141], "wayon": 54, "we": [33, 45, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 94, 95, 96, 97, 100, 101, 105, 109, 110, 114, 117, 125, 126, 127, 136, 138, 139, 141, 142], "weak": [32, 126, 127, 140], "weakest": 60, "wealth": [7, 85], "websit": [55, 56, 95, 137], "wedg": [54, 77], "week": 141, "wei": 125, "weight": [21, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 42, 43, 46, 47, 54, 55, 56, 57, 60, 61, 64, 73, 74, 77, 78, 83, 87, 93, 95, 96, 97, 110, 115, 118, 125, 126, 133, 134, 141], "weights_bar": [29, 33, 74], "weights_dict": 74, "weiss": [137, 139], "well": [4, 5, 6, 46, 47, 52, 54, 66, 70, 75, 76, 77, 84, 88, 89, 92, 109, 138, 139], "were": [55, 57, 78, 79, 87, 142], "what": [53, 70, 75, 140], "when": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 55, 58, 72, 74, 78, 80, 84, 96, 105, 108, 110, 125, 137, 138, 139, 141], "whenev": [55, 78], "whera": [126, 134], "where": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 43, 44, 45, 47, 51, 52, 54, 55, 57, 58, 59, 60, 61, 64, 65, 66, 69, 73, 77, 78, 80, 81, 82, 83, 84, 86, 87, 88, 89, 94, 95, 96, 97, 98, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 132, 133, 134, 136, 138, 139, 141, 142], "wherea": [11, 57, 58, 60, 64, 84, 86, 87, 96, 100, 110, 118, 126, 133, 142], "whether": [4, 5, 6, 11, 14, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 55, 59, 75, 78, 79, 83, 84, 86, 90, 92, 95, 96, 126, 127, 141], "which": [4, 5, 6, 9, 11, 20, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 46, 51, 52, 53, 55, 56, 57, 58, 60, 61, 63, 64, 65, 66, 70, 72, 73, 75, 76, 78, 79, 81, 83, 85, 86, 87, 89, 90, 92, 94, 95, 96, 100, 110, 125, 126, 127, 132, 133, 134, 136, 138, 141, 142], "while": [51, 65, 96], "white": [54, 71, 72, 77, 86], "whitegrid": [78, 79], "whitnei": [86, 140], "who": [53, 55, 78, 86], "whole": [52, 58, 66, 83, 89, 95, 126, 127], "whom": 96, "widehat": [60, 61, 96], "width": [21, 24, 52, 54, 67, 68, 70], "wiki": 141, "wiksel": 140, "wild": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 125], "window": 138, "wise": [71, 72], "wish": 138, "within": [40, 54, 60, 61, 71, 72, 77, 81, 83], "without": [14, 20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 40, 51, 52, 60, 61, 65, 66, 75, 76, 86, 89, 93, 95, 96, 126, 127, 138, 141], "wolf": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 125], "won": 86, "word": [40, 83, 96, 141, 142], "work": [42, 43, 46, 47, 60, 61, 63, 64, 72, 73, 75, 80, 85, 86, 95, 96, 125, 138, 140], "workflow": [137, 141], "workspac": 78, "world": 140, "worri": 86, "wors": [43, 47], "would": [43, 47, 53, 55, 56, 60, 61, 67, 68, 70, 75, 78, 79, 83, 85, 86, 94, 95, 126, 136, 142], "wrapper": [53, 83, 95], "wright": 84, "write": [52, 53, 57, 58, 66, 72, 87, 89, 126, 136], "written": [96, 110, 126, 133, 134], "wrong": [75, 80], "wspace": 75, "wurd": [54, 55, 56], "www": [137, 138], "x": [4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 31, 32, 33, 35, 37, 38, 39, 40, 41, 42, 43, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 92, 94, 95, 96, 98, 100, 102, 103, 104, 105, 106, 107, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 130, 131, 133, 134, 135, 136, 139, 142], "x0": [64, 80, 83], "x1": [54, 56, 57, 58, 64, 74, 76, 77, 80, 83, 85, 86, 87, 90, 92, 94, 95, 96, 110, 125, 126, 127, 139], "x10": [54, 56, 57, 74, 76, 77, 87, 90, 92, 95, 96, 110, 125, 139], "x100": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x11": [54, 56, 57, 77, 87, 90, 92, 95, 96, 110, 125, 139], "x12": [54, 56, 57, 77, 87, 90, 92, 95, 96, 110, 125, 139], "x13": [54, 56, 57, 77, 87, 90, 92, 95, 96, 110, 125, 139], "x14": [54, 56, 57, 77, 87, 90, 92, 95, 96, 110, 125, 139], "x15": [54, 56, 57, 77, 87, 90, 92, 95, 96, 110, 125, 139], "x16": [54, 56, 57, 77, 87, 90, 92, 95, 96, 110, 125, 139], "x17": [54, 56, 57, 77, 87, 90, 92, 95, 96, 110, 125, 139], "x18": [54, 56, 57, 77, 87, 90, 92, 95, 96, 110, 125, 139], "x19": [54, 56, 57, 77, 87, 90, 92, 95, 96, 110, 125, 139], "x1x2x3x4x5x6x7x8x9x10": 54, "x2": [54, 56, 57, 58, 64, 74, 76, 77, 83, 85, 86, 87, 90, 92, 94, 95, 96, 110, 125, 139], "x20": [54, 56, 57, 77, 87, 90, 92, 95, 96, 110, 125, 139], "x21": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x22": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x23": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x24": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x25": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x26": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x27": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x28": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x29": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x2_dummi": 86, "x2_preds_control": 86, "x2_preds_treat": 86, "x3": [54, 56, 57, 58, 64, 74, 76, 77, 85, 86, 87, 90, 92, 94, 95, 96, 110, 125, 139], "x30": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x31": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x32": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x33": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x34": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x35": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x36": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x37": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x38": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x39": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x4": [54, 56, 57, 58, 64, 74, 76, 77, 85, 86, 87, 90, 92, 95, 96, 110, 125, 139], "x40": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x41": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x42": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x43": [54, 56, 57, 76, 77, 87, 90, 92, 96, 139], "x44": [54, 56, 57, 76, 77, 87, 90, 92, 96, 139], "x45": [54, 56, 57, 76, 77, 87, 90, 92, 96, 139], "x46": [54, 56, 57, 76, 77, 87, 90, 92, 96, 139], "x47": [54, 56, 57, 76, 77, 87, 90, 92, 96, 139], "x48": [54, 56, 57, 76, 77, 87, 90, 92, 96, 139], "x49": [54, 56, 57, 76, 77, 87, 90, 92, 96, 139], "x5": [54, 56, 57, 74, 76, 77, 86, 87, 90, 92, 95, 96, 110, 125, 139], "x50": [54, 56, 57, 76, 77, 87, 90, 92, 96, 139], "x51": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x52": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x53": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x54": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x55": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x56": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x57": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x58": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x59": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x6": [54, 56, 57, 74, 76, 77, 87, 90, 92, 95, 96, 110, 125, 139], "x60": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x61": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x62": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x63": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x64": [54, 56, 57, 60, 61, 77, 78, 87, 90, 92, 96, 139], "x65": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x66": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x67": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x68": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x69": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x7": [54, 56, 57, 74, 76, 77, 87, 90, 92, 95, 96, 110, 125, 139], "x70": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x71": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x72": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x73": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x74": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x75": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x76": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x77": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x78": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x79": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x8": [54, 56, 57, 74, 76, 77, 87, 90, 92, 95, 96, 110, 125, 139], "x80": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x81": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x82": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x83": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x84": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x85": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x86": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x87": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x88": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x89": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x9": [54, 56, 57, 74, 76, 77, 87, 90, 92, 95, 96, 110, 125, 139], "x90": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x91": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x92": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x93": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x94": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x95": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x96": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x96x97x98x99x100ydcluster_var_icluster_var_jz": 54, "x97": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x98": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x99": [54, 56, 57, 77, 87, 90, 92, 96, 139], "x_": [16, 17, 25, 52, 54, 59, 66, 77, 86, 89], "x_0": [59, 67, 68, 71, 72, 73], "x_1": [9, 10, 14, 25, 26, 38, 39, 59, 67, 68, 69, 71, 72, 73, 82, 86, 96, 106, 107, 126, 127, 139], "x_1x_3": [69, 82], "x_2": [9, 10, 14, 25, 26, 59, 67, 68, 69, 71, 72, 73, 82, 86, 126, 127], "x_3": [9, 10, 14, 25, 26, 59, 67, 68, 71, 72, 73, 126, 127], "x_4": [9, 10, 14, 25, 26, 67, 68, 69, 71, 72, 73, 82], "x_5": [9, 10, 14, 67, 68, 71, 72], "x_6": [67, 68, 71, 72], "x_7": [67, 68, 71, 72], "x_8": [67, 68, 71, 72], "x_9": [67, 68, 71, 72], "x_binary_control": 86, "x_binary_tr": 86, "x_col": [4, 5, 6, 24, 51, 54, 55, 56, 60, 61, 65, 70, 77, 78, 79, 81, 83, 84, 85, 86, 90, 91, 92, 95, 96, 97, 100, 139, 141, 142], "x_cols_bench": 86, "x_cols_binari": 86, "x_cols_poli": 77, "x_conf": 82, "x_conf_tru": 82, "x_df": 59, "x_domain": 56, "x_i": [11, 12, 13, 15, 17, 18, 19, 41, 52, 57, 58, 66, 69, 71, 72, 80, 82, 83, 87, 89, 94, 96, 98, 100, 101, 108, 110, 114], "x_p": [38, 39, 96, 106, 107, 139], "x_train": 76, "x_true": [69, 82], "x_var": 56, "xaxis_titl": [67, 68, 70, 83, 86], "xformla": 53, "xgb": 76, "xgb_untuned_l": 76, "xgb_untuned_m": 76, "xgbclassifi": [75, 78, 80, 142], "xgboost": [52, 55, 75, 78, 80, 142], "xgbregressor": [75, 76, 78, 80, 142], "xi": [14, 25, 26, 96], "xi_": 125, "xi_0": [16, 54, 77], "xi_i": [57, 87], "xiaoji": 140, "xintercept": [52, 57], "xlab": [52, 54, 55], "xlabel": [59, 60, 64, 67, 68, 69, 71, 72, 76, 78, 79, 82], "xlim": [52, 55], "xtick": [64, 76], "xval": [56, 95], "xx": 66, "y": [4, 5, 6, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 102, 104, 105, 106, 107, 109, 110, 111, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 132, 133, 134, 135, 136, 139, 142], "y0": [53, 60, 64, 69, 82], "y0_cvar": 69, "y0_quant": [69, 82], "y1": [53, 60, 69, 82], "y1_cvar": 69, "y1_quant": [69, 82], "y_": [16, 24, 25, 54, 57, 58, 59, 60, 77, 87, 96, 98, 100, 101, 108, 110, 114], "y_0": [20, 22, 26, 41, 110, 113], "y_1": [20, 22, 26, 41, 110, 113], "y_col": [4, 5, 6, 24, 51, 52, 54, 55, 56, 57, 60, 61, 65, 67, 68, 70, 71, 72, 74, 77, 78, 79, 81, 83, 84, 85, 88, 89, 90, 91, 92, 95, 96, 97, 100, 109, 110, 139, 141, 142], "y_df": [59, 81], "y_diff": 59, "y_i": [11, 12, 13, 15, 17, 18, 19, 52, 57, 58, 66, 69, 80, 81, 82, 83, 87, 89, 96, 98, 108], "y_label": [21, 24], "y_lower_quantil": 60, "y_mean": 60, "y_pred": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 75, 95], "y_train": 76, "y_true": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 75, 95], "y_upper_quantil": 60, "ya": 140, "yasui": 140, "yata": 140, "yaxis_titl": [67, 68, 70, 83, 86], "year": 137, "yerr": [59, 64, 71, 72, 76, 78, 80, 83], "yet": [54, 60, 61, 63, 96, 100, 101], "yggvpl": 77, "yield": 96, "yintercept": 55, "ylab": [52, 54, 55], "ylabel": [59, 60, 64, 67, 68, 69, 71, 72, 76, 78, 79, 82], "ylim": 78, "ymax": 55, "ymin": 55, "yname": 53, "york": 140, "you": [42, 43, 46, 47, 51, 52, 59, 60, 61, 65, 72, 77, 85, 96, 137, 138, 142], "your": [75, 138], "ython": 137, "yukun": 140, "yusuk": 140, "yuya": 140, "yy": 66, "z": [4, 5, 6, 9, 10, 12, 14, 15, 16, 19, 25, 26, 32, 34, 37, 38, 51, 54, 55, 57, 60, 65, 67, 68, 70, 77, 78, 82, 84, 86, 87, 94, 96, 104, 106, 110, 117, 119, 121, 124, 125, 141], "z1": [6, 24, 38, 60, 91, 92, 96, 97, 98, 100], "z2": [6, 24, 60, 91, 92, 96, 97, 98, 100], "z3": [6, 24, 60, 91, 92, 96, 97, 98, 100], "z4": [6, 24, 60, 91, 92, 96, 97, 98, 100], "z_": [16, 54, 77], "z_1": [9, 10, 14, 60], "z_2": [9, 10, 14], "z_3": [9, 10, 14], "z_4": [9, 10, 14, 60], "z_5": 9, "z_col": [4, 5, 6, 32, 34, 38, 51, 54, 55, 57, 65, 77, 78, 79, 84, 87, 90, 92, 94, 96, 141], "z_i": [15, 19, 57, 82, 87, 96], "z_j": [9, 10, 14, 25, 26], "z_true": 82, "zadik": 140, "zaxis_titl": [67, 68, 70], "zero": [22, 26, 41, 58, 59, 60, 69, 74, 75, 81, 82, 85, 86, 96, 110, 114, 125, 126, 131], "zeros_lik": 82, "zeta": [32, 38, 39, 55, 78, 94, 96, 104, 106, 107, 139], "zeta_": [16, 54, 77], "zeta_0": [16, 54, 77], "zeta_i": [13, 15, 17, 52, 66, 89], "zeta_j": 125, "zhang": 140, "zhao": [9, 10, 14, 20, 22, 23, 26, 53, 58, 60, 96, 98, 101, 140], "zimmert": [58, 96, 101, 140], "zip": [67, 68], "zorder": 64, "\u03c4_x0": 80, "\u03c4_x1": 80, "\u2139": 52}, "titles": ["API Reference", "<span class=\"section-number\">1. </span>DoubleML Data Class", "<span class=\"section-number\">3. </span>Datasets", "<span class=\"section-number\">2. </span>DoubleML Models", "<span class=\"section-number\">1.2. </span>doubleml.data.DoubleMLClusterData", "<span class=\"section-number\">1.1. </span>doubleml.data.DoubleMLData", "<span class=\"section-number\">1.3. </span>doubleml.data.DoubleMLPanelData", "<span class=\"section-number\">3.1.1. </span>doubleml.datasets.fetch_401K", "<span class=\"section-number\">3.1.2. </span>doubleml.datasets.fetch_bonus", "<span class=\"section-number\">3.2.9. </span>doubleml.datasets.make_confounded_irm_data", "<span class=\"section-number\">3.2.8. </span>doubleml.datasets.make_confounded_plr_data", "<span class=\"section-number\">3.2.10. </span>doubleml.datasets.make_heterogeneous_data", "<span class=\"section-number\">3.2.4. </span>doubleml.datasets.make_iivm_data", "<span class=\"section-number\">3.2.3. </span>doubleml.datasets.make_irm_data", "<span class=\"section-number\">3.2.11. </span>doubleml.datasets.make_irm_data_discrete_treatments", "<span class=\"section-number\">3.2.2. </span>doubleml.datasets.make_pliv_CHS2015", "<span class=\"section-number\">3.2.6. </span>doubleml.datasets.make_pliv_multiway_cluster_CKMS2021", "<span class=\"section-number\">3.2.1. </span>doubleml.datasets.make_plr_CCDDHNR2018", "<span class=\"section-number\">3.2.5. </span>doubleml.datasets.make_plr_turrell2018", "<span class=\"section-number\">3.2.7. </span>doubleml.datasets.make_ssm_data", "<span class=\"section-number\">2.3.4. </span>doubleml.did.DoubleMLDID", "<span class=\"section-number\">2.3.2. </span>doubleml.did.DoubleMLDIDAggregation", "<span class=\"section-number\">2.3.3. </span>doubleml.did.DoubleMLDIDBinary", "<span class=\"section-number\">2.3.5. </span>doubleml.did.DoubleMLDIDCS", "<span class=\"section-number\">2.3.1. </span>doubleml.did.DoubleMLDIDMulti", "<span class=\"section-number\">3.2.13. </span>doubleml.did.datasets.make_did_CS2021", "<span class=\"section-number\">3.2.12. </span>doubleml.did.datasets.make_did_SZ2020", "<span class=\"section-number\">5.1. </span>doubleml.double_ml_score_mixins.LinearScoreMixin", "<span class=\"section-number\">5.2. </span>doubleml.double_ml_score_mixins.NonLinearScoreMixin", "<span class=\"section-number\">2.2.2. </span>doubleml.irm.DoubleMLAPO", "<span class=\"section-number\">2.2.3. </span>doubleml.irm.DoubleMLAPOS", "<span class=\"section-number\">2.2.7. </span>doubleml.irm.DoubleMLCVAR", "<span class=\"section-number\">2.2.4. </span>doubleml.irm.DoubleMLIIVM", "<span class=\"section-number\">2.2.1. </span>doubleml.irm.DoubleMLIRM", "<span class=\"section-number\">2.2.6. </span>doubleml.irm.DoubleMLLPQ", "<span class=\"section-number\">2.2.5. </span>doubleml.irm.DoubleMLPQ", "<span class=\"section-number\">2.2.8. </span>doubleml.irm.DoubleMLQTE", "<span class=\"section-number\">2.2.9. </span>doubleml.irm.DoubleMLSSM", "<span class=\"section-number\">2.1.2. </span>doubleml.plm.DoubleMLPLIV", "<span class=\"section-number\">2.1.1. </span>doubleml.plm.DoubleMLPLR", "<span class=\"section-number\">2.4.1. </span>doubleml.rdd.RDFlex", "<span class=\"section-number\">3.2.14. </span>doubleml.rdd.datasets.make_simple_rdd_data", "<span class=\"section-number\">4.1.2. </span>doubleml.utils.DMLDummyClassifier", "<span class=\"section-number\">4.1.1. </span>doubleml.utils.DMLDummyRegressor", "<span class=\"section-number\">4.1.3. </span>doubleml.utils.DoubleMLBLP", "<span class=\"section-number\">4.1.4. </span>doubleml.utils.DoubleMLPolicyTree", "<span class=\"section-number\">4.1.6. </span>doubleml.utils.GlobalClassifier", "<span class=\"section-number\">4.1.5. </span>doubleml.utils.GlobalRegressor", "<span class=\"section-number\">4.2.1. </span>doubleml.utils.gain_statistics", "<span class=\"section-number\">5. </span>Score Mixin Classes for DoubleML Models", "<span class=\"section-number\">4. </span>Utility Classes and Functions", "R: Basic Instrumental Variables Calculation", "R: Basics of Double Machine Learning", "R: DoubleML for Difference-in-Differences", "R: Cluster Robust Double Machine Learning", "R: Impact of 401(k) on Financial Wealth", "R: Ensemble Learners and More with <code class=\"docutils literal notranslate\"><span class=\"pre\">mlr3pipelines</span></code>", "R: Sample Selection Models", "Python: Difference-in-Differences", "Python: Difference-in-Differences Pre-Testing", "Python: Panel Data with Multiple Time Periods", "Python: Panel Data Introduction", "DML: Bonus Data", "Examples", "Python: Average Potential Outcome (APO) Models", "Python: Basic Instrumental Variables calculation", "Python: Basics of Double Machine Learning", "Python: Conditional Average Treatment Effects (CATEs) for IRM models", "Python: Conditional Average Treatment Effects (CATEs) for PLR models", "Python: Conditional Value at Risk of potential outcomes", "Python: First Stage and Causal Estimation", "Python: Group Average Treatment Effects (GATEs) for IRM models", "Python: Group Average Treatment Effects (GATEs) for PLR models", "Python: GATE Sensitivity Analysis", "Python: IRM and APO Model Comparison", "Python: Choice of learners", "DoubleML meets FLAML - How to tune learners automatically within <code class=\"docutils literal notranslate\"><span class=\"pre\">DoubleML</span></code>", "Python: Cluster Robust Double Machine Learning", "Python: Impact of 401(k) on Financial Wealth", "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)", "Python: PLM and IRM for Multiple Treatments", "Python: Policy Learning with Trees", "Python: Potential Quantiles and Quantile Treatment Effects", "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)", "Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments", "Python: Sensitivity Analysis", "Example: Sensitivity Analysis for Causal ML", "Python: Sample Selection Models", "<span class=\"section-number\">6. </span>Double machine learning algorithms", "<span class=\"section-number\">1. </span>The basics of double/debiased machine learning", "DoubleMLData from dataframes", "&lt;no title&gt;", "<span class=\"section-number\">2. </span>Data Backend", "User Guide", "<span class=\"section-number\">4. </span>Heterogeneous treatment effects", "<span class=\"section-number\">7. </span>Learners, hyperparameters and hyperparameter tuning", "<span class=\"section-number\">3. </span>Models", "&lt;no title&gt;", "Panel Data", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">9. </span>Sample-splitting, cross-fitting and repeated cross-fitting", "<span class=\"section-number\">5. </span>Score functions", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">8. </span>Variance estimation and confidence intervals", "<span class=\"section-number\">10. </span>Sensitivity analysis", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "DoubleML", "Installing DoubleML", "Getting Started", "Double Machine Learning Literature", "Release Notes", "DoubleML Workflow"], "titleterms": {"": 76, "0": 142, "1": [76, 86, 142], "2": [76, 86, 142], "2011": 86, "2023": 86, "3": [76, 86, 142], "4": [86, 142], "401": [55, 78, 79, 85], "5": [86, 142], "6": 142, "7": 142, "95": 76, "A": [54, 77], "ATE": [57, 73, 80, 87], "No": [54, 77], "One": [54, 67, 68, 77], "That": 84, "The": [55, 78, 80, 89, 139], "acknowledg": [53, 137], "acycl": [51, 65], "addit": 80, "adjust": [60, 83], "advanc": [83, 95, 125], "aggreg": [60, 61, 96], "al": 86, "algorithm": [88, 126, 137, 139], "all": 60, "altern": 110, "analysi": [60, 64, 73, 74, 85, 86, 126, 142], "anticip": 60, "api": [0, 76], "apo": [64, 74, 96, 110, 126], "applic": [54, 77, 85], "approach": [52, 66, 75, 89], "ar": 84, "arah": 86, "arbitrari": 80, "arrai": [90, 92], "asset": [55, 78], "assumpt": 86, "att": [58, 60, 61], "augment": 80, "automat": 76, "automl": 76, "averag": [55, 64, 67, 68, 71, 72, 74, 78, 94, 96, 110, 126], "backend": [54, 55, 77, 78, 92, 139, 142], "band": 125, "base": 56, "basic": [51, 52, 60, 65, 66, 89], "benchmark": [85, 86, 126], "bia": [52, 66, 89], "binari": [96, 110], "bonu": 62, "bootstrap": 125, "build": 138, "calcul": [51, 65], "call": 76, "callabl": 110, "case": 63, "cate": [67, 68, 80, 94], "causal": [62, 64, 70, 86, 110, 139, 142], "chernozhukov": 86, "choic": 75, "citat": 137, "class": [1, 49, 50, 54, 77], "cluster": [54, 77], "code": 137, "coeffici": 76, "combin": [60, 70], "compar": [75, 76], "comparison": [53, 74, 76], "comput": [75, 76], "conclus": [76, 86], "conda": 138, "condit": [67, 68, 69, 79, 94, 110], "confid": [76, 84, 125], "construct": 95, "contrast": 64, "control": 60, "covari": [60, 83], "coverag": [58, 70], "cran": 138, "creat": 76, "cross": [54, 58, 77, 96, 98, 109, 110, 126, 139], "custom": [75, 76], "cvar": [69, 79, 94, 110], "dag": [51, 65], "data": [1, 4, 5, 6, 51, 52, 54, 55, 56, 57, 58, 60, 61, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, 81, 82, 83, 85, 86, 87, 89, 92, 96, 98, 110, 126, 139, 142], "datafram": [90, 92], "dataset": [2, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 41, 62], "debias": [52, 66, 89, 139], "default": 76, "defin": [54, 77], "demo": 53, "depend": 138, "descript": 60, "design": [83, 96], "detail": [53, 60, 61, 96], "develop": 138, "dgp": [52, 64, 66], "did": [3, 20, 21, 22, 23, 24, 25, 26, 53, 96], "differ": [53, 58, 59, 63, 75, 96, 110, 125, 126], "dimension": [67, 68], "direct": [51, 65], "disclaim": 86, "discontinu": [83, 96], "distribut": [57, 87], "dml": [54, 62, 77, 109, 139, 142], "dml1": 88, "dml2": 88, "dmldummyclassifi": 42, "dmldummyregressor": 43, "doubl": [52, 54, 66, 77, 88, 89, 137, 139, 140], "double_ml_score_mixin": [27, 28], "doubleml": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 53, 55, 56, 65, 76, 78, 85, 86, 125, 137, 138, 142], "doublemlapo": [29, 30], "doublemlblp": 44, "doublemlclusterdata": [4, 54, 77], "doublemlcvar": 31, "doublemldata": [5, 55, 78, 90, 92, 139], "doublemldid": 20, "doublemldidaggreg": 21, "doublemldidbinari": 22, "doublemldidc": 23, "doublemldidmulti": 24, "doublemliivm": 32, "doublemlirm": 33, "doublemllpq": 34, "doublemlpaneldata": [6, 60, 92], "doublemlpliv": [38, 54, 77], "doublemlplr": 39, "doublemlpolicytre": 45, "doublemlpq": 35, "doublemlqt": 36, "doublemlssm": 37, "effect": [55, 60, 61, 63, 67, 68, 69, 71, 72, 74, 78, 79, 80, 82, 85, 86, 94, 96], "elig": [55, 78], "empir": 70, "ensembl": [56, 83], "error": [54, 77], "estim": [51, 55, 57, 58, 60, 61, 62, 65, 70, 73, 76, 78, 79, 80, 82, 85, 86, 87, 109, 110, 125, 139, 142], "et": 86, "evalu": [75, 76, 95], "event": [60, 61], "exampl": [53, 54, 63, 67, 68, 77, 85, 86], "exploit": [53, 56], "extern": [95, 109], "featur": [56, 137], "fetch_401k": 7, "fetch_bonu": 8, "figur": 80, "file": 138, "final": 53, "financi": [55, 78, 79], "first": 70, "fit": [54, 76, 77, 109, 139], "flaml": 76, "flexibl": 83, "fold": [76, 109], "forest": 62, "formul": [86, 142], "from": [53, 56, 90, 92, 138], "full": 76, "function": [50, 53, 54, 77, 110, 139], "fuzzi": [83, 96], "gain_statist": 48, "gate": [71, 72, 73, 94], "gatet": 73, "gener": [2, 52, 63, 64, 66, 76, 83, 89, 126], "get": 139, "github": 138, "global": 83, "globalclassifi": 46, "globalregressor": 47, "graph": [51, 65], "group": [60, 61, 71, 72, 94], "guid": 93, "helper": [54, 77], "heterogen": [63, 74, 80, 94], "how": [56, 76], "hyperparamet": [74, 95], "identif": 86, "iivm": [55, 78, 96, 110], "impact": [55, 78, 79], "implement": [88, 96, 110, 126], "induc": [52, 66, 89], "infer": [125, 142], "initi": [54, 76, 77], "instal": 138, "instrument": [51, 65, 84], "integr": 53, "interact": [55, 71, 78, 81, 96, 110, 126], "interv": [76, 84, 125], "introduct": 61, "invers": 80, "irm": [3, 29, 30, 31, 32, 33, 34, 35, 36, 37, 55, 62, 67, 71, 74, 78, 80, 81, 85, 94, 96, 110, 126], "iv": [51, 55, 65, 78, 96, 110], "k": [55, 78, 79, 85, 109], "lambda": 70, "lasso": [62, 70], "latest": 138, "lear": [54, 77], "learn": [52, 54, 66, 77, 81, 88, 89, 94, 137, 139, 140], "learner": [56, 62, 74, 75, 76, 83, 95, 139], "less": 76, "level": 96, "linear": [55, 60, 72, 78, 80, 83, 96, 110, 126], "linearscoremixin": 27, "literatur": 140, "load": [54, 62, 77, 86], "loader": 2, "local": [55, 78, 79, 82, 83, 110], "loss": 70, "lpq": [82, 110], "lqte": [79, 82], "m": 109, "machin": [52, 54, 66, 77, 88, 89, 137, 139, 140], "main": 137, "mainten": 137, "make_confounded_irm_data": 9, "make_confounded_plr_data": 10, "make_did_cs2021": 25, "make_did_sz2020": 26, "make_heterogeneous_data": 11, "make_iivm_data": 12, "make_irm_data": 13, "make_irm_data_discrete_treat": 14, "make_pliv_chs2015": 15, "make_pliv_multiway_cluster_ckms2021": 16, "make_plr_ccddhnr2018": 17, "make_plr_turrell2018": 18, "make_simple_rdd_data": 41, "make_ssm_data": 19, "mar": [57, 87], "market": [54, 77], "matric": [90, 92], "meet": 76, "method": [76, 142], "metric": [75, 76], "minimum": 95, "miss": [57, 87], "missing": [96, 110], "mixin": 49, "ml": [52, 53, 66, 86, 89, 142], "mlr3": 56, "mlr3extralearn": 56, "mlr3learner": 56, "mlr3pipelin": 56, "model": [3, 49, 55, 57, 62, 64, 67, 68, 71, 72, 74, 76, 78, 80, 81, 84, 86, 87, 94, 96, 109, 110, 125, 126, 139, 142], "modul": 62, "more": 56, "motiv": [54, 77], "multipl": [60, 64, 80, 96], "multipli": 125, "naiv": [51, 65], "net": [55, 78], "neyman": [110, 139], "nonignor": [57, 87, 96, 110], "nonlinearscoremixin": 28, "nonrespons": [57, 87, 96, 110], "note": 141, "nuisanc": [76, 139], "object": [54, 77, 85], "option": 138, "orthogon": [52, 66, 89, 110, 139], "out": [52, 66, 89], "outcom": [57, 58, 64, 69, 87, 94, 96, 110, 126], "over": 125, "overcom": [52, 66, 89], "overfit": [52, 66, 89], "overlap": 80, "packag": [53, 55, 78, 138], "panel": [58, 60, 61, 96, 98, 110, 126], "paramet": [56, 62, 76, 110], "partial": [52, 55, 66, 72, 78, 80, 89, 96, 110, 126], "particip": [55, 78], "partit": 109, "penalti": 70, "perform": [53, 80], "period": [60, 96, 110, 126], "pip": 138, "pipelin": 95, "pliv": [96, 110], "plm": [3, 38, 39, 80, 96, 110, 126], "plot": [54, 76, 77], "plr": [55, 62, 68, 72, 78, 94, 96, 110, 126], "polici": [81, 94], "potenti": [64, 69, 79, 82, 94, 96, 110, 126], "pq": [82, 94, 110], "pre": 59, "predict": [53, 95], "preprocess": 56, "problem": 142, "process": [52, 54, 64, 66, 77, 89], "product": [54, 77], "propens": 80, "provid": 109, "python": [58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 84, 85, 87, 95, 138], "qte": [82, 94], "qualiti": 70, "quantil": [79, 82, 94, 110], "r": [51, 52, 53, 54, 55, 56, 57, 63, 95, 138], "random": [57, 62, 87, 96, 110], "rank": 80, "rdd": [3, 40, 41, 83, 96], "rdflex": 40, "real": [54, 77], "refer": [0, 51, 53, 54, 56, 65, 70, 75, 76, 77, 80, 84, 86, 89, 95, 109, 125, 137, 139], "regress": [55, 71, 72, 78, 81, 83, 96, 110, 126], "regular": [52, 66, 89], "releas": [138, 141], "remark": 53, "remov": [52, 66, 89], "repeat": [58, 96, 98, 109, 110, 126], "repetit": 109, "requir": 95, "respect": [54, 77], "result": [54, 55, 77, 78, 80], "risk": [69, 79, 94, 110], "robust": [54, 77, 84], "run": 84, "sampl": [52, 57, 66, 76, 87, 89, 96, 109, 110], "sandbox": 63, "score": [49, 52, 66, 80, 89, 110, 139], "section": [58, 96, 98, 110, 126], "select": [57, 60, 87, 96, 110], "sensit": [60, 64, 73, 74, 85, 86, 126, 142], "set": [56, 95], "sharp": [83, 96], "simpl": [52, 66, 89], "simul": [51, 54, 58, 65, 77, 84, 85], "simultan": 125, "singl": 64, "small": 84, "sourc": [137, 138], "special": 92, "specif": [126, 142], "specifi": [62, 95, 110], "split": [52, 66, 89, 109], "ssm": 96, "stack": 83, "stage": 70, "standard": [54, 75, 77], "start": 139, "step": 76, "studi": [60, 61, 63], "summari": [55, 76, 78, 80], "test": 59, "theori": 126, "time": [60, 61, 75, 76], "train": 76, "treat": 74, "treatment": [55, 67, 68, 69, 71, 72, 74, 78, 79, 80, 82, 94, 96, 110, 126], "tree": [81, 94], "tune": [56, 76, 95], "two": [54, 67, 68, 77, 96, 110, 126], "type": 92, "under": [57, 80, 87], "untun": 76, "up": 56, "us": [51, 53, 56, 62, 65, 76, 95], "user": 93, "util": [42, 43, 44, 45, 46, 47, 48, 50], "v": 70, "valid": 125, "valu": [69, 79, 94, 110], "vanderweel": 86, "variabl": [51, 65, 84], "varianc": 125, "version": 138, "via": 110, "wai": [54, 77], "weak": 84, "wealth": [55, 78, 79], "weight": [80, 94], "when": 76, "whl": 138, "within": 76, "without": [83, 109], "workflow": 142, "xgboost": 76, "zero": [54, 77]}})