Search.setIndex({"alltitles": {"(One-Way) Cluster Robust Double Machine Learing": [[58, "(One-Way)-Cluster-Robust-Double-Machine-Learing"], [84, "(One-Way)-Cluster-Robust-Double-Machine-Learing"]], "0. Problem Formulation": [[153, "problem-formulation"]], "1. Data Backend": [[153, "data-backend"]], "1. Formulation of Causal Model & Identification Assumptions": [[93, "1.-Formulation-of-Causal-Model-&-Identification-Assumptions"]], "2. Causal Model": [[153, "causal-model"]], "2. Estimation of Causal Effect": [[93, "2.-Estimation-of-Causal-Effect"]], "3. ML Methods": [[153, "ml-methods"]], "3. Sensitivity Analysis": [[93, "3.-Sensitivity-Analysis"]], "4. Benchmarking Analysis": [[93, "4.-Benchmarking-Analysis"]], "4. DML Specifications": [[153, "dml-specifications"]], "5. Conclusion": [[93, "5.-Conclusion"]], "5. Estimation": [[153, "estimation"]], "6. Inference": [[153, "inference"]], "7. Sensitivity Analysis": [[153, "sensitivity-analysis"]], "A Motivating Example: Two-Way Cluster Robust DML": [[58, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"], [84, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"]], "API Reference": [[0, null]], "ATE Estimation and Sensitivity": [[80, "ATE-Estimation-and-Sensitivity"]], "ATE estimates distribution": [[61, "ATE-estimates-distribution"], [61, "id3"], [94, "ATE-estimates-distribution"], [94, "id3"]], "ATT Estimation": [[64, "ATT-Estimation"], [64, "id1"], [66, "ATT-Estimation"], [67, "ATT-Estimation"], [67, "id1"]], "ATT Estimation: Conditional Parallel Trends": [[65, "ATT-Estimation:-Conditional-Parallel-Trends"]], "ATT Estimation: Unconditional Parallel Trends": [[65, "ATT-Estimation:-Unconditional-Parallel-Trends"]], "ATTE Estimation": [[62, "ATTE-Estimation"], [62, "id2"]], "Acknowledgements": [[148, "acknowledgements"]], "Acknowledgements and Final Remarks": [[57, "Acknowledgements-and-Final-Remarks"]], "Additional Results: CATE estimates": [[87, "Additional-Results:-CATE-estimates"]], "Advanced: External Predictions": [[105, "advanced-external-predictions"]], "Advanced: Global and Local Learners, Stacked Ensembles": [[90, "Advanced:-Global-and-Local-Learners,-Stacked-Ensembles"]], "Aggregated Effects": [[64, "Aggregated-Effects"], [67, "Aggregated-Effects"]], "Aggregation Details": [[64, "Aggregation-Details"], [65, "Aggregation-Details"], [66, "Aggregation-Details"], [67, "Aggregation-Details"]], "Algorithm DML1": [[95, "algorithm-dml1"]], "Algorithm DML2": [[95, "algorithm-dml2"]], "All Combinations": [[64, "All-Combinations"]], "All combinations": [[67, "All-combinations"]], "Anticipation": [[64, "Anticipation"], [67, "Anticipation"]], "Application Results": [[58, "Application-Results"], [84, "Application-Results"]], "Application: 401(k)": [[92, "Application:-401(k)"]], "AutoML with less Computation time": [[83, "AutoML-with-less-Computation-time"]], "Average Potential Outcome (APOs)": [[71, "Average-Potential-Outcome-(APOs)"]], "Average Potential Outcomes (APOs)": [[106, "average-potential-outcomes-apos"], [121, "average-potential-outcomes-apos"], [137, "average-potential-outcomes-apos"]], "Average Potential Outcomes (APOs) for Multiple Treatment Levels": [[106, "average-potential-outcomes-apos-for-multiple-treatment-levels"]], "Average Treatment Effect": [[81, "Average-Treatment-Effect"]], "Average Treatment Effect on the Treated": [[81, "Average-Treatment-Effect-on-the-Treated"]], "Basics": [[64, "Basics"], [67, "Basics"]], "Benchmarking": [[137, "benchmarking"]], "Benchmarking Analysis": [[92, "Benchmarking-Analysis"]], "Binary Interactive Regression Model (IRM)": [[106, "binary-interactive-regression-model-irm"], [121, "binary-interactive-regression-model-irm"]], "CATEs for IRM models": [[104, "cates-for-irm-models"]], "CATEs for PLR models": [[104, "cates-for-plr-models"]], "CVaR Treatment Effects": [[76, "CVaR-Treatment-Effects"]], "CVaR of potential outcomes": [[104, "cvar-of-potential-outcomes"]], "CVaR treatment effects": [[104, "cvar-treatment-effects"]], "Causal Analysis with DoubleML": [[93, "Causal-Analysis-with-DoubleML"]], "Causal Contrasts": [[71, "Causal-Contrasts"]], "Causal Research Question": [[65, "Causal-Research-Question"]], "Causal estimation vs. lasso penalty \\lambda": [[77, "Causal-estimation-vs.-lasso-penalty-\\lambda"]], "Chernozhukov et al. (2023): Benchmarking": [[93, "Chernozhukov-et-al.-(2023):-Benchmarking"]], "Citation": [[148, "citation"]], "Cluster Robust Cross Fitting": [[58, "Cluster-Robust-Cross-Fitting"], [84, "Cluster-Robust-Cross-Fitting"]], "Cluster Robust Standard Errors": [[58, "Cluster-Robust-Standard-Errors"], [84, "Cluster-Robust-Standard-Errors"]], "Clustering and double machine learning": [[58, "Clustering-and-double-machine-learning"], [84, "Clustering-and-double-machine-learning"]], "Combined loss vs. lasso penalty \\lambda": [[77, "Combined-loss-vs.-lasso-penalty-\\lambda"]], "Compare Metrics for Nuisance Estimation": [[83, "Compare-Metrics-for-Nuisance-Estimation"]], "Comparing different learners": [[82, "Comparing-different-learners"]], "Comparison and summary": [[83, "Comparison-and-summary"]], "Comparison to AutoML with less Computation time and Untuned XGBoost Learners": [[83, "Comparison-to-AutoML-with-less-Computation-time-and-Untuned-XGBoost-Learners"]], "Comparison to did package": [[57, "Comparison-to-did-package"]], "Computation time": [[82, "Computation-time"]], "Conclusion": [[83, "Conclusion"]], "Conditional Value at Risk (CVaR)": [[76, "Conditional-Value-at-Risk-(CVaR)"]], "Conditional average treatment effects (CATEs)": [[104, "conditional-average-treatment-effects-cates"]], "Conditional value at risk (CVaR)": [[104, "conditional-value-at-risk-cvar"], [121, "conditional-value-at-risk-cvar"]], "Confidence bands and multiplier bootstrap for valid simultaneous inference": [[136, "confidence-bands-and-multiplier-bootstrap-for-valid-simultaneous-inference"]], "Control Groups": [[64, "Control-Groups"], [67, "Control-Groups"]], "Coverage Simulation": [[62, "Coverage-Simulation"], [62, "id3"]], "Creating the DoubleMLData Object": [[70, "Creating-the-DoubleMLData-Object"]], "Cross-fitting with K folds": [[120, "cross-fitting-with-k-folds"]], "Cross-fitting, DML algorithms and Neyman-orthogonal score functions": [[150, "cross-fitting-dml-algorithms-and-neyman-orthogonal-score-functions"]], "Custom evaluation metrics": [[82, "Custom-evaluation-metrics"]], "DML: Bonus Data": [[68, null]], "Data": [[59, "Data"], [61, "Data"], [61, "id1"], [62, "Data"], [62, "id1"], [64, "Data"], [65, "Data"], [66, "Data"], [67, "Data"], [74, "Data"], [75, "Data"], [76, "Data"], [78, "Data"], [79, "Data"], [80, "Data"], [81, "Data"], [85, "Data"], [86, "Data"], [88, "Data"], [89, "Data"], [89, "id1"], [92, "Data"], [94, "Data"], [94, "id1"], [150, "data"]], "Data Backend": [[102, null]], "Data Description": [[64, "Data-Description"], [67, "Data-Description"]], "Data Details": [[64, "Data-Details"], [67, "Data-Details"]], "Data Generating Process (DGP)": [[56, "Data-Generating-Process-(DGP)"], [70, "Data-Generating-Process-(DGP)"], [71, "Data-Generating-Process-(DGP)"], [73, "Data-Generating-Process-(DGP)"]], "Data Generation": [[83, "Data-Generation"]], "Data Simulation": [[55, "Data-Simulation"], [72, "Data-Simulation"]], "Data and Effect Estimation": [[92, "Data-and-Effect-Estimation"]], "Data generating process": [[96, "data-generating-process"]], "Data preprocessing": [[60, "Data-preprocessing"]], "Data with Anticipation": [[64, "Data-with-Anticipation"], [67, "Data-with-Anticipation"]], "Data-Backend for Cluster Data": [[58, "Data-Backend-for-Cluster-Data"], [84, "Data-Backend-for-Cluster-Data"]], "Dataset Generators": [[2, "dataset-generators"]], "Dataset Loaders": [[2, "dataset-loaders"]], "Datasets": [[2, null]], "Define Helper Functions for Plotting": [[58, "Define-Helper-Functions-for-Plotting"], [84, "Define-Helper-Functions-for-Plotting"]], "Demo Example from did": [[57, "Demo-Example-from-did"]], "Details on Predictive Performance": [[57, "Details-on-Predictive-Performance"]], "Difference-in-Differences": [[69, "difference-in-differences"]], "Difference-in-Differences Models": [[121, "difference-in-differences-models"], [137, "difference-in-differences-models"]], "Difference-in-Differences Models (DID)": [[106, "difference-in-differences-models-did"]], "Difference-in-Differences for Panel Data": [[137, "difference-in-differences-for-panel-data"]], "Difference-in-Differences for repeated cross-sections": [[137, "difference-in-differences-for-repeated-cross-sections"]], "Disclaimer": [[93, "Disclaimer"]], "Double Machine Learning Algorithm": [[148, "double-machine-learning-algorithm"]], "Double Machine Learning Literature": [[151, null]], "Double machine learning algorithms": [[95, null]], "Double/debiased machine learning": [[56, "Double/debiased-machine-learning"], [73, "Double/debiased-machine-learning"], [96, "double-debiased-machine-learning"]], "DoubleML": [[148, null]], "DoubleML Data Class": [[1, null]], "DoubleML Models": [[3, null]], "DoubleML Object": [[92, "DoubleML-Object"]], "DoubleML Workflow": [[153, null]], "DoubleML meets FLAML - How to tune learners automatically within DoubleML": [[83, null]], "DoubleML with TabPFN": [[70, "DoubleML-with-TabPFN"]], "DoubleMLDIDData": [[102, "doublemldiddata"]], "DoubleMLData": [[102, "doublemldata"]], "DoubleMLData from arrays and matrices": [[97, "doublemldata-from-arrays-and-matrices"], [102, "doublemldata-from-arrays-and-matrices"]], "DoubleMLData from dataframes": [[97, null], [102, "doublemldata-from-dataframes"]], "DoubleMLPanelData": [[64, "DoubleMLPanelData"], [67, "DoubleMLPanelData"], [102, "doublemlpaneldata"]], "DoubleMLRDDData": [[102, "doublemlrdddata"]], "DoubleMLSSMData": [[102, "doublemlssmdata"]], "Effect Aggregation": [[65, "Effect-Aggregation"], [66, "Effect-Aggregation"], [106, "effect-aggregation"]], "Effect Heterogeneity": [[69, "effect-heterogeneity"], [81, "Effect-Heterogeneity"]], "Empirical coverage vs. lasso penalty \\lambda": [[77, "Empirical-coverage-vs.-lasso-penalty-\\lambda"]], "Estimate DML models without sample-splitting": [[120, "estimate-dml-models-without-sample-splitting"]], "Estimate double/debiased machine learning models": [[150, "estimate-double-debiased-machine-learning-models"]], "Estimating Potential Quantiles and Quantile Treatment Effects": [[86, "Estimating-Potential-Quantiles-and-Quantile-Treatment-Effects"]], "Estimating local quantile treatment effects (LQTEs)": [[86, "Estimating-local-quantile-treatment-effects-(LQTEs)"]], "Estimating the Average Treatment Effect of 401(k) Eligibility on Net Financial Assets": [[59, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"], [85, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"]], "Estimating the treatment effect on the Conditional Value a Risk (CVaR)": [[86, "Estimating-the-treatment-effect-on-the-Conditional-Value-a-Risk-(CVaR)"]], "Estimation": [[61, "Estimation"], [61, "id2"], [94, "Estimation"], [94, "id2"]], "Estimation of Average Potential Outcomes": [[70, "Estimation-of-Average-Potential-Outcomes"]], "Estimation quality vs. \\lambda": [[77, "Estimation-quality-vs.-\\lambda"]], "Evaluate learners": [[105, "evaluate-learners"]], "Event Study Aggregation": [[64, "Event-Study-Aggregation"], [65, "Event-Study-Aggregation"], [66, "Event-Study-Aggregation"], [67, "Event-Study-Aggregation"]], "Example usage": [[98, "example-usage"], [99, "example-usage"], [100, "example-usage"], [101, "example-usage"], [102, "example-usage"], [102, "id6"], [102, "id8"], [102, "id10"]], "Example: Sensitivity Analysis for Causal ML": [[93, null]], "Examples": [[69, null]], "Exploiting the Functionalities of did": [[57, "Exploiting-the-Functionalities-of-did"]], "Externally provide a sample splitting / partition": [[120, "externally-provide-a-sample-splitting-partition"]], "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)": [[90, null]], "Fuzzy RDD": [[90, "Fuzzy-RDD"]], "Fuzzy RDD Without Adjustment": [[90, "Fuzzy-RDD-Without-Adjustment"]], "Fuzzy RDD with Flexible Adjustment": [[90, "Fuzzy-RDD-with-Flexible-Adjustment"]], "Fuzzy RDD with Linear Adjustment": [[90, "Fuzzy-RDD-with-Linear-Adjustment"]], "Fuzzy Regression Discontinuity Design": [[106, "fuzzy-regression-discontinuity-design"]], "GATE Estimation and Sensitivity": [[80, "GATE-Estimation-and-Sensitivity"]], "GATET Estimation and Sensitivity": [[80, "GATET-Estimation-and-Sensitivity"]], "GATEs for IRM models": [[104, "gates-for-irm-models"]], "GATEs for PLR models": [[104, "gates-for-plr-models"]], "General Examples": [[69, "general-examples"]], "General algorithm": [[137, "general-algorithm"]], "Generate Fuzzy Data": [[90, "Generate-Fuzzy-Data"]], "Generate Sharp Data": [[90, "Generate-Sharp-Data"]], "Getting Started": [[150, null]], "Group Aggregation": [[64, "Group-Aggregation"], [66, "Group-Aggregation"], [67, "Group-Aggregation"]], "Group Average Treatment Effects (GATEs)": [[78, "Group-Average-Treatment-Effects-(GATEs)"], [79, "Group-Average-Treatment-Effects-(GATEs)"]], "Group average treatment effects (GATEs)": [[104, "group-average-treatment-effects-gates"]], "Group-Time Combinations": [[64, "Group-Time-Combinations"], [67, "Group-Time-Combinations"]], "Heterogeneous treatment effects": [[104, null]], "How to exploit more features of mlr3pipelines in DoubleML": [[60, "How-to-exploit-more-features-of-mlr3pipelines-in-DoubleML"]], "Hyperparameter tuning": [[105, "hyperparameter-tuning"], [105, "id16"]], "Hyperparameter tuning with pipelines": [[105, "hyperparameter-tuning-with-pipelines"]], "Implementation": [[137, "implementation"]], "Implementation Details": [[106, "implementation-details"]], "Implementation of the double machine learning algorithms": [[95, "implementation-of-the-double-machine-learning-algorithms"]], "Implementation of the score function and the estimate of the causal parameter": [[121, "implementation-of-the-score-function-and-the-estimate-of-the-causal-parameter"]], "Implemented Neyman orthogonal score functions": [[121, "implemented-neyman-orthogonal-score-functions"]], "Imports and Setup": [[70, "Imports-and-Setup"]], "Initialize DoubleMLClusterData object": [[58, "Initialize-DoubleMLClusterData-object"]], "Initialize DoubleMLData object with clusters": [[84, "Initialize-DoubleMLData-object-with-clusters"]], "Initialize the objects of class DoubleMLPLIV": [[58, "Initialize-the-objects-of-class-DoubleMLPLIV"], [84, "Initialize-the-objects-of-class-DoubleMLPLIV"]], "Installing DoubleML": [[149, null]], "Instrumental Variables Directed Acyclic Graph (IV - DAG)": [[55, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"], [72, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"]], "Interactive IV Model (IIVM)": [[59, "Interactive-IV-Model-(IIVM)"], [85, "Interactive-IV-Model-(IIVM)"]], "Interactive IV model (IIVM)": [[106, "interactive-iv-model-iivm"], [121, "interactive-iv-model-iivm"]], "Interactive Regression Model (IRM)": [[59, "Interactive-Regression-Model-(IRM)"], [78, "Interactive-Regression-Model-(IRM)"], [85, "Interactive-Regression-Model-(IRM)"], [88, "Interactive-Regression-Model-(IRM)"]], "Interactive regression model (IRM)": [[137, "interactive-regression-model-irm"]], "Interactive regression models (IRM)": [[106, "interactive-regression-models-irm"], [121, "interactive-regression-models-irm"], [137, "interactive-regression-models-irm"]], "Key Takeaways": [[70, "Key-Takeaways"]], "Key arguments": [[98, null], [99, null], [100, null], [101, null], [102, "key-arguments"], [102, "id5"], [102, "id7"], [102, "id9"]], "Learners and Hyperparameters": [[81, "Learners-and-Hyperparameters"]], "Learners to estimate the nuisance models": [[150, "learners-to-estimate-the-nuisance-models"]], "Learners, hyperparameters and hyperparameter tuning": [[105, null]], "Linear Covariate Adjustment": [[64, "Linear-Covariate-Adjustment"], [67, "Linear-Covariate-Adjustment"]], "Load Data": [[93, "Load-Data"]], "Load and Process Data": [[58, "Load-and-Process-Data"], [84, "Load-and-Process-Data"]], "Load bonus data using the dml datasets module": [[68, "Load-bonus-data-using-the-dml-datasets-module"]], "Local Average Treatment Effects of 401(k) Participation on Net Financial Assets": [[59, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"], [85, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"]], "Local Potential Quantile Estimation": [[89, "Local-Potential-Quantile-Estimation"]], "Local Potential Quantiles (LPQs)": [[89, "Local-Potential-Quantiles-(LPQs)"]], "Local Quantile Treatment Effects (LQTEs)": [[89, "Local-Quantile-Treatment-Effects-(LQTEs)"]], "Local potential quantiles (LPQs)": [[121, "local-potential-quantiles-lpqs"]], "Machine Learning Methods Comparison": [[70, "Machine-Learning-Methods-Comparison"]], "Main Features": [[148, "main-features"]], "Minimum requirements for learners": [[105, "minimum-requirements-for-learners"], [105, "id2"]], "Missingness at Random": [[106, "missingness-at-random"], [121, "missingness-at-random"]], "Model Performance Evaluation": [[70, "Model-Performance-Evaluation"]], "Model-specific implementations": [[137, "model-specific-implementations"]], "Models": [[106, null]], "Motivation": [[58, "Motivation"], [84, "Motivation"]], "Multiple Average Potential Outcome Models (APOS)": [[71, "Multiple-Average-Potential-Outcome-Models-(APOS)"]], "Naive estimation": [[55, "Naive-estimation"], [72, "Naive-estimation"]], "No Clustering / Zero-Way Clustering": [[58, "No-Clustering-/-Zero-Way-Clustering"], [84, "No-Clustering-/-Zero-Way-Clustering"]], "Nonignorable Nonresponse": [[106, "nonignorable-nonresponse"], [121, "nonignorable-nonresponse"]], "One-Way Clustering with Respect to the Market": [[58, "One-Way-Clustering-with-Respect-to-the-Market"], [84, "One-Way-Clustering-with-Respect-to-the-Market"]], "One-Way Clustering with Respect to the Product": [[58, "One-Way-Clustering-with-Respect-to-the-Product"], [84, "One-Way-Clustering-with-Respect-to-the-Product"]], "One-dimensional Example": [[74, "One-dimensional-Example"], [75, "One-dimensional-Example"]], "Outcome missing at random (MAR)": [[61, "Outcome-missing-at-random-(MAR)"], [94, "Outcome-missing-at-random-(MAR)"]], "Outcome missing under nonignorable nonresponse": [[61, "Outcome-missing-under-nonignorable-nonresponse"], [94, "Outcome-missing-under-nonignorable-nonresponse"]], "Overcoming regularization bias by orthogonalization": [[56, "Overcoming-regularization-bias-by-orthogonalization"], [73, "Overcoming-regularization-bias-by-orthogonalization"], [96, "overcoming-regularization-bias-by-orthogonalization"]], "Panel Data": [[106, "id9"], [108, null], [121, "panel-data"], [121, "id3"], [137, "panel-data"]], "Panel Data (Repeated Outcomes)": [[62, "Panel-Data-(Repeated-Outcomes)"]], "Panel data": [[106, "panel-data"]], "Parameter tuning": [[60, "Parameter-tuning"]], "Parameters & Implementation": [[106, "parameters-implementation"]], "Partialling out score": [[56, "Partialling-out-score"], [73, "Partialling-out-score"], [96, "partialling-out-score"]], "Partially Linear Regression Model (PLR)": [[59, "Partially-Linear-Regression-Model-(PLR)"], [79, "Partially-Linear-Regression-Model-(PLR)"], [85, "Partially-Linear-Regression-Model-(PLR)"]], "Partially linear IV regression model (PLIV)": [[106, "partially-linear-iv-regression-model-pliv"], [121, "partially-linear-iv-regression-model-pliv"]], "Partially linear models (PLM)": [[106, "partially-linear-models-plm"], [121, "partially-linear-models-plm"], [137, "partially-linear-models-plm"]], "Partially linear regression model (PLR)": [[106, "partially-linear-regression-model-plr"], [121, "partially-linear-regression-model-plr"], [137, "partially-linear-regression-model-plr"]], "Performance Summary and Insights": [[70, "Performance-Summary-and-Insights"]], "Plot Coefficients and 95% Confidence Intervals": [[83, "Plot-Coefficients-and-95%-Confidence-Intervals"]], "Policy Learning with Trees": [[88, "Policy-Learning-with-Trees"], [104, "policy-learning-with-trees"]], "Potential Quantile Estimation": [[89, "Potential-Quantile-Estimation"]], "Potential Quantiles (PQs)": [[89, "Potential-Quantiles-(PQs)"]], "Potential quantiles (PQs)": [[104, "potential-quantiles-pqs"], [121, "potential-quantiles-pqs"]], "Python: Average Potential Outcome (APO) Models": [[71, null]], "Python: Basic Instrumental Variables calculation": [[72, null]], "Python: Basics of Double Machine Learning": [[73, null]], "Python: Building the package from source": [[149, "python-building-the-package-from-source"]], "Python: Case studies": [[69, "python-case-studies"]], "Python: Causal Machine Learning with TabPFN": [[70, null]], "Python: Choice of learners": [[82, null]], "Python: Cluster Robust Double Machine Learning": [[84, null]], "Python: Conditional Average Treatment Effects (CATEs) for IRM models": [[74, null]], "Python: Conditional Average Treatment Effects (CATEs) for PLR models": [[75, null]], "Python: Conditional Value at Risk of potential outcomes": [[76, null]], "Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments": [[91, null]], "Python: Difference-in-Differences": [[62, null]], "Python: Difference-in-Differences Pre-Testing": [[63, null]], "Python: First Stage and Causal Estimation": [[77, null]], "Python: GATE Sensitivity Analysis": [[80, null]], "Python: Group Average Treatment Effects (GATEs) for IRM models": [[78, null]], "Python: Group Average Treatment Effects (GATEs) for PLR models": [[79, null]], "Python: IRM and APO Model Comparison": [[81, null]], "Python: Impact of 401(k) on Financial Wealth": [[85, null]], "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)": [[86, null]], "Python: Installing DoubleML": [[149, "python-installing-doubleml"]], "Python: Installing a released version from a .whl file": [[149, "python-installing-a-released-version-from-a-whl-file"]], "Python: Installing the latest release from pip or conda": [[149, "python-installing-the-latest-release-from-pip-or-conda"]], "Python: Learners and hyperparameters": [[105, "python-learners-and-hyperparameters"]], "Python: Optional Dependencies": [[149, "python-optional-dependencies"]], "Python: PLM and IRM for Multiple Treatments": [[87, null]], "Python: Panel Data Introduction": [[66, null]], "Python: Panel Data with Multiple Time Periods": [[64, null]], "Python: Policy Learning with Trees": [[88, null]], "Python: Potential Quantiles and Quantile Treatment Effects": [[89, null]], "Python: Real-Data Example for Multi-Period Difference-in-Differences": [[65, null]], "Python: Repeated Cross-Sectional Data with Multiple Time Periods": [[67, null]], "Python: Sample Selection Models": [[94, null]], "Python: Sensitivity Analysis": [[92, null]], "Quantile Treatment Effects (QTEs)": [[89, "Quantile-Treatment-Effects-(QTEs)"]], "Quantile treatment effects (QTEs)": [[104, "quantile-treatment-effects-qtes"]], "Quantiles": [[104, "quantiles"]], "R: Basic Instrumental Variables Calculation": [[55, null]], "R: Basics of Double Machine Learning": [[56, null]], "R: Case studies": [[69, "r-case-studies"]], "R: Cluster Robust Double Machine Learning": [[58, null]], "R: DoubleML for Difference-in-Differences": [[57, null]], "R: Ensemble Learners and More with mlr3pipelines": [[60, null]], "R: Impact of 401(k) on Financial Wealth": [[59, null]], "R: Installing DoubleML": [[149, "r-installing-doubleml"]], "R: Installing the development version from GitHub": [[149, "r-installing-the-development-version-from-github"]], "R: Installing the latest release from CRAN": [[149, "r-installing-the-latest-release-from-cran"]], "R: Learners and hyperparameters": [[105, "r-learners-and-hyperparameters"]], "R: Sample Selection Models": [[61, null]], "Ranking Treatment Effects under Treatment Propensity and Treatment Effect Heterogeneity": [[87, "Ranking-Treatment-Effects-under-Treatment-Propensity-and-Treatment-Effect-Heterogeneity"]], "Real-Data Application": [[58, "Real-Data-Application"], [84, "Real-Data-Application"]], "References": [[55, "References"], [57, "References"], [58, "References"], [60, "References"], [72, "References"], [77, "References"], [82, "References"], [83, "References"], [84, "References"], [87, "References"], [91, "References"], [93, "References"], [96, "references"], [105, "references"], [120, "references"], [136, "references"], [148, "references"], [150, "references"]], "Regression Discontinuity Designs (RDD)": [[106, "regression-discontinuity-designs-rdd"]], "Regularization Bias in Simple ML-Approaches": [[56, "Regularization-Bias-in-Simple-ML-Approaches"], [73, "Regularization-Bias-in-Simple-ML-Approaches"]], "Regularization bias in simple ML-approaches": [[96, "regularization-bias-in-simple-ml-approaches"]], "Release Notes": [[152, null]], "Repeated Cross-Sectional Data": [[62, "Repeated-Cross-Sectional-Data"], [121, "repeated-cross-sectional-data"], [121, "id4"], [137, "repeated-cross-sectional-data"]], "Repeated cross-fitting with K folds and M repetitions": [[120, "repeated-cross-fitting-with-k-folds-and-m-repetitions"]], "Repeated cross-sections": [[106, "repeated-cross-sections"], [106, "id10"], [108, "repeated-cross-sections"]], "Running a small simulation": [[91, "Running-a-small-simulation"]], "Sample Selection Models": [[121, "sample-selection-models"]], "Sample Selection Models (SSM)": [[106, "sample-selection-models-ssm"]], "Sample splitting to remove bias induced by overfitting": [[56, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [73, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [96, "sample-splitting-to-remove-bias-induced-by-overfitting"]], "Sample-splitting without cross-fitting": [[120, "sample-splitting-without-cross-fitting"]], "Sample-splitting, cross-fitting and repeated cross-fitting": [[120, null]], "Sandbox/Archive": [[69, "sandbox-archive"]], "Score Mixin Classes for DoubleML Models": [[53, null]], "Score functions": [[121, null]], "Selected Combinations": [[64, "Selected-Combinations"], [67, "Selected-Combinations"]], "Sensitivity Analysis": [[64, "Sensitivity-Analysis"], [67, "Sensitivity-Analysis"], [71, "Sensitivity-Analysis"], [81, "Sensitivity-Analysis"], [92, "Sensitivity-Analysis"], [92, "id1"]], "Sensitivity Analysis with IRM": [[92, "Sensitivity-Analysis-with-IRM"]], "Sensitivity analysis": [[137, null]], "Set up learners based on mlr3pipelines": [[60, "Set-up-learners-based-on-mlr3pipelines"]], "Sharp RDD": [[90, "Sharp-RDD"]], "Sharp RDD Without Adjustment": [[90, "Sharp-RDD-Without-Adjustment"]], "Sharp RDD with Flexible Adjustment": [[90, "Sharp-RDD-with-Flexible-Adjustment"]], "Sharp RDD with Linear Adjustment": [[90, "Sharp-RDD-with-Linear-Adjustment"]], "Sharp Regression Discontinuity Design": [[106, "sharp-regression-discontinuity-design"]], "Simulate two-way cluster data": [[58, "Simulate-two-way-cluster-data"], [84, "Simulate-two-way-cluster-data"]], "Simulation Example": [[92, "Simulation-Example"]], "Simultaneous inference over different DoubleML models (advanced)": [[136, "simultaneous-inference-over-different-doubleml-models-advanced"]], "Single Average Potential Outcome Models (APO)": [[71, "Single-Average-Potential-Outcome-Models-(APO)"]], "Source code and maintenance": [[148, "source-code-and-maintenance"]], "Special Data Types": [[102, "special-data-types"]], "Specify learner and estimate causal parameter: IRM model with Lasso as learner": [[68, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: IRM model with random forest as learner": [[68, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-random-forest-as-learner"]], "Specify learner and estimate causal parameter: PLR model with Lasso as learner": [[68, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: PLR model with random forest as learner": [[68, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-random-forest-as-learner"]], "Specifying alternative score functions via callables": [[121, "specifying-alternative-score-functions-via-callables"]], "Specifying learners and set hyperparameters": [[105, "specifying-learners-and-set-hyperparameters"], [105, "id9"]], "Standard approach": [[82, "Standard-approach"]], "Step 1: Custom API for FLAML Models within DoubleML": [[83, "Step-1:-Custom-API-for-FLAML-Models-within-DoubleML"]], "Step 1: Initialize and Train the AutoML Models:": [[83, "Step-1:-Initialize-and-Train-the-AutoML-Models:"]], "Step 2: Evaluate the Tuned Models": [[83, "Step-2:-Evaluate-the-Tuned-Models"]], "Step 2: Using the API when calling DoubleML\u2019s .fit() Method": [[83, "Step-2:-Using-the-API-when-calling-DoubleML's-.fit()-Method"]], "Step 3: Create and Fit DoubleML Model": [[83, "Step-3:-Create-and-Fit-DoubleML-Model"]], "Summary Figure": [[87, "Summary-Figure"]], "Summary of Results": [[59, "Summary-of-Results"], [85, "Summary-of-Results"]], "The Augmented Inverse Propensity Weighting Model estimates the ATE under arbitrary effect and propensity score heterogeneity": [[87, "The-Augmented-Inverse-Propensity-Weighting-Model-estimates-the-ATE-under-arbitrary-effect-and-propensity-score-heterogeneity"]], "The Data Backend: DoubleMLData": [[59, "The-Data-Backend:-DoubleMLData"], [85, "The-Data-Backend:-DoubleMLData"]], "The DoubleML package": [[59, "The-DoubleML-package"], [85, "The-DoubleML-package"]], "The Partially Linear Model performs overlap weighting": [[87, "The-Partially-Linear-Model-performs-overlap-weighting"]], "The basics of double/debiased machine learning": [[96, null]], "The causal model": [[150, "the-causal-model"]], "The data-backend DoubleMLData": [[150, "the-data-backend-doublemldata"]], "Theory": [[137, "theory"]], "Time Aggregation": [[64, "Time-Aggregation"], [66, "Time-Aggregation"], [67, "Time-Aggregation"]], "Tuning on the Folds": [[83, "Tuning-on-the-Folds"]], "Tuning on the full Sample": [[83, "Tuning-on-the-full-Sample"]], "Two treatment periods": [[106, "two-treatment-periods"], [121, "two-treatment-periods"], [137, "two-treatment-periods"]], "Two-Dimensional Example": [[74, "Two-Dimensional-Example"], [75, "Two-Dimensional-Example"]], "Two-Way Clustering with Respect to Product and Market": [[58, "Two-Way-Clustering-with-Respect-to-Product-and-Market"], [84, "Two-Way-Clustering-with-Respect-to-Product-and-Market"]], "Universal Base Period": [[64, "Universal-Base-Period"], [67, "Universal-Base-Period"]], "Untuned (default parameter) XGBoost": [[83, "Untuned-(default-parameter)-XGBoost"]], "Use ensemble learners based on mlr3pipelines": [[60, "Use-ensemble-learners-based-on-mlr3pipelines"]], "User Guide": [[103, null]], "Using DoubleML": [[55, "Using-DoubleML"], [72, "Using-DoubleML"]], "Using ML for DiD: Integrating DoubleML in did": [[57, "Using-ML-for-DiD:-Integrating-DoubleML-in-did"]], "Using learners from mlr3, mlr3learners and mlr3extralearners": [[60, "Using-learners-from-mlr3,-mlr3learners-and-mlr3extralearners"]], "Using pipelines to construct learners": [[105, "using-pipelines-to-construct-learners"]], "Utility Classes": [[54, "utility-classes"]], "Utility Classes and Functions": [[54, null]], "Utility Functions": [[54, "utility-functions"]], "VanderWeele and Arah (2011): Benchmarking": [[93, "VanderWeele-and-Arah-(2011):-Benchmarking"]], "Variance estimation": [[136, "variance-estimation"]], "Variance estimation and confidence intervals": [[136, null]], "Visualizing Average Potential Outcomes": [[70, "Visualizing-Average-Potential-Outcomes"]], "Visualizing Average Treatment Effects": [[70, "Visualizing-Average-Treatment-Effects"]], "Visualizing the Treatment Effect Structure": [[70, "Visualizing-the-Treatment-Effect-Structure"]], "Weighted Average Treatment Effects": [[104, "weighted-average-treatment-effects"]], "doubleml.data.DoubleMLClusterData": [[4, null]], "doubleml.data.DoubleMLDIDData": [[5, null]], "doubleml.data.DoubleMLData": [[6, null]], "doubleml.data.DoubleMLPanelData": [[7, null]], "doubleml.data.DoubleMLRDDData": [[8, null]], "doubleml.data.DoubleMLSSMData": [[9, null]], "doubleml.datasets.fetch_401K": [[10, null]], "doubleml.datasets.fetch_bonus": [[11, null]], "doubleml.did": [[3, "doubleml-did"]], "doubleml.did.DoubleMLDID": [[12, null]], "doubleml.did.DoubleMLDIDAggregation": [[13, null]], "doubleml.did.DoubleMLDIDBinary": [[14, null]], "doubleml.did.DoubleMLDIDCS": [[15, null]], "doubleml.did.DoubleMLDIDMulti": [[16, null]], "doubleml.did.datasets.make_did_CS2021": [[17, null]], "doubleml.did.datasets.make_did_SZ2020": [[18, null]], "doubleml.did.datasets.make_did_cs_CS2021": [[19, null]], "doubleml.double_ml_score_mixins.LinearScoreMixin": [[20, null]], "doubleml.double_ml_score_mixins.NonLinearScoreMixin": [[21, null]], "doubleml.irm": [[3, "doubleml-irm"]], "doubleml.irm.DoubleMLAPO": [[22, null]], "doubleml.irm.DoubleMLAPOS": [[23, null]], "doubleml.irm.DoubleMLCVAR": [[24, null]], "doubleml.irm.DoubleMLIIVM": [[25, null]], "doubleml.irm.DoubleMLIRM": [[26, null]], "doubleml.irm.DoubleMLLPQ": [[27, null]], "doubleml.irm.DoubleMLPQ": [[28, null]], "doubleml.irm.DoubleMLQTE": [[29, null]], "doubleml.irm.DoubleMLSSM": [[30, null]], "doubleml.irm.datasets.make_confounded_irm_data": [[31, null]], "doubleml.irm.datasets.make_heterogeneous_data": [[32, null]], "doubleml.irm.datasets.make_iivm_data": [[33, null]], "doubleml.irm.datasets.make_irm_data": [[34, null]], "doubleml.irm.datasets.make_irm_data_discrete_treatments": [[35, null]], "doubleml.irm.datasets.make_ssm_data": [[36, null]], "doubleml.plm": [[3, "doubleml-plm"]], "doubleml.plm.DoubleMLPLIV": [[37, null]], "doubleml.plm.DoubleMLPLR": [[38, null]], "doubleml.plm.datasets.make_confounded_plr_data": [[39, null]], "doubleml.plm.datasets.make_pliv_CHS2015": [[40, null]], "doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021": [[41, null]], "doubleml.plm.datasets.make_plr_CCDDHNR2018": [[42, null]], "doubleml.plm.datasets.make_plr_turrell2018": [[43, null]], "doubleml.rdd": [[3, "doubleml-rdd"]], "doubleml.rdd.RDFlex": [[44, null]], "doubleml.rdd.datasets.make_simple_rdd_data": [[45, null]], "doubleml.utils.DMLDummyClassifier": [[46, null]], "doubleml.utils.DMLDummyRegressor": [[47, null]], "doubleml.utils.DoubleMLBLP": [[48, null]], "doubleml.utils.DoubleMLPolicyTree": [[49, null]], "doubleml.utils.GlobalClassifier": [[50, null]], "doubleml.utils.GlobalRegressor": [[51, null]], "doubleml.utils.gain_statistics": [[52, null]]}, "docnames": ["api/api", "api/data_class", "api/datasets", "api/dml_models", "api/generated/doubleml.data.DoubleMLClusterData", "api/generated/doubleml.data.DoubleMLDIDData", "api/generated/doubleml.data.DoubleMLData", "api/generated/doubleml.data.DoubleMLPanelData", "api/generated/doubleml.data.DoubleMLRDDData", "api/generated/doubleml.data.DoubleMLSSMData", "api/generated/doubleml.datasets.fetch_401K", "api/generated/doubleml.datasets.fetch_bonus", "api/generated/doubleml.did.DoubleMLDID", "api/generated/doubleml.did.DoubleMLDIDAggregation", "api/generated/doubleml.did.DoubleMLDIDBinary", "api/generated/doubleml.did.DoubleMLDIDCS", "api/generated/doubleml.did.DoubleMLDIDMulti", "api/generated/doubleml.did.datasets.make_did_CS2021", "api/generated/doubleml.did.datasets.make_did_SZ2020", "api/generated/doubleml.did.datasets.make_did_cs_CS2021", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin", "api/generated/doubleml.irm.DoubleMLAPO", "api/generated/doubleml.irm.DoubleMLAPOS", "api/generated/doubleml.irm.DoubleMLCVAR", "api/generated/doubleml.irm.DoubleMLIIVM", "api/generated/doubleml.irm.DoubleMLIRM", "api/generated/doubleml.irm.DoubleMLLPQ", "api/generated/doubleml.irm.DoubleMLPQ", "api/generated/doubleml.irm.DoubleMLQTE", "api/generated/doubleml.irm.DoubleMLSSM", "api/generated/doubleml.irm.datasets.make_confounded_irm_data", "api/generated/doubleml.irm.datasets.make_heterogeneous_data", "api/generated/doubleml.irm.datasets.make_iivm_data", "api/generated/doubleml.irm.datasets.make_irm_data", "api/generated/doubleml.irm.datasets.make_irm_data_discrete_treatments", "api/generated/doubleml.irm.datasets.make_ssm_data", "api/generated/doubleml.plm.DoubleMLPLIV", "api/generated/doubleml.plm.DoubleMLPLR", "api/generated/doubleml.plm.datasets.make_confounded_plr_data", "api/generated/doubleml.plm.datasets.make_pliv_CHS2015", "api/generated/doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021", "api/generated/doubleml.plm.datasets.make_plr_CCDDHNR2018", "api/generated/doubleml.plm.datasets.make_plr_turrell2018", "api/generated/doubleml.rdd.RDFlex", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data", "api/generated/doubleml.utils.DMLDummyClassifier", "api/generated/doubleml.utils.DMLDummyRegressor", "api/generated/doubleml.utils.DoubleMLBLP", "api/generated/doubleml.utils.DoubleMLPolicyTree", "api/generated/doubleml.utils.GlobalClassifier", "api/generated/doubleml.utils.GlobalRegressor", "api/generated/doubleml.utils.gain_statistics", "api/mixins", "api/utility", "examples/R_double_ml_basic_iv", "examples/R_double_ml_basics", "examples/R_double_ml_did", "examples/R_double_ml_multiway_cluster", "examples/R_double_ml_pension", "examples/R_double_ml_pipeline", "examples/R_double_ml_ssm", "examples/did/py_did", "examples/did/py_did_pretest", "examples/did/py_panel", "examples/did/py_panel_data_example", "examples/did/py_panel_simple", "examples/did/py_rep_cs", "examples/double_ml_bonus_data", "examples/index", "examples/learners/py_tabpfn", "examples/py_double_ml_apo", "examples/py_double_ml_basic_iv", "examples/py_double_ml_basics", "examples/py_double_ml_cate", "examples/py_double_ml_cate_plr", "examples/py_double_ml_cvar", "examples/py_double_ml_firststage", "examples/py_double_ml_gate", "examples/py_double_ml_gate_plr", "examples/py_double_ml_gate_sensitivity", "examples/py_double_ml_irm_vs_apo", "examples/py_double_ml_learner", "examples/py_double_ml_meets_flaml", "examples/py_double_ml_multiway_cluster", "examples/py_double_ml_pension", "examples/py_double_ml_pension_qte", "examples/py_double_ml_plm_irm_hetfx", "examples/py_double_ml_policy_tree", "examples/py_double_ml_pq", "examples/py_double_ml_rdflex", "examples/py_double_ml_robust_iv", "examples/py_double_ml_sensitivity", "examples/py_double_ml_sensitivity_booking", "examples/py_double_ml_ssm", "guide/algorithms", "guide/basics", "guide/data/base_data", "guide/data/did_data", "guide/data/panel_data", "guide/data/rdd_data", "guide/data/ssm_data", "guide/data_backend", "guide/guide", "guide/heterogeneity", "guide/learners", "guide/models", "guide/models/did/did_aggregation", "guide/models/did/did_binary", "guide/models/did/did_cs", "guide/models/did/did_implementation", "guide/models/did/did_pa", "guide/models/did/did_setup", "guide/models/irm/apo", "guide/models/irm/apos", "guide/models/irm/iivm", "guide/models/irm/irm", "guide/models/plm/pliv", "guide/models/plm/plr", "guide/models/ssm/ssm", "guide/resampling", "guide/scores", "guide/scores/did/did_cs_binary_score", "guide/scores/did/did_cs_score", "guide/scores/did/did_pa_binary_score", "guide/scores/did/did_pa_score", "guide/scores/irm/apo_score", "guide/scores/irm/cvar_score", "guide/scores/irm/iivm_score", "guide/scores/irm/irm_score", "guide/scores/irm/lpq_score", "guide/scores/irm/pq_score", "guide/scores/plm/pliv_score", "guide/scores/plm/plr_score", "guide/scores/ssm/mar_score", "guide/scores/ssm/nr_score", "guide/se_confint", "guide/sensitivity", "guide/sensitivity/benchmarking", "guide/sensitivity/did/did_cs_binary_sensitivity", "guide/sensitivity/did/did_cs_sensitivity", "guide/sensitivity/did/did_pa_binary_sensitivity", "guide/sensitivity/did/did_pa_sensitivity", "guide/sensitivity/implementation", "guide/sensitivity/irm/apo_sensitivity", "guide/sensitivity/irm/irm_sensitivity", "guide/sensitivity/plm/plr_sensitivity", "guide/sensitivity/theory", "index", "intro/install", "intro/intro", "literature/literature", "release/release", "workflow/workflow"], "envversion": {"nbsphinx": 4, "sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1}, "filenames": ["api/api.rst", "api/data_class.rst", "api/datasets.rst", "api/dml_models.rst", "api/generated/doubleml.data.DoubleMLClusterData.rst", "api/generated/doubleml.data.DoubleMLDIDData.rst", "api/generated/doubleml.data.DoubleMLData.rst", "api/generated/doubleml.data.DoubleMLPanelData.rst", "api/generated/doubleml.data.DoubleMLRDDData.rst", "api/generated/doubleml.data.DoubleMLSSMData.rst", "api/generated/doubleml.datasets.fetch_401K.rst", "api/generated/doubleml.datasets.fetch_bonus.rst", "api/generated/doubleml.did.DoubleMLDID.rst", "api/generated/doubleml.did.DoubleMLDIDAggregation.rst", "api/generated/doubleml.did.DoubleMLDIDBinary.rst", "api/generated/doubleml.did.DoubleMLDIDCS.rst", "api/generated/doubleml.did.DoubleMLDIDMulti.rst", "api/generated/doubleml.did.datasets.make_did_CS2021.rst", "api/generated/doubleml.did.datasets.make_did_SZ2020.rst", "api/generated/doubleml.did.datasets.make_did_cs_CS2021.rst", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin.rst", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin.rst", "api/generated/doubleml.irm.DoubleMLAPO.rst", "api/generated/doubleml.irm.DoubleMLAPOS.rst", "api/generated/doubleml.irm.DoubleMLCVAR.rst", "api/generated/doubleml.irm.DoubleMLIIVM.rst", "api/generated/doubleml.irm.DoubleMLIRM.rst", "api/generated/doubleml.irm.DoubleMLLPQ.rst", "api/generated/doubleml.irm.DoubleMLPQ.rst", "api/generated/doubleml.irm.DoubleMLQTE.rst", "api/generated/doubleml.irm.DoubleMLSSM.rst", "api/generated/doubleml.irm.datasets.make_confounded_irm_data.rst", "api/generated/doubleml.irm.datasets.make_heterogeneous_data.rst", "api/generated/doubleml.irm.datasets.make_iivm_data.rst", "api/generated/doubleml.irm.datasets.make_irm_data.rst", "api/generated/doubleml.irm.datasets.make_irm_data_discrete_treatments.rst", "api/generated/doubleml.irm.datasets.make_ssm_data.rst", "api/generated/doubleml.plm.DoubleMLPLIV.rst", "api/generated/doubleml.plm.DoubleMLPLR.rst", "api/generated/doubleml.plm.datasets.make_confounded_plr_data.rst", "api/generated/doubleml.plm.datasets.make_pliv_CHS2015.rst", "api/generated/doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021.rst", "api/generated/doubleml.plm.datasets.make_plr_CCDDHNR2018.rst", "api/generated/doubleml.plm.datasets.make_plr_turrell2018.rst", "api/generated/doubleml.rdd.RDFlex.rst", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data.rst", "api/generated/doubleml.utils.DMLDummyClassifier.rst", "api/generated/doubleml.utils.DMLDummyRegressor.rst", "api/generated/doubleml.utils.DoubleMLBLP.rst", "api/generated/doubleml.utils.DoubleMLPolicyTree.rst", "api/generated/doubleml.utils.GlobalClassifier.rst", "api/generated/doubleml.utils.GlobalRegressor.rst", "api/generated/doubleml.utils.gain_statistics.rst", "api/mixins.rst", "api/utility.rst", "examples/R_double_ml_basic_iv.ipynb", "examples/R_double_ml_basics.ipynb", "examples/R_double_ml_did.ipynb", "examples/R_double_ml_multiway_cluster.ipynb", "examples/R_double_ml_pension.ipynb", "examples/R_double_ml_pipeline.ipynb", "examples/R_double_ml_ssm.ipynb", "examples/did/py_did.ipynb", "examples/did/py_did_pretest.ipynb", "examples/did/py_panel.ipynb", "examples/did/py_panel_data_example.ipynb", "examples/did/py_panel_simple.ipynb", "examples/did/py_rep_cs.ipynb", "examples/double_ml_bonus_data.ipynb", "examples/index.rst", "examples/learners/py_tabpfn.ipynb", "examples/py_double_ml_apo.ipynb", "examples/py_double_ml_basic_iv.ipynb", "examples/py_double_ml_basics.ipynb", "examples/py_double_ml_cate.ipynb", "examples/py_double_ml_cate_plr.ipynb", "examples/py_double_ml_cvar.ipynb", "examples/py_double_ml_firststage.ipynb", "examples/py_double_ml_gate.ipynb", "examples/py_double_ml_gate_plr.ipynb", "examples/py_double_ml_gate_sensitivity.ipynb", "examples/py_double_ml_irm_vs_apo.ipynb", "examples/py_double_ml_learner.ipynb", "examples/py_double_ml_meets_flaml.ipynb", "examples/py_double_ml_multiway_cluster.ipynb", "examples/py_double_ml_pension.ipynb", "examples/py_double_ml_pension_qte.ipynb", "examples/py_double_ml_plm_irm_hetfx.ipynb", "examples/py_double_ml_policy_tree.ipynb", "examples/py_double_ml_pq.ipynb", "examples/py_double_ml_rdflex.ipynb", "examples/py_double_ml_robust_iv.ipynb", "examples/py_double_ml_sensitivity.ipynb", "examples/py_double_ml_sensitivity_booking.ipynb", "examples/py_double_ml_ssm.ipynb", "guide/algorithms.rst", "guide/basics.rst", "guide/data/base_data.rst", "guide/data/did_data.rst", "guide/data/panel_data.rst", "guide/data/rdd_data.rst", "guide/data/ssm_data.rst", "guide/data_backend.rst", "guide/guide.rst", "guide/heterogeneity.rst", "guide/learners.rst", "guide/models.rst", "guide/models/did/did_aggregation.rst", "guide/models/did/did_binary.rst", "guide/models/did/did_cs.rst", "guide/models/did/did_implementation.rst", "guide/models/did/did_pa.rst", "guide/models/did/did_setup.rst", "guide/models/irm/apo.rst", "guide/models/irm/apos.rst", "guide/models/irm/iivm.rst", "guide/models/irm/irm.rst", "guide/models/plm/pliv.rst", "guide/models/plm/plr.rst", "guide/models/ssm/ssm.rst", "guide/resampling.rst", "guide/scores.rst", "guide/scores/did/did_cs_binary_score.rst", "guide/scores/did/did_cs_score.rst", "guide/scores/did/did_pa_binary_score.rst", "guide/scores/did/did_pa_score.rst", "guide/scores/irm/apo_score.rst", "guide/scores/irm/cvar_score.rst", "guide/scores/irm/iivm_score.rst", "guide/scores/irm/irm_score.rst", "guide/scores/irm/lpq_score.rst", "guide/scores/irm/pq_score.rst", "guide/scores/plm/pliv_score.rst", "guide/scores/plm/plr_score.rst", "guide/scores/ssm/mar_score.rst", "guide/scores/ssm/nr_score.rst", "guide/se_confint.rst", "guide/sensitivity.rst", "guide/sensitivity/benchmarking.rst", "guide/sensitivity/did/did_cs_binary_sensitivity.rst", "guide/sensitivity/did/did_cs_sensitivity.rst", "guide/sensitivity/did/did_pa_binary_sensitivity.rst", "guide/sensitivity/did/did_pa_sensitivity.rst", "guide/sensitivity/implementation.rst", "guide/sensitivity/irm/apo_sensitivity.rst", "guide/sensitivity/irm/irm_sensitivity.rst", "guide/sensitivity/plm/plr_sensitivity.rst", "guide/sensitivity/theory.rst", "index.rst", "intro/install.rst", "intro/intro.rst", "literature/literature.rst", "release/release.rst", "workflow/workflow.rst"], "indexentries": {"aggregate() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.aggregate", false]], "aggregate_over_splits() (doubleml.rdd.rdflex method)": [[44, "doubleml.rdd.RDFlex.aggregate_over_splits", false]], "bootstrap() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.bootstrap", false]], "bootstrap() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.bootstrap", false]], "capo() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.capo", false]], "cate() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.cate", false]], "cate() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.cate", false]], "causal_contrast() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.causal_contrast", false]], "confint() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.confint", false]], "confint() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.confint", false]], "confint() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.confint", false]], "confint() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.confint", false]], "confint() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.confint", false]], "confint() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.confint", false]], "confint() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.confint", false]], "confint() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.confint", false]], "confint() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.confint", false]], "confint() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.confint", false]], "confint() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.confint", false]], "confint() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.confint", false]], "confint() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.confint", false]], "confint() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.confint", false]], "confint() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.confint", false]], "confint() (doubleml.rdd.rdflex method)": [[44, "doubleml.rdd.RDFlex.confint", false]], "confint() (doubleml.utils.doublemlblp method)": [[48, "doubleml.utils.DoubleMLBLP.confint", false]], "construct_framework() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.construct_framework", false]], "construct_framework() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.construct_framework", false]], "dmldummyclassifier (class in doubleml.utils)": [[46, "doubleml.utils.DMLDummyClassifier", false]], "dmldummyregressor (class in doubleml.utils)": [[47, "doubleml.utils.DMLDummyRegressor", false]], "doublemlapo (class in doubleml.irm)": [[22, "doubleml.irm.DoubleMLAPO", false]], "doublemlapos (class in doubleml.irm)": [[23, "doubleml.irm.DoubleMLAPOS", false]], "doublemlblp (class in doubleml.utils)": [[48, "doubleml.utils.DoubleMLBLP", false]], "doublemlclusterdata (class in doubleml.data)": [[4, "doubleml.data.DoubleMLClusterData", false]], "doublemlcvar (class in doubleml.irm)": [[24, "doubleml.irm.DoubleMLCVAR", false]], "doublemldata (class in doubleml.data)": [[6, "doubleml.data.DoubleMLData", false]], "doublemldid (class in doubleml.did)": [[12, "doubleml.did.DoubleMLDID", false]], "doublemldidaggregation (class in doubleml.did)": [[13, "doubleml.did.DoubleMLDIDAggregation", false]], "doublemldidbinary (class in doubleml.did)": [[14, "doubleml.did.DoubleMLDIDBinary", false]], "doublemldidcs (class in doubleml.did)": [[15, "doubleml.did.DoubleMLDIDCS", false]], "doublemldiddata (class in doubleml.data)": [[5, "doubleml.data.DoubleMLDIDData", false]], "doublemldidmulti (class in doubleml.did)": [[16, "doubleml.did.DoubleMLDIDMulti", false]], "doublemliivm (class in doubleml.irm)": [[25, "doubleml.irm.DoubleMLIIVM", false]], "doublemlirm (class in doubleml.irm)": [[26, "doubleml.irm.DoubleMLIRM", false]], "doublemllpq (class in doubleml.irm)": [[27, "doubleml.irm.DoubleMLLPQ", false]], "doublemlpaneldata (class in doubleml.data)": [[7, "doubleml.data.DoubleMLPanelData", false]], "doublemlpliv (class in doubleml.plm)": [[37, "doubleml.plm.DoubleMLPLIV", false]], "doublemlplr (class in doubleml.plm)": [[38, "doubleml.plm.DoubleMLPLR", false]], "doublemlpolicytree (class in doubleml.utils)": [[49, "doubleml.utils.DoubleMLPolicyTree", false]], "doublemlpq (class in doubleml.irm)": [[28, "doubleml.irm.DoubleMLPQ", false]], "doublemlqte (class in doubleml.irm)": [[29, "doubleml.irm.DoubleMLQTE", false]], "doublemlrdddata (class in doubleml.data)": [[8, "doubleml.data.DoubleMLRDDData", false]], "doublemlssm (class in doubleml.irm)": [[30, "doubleml.irm.DoubleMLSSM", false]], "doublemlssmdata (class in doubleml.data)": [[9, "doubleml.data.DoubleMLSSMData", false]], "draw_sample_splitting() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.draw_sample_splitting", false]], "evaluate_learners() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.evaluate_learners", false]], "fetch_401k() (in module doubleml.datasets)": [[10, "doubleml.datasets.fetch_401K", false]], "fetch_bonus() (in module doubleml.datasets)": [[11, "doubleml.datasets.fetch_bonus", false]], "fit() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.fit", false]], "fit() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.fit", false]], "fit() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.fit", false]], "fit() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.fit", false]], "fit() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.fit", false]], "fit() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.fit", false]], "fit() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.fit", false]], "fit() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.fit", false]], "fit() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.fit", false]], "fit() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.fit", false]], "fit() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.fit", false]], "fit() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.fit", false]], "fit() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.fit", false]], "fit() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.fit", false]], "fit() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.fit", false]], "fit() (doubleml.rdd.rdflex method)": [[44, "doubleml.rdd.RDFlex.fit", false]], "fit() (doubleml.utils.dmldummyclassifier method)": [[46, "doubleml.utils.DMLDummyClassifier.fit", false]], "fit() (doubleml.utils.dmldummyregressor method)": [[47, "doubleml.utils.DMLDummyRegressor.fit", false]], "fit() (doubleml.utils.doublemlblp method)": [[48, "doubleml.utils.DoubleMLBLP.fit", false]], "fit() (doubleml.utils.doublemlpolicytree method)": [[49, "doubleml.utils.DoubleMLPolicyTree.fit", false]], "fit() (doubleml.utils.globalclassifier method)": [[50, "doubleml.utils.GlobalClassifier.fit", false]], "fit() (doubleml.utils.globalregressor method)": [[51, "doubleml.utils.GlobalRegressor.fit", false]], "from_arrays() (doubleml.data.doublemlclusterdata class method)": [[4, "doubleml.data.DoubleMLClusterData.from_arrays", false]], "from_arrays() (doubleml.data.doublemldata class method)": [[6, "doubleml.data.DoubleMLData.from_arrays", false]], "from_arrays() (doubleml.data.doublemldiddata class method)": [[5, "doubleml.data.DoubleMLDIDData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlpaneldata class method)": [[7, "doubleml.data.DoubleMLPanelData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlrdddata class method)": [[8, "doubleml.data.DoubleMLRDDData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlssmdata class method)": [[9, "doubleml.data.DoubleMLSSMData.from_arrays", false]], "gain_statistics() (in module doubleml.utils)": [[52, "doubleml.utils.gain_statistics", false]], "gapo() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.gapo", false]], "gate() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.gate", false]], "gate() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.gate", false]], "get_metadata_routing() (doubleml.utils.dmldummyclassifier method)": [[46, "doubleml.utils.DMLDummyClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.dmldummyregressor method)": [[47, "doubleml.utils.DMLDummyRegressor.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalclassifier method)": [[50, "doubleml.utils.GlobalClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalregressor method)": [[51, "doubleml.utils.GlobalRegressor.get_metadata_routing", false]], "get_params() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.get_params", false]], "get_params() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.get_params", false]], "get_params() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.get_params", false]], "get_params() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.get_params", false]], "get_params() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.get_params", false]], "get_params() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.get_params", false]], "get_params() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.get_params", false]], "get_params() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.get_params", false]], "get_params() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.get_params", false]], "get_params() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.get_params", false]], "get_params() (doubleml.utils.dmldummyclassifier method)": [[46, "doubleml.utils.DMLDummyClassifier.get_params", false]], "get_params() (doubleml.utils.dmldummyregressor method)": [[47, "doubleml.utils.DMLDummyRegressor.get_params", false]], "get_params() (doubleml.utils.globalclassifier method)": [[50, "doubleml.utils.GlobalClassifier.get_params", false]], "get_params() (doubleml.utils.globalregressor method)": [[51, "doubleml.utils.GlobalRegressor.get_params", false]], "globalclassifier (class in doubleml.utils)": [[50, "doubleml.utils.GlobalClassifier", false]], "globalregressor (class in doubleml.utils)": [[51, "doubleml.utils.GlobalRegressor", false]], "linearscoremixin (class in doubleml.double_ml_score_mixins)": [[20, "doubleml.double_ml_score_mixins.LinearScoreMixin", false]], "make_confounded_irm_data() (in module doubleml.irm.datasets)": [[31, "doubleml.irm.datasets.make_confounded_irm_data", false]], "make_confounded_plr_data() (in module doubleml.plm.datasets)": [[39, "doubleml.plm.datasets.make_confounded_plr_data", false]], "make_did_cs2021() (in module doubleml.did.datasets)": [[17, "doubleml.did.datasets.make_did_CS2021", false]], "make_did_cs_cs2021() (in module doubleml.did.datasets)": [[19, "doubleml.did.datasets.make_did_cs_CS2021", false]], "make_did_sz2020() (in module doubleml.did.datasets)": [[18, "doubleml.did.datasets.make_did_SZ2020", false]], "make_heterogeneous_data() (in module doubleml.irm.datasets)": [[32, "doubleml.irm.datasets.make_heterogeneous_data", false]], "make_iivm_data() (in module doubleml.irm.datasets)": [[33, "doubleml.irm.datasets.make_iivm_data", false]], "make_irm_data() (in module doubleml.irm.datasets)": [[34, "doubleml.irm.datasets.make_irm_data", false]], "make_irm_data_discrete_treatments() (in module doubleml.irm.datasets)": [[35, "doubleml.irm.datasets.make_irm_data_discrete_treatments", false]], "make_pliv_chs2015() (in module doubleml.plm.datasets)": [[40, "doubleml.plm.datasets.make_pliv_CHS2015", false]], "make_pliv_multiway_cluster_ckms2021() (in module doubleml.plm.datasets)": [[41, "doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021", false]], "make_plr_ccddhnr2018() (in module doubleml.plm.datasets)": [[42, "doubleml.plm.datasets.make_plr_CCDDHNR2018", false]], "make_plr_turrell2018() (in module doubleml.plm.datasets)": [[43, "doubleml.plm.datasets.make_plr_turrell2018", false]], "make_simple_rdd_data() (in module doubleml.rdd.datasets)": [[45, "doubleml.rdd.datasets.make_simple_rdd_data", false]], "make_ssm_data() (in module doubleml.irm.datasets)": [[36, "doubleml.irm.datasets.make_ssm_data", false]], "nonlinearscoremixin (class in doubleml.double_ml_score_mixins)": [[21, "doubleml.double_ml_score_mixins.NonLinearScoreMixin", false]], "p_adjust() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.p_adjust", false]], "p_adjust() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.p_adjust", false]], "plot_effects() (doubleml.did.doublemldidaggregation method)": [[13, "doubleml.did.DoubleMLDIDAggregation.plot_effects", false]], "plot_effects() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.plot_effects", false]], "plot_tree() (doubleml.utils.doublemlpolicytree method)": [[49, "doubleml.utils.DoubleMLPolicyTree.plot_tree", false]], "policy_tree() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.policy_tree", false]], "predict() (doubleml.utils.dmldummyclassifier method)": [[46, "doubleml.utils.DMLDummyClassifier.predict", false]], "predict() (doubleml.utils.dmldummyregressor method)": [[47, "doubleml.utils.DMLDummyRegressor.predict", false]], "predict() (doubleml.utils.doublemlpolicytree method)": [[49, "doubleml.utils.DoubleMLPolicyTree.predict", false]], "predict() (doubleml.utils.globalclassifier method)": [[50, "doubleml.utils.GlobalClassifier.predict", false]], "predict() (doubleml.utils.globalregressor method)": [[51, "doubleml.utils.GlobalRegressor.predict", false]], "predict_proba() (doubleml.utils.dmldummyclassifier method)": [[46, "doubleml.utils.DMLDummyClassifier.predict_proba", false]], "predict_proba() (doubleml.utils.globalclassifier method)": [[50, "doubleml.utils.GlobalClassifier.predict_proba", false]], "rdflex (class in doubleml.rdd)": [[44, "doubleml.rdd.RDFlex", false]], "robust_confset() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.robust_confset", false]], "score() (doubleml.utils.dmldummyclassifier method)": [[46, "doubleml.utils.DMLDummyClassifier.score", false]], "score() (doubleml.utils.dmldummyregressor method)": [[47, "doubleml.utils.DMLDummyRegressor.score", false]], "score() (doubleml.utils.globalclassifier method)": [[50, "doubleml.utils.GlobalClassifier.score", false]], "score() (doubleml.utils.globalregressor method)": [[51, "doubleml.utils.GlobalRegressor.score", false]], "sensitivity_analysis() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.sensitivity_analysis", false]], "sensitivity_benchmark() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.sensitivity_benchmark", false]], "sensitivity_plot() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.sensitivity_plot", false]], "set_fit_request() (doubleml.utils.globalclassifier method)": [[50, "doubleml.utils.GlobalClassifier.set_fit_request", false]], "set_fit_request() (doubleml.utils.globalregressor method)": [[51, "doubleml.utils.GlobalRegressor.set_fit_request", false]], "set_ml_nuisance_params() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.set_ml_nuisance_params", false]], "set_params() (doubleml.utils.dmldummyclassifier method)": [[46, "doubleml.utils.DMLDummyClassifier.set_params", false]], "set_params() (doubleml.utils.dmldummyregressor method)": [[47, "doubleml.utils.DMLDummyRegressor.set_params", false]], "set_params() (doubleml.utils.globalclassifier method)": [[50, "doubleml.utils.GlobalClassifier.set_params", false]], "set_params() (doubleml.utils.globalregressor method)": [[51, "doubleml.utils.GlobalRegressor.set_params", false]], "set_sample_splitting() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.set_sample_splitting", false]], "set_score_request() (doubleml.utils.dmldummyclassifier method)": [[46, "doubleml.utils.DMLDummyClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.dmldummyregressor method)": [[47, "doubleml.utils.DMLDummyRegressor.set_score_request", false]], "set_score_request() (doubleml.utils.globalclassifier method)": [[50, "doubleml.utils.GlobalClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.globalregressor method)": [[51, "doubleml.utils.GlobalRegressor.set_score_request", false]], "set_x_d() (doubleml.data.doublemlclusterdata method)": [[4, "doubleml.data.DoubleMLClusterData.set_x_d", false]], "set_x_d() (doubleml.data.doublemldata method)": [[6, "doubleml.data.DoubleMLData.set_x_d", false]], "set_x_d() (doubleml.data.doublemldiddata method)": [[5, "doubleml.data.DoubleMLDIDData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlpaneldata method)": [[7, "doubleml.data.DoubleMLPanelData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlrdddata method)": [[8, "doubleml.data.DoubleMLRDDData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlssmdata method)": [[9, "doubleml.data.DoubleMLSSMData.set_x_d", false]], "tune() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.tune", false]], "tune() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.tune", false]], "tune() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.tune", false]], "tune() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.tune", false]], "tune() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.tune", false]], "tune() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.tune", false]], "tune() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.tune", false]], "tune() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.tune", false]], "tune() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.tune", false]], "tune() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.tune", false]], "tune() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.tune", false]], "tune() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.tune", false]]}, "objects": {"doubleml.data": [[4, 0, 1, "", "DoubleMLClusterData"], [5, 0, 1, "", "DoubleMLDIDData"], [6, 0, 1, "", "DoubleMLData"], [7, 0, 1, "", "DoubleMLPanelData"], [8, 0, 1, "", "DoubleMLRDDData"], [9, 0, 1, "", "DoubleMLSSMData"]], "doubleml.data.DoubleMLClusterData": [[4, 1, 1, "", "from_arrays"], [4, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLDIDData": [[5, 1, 1, "", "from_arrays"], [5, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLData": [[6, 1, 1, "", "from_arrays"], [6, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLPanelData": [[7, 1, 1, "", "from_arrays"], [7, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLRDDData": [[8, 1, 1, "", "from_arrays"], [8, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLSSMData": [[9, 1, 1, "", "from_arrays"], [9, 1, 1, "", "set_x_d"]], "doubleml.datasets": [[10, 2, 1, "", "fetch_401K"], [11, 2, 1, "", "fetch_bonus"]], "doubleml.did": [[12, 0, 1, "", "DoubleMLDID"], [13, 0, 1, "", "DoubleMLDIDAggregation"], [14, 0, 1, "", "DoubleMLDIDBinary"], [15, 0, 1, "", "DoubleMLDIDCS"], [16, 0, 1, "", "DoubleMLDIDMulti"]], "doubleml.did.DoubleMLDID": [[12, 1, 1, "", "bootstrap"], [12, 1, 1, "", "confint"], [12, 1, 1, "", "construct_framework"], [12, 1, 1, "", "draw_sample_splitting"], [12, 1, 1, "", "evaluate_learners"], [12, 1, 1, "", "fit"], [12, 1, 1, "", "get_params"], [12, 1, 1, "", "p_adjust"], [12, 1, 1, "", "sensitivity_analysis"], [12, 1, 1, "", "sensitivity_benchmark"], [12, 1, 1, "", "sensitivity_plot"], [12, 1, 1, "", "set_ml_nuisance_params"], [12, 1, 1, "", "set_sample_splitting"], [12, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDAggregation": [[13, 1, 1, "", "plot_effects"]], "doubleml.did.DoubleMLDIDBinary": [[14, 1, 1, "", "bootstrap"], [14, 1, 1, "", "confint"], [14, 1, 1, "", "construct_framework"], [14, 1, 1, "", "draw_sample_splitting"], [14, 1, 1, "", "evaluate_learners"], [14, 1, 1, "", "fit"], [14, 1, 1, "", "get_params"], [14, 1, 1, "", "p_adjust"], [14, 1, 1, "", "sensitivity_analysis"], [14, 1, 1, "", "sensitivity_benchmark"], [14, 1, 1, "", "sensitivity_plot"], [14, 1, 1, "", "set_ml_nuisance_params"], [14, 1, 1, "", "set_sample_splitting"], [14, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDCS": [[15, 1, 1, "", "bootstrap"], [15, 1, 1, "", "confint"], [15, 1, 1, "", "construct_framework"], [15, 1, 1, "", "draw_sample_splitting"], [15, 1, 1, "", "evaluate_learners"], [15, 1, 1, "", "fit"], [15, 1, 1, "", "get_params"], [15, 1, 1, "", "p_adjust"], [15, 1, 1, "", "sensitivity_analysis"], [15, 1, 1, "", "sensitivity_benchmark"], [15, 1, 1, "", "sensitivity_plot"], [15, 1, 1, "", "set_ml_nuisance_params"], [15, 1, 1, "", "set_sample_splitting"], [15, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDMulti": [[16, 1, 1, "", "aggregate"], [16, 1, 1, "", "bootstrap"], [16, 1, 1, "", "confint"], [16, 1, 1, "", "fit"], [16, 1, 1, "", "p_adjust"], [16, 1, 1, "", "plot_effects"], [16, 1, 1, "", "sensitivity_analysis"], [16, 1, 1, "", "sensitivity_benchmark"], [16, 1, 1, "", "sensitivity_plot"]], "doubleml.did.datasets": [[17, 2, 1, "", "make_did_CS2021"], [18, 2, 1, "", "make_did_SZ2020"], [19, 2, 1, "", "make_did_cs_CS2021"]], "doubleml.double_ml_score_mixins": [[20, 0, 1, "", "LinearScoreMixin"], [21, 0, 1, "", "NonLinearScoreMixin"]], "doubleml.irm": [[22, 0, 1, "", "DoubleMLAPO"], [23, 0, 1, "", "DoubleMLAPOS"], [24, 0, 1, "", "DoubleMLCVAR"], [25, 0, 1, "", "DoubleMLIIVM"], [26, 0, 1, "", "DoubleMLIRM"], [27, 0, 1, "", "DoubleMLLPQ"], [28, 0, 1, "", "DoubleMLPQ"], [29, 0, 1, "", "DoubleMLQTE"], [30, 0, 1, "", "DoubleMLSSM"]], "doubleml.irm.DoubleMLAPO": [[22, 1, 1, "", "bootstrap"], [22, 1, 1, "", "capo"], [22, 1, 1, "", "confint"], [22, 1, 1, "", "construct_framework"], [22, 1, 1, "", "draw_sample_splitting"], [22, 1, 1, "", "evaluate_learners"], [22, 1, 1, "", "fit"], [22, 1, 1, "", "gapo"], [22, 1, 1, "", "get_params"], [22, 1, 1, "", "p_adjust"], [22, 1, 1, "", "sensitivity_analysis"], [22, 1, 1, "", "sensitivity_benchmark"], [22, 1, 1, "", "sensitivity_plot"], [22, 1, 1, "", "set_ml_nuisance_params"], [22, 1, 1, "", "set_sample_splitting"], [22, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLAPOS": [[23, 1, 1, "", "bootstrap"], [23, 1, 1, "", "causal_contrast"], [23, 1, 1, "", "confint"], [23, 1, 1, "", "draw_sample_splitting"], [23, 1, 1, "", "fit"], [23, 1, 1, "", "sensitivity_analysis"], [23, 1, 1, "", "sensitivity_benchmark"], [23, 1, 1, "", "sensitivity_plot"], [23, 1, 1, "", "set_sample_splitting"]], "doubleml.irm.DoubleMLCVAR": [[24, 1, 1, "", "bootstrap"], [24, 1, 1, "", "confint"], [24, 1, 1, "", "construct_framework"], [24, 1, 1, "", "draw_sample_splitting"], [24, 1, 1, "", "evaluate_learners"], [24, 1, 1, "", "fit"], [24, 1, 1, "", "get_params"], [24, 1, 1, "", "p_adjust"], [24, 1, 1, "", "sensitivity_analysis"], [24, 1, 1, "", "sensitivity_benchmark"], [24, 1, 1, "", "sensitivity_plot"], [24, 1, 1, "", "set_ml_nuisance_params"], [24, 1, 1, "", "set_sample_splitting"], [24, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLIIVM": [[25, 1, 1, "", "bootstrap"], [25, 1, 1, "", "confint"], [25, 1, 1, "", "construct_framework"], [25, 1, 1, "", "draw_sample_splitting"], [25, 1, 1, "", "evaluate_learners"], [25, 1, 1, "", "fit"], [25, 1, 1, "", "get_params"], [25, 1, 1, "", "p_adjust"], [25, 1, 1, "", "robust_confset"], [25, 1, 1, "", "sensitivity_analysis"], [25, 1, 1, "", "sensitivity_benchmark"], [25, 1, 1, "", "sensitivity_plot"], [25, 1, 1, "", "set_ml_nuisance_params"], [25, 1, 1, "", "set_sample_splitting"], [25, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLIRM": [[26, 1, 1, "", "bootstrap"], [26, 1, 1, "", "cate"], [26, 1, 1, "", "confint"], [26, 1, 1, "", "construct_framework"], [26, 1, 1, "", "draw_sample_splitting"], [26, 1, 1, "", "evaluate_learners"], [26, 1, 1, "", "fit"], [26, 1, 1, "", "gate"], [26, 1, 1, "", "get_params"], [26, 1, 1, "", "p_adjust"], [26, 1, 1, "", "policy_tree"], [26, 1, 1, "", "sensitivity_analysis"], [26, 1, 1, "", "sensitivity_benchmark"], [26, 1, 1, "", "sensitivity_plot"], [26, 1, 1, "", "set_ml_nuisance_params"], [26, 1, 1, "", "set_sample_splitting"], [26, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLLPQ": [[27, 1, 1, "", "bootstrap"], [27, 1, 1, "", "confint"], [27, 1, 1, "", "construct_framework"], [27, 1, 1, "", "draw_sample_splitting"], [27, 1, 1, "", "evaluate_learners"], [27, 1, 1, "", "fit"], [27, 1, 1, "", "get_params"], [27, 1, 1, "", "p_adjust"], [27, 1, 1, "", "sensitivity_analysis"], [27, 1, 1, "", "sensitivity_benchmark"], [27, 1, 1, "", "sensitivity_plot"], [27, 1, 1, "", "set_ml_nuisance_params"], [27, 1, 1, "", "set_sample_splitting"], [27, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLPQ": [[28, 1, 1, "", "bootstrap"], [28, 1, 1, "", "confint"], [28, 1, 1, "", "construct_framework"], [28, 1, 1, "", "draw_sample_splitting"], [28, 1, 1, "", "evaluate_learners"], [28, 1, 1, "", "fit"], [28, 1, 1, "", "get_params"], [28, 1, 1, "", "p_adjust"], [28, 1, 1, "", "sensitivity_analysis"], [28, 1, 1, "", "sensitivity_benchmark"], [28, 1, 1, "", "sensitivity_plot"], [28, 1, 1, "", "set_ml_nuisance_params"], [28, 1, 1, "", "set_sample_splitting"], [28, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLQTE": [[29, 1, 1, "", "bootstrap"], [29, 1, 1, "", "confint"], [29, 1, 1, "", "draw_sample_splitting"], [29, 1, 1, "", "fit"], [29, 1, 1, "", "p_adjust"], [29, 1, 1, "", "set_sample_splitting"]], "doubleml.irm.DoubleMLSSM": [[30, 1, 1, "", "bootstrap"], [30, 1, 1, "", "confint"], [30, 1, 1, "", "construct_framework"], [30, 1, 1, "", "draw_sample_splitting"], [30, 1, 1, "", "evaluate_learners"], [30, 1, 1, "", "fit"], [30, 1, 1, "", "get_params"], [30, 1, 1, "", "p_adjust"], [30, 1, 1, "", "sensitivity_analysis"], [30, 1, 1, "", "sensitivity_benchmark"], [30, 1, 1, "", "sensitivity_plot"], [30, 1, 1, "", "set_ml_nuisance_params"], [30, 1, 1, "", "set_sample_splitting"], [30, 1, 1, "", "tune"]], "doubleml.irm.datasets": [[31, 2, 1, "", "make_confounded_irm_data"], [32, 2, 1, "", "make_heterogeneous_data"], [33, 2, 1, "", "make_iivm_data"], [34, 2, 1, "", "make_irm_data"], [35, 2, 1, "", "make_irm_data_discrete_treatments"], [36, 2, 1, "", "make_ssm_data"]], "doubleml.plm": [[37, 0, 1, "", "DoubleMLPLIV"], [38, 0, 1, "", "DoubleMLPLR"]], "doubleml.plm.DoubleMLPLIV": [[37, 1, 1, "", "bootstrap"], [37, 1, 1, "", "confint"], [37, 1, 1, "", "construct_framework"], [37, 1, 1, "", "draw_sample_splitting"], [37, 1, 1, "", "evaluate_learners"], [37, 1, 1, "", "fit"], [37, 1, 1, "", "get_params"], [37, 1, 1, "", "p_adjust"], [37, 1, 1, "", "sensitivity_analysis"], [37, 1, 1, "", "sensitivity_benchmark"], [37, 1, 1, "", "sensitivity_plot"], [37, 1, 1, "", "set_ml_nuisance_params"], [37, 1, 1, "", "set_sample_splitting"], [37, 1, 1, "", "tune"]], "doubleml.plm.DoubleMLPLR": [[38, 1, 1, "", "bootstrap"], [38, 1, 1, "", "cate"], [38, 1, 1, "", "confint"], [38, 1, 1, "", "construct_framework"], [38, 1, 1, "", "draw_sample_splitting"], [38, 1, 1, "", "evaluate_learners"], [38, 1, 1, "", "fit"], [38, 1, 1, "", "gate"], [38, 1, 1, "", "get_params"], [38, 1, 1, "", "p_adjust"], [38, 1, 1, "", "sensitivity_analysis"], [38, 1, 1, "", "sensitivity_benchmark"], [38, 1, 1, "", "sensitivity_plot"], [38, 1, 1, "", "set_ml_nuisance_params"], [38, 1, 1, "", "set_sample_splitting"], [38, 1, 1, "", "tune"]], "doubleml.plm.datasets": [[39, 2, 1, "", "make_confounded_plr_data"], [40, 2, 1, "", "make_pliv_CHS2015"], [41, 2, 1, "", "make_pliv_multiway_cluster_CKMS2021"], [42, 2, 1, "", "make_plr_CCDDHNR2018"], [43, 2, 1, "", "make_plr_turrell2018"]], "doubleml.rdd": [[44, 0, 1, "", "RDFlex"]], "doubleml.rdd.RDFlex": [[44, 1, 1, "", "aggregate_over_splits"], [44, 1, 1, "", "confint"], [44, 1, 1, "", "fit"]], "doubleml.rdd.datasets": [[45, 2, 1, "", "make_simple_rdd_data"]], "doubleml.utils": [[46, 0, 1, "", "DMLDummyClassifier"], [47, 0, 1, "", "DMLDummyRegressor"], [48, 0, 1, "", "DoubleMLBLP"], [49, 0, 1, "", "DoubleMLPolicyTree"], [50, 0, 1, "", "GlobalClassifier"], [51, 0, 1, "", "GlobalRegressor"], [52, 2, 1, "", "gain_statistics"]], "doubleml.utils.DMLDummyClassifier": [[46, 1, 1, "", "fit"], [46, 1, 1, "", "get_metadata_routing"], [46, 1, 1, "", "get_params"], [46, 1, 1, "", "predict"], [46, 1, 1, "", "predict_proba"], [46, 1, 1, "", "score"], [46, 1, 1, "", "set_params"], [46, 1, 1, "", "set_score_request"]], "doubleml.utils.DMLDummyRegressor": [[47, 1, 1, "", "fit"], [47, 1, 1, "", "get_metadata_routing"], [47, 1, 1, "", "get_params"], [47, 1, 1, "", "predict"], [47, 1, 1, "", "score"], [47, 1, 1, "", "set_params"], [47, 1, 1, "", "set_score_request"]], "doubleml.utils.DoubleMLBLP": [[48, 1, 1, "", "confint"], [48, 1, 1, "", "fit"]], "doubleml.utils.DoubleMLPolicyTree": [[49, 1, 1, "", "fit"], [49, 1, 1, "", "plot_tree"], [49, 1, 1, "", "predict"]], "doubleml.utils.GlobalClassifier": [[50, 1, 1, "", "fit"], [50, 1, 1, "", "get_metadata_routing"], [50, 1, 1, "", "get_params"], [50, 1, 1, "", "predict"], [50, 1, 1, "", "predict_proba"], [50, 1, 1, "", "score"], [50, 1, 1, "", "set_fit_request"], [50, 1, 1, "", "set_params"], [50, 1, 1, "", "set_score_request"]], "doubleml.utils.GlobalRegressor": [[51, 1, 1, "", "fit"], [51, 1, 1, "", "get_metadata_routing"], [51, 1, 1, "", "get_params"], [51, 1, 1, "", "predict"], [51, 1, 1, "", "score"], [51, 1, 1, "", "set_fit_request"], [51, 1, 1, "", "set_params"], [51, 1, 1, "", "set_score_request"]]}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function"}, "terms": {"": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 44, 48, 50, 51, 55, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 70, 71, 77, 78, 79, 80, 82, 84, 85, 86, 90, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 105, 106, 108, 109, 111, 112, 119, 121, 134, 135, 136, 137, 138, 148, 150, 151, 152, 153], "0": [4, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 51, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 149, 150, 152], "00": [64, 67, 78, 79, 81, 85, 86, 120], "000": [91, 136, 153], "00000": 81, "000000": [64, 66, 67, 68, 71, 81, 85, 86, 97, 102, 104, 150], "0000000": 136, "0000000000000010000100": [60, 97, 102, 150], "000000e": [64, 67, 78, 79, 81, 85, 86], "000006": 71, "000017": 89, "000025": 84, "000034": 85, "000039": 84, "000063": 65, "000064": 72, "000067": 84, "000076": 106, "000091": 84, "0001": [65, 68, 85], "000107": 65, "000109": 65, "000148": 65, "0001953914": 120, "000219": [28, 104], "000242": [29, 104], "000320": 89, "00032016": 89, "000336": 67, "000341": 84, "000351": 67, "000383": 64, "000392": 64, "000411": 67, "000413": 67, "000441": 67, "000442": 84, "000448": 67, "00047580260495": 55, "000488": 84, "000492": 64, "000494": 80, "0005": 68, "000522": 84, "000531": 65, "000584": 67, "000589": 64, "000593": 65, "0005a80b528f": 60, "000612": 67, "000617": 64, "000626": 67, "000631": 65, "000637": 64, "000639": 64, "000670": 84, "000743": 92, "0008": 65, "000837": 65, "000915799": 136, "0009157990": 136, "000916": [98, 102], "000943": [74, 75], "001": [17, 19, 55, 57, 58, 59, 60, 61, 73, 105, 106, 120, 121, 136, 150, 153], "001049": [106, 111], "001051": 84, "001145": 86, "001244": 65, "001284": 64, "001301": 65, "00133": 60, "00138944": [95, 121], "001403": 90, "001443": 65, "001471": 81, "001494": [104, 105, 106], "001531": 65, "0016": [59, 65, 85], "001603": [106, 111], "001666": 65, "001698": 81, "001714": 104, "0018": [59, 85], "001835": 65, "0019": 68, "001907": 81, "002037900301454": 83, "002053": 65, "002169338": 136, "0021693380": 136, "0021693381": 136, "002257": 64, "002277": 74, "002290": 63, "0023": 57, "002436": 80, "002481": 65, "002537": 67, "0026": 68, "002601": [106, 107], "002779": 92, "0028": [57, 59, 65, 85], "002821": 93, "00282133350419121": 93, "00290774": [106, 109], "002965": 64, "002983": 84, "003": [18, 31, 39, 87, 91], "003045": 81, "003074": 106, "003134": 89, "003149": 64, "003187": 74, "003220": 71, "003224": 67, "0033": 65, "003310": 65, "003323": 66, "003327": 65, "003328": 89, "003357": 65, "003359": 67, "003398": 67, "0034": 77, "003404": 71, "003415": 71, "003427": 84, "003542": 65, "003607": 75, "003779": 80, "003836": 89, "0039": 65, "003924": 80, "003944": 74, "003962": 65, "003975": 74, "004003": 65, "004044": 65, "004052": 65, "00409412": [95, 121], "0042": [59, 85], "004218": 67, "004226": 65, "004245": 65, "004253": 71, "004313": 65, "004392": 80, "004457": 67, "004499": 65, "004526": 71, "004542": 81, "0046": 65, "004688": 25, "0047": [59, 85], "004846": 93, "005": 87, "005003": 64, "005104911": 120, "00518448": [106, 111], "005339": [74, 75], "005603": 67, "005857": 84, "005878": 65, "005e": 106, "006055": 71, "0060715124549546": 83, "006267": 75, "0065": 65, "006626": 65, "0067": 65, "006922": 68, "006958": [74, 75], "00728": 150, "0073": 68, "007421": 104, "007604": 67, "00778625": 120, "007789e": 83, "00789846": 64, "008": 93, "008023": 86, "008223": [74, 75], "008302": 65, "008487": 68, "008642": 104, "008774116": 120, "008825": 76, "008825189994473": 76, "008883698": 121, "00888458890362062": 95, "008884589": 95, "008dbd": 87, "008e80": 87, "009": [87, 93], "009031": 64, "009110": 64, "009122": 89, "009165": 67, "009171": [106, 109], "0092": 65, "009248": 65, "009255": 74, "009329847": 121, "009428": 76, "00944171905420782": 93, "00950122695463054": 95, "009501226954630540": 95, "009501227": 95, "009645422": 58, "009656": 89, "00972": 68, "009727": [106, 107], "009790": 86, "009840": 87, "009859": 64, "009986": 89, "01": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 49, 55, 58, 59, 60, 61, 64, 67, 74, 75, 81, 85, 86, 87, 88, 89, 90, 105, 106, 109, 111, 120, 121, 136, 150, 153], "010001": 87, "010045": 83, "010213": 92, "010269": 84, "010450": 58, "0105": 65, "01063747": 64, "01091843": [106, 109], "010940": 84, "011": 87, "011131": 89, "0112": 57, "011204": 81, "01128": 68, "011323": [106, 107], "011598": 89, "011651": 87, "0118095": 58, "011823": 92, "011826e": 67, "011838": 65, "011988e": 89, "012012": 65, "01219": 60, "0124105481660435": 83, "012413": 64, "012554": 64, "012600": 66, "012663": 67, "01274": 93, "012756": 67, "012824": 64, "012831": 93, "013034": 93, "013096": 64, "013128": 74, "013204": 87, "013301": 65, "013450": 81, "013473": 65, "01351638": 58, "013593": 92, "013677": 90, "013712": 74, "01398951": 58, "013990": 136, "014": 90, "01403089": 58, "014080": [74, 75], "014141e": 64, "014198": 65, "014275": 65, "014431": 86, "014432": 63, "014525": 70, "014531": 67, "014637": 84, "014681": 92, "014721": 86, "014873e": 74, "015": [60, 87], "015038": 76, "015066": 65, "0151": 65, "015165": 65, "015440": 67, "015496": 65, "0155": 65, "015552": 74, "015565": 89, "015634": 65, "015698": 89, "015703": 65, "01574297": 89, "015743": 89, "015807": 65, "015831": 74, "016011": 75, "016154": 84, "016200": [65, 74, 75], "01630715": 120, "0164": 65, "016423": 67, "01643": 151, "016432": 65, "016518": 86, "0167": 65, "016702": 78, "016703": 65, "016832": 67, "016857": 65, "017": 60, "01708441": 120, "017140": 74, "017185": 86, "01718716": 120, "017330": 67, "017393e": 136, "017443": 65, "017660": 81, "01772": 138, "017777e": 75, "017800092": 136, "0178000920": 136, "017861": 65, "0179": 65, "017904": 65, "017964": 65, "018": 60, "018092": 104, "018148": 89, "018223": 67, "018468": 65, "018496": 65, "0184989": [106, 109], "018508": 74, "018611": 66, "0187512020118494": 83, "01903": [60, 105, 148, 150], "01916030e": 120, "019176": 87, "019188": 65, "01925597": 58, "019439633": 136, "0194396330": 136, "0194396331": 136, "019464": 65, "019468": 65, "0195": 65, "019535": 65, "019596": 76, "019651": 65, "019660": [29, 104], "01984083": 64, "019891": 65, "01990373": 94, "019941": 65, "01996051": 120, "02": [64, 67, 74, 75, 85, 86, 87, 89, 104, 106, 109, 111, 120], "020123": 70, "02016117": 150, "020166": 89, "0202": 65, "020254": 65, "020271": 84, "020272": 81, "020360838": 136, "0203608380": 136, "0203608381": 136, "02052929": [95, 121], "02079162e": 120, "020819": 104, "02082084": 64, "02092": 150, "020943": 65, "021243": 64, "021269": [78, 79], "02163217": 58, "021679": 64, "021690": 79, "021719": 65, "021764": 67, "021842": 67, "021866": 88, "021926": 76, "022": 87, "022181": 74, "022258": 81, "022295e": 74, "022305": 65, "02247976": 58, "022511": 86, "02260304": [106, 111], "022633": 65, "022768": 68, "022783": 92, "022915": 84, "022950": 86, "023": 87, "023020e": [85, 86], "023052": 75, "023229": 65, "023256": 89, "023276": 65, "0233": 65, "023563": 136, "0236": 65, "023631": 67, "023960": 66, "024176": 64, "0242": 65, "024266": 81, "024346": 74, "024355": 63, "024364": 137, "024401": [78, 79], "024470": 65, "024604": 84, "02467": 91, "024782": 89, "024789": 64, "024805": 64, "024926": 63, "025": [74, 75, 78, 79, 81, 87], "025077": [75, 136], "0253": 60, "025300e": 75, "025407": 83, "025443": 68, "025476": 65, "025496": 74, "025616": 64, "025653": 65, "025675": 64, "0257": 57, "025813114": 136, "0258131140": 136, "02584": 60, "025841": 81, "025964": 81, "0260": 65, "026292": 70, "026662": 65, "026699": 64, "026723": 76, "026725": 65, "026948": 65, "026966": 81, "02699695": 82, "027196": 65, "027252": 65, "027348": 65, "027537": 67, "02791": 68, "0281": 60, "028146": 83, "028220": 67, "028279": 65, "028520": [74, 75], "028553": 65, "028630": [106, 107], "028642": 65, "028665": 65, "028731": 104, "028763": 65, "028776": 65, "028868": 86, "029": 87, "02900983": 89, "029010": 89, "029022": 75, "029209": 153, "029364": [137, 143], "0294": 65, "029772": 64, "029831": 89, "029910e": [85, 86], "02e": 59, "03": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 61, 64, 67, 71, 74, 75, 76, 80, 81, 85, 86, 89, 90, 92, 93, 106, 107, 109, 111, 120, 137, 143, 153], "030059": 104, "0301": 60, "030123": [106, 109], "03018": 27, "030329": 65, "030346": 150, "030366": 65, "0304": 65, "03045": 61, "0305": 65, "030683": 66, "0307": [60, 70], "030762": 65, "030934": 89, "030962": 89, "031075": 83, "031089": 65, "0311": 65, "03113": 94, "031134": 105, "031156": 75, "0312": 65, "031269": 68, "031323": 83, "031337": 67, "031491": 83, "031639": 89, "031692": 83, "031712": 86, "031801": 87, "031820": 75, "03191": 151, "031937": 65, "032009": 67, "032058": 64, "032147": 65, "03220": 152, "032279": 65, "0323": 57, "032317": 67, "03244552": 105, "0325": 150, "032565": 67, "032675": [106, 109], "032941": 74, "032953": 92, "033413": 67, "033442": 65, "0335": 65, "033523": 65, "033643": 65, "033661": 83, "033779": 83, "033946": [78, 79], "034": 55, "034065": 75, "03411": 150, "034195": 67, "034245": 66, "034257": 67, "034353": 65, "03438": 61, "0344": 65, "034690": 76, "034812763": 136, "0348127630": 136, "0348127631": 136, "034828": 65, "034846": 85, "03489": [41, 58, 84], "0349": 65, "035061": 66, "035119185": 136, "0351191850": 136, "0351191851": 136, "035210": 65, "035264": 75, "035265": 76, "035309": 65, "03534806": [106, 109], "03536": 150, "03538": 60, "03539": 60, "035391": 68, "0354": [60, 65], "035411": 150, "035441": 75, "03545": 60, "035501": 65, "035545": 68, "035559": 65, "035572": 68, "035689": [106, 111], "035730": 89, "03574": 68, "035748": 65, "035762": 89, "035785": 75, "0358": 65, "0359": 60, "035977": 87, "036129015": 136, "0361290150": 136, "0361290151": 136, "036143": 89, "036147": 89, "036185": 65, "0362": 65, "036240": 71, "036282": 67, "036729": 84, "0368": 57, "037008": [78, 79], "037114": 81, "0373": 70, "03733244": 120, "0374": 60, "037504": 75, "037509": 94, "037529": 81, "037577": [106, 107], "037747": [74, 75], "037757": 65, "03783666": 89, "037837": 89, "037874": 67, "03796621": 120, "038103": 81, "038174": 65, "038385": 64, "038425": 66, "038726": 66, "038812": 88, "038819": 65, "038831": 83, "038845": 74, "038987": 64, "039036": 74, "039086": 67, "039141": 71, "039154": 86, "03917696": [121, 136], "0392": 65, "03920960e": 120, "039252": 65, "039302e": 76, "039661": 81, "0397": 65, "039895": 81, "039989": 64, "039991e": 74, "04": [39, 44, 59, 64, 67, 71, 74, 75, 85, 86, 89, 90, 92, 106, 107, 109, 111, 120, 153], "040010": 81, "040079": 75, "040112": 136, "040139": [67, 74, 75], "0402": 65, "040488": 66, "040533": [121, 136], "04053339": 136, "040562": 74, "040564": 65, "040629": 38, "040688": 74, "040784": 71, "0408": 74, "0409": 65, "040912": 75, "040919": 75, "041147": 76, "041177": 87, "041229": 65, "041262": 83, "041284": 76, "0413": 65, "041387": 76, "041472": 64, "041491e": 76, "04165": 106, "041663": 65, "041795": 65, "0418": 57, "041831": 76, "041925": 74, "042034": 93, "042060": 83, "042249": [65, 75], "042265": 76, "042291": 64, "042428": 65, "0425": 105, "042559": 65, "042583": 86, "0428": 94, "042804": 81, "042844e": 89, "043026": 64, "043082": [106, 111], "043092": 65, "043152": 87, "04322371": 120, "0433": 57, "043407": 65, "0434e374": 60, "043694": 65, "043769": 70, "043853": 65, "04387": 105, "0439": 65, "043955": 64, "043998": 75, "044035": 65, "044113": 76, "04415": 60, "044176": 81, "044239": 81, "04424": 60, "044447": [98, 102], "04444978": 136, "044449780": 136, "044467": 64, "0445": 105, "044595": 65, "04465": 58, "044704": 75, "04486": 150, "04487585": [137, 143], "04491": 106, "044929": 81, "04497975": [137, 143], "044982": 66, "045": 87, "04501612": 136, "04502": [105, 121, 136], "045144": 84, "045172": 81, "045313": 74, "045379": 150, "04552": 84, "045553": 76, "045624": 63, "04563": 105, "045638": 74, "045754": 89, "045765": 66, "045769": 65, "045776": 65, "04586": 105, "045932": 89, "045984": 81, "045993": 105, "045995": 64, "046088": 89, "04608822": 89, "04625": 105, "046451": 81, "046507": [106, 111], "046525": 104, "046527": 76, "046587": 75, "0466028": 58, "046663": 66, "046728": 92, "046788": 86, "04682310e": 120, "046922": 105, "047": 87, "047094": 64, "047194": 25, "047239": 81, "04727807": 120, "047288": 104, "047364": 65, "047652e": 75, "0477": 70, "047724": 74, "047873": 81, "047954": 84, "048": 87, "048090": 70, "04813196": 120, "048308": 79, "048338": 64, "048416": 66, "048476": 81, "048694": 67, "048699": 94, "048723": 105, "048853": 75, "048936": 65, "049138": 67, "049573": [106, 111], "04973": 75, "05": [44, 55, 57, 58, 59, 60, 61, 64, 67, 74, 75, 76, 77, 84, 85, 86, 87, 89, 90, 93, 105, 106, 107, 109, 111, 120, 121, 136, 150, 153], "050": 87, "05039": 92, "050441": 64, "050494e": 75, "050538": 75, "050855": 67, "051": 60, "051003": 65, "051074": 67, "051101": 64, "051186": 89, "051327": 66, "051419": 66, "051651": 90, "051712": 88, "051870e": 76, "051940": 65, "052": 90, "052023": 81, "0520233166790431": 83, "052198": 66, "052297": 65, "0523": 65, "052380": 74, "052488": 79, "052502": 89, "052745": 76, "052811": 86, "052959": 64, "053": [60, 106], "053049": 75, "0533": 57, "053331": 76, "053389": 136, "053436": 26, "053541": 89, "053558": 76, "053659": 62, "053681": 67, "0538179888": 120, "053842535": 120, "053849e": 74, "054": [60, 90], "054064": 83, "054068": 84, "054162": 84, "054348": 136, "054370": 76, "054529": 136, "054771e": 89, "0548031": 120, "054975": 65, "055090": 66, "055165": 92, "055171": 75, "055322": 67, "055327": 65, "055393": 65, "055439": 86, "055493": 93, "055680": 136, "055880": 65, "056": 90, "056120": 67, "056300": 66, "056376": 67, "056499": 79, "0567": 65, "056745": 74, "056764": 74, "056915": 81, "05703772": 64, "057371": 86, "057484": 65, "0576": [59, 85], "057762": 89, "057792": 74, "057943119": 120, "057962": 76, "058001": 65, "058042": 136, "058175": 89, "058375": 71, "058463": 89, "058508": 94, "058595": 74, "0587": 65, "05870770": 120, "05891": 106, "0590": 57, "059003": 65, "059094": 65, "059117": 65, "059128": 74, "05936656": 120, "059384": 89, "059630": 63, "059685": 89, "059896": 65, "0599": 65, "06": [18, 31, 39, 55, 64, 67, 71, 74, 75, 76, 85, 86, 89, 104, 105], "060": 87, "060016": 71, "06008533": 106, "060167": 65, "060201": 89, "060212": [85, 86], "0603268864456956": 83, "060417": 74, "060845": 136, "060933": 74, "060994": 66, "0611": 57, "06111111": 60, "061205": 66, "0615": 57, "061514": 65, "061709": 66, "062": [90, 106], "062209": 65, "062414": 86, "062628": 67, "06269": 91, "06270135": 82, "0628": 57, "062964": 136, "062988": 74, "063017": 71, "063144": 65, "063175": 66, "0632": 57, "063234e": 75, "063240": 64, "06329768": [106, 109], "063312": 86, "063327": 81, "063428e": 86, "0635": 57, "063590": 89, "063593": 75, "0636": 57, "063700": 74, "0638": 57, "063881": 106, "06397789": 89, "063978": 89, "0640": 57, "064015": 66, "064164": 86, "064213": 75, "06428": 85, "064280": 85, "064384": 64, "0645": 57, "064600": 66, "0646222": 59, "0647": 57, "0649": 57, "064948": 66, "065": 93, "0653": 57, "065356": [78, 79], "065384": 66, "0654": 57, "0655": 57, "065653": 66, "065725": 76, "0659": 57, "065934": 66, "065969": 106, "065976": 81, "066068": 64, "0661": 65, "066180": 65, "0662": 57, "066295": 81, "066361": 65, "0664": 65, "066407": 66, "066464": 92, "066478": 83, "066548": 67, "066755": 66, "0669": 57, "06692492": 120, "067046e": 74, "0671": 57, "067212": 81, "0673": 57, "067436": 83, "067459": 65, "0675": 57, "067528": 93, "067569": 66, "067639": 86, "067721": 136, "067892": 64, "068030": 67, "068073": 75, "06811": 106, "068148": 89, "068221": 88, "06827": 92, "068377": 86, "068514": 74, "068638": 66, "068700": 104, "068934": 71, "06895837": 58, "069144": 83, "069161": 66, "06932418": 120, "069443": 71, "0695854": 58, "069589": 81, "069882e": 74, "07": [55, 74, 75, 85, 86, 89, 90, 93, 120], "070010": 67, "070020": 89, "0702127": 58, "070393": 86, "0704": [57, 65], "070422": 64, "070433": 81, "070497": 93, "070534": 30, "070552": 74, "0707": 57, "070751": 74, "070797": 86, "07085301": 106, "070884": 89, "0711": 57, "071285": 136, "07136": [58, 84], "071362": 74, "071393e": 64, "071543e": 76, "0716": 57, "07168291": 58, "071731": 86, "071777": 105, "071782": [29, 104], "0718": 70, "071889": 65, "0719": 57, "07202564": [78, 79], "072071": 64, "072109": 87, "072153": 65, "07222222": 60, "072293": 88, "07229774": [106, 111], "072516": 81, "072605": 71, "0727": 106, "072745": 65, "072932": 65, "073": 90, "073013": 89, "073207": 84, "073275": 74, "073320": 67, "073384": 81, "073447": [106, 111], "07347676": 58, "07350015": [36, 41, 58, 84], "073518": 76, "0735183279373635": 76, "073520": 76, "0736": 57, "07366": [60, 105], "073694": 75, "073875": 64, "074110": 64, "0743": 57, "074304": 136, "07436521": 120, "074426": 89, "07456127": 58, "074617": 75, "074700e": 86, "074746": 64, "07479278": 92, "074927": 71, "075226": 67, "075261": 63, "075384": 89, "07538443": 89, "07544271e": 120, "075600": 65, "07561": 150, "07564554e": 120, "075697": 64, "075720": 66, "0758": 93, "075809": 71, "075820": 64, "075869": 105, "076019": 85, "076119": 86, "076156": 136, "076179312": 136, "0761793120": 136, "0762": 70, "076322": 89, "076347": 76, "0765": 60, "076596": 74, "076653": [106, 111], "076684": 150, "07685043": 120, "076873": 65, "07689": 60, "07691847": 120, "076953": [78, 79], "076971": 68, "077090": 83, "077144e": 75, "077257": 65, "07727773e": 120, "077319": 89, "077502": [137, 143], "077527e": 86, "077555": 74, "077592": 81, "077702": 71, "0777777777777778": 105, "07777778": [60, 105], "07781396": 120, "077883": 89, "07788588": [106, 111], "077920": [98, 102], "077923e": 74, "07796": 106, "078017": 74, "078093": 65, "078096": 136, "078207": 68, "078214": 64, "07823667": 120, "07828372": 136, "078426": 104, "078474": 136, "078709": 75, "078810": 89, "078991": 65, "079085": 68, "07915": 60, "079186": 64, "07919896": 120, "07927087": 120, "079320": 64, "07942v3": 151, "079458e": 85, "079500e": 74, "07961": 92, "079652": 66, "079761": [98, 102], "07978296": 120, "08": [67, 76, 86, 89, 93, 106], "080": 87, "080121": 86, "08031571": 120, "080729": 70, "080854": 86, "0809": 65, "08091581": 120, "080926": 65, "080947": 68, "081": 60, "081074": 65, "081100": 89, "081230": [74, 75], "081379": 65, "081396": 79, "081488": 84, "08154161": 120, "08181827e": 120, "082": 55, "0820": [57, 65], "082197": 81, "082263": 37, "082297": 120, "082400e": 74, "082493": 65, "082574": 26, "082804": 63, "082858": 75, "082905": [106, 111], "082973": 84, "083079": 86, "083258": 136, "083318": 136, "08333333": 60, "08333617": 120, "0835771416": 58, "08357776": 67, "0836": 81, "08364": 81, "083706": 93, "083949": 93, "084": 58, "084007": 80, "084042": 65, "084156": 75, "084269": 86, "084323": 74, "084337": 106, "084795": 65, "084932": 64, "084933": 65, "085279e": 78, "0853505": 58, "085395": 74, "085566": 76, "085592": 81, "085671": 74, "08590328": 120, "086004": 81, "08602774e": 120, "0862": 148, "086264": 76, "086401570476133": 76, "086402": 76, "08664208": 120, "086679": 105, "086826": 78, "0871486": 65, "087196": 64, "0872": 57, "087222": 75, "087363": 67, "087561": 74, "087566": 83, "087634": 74, "087745": 75, "087947": 89, "088048": 89, "088082": 83, "088282": 79, "088288": [98, 102], "088357": 89, "088432": 64, "08848": 105, "088482": [29, 104], "088504e": 37, "088792": 83, "088836": 83, "08888889": 60, "088935": [106, 109], "089": 120, "08903651": 65, "0894": 57, "08946464": 120, "08968939": 58, "089994": 64, "08e": 59, "09": [67, 74, 75, 76, 85, 86, 89, 104, 120], "09000000000000001": 105, "09007683": 120, "09015": 57, "090255": 89, "090363": 88, "090436": 75, "090735": 64, "090804": 66, "090816": 64, "090824": 64, "091179e": 74, "091391": 136, "091406": 137, "091479": 67, "091535": 74, "0916": 57, "09170273": 120, "091824": 75, "091955": 66, "091992": 88, "092043": 65, "09211533": 120, "092229": 93, "092263": 106, "092365": 136, "092453": 89, "09245337": 89, "09246695": 120, "092523": 65, "092646": 89, "092891": 65, "092919": 138, "092935": 74, "0929369228758206": 83, "093043": 89, "09310496": 136, "093153": 89, "09317": 120, "093480": 66, "0935": 106, "09351167": 106, "093746": 136, "093805": 64, "093832": 64, "093950": 84, "094026": 84, "094118": 89, "094378e": 74, "094381": 84, "094420": 86, "09444444": 60, "094581e": 75, "094595": 70, "09471037": 120, "094766": 88, "094829": 106, "094977": 67, "094999": 89, "09500344": 120, "095104": 71, "095627": 64, "095654": 74, "095781": 24, "095785": 71, "09603": 148, "096089159": 120, "096094": 64, "096337": 84, "096418": 71, "096616": 104, "096658": 64, "096688": 75, "09682314": 106, "096915": 93, "097": 90, "097009": 81, "097140": 78, "097157": 93, "097208": 88, "097468": 76, "09756": 91, "09759": 120, "09779675": 136, "097796750": 136, "098": 59, "098202": 70, "098207": 65, "098256": 89, "09830758": 92, "098308": 92, "098319": 89, "0986": 57, "098711": 64, "098712": 89, "09879814e": 120, "09886149": 120, "098879": 64, "098901": 81, "098985": 64, "099": 90, "099001": 75, "09916607": 120, "099307": 75, "099449": 64, "099485": [106, 107], "09948738": 65, "099647": 88, "099670": 86, "0997": 120, "099731": [74, 75], "09980311": 136, "09988": 151, "0_": 40, "0ff823b17d45": 60, "0x1747bdd4520": 68, "0x1747bdd6b90": 68, "0x7f21e7546ba0": 153, "0x7f21ea72d1c0": 136, "0x7f21ea72d400": 136, "0x7f21ea7a0cb0": 136, "0x7f21ea7a3080": 136, "0x7f21f9206720": 105, "0x7f21f9206900": 105, "0x7f21f92419a0": 106, "0x7f21f94e42c0": 105, "0x7f21f95d47a0": 143, "0x7f21fa6cd400": 137, "0x7f21faa59eb0": 106, "0x7f21faa6dd90": 106, "0x7f21fac8fe90": 107, "0x7f21fae61910": 107, "0x7f21fae650d0": 106, "0x7f21fafa1190": 106, "0x7f21fb0b8200": 106, "0x7f46d9d9e840": 93, "0x7fe1bc1ad3d0": 88, "1": [12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 51, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 115, 116, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152], "10": [10, 11, 12, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 35, 36, 37, 38, 39, 41, 42, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 99, 102, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 137, 143, 148, 150, 151, 152, 153], "100": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 41, 43, 58, 60, 61, 62, 74, 75, 77, 80, 81, 82, 84, 87, 91, 93, 94, 95, 97, 102, 104, 105, 106, 108, 120, 121, 136, 137, 143, 150, 152], "1000": [17, 19, 25, 27, 56, 62, 63, 70, 72, 73, 78, 79, 80, 82, 83, 85, 86, 90, 92, 93, 96, 106], "10000": [55, 63, 67, 74, 75, 85, 89], "100000e": 86, "10001": 67, "100044": 79, "10019": 67, "100208": 104, "1003": 70, "100356": 76, "10038": 92, "10039862": [94, 106], "100479": 64, "100517": 136, "100715": 74, "10074": 86, "10079785": 106, "100807": [74, 75], "100854": 65, "100858": 92, "10089588": 89, "100896": 89, "1009": 65, "100923": 89, "10098": 67, "100_000": 87, "101": [18, 31, 39, 57, 90, 104, 151, 152], "1012161720262732343848495053545969747794": 120, "10126": 86, "10127930": 136, "101279300": 136, "1015": [59, 85], "1016": [17, 18, 19, 31, 39, 57], "1016010": 59, "1016338581630878": 83, "1017162": 120, "101764": 67, "1018": 86, "10185202": 65, "101998": 71, "102": [97, 102, 104, 150, 152], "10235": 86, "102616": 76, "10277": 86, "102775": 76, "10299": 85, "103": [74, 84, 90, 97, 102, 104, 152], "10307": 136, "1030891095588866": 83, "1031": 86, "103179163001313": 83, "10348": 85, "103497": 89, "103806": 76, "103829e": 64, "10396": 85, "104": [59, 85, 104, 152], "1040": 86, "10406": 86, "104087": 74, "1041": 57, "10414": 86, "10449": 91, "104492": 81, "1045303": 58, "104681": 67, "104710": 64, "104787": 84, "104849": 74, "10491572": 65, "105": [40, 58, 81, 84, 104, 152], "105318": 89, "1054": 60, "105461": 104, "10548": 87, "1055": [57, 91], "105722": 64, "106": [60, 104, 152], "10607": [68, 97, 102, 150], "106116": 86, "106259": 64, "10627": 86, "106274": 67, "106302": 64, "10637173e": 120, "106391": 136, "1065": [77, 82, 83], "106595": [106, 108], "106715": 80, "107": [60, 65, 93, 104, 152], "107073": 76, "107156": 81, "107295": 136, "1073": 86, "107413": 74, "107423": 67, "107467": [98, 102], "10747": [68, 97, 102, 150], "107733": 64, "107746": 89, "10791": 86, "107932": 64, "107935": 89, "107974": 65, "108": [104, 148, 151, 152], "1080": [36, 41, 57, 58, 84], "108188": 65, "10824": [68, 97, 102, 150], "108257e": 86, "108259": 71, "10829": 86, "10831": [68, 97, 102, 150], "10852124": 120, "108783": 76, "108783087402629": 76, "10878571": 89, "108786": 89, "108870": [106, 111], "109": [74, 104], "10903": 85, "109069": 136, "109079e": 89, "109196": 67, "109227": 64, "109273": 84, "109277": 81, "10928": 86, "1093": 77, "109454": 86, "109470": 75, "10954427": 65, "1096": [57, 91], "10967": 85, "109771762": 120, "109861": 150, "109945": 66, "1099472942084532": 72, "10e": [76, 89], "11": [38, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 89, 90, 92, 93, 94, 95, 97, 102, 104, 105, 106, 107, 108, 111, 120, 121, 136, 137, 143, 150, 153], "110": [104, 152], "11000115": 65, "110081": 81, "1101": 86, "11019365749799062": 93, "110194": 93, "110359": 84, "110365": 93, "110557": 83, "110681": 92, "11071087": [94, 106], "110717": 136, "110742": 83, "1109": 86, "111": [75, 104, 106, 152], "111043": 78, "1111": [10, 11, 42, 56, 58, 73, 77, 84, 93, 96, 106, 137, 143, 148], "11120": 86, "111352344760325": 83, "111577": 86, "1117": [77, 82, 83], "111783": 86, "1118": 59, "11199615e": 120, "112": [60, 104, 152], "1120": 85, "1120401": 65, "112078": 104, "11208236": [95, 121], "112135": 76, "1121351274811793": 76, "1122": 86, "112216": 76, "113": [10, 104, 152], "113005": 86, "113022": 81, "11311": 85, "113149": 81, "113207": 89, "113270": 76, "113558": 66, "113657": 66, "11376": 86, "113780": 84, "113795": 64, "113952": 81, "11399": 85, "114": [104, 152], "114026": 83, "1144": 86, "1144500": 58, "11447": 92, "114530": 78, "1145370": 58, "114570": 75, "11458": 86, "114647": 76, "1147": 57, "114834": 86, "114989": 71, "115": [104, 152], "11500": [85, 153], "115060e": 89, "1151": 86, "115297e": [85, 86], "11530": 86, "115450": 67, "11552911": 92, "115636": 75, "11570": 85, "11588": 86, "115901": 71, "115972": 74, "116": [104, 152], "116080": 64, "116090": [106, 109], "116274": 76, "116341": 65, "116545": 64, "1166": [85, 151], "1167": 85, "11673": 86, "11676": 86, "116821": 64, "116863": 67, "117": [74, 104], "11700": 153, "117112": 75, "117194": 88, "11720": 86, "117242": 89, "11724226": 89, "117366": 89, "117407": 64, "11743": 153, "11750": 86, "117535": 64, "1176": 57, "1177": 57, "117710": 76, "11789998": 89, "117900": 89, "11792": 59, "11796": 86, "118": 104, "1182": 59, "11820": 86, "11823404": 93, "118255": 89, "11850": 86, "118596": 76, "1186": 59, "118601": 84, "11861": 59, "1187": 92, "118708e": 84, "1187339840850312": 84, "118776": 67, "118799": 86, "118938e": 106, "118952": 84, "119": [93, 104, 152], "1193": 65, "11935": 92, "119409": 83, "11942111": [106, 111], "119471": 66, "119766": 89, "1198": [58, 84], "119820": [106, 109], "119998": 64, "12": [4, 13, 16, 17, 19, 30, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 70, 71, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 92, 93, 95, 97, 102, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 137, 143, 148, 150, 151, 152, 153], "120": [61, 62, 83, 94, 104, 152], "12002": 85, "1200x600": [64, 65, 67], "1200x800": [64, 65, 67], "1202": 151, "120567": [78, 79], "120721": 84, "12080467": [106, 109], "12097": [10, 11, 42, 58, 77, 84, 86, 96, 148], "121": [86, 104, 152], "1210": 86, "12105472": 136, "121054720": 136, "1211": 86, "121297": 86, "1213405": 58, "1214": 136, "121584e": 89, "12167329": 120, "121774": 80, "121824": 75, "12196389e": 120, "122": [18, 31, 39, 57, 64, 90, 97, 102, 104, 151, 152], "122089e": 67, "12214": 59, "12223182e": 120, "122408": 76, "122421": 81, "122777": 136, "123": [44, 59, 60, 64, 67, 85, 93, 104, 152, 153], "1230": 86, "123192": 93, "12323": 86, "1234": [55, 56, 57, 68, 72, 73, 90, 91, 96, 105, 120, 136], "12348": 93, "123501e": 86, "123581": 66, "123779": 67, "123875": 64, "123950": [106, 107, 111], "124": 104, "12410": 86, "1241182": 120, "124554": 67, "124582": 64, "124805": 85, "124825": 75, "125": [104, 152], "12500": 85, "125065": 136, "12539340": 136, "125405": 67, "125613": 64, "12572": 86, "125782": 64, "1258": [58, 86], "125849": 87, "125875": 67, "126": [104, 152], "12612": 86, "12616": 86, "126777": 136, "126802": 86, "12689": 86, "12690934": 67, "126916": 67, "127": [31, 104, 152], "12705095": [121, 136], "12707800": 58, "127337": 81, "1273824": 120, "127420": [106, 109], "1275": 86, "12752825": 136, "127563": 92, "1277": 87, "127889": 104, "128": [59, 104, 152], "12802": 59, "12814": 86, "128229": 81, "128300e": 75, "128312": 89, "128393": 86, "128408": 84, "128412": [98, 102], "1285": 57, "12861": 86, "128651": 75, "129": [55, 84, 104, 152], "129063": 67, "129155": 64, "12945": 151, "12947879": [106, 109], "1295": [57, 86], "12955": 85, "129606": 74, "129630": 64, "1297": 86, "129798": 74, "12980769e": 120, "12983057": 106, "12994995": 120, "13": [17, 18, 19, 33, 35, 39, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 74, 75, 76, 78, 79, 80, 81, 83, 84, 85, 86, 89, 90, 91, 92, 93, 95, 97, 102, 104, 105, 106, 108, 109, 120, 121, 136, 137, 143, 150, 153], "130": [60, 84, 104, 152], "1300086": 65, "13003274": [106, 109], "130122": 92, "13034980e": 120, "130370": 76, "1306": 92, "130829": 89, "13084": 106, "130842": 66, "13091": 86, "13094631": 65, "130971": 78, "131": [104, 152], "13102231": 106, "13119": 92, "1312": 153, "131204": 66, "131207": 67, "1313": [59, 153], "13134669": 65, "13137893e": 120, "131483": 81, "1317": 65, "131737": 65, "1318": 57, "131842": 81, "131928": [98, 102], "132": [60, 74, 84, 104, 152], "13208": 153, "1321": [85, 153], "132248": 104, "132307": 64, "1324": [59, 85], "132454": 63, "132481": 104, "1325": 59, "132519": 67, "132551": 64, "13257": 85, "132671": 76, "1328": 92, "132941": 86, "132982": 74, "133": [60, 97, 102, 104, 151, 152], "133202": 86, "133421": 86, "133558": 64, "13357": 86, "133596": 89, "133839": 81, "13398": 93, "133f5a": 87, "134": [84, 94, 104, 152], "134037": 66, "1340371": 57, "1341": 59, "134211": 89, "1343": [85, 86], "134467": 65, "134542": 74, "134567": 86, "1346035": 59, "134687": 86, "13474": 86, "134765": 86, "134784": 106, "134784e": 74, "1348": 85, "13481782": 120, "13490": 86, "135": [60, 87, 104, 106, 152], "13505272": 58, "135078": 67, "135142": 75, "13523058": 65, "135352": 12, "135375": 71, "135379": 136, "135494": 67, "135596": 86, "135665": 75, "135707": 105, "135856": 89, "13585644": 89, "135871": 84, "1359": 86, "136": [68, 84, 87, 93, 104, 152], "1360": 59, "13602": 93, "136049": 64, "136089": 84, "136102": 74, "13642": 86, "136442": 84, "1366": 87, "136664": 65, "136836": 84, "136885": 86, "137": [31, 60, 68, 104, 152], "1371": 86, "137125": 67, "137165": 106, "137213": 75, "137230": 83, "137282": 65, "1373": 65, "137366": 64, "137369": 64, "137396": 89, "1378": 86, "137850": 66, "13795289": 120, "137999": 106, "138": [104, 152], "1380": 85, "13809": 86, "138142": 78, "138264": 93, "138378": 76, "1384": 85, "138553": 64, "1386": 57, "13868238": 136, "138682380": 136, "138698": 136, "1387": 57, "13893": 86, "138985": 64, "139": [93, 104, 120, 150], "139117e": 74, "139119": 67, "1394": 64, "139491": 136, "139508": 81, "13956": 92, "139622": 86, "1398": 86, "139830": [106, 111], "1399": 57, "139936": 64, "14": [55, 56, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 70, 71, 74, 75, 76, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 92, 93, 95, 97, 102, 104, 105, 106, 108, 109, 120, 121, 136, 137, 143, 150, 151, 153], "140": [61, 62, 81, 86, 94, 104, 152], "1400": 86, "14000073": 120, "140073": 74, "1401": 57, "140159": 67, "14018": 106, "140755": 66, "140770": [74, 75], "140833": 76, "140861": 58, "140900": 65, "14093001": 65, "141": [86, 104, 152], "141002": 75, "141101e": 86, "14112993": 65, "14114": 92, "14122987": 65, "141247": 71, "14141": 86, "141546": 136, "141820": 76, "1419": 120, "142": [104, 152], "14200098": 136, "142045e": 84, "142119": 74, "142270": 63, "142382": 74, "1424": 105, "142482": 89, "1425": 65, "14264911": 65, "14268": 106, "14281403493938022": 105, "142889": 67, "14289": 86, "143": [97, 102, 104, 152], "143260": 64, "143342": 75, "143495": 104, "1435": 86, "143534": 74, "14368145": 136, "144": [104, 152], "14400": 85, "14405": 86, "14406": 86, "144084": 76, "1441": 57, "1443": 86, "14440": 67, "144517": 65, "144669": 89, "14468": 67, "144800": 76, "144813": 71, "144908": 88, "144971": 85, "145": [104, 152], "145082": 64, "145134": 64, "145245": 89, "14530": 86, "14532650": 136, "145614": 67, "145625": 89, "145748": 136, "14588": 86, "146": [104, 152], "146037": 89, "146087": 150, "146214": [98, 102], "14625": 86, "146265": 64, "146435": 75, "1465": 59, "14659004": 67, "146641": 136, "1468110": 120, "1468115": 58, "147": [104, 152], "14702": 68, "147121": 89, "14744": 86, "14772": 86, "147823": 64, "1479": [70, 86], "14790924": 136, "147909240": 136, "147927": 68, "14795": 86, "148": [104, 152], "148005": 71, "14803": 86, "148134": [74, 75], "148161": 89, "148210": 76, "1482102407485826": 76, "14845": 68, "148455": 76, "1484554868601506": 76, "1485": 86, "148750e": [85, 86], "148835": 86, "149": [104, 152], "14915889": 65, "1492": 55, "149215e": 75, "149228": 93, "149285": 89, "149472": 93, "149714": 84, "149858": [28, 104], "149898": 89, "15": [11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 56, 58, 59, 60, 62, 64, 67, 70, 71, 74, 75, 76, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 93, 97, 102, 104, 105, 106, 108, 109, 120, 121, 136, 137, 143, 150, 153], "150": [40, 60, 93, 104, 152], "15000": [59, 85], "150000": 59, "15000000000000002": [76, 86, 89, 105], "1502": 58, "150200": 84, "150234": 86, "150408": 58, "150435": 86, "150438": 64, "150614": 68, "150719e": 85, "1509": 70, "15092679": 65, "151": [104, 152], "151063": 74, "15108": 86, "15111868": 65, "151447": 83, "15144717": 65, "151636": 76, "15173626": 65, "151819": 89, "151849": 64, "15194": 85, "152": [104, 152], "15203203": 65, "152148": [74, 75], "152353": 75, "152706": 106, "15287": 86, "152926": 63, "153": [93, 104, 152], "153119": 76, "153293": 86, "1532979": 120, "153314": 75, "15339": 86, "153398": 86, "15354": 92, "153587": 84, "153633": 68, "153639": 120, "153867": 67, "153935": 75, "153983": 76, "1539831483813536": 76, "154": [70, 104], "15430": 153, "154421": 136, "154445": 67, "1545": 86, "154557": 89, "154707": 71, "154758": 136, "154821": [106, 109], "154828": 76, "155": [104, 152], "155000": 85, "155025": 89, "155120": 89, "155160": 71, "15518": 67, "1554": 86, "155423": 71, "15544": 67, "15549": 86, "15556": 86, "1557093": 58, "156": [104, 152], "156021": 89, "156169": 75, "156202": [74, 75], "156317": [74, 75], "156328": 83, "1564": 136, "156545": 136, "156651": 67, "156684": 75, "156704": 86, "1569": 86, "156969": 76, "156988": 70, "157": [75, 104, 152], "157091": 136, "157154": 74, "157370": 64, "157394": 64, "157470": 88, "1576": 86, "1577657": 58, "157e": 106, "158": [87, 104, 152], "158007": 89, "15815035": 59, "158178": 76, "158198": 86, "1582": 86, "158288": 86, "158318": 65, "158370": 64, "1586": 86, "158682": 83, "158697": 136, "158726": 104, "1589": 86, "158905": 64, "159": 152, "159001": 64, "15915": 66, "15916": [57, 66], "159272": 67, "159386": 92, "15943851": 65, "159466": 89, "15946647": 89, "1596": 60, "159633e": 75, "159826": 86, "159841": 74, "159959": 86, "16": [24, 55, 56, 58, 59, 60, 61, 62, 64, 66, 67, 70, 71, 74, 75, 76, 79, 80, 81, 84, 85, 86, 89, 90, 92, 93, 97, 102, 104, 105, 106, 108, 109, 120, 121, 136, 137, 143, 150, 153], "160": [61, 62, 94, 152], "160058": 64, "160334": 67, "160363": 67, "16037416": 65, "1604": 59, "160499": 65, "160825": 78, "160836": 81, "160932": 76, "161": [60, 70, 151, 152], "161049": 75, "161141": 84, "161236": 89, "161243": 89, "161269": 74, "161288": 75, "16141846": 65, "161424293035374657616264687175838590100": 120, "1614242930353746576162646871758385901001012161720262732343848495053545969747794891519212225284255636667707381929598992371123404547515660657682848687889396": 120, "1614242930353746576162646871758385901004513183133363941434452587278798089919710121617202627323438484950535459697477942371123404547515660657682848687889396": 120, "16142429303537465761626468717583859010045131831333639414344525872787980899197101216172026273234384849505354596974779489151921222528425563666770738192959899": 120, "16142429303537465761626468717583859010045131831333639414344525872787980899197891519212225284255636667707381929598992371123404547515660657682848687889396": 120, "1614314": 120, "161441": 62, "1619": 59, "162": [70, 152], "16201": 86, "16211": 85, "162153": 89, "1622": 86, "16240685": 65, "16241": 86, "162436": 93, "16245836": 120, "16249": 86, "162499": 66, "1626685": 58, "162683": 93, "162710": 76, "162752": 83, "162785": 64, "1628": 85, "162909": 84, "162930": 86, "163": 152, "163053": 66, "163194": 89, "1634764": 65, "163519": [106, 109], "163566": 86, "1635990": 120, "16386126": 65, "163895": 76, "164": [70, 71, 86, 90, 152], "164034": 136, "16441371": 65, "164467": 85, "164608": 89, "164698": 80, "1648": 57, "164801": 89, "164805": 76, "164864": 84, "164943": 83, "165": [70, 93, 152], "16500": 85, "165178": 89, "1653": 86, "16536299": 136, "165362990": 136, "16539906e": 120, "165419": 89, "16553": 85, "165549": 150, "165707": 71, "165740": 64, "165769": 65, "16590": 86, "166": 152, "1661": 85, "16618": 86, "166266": 64, "166322": [98, 102], "166375": 104, "166517": 81, "166904": 86, "167": [59, 85, 152], "167035": 86, "167547": 89, "167581e": 74, "1676": 86, "167653": 66, "167930": 86, "167993": 136, "168": 152, "16803512": 136, "168092": 136, "1681": [57, 85], "168184": 65, "168195": 92, "1682": 65, "1684": 65, "168466": 64, "168614": 89, "168931": 89, "169": [60, 152], "1691": [57, 86], "16910": 86, "169117": 93, "169196": 89, "169220e": 76, "16951": 86, "169622": 64, "16984": 86, "169858": 83, "17": [56, 58, 59, 60, 62, 64, 67, 70, 71, 74, 75, 80, 81, 83, 84, 85, 86, 89, 90, 92, 93, 97, 102, 104, 105, 106, 108, 120, 121, 136, 137, 143, 150, 153], "170": 152, "1705": 86, "170697": 67, "170705": 104, "170709e": 75, "17083": 86, "170933": [106, 111], "171": [70, 152], "1712": 151, "171263": 67, "1714": 59, "171554": 67, "171575": 89, "171696": 104, "171815": 105, "171833": 75, "171848e": 74, "1719": [64, 65, 66, 67], "172": [70, 87, 90, 152, 153], "172022": 136, "172083": 75, "17227725": 65, "17237288": 65, "17240948": 65, "17260487": 120, "172722": 64, "172793": 89, "17283588": 65, "173": [70, 152], "17327678": 65, "173400": 70, "173504": 79, "173520": 65, "17358502": 65, "17372": 86, "17385178": 105, "173969": 136, "173e": 90, "174": 152, "174127": 65, "174177": 89, "174185": 89, "174499": 136, "174516e": 89, "17453": 86, "1746": 86, "17469": 86, "174726": 67, "174793": 87, "174835": 75, "174871": 67, "174924": 64, "174968": 85, "175": [70, 152], "17500": 86, "1751": [70, 85], "175254": 85, "175284": 76, "17536": 86, "175369": 75, "1755": 70, "175635027": 58, "17576": 86, "175894": 93, "175931": [104, 105, 106], "176258": 64, "176495": 89, "17655394": 89, "176554": 89, "176929": 136, "176932": 64, "177": [151, 152], "177007": 89, "17700723": 89, "177043": [74, 75], "177094": 70, "1772821": 120, "177290": 67, "1773": 86, "177397": 86, "1774": 57, "177496": 89, "177611": 89, "177740": 74, "177751": 89, "177830": 75, "177905920": 120, "177909": 67, "177995": 89, "178": [86, 152], "17800": 86, "1780429747": 120, "17807": 86, "178169": 78, "178218": 75, "17823": 60, "178565": 64, "178704": 136, "178763": 89, "178934": 136, "179": [86, 98, 102, 152], "179026": 75, "179101": 104, "1795": 70, "179548": [98, 102], "1795850": 58, "179588e": 89, "179777": 75, "1798913180930109556": 87, "18": [56, 58, 59, 60, 62, 64, 67, 68, 70, 74, 75, 78, 80, 81, 82, 83, 84, 85, 86, 89, 90, 92, 93, 97, 102, 104, 105, 106, 108, 120, 136, 150, 153], "180": [61, 62, 67, 94, 152], "18024": 64, "180262": 75, "180271": 81, "1803": 57, "18030": 86, "1805662": 120, "180575": [78, 79], "1807": [57, 86], "1809": 151, "180951": 89, "181": 152, "1812": 86, "181330": 71, "1813777": 65, "1814": 57, "18141": 86, "181446": 136, "18193847": 65, "181997": 64, "182": [64, 152], "1820": 57, "182427": 75, "182633": 89, "18273096": [106, 109], "182849": 89, "183": [60, 64, 70, 85, 106, 152], "1832": 86, "183204": 66, "183288": 64, "183339": 74, "18335797": 67, "183373": 106, "183526": 76, "183553": 90, "18356413": 106, "18368": 86, "183727": 64, "183814": 86, "183855": 105, "183888": 84, "184": [60, 64, 151, 152], "184224": 71, "184247": 74, "184256": 64, "184303": [106, 111], "184347": 75, "185": [59, 60, 67], "185130": 90, "18516129": [106, 111], "185480": 67, "18551018": 67, "185984": 74, "186": [64, 70, 152], "18604": 86, "1862": 57, "18622": 86, "186237": 75, "18637": 148, "186572": 64, "186589": 71, "18666": 86, "186689": 106, "186735": 89, "186775": [106, 107, 111], "18678094e": 120, "186836": 89, "186864": 83, "186868": 62, "187": 152, "187153": 136, "187186": 64, "187664": 74, "187690": 89, "18773": 86, "188": [64, 67, 152], "188175": 89, "1881752": 89, "188223": 89, "188400": 75, "188429": 67, "188541": [106, 107], "1887": [104, 105, 106], "188760": 81, "1888": 86, "18888149e": 120, "188882": 75, "188991": 136, "189": [60, 86, 90, 152], "189195": 86, "189248": 74, "189403": 64, "1895815": [41, 58, 84], "189737": 89, "189739": 71, "18976": 86, "189850": 66, "189882": 66, "189998": 89, "19": [56, 58, 59, 60, 62, 64, 67, 70, 74, 75, 81, 83, 84, 85, 86, 89, 90, 92, 93, 97, 102, 104, 105, 106, 108, 109, 120, 136, 150, 153], "190": [60, 64, 152], "190000e": 86, "190094": 67, "190096": 136, "190231": 86, "19031969": 89, "190320": 89, "19033538": 58, "190534": [106, 109], "190648": 25, "190678": 67, "19073905e": 120, "190809": 89, "190892": 93, "1909": [41, 58, 84], "190915": 76, "190921": 79, "190976": 90, "190982": 89, "191": [60, 64, 67, 151, 152], "191149": 66, "191192": 74, "1912": 151, "191223": 75, "1912705": 96, "191294": 75, "191534": 85, "191698": 64, "1918": 57, "19182": 64, "192": [64, 152], "192240": 136, "1923": 86, "192310": 67, "192526": 92, "19252647": 92, "192539": [29, 104], "19264": 67, "192888": 67, "192952": 71, "193": 152, "193060": 89, "193091": 67, "193253": 74, "193285": 74, "193308": [29, 104], "193341": 75, "193375": 81, "193644": 86, "19374710e": 120, "19382": 86, "193833": 86, "193849": 90, "19385": 86, "193f0d909729": 60, "194": [64, 67, 70, 82, 152], "194042": 67, "194092": 74, "1941": 59, "194114": 64, "19413": [85, 86], "194236": 66, "194303": 75, "194786": [106, 107], "195": [64, 67, 99, 102, 152], "19508": 93, "19508031003642456": 93, "19509680e": 120, "195377": 89, "195396": 89, "19550": 86, "195564": 84, "19559": [59, 85], "195781": 74, "1959": 151, "195963": 81, "196": 152, "196139": 64, "196189": 89, "196478e": 75, "196655": 81, "19680840": 136, "196824": 89, "196835": 83, "196e": 44, "197": [64, 98, 102, 152], "1970": 86, "19704": 67, "19705": 86, "197225": [68, 97, 102, 150], "1972250000001000100001": [60, 97, 102, 150], "197424": 105, "197482": 65, "197484": 136, "1975": 86, "19756": 86, "19758": 86, "197600": 63, "197920": 74, "197926": 64, "19793": 86, "19798": 86, "198": [64, 67, 152], "198221": 86, "19824": 86, "198493": 81, "198503": 90, "198549": 68, "198617": 88, "198687": 59, "19877413": 67, "1988": [56, 73, 96, 106], "199": [64, 152], "1990": [59, 85, 86], "1991": [59, 85, 86, 153], "199130e": 67, "199281e": 89, "19929475": 67, "199412": 75, "199458": 136, "1995": [58, 84], "1998": 87, "19983954": 94, "1999": 87, "1_": [76, 89], "1d": 13, "1e": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 70, 81, 86], "1f77b4": 63, "1m": 105, "1x_4x_3": 63, "2": [10, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 47, 49, 51, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 115, 120, 121, 136, 137, 138, 140, 142, 143, 144, 145, 146, 147, 149, 150, 151, 152], "20": [12, 14, 15, 17, 18, 19, 22, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 37, 38, 39, 41, 42, 43, 56, 58, 59, 60, 61, 62, 64, 67, 70, 74, 75, 76, 80, 81, 82, 83, 84, 85, 86, 89, 92, 93, 94, 95, 96, 97, 102, 104, 105, 106, 108, 120, 121, 136, 137, 143, 150, 153], "200": [17, 19, 32, 35, 40, 57, 61, 62, 64, 67, 76, 77, 82, 88, 89, 94, 96, 98, 101, 102, 105, 151, 152], "2000": [11, 30, 59, 61, 71, 74, 75, 76, 81, 85, 86, 89, 91, 94, 104, 106], "20000": [59, 85], "20000000000000004": [76, 86, 89], "200000e": 86, "200049": 74, "200071": 64, "2001": 65, "20010": 86, "200110": 86, "200135": 64, "2003": [10, 65, 151], "200303": 150, "200308": 67, "200393": 64, "2004": [65, 106, 110], "20049927": 65, "2005": [62, 65], "20055": 86, "2006": [65, 86, 106, 110], "2007": [65, 106, 110], "20074": 86, "20097996": 65, "200982": 78, "20098932": 65, "201": [60, 64, 152], "2010": [58, 84], "201066": 64, "2011": [58, 84, 148, 150], "2013": [77, 136, 151], "20137799": 65, "2014": [136, 151], "2015": [40, 151], "201514": [106, 109], "201528": [74, 75], "20158": 86, "2016": 87, "20167273": 82, "2017": [34, 151], "201768": 84, "2018": [10, 11, 42, 43, 56, 58, 59, 62, 73, 77, 82, 84, 85, 86, 91, 92, 96, 106, 112, 120, 121, 129, 136, 148, 151, 152], "201834": 64, "2019": [32, 60, 74, 75, 76, 78, 79, 86, 89, 92, 105, 121, 127, 130, 131, 148, 150, 151], "201e": 90, "202": [64, 67, 152], "2020": [12, 14, 15, 17, 18, 19, 31, 33, 35, 39, 57, 60, 62, 64, 67, 93, 105, 106, 108, 112, 137, 138, 151], "2020435": 58, "2021": [17, 19, 41, 57, 58, 60, 64, 65, 66, 67, 74, 75, 84, 106, 107, 110, 112, 151, 152], "20219609": 58, "2022": [92, 93, 106, 108, 112, 137, 138, 147, 148, 151], "2023": [36, 61, 91, 94, 106, 119, 121, 134, 135, 151], "2024": [55, 72, 77, 82, 83, 87, 90, 91, 93, 106, 148, 151], "2025": [17, 19, 64, 67, 91, 106, 107, 109, 111], "202603": 75, "202650e": 76, "20269": 86, "202846": 75, "203": [59, 74, 85, 152], "203284": 76, "20329": 86, "203828": 86, "204": [64, 67, 152], "204007": 89, "20400735": 89, "204362": 93, "204455": 75, "204482": 89, "204794": 89, "204893": 71, "205": [67, 90, 92, 152], "205187": 76, "205224": 92, "205245": 86, "205333e": 85, "2055": 70, "20586208": 65, "205869": 65, "205938": 84, "206": [67, 152], "206253": [85, 86], "206256": 81, "20661937": 65, "207": [64, 67, 70, 90, 106, 152], "207363": 67, "2075": 57, "20753": 66, "207834": 75, "20783816": 58, "207840": 79, "207845": 67, "207885": 85, "207912": 136, "207936": 64, "208": [64, 67, 70, 71, 152], "2080787": 58, "2082": 65, "208203": 65, "20823898": 58, "208300": 89, "208519": 67, "2086": [70, 86], "208625": 64, "208741": 67, "208840": 70, "209": [64, 67, 70, 71], "2091": 70, "209219e": 92, "209257": 12, "209405": 64, "20956": 86, "209894": 89, "209912": 65, "21": [10, 11, 42, 56, 58, 59, 60, 62, 64, 67, 70, 74, 75, 77, 83, 84, 85, 86, 89, 91, 92, 93, 96, 97, 102, 104, 105, 106, 108, 120, 136, 148, 150, 151, 153], "210": [17, 18, 19, 35, 39, 64, 67, 70, 71, 86, 87], "210095": 67, "2103": 148, "2103034": 58, "210319": [74, 75], "210323": 89, "2104": 152, "2107": 151, "21082": 86, "211": [64, 67, 70, 71, 90, 152], "211002": [106, 111], "21105": [60, 105, 148, 150], "2112": 93, "2114026": 89, "211403": 89, "21142": 86, "211423": 64, "211534": 76, "2116": 70, "211722": 64, "211939": 70, "212": [64, 67, 71, 152, 153], "212015": 67, "2121": 86, "212317": 89, "212491": 89, "21257396e": 120, "212811": 71, "212844": 84, "212863": 74, "213": [64, 67, 70, 71, 90, 151, 152], "213070": 75, "213100": 67, "213133": 64, "213135": 75, "213199": 106, "213398": 64, "21342": 86, "21351": 86, "213743e": 75, "2139": 33, "214": [64, 67, 70, 87, 152], "214764": 92, "214769": 81, "215": [71, 152], "215085": 67, "215159": 86, "215342": 89, "2155": 86, "21550": 86, "215525": 65, "21562": 86, "215771": 65, "215965": 64, "215967": 136, "216": [64, 70, 71, 152], "216113": 89, "216207": 105, "21624417": 58, "2163": 86, "216344": 89, "21669513e": 120, "2167": 86, "216728e": 64, "216943": 104, "217": [64, 67, 70, 71, 90, 151, 152], "21716": 86, "2171802": [58, 84], "217244": 27, "217407": 67, "2175": 86, "217605": 66, "217684": 81, "217804": 64, "218": [64, 67, 70, 71, 152], "21804": [59, 85], "218223": 83, "218383": 71, "218427": 67, "218546": 89, "2189": 86, "218919": 70, "218924": 86, "219": [18, 31, 39, 57, 70, 71, 98, 102, 151, 152], "2191274": 58, "21948504": 120, "219585": 71, "219607": 67, "219965": 67, "22": [56, 58, 59, 60, 62, 64, 67, 74, 75, 83, 84, 85, 89, 90, 92, 93, 104, 105, 106, 108, 120, 136, 150, 153], "220": [44, 64, 67, 70, 71, 152], "220171": [106, 109], "220211": 86, "220307": 64, "220398": 74, "220407": 64, "2205336": 67, "220568": [106, 109], "220772": 89, "221": [67, 70, 71, 152], "2213": 84, "2214": 84, "221402": 64, "2215": 84, "2216": 84, "2217": [58, 84], "2218": 151, "222": [64, 67, 70, 87, 152], "2222": [56, 58, 73, 106], "222261": 104, "2223": 70, "222306": 83, "222652": 66, "22272803e": 120, "222843": 89, "222882": 75, "223": [90, 152], "223095": 62, "22336235": 58, "223485956098176": [78, 79], "22375856": 58, "22390": 85, "224": [64, 67, 90, 152], "2244": 151, "224897": [74, 75], "225": [17, 19, 57, 67, 94, 151, 152], "22505965": 58, "22507006e": 120, "22515044": [106, 109], "225164": 66, "225175": 89, "225195": 67, "22528": 86, "225350": 75, "225427": 71, "225574": 84, "225595": 67, "2256": 86, "22562": 86, "225635": 89, "22563538": 89, "225764": 83, "225776": 93, "225899": [106, 111], "225917": 83, "226": [64, 152], "2264": 57, "226479": 81, "226524": 89, "226598": 84, "226938": 79, "227": [64, 67, 98, 102, 152], "227018": 76, "227018245501943": 76, "227086": 83, "2271071": 36, "227128": 64, "227407": 64, "227421": 64, "2276": 57, "227679": 67, "227767": 64, "2279": 86, "227932e": 85, "228": [64, 67, 152], "228035": 86, "2281": 86, "228597e": 75, "228630": 75, "228648": 59, "228661": 65, "2287": 70, "228725": 86, "229": [59, 152], "22902276": 65, "229117": 64, "22913469": 65, "22913653": 65, "22913855": 65, "22914421": 65, "22925": 86, "22930979": 65, "229443": 89, "229452": [104, 105, 106], "229472": 85, "2295": [67, 86], "22959": 86, "229717": 64, "229726": 86, "229759": 105, "2298": 57, "229961": [74, 75], "229994": [74, 75], "22m": 105, "23": [15, 47, 51, 58, 59, 60, 62, 64, 67, 74, 75, 83, 84, 85, 89, 92, 93, 97, 102, 104, 105, 106, 120, 136, 148, 150, 151, 153], "230": [17, 19, 57, 64, 151, 152], "23000397": 65, "230009": [78, 79], "23005245": 65, "230055": [106, 109], "2302": 91, "2307": [58, 84, 91, 96], "2308": 92, "230821": 86, "230827": 64, "230842": 74, "230956": 63, "231": [10, 87, 98, 102, 152], "23113": 106, "231153": 75, "231281": 70, "231310": 89, "231430": 136, "231467": 106, "231734": 106, "231798": 106, "231964": 67, "231986": 89, "231e": 90, "232": [67, 152], "232040": 64, "232134": [74, 75], "232157": 75, "23261586": 120, "232868e": 75, "232959": [78, 79], "232e": 106, "233": [34, 67, 152], "233029": 74, "2331": 86, "233154": 153, "233214": 65, "23324169": 120, "233415": 86, "2335": 57, "23368": 86, "233705": 75, "23391813": 65, "234": [64, 151, 152], "23404391": [106, 109], "234137": 93, "234153": 93, "23450361": 120, "234534": 76, "234605": 68, "234910": 84, "235": [64, 67, 151, 152], "235291": 74, "235423": 64, "23549489": [106, 109], "2359": 153, "236": 152, "236008": 76, "236015e": 74, "23611": 86, "236296": 67, "236411": 89, "23690345e": 120, "237": [60, 152], "2371123404547515660657682848687889396": 120, "237115": 75, "237192": 64, "237200e": 74, "237226": 67, "237341": 74, "237461": 92, "23751359e": 120, "237684": 65, "237776": 67, "237893": 67, "238": [58, 84, 152], "238012": 89, "23801203": 89, "238096": 66, "238101": 89, "238225": 136, "238251": 76, "238529": 26, "23856": 86, "238619": 71, "239": 152, "239019": 81, "239062": 65, "239243": 74, "239313": 75, "239352": 86, "239446": 67, "239471": 70, "239621": 66, "23968": 86, "239733": 64, "239845": 86, "23e": 59, "24": [58, 59, 60, 64, 67, 74, 75, 83, 84, 85, 89, 92, 93, 94, 104, 105, 106, 120, 136, 150, 151, 152, 153], "240": 83, "240127": [74, 75], "240146": 75, "240295": 92, "2403": 91, "240443": 64, "240532": [74, 75], "240601": [106, 109], "2407": 57, "24080030a4d": 60, "240813": 80, "241": 152, "241049": 89, "241064": 75, "24110604": 120, "241459": 66, "241503": 104, "2416": 57, "241645": 75, "241678": 74, "241827": 75, "241841": 88, "241962": 93, "241973": 86, "241e": 90, "242": [151, 152], "242124": [85, 86], "242139": 136, "242158": [85, 86], "2422": 70, "2424": 81, "242427": 81, "242559": 62, "242815": 136, "242864": 86, "242902": 89, "243": [86, 152], "243056": 66, "2430561": 57, "243246": 89, "243457": 88, "2438": 86, "24390561": 67, "243e": 90, "244": [44, 86, 152], "244455": 89, "2445": 86, "244622": 136, "24469564": 150, "244722": 67, "245": [151, 152], "245027": 70, "245062": 89, "2451": 57, "24510393": 59, "24516115": 67, "245370": 84, "245512": 89, "245531": 74, "245566": 67, "245720": 63, "246": 152, "246624": 104, "2467506": 58, "246753": 89, "2468": 86, "246879": 89, "247": [90, 152], "247020": 76, "247057e": 89, "2472": 86, "247617": 104, "24774": [85, 86], "247806": 67, "247826": 84, "247977": 74, "248": 86, "248113": 67, "248171": 89, "248441": 104, "248487": 83, "248638": 76, "249": [58, 84, 87, 152], "2491": 86, "249100": 84, "24917": 86, "249306": 83, "249601": [106, 107, 111], "2499": [65, 99, 102], "25": [17, 18, 19, 29, 30, 31, 35, 39, 40, 41, 42, 58, 59, 60, 63, 64, 67, 70, 74, 75, 76, 77, 83, 84, 85, 86, 89, 93, 104, 105, 106, 120, 136, 150, 153], "250": [55, 87, 152], "2500": [65, 86, 99, 102, 106, 111], "25000000000000006": [76, 86, 89], "250083": 86, "250092": 67, "2501": 86, "250210": 76, "250341": [106, 109], "250425": 76, "250529": 83, "2506": 91, "250691": 64, "250838": 83, "251": [85, 86, 92], "251128": 67, "251152": 86, "251412": 75, "25143306": 120, "251480": 75, "252": 86, "252133": 86, "252253": 92, "252280": 64, "25240463": 106, "252524": 89, "252601": 136, "253026": [74, 75], "253675": 85, "253724": 89, "25374": 86, "254": [86, 152], "25401679": 58, "254038": 79, "254083": 75, "2543": 86, "254324": 76, "254400": 136, "2545": 65, "255": [86, 152], "255034e": 75, "255151e": 89, "255377": 64, "255995": 74, "256": [86, 105], "256082": 104, "256416": 89, "25643": 86, "256567": 84, "25672": 86, "2568": 65, "256944": 89, "257019": 75, "257037": 86, "257207": 58, "257377": 63, "2575": [106, 109], "257523": 74, "257565": 67, "257762": 89, "257888": 67, "25799809": 65, "258042": 65, "258083": 75, "258158": [74, 75], "2584": 86, "2585053": 65, "258522": 74, "258541e": 30, "25855": 62, "2591489": 65, "259164": 75, "259318": 67, "259367": 104, "259395": 80, "2594": [59, 85], "259828": [74, 75], "259875": 75, "25992344": 65, "25x_3": 63, "26": [58, 59, 60, 62, 64, 67, 68, 70, 74, 75, 83, 84, 85, 97, 102, 104, 105, 106, 120, 136, 150], "26016": 86, "260161": [28, 104], "260211": [74, 75], "260356": 85, "260360": 89, "260684e": 64, "260738": [106, 111], "260762": 71, "261": 90, "2610": 86, "261318": 64, "2615": 70, "261624": [85, 86], "261685": 86, "261686": 83, "261771": 67, "261836": 64, "261903": 84, "2619317": 58, "2620": 86, "262083": 83, "262204": 86, "262829": 120, "263": [10, 86, 152], "2633": 86, "263942e": 75, "264": [151, 152], "264086": 63, "26410825": 67, "264426": 83, "265": 152, "2651": 106, "265119": 88, "2652": [60, 85, 86], "265363": 67, "265410": 64, "265547": 86, "265669": 70, "265787": 64, "2658": 79, "265929": 90, "266": 152, "266147": 90, "2663649": 120, "266686": 74, "266724": 86, "266922": 136, "267": 87, "2670691": 58, "267164": 83, "267305": 64, "267500": 84, "267581": 86, "267950": 89, "268": 152, "268628e": 75, "268720": 67, "268942": 89, "268943": 74, "268977": 86, "268998": 59, "269043": 89, "269112": 106, "26bd56a6": 60, "26e": 59, "27": [17, 18, 19, 35, 39, 56, 58, 59, 60, 61, 62, 64, 67, 68, 70, 74, 75, 83, 84, 85, 97, 102, 104, 105, 106, 120, 136, 150, 151], "270": 152, "2700": 60, "270000e": 86, "270248": [106, 111], "270474": 67, "270644": [74, 75], "27066": 86, "271": [55, 152], "271004": [85, 86], "271058": 67, "271183": 85, "2714838731": 84, "271556": [106, 107], "271867": 84, "271951": 87, "272332e": 74, "272408": 75, "272505": 83, "272610": [106, 109], "272643": 86, "272773": 64, "272945e": 87, "273": 60, "273208": 86, "273299": 75, "273356": 76, "273498": 64, "27371": [59, 85], "27372": [59, 85], "273737": 67, "274": [60, 86], "2740991": 57, "274106": 87, "274251e": 85, "27429763": [106, 108], "27461519": [106, 111], "27472": 86, "274793": 89, "274825": [29, 104], "2754": 57, "275471": 67, "275538": [98, 102], "275596": 136, "275775": 64, "276": [60, 152], "2760": 86, "276148": 89, "276189e": 84, "2762": 86, "276213": 64, "276311": 86, "27638964": 67, "2766091": 59, "27695": 86, "277": 120, "277179": 70, "277299": 68, "277421": 64, "277512": 75, "27794072": [106, 109], "277987": 83, "278": [78, 92, 152], "2780": 58, "278000": 84, "278035": 71, "278391": 86, "278434": 78, "278454": 81, "2786": 136, "278724": 70, "278804": 75, "278881": 64, "278907": 89, "279": 152, "27927": 86, "2793": 65, "279451": 67, "27951256e": 120, "279524": 89, "279595": 71, "279824": 70, "27991": 86, "279933e": 75, "28": [58, 59, 60, 64, 66, 67, 74, 75, 77, 83, 84, 85, 104, 105, 106, 120, 136, 150, 152], "280196": 79, "280454dd": 60, "280514": 136, "280543": 67, "280963": 88, "281": [90, 152], "281024": 89, "28111364": 59, "2815": 86, "2816626": 120, "2818": 57, "2819": 136, "282": [90, 151, 152], "282200": 79, "282216": 64, "282435": 67, "2825": [148, 150], "2830": [148, 150], "283041": 74, "283207": 74, "28326": 86, "283386": 74, "2836": 57, "2836059": 58, "28367": 86, "283822": 64, "2838546": [106, 109], "283992": 74, "283e": 90, "284": 152, "284073": 89, "28425026": 92, "284271": 80, "284397": 153, "28452": [59, 85], "284775": 64, "2849": 86, "284987": 86, "285": [90, 106, 152], "285001": 71, "285160": 84, "285276": 89, "285483": 81, "285511": 64, "285748": 67, "285783": 70, "285928": 70, "285e": 90, "286": 152, "286203": 74, "286285": 64, "286371": 74, "2865": [57, 86], "286507": 76, "286563e": 86, "286971": 64, "287": 152, "287041": 89, "287123": 104, "287196": 74, "287598": 64, "287631": 66, "287746": 67, "287815": 92, "287926": 89, "288": [87, 152], "288157": 64, "289": [151, 152], "289062": [85, 86], "289081": 89, "289170": 84, "2891908": 120, "289357": 75, "289440": [74, 75], "289555": 81, "289983": 83, "29": [58, 59, 60, 64, 67, 74, 75, 83, 84, 85, 92, 104, 105, 106, 120, 136, 150], "290": 106, "290565": 75, "290736e": 75, "290852": 67, "290901": 71, "290987": 85, "291": [55, 90, 152], "2910": 86, "291008": 74, "291011": [106, 108], "291071": 89, "29107127": 89, "291406": 89, "291434": 75, "291500e": [85, 86], "291517": [74, 75], "29168951": [106, 111], "291963": 89, "292": [88, 152], "2920": 86, "292028": 76, "292047": 136, "292105": 89, "2925": 60, "292862": 67, "292997": 89, "29299726": 89, "293218": 89, "293669": 86, "293935": 64, "294": 152, "294067": [74, 75], "294123": 104, "294134": 64, "294449": 74, "295": [151, 152], "295307": 74, "295625": 64, "295837": [68, 97, 102, 150], "2958370000000100000100": [60, 97, 102, 150], "2958370001000010011100": [60, 97, 102, 150], "2958371000000010010100": [60, 97, 102, 150], "295855642811191": 76, "295856": 76, "295868": 86, "296070e": 67, "296099": 71, "2962": 70, "296389": 67, "296714": 67, "296729": 84, "29675887": 89, "296759": 89, "29678199": [95, 121], "296901": 74, "296989": 64, "297": 152, "297276": [106, 111], "297287": [74, 75], "297349": [78, 79], "297611": 67, "297682": 89, "2977": 86, "29784405": 92, "298": [34, 60], "298076": 74, "298120": 76, "298150": 83, "298229": 64, "298235e": 86, "298327": 71, "299": [60, 90], "299398": 70, "299537": 79, "299755": 83, "2999": 71, "29999": 64, "2_": [36, 61, 94, 137, 138, 147], "2_x": [36, 61, 94], "2d": [13, 121, 130], "2dx_5": [76, 89], "2e": [55, 57, 58, 59, 60, 61, 105, 106, 121, 136, 150], "2f": 80, "2m": [137, 143, 147], "2n_t": 63, "2x": 89, "2x_0": [32, 74, 75, 78, 79], "2x_4": 63, "3": [12, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 44, 45, 46, 47, 50, 51, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 111, 112, 120, 121, 136, 137, 143, 148, 149, 150, 151, 152], "30": [32, 55, 56, 58, 60, 61, 62, 64, 66, 67, 70, 71, 72, 73, 74, 75, 76, 83, 84, 85, 86, 89, 90, 104, 105, 106, 120, 136, 150], "300": [56, 73, 76, 86, 89, 96, 151], "3000": 71, "30000": 64, "30000000000000004": [76, 86, 89], "300031": 74, "30031116e": 120, "300892e": 75, "30093956": 92, "301": 60, "301149": 64, "301366": 136, "301371": 89, "3016": 85, "301737": 74, "30175": 86, "30186": 86, "301901": 64, "301912": 65, "302149": 74, "30227357": [106, 109], "30229388": 89, "302294": 89, "302648": 84, "303": 87, "303007": 74, "303324": 84, "303835": 84, "303f00f0bd62": 60, "304130": 89, "304159": 89, "304195": 67, "304201": 63, "304559": 64, "305142": 83, "305272": 62, "30529": 86, "305341": 89, "305612": 84, "305687": 89, "305775": 89, "305850": 64, "305b": 60, "30628774": 89, "306288": 89, "306297": 74, "30645": 86, "30672815": 58, "306758": 66, "306863": 67, "306915": 84, "306963": 89, "307139": 64, "307407": 89, "307444": 83, "30798": 64, "308": 86, "308239": 86, "308568": 75, "308680": 67, "308774": 74, "308841": 64, "30917769": [78, 79], "309464": 86, "309577": 67, "309772": 84, "309901": 70, "31": [58, 59, 60, 61, 64, 66, 67, 70, 74, 75, 83, 84, 85, 104, 105, 106, 120, 136, 150, 153], "310711": 64, "310883": 64, "311": 90, "311205": 66, "311321": 75, "311667": 75, "311740": 83, "311869": 104, "312030": 86, "3121080": 120, "312172": 78, "3125": 86, "312652": 90, "312869": 67, "313": 106, "313056": 136, "313162": 67, "313168": 67, "313209": 76, "313409": 64, "313535": 89, "31378": 60, "313870": 81, "314": 120, "3141": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 58, 60, 61, 68, 84, 95, 97, 102, 104, 105, 106, 121, 136, 150], "314247": 93, "314341": 74, "3146": 30, "314625": 75, "31476": [85, 86], "314781": 78, "315": 152, "315031": 93, "315036": 74, "315155": 75, "315290": [78, 79], "315310": 74, "315769e": 74, "316": [60, 152], "316010": 67, "316193": 89, "31633": 86, "316407": 106, "316717": [74, 75], "316826": 74, "316863": 75, "317394": 63, "317487": 89, "317607": 89, "317946": 70, "318": [60, 152], "318584": 136, "318740": 67, "318753": [78, 79], "319": [55, 60, 152], "319014": 67, "31907984": 64, "319100": [78, 79], "31910229": [106, 111], "319420": 81, "319634": 86, "319759": 89, "31978718": 120, "319850": 89, "319903": 62, "32": [38, 58, 59, 60, 64, 66, 67, 74, 75, 83, 84, 85, 86, 94, 104, 105, 106, 108, 120, 121, 136, 150], "320": 86, "320000e": 86, "320006": 64, "320127": 64, "320314": 85, "320633": 76, "32068813": 120, "321": 152, "321610": 64, "321686": 136, "321997": 67, "322150": 64, "322186e": 74, "322404": 92, "322751": 75, "323": 87, "3235": 86, "323636": 85, "32366503": [106, 109], "323679": 84, "324": [59, 152], "32458367": 58, "3245837": 89, "32484478": 67, "324871e": 67, "325046": 86, "325056": 89, "325370": 70, "3253969": 120, "32547467": 67, "325486": 74, "325599": 74, "325819": 86, "325857": 64, "326": 90, "326121e": 64, "326148": 75, "326351": 67, "3264": 70, "326452": 64, "326721": 75, "326740": 89, "32674263": 66, "326871": 93, "3268714482135149": 93, "327": [86, 152], "327257": 74, "327343": 67, "327803": 104, "32789": 86, "327958": 71, "328367": 86, "328467": 64, "328471": 74, "32875335": 66, "329": 87, "329310": 64, "32950022e": 120, "329660": 64, "329850": 70, "329868": 64, "3299": 70, "33": [58, 59, 60, 64, 67, 74, 75, 84, 85, 86, 104, 105, 106, 120, 136, 150, 151], "330": 152, "3300": [59, 85], "330000e": 86, "330068": 74, "330100": 74, "330163": 75, "330285": [74, 75], "3304269": 58, "330615": 89, "33065771": 89, "330658": 89, "330669e": 67, "330731": [29, 104], "331215": 83, "331521": 89, "33153576": 67, "331602": 86, "33168943": 86, "33175566": 89, "331756": 89, "331823": 64, "331913": 78, "33219688": 120, "332502": 75, "332782": [29, 104], "3329": 86, "332996": 84, "333250": 86, "3333": [56, 58, 73, 104, 105, 106], "3333333": 60, "33333333": [64, 66, 67], "33335939e": 120, "3334": [70, 86], "333581": 85, "333655": 74, "333704": 75, "333914": 70, "333955": 75, "334": 59, "334717": 70, "334750": 76, "335278": 67, "335446": 71, "335609e": 89, "335846": 89, "33585927": 64, "336": 152, "33613": [106, 107], "336293": 67, "336324": 64, "336382": 75, "336461": 86, "336498": 89, "336601": 67, "336612": 63, "33673715": 120, "336870": 65, "337": 87, "3371": 86, "33755194": 120, "3376": 57, "338": 92, "338443": 67, "33849": 86, "3386": 120, "338603": 74, "338671": 64, "338775": 76, "338855": 86, "338908": 76, "339268": 89, "339269": 92, "339443": 75, "339570": 89, "339762e": 86, "339871": 67, "339875": [78, 79], "34": [56, 57, 58, 59, 60, 61, 64, 66, 67, 74, 75, 84, 85, 92, 104, 105, 106, 120, 136, 153], "340": [55, 59, 86], "340029": 75, "340142": 106, "340189": 67, "340217": 65, "340235": 81, "340274": 92, "340586": 67, "340712": 86, "341336": 27, "341653": 64, "341755e": 74, "342117": 81, "3422": 86, "342362": 71, "342506": 67, "342675": 58, "34287815": 92, "342992": 84, "343": [90, 152], "343368": 64, "343639": 104, "343685": 74, "34375": 85, "343828": 74, "344212": 153, "344440": 104, "34450402": 66, "344505": [85, 86], "344579": [106, 109], "344609": 67, "344640": 89, "344748": 88, "344787": [74, 75], "344834": 63, "345": 152, "34506089": 64, "3454": 86, "345852": 75, "345903": 89, "345989": 74, "346107": 85, "346206": 89, "346238": 92, "346269": 75, "346678": 88, "346683": 76, "3466832975109777": 76, "346964": 74, "347213": 84, "347310": [29, 104], "348": 90, "348319": 75, "348338": 86, "34860096": 65, "348617": 89, "348622": 90, "348697": 76, "3486970271639334": 76, "34869971": 65, "348700": 75, "348732": 64, "348980e": 75, "34917586": 65, "349213": 66, "3492131": 57, "349638": 75, "34967621": 58, "34968055": 67, "349772": 79, "34980811": 82, "349875": 65, "349900e": 86, "34m": 105, "34mglmnet": 105, "34mmlr3": 105, "34mmlr3learner": 105, "34mmlr3pipelin": 105, "34mranger": 105, "34mrpart": 105, "35": [55, 59, 60, 64, 67, 74, 75, 76, 78, 84, 85, 86, 89, 104, 105, 106, 120, 136, 137, 143, 153], "3500000000000001": [76, 86, 89], "350165": 105, "350208": 74, "350323": 86, "350518": 89, "350533": 89, "35053317": 89, "350712": [78, 79], "35077502": [137, 143], "351220": 75, "35155": 86, "351963": 64, "352": [59, 84], "352246": 89, "352250e": [85, 86], "3522697": 58, "35283906": 67, "352867": 67, "35292": 86, "353105": 30, "353325": 64, "35365143": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "353748e": 89, "3538": 57, "354": 86, "354188": 63, "354371": 89, "354644": 64, "354688": 37, "355065": 71, "355627": 104, "355651": 75, "35596028": 65, "35596038": 65, "35596085": 65, "35596313": 65, "356077": 67, "356089": 67, "356136e": 86, "356167": 79, "356188": 64, "35620768e": 120, "356268": 64, "3566": 86, "3568": 106, "356963": 87, "357": 86, "357092": 64, "357170": 74, "35731523": [106, 108], "357468": 64, "357586986897548": 62, "357654": 86, "358158": [85, 153], "358289": 84, "358361": 66, "358395": 92, "358690e": 86, "358799": 136, "3589551": 67, "359": [90, 153], "359083": 70, "359161e": 74, "359229": 71, "3593": [82, 92], "359307": 75, "3598": 86, "35th": 151, "36": [59, 60, 64, 66, 67, 70, 74, 75, 84, 85, 104, 105, 106, 120, 136], "360004": 89, "360065": 136, "360249": 80, "360475": [74, 75], "360572": 75, "360580": 67, "360683": 76, "360801": 76, "360965": [98, 102], "361": 90, "361374": [106, 109], "361521": 37, "361623": 81, "3619201": 33, "36213": 86, "362157": 74, "362216": 66, "36231307e": 120, "362760": 86, "3628028": 120, "363103": 76, "3631031251500065": 76, "363276": 58, "363420": 64, "363576": 83, "363669": 67, "363712": 67, "363934": 66, "364221": 74, "364276": 83, "3643": 136, "364340": [106, 109], "364595": 58, "3647": 60, "364800": 89, "365177": 67, "365275": 70, "365527": 86, "365551": 75, "36557195e": 120, "36566025e": 120, "365993": 67, "366188": [98, 102], "3663": 70, "366310": 74, "366718627": 58, "366950": 74, "36696349": [106, 111], "366986": 67, "367": [44, 90], "367017": 83, "367181": 74, "367323": 89, "367366": 81, "367425": 70, "367571": 76, "367625": 89, "368": [64, 66, 67, 86], "368152": 84, "3682": [59, 85, 86], "368324": 84, "368577": 104, "3687415": 120, "369419": 67, "369556": 76, "3696": 92, "369796": 89, "369869": 85, "369975": 67, "369981": 84, "36e": 120, "36m": 105, "37": [59, 66, 70, 74, 75, 81, 82, 84, 85, 104, 105, 106, 120, 136], "370000e": 86, "370165": 86, "370254e": 85, "3702770": 58, "370736": 84, "3707775": 58, "370908": 74, "3711317415516624": 76, "371132": 76, "3712": 86, "371244": 67, "371357": [85, 86], "371429": 76, "371850e": 75, "37191227": [106, 109], "372": 151, "37200": [85, 86], "372097": 76, "3722": 86, "372309": 64, "3724": 86, "372427": 75, "3727679": 58, "373235": 86, "373451": 104, "3738573": 58, "374335": 89, "37433503": 89, "3745": 86, "374812e": 86, "374862": 74, "374917e": 74, "375081": 86, "375088": 67, "375274": 74, "375403": 62, "375465": 89, "375995": 83, "376399": [106, 109], "3766": 81, "376617": 81, "376632": 67, "376760": 75, "376806": 75, "377147": 104, "377246": 86, "377311": 89, "377590": 64, "377669": 75, "377955": 89, "378068": 64, "378161": 65, "378351": 25, "378367": 89, "37839084": 120, "378588": 74, "378596": 84, "378727": 74, "378834": 89, "3788859": 58, "379": 151, "379051": 67, "379419": 64, "379614": 89, "379626": 74, "379981e": 75, "38": [60, 74, 75, 85, 104, 105, 106, 120, 136], "3800694": 58, "38064094": 67, "380837": [85, 86], "381": 90, "381072": 89, "381228": 67, "381247": 86, "381278": 86, "381374": 64, "381603": 74, "381684e": 85, "381685e": 86, "381689": 89, "382024": [98, 102], "382285": 86, "382521": 67, "382582e": 24, "382872": 76, "383297": 89, "38348": 86, "383725": 67, "384": 86, "384189": 83, "384208": 83, "384443": 75, "384677": 71, "38470495": [106, 111], "3848": 86, "3848244": 66, "384865": 75, "384883": 86, "384928": 74, "385160": 75, "3852": 70, "385240": 136, "385663": 87, "385917": 84, "386": [60, 86], "386102": 76, "386502": 86, "386831": 71, "387": 60, "38700256": 67, "387056": 64, "3871": 57, "387426": 89, "387433": 67, "3875964": 120, "387780": 89, "388005": 86, "388026": 74, "388071": 89, "388185": 71, "38818693": 120, "388216e": 105, "38898864": 89, "388989": 89, "389": 60, "389126": 106, "389489": [106, 107], "38973512e": 120, "389755": 74, "38983785": 62, "38990574": 120, "39": [55, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 70, 71, 74, 75, 80, 82, 84, 85, 86, 87, 92, 93, 94, 104, 105, 106, 120, 136], "39010121e": 120, "390126e": 67, "390155": 83, "390304": [106, 109], "390379": 89, "390681": 67, "390969": 67, "391377": 93, "391389": 70, "391454": 67, "39186467": 82, "392128": 75, "392242": 80, "392472": 86, "39255938": [106, 109], "392623": 75, "392811": 67, "392833": 92, "392864e": [85, 86], "392917": 74, "393": 153, "393053": 67, "393431": 64, "393604": 76, "393654": 71, "394149": 64, "394226": 75, "39425708": 58, "394757": 83, "395076e": 86, "395195": 84, "395268": 104, "395376": 66, "395480": 70, "395569": 74, "395603": 74, "3958": 106, "39581951": 66, "396": [44, 106], "3961": 86, "39611477": 59, "39621961e": 120, "396272": 83, "396531": 86, "396985": 84, "396992": [74, 75], "397140": 76, "397155": 75, "39727": 86, "397313": 57, "397578": 80, "397811": 92, "3979": 66, "398": [97, 102, 150], "398000e": 86, "398166": 71, "39830606": 66, "398373": 64, "3985": 86, "398770": 89, "398999": 104, "399": 59, "399056": 89, "399223": 63, "399343e": 74, "399355": 63, "399470": 70, "399679": 106, "399692": 89, "399858": 93, "39996997": 67, "39m": 105, "3cd0": 60, "3dx_1": [76, 89], "3e1c": 60, "3ec2": 60, "3f5d93": 87, "3x_": 89, "3x_4": [76, 89], "4": [12, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 36, 37, 38, 39, 44, 45, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 137, 143, 150, 152], "40": [58, 61, 62, 74, 75, 76, 85, 86, 89, 94, 97, 102, 104, 105, 106, 120, 136, 137, 143], "400": 84, "4000": [64, 67], "4000000000000001": 105, "40000000000000013": [76, 86, 89], "400113": 81, "40029364": [137, 143], "400587": 83, "400823": 89, "400905": 71, "401": [10, 153], "401009": 83, "401247": [121, 136], "40127723e": 120, "401690e": 75, "401931": [78, 79], "402": [97, 102], "402077": 86, "402113": 136, "402301e": 105, "402619": 38, "403": [87, 90], "403113": 76, "403113188429014": 76, "403425": 89, "40359107": 62, "403626490670169": 95, "4036264906701690": 95, "403626491": 95, "403715": 24, "403771948": 121, "403852": 64, "4039": 57, "404034": 64, "404261": 66, "404267": 74, "404300": 71, "404318": 57, "404411": 74, "40452": 86, "404550": 88, "404824": 86, "405000": 86, "405050": 75, "405203": 63, "4053326": 66, "405677": 70, "40576478": 66, "40583": 57, "405890": [29, 104], "40611565": 66, "406285": 89, "406446": 76, "4065173": 120, "40676": 57, "407": 90, "407065": 67, "40732": 81, "407558": 74, "407565": 74, "408476": [137, 143], "40847623": [137, 143], "408509": 75, "408539": 89, "408565": 89, "408669e": 67, "40892173": 66, "409154": 57, "4093": 92, "409390": 86, "409395": 89, "409746": 76, "409767": 64, "409848": [74, 75], "41": [74, 75, 85, 86, 104, 105, 106, 120, 136], "410100": 74, "410393": 76, "41044767": 66, "410667": 104, "410681": 63, "410682": 74, "4107": 70, "410795": 84, "410926": 67, "41093655": 120, "411132": 66, "411146e": 75, "411190": [74, 75], "411215": 67, "411264": 70, "411295": 89, "411304": [74, 75], "411454": 66, "411582": 89, "411768": 75, "411977": 70, "412": 86, "412127": 89, "412304": 93, "4123677": 120, "412406": 86, "412477": 63, "412653": 84, "412714": 76, "412726": 75, "412864": 78, "412941e": 75, "413": 86, "41318198": 66, "413247e": 74, "41336": 105, "413376": 106, "41341040": 58, "413784": 86, "414": 90, "414073": 26, "41428195": [106, 109], "414533": 75, "41495635": 67, "41525168e": 120, "415294": 89, "415375": 74, "415512": 67, "415556": 104, "415634": 64, "41566": 106, "415812": 153, "416052": 71, "416132": 75, "416470": 67, "4166": 86, "4166667": 60, "416737": 83, "416899": 74, "416919": 75, "416e": 90, "41722184": 66, "417502": 70, "417625": 64, "417640": 74, "417669": 86, "417727": 85, "41775942": 67, "417767": [78, 79], "417834": 71, "417981": 67, "41798768e": 120, "417988": 89, "418": [44, 55], "418056": 89, "41805621": 89, "418400": 81, "418591": 86, "418741": 71, "418806e": 76, "418969": 104, "41918406e": 120, "419261": 66, "419371": 89, "419871": 71, "419888": 64, "41989983e": 120, "4199952": 58, "41e5": 60, "42": [12, 15, 16, 35, 61, 62, 63, 64, 66, 70, 71, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 86, 88, 89, 92, 93, 94, 99, 102, 104, 105, 106, 107, 108, 109, 111, 120, 136, 151, 153], "420232": 64, "420308e": 86, "420360": 64, "42073312": 58, "42094064": [106, 111], "4209524": 66, "420967": 76, "421083": 57, "4211349413": 58, "421163": 74, "421200": 93, "421297e": 75, "421357": [78, 79], "42143": 86, "421521": 67, "421576e": 86, "421793": 92, "421919": 86, "422007": 92, "422116": 86, "422119": 86, "422293e": 104, "422325": 76, "422591": 75, "42284867": 66, "42310962": 66, "4235839": [78, 79], "4239072": 66, "423921e": 104, "423951": 57, "424108": 76, "424127": 120, "42412729": 58, "424292": 74, "424328": 89, "424465": 64, "424573": 83, "424589": 64, "424651": 106, "424717": 76, "424748": 93, "424879": 86, "425": 84, "425000e": 86, "425002": 87, "425103": 57, "4252": 70, "425325": 81, "425414": [98, 102], "425493": 57, "42550": 86, "425636": 81, "425832": 64, "426055": 57, "42622683": 67, "426540": 84, "426540301": 58, "42654824": 67, "426710": 64, "426734": 83, "426736": 86, "427": 86, "427486": [74, 75], "42755087": 92, "427551": 92, "427573": 84, "42757699": 66, "427654": 81, "427725": 89, "428": [136, 153], "428046": 88, "42811700": 153, "428255": 89, "428411": [85, 86], "42844476": 66, "428467": 89, "4284675": 89, "428771": [29, 104], "4290": 57, "429031": 67, "429057": 75, "429133": 83, "429230": 74, "429298": 83, "429309": 86, "42934105": 94, "429705": 74, "42ba": 60, "43": [59, 66, 71, 74, 75, 86, 104, 105, 106, 120, 136], "430298e": [85, 86], "430345": 83, "430465": [106, 111], "430595": 75, "430608": 74, "431061e": 74, "4311947070055128": 105, "431306": 89, "431437": 92, "431701914": 148, "431929": 86, "431998": 71, "4320": 70, "432130e": 85, "432300e": 89, "43231359e": 120, "432374": 67, "432390": 64, "432707": 90, "43294": 60, "432f": 60, "433": [60, 90], "4330": 86, "433221": 76, "433473": 86, "43361991": 89, "433620": 89, "433630": [106, 109], "433750": 74, "4339": 57, "434054e": 81, "434535": 89, "43453524": 89, "4347": 70, "43482": 86, "435": 60, "43503345": 106, "4352": 70, "435206": 67, "435401": 84, "43543688": 67, "435466": 64, "4357": 86, "435927": 86, "435967": 84, "436": [60, 86], "436327": 86, "4364": 70, "436764": 90, "436806": 86, "436845": 67, "437134": 86, "43752681": 66, "437603": 67, "437666": 64, "437667": 85, "437924": 86, "438": [55, 84], "438219": 89, "438537": 67, "438569": 86, "4386302": 120, "43883": 79, "438834": 75, "438855": 67, "438960": 84, "439401e": 75, "439541": [85, 86], "439675": 90, "439699": 71, "43989": 104, "43993449": 67, "43e": 120, "43f0": 60, "44": [64, 66, 71, 74, 75, 104, 105, 106, 108, 120, 136], "440180": 67, "440320": 86, "440412": 64, "440605": 105, "440700": 67, "440747": 74, "440892e": 64, "44097737": 67, "440a": 60, "441004": 86, "441068": 67, "441153": 89, "441209": 89, "44124313": 106, "44127812": 67, "441282": 74, "441311": 78, "4416552": 58, "441849": 74, "442244": 64, "442254": 64, "442847": [106, 107], "443016": 76, "443032": 85, "44312177": 59, "443672": [106, 111], "443686": 89, "443691": 64, "4438": 86, "443901": 67, "443e": 90, "444046": 86, "4444": [56, 58, 73, 106], "444500": [85, 86], "4448": 65, "444805": 83, "445": 90, "4453070": 120, "445476": 74, "44563945e": 120, "4460": 70, "446023": 88, "4462": [60, 70], "446217": 67, "44647451": 92, "44713577e": 120, "447624": [74, 75], "447669": 67, "447706": 76, "447863": 76, "447863428811719": 76, "448": 86, "448252": 75, "448456e": 75, "448569": 74, "448587": 76, "4487": 86, "448745": 89, "448842": 75, "44890536": 120, "448923": 80, "449107": 15, "449150": [29, 104], "449189": 66, "449677": 81, "449994": 64, "44fa97767be8": 60, "45": [62, 74, 75, 76, 80, 83, 86, 89, 104, 105, 106, 120, 136], "4500": 85, "45000000000000007": [76, 86, 89, 105], "450000e": 86, "450152": 84, "4505648": [106, 109], "450812e": 75, "450870601": 58, "4509": 70, "4510": 70, "451312e": 74, "45131831333639414344525872787980899197": 120, "451318313336394143445258727879808991971012161720262732343848495053545969747794891519212225284255636667707381929598992371123404547515660657682848687889396": 120, "452": 60, "452091": 86, "452114": 104, "452488701": 58, "452489": 84, "452623": 75, "453": 60, "4531084": 67, "453279": 74, "4535": 86, "4536": 86, "4539": 60, "45397249": 67, "454322": 67, "454406": 71, "45451": 86, "45467447": 120, "455": 60, "455078": 76, "455091": 75, "455107": 76, "4552": 60, "455293": 76, "4552b8af": 60, "455448": 92, "455672": 86, "45594267": [106, 109], "455981": 137, "456225": 89, "456370": 84, "456458e": 74, "4566031": 136, "45660310": 136, "456617": 89, "4567": 92, "456779": 64, "456892": 76, "456974": 67, "457088": 89, "458114": 86, "458196": [106, 109], "45828123": 67, "458307": 138, "4584": 86, "458420": 86, "4584447": 58, "458502": 67, "458784": 74, "458814": [67, 81], "458855": 59, "458977": 64, "459084": 64, "459173": 65, "4592": 58, "459200": 84, "459383": 76, "459418": 75, "45957837": 120, "459760": 86, "459812": 76, "46": [66, 70, 74, 75, 80, 87, 104, 105, 106, 120, 136], "4601": 86, "460207": [74, 75], "460218": 76, "460252": 64, "460289": 89, "460744": 85, "4610": 153, "461129": 67, "461227e": 74, "461396": 83, "461412": 104, "461469": 65, "461493": 83, "461629": 93, "462319": 86, "462451": 76, "462567": 75, "462604": 67, "46295656": 66, "462979": 74, "463": 55, "463208": 86, "46328686": [106, 109], "463325": 89, "463337": 67, "4634": 86, "463418": 93, "463766": 79, "463816": [106, 111], "463903": 75, "463b": 60, "464": 106, "464076": 76, "4642": 70, "464282": 84, "464469": 67, "46448227": 120, "4645": 70, "464668": 27, "465": 71, "4650416": 120, "46507214": 92, "465117": 67, "465476": 70, "465649": 93, "465730": 93, "4659651": 95, "465965114589023": 95, "4659651145890230": 95, "465981": 89, "46598119": 89, "4660": 70, "466047": 89, "46618738": 120, "466440": 76, "466684": 86, "466756": 89, "46709481": 120, "46722576e": 120, "46732972": 120, "467613": 84, "467613401": 58, "467681": [74, 75], "467770": 76, "468001": 86, "468045": 67, "468072": 75, "468075": 89, "46807543": 89, "468406": 86, "468449": [106, 111], "468907": 71, "468d": 60, "469": 60, "469170": 86, "469379": 86, "469474": 93, "469676": 71, "469825": 76, "469895": 75, "469905": 75, "47": [59, 70, 71, 74, 75, 78, 85, 92, 104, 105, 106, 120, 136, 152], "470055": 75, "470060": 67, "470365": 86, "470904": 74, "4709299": 120, "471": 71, "471281": 64, "471435": 90, "471454": [98, 102], "4716": 70, "471622": 74, "471666": [98, 102], "472192": 67, "472255": 86, "472699": 75, "472891": 89, "472e": 60, "473099": 76, "47319": 106, "47419634": 150, "474214": [78, 79], "474415": 67, "474731": 104, "4750272": 120, "475177": 67, "475229": 66, "475395": 86, "475569": 74, "475e": 90, "476121": [106, 109], "47615288": 67, "476304": [106, 109], "476581": 64, "476856": 76, "477130": [74, 75], "477150": 89, "477247": 75, "477354": 86, "477357": 90, "477395": 86, "477474": 84, "47759584": 120, "4776": 70, "47782": 86, "478059": 75, "478064": 75, "478399": 86, "47857478": 120, "4788": 70, "478896": 86, "47966100e": 120, "479722": 75, "479778": 64, "479851": 66, "479876": [78, 79], "479882": 75, "479928": 89, "479959": [106, 111], "47be": 60, "48": [60, 71, 74, 75, 79, 85, 86, 87, 104, 105, 106, 120, 136], "480": 71, "480000e": 86, "480133e": 89, "480199": 81, "48029755": 92, "480579": 75, "48069071": [95, 121, 136], "480691": [121, 136], "480800e": 89, "481172": 89, "481399": [85, 86], "4814": 82, "481417": 64, "481705": 106, "481713": 81, "481761e": 86, "482": [60, 71], "482038": 76, "482179": 74, "4822": 70, "482251": 38, "482349": 78, "482461": [137, 143], "48246134": [137, 143], "482483": 89, "482508": 83, "482738": 67, "482790": 63, "482898e": 75, "48296": 92, "483": [55, 90, 106], "483092": 67, "48315": 92, "483186": 63, "483192": [85, 86], "48331": 92, "483531": 89, "48353114": 89, "483642": 86, "483711": 89, "483717": 76, "483855": 86, "48390784": 106, "484": 86, "484004": 64, "48404": 58, "4842": 86, "484208": 64, "484303": 75, "484588": 64, "484640": 89, "48489102": [106, 109], "4849": [60, 70], "484915": 65, "485": [60, 86], "485197": 74, "485468": [98, 102], "48550": 93, "485617": [85, 86], "48583": [85, 86], "485871": 79, "486": [40, 86], "4860": 70, "486178e": 74, "486202": 76, "486285": 64, "486342": 86, "48638": 66, "486532": 89, "486914": 64, "487": [71, 86], "487268": 64, "487352": 65, "487467": 86, "487524": 81, "487641e": 89, "487793": 75, "487872": 72, "48812682": 120, "488394": 74, "488455": 83, "488644": 67, "488811": 89, "488854": 64, "488909": [85, 86], "488982e": 76, "489054": 88, "4890850": 120, "48918051": 67, "489488": [106, 111], "4895498": 89, "489550": 89, "489567": 91, "489699": 76, "489951": 75, "49": [60, 71, 74, 75, 104, 105, 120, 136], "490070931": 58, "490488e": [85, 86], "490689": 83, "490896": 71, "490898": 86, "490941": 86, "49098": 92, "491034": 74, "491210": 67, "491245": 84, "491249": 83, "49135": 30, "4915707": [106, 108], "492": 86, "4923156": 95, "49231564722955": 95, "492315647229550": 95, "492410": 89, "492417e": 106, "492637": 80, "492656": 75, "49270769e": 120, "493": [90, 106, 151], "4931": 86, "493144": 93, "493195": 81, "493219": 89, "493325": 12, "493426": 90, "493792": 83, "494": 90, "494089": 75, "494324": 84, "494324401": 58, "494761": 67, "494930": 67, "495": 88, "495106": 67, "49530782": 58, "495405": 89, "495657": 76, "495752": 89, "49596416e": 120, "496": 88, "496300e": 86, "49650883": 92, "496551": 89, "496591": [106, 111], "496714": 93, "496777": 153, "49693": 105, "497": 88, "497355": 64, "497398": 64, "497422": 75, "4976": 70, "497655": 15, "497893": 87, "497964": 104, "498": [86, 88], "498122": 81, "498448": 67, "498626": 67, "49869875": 67, "498873": 87, "49888811": 67, "498921": 89, "498968": 67, "498979": 86, "498992": 74, "498f": 60, "499": [88, 97, 102, 150], "499000e": [85, 86], "49d4": 60, "4a53": 60, "4b8f": 60, "4dba": 60, "4dd2": 60, "4e": [58, 59], "4ecd": 60, "4fee": 60, "4x": 89, "4x_0": [32, 74, 75, 78, 79], "4x_1": [32, 74, 75], "5": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 111, 120, 121, 129, 136, 137, 143, 149, 150, 152], "50": [29, 58, 60, 61, 63, 67, 70, 71, 76, 79, 82, 83, 85, 86, 87, 89, 104, 105, 120, 136], "500": [7, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 37, 38, 39, 42, 48, 56, 60, 64, 65, 66, 67, 68, 73, 74, 75, 78, 79, 82, 85, 88, 90, 92, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 111, 121, 136, 137, 143, 150, 153], "5000": [45, 64, 65, 66, 67, 74, 75, 76, 89, 91], "50000": 84, "500000": [85, 86], "5000000000000001": [76, 86, 89], "500000e": 86, "500084": 89, "500239": 64, "500267": 80, "5003517412": 58, "500635": 86, "50093148e": 120, "501021": 86, "501047e": 74, "501127": 87, "501203": 83, "501403": 89, "502005": 104, "502084": 106, "502107": 87, "502205": 74, "502268": 86, "502494": 76, "5025850": 58, "502595": 75, "502612": 89, "5028310": 120, "502901": 75, "502995": 89, "503": [44, 55], "503089": 89, "503504": 105, "50353441": 67, "503700": 71, "50379354": 67, "50398782e": 120, "504286": 84, "5042861": 58, "504548e": 75, "5050973": 58, "505264": 74, "505353": 75, "50536577": 67, "505795": [106, 109], "506050": 74, "506644": 74, "50672034": 58, "506900e": 89, "506903": 76, "506966": 87, "507": 90, "507285": 81, "50768b": 87, "508153": 88, "508406": 88, "508433": 74, "508459": 84, "508756": [98, 102], "5089": 86, "508947": [106, 108], "509059": 86, "509102": 86, "509196": 89, "509461": 89, "509651": 83, "50967": 93, "509692": 67, "5097": 93, "5098": [68, 97, 102, 150], "509853": 89, "5099": [60, 68, 97, 102, 150], "509951": 76, "509958": 84, "51": [57, 59, 60, 66, 71, 78, 104, 105, 120, 136, 152, 153], "510000e": [85, 86], "5101": 70, "510121": 74, "510385": 84, "510449": 64, "510555": 71, "510586": 89, "51079110": 58, "51089359": 67, "510971": 83, "511113": 64, "5115547": 95, "5115547181877": 95, "51155471818770": 95, "511665": 74, "511668": 93, "5116683753999616": 93, "5118": 70, "512": 84, "512108": 89, "512149": 89, "51214922": 89, "51243406e": 120, "512519": 84, "512572": 89, "512657e": 86, "512672": [106, 137, 143], "512688": 67, "512725": 67, "513061": 64, "5131": 85, "513222": [64, 81], "5135": 70, "513624": 104, "513658": 86, "513992": 89, "514": 60, "514173": 75, "514775": 67, "51494845": [106, 111], "515031": 74, "515338e": 74, "515358": 76, "5154": 86, "5154782580830215": 84, "5155": 60, "515672": 75, "516": 60, "516125": 76, "516145": 86, "516222": 89, "516242": 75, "516255": 89, "516256": 89, "516479": 67, "516528": 89, "5166": 70, "516797": 75, "517": [60, 84], "517279": 75, "5175": 86, "517510812139451": 62, "517753": 74, "517798": 71, "518175": 84, "518375": 75, "518405e": 67, "518439919": 120, "51846698": 67, "518478": 71, "518610": 106, "518845": 67, "518846": 84, "518854": 71, "518978": 83, "519530": 67, "51966955": 58, "519710": 89, "519993": 67, "52": [57, 60, 80, 83, 90, 104, 105, 120, 136], "520": 86, "520096": 83, "520412": 67, "520415": 74, "520605": 67, "520641": 92, "520930": 76, "521002": 76, "521104": 86, "521233": 71, "5213": 70, "521344": 64, "521563e": 67, "521611": 75, "521632": 74, "521788": 74, "522398": 83, "522605": 86, "522753": 37, "522835": 63, "523030": 93, "523163": 76, "52343523e": 120, "523556": 64, "523794e": 89, "523977545": 58, "524182": 64, "524215": 86, "52424539": 58, "524657": 89, "524817": 83, "524934": [74, 75], "525064": 71, "52510803": 59, "5251546891842586": 93, "525361": 64, "5255": 60, "525722": 74, "52590": [59, 85], "526": 84, "526532": 86, "526582": 81, "526769": [74, 75], "526984": 75, "527226": 74, "52732": 105, "52732041": 67, "527452": 75, "52749098": 67, "527540": 74, "527644e": 89, "5277": 70, "528000e": 92, "5282587": 120, "528381e": 94, "528504": 64, "528573": 70, "52857988": 120, "528725": 89, "528763": 75, "528937": [78, 79], "528996901": 58, "528997": 84, "529": 84, "529405": 57, "529468": 104, "529782": 57, "52987076": 67, "53": [57, 60, 71, 80, 84, 97, 102, 104, 105, 106, 108, 120, 136, 148, 151, 153], "5301": 86, "530659": 80, "530793": 74, "530830": 86, "530940": 89, "53094017": 89, "531": 60, "531223": 76, "531586": 67, "531594": 86, "531682": 70, "531898": 64, "531999": 86, "53209683": 106, "532266": 76, "532421": 78, "53257": 105, "532738": 89, "53273833": 89, "5328": 86, "533": 90, "533283": 106, "533316": 86, "533489": 63, "533871": 86, "533900": 89, "534139": 71, "5346": 60, "53511151": 67, "535179": 89, "535318": 89, "53606675": 89, "536067": 89, "536778e": 75, "536792": 86, "536798e": [85, 86], "537438": [106, 107], "537681": 86, "537748": 67, "53791422": [94, 106], "538": 60, "538105": 75, "5382": 92, "538282": 86, "538410": 70, "53849791": 89, "538498": 89, "538513": [98, 102], "538527": 64, "538937": [85, 86], "539047": 67, "539455": 89, "539491": [78, 79], "539767": 76, "54": [57, 59, 60, 70, 83, 90, 96, 104, 105, 120, 136, 152, 153], "540068": 67, "540101": 89, "54010127": 89, "54014493": [106, 109], "540240": 86, "540375": 81, "5405": 81, "540549": 81, "540578": 83, "5408": 57, "541": 90, "541010": 89, "541060": 81, "541159": 89, "54119805": 62, "54139818": 67, "54163": 92, "5416844": 95, "541684435562712": 95, "542": 90, "542136": 74, "542159": 75, "542170": 75, "542268": [106, 107, 111], "5424": 70, "542446": 79, "542451": 89, "542560": 93, "542647": 89, "542648": 104, "542671": 84, "542769": 89, "542883": [137, 143], "5428834": [137, 143], "542919": 104, "542989": 89, "543": [84, 86], "543014": 86, "543052": 71, "543075": 76, "543136": 76, "543380": 84, "5434231": 95, "543423145188043": 95, "5436005": 58, "5436876323961168": 76, "543688": 76, "543691": 75, "543734": 86, "54373574": [106, 109], "543764": 79, "54378": 92, "544097": 89, "544178": 67, "544383": 93, "544555": 84, "54483": [121, 136], "5448331": [121, 136], "54517706e": 120, "545492": 71, "54550506": 90, "545605e": 89, "545930": 104, "54621856": 67, "546260": 83, "546438": 86, "546866e": 67, "546928": [106, 109], "546953": 75, "547039": 74, "54716": 92, "547306": 76, "5473064979425033": 76, "547324": 75, "547482": 70, "5475": 86, "5479": 86, "547909": 86, "54857": 66, "5494": 86, "549789": 66, "5499725": 67, "55": [59, 60, 76, 83, 85, 86, 89, 104, 105, 120, 136, 153], "5500000000000002": [76, 86, 89], "550176": 83, "550611": 67, "551010e": 86, "551317": 75, "551686": 76, "55176": 105, "552071": 64, "552081": 83, "552694e": 74, "552727": 84, "552776": 89, "553004": 71, "55307": 105, "55321626": 67, "553499": 66, "553522": 75, "553672": 86, "553754": 104, "553878": [28, 104], "554076": 76, "554793": 67, "554793e": 120, "554937": [106, 109], "554986": 76, "554986206618521": 76, "555": 84, "5550": 70, "555137": 75, "555445": 88, "555498": 89, "5555": [56, 73], "555536": 74, "555954": 86, "556191": [74, 75], "55642562": [106, 109], "556533e": 86, "556792": 89, "5574dcd4": 60, "557595": 84, "557741": 83, "5578": 65, "557942": 67, "557999": 84, "558134": [74, 75], "55820564": [106, 109], "55828259": 94, "558332": 64, "5584": 84, "5585": 84, "55855122": 62, "55863386": 106, "558655": 76, "5589": 84, "558954": 64, "558996": 86, "559": 153, "5590": 84, "559144": 76, "559186": 76, "5592": 84, "559394": 89, "559522": 89, "559592e": 74, "559680": 86, "559712": [98, 102], "55dc37e31fb1": 60, "55e": 59, "56": [60, 70, 85, 96, 104, 105, 120, 136, 148, 151, 153], "560044": 66, "560135": [121, 136], "56018481": 89, "560185": 89, "560440": 70, "560530": 75, "560689": 57, "560723": 80, "561119": 64, "561196": 64, "561232e": 67, "561309": 86, "561348": 75, "5614534": [106, 109], "5616": 85, "561785": [106, 108], "561883": 38, "562013": 89, "56223": 92, "562288": 93, "562390": 104, "562556": 66, "5625561": 57, "562698": 65, "562712": [74, 75], "562866": 86, "563067": 90, "56327232": 67, "563374e": 76, "563503": 89, "563563": 81, "563673": 86, "563690": 86, "563760": 86, "5638": 86, "56387280e": 120, "56390147e": 120, "56403823": [106, 109], "564045": 89, "564073": 86, "564232": [74, 75], "56425415": [106, 109], "5644": 70, "564451": 75, "564577": 86, "564800e": 86, "564829": 76, "565": 106, "5650": 70, "565066": 76, "565373": 74, "56553": 120, "565915": 86, "566": 93, "566024": 89, "56611407": [106, 109], "566388": 74, "566600": 89, "56670073": 89, "566701": 89, "567004": 92, "567364": 75, "567695": 74, "567945": [78, 79], "568071": 86, "568287": 81, "5686484": 120, "5687224": 67, "56876517": [106, 109], "568874": 67, "568932": [106, 111], "569315e": 75, "569449": 104, "569540": 75, "56969222": 120, "569911": 58, "5699994715": 58, "57": [60, 85, 90, 104, 105, 120, 136, 153], "57015458": [106, 109], "5702": 70, "570486": 57, "570562": 57, "570692": 67, "570722": 150, "5708": 86, "570936": 74, "57111297": 67, "571429": 76, "5714294154804167": 76, "571694": 67, "571778": 57, "5718": 86, "5722": 85, "572408e": 75, "57245066": 89, "572451": 89, "572717": 91, "572730": 70, "572991": 75, "5732": 70, "573689": 70, "573700": 63, "573750": 64, "57398537": 120, "574": 60, "574160": 81, "5748": 105, "57496671": 58, "575": 11, "575329": 70, "575381": 85, "57572422": 92, "575810": 74, "57585824": 92, "57592948e": 120, "57599221": 92, "575e": 90, "576": 60, "5763996": 58, "57643609": 92, "576879": 84, "577": 60, "5770": 85, "57715074": 58, "577271": 84, "577273": 74, "57751232": 86, "577541": 67, "5776971": 92, "57775704": 92, "577807": [74, 75], "577813": 74, "577884": 86, "577e": 90, "578081": 86, "578307": 89, "57843836": [106, 109], "578557": 75, "578839": 65, "578847e": 76, "579125": 85, "57914935": 59, "579197": 81, "579213": 93, "579238": 76, "579322e": 85, "579521": 88, "579875e": 74, "57e": 59, "58": [31, 59, 85, 93, 104, 105, 120, 136, 152], "5800": 86, "58000": 85, "5804": [60, 86], "580414": 93, "580751": 85, "580853": 74, "580922": 78, "581849": 75, "58185345": 67, "581880": 83, "582031": 85, "582146": 75, "582331": 86, "58241568": 120, "5825085": [106, 109], "582754": 91, "582761": 76, "582776": 66, "582991": 85, "583052e": 64, "583076": 86, "583195": [74, 75], "583201": 75, "5833333": 60, "583534": 89, "583730": 67, "583883": 67, "5840": 70, "584057e": 74, "584742": 79, "584849": 76, "584928": 74, "585": 55, "5852": 86, "585394": [106, 107], "585426": 120, "585479": 81, "585575": 66, "585793": 76, "586362": 89, "5864": 57, "5867": 86, "586794": 74, "5868472": 58, "587135": 75, "587594": 86, "587671": 87, "587690e": 67, "587801": 70, "588044": 67, "58812": 105, "5882": 85, "588233": 74, "588364": 104, "588396": 76, "5883964619856044": 76, "588476": 64, "588854": 74, "589958": 75, "59": [64, 75, 104, 105, 106, 120, 136], "590": 86, "590098": 76, "590320": 63, "5905": 85, "590504": 67, "590530": [106, 109], "590736": 89, "590738": 89, "590751": 67, "590813": 89, "590904": 75, "590911": 76, "590990": 64, "590991": 76, "591080": 63, "591441": 24, "591652": 85, "591678": 85, "59199423e": 120, "592093": 78, "592186": 75, "592337": 67, "592681e": 76, "59300411": [106, 109], "59307502e": 120, "5931003": [106, 109], "593648": 105, "593981": 104, "594": 11, "594316e": 89, "594317": 70, "594532": 67, "594665": 67, "595353": 76, "59563003": [106, 111], "596": 86, "596076e": 86, "596270": [78, 79], "596460": 75, "59673812": 120, "596758": 74, "597": 59, "597190": 67, "5979": 70, "597923": 86, "59798": 86, "59827652": [106, 109], "598371": 86, "59854797": 106, "5985730": 59, "598761e": 75, "599141": 67, "599297": [104, 105, 106], "599334": [106, 111], "599586e": 83, "59969": 67, "59970": 67, "5998908": 67, "5cb31a99b9cc": 60, "5d": [76, 89], "5x_2": 63, "5x_3": 63, "5z_i": 89, "6": [11, 12, 13, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 70, 71, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 97, 98, 99, 101, 102, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 137, 143, 148, 150, 151, 152], "60": [58, 61, 62, 70, 76, 86, 87, 89, 94, 104, 105, 120, 136, 151], "600": 84, "6000": 86, "6000000000000002": [76, 86, 89], "600000e": 86, "600195": 74, "600254": 88, "600694": 106, "600776": 71, "601": 59, "601061": 76, "60113395": [106, 109], "601290": 67, "601598": 84, "601757": 86, "601783e": 75, "601984": 75, "602079": 79, "602168": 76, "602322": 104, "602386e": 74, "602492": 74, "60253920": 120, "602569": 64, "602587": 89, "602628": 76, "604011": 67, "604110": 86, "604532": 86, "60458433": [106, 109], "604603": 15, "60481177": [106, 109], "60482878": 65, "604841": [85, 86], "60488374": [106, 109], "605": 86, "605041": 67, "605130": 67, "605195": 79, "6054678": 65, "606034": 89, "606129": 89, "606342": 76, "606733": 86, "6068": [58, 120], "606800": 84, "606954": 76, "607264": 74, "6075": 153, "60750488": 67, "60753412": [106, 109], "607618": 89, "607664e": 64, "60788751": 65, "607900e": 75, "60795263": [106, 109], "608": [77, 90], "60857": 57, "608818": 92, "6088417": 65, "60884391": 65, "6088451": 65, "60885387": 65, "609": 90, "609575": [106, 108], "60960239": 65, "61": [64, 71, 104, 105, 120, 136, 152], "610045": 64, "610087": 64, "610318": 71, "610703": 67, "611": 153, "611269": 84, "61170069": 90, "611859": 79, "611995": 86, "612": 90, "612246": 81, "612792": 86, "613244": 75, "6133": 59, "61336179": 67, "613408": 89, "613498": 86, "613574": 80, "613622": 75, "613691": [106, 108], "613950": 76, "614": 90, "61404894": 106, "614188": 84, "6144": 70, "615": 71, "6151207": 120, "61518888": 67, "615209": 67, "61545173": 67, "615498": 24, "615863": [78, 79], "616": 87, "616011": 64, "616086": 86, "616116": 86, "616372": 85, "616617": 75, "61669761": [137, 143], "616698": [137, 143], "616828": 86, "6171256": 67, "61728": 104, "6173": 60, "617481": 86, "61771229": 90, "618069": 85, "61810738": 59, "6181471": 120, "618574": 75, "618722": 86, "618753": 86, "618881": 75, "619": 90, "619128": 75, "619294": 71, "619351": [74, 75], "619390": [74, 75], "619454": 63, "619613": 85, "61988459": 67, "619903": 75, "61e": [59, 153], "62": [24, 71, 79, 80, 87, 104, 105, 120, 136], "620026": 89, "620156": 89, "62083859": 67, "620874e": 106, "620995": 94, "62108072": [106, 109], "621094": [85, 86], "621275": 88, "621359": 104, "621490": 89, "6215": 85, "621953": 89, "62195343": 89, "622": [55, 86, 90], "622153": 86, "622272": 71, "6224": 58, "622502": 83, "623024": 76, "623173": 74, "623310": 67, "624": 84, "6240": 92, "6241": 86, "624206": 78, "6243811": 58, "624535": 105, "624764": 75, "624798": 85, "624818": 75, "624919": 86, "625": [58, 84], "625002": 86, "625159": 80, "625165": 88, "625477": 89, "62572043": 120, "625767": 74, "625780": 87, "625891": [78, 79], "625950": 67, "626141e": 64, "626433": 89, "626633": 75, "626765": 86, "626998": 70, "627275e": 64, "627319": 83, "627505": [78, 79], "627560": 89, "627564": 76, "627754": 70, "628": 90, "628069": 84, "628834": 86, "629319": 86, "629549": 75, "629576": 64, "629595": 30, "6296333": 120, "629740": 74, "629835": 86, "629e": 90, "63": [58, 71, 84, 104, 105, 120, 136, 151, 152], "630150e": 89, "630222": 67, "630880": 90, "630914": 80, "631083": 75, "63117637": 106, "631333": 89, "631350": 67, "6318": [85, 153], "632058": 84, "632407": 70, "63245862e": 120, "632747e": 89, "632995": 67, "6330631": 136, "633433": 84, "634": 90, "63407762": 153, "634078": [85, 153], "634577": 136, "63489968": 67, "63499": 86, "635": 44, "635000e": [85, 86], "635043": 67, "635199": [85, 86], "635757": 86, "635768": 74, "63593298": [94, 106], "636048": 106, "636453": 27, "637108": 76, "637240": 86, "637326": 89, "6379": 85, "63817859": 89, "638179": 89, "638194": 67, "638264": 89, "638488": 80, "639": 85, "639135": 84, "63916605": 59, "639529": 65, "639580": 75, "639603": 75, "64": [71, 79, 85, 86, 90, 104, 105, 120, 136, 150], "640": 86, "640769": 64, "641528": 89, "642": 90, "642016": 89, "642096": 88, "642329": 71, "642648": [106, 107], "64269": 92, "6427": 86, "64340": 92, "643480": 67, "643512": 76, "643623": 89, "64362337": 89, "643679": [106, 111], "643752": 89, "643793": 64, "643939": 71, "644113": 104, "644368": 67, "644371": 75, "644665": 76, "64476745e": 120, "644799": 63, "644985": 74, "645": 86, "645583": 71, "64578228": 67, "64579": 57, "6458": 58, "645800": 84, "64606074": 67, "646117": 75, "646134": 70, "64621116": 67, "646937": 63, "647004": 106, "647196": 63, "64723": 92, "647254e": 74, "647689": 93, "647750": 86, "647864": 104, "647873": 89, "64797": 92, "648": 85, "648000e": 86, "648355": 74, "648690": 75, "648769": 75, "64877932": 67, "648978": 64, "649": 151, "649158": 89, "649514": 74, "649738": 74, "649773": 64, "649969": 86, "65": [71, 76, 80, 82, 86, 89, 90, 104, 105, 120, 136], "650": [77, 106], "6500000000000001": [76, 86, 89], "65002473": 67, "6502": 86, "650389": 66, "650574": 64, "650867": 76, "651127": 75, "651662": 86, "651919": 89, "652": 55, "6522": 151, "652324": 71, "652350": 84, "652450e": [85, 86], "652526": 78, "6527": 77, "652778": 84, "653008e": 85, "653172": 67, "653424": 78, "6536551": 120, "653829": 81, "653846": 76, "653901": [74, 75], "654070e": 106, "654401": 67, "654755": 63, "654964": 83, "655187": 67, "655284": 89, "6553": 153, "6554": 151, "655547": 74, "65557405e": 120, "655959": 81, "656143": 64, "6565": 65, "656840": 67, "657": 60, "657283": 92, "658021": [106, 111], "658267": 89, "658592": 75, "6586": 57, "658612": [98, 102], "658702": 75, "659": 60, "659245": [74, 75], "659339": 75, "659387": 66, "6593871": 57, "659423": [74, 75], "659459": 64, "659473": 93, "659524": 64, "659735": 74, "659755": 90, "659799": [98, 102], "659835": 75, "66": [71, 81, 87, 104, 105, 120, 136, 150, 152], "660": [60, 106], "660073": 75, "660128": 76, "660320": 79, "66044339": 64, "66046": 120, "660479": [106, 108], "6607402": 90, "660776": 89, "66078083": 64, "660788": [106, 109], "66097406": 64, "661145": 86, "66133": 106, "661338e": 86, "661388": 74, "662028": 64, "66212647": 66, "6625": 86, "662672": 64, "663177": 71, "663182": 76, "66321154": 66, "66342492": 66, "6634357241067574": 93, "663529": 89, "663533": 86, "663672": 81, "663765": 75, "66377109": 67, "664409": 75, "66459015": 67, "664797": 74, "664846": 84, "665264": 89, "665602": 83, "665653": [106, 111], "665868e": 86, "665974": 83, "66601815": [106, 108], "666104": 89, "666140": 67, "666307": 63, "666508": 67, "6666667": 60, "666742": 104, "666750": 86, "66686444": 67, "66686516": 67, "666959e": 81, "667": 84, "667156": 67, "667285944503316": 76, "667286": 76, "66749887": 67, "667536": 89, "667682": 62, "6678": 70, "66783628": 64, "667981": 74, "667985": 80, "668376": 87, "668452": 80, "668584": 63, "669130": 83, "669562": 78, "669579": 75, "67": [55, 60, 70, 85, 93, 104, 105, 120, 136, 150], "670024": 64, "670867": [29, 104], "67095958": [106, 109], "671224": 75, "671271": [74, 75], "671420": 83, "671633": 86, "671690": 74, "67199": 86, "6722": 60, "672234": [74, 75], "672384": [74, 75], "67245350": 58, "672511": 74, "67302703": 64, "673092": [74, 75], "673302": 84, "673330": 75, "6734628878523097": 76, "673463": 76, "673581815999206": 76, "673582": 76, "67410934": 58, "674181": 86, "674304": 64, "674319": 67, "6745349414": 58, "67456": 93, "674609": 76, "675011": 76, "675011328023803": 76, "675233": 75, "675625": 104, "675653": [106, 107], "675800": 64, "676093e": 78, "67618978": 62, "676405": 76, "6765": [59, 85], "676534": 136, "676641": 74, "6766618": 120, "67674909": 62, "676756": 89, "676807": 85, "676970": 67, "677123": 74, "677614": 89, "677980": 76, "678": 90, "678117": 86, "678826": 76, "67935394": 66, "67936506": [106, 108], "67952041": 66, "679539": 84, "67954864": 66, "679789e": 74, "67ad635a": 60, "68": [60, 71, 87, 91, 92, 104, 105, 120, 136], "680": 86, "680339": 65, "680366": 86, "680677": [106, 109], "681040": 64, "6810775": 92, "681176": 84, "681246": 75, "681521": 74, "6815476": 120, "681641": 67, "681817dcfcda": 60, "682": 106, "682235": 65, "682315": 86, "6826": 85, "683": [55, 84], "683331": 76, "683487": 75, "683581": 106, "6836083": 120, "683637e": 81, "683687": 75, "683942": 89, "683984": 37, "684": 153, "68410364": 59, "68411700": [59, 153], "684128": 75, "684142": 74, "684502": 89, "684899": 70, "685104": 12, "685107": 89, "685143": 83, "68554404e": 120, "68562150e": 120, "685807": 89, "685989": 106, "686270": 75, "686455": 67, "686627": 74, "687345": 89, "687612": 75, "687647": 89, "687854": 63, "687871": 84, "6878711": 58, "688": 151, "6882": 70, "688540": 104, "688886": 104, "688918": 86, "688956": 74, "689072": [106, 109], "689088": [74, 75], "689188": 63, "689392": 89, "689542": [98, 102], "689932": 74, "69": [80, 104, 105, 120, 136, 152], "690051": 67, "69046673": 66, "690536": 86, "691097": 74, "69120496": 66, "6912516": 64, "69140475e": 120, "691423": 74, "691511": 85, "69174251": 66, "691761": 86, "691848e": 75, "691911": 104, "69225343": 64, "692297": 75, "69241782": 64, "6924947": 120, "692579": 75, "6927084": 64, "692725": 89, "692907": 86, "692959": 74, "693345": 86, "693359": 86, "6934117220290754": 76, "693412": 76, "693495e": 86, "693796": 84, "69393508": 64, "694154": 76, "694170": 67, "694561": 90, "694755": 86, "694919": 84, "695045": 74, "69508862": 106, "69520523": 62, "6955": 86, "695536": 64, "695581": 80, "695582": 83, "69562150e": 120, "69572427": [106, 111], "695920": 66, "695928": 74, "696011": [28, 104], "696224": 104, "696289": [78, 79], "6964521040": 120, "6964565": 120, "696760": 64, "69684828": 106, "696966": 75, "697": 84, "697000": 76, "697118": 89, "697420": [78, 79], "697616": 75, "697693": 74, "698223": 63, "698244": 63, "69840389e": 120, "698419": 65, "698509": 74, "698642": [106, 111], "698694": 84, "699035": 89, "699041": 64, "699082": 76, "69921": 60, "699259e": 89, "699333": 76, "699697": 75, "699876": 64, "6_design_1a": 77, "6_r2d_0": 77, "6_r2y_0": 77, "6b": 136, "6cea": 60, "7": [12, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 42, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 137, 143, 150, 151, 152], "70": [59, 61, 76, 85, 86, 89, 104, 105, 120, 136, 152], "700": [74, 75, 77, 84], "7000000000000002": [76, 86, 89], "700015": 89, "70007159": 89, "700072": 89, "700102": 89, "700314": 71, "7004377": 120, "700458": 74, "700596": 89, "700643": [106, 109], "701088": 85, "701106": 80, "701413": 86, "701491": 78, "70166772": 64, "701672e": 76, "701841e": 79, "702248": 70, "702500": 86, "702520": 67, "703049": 74, "70305686": [106, 111], "703108e": 38, "703325": 81, "70344386": [106, 111], "7040": 86, "704482": 81, "7045": 81, "704558": 81, "704814": 74, "704896": 81, "70499654": 64, "705090": 75, "705354": 74, "70557077": [106, 111], "705581": 86, "7055958": 95, "705595810371231": 95, "7055958103712310": 95, "705794": 75, "70583": 92, "706077": 75, "706122": 75, "706208": 86, "706430": 74, "7065513": 120, "706645": 76, "706862": 15, "707101": 76, "707125": 75, "7074": 64, "707441": 75, "707738": 75, "70774361": [106, 111], "707868": 89, "707963e": 85, "708190": 84, "708235": 74, "708459": 89, "708472": 75, "708784": 64, "708821": 71, "708837": 71, "709026": 63, "709596": 74, "709606": [29, 104], "70980908": 64, "71": [104, 105, 120, 136, 152], "710059": 71, "710137": 67, "710319": 75, "710515": 74, "711176": 67, "711328": 86, "711383e": 74, "711518": 86, "711638": 106, "7117": 120, "712064": 74, "712065": 78, "712072": 88, "712095": 71, "712268": 74, "712372e": 75, "712503": 92, "712592": 85, "712665": 120, "712846": 104, "712921": 86, "712960": 76, "713": 86, "713457": 74, "713582": 83, "713993": 75, "714240": 84, "714250": 75, "7142787": 89, "714279": 89, "714321": 85, "714534e": 75, "715013": 86, "715085": 65, "715179e": 86, "715196": 67, "715407": 76, "715447": 67, "715457": 64, "71548914": 64, "7155": 86, "71563193": 64, "7157": 86, "715800": 64, "7158581": 58, "716": 86, "716013e": 74, "716098": 71, "7161": 86, "716387": 74, "716427e": 75, "716456": 89, "716615": 71, "716762": 76, "716793": 76, "716799": 84, "7167991": 58, "717185": 89, "717916": 70, "718294": 62, "718686": 93, "719217": 67, "7193": 86, "719552": 75, "71975275": 64, "71991788": 120, "72": [61, 81, 104, 105, 120, 136, 152], "720084": 64, "7203799": 120, "7204309": [106, 111], "720559": 74, "720571": 89, "720573": 74, "720589": 90, "720664": 84, "7207186": 120, "720733": 64, "721018": 74, "721071": 89, "721118": 83, "721245": 75, "7215093d9089": 60, "72155839e": 120, "721609": 86, "722": 84, "72228103": [106, 109], "722316": 89, "7224": 86, "722634": 89, "72269685": [106, 111], "722752": 65, "722848": 76, "722900": 64, "723": 60, "72317742": 64, "723314": 89, "723342": 104, "723345e": 89, "723516": 65, "723657": 74, "723689": 86, "723846": 71, "7239": 86, "7241399": 58, "724338": 89, "724767": [78, 79], "724918": 93, "725": 60, "725010": 71, "725031": 89, "725061": 74, "725087": 86, "725166": 89, "725240": 70, "725565": 74, "725584": 66, "725802": 15, "725919": 74, "726": [60, 90], "726232": 67, "7268131": 58, "727159e": 75, "727501": 86, "727543": 63, "727693": 86, "727780": 86, "727976": 76, "728": 55, "7282094": [106, 108], "728710": 89, "72875815e": 120, "728e": 90, "729668": 104, "729867": 74, "72987186": [106, 111], "73": [59, 71, 82, 104, 105, 120, 136], "730629": 83, "7308": 57, "730809": 74, "731174": 74, "731382": 64, "731754": 76, "731928": 83, "731992": 64, "732": 90, "732067": 74, "732137": 74, "732150": 75, "7326": 86, "73285": 27, "733047": 75, "733644": 74, "7339": 70, "7342563": 120, "734336": 67, "734635": 74, "734770": 75, "734948": 89, "735369e": 104, "735694": 89, "73569431": 89, "735848": 104, "735941": 26, "735964": 63, "736082": [74, 75], "736400": 65, "736823": 75, "7370": 70, "73750654": [106, 109], "7375615": 59, "73764317e": 120, "737796": 86, "737951": [74, 75], "738": 85, "738065": 75, "738147": 86, "738315": 86, "738630": 67, "738876": 75, "738886": 64, "739": 86, "739063": 74, "739089": [106, 111], "7394": 65, "739595": 90, "739720": 86, "739817": 80, "74": [31, 59, 75, 85, 104, 105, 120, 136, 152], "740": 84, "740180e": 89, "740367": 74, "740417": 85, "7405": 86, "740505": 71, "7407627053044026": 76, "740763": 76, "740785": 74, "740869": 76, "741": 153, "741104": 76, "741380": 90, "741486": 67, "741523": 71, "741702": 89, "7418": 57, "74189": 60, "742128": 89, "742365": 88, "742375": 74, "742407": 88, "742411": 74, "742907": 89, "7432": 57, "743203": 70, "743341": 75, "743609": 74, "7437": 86, "74402577": 89, "744026": 89, "744211": 86, "744236": 92, "7446": 64, "74461783e": 120, "74475816": [106, 111], "745444": 74, "745536": 70, "745548": 66, "745638": 85, "745881": 74, "746361": 89, "746843": 79, "747164": 89, "747215": 64, "747278": 67, "747538": 70, "747945": 58, "748084": 75, "748284": 83, "748377": 85, "748513": 86, "748568": 87, "748945": 86, "74938952": [106, 108], "749540": 70, "749854893": 121, "75": [18, 29, 31, 60, 63, 71, 76, 81, 85, 86, 89, 104, 105, 120, 136, 152], "750": 153, "75000": 93, "7500000000000002": [76, 86, 89], "750040": 64, "750275": 86, "750597": 75, "750701": 71, "751013": 86, "751081": 86, "751633": 86, "75171": 85, "751710": [76, 85], "751712655588833": 95, "7517126555888330": 95, "751712656": 95, "752015": 25, "752696": 71, "752871": [106, 109], "752909": 81, "752979": 67, "753": 153, "75329595": [106, 109], "7533": 85, "753323": 74, "753393": 74, "753505": 67, "753523": 89, "753866": 75, "754469": 74, "754499": 75, "754692": 104, "7548": 93, "754870": 84, "755645": 66, "755688": 74, "755701e": 74, "755885": 104, "7560824": 58, "756200": 71, "756585519864526": 76, "756586": 76, "756805": 84, "756867e": 86, "756905": 15, "756969": 76, "756989": 67, "757": [90, 151], "757136": 86, "757151": [74, 75], "757411": 66, "757559": 81, "757596": 76, "757663": 78, "757690": 89, "757819": 84, "758027": 86, "758695": [106, 109], "758831": 75, "75887": 60, "759054": 75, "759833": 75, "76": [104, 105, 120, 136, 151, 152], "760058": 64, "760104": 89, "760155": 86, "7603": 57, "760386": [106, 108], "760517": 70, "760778": 84, "760861": 64, "760915": 63, "761": [55, 58, 84], "761224": 71, "761429": 75, "761714": 76, "761737": 67, "761823": 67, "762284": 89, "76228406": 89, "762299": [106, 111], "762719": 65, "7628744": [106, 109], "762900620": 120, "763597": 66, "764093": [74, 75], "76419024e": 120, "764315": 89, "76444177e": 120, "764859": 62, "764953": 85, "7650": 64, "76535102": [106, 111], "765363": [74, 75], "765500e": [85, 86], "7656": 86, "765710e": 94, "765792": 89, "76591188": 58, "765960": 74, "7660": 57, "766005": 89, "76608187": 65, "766499": 89, "766585": 86, "7669": 86, "766940": 71, "76702611e": 120, "767188": [78, 79], "767435": 93, "767549": 75, "76755045": 120, "76766789": 120, "768273": [78, 79], "768763": 75, "768798": 71, "769063": 86, "769361": 89, "769805": 89, "77": [90, 104, 105, 120, 136], "770556": 86, "770838": 70, "770944": [78, 79], "7710": 92, "771157": 136, "771275": 86, "771383e": 86, "7714": 87, "771529": 83, "7716982": 59, "771876": 86, "771965": 86, "771986": 62, "77208424": 120, "772104": 74, "772157": 81, "77227783e": 120, "772396": 75, "772444": 104, "772612": 89, "77261209": 89, "772698": 67, "772791": 86, "77289874e": 120, "773": 60, "773177": 76, "77329414": [106, 109], "773339": 81, "773639": 67, "77401500e": 120, "774446": 87, "774562": 64, "774683e": 84, "775": [60, 86], "775191": [74, 75], "775285": 74, "775684": 67, "775969": 92, "776254e": 74, "7763": 85, "776887": 85, "777": 87, "777199": 70, "77746575": [106, 111], "7776071": 58, "777728": 104, "777867": 81, "777e": 90, "778400": 74, "7786": 57, "779": 90, "779068": 81, "779084": 67, "779108": 74, "779167": 24, "779430": 67, "779517": [74, 75], "779682": 76, "779727e": 86, "78": [78, 90, 104, 105, 120, 136, 152], "780": 60, "7800": 65, "780216": 66, "78032571": 67, "780338": 74, "780458": 89, "780856": 85, "780943": 89, "781076": 86, "781681": 89, "781847": 67, "782": 60, "782159": 86, "782643": 89, "7829757": 120, "78299613": 67, "783": [60, 86], "7830": 64, "783276": 106, "7833": 57, "7838": 57, "78386025": [106, 111], "784": 136, "784238": 84, "784405": 92, "784841": 86, "784872": 71, "784943": 67, "785": 60, "785038": 75, "785107": 76, "785153": 75, "785815": 71, "785911": 89, "785979": 83, "785e": 44, "786": 60, "786191": 80, "786237": 74, "78653422": 67, "7865872": 67, "786744": 76, "786986": 71, "78711285e": 120, "787695": [106, 109], "78777": 92, "788": 151, "78818": 60, "788267": 83, "788540": 64, "788868": 75, "789032": 74, "789039": 75, "789330": 75, "789355e": 92, "78989584": 120, "79": [71, 104, 105, 120, 152], "790000e": 86, "790039e": 74, "790115": 86, "790142": 83, "790261": 104, "7902927": 120, "790723": [78, 79], "7910908091075142": 76, "791091": 76, "791097": 85, "791241": 89, "791297": [29, 104], "79164735": [106, 109], "7919965": [106, 109], "792396": 71, "792939": 76, "792972": 104, "793297e": 86, "79330022": [106, 111], "793315": 104, "79338596e": 120, "793570": 89, "793598": 75, "793735": 89, "793818": [74, 75], "793896": 65, "7939": 65, "793994": 64, "794": 106, "794046": 66, "794119": 86, "794366": 86, "79458848e": 120, "794755": 86, "794805": 78, "795": [90, 120], "7954719": 120, "795647": 89, "795932": 105, "796014": 75, "7963": 86, "796340": 86, "796352": 67, "796374": 65, "796384": 75, "796596e": 74, "79668810": 120, "796e": 90, "797086": 74, "797189": [106, 111], "797280": 89, "7973": 65, "797323": 84, "797454": 106, "797737": 136, "797868": 75, "79792890e": 120, "797965": 136, "798071": 12, "798308": 85, "798783": [78, 79], "798885": 67, "7989": 64, "79894630": 120, "799403": 89, "79953099": [106, 111], "7b428990": 60, "7x": 89, "8": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 49, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 70, 71, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 97, 98, 102, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 137, 143, 150, 152, 153], "80": [61, 62, 70, 76, 86, 89, 94, 104, 105, 120, 152], "800": 84, "8000": [36, 61, 94], "8000000000000002": [76, 86, 89], "800000e": 86, "8001": 65, "800143": 74, "800325": 87, "800326e": 74, "800351": 74, "801130": 86, "801316": 67, "802": 90, "802738": 104, "8031014": 120, "803300": 74, "803492e": 89, "803889e": 86, "804219": 89, "804284": 92, "804316": 89, "804549": 86, "804695": 67, "8048": 59, "804828": 89, "805007": 84, "805153e": [85, 86], "805293": 75, "8055563": 58, "805720": 89, "805774": 74, "8059": 85, "806167": 86, "806220e": 86, "806356": 86, "806554": 74, "80665762": 67, "806732": 80, "80696592e": 120, "80714504e": 120, "807879": 89, "808": [59, 136], "808246": 85, "808347": 62, "808640": 86, "809": 86, "80908782": 120, "809125": 74, "809246": 67, "809278": 62, "8095": 87, "809913": [74, 75], "80a8": 60, "81": [58, 74, 77, 80, 104, 105, 120, 152], "81002084": 67, "810044": 85, "810134": 89, "8102": [57, 85], "810306": 71, "810322": 74, "810382": [85, 86], "810419": 75, "810650": 89, "810895": 75, "810916": 86, "811011": 75, "811155": 80, "81128199": 89, "811282": 89, "81129788": 64, "811387": 66, "811458": 85, "811513": 75, "8116912": 136, "811696": 74, "811825": 84, "811901": 89, "81190107": 89, "812": 90, "812028": [106, 107], "812126": 65, "8130": 70, "8132463": 58, "813293": 89, "813342": 136, "813682": 86, "814246e": 75, "814257": 64, "814268": 64, "814351": 76, "814717": 76, "814787": 66, "814913": 84, "815": 87, "815213e": 75, "815224": 136, "815226": [106, 108], "815596": 65, "815993": 89, "8160": 86, "816176": 93, "816318": 84, "816373": 74, "816896": 70, "816982": 74, "817": 55, "817119": 74, "817660": 67, "817967": 106, "818029": 86, "81827267": 89, "818273": 89, "818289": 89, "81828926": 89, "818380": [74, 75], "81856": 60, "818853": 87, "819223": 104, "82": [93, 104, 105, 120, 152], "8202": 59, "820366": 84, "8208": 86, "8209": 59, "820963": 71, "820993": 86, "821": 151, "8210": 59, "821009": 86, "821021": 76, "821566": 89, "821855": 104, "821995": 75, "822072": 64, "8221": 57, "822289": [85, 153], "82228913": 153, "822482": 76, "8227": 86, "822822": 76, "823247": 89, "823273": [74, 75], "82329138": 82, "823772": [106, 109], "823887": 67, "824140": 70, "824350": [74, 75], "824701": 76, "824750": 76, "824889": 76, "824961e": 86, "8250": 57, "825038": 64, "825587": 75, "825617": 84, "8260": 85, "826065": [74, 75], "826074e": 84, "826329": 67, "826426": [106, 108], "826467e": 74, "826492": 89, "826519": [29, 104], "826528": 67, "82666866e": 120, "82684324": 92, "826897": 89, "827381": 89, "827735": 89, "827874854703913": 76, "827875": 76, "827938162750831": [78, 79], "828077": 66, "828157": 71, "828618": 81, "828778e": 74, "828912": 74, "828915": [78, 79], "829358": 65, "829543": 76, "829730e": 75, "829764": 104, "83": [104, 105, 120, 152], "830273": 71, "830301": 88, "830442": 74, "830467": 74, "831": [87, 90], "831019": 76, "831190": 75, "831278": 74, "831741": 74, "831833": [106, 111], "832086": 89, "8324": 106, "8326928": 92, "832693": 92, "832875": 89, "83287529": 89, "8329604": 64, "832965": 86, "833018": 86, "833024": 84, "833065": 71, "833096": 86, "833227e": 105, "833563": 83, "833907": 84, "834133": 81, "83436056": [106, 109], "834729": 67, "834916": [106, 109], "835": 90, "8350": 86, "835239": [106, 111], "835344": 71, "835750": 81, "835839": 86, "835935": 75, "836234": 106, "838114": 89, "838235": 87, "83905": 12, "839502": 67, "83e": 120, "84": [60, 80, 90, 104, 105, 106, 120, 152], "840041": 86, "8403": 70, "840303": 89, "84030318": 89, "840673": 74, "840718": [106, 108], "840836": 89, "840995e": 85, "841": [58, 84], "841132": 85, "8415": 59, "841688": 87, "841712": 86, "841847": 86, "842132": 106, "842205": 86, "842405": 76, "842625": 84, "842746": 89, "8428": 85, "842853": 89, "843": 86, "843018": 104, "843296": 86, "843730": 84, "843796": 74, "844107e": 75, "844308": 89, "8445": 82, "844549": [78, 79], "844663": 71, "844667": 136, "844707": 89, "844893": 84, "8450": 70, "845241": 90, "846602": 86, "846707": 86, "846885": 64, "846906": 67, "847029": 75, "847136": 76, "847555": 74, "847595": [28, 104], "847948": 76, "847962": 74, "848578": 86, "848757e": 85, "848868": 76, "848997": 86, "84930915e": 120, "849391": 67, "849427": 104, "849440": 67, "849747": 92, "849766": 86, "8497f641": 60, "8498": 86, "85": [34, 76, 80, 82, 86, 89, 94, 104, 105, 120], "8500000000000002": [76, 86, 89], "850038": 71, "850321": 84, "850439": 75, "850480": 66, "850573": 67, "850575": [74, 75], "850656": 81, "851": 151, "851012": 83, "851100": 71, "8513": 60, "851366": 84, "852016": 89, "852592e": 83, "8526": 70, "85280376": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "853": 55, "853916": 67, "85397773": [106, 108], "853990": 64, "85402594": 82, "854525": 70, "855": 86, "855035": 74, "855179": 64, "85550053": 120, "855780": 89, "855862": 75, "856404": 79, "856552": 71, "856758": 104, "8571": 57, "857161": 89, "857368": 64, "857372": [106, 109], "857515": 104, "857544": 84, "858212e": 75, "858577": 88, "858635": 83, "858952": 71, "859": 86, "85911521e": 120, "85912862": 136, "859129": [121, 136], "8597": 85, "85974356": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "85c5": 60, "85e": 59, "86": [104, 105, 120, 152], "860091": 64, "860328": 66, "860378": 86, "860663": 136, "860668": 64, "860804": 89, "861019": 71, "861210": [106, 111], "861505": 70, "861519": 74, "862043": [78, 79], "862274": 78, "862359": 76, "86284907": 64, "863772": 85, "863982270": 121, "864": 90, "86415573": 59, "86424193e": 120, "8644": 60, "864404": 86, "864524": 87, "864664": 71, "864805": 65, "865313": 86, "865442": 83, "865562": [74, 75], "865860": [85, 86], "865914": 75, "866102": [74, 75], "86610467": 89, "866105": 89, "866179899731091": 95, "866179900": 95, "8663": 65, "866579": 86, "866798": 86, "867333": 67, "867565": 89, "8679": 86, "868": 60, "8681": 82, "86897905": [106, 111], "869": [60, 90], "869020": 76, "869195": 75, "869398": 75, "869477": 74, "869585": 38, "869586": 80, "869658": 70, "869778": 88, "87": [59, 74, 80, 84, 104, 105, 120, 152], "870": 66, "8700": 59, "870099": [78, 79], "870260": 89, "870332": 89, "870444": 85, "870491": 65, "870870": [106, 109], "871": 60, "871545e": 74, "871923": 75, "871937": 64, "871972": 81, "872": 90, "872132": 75, "872354": 89, "872371": 67, "872727": 74, "872768": 89, "872852": 89, "87290240e": 120, "87309461": [106, 111], "873198": 86, "873275": 70, "873369": 67, "873677": [78, 79], "873848": 66, "87384812361": 57, "87384812362": 57, "87430335": 136, "874303353": 136, "874307": 65, "874323e": 67, "874702": [78, 79], "8751": 86, "8759": 86, "876": 90, "876233": 66, "87623301": 57, "876431e": 76, "8767": 70, "87674597e": 120, "8768": 57, "8771": 86, "877153": 86, "877455": 88, "877833": [74, 75], "877903": 71, "878122": 83, "878281": 89, "878402": 74, "878746": 71, "878792": 66, "878802": 86, "878895": 71, "878968e": 74, "879": 106, "879103": 76, "879509": 74, "87e": 59, "88": [59, 80, 90, 104, 120], "88008518": 64, "880106": 84, "880217": [106, 109], "880579": 89, "880591": 88, "880886": 85, "8810": 85, "881201": 86, "88125046e": 120, "881322": 67, "881465": 63, "881581": 26, "88173062": 58, "881937": 71, "882": 55, "882003": 67, "882641": 85, "882788": 66, "882896": 89, "882928": 71, "882965": 70, "883301": 76, "883485": 75, "883622": 89, "883914": 76, "8843": 92, "8845": 57, "884821": 104, "884996": 76, "8850": 59, "885065": 89, "885513": 86, "885832": 90, "885956": 74, "885978": [78, 79], "886041": 75, "886086": [74, 75], "88629": 57, "886314": 75, "886611": 86, "88664": 60, "8867": 65, "887182": [106, 107], "887212": 86, "887556": 76, "887648": 75, "887680": 74, "887731": 62, "887778": 86, "888146": 84, "8881461": 58, "888204": 67, "888217": 86, "888239": 67, "888343": 67, "888352": 71, "888423": 86, "888445": 75, "888502": 64, "888757": 89, "888775": 79, "889027": 67, "889236": 70, "889293": 89, "889326": 75, "889566": 92, "889638": 71, "889733": 89, "889792": 75, "88988263e": 120, "889913": [74, 75], "88ad": 60, "89": [59, 75, 104, 120, 151, 152], "890": [58, 84], "890204": [106, 111], "89027368": 136, "890273683": 136, "890318": 74, "89035917": 80, "890372": [68, 97, 102, 150], "8903720000100010000010": [60, 97, 102, 150], "8904": 55, "890454": 105, "890460": 67, "890665": 81, "890855": 71, "8909": [58, 85, 153], "890933": 70, "89151921222528425563666770738192959899": 120, "891527": [106, 111], "891606": 85, "891752": 75, "891997": 74, "892": 60, "892648": 89, "892796": [74, 75], "892828": 81, "893": 60, "8932105": 58, "893461": 71, "8935882": 120, "893649": [74, 75], "893851": 89, "893884": 86, "894": 60, "894329e": 86, "894448": 75, "8946549": 61, "89472978": [106, 111], "895106": [74, 75], "895308": 86, "895333": 89, "895442": 85, "895690": [74, 75], "895768e": 76, "896023": 89, "896182e": 81, "896263": 81, "89664597": 120, "896761": 65, "897220": 89, "897257": 67, "8974": 85, "8974226": [106, 109], "897451": 74, "897495e": 75, "898183": 71, "898722": 89, "899021": 104, "899250": 71, "899296": [106, 109], "899460": 89, "8996": 82, "899654": 71, "899662e": 74, "899716": 75, "8bdee1a1d83d": 60, "8da924c": 60, "8e3aa840": 60, "9": [12, 14, 15, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 97, 102, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 137, 143, 149, 150, 152, 153], "90": [40, 59, 61, 62, 76, 86, 89, 94, 104, 120, 152], "9000000000000002": [76, 86, 89], "900021": 105, "900127": [106, 111], "900829": 81, "901013": 75, "901148": 89, "901241": 64, "90136": 85, "901360": 85, "90145324": [106, 111], "901526": 80, "901705": 74, "90196882": 120, "901998": 83, "902": 136, "902573": 76, "902920": 104, "903056e": 89, "903339": 76, "903366e": 76, "903369": 64, "903418": 84, "903674": 74, "903681": 89, "903731": 83, "903767": [74, 75], "903949": 67, "904225": 64, "904315": 74, "904396": 74, "9045": 86, "90499566": 64, "905042": 75, "905494": 76, "9056282": 64, "905724": 70, "905858": 93, "905951": 92, "906051": 76, "906051023766621": 76, "906072": 104, "906195": [106, 109], "9067": 86, "906716732639898": [78, 79], "906757": 72, "907115": 89, "907176": 89, "907491": 76, "907512e": 67, "907801": 84, "907879": 71, "90794478": 136, "907944783": 136, "908024": 93, "908663": 75, "909187": 67, "909304": [74, 75], "909571": 71, "90963122e": 120, "909942e": 104, "909997": [85, 153], "91": [91, 104, 120, 152], "91053356": [106, 109], "910895": 75, "9109": 60, "91102953": 89, "911030": 89, "9112": 85, "911277": 75, "911413": 67, "911597": 67, "912230": [74, 75], "9126": [59, 153], "9127": [59, 153], "912903": 74, "913": 60, "913008": 67, "913145": 66, "91315015": 58, "913280": 93, "913371": 75, "913415e": 74, "913585": 81, "913774": 76, "9142": 86, "91438767e": 120, "9145": 57, "914598": 71, "915": [59, 60, 85, 86], "915000e": [85, 86], "915225": 70, "915260e": 74, "915488": [78, 79], "915780": 65, "916171": 88, "916236": 57, "916359": 71, "9166667": 60, "91683613": [106, 109], "916913": 89, "916914": 89, "916921": 86, "916930": 74, "916984": 89, "917": 60, "917000": 75, "91734709": 64, "917497": 78, "917713": 67, "91771387": 89, "917714": 89, "9179": 86, "918": [55, 90], "918227": 76, "918293": [106, 111], "918493": 66, "919514": 66, "919879": 86, "919969": 74, "91e": 59, "92": [64, 91, 104, 105, 120, 152], "920052": 75, "920283": 86, "920337": 79, "920695": 86, "9209": 57, "9210": 86, "921061": 93, "921198": 81, "921256e": 75, "921372": 76, "921778": 81, "92178092": 64, "921913": 84, "921956": [74, 75], "921e4f0d": 60, "922251": 74, "922656": 70, "922668": 81, "922996": 84, "923": 87, "923084e": 76, "923517": 94, "923607": 89, "92369755": 58, "923943": 153, "924002": 89, "924232e": 83, "924248": 67, "9243": 86, "924394": 64, "924396": [78, 79], "924443": 71, "924540": 83, "924634": 63, "924724": 76, "924732": 86, "9248": 60, "924821": 76, "924921": 104, "925248": [78, 79], "925410": 70, "925537": 67, "92566": 106, "925660": 74, "925736": 76, "925994": 85, "925995": 75, "926227": 75, "926260": 78, "926621": 76, "926664": 65, "926685": 84, "927": 56, "927074": 89, "927232": 86, "9274": 86, "92742841": 64, "928269": 86, "92827999": 106, "92881435e": 120, "9289949": 120, "92905": 58, "929475": 67, "929607": 86, "929643": 74, "92964644": [106, 109], "92972925e": 136, "929729e": [121, 136], "93": [59, 81, 91, 104, 105, 120, 152], "9304028": 58, "93074943": [106, 111], "931": 96, "931142": 66, "93134081": [106, 109], "931479": 89, "931978": 150, "932027": 76, "932259": 66, "932404e": 86, "9325": 57, "932527": 66, "9327": 57, "932973": 89, "933": 87, "93300": 106, "933322": 75, "933671": 75, "933857": 75, "933996": 76, "934058": 74, "934243": 75, "934262": 66, "934270": 84, "934433": [74, 75], "9345": 60, "934500": 75, "934511": 136, "93458": 92, "934963": 75, "934992": 76, "935": [44, 82, 106], "9350649": 64, "935220": [106, 107], "935578": 64, "935591": 89, "935730": 89, "935764": 75, "935793": [106, 109], "935795": 65, "9358": 65, "935961": 64, "935989": 84, "9359891": 58, "93648": 94, "936494": 74, "936739": 89, "937040": 65, "937213": 64, "937586": 86, "938": 136, "938069": 64, "938263": [106, 111], "938621": 67, "9386744462704798": 83, "9388": 86, "938836": [106, 111], "939068": [78, 79], "939250": 74, "939291": 64, "939458": 74, "9395": 86, "93958082416": 153, "94": [82, 104, 120, 152, 153], "9406007": 120, "940606": 67, "9411078": 120, "941440": 74, "941709": 76, "9417090334740552": 76, "942139": 79, "942312": 89, "942460e": 89, "9425": 57, "942550": 86, "942629": 86, "942661": 84, "94309994e": 120, "943548": 81, "943631": 64, "943693": [106, 107], "944045e": 89, "94406974": 64, "944253e": 89, "944266": [78, 79], "94427158": [106, 111], "944280": 86, "944317": 86, "94441007e": 120, "944630": 89, "94473": 61, "945423": 67, "945881": 74, "94604394": 64, "946180": 81, "94629": 94, "946297": 76, "946406": 79, "946433": 89, "946533": 74, "946968": 76, "947": 55, "947219": 66, "947440": 88, "947466": 105, "947532": 64, "947613": 75, "947855": 71, "9480": 86, "948112": 90, "948164": 67, "948508": 64, "948522": 67, "948785e": 74, "948975": 80, "94906344": 58, "949241": 136, "949456": 89, "949866": 75, "949912": 70, "95": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 48, 57, 59, 61, 62, 63, 64, 65, 66, 67, 70, 71, 74, 75, 76, 77, 78, 79, 80, 81, 85, 86, 89, 90, 92, 93, 94, 104, 106, 120, 136, 137, 143, 152, 153], "9500": 86, "950158": 74, "950280": 89, "9505003509": 120, "950545": 72, "95062986e": 120, "95110806": 64, "951363": 67, "951532": 84, "951898": 66, "951920": 88, "952": [59, 90, 153], "952146": [106, 111], "9523": 57, "952785": 66, "952839": 89, "95305": 61, "95311164": [106, 111], "95316914": 120, "9532": 86, "95338161": 64, "953683": 84, "953704": 74, "95372559e": 120, "953884": 106, "954": 136, "95401167e": 120, "954890": 64, "9552": 57, "955541": [29, 104], "95559917": 105, "955701": 74, "955e": 106, "956047": 58, "956061": 64, "9561": 57, "956272": 84, "956574": 86, "956588": 106, "956877": 75, "957229": 79, "957339": 84, "957375": 84, "957417": 86, "957479": 76, "9574793755564219": 76, "957745": 76, "9579": 59, "957996": 76, "958": 136, "9580": 59, "958105": 104, "958608": 70, "958612": 70, "958636": 80, "958681": 67, "958789": [106, 109], "959132": 75, "959384": 75, "959613": 104, "95e": 59, "96": [59, 62, 74, 75, 87, 104, 120, 152], "960049": 71, "960236": 106, "960808": 76, "960834": 75, "960846": 86, "9609": 57, "961385": 66, "961538": 86, "961962": 76, "96231235": 64, "962364": 71, "962373": 75, "9625": 86, "962521": 67, "96277607": 64, "962954": 75, "963367": 70, "96337688": 64, "963427e": 75, "9636": 70, "964025e": 89, "964065e": 75, "964261e": 84, "964317": 86, "964430": 65, "9647": 57, "965": 62, "965341": 75, "965531": 106, "965696": 74, "96582": 105, "966015": 89, "966031": 70, "966097": 30, "966342e": 67, "967092": 75, "967206": 70, "967467": 92, "967764": 67, "968127": 71, "968134e": 89, "968258e": 74, "968288": 86, "969141": [104, 105, 106], "969446": 64, "96963737": 64, "969702563412915": 76, "969703": 76, "9699": 85, "969925e": 75, "97": [12, 15, 24, 25, 26, 27, 28, 29, 30, 37, 38, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 150, 152, 153], "970065": 89, "9701": 70, "970150": 75, "970251": 66, "971058": [78, 79], "971059": 70, "971132": 86, "971967": [106, 109], "972051": 86, "972509": 75, "972551": 67, "972735": 70, "972745": 74, "973140": 70, "97314470": 58, "973156": 104, "973229": 75, "973241": 89, "97345485": 120, "973741": 75, "973874": 76, "973890": 75, "974202": 76, "974213": 75, "97428343": 64, "97441062": [78, 79], "974414": 76, "974487": 74, "97470872": 92, "9748910611": 58, "97499195": 89, "974992": 89, "975": [74, 75, 78, 79, 81, 82, 87, 106, 109], "975289": 71, "9753": 60, "975450": 75, "975727": 66, "976088": 89, "97619643": [106, 111], "9764": 70, "976487": 64, "976548e": 75, "976562": 89, "977202": 75, "977280": [74, 75], "977295": 86, "9773": 86, "977489": 86, "977507": 75, "977820": 74, "978000e": 86, "9783": 70, "978303": 65, "978618": 70, "9787": 86, "978977": 89, "979": [55, 90], "979031": 70, "979384": 81, "979475": 74, "979702": 74, "979857": 74, "979966": 81, "979971e": 74, "98": [74, 75, 104, 120, 152], "98023747": [106, 109], "9802393": 58, "980440": 75, "980643e": 76, "981104": 88, "981403": 75, "981438": 74, "981451": 64, "981672": 76, "981715": 74, "981794": 66, "98190670": 120, "982019e": 75, "982353e": 86, "982417": 76, "98259068": 64, "982720": 74, "982815": 86, "983192": 89, "983253": 74, "983482": 86, "983483": 71, "983759": 153, "98393441": 92, "984083": [78, 79], "984551": 25, "984562": 89, "98456477": 64, "984866": 136, "984872": [74, 75], "984937": 76, "985": 87, "98505871e": 120, "985207": [74, 75], "985279": 86, "985477": 64, "985569": 86, "985583": 64, "985654": 75, "9857583": 120, "986249": 85, "986417": 74, "98673": 80, "986943": 66, "9870004": 60, "9875": 57, "98750578": 62, "987726": 75, "988": 87, "9880384": 60, "988421": [74, 75], "988463": 89, "988541": 74, "988833751": 120, "989046": 66, "989999": 87, "99": [59, 71, 74, 75, 86, 87, 104, 120, 152], "990160": 87, "990210": 86, "990567e": 86, "990903": 74, "991": 60, "9912": 67, "991281": 66, "9914": [85, 86, 92], "9915": [59, 85, 86, 92], "991512": 59, "9916903": 64, "991963": [74, 75], "991977": 86, "991988": 74, "992": [87, 90], "99232145": 92, "992388": 67, "992582": [74, 75], "993201": 75, "993416": 81, "993512": [106, 109], "994": 90, "994168239": 58, "994208": 71, "994304": 86, "994332": 72, "994377": 74, "9944": [106, 108], "9948104": 61, "994864": 86, "994937": 79, "9951": 57, "995248": 89, "99549118e": 120, "99571372e": 120, "996025": 67, "9961": 85, "9961392": 58, "996313": 74, "996413": 86, "9966": 67, "996934": 84, "9970": 86, "997034": 94, "9974": 67, "997494": 94, "997571": 84, "997621": 76, "997830": 67, "997934": [78, 79], "998050": 88, "998063": 72, "99864670889": 153, "998855": 86, "999": [63, 70, 80, 92, 153], "999016": 64, "999120": 88, "999207": 89, "9995": [63, 74, 75], "9996": [63, 74, 75], "9996553": 59, "9997": [63, 74, 75], "9998": [63, 74, 75], "9999": [63, 74, 75], "99c8": 60, "9e": 120, "A": [10, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 34, 35, 37, 38, 39, 43, 44, 46, 47, 48, 50, 51, 55, 56, 57, 59, 60, 64, 65, 66, 67, 70, 72, 73, 77, 82, 83, 87, 88, 90, 91, 92, 93, 96, 97, 102, 104, 105, 106, 107, 109, 111, 119, 136, 137, 138, 144, 145, 146, 147, 148, 150, 151, 153], "ATE": [26, 30, 31, 59, 68, 70, 71, 81, 85, 92, 93, 104, 106, 116, 119, 121, 129, 137, 145], "ATEs": [70, 71, 87], "And": [61, 87, 94, 137, 139, 140], "As": [56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 70, 71, 73, 74, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86, 87, 89, 93, 94, 95, 105, 106, 109, 110, 111, 120, 121, 123, 125, 127, 136, 137, 138, 147, 153], "At": [18, 31, 39, 58, 62, 63, 65, 71, 80, 82, 84, 86, 89, 153], "Being": 153, "But": [81, 82], "By": [57, 58, 84, 90, 93, 105, 106, 109, 111, 137, 143], "For": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 25, 26, 38, 47, 51, 55, 57, 58, 60, 62, 64, 65, 66, 67, 70, 71, 72, 80, 81, 82, 83, 84, 86, 88, 90, 92, 93, 95, 96, 97, 98, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 147, 148, 149, 150, 153], "ITE": [35, 70, 71], "ITEs": [70, 71], "If": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 56, 58, 62, 64, 65, 66, 67, 70, 73, 74, 75, 81, 82, 84, 86, 90, 96, 97, 102, 104, 105, 106, 108, 114, 121, 122, 123, 124, 125, 126, 129, 136, 137, 138, 139, 140, 141, 142, 143, 146, 147, 148, 153], "In": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 46, 50, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 120, 121, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153], "It": [13, 57, 58, 59, 70, 74, 75, 77, 78, 79, 84, 85, 86, 90, 91, 93, 98, 101, 102, 105, 106, 107, 120, 148, 152], "No": [33, 55, 57, 59, 60, 61, 62, 64, 65, 66, 67, 68, 70, 71, 80, 85, 86, 90, 92, 94, 97, 98, 99, 100, 101, 102, 105, 106, 108, 109, 111, 121, 136, 150, 151], "Not": [106, 112], "Of": [82, 136, 153], "On": [56, 73, 83, 87, 91, 96, 151], "One": [59, 64, 67, 85, 86, 93, 104, 136], "Or": 44, "Such": [93, 105], "That": [44, 153], "The": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 112, 115, 117, 118, 119, 120, 121, 126, 129, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 149, 151, 152, 153], "Then": [18, 76, 89, 91, 106, 136, 137, 147, 148, 149], "There": [59, 85, 93, 106, 112, 149, 153], "These": [25, 59, 60, 65, 69, 83, 85, 88, 90, 92, 104, 106, 153], "To": [19, 21, 55, 56, 58, 59, 60, 61, 62, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 78, 79, 81, 82, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 96, 97, 102, 104, 105, 106, 110, 120, 136, 137, 138, 143, 147, 148, 149, 150, 153], "With": [34, 74, 75, 105, 151], "_": [56, 58, 62, 63, 64, 66, 67, 73, 74, 75, 76, 77, 78, 79, 84, 85, 86, 88, 89, 90, 91, 95, 96, 101, 102, 104, 106, 109, 112, 120, 121, 123, 136, 137, 138, 143], "_0": [56, 58, 73, 77, 84, 95, 96, 120, 121, 134, 135, 136, 137, 147], "_1": [17, 18, 19, 31, 35, 39, 61, 87, 94, 121, 134, 135], "_2": [17, 18, 19, 31, 35, 39, 87], "_3": [17, 18, 19, 31, 35, 39], "_4": [17, 18, 19, 31, 35, 39], "_5": [31, 35], "__": [50, 51], "__init__": [83, 91], "__version__": 149, "_a": [121, 123, 125], "_all_coef": 120, "_all_s": 120, "_b": [121, 123, 125], "_check": 70, "_compute_scor": 21, "_compute_score_deriv": 21, "_coordinate_desc": 84, "_d": [70, 90, 106], "_est_causal_pars_and_s": 152, "_estimator_typ": 83, "_h": [90, 106], "_i": [56, 61, 73, 89, 94, 96, 106, 112], "_id": 120, "_j": [17, 18, 19, 31, 35, 39, 41, 58, 84, 136], "_l": 105, "_lower_quantil": [64, 67], "_m": [105, 120], "_mean": [64, 67], "_n": [121, 122, 123, 124, 125, 129, 136, 137, 143, 146], "_n_folds_per_clust": 84, "_offset": 105, "_pred": 105, "_rmse": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "_upper_quantil": [64, 67], "_x": 45, "_y": [90, 106], "a0": 83, "a09a": 60, "a09b": 60, "a1": 83, "a3d9": 60, "a4a147": 87, "a5e6": 60, "a5e7": 60, "a6ba": 60, "a79359d2da46": 60, "a840": 60, "a_": [61, 94], "a_0": 42, "a_1": 42, "a_j": [106, 113], "ab": [57, 91, 148], "ab71": 60, "abadi": [10, 62], "abb0fd28": 60, "abdt": [68, 97, 102, 150], "abl": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 56, 73, 82, 86, 87, 105, 137, 138, 147], "about": [14, 16, 59, 62, 63, 70, 82, 85, 91, 106, 148, 150, 153], "abov": [56, 59, 65, 71, 73, 74, 75, 78, 79, 82, 83, 85, 87, 88, 89, 90, 93, 96, 104, 105, 106, 110, 112, 149], "absolut": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 82, 105], "abstract": [21, 57, 58, 84, 121, 148, 152], "abus": [64, 67, 106, 109, 111, 112], "acc": [23, 57], "acceler": 70, "accept": [17, 19, 98, 102, 104, 105], "access": [46, 47, 57, 59, 64, 65, 66, 67, 70, 78, 79, 80, 82, 92, 100, 102, 105, 137, 143, 153], "accompani": 148, "accord": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 56, 59, 61, 62, 65, 71, 73, 76, 85, 89, 90, 93, 94, 105, 106, 107, 112, 136, 137, 139, 140, 141, 142, 144, 145, 153], "accordingli": [61, 62, 82, 83, 85, 90, 94], "account": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 58, 59, 64, 67, 84, 85, 86, 92, 93, 137, 143, 147, 153], "accumul": [59, 85, 86, 92], "accur": 70, "accuraci": [46, 50, 57, 106], "acemoglu": 151, "achiev": [58, 70, 81, 84, 88, 93, 106, 136], "acic_2024_post": 87, "acknowledg": [59, 60, 85], "acm": 151, "acov": 151, "across": [13, 17, 19, 59, 70, 85, 87, 153], "action": 152, "activ": [4, 5, 6, 7, 8, 9, 149, 152], "actual": [44, 64, 67, 80, 93], "acycl": [61, 94, 153], "ad": [5, 6, 7, 8, 9, 10, 11, 21, 46, 47, 50, 51, 80, 97, 102, 105, 106, 136, 137, 138, 152], "adapt": [25, 85, 152], "add": [57, 58, 61, 62, 63, 68, 70, 71, 78, 79, 80, 87, 89, 90, 92, 93, 94, 105, 106, 151, 152], "add_trac": 93, "addit": [12, 13, 14, 15, 16, 17, 18, 19, 22, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 41, 42, 43, 45, 48, 64, 65, 66, 67, 77, 93, 98, 99, 100, 101, 102, 105, 106, 107, 121, 130, 137, 144, 145, 147, 151, 152], "addition": [31, 39, 67, 71, 76, 86, 92, 105, 106, 120, 136, 137, 143, 150], "additional_inform": 13, "additional_paramet": 13, "address": 93, "adel": 151, "adj": [90, 93], "adj_coef_bench": 93, "adj_est": 93, "adj_vanderweelearah": 93, "adjust": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 49, 58, 63, 81, 84, 86, 92, 93, 104, 106, 112, 136, 137, 143, 151, 152, 153], "adopt": [62, 106, 108, 112], "advanc": [83, 103, 120, 151], "advantag": [56, 57, 59, 71, 73, 85, 86, 96, 149], "advers": [137, 138], "adversari": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 92, 137, 143, 147], "ae": [56, 58, 59, 61], "ae56": 60, "ae89": 60, "aesthet": 56, "aeturrel": 43, "afd9e4": 87, "affect": [70, 71, 77, 106, 152, 153], "after": [57, 59, 60, 61, 62, 77, 85, 86, 93, 94, 104, 105, 106, 112, 137, 139, 143, 149, 153], "after_stat": 56, "ag": [59, 85, 86, 88, 92, 153], "again": [56, 57, 58, 59, 61, 62, 64, 67, 71, 73, 80, 83, 84, 85, 90, 91, 92, 93, 94, 96, 137, 139, 140], "against": [62, 70, 80, 82, 88, 105], "agebra": 104, "agegt54": [60, 68, 97, 102, 150], "agelt35": [60, 68, 97, 102, 150], "agg": [57, 64, 67, 91], "agg_df": [64, 67], "agg_df_anticip": [64, 67], "agg_dict": [64, 67], "agg_dictionari": [64, 67], "agg_did_obj": [106, 107], "aggrag": [106, 107], "aggreg": [13, 16, 57, 95, 107, 120, 152], "aggregate_over_split": 44, "aggregated_eventstudi": [64, 65, 66, 67], "aggregated_framework": [64, 65, 66, 67, 106, 107], "aggregated_group": [64, 67], "aggregated_tim": [64, 66, 67], "aggregation_0": 13, "aggregation_1": 13, "aggregation_color_idx": 13, "aggregation_method_nam": 13, "aggregation_nam": 13, "aggregation_weight": [13, 64, 65, 66, 67], "aggt": 57, "ai": [66, 91, 151], "aim": 90, "aipw": 87, "aipw_est_1": 87, "aipw_est_2": 87, "aipw_obj_1": 87, "aipw_obj_2": 87, "air": [58, 84], "al": [10, 11, 32, 34, 41, 42, 56, 58, 59, 60, 62, 73, 74, 75, 76, 77, 78, 79, 82, 84, 85, 86, 89, 92, 96, 106, 108, 112, 120, 121, 127, 129, 130, 131, 136, 137, 138, 147, 148, 150, 152], "alexandr": [77, 151], "algebra": 106, "algorithm": [55, 57, 58, 60, 61, 62, 64, 65, 66, 67, 71, 73, 76, 81, 82, 84, 86, 89, 92, 94, 103, 105, 106, 108, 109, 111, 120, 121, 136, 152, 153], "alia": [46, 47, 50, 51], "align": [56, 58, 61, 63, 64, 67, 73, 76, 82, 84, 85, 87, 88, 89, 94, 106, 109, 111, 112, 121, 123, 125, 152], "all": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 47, 50, 51, 52, 56, 57, 58, 59, 61, 62, 70, 71, 73, 80, 81, 82, 83, 84, 85, 86, 88, 90, 93, 94, 96, 97, 99, 102, 104, 105, 106, 107, 108, 110, 112, 120, 121, 123, 125, 136, 137, 147, 148, 149, 152], "all_coef": 120, "all_dml1_coef": 95, "all_s": 120, "all_smpl": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 81], "all_smpls_clust": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38], "all_z_col": [58, 84], "allow": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 46, 47, 50, 51, 59, 64, 67, 70, 71, 85, 86, 90, 102, 104, 105, 106, 120, 121, 136, 148, 152, 153], "almqvist": 151, "along": [70, 105], "alongsid": [98, 100, 102], "alpha": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 40, 42, 56, 58, 59, 61, 64, 67, 68, 70, 71, 73, 74, 75, 76, 77, 81, 82, 83, 84, 85, 86, 89, 95, 96, 104, 105, 106, 120, 121, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147], "alpha_": [41, 58, 84, 105], "alpha_0": [137, 147], "alpha_ml_l": 77, "alpha_ml_m": 77, "alpha_x": [25, 33, 106], "alreadi": [18, 61, 62, 64, 67, 91, 94, 105, 106, 108], "also": [12, 14, 15, 16, 22, 25, 26, 38, 55, 56, 57, 58, 59, 60, 62, 64, 65, 67, 70, 71, 72, 73, 74, 75, 78, 79, 80, 82, 83, 84, 85, 86, 88, 90, 92, 93, 96, 104, 105, 106, 120, 121, 136, 137, 138, 149, 150, 152, 153], "alter": [58, 84], "altern": [57, 59, 60, 65, 85, 88, 103, 105, 136, 148, 150], "although": 93, "alwai": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 47, 51, 57, 90, 152], "always_tak": [25, 59, 85], "alyssa": 151, "amamb": 84, "american": [40, 87], "amgrem": 84, "amhorn": 84, "amit": [93, 151], "amjavl": 84, "ammata": 84, "among": [59, 77, 85, 86, 92, 93], "amount": [16, 59, 83, 85, 86, 153], "amp": [55, 58, 60, 61, 62, 64, 65, 66, 67, 84, 86, 92, 94], "an": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 46, 47, 50, 51, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 71, 73, 74, 75, 77, 80, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 104, 105, 106, 107, 110, 112, 113, 120, 121, 136, 137, 138, 143, 148, 150, 151, 152, 153], "analog": [20, 21, 58, 64, 65, 67, 84, 86, 92, 104, 106, 108, 121, 122, 123, 124, 125, 136, 137, 143], "analys": [97, 102, 153], "analysi": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 45, 56, 58, 59, 73, 84, 85, 86, 91, 96, 103, 104, 106, 111, 138, 143, 147, 148, 152], "analyst": 91, "analyt": [87, 89], "analyz": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 59, 85, 86, 92, 153], "ancillari": 93, "andrea": 151, "angl": 59, "angrist": 87, "ani": [55, 56, 57, 60, 61, 62, 70, 72, 73, 91, 93, 94, 96, 106, 149, 153], "anna": [12, 14, 15, 17, 18, 19, 31, 35, 39, 57, 62, 64, 65, 66, 67, 106, 107, 108, 110, 112, 151], "annal": [136, 151], "anneal": 105, "annot": 56, "annual": 151, "anoth": [56, 57, 58, 59, 73, 82, 83, 84, 91, 96, 105, 106, 114], "anticip": [14, 16, 17, 19, 57, 65, 66, 106, 107, 109, 110, 111, 112], "anticipation_period": [14, 16, 17, 19, 64, 67], "anymor": [58, 84], "aos1161": 136, "aos1230": 136, "aos1671": 136, "ap": [59, 85], "ape_e401_uncond": 59, "ape_p401_uncond": 59, "api": [70, 97, 102, 148, 152], "apo": [22, 23, 70, 113, 126, 144], "apo_result": 70, "apoorva": 152, "apoorva__l": 87, "apoorval": 87, "app": 152, "appdata": 70, "appeal": 93, "append": [70, 73, 82, 91, 96], "appendix": [34, 36, 61, 64, 67, 92, 94, 137, 138], "appli": [11, 12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 55, 56, 58, 59, 60, 61, 63, 64, 66, 67, 73, 81, 82, 84, 85, 86, 90, 91, 93, 94, 96, 106, 120, 121, 136, 148, 150, 151, 152, 153], "applic": [56, 62, 70, 73, 87, 93, 96, 98, 102, 104, 120, 151, 153], "applicatoin": 65, "apply_along_axi": 88, "apply_cross_fit": [56, 120], "apply_crossfit": 152, "appreci": 148, "approach": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 57, 58, 70, 71, 84, 90, 92, 93, 103, 105, 106, 120, 136, 137, 138, 149, 151, 153], "appropri": [59, 77, 85, 106, 120, 153], "approx": [104, 106, 109, 111], "approxim": [56, 67, 73, 74, 75, 76, 82, 89, 93, 96, 104, 106, 136, 152, 153], "april": 64, "apt": 149, "ar": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 37, 38, 39, 42, 43, 44, 46, 47, 48, 49, 50, 51, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 73, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 129, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 152, 153], "arang": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 63, 73, 76, 86, 88, 89, 92, 93, 97, 102, 105], "arbitrarili": [47, 51], "architectur": [70, 121, 151], "arellano": 151, "arg": [83, 90, 104, 106], "argmax": 91, "argmin": 82, "argu": [56, 59, 73, 85, 86, 92, 96, 153], "argument": [12, 14, 15, 17, 18, 19, 22, 24, 25, 26, 27, 28, 30, 37, 38, 41, 42, 43, 44, 45, 48, 59, 62, 64, 65, 66, 67, 74, 75, 80, 82, 84, 85, 86, 91, 95, 97, 104, 105, 106, 107, 152, 153], "aris": [56, 57, 58, 65, 73, 84, 93, 96, 153], "aronow": 87, "around": [57, 59, 70, 85, 86, 90, 106, 121], "arr": 88, "arrai": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 46, 47, 48, 49, 50, 51, 61, 62, 64, 65, 66, 67, 70, 71, 73, 74, 75, 76, 82, 84, 87, 88, 91, 92, 93, 94, 95, 96, 98, 100, 101, 104, 105, 120, 136, 137, 143, 150, 152, 153], "arrang": 58, "array_lik": 29, "articl": [43, 148], "arxiv": [41, 57, 58, 84, 91, 93, 148, 151, 152], "as_learn": [60, 105], "asarrai": [74, 75], "aspect": [59, 85, 86], "assert": 105, "assess": 57, "asset": [86, 92, 153], "assign": [4, 5, 6, 7, 8, 9, 17, 19, 59, 64, 67, 70, 85, 90, 97, 102, 104, 105, 106, 107, 119, 137, 153], "assmput": [106, 119], "associ": [59, 77, 85, 106, 136, 151], "assum": [55, 58, 62, 72, 84, 87, 88, 91, 93, 106, 107, 108, 110, 112, 121, 122, 123, 124, 125, 136, 137, 147, 153], "assumpt": [57, 58, 59, 61, 62, 63, 64, 65, 67, 82, 84, 85, 87, 90, 94, 106, 107, 108, 110, 112, 119, 136, 153], "assur": 152, "astyp": [64, 66, 72, 90, 93], "asymptot": [20, 21, 56, 58, 70, 73, 84, 96, 120, 136, 151], "ate": [70, 71], "ate_estim": [61, 94], "ates": [70, 71], "athei": 151, "att": [16, 17, 19, 26, 31, 57, 63, 80, 81, 88, 93, 104, 106, 107, 108, 109, 110, 111, 112, 116, 121, 129, 137, 145, 152], "att_": [106, 110], "att_gt": [57, 65, 66], "attach": 57, "atte_estim": 62, "attempt": [46, 47], "attenu": [59, 85], "attr": 59, "attribut": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 48, 49, 50, 51, 82, 83, 95, 99, 102, 105, 120, 121, 136], "attributeerror": [46, 47], "attrict": 106, "attrit": [30, 61, 94, 106, 119], "au": [60, 105, 148, 150], "auc": 57, "author": [57, 93, 148], "auto_ml": 83, "autodoubleml": 83, "autom": 83, "automat": [13, 16, 56, 64, 66, 67, 73, 80, 96, 104, 137, 143], "automl": 152, "automl_l": 83, "automl_l_lesstim": 83, "automl_m": 83, "automl_m_lesstim": 83, "automobil": [58, 84], "autos": 77, "autosklearn": 83, "auxiliari": [56, 73, 96], "avail": [33, 57, 59, 60, 62, 64, 65, 67, 70, 71, 77, 82, 85, 86, 87, 88, 90, 93, 96, 97, 102, 104, 105, 106, 107, 108, 109, 111, 137, 147, 148, 149, 152, 153], "avaiv": 49, "aver": 71, "averag": [17, 18, 19, 22, 23, 25, 26, 31, 38, 39, 55, 57, 60, 61, 62, 63, 64, 66, 67, 72, 80, 86, 87, 88, 90, 91, 92, 93, 94, 103, 107, 108, 112, 113, 114, 115, 116, 119, 126, 129, 136, 144, 145, 151, 152, 153], "average_it": [70, 71], "avoid": [56, 57, 73, 90, 106, 120, 149, 152], "awai": 92, "ax": [13, 16, 63, 64, 65, 66, 67, 70, 71, 73, 74, 75, 76, 78, 79, 82, 83, 84, 85, 86, 87, 89, 90], "ax1": [70, 71, 76, 81, 86, 89], "ax2": [70, 71, 76, 81, 86, 89], "axhlin": [63, 70, 83, 90], "axi": [13, 16, 58, 59, 70, 71, 77, 81, 82, 84, 85, 87, 88, 90], "axvlin": [70, 71, 73], "b": [12, 14, 15, 16, 17, 19, 43, 56, 58, 60, 73, 74, 75, 84, 87, 89, 90, 93, 96, 104, 105, 106, 112, 136, 137, 147, 148, 150, 151], "b208": 60, "b371": 60, "b5d34a6f42b": 60, "b5d7": 60, "b_": 106, "b_0": 42, "b_1": 42, "b_j": 43, "bach": [77, 82, 83, 93, 148, 151, 152], "back": 70, "backbon": 82, "backend": [5, 6, 7, 8, 9, 57, 86, 92, 93, 97, 99, 101, 103, 152], "backward": [4, 97, 102, 152], "bad": 87, "bag": 70, "balanc": [59, 64, 65, 67, 85, 86], "bam5698": 70, "band": [57, 103, 153], "bandwidth": [27, 28, 29, 44, 90, 106, 152], "bar": [80, 83, 85, 104, 106, 121, 126, 129, 137, 144], "base": [12, 14, 15, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 37, 38, 45, 49, 56, 57, 58, 59, 61, 62, 63, 65, 66, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 100, 102, 104, 105, 106, 107, 110, 112, 119, 120, 121, 136, 137, 138, 143, 148, 150, 151, 152, 153], "base_estim": [50, 51, 90], "baseestim": 91, "baselin": [14, 35, 59, 83, 85], "basi": [22, 26, 38, 48, 67, 74, 75, 81, 104], "basic": [57, 58, 59, 62, 66, 84, 85, 86, 87, 90, 92, 93, 103, 105], "basis_df": 81, "basis_matrix": 81, "batch": 60, "battocchi": 151, "bay": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 136], "bb2913dc": 60, "bbotk": [60, 105, 152], "bbox_inch": 73, "bbox_to_anchor": 73, "bcallaway11": [57, 65], "bd929a9e": 60, "bde4": 60, "becam": [59, 85, 86], "becaus": [47, 51, 55, 56, 57, 58, 72, 73, 80, 84, 87, 91, 93, 96, 153], "becker": [60, 105], "becom": [58, 83, 84, 104, 120], "bee": 63, "been": [13, 16, 58, 59, 64, 66, 67, 83, 84, 85, 86, 92, 93, 104, 105, 106, 112, 148, 152], "befor": [17, 19, 57, 59, 63, 64, 66, 67, 71, 80, 85, 89, 93, 106, 108, 153], "begin": [33, 40, 41, 56, 58, 59, 60, 61, 63, 64, 67, 73, 76, 82, 84, 85, 87, 88, 89, 94, 95, 97, 102, 105, 106, 109, 111, 112, 120, 121, 123, 125, 136, 150, 153], "behav": [64, 66, 67, 91], "behavior": [59, 87, 105], "behind": [65, 106], "being": [17, 19, 20, 21, 35, 36, 45, 50, 51, 58, 67, 84, 90, 93, 106, 111, 112, 120, 121, 127, 136, 137, 143, 148], "belloni": [34, 77, 136, 151], "below": [55, 59, 65, 72, 85, 87, 91, 106, 149, 150], "bench_x1": 93, "bench_x2": 93, "benchmark": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 52, 70, 71, 80, 138, 152], "benchmark_dict": [52, 92], "benchmark_inc": 92, "benchmark_pira": 92, "benchmark_result": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38], "benchmark_twoearn": 92, "benchmarking_set": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 71, 80, 92, 93, 137, 138], "benchmarking_vari": 80, "benefit": [56, 59, 73, 85, 96], "bernoulli": 33, "berri": [58, 84], "besid": 150, "best": [22, 26, 38, 47, 48, 51, 64, 67, 70, 74, 75, 78, 79, 83, 149], "best_loss": 83, "beta": [30, 33, 34, 36, 40, 59, 61, 85, 88, 90, 94, 106], "beta_": [61, 94], "beta_0": [32, 61, 88, 94, 104], "beta_a": [31, 39, 93], "beta_j": [33, 34, 36, 40], "better": [57, 64, 67, 70, 71, 82, 93, 106, 112], "between": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 55, 61, 63, 64, 67, 70, 71, 72, 76, 77, 83, 87, 89, 91, 92, 93, 94, 106, 114, 121, 122, 123, 124, 125, 129, 132, 133, 136, 137, 147, 150, 152], "betwen": [55, 72], "beyond": 151, "bia": [36, 55, 61, 72, 77, 90, 93, 94, 103, 106, 119, 120, 121, 134, 135, 137, 147, 151, 152], "bias": [55, 59, 64, 67, 72, 85, 86, 92, 153], "bias_bench": 93, "bibtex": 148, "big": [77, 95, 120, 121, 122, 123, 130, 136, 137, 139, 140, 141, 142, 145, 146, 147], "bigg": [58, 84, 121, 128, 129, 137, 140, 145], "bilia": 11, "bilinski": 151, "bin": [56, 70, 71, 73, 149], "binari": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 32, 38, 45, 55, 57, 59, 60, 62, 72, 80, 81, 82, 85, 87, 88, 93, 104, 105, 108, 112, 115, 116, 119, 137, 144, 145, 152, 153], "binary_outcom": 45, "binary_treat": [32, 74, 78, 80], "bind": 152, "binder": [60, 105, 148, 150, 152], "binomi": [72, 87, 88, 89, 91], "bischl": [60, 105, 148, 150], "black": [56, 60, 68, 97, 102, 150], "blob": 57, "blog": 43, "blondel": [148, 150], "blp": [48, 58, 84], "blp_data": [58, 84], "blp_model": [78, 79], "blue": [56, 58, 61, 84], "bodori": 151, "bond": [59, 85, 86], "bonferroni": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 136], "bonu": [11, 60, 97, 102, 150], "book": [60, 91, 93, 105, 151], "bool": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 44, 45, 46, 47, 48, 50, 51, 80, 90], "boolean": [36, 78, 79, 97, 102, 120], "boost": [55, 59, 62, 64, 67, 70, 72, 82, 85], "boost_class": [59, 85], "boost_summari": 85, "boostrap": [76, 152], "bootstrap": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 48, 64, 65, 66, 67, 71, 74, 75, 76, 78, 79, 86, 89, 103, 104, 106, 107, 120, 121, 148, 150, 152, 153], "both": [12, 13, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 57, 59, 60, 62, 63, 70, 81, 82, 83, 85, 86, 88, 90, 91, 92, 93, 97, 98, 102, 105, 106, 109, 110, 111, 136, 137, 138, 143, 146, 147, 152, 153], "bottom": [58, 59, 82, 84, 85, 86], "bound": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 35, 37, 38, 59, 64, 67, 70, 71, 80, 81, 85, 92, 93, 137, 138, 143, 147, 152, 153], "branch": 60, "brantli": [57, 151], "break": [56, 152], "breviti": 153, "brew": 149, "brewer": 58, "bridg": 93, "brief": 96, "briefli": 65, "bring": [55, 72], "brucher": [148, 150], "bsd": [148, 152], "bst": 85, "budget": [83, 105], "bug": [148, 152], "build": [58, 82, 84, 88], "build_design_matric": [74, 75], "build_sim_dataset": 57, "built": [49, 83, 105, 148], "bureau": [93, 120, 151], "busi": [36, 41, 58, 84, 93, 151], "b\u00fchlmann": 151, "c": [10, 11, 17, 18, 19, 34, 39, 40, 42, 55, 56, 57, 58, 59, 60, 63, 68, 70, 72, 73, 77, 78, 79, 84, 85, 87, 90, 91, 96, 97, 102, 105, 106, 112, 121, 123, 125, 137, 140, 142, 148, 149, 150, 151, 153], "c1": [10, 11, 42, 58, 77, 84, 91, 96, 148, 151], "c68": [10, 11, 42, 58, 77, 84, 91, 96, 148, 151], "c895": 60, "c_": [16, 106, 109, 111, 112, 121, 123, 125, 136], "c_d": [34, 137, 145, 146, 147], "c_i": [106, 112], "c_y": [34, 137, 147], "ca1af7be64b2": 60, "caac5a95": 60, "calcualt": 88, "calcul": [22, 26, 38, 57, 59, 64, 67, 70, 71, 74, 75, 76, 78, 79, 82, 83, 85, 89, 92, 137, 143, 147], "calendar": [64, 65, 66, 67], "calibr": [82, 83, 93], "call": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 47, 50, 51, 55, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 70, 72, 74, 75, 76, 78, 79, 85, 86, 88, 89, 90, 92, 93, 94, 97, 102, 105, 120, 121, 136, 137, 143, 147, 150, 152, 153], "callabl": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 56, 73, 74, 75, 82, 103, 105, 148], "callawai": [17, 19, 57, 64, 65, 66, 67, 106, 107, 110, 112, 151], "camera": 77, "cameron": [58, 84], "can": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 44, 47, 49, 51, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 102, 104, 105, 106, 107, 108, 110, 112, 113, 119, 120, 121, 122, 123, 124, 125, 126, 129, 132, 133, 136, 137, 138, 143, 144, 145, 146, 147, 148, 149, 150, 152, 153], "candid": 93, "cannot": [82, 90, 93, 106, 153], "capabl": [5, 6, 7, 8, 9, 55, 72], "capo": [22, 81], "capo0": 81, "capo1": 81, "capsiz": [70, 71, 83, 87, 90], "capthick": [70, 71, 90], "cardin": [58, 84], "care": [65, 91, 105], "carlo": [31, 32, 35, 39, 74, 75, 78, 79, 93, 151], "casalicchio": [60, 105, 148, 150], "case": [4, 5, 6, 7, 8, 9, 11, 14, 22, 25, 26, 32, 44, 55, 58, 59, 64, 65, 67, 72, 74, 75, 76, 77, 80, 81, 83, 84, 88, 89, 90, 91, 92, 93, 97, 102, 104, 105, 106, 108, 112, 119, 120, 121, 123, 125, 136, 137, 143, 150, 152, 153], "cat": [56, 152], "catboost": 82, "cate": [26, 38, 48, 81, 103, 152], "cate_obj": 104, "categori": 70, "cattaneo": [106, 151], "caus": [56, 73, 90, 96], "causal": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 44, 55, 56, 58, 59, 60, 61, 72, 73, 81, 82, 83, 84, 85, 87, 91, 92, 94, 95, 96, 97, 100, 102, 103, 106, 112, 120, 136, 137, 143, 151], "causal_contrast": [23, 70, 71, 81, 106], "causal_contrast_att": 81, "causal_contrast_c": 81, "causal_contrast_model": [70, 71, 106], "causal_contrast_result": 70, "causaldml": 151, "causalml": [91, 151], "causalweight": 151, "caution": 136, "caveat": 93, "cbind": 58, "cbook": [64, 65, 66, 67], "cc": 85, "ccp_alpha": [26, 49, 85], "cd": 149, "cd_fast": 84, "cda85647": 60, "cdf": 104, "cdid": [58, 84], "cdot": [17, 18, 19, 31, 35, 39, 45, 58, 63, 64, 67, 76, 80, 84, 87, 89, 90, 93, 104, 106, 107, 109, 111, 112, 120, 121, 123, 125, 126, 129, 130, 134, 135, 136, 137, 140, 142, 144], "cdot1": 80, "cell": 83, "center": [64, 65, 67, 70, 77], "central": [120, 152], "certain": [106, 121, 123, 125], "cexcol": 58, "cexrow": 58, "cf_d": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 52, 64, 67, 71, 80, 81, 92, 93, 137, 138, 143, 144, 145, 146, 147, 153], "cf_y": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 52, 64, 67, 71, 80, 81, 92, 93, 137, 138, 143, 144, 145, 146, 147, 153], "cff": 152, "chad": 93, "challeng": [58, 84, 137, 138], "chanc": 67, "chang": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 50, 51, 59, 61, 62, 66, 86, 92, 93, 94, 106, 111, 112, 121, 125, 129, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 149, 151, 152], "channel": 153, "chapter": [20, 21, 60, 91, 105, 137, 147], "charact": [59, 60, 105, 152], "characterist": [92, 153], "chart": 83, "check": [46, 47, 50, 51, 56, 59, 62, 63, 64, 67, 73, 82, 83, 85, 86, 91, 95, 96, 106, 107, 148, 149, 152], "check_data": 152, "check_scor": 152, "checkmat": 152, "chernozhukov": [10, 11, 34, 40, 42, 56, 58, 59, 73, 77, 82, 83, 84, 85, 86, 91, 92, 96, 120, 121, 129, 136, 137, 138, 147, 148, 151, 152], "chetverikov": [10, 11, 42, 58, 77, 84, 91, 96, 136, 148, 151], "chiang": [41, 58, 84, 151], "chieh": 151, "choic": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 59, 64, 65, 66, 67, 77, 85, 88, 104, 105, 106, 110, 121, 123, 125, 137, 138, 143, 147, 152], "cholecyst": 91, "choos": [55, 59, 65, 72, 73, 77, 82, 85, 86, 95, 106, 110, 120, 121, 122, 123, 124, 125, 129, 132, 133, 136, 150, 153], "chosen": [22, 35, 39, 82, 105, 106], "chou": 87, "chr": 59, "christian": [77, 151], "christoph": 151, "chunk": 105, "ci": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 44, 62, 63, 64, 65, 66, 67, 70, 71, 74, 75, 76, 78, 79, 80, 81, 83, 85, 86, 89, 90, 92, 93, 104, 106, 137, 143, 152, 153], "ci_at": [70, 71], "ci_cvar": [76, 86], "ci_cvar_0": 76, "ci_cvar_1": 76, "ci_joint": [64, 65, 66, 67, 71], "ci_joint_cvar": 76, "ci_joint_lqt": 89, "ci_joint_qt": 89, "ci_length": 62, "ci_low": [70, 71], "ci_lpq_0": 89, "ci_lpq_1": 89, "ci_lqt": [86, 89], "ci_pointwis": [70, 71], "ci_pq_0": [86, 89], "ci_pq_1": [86, 89], "ci_qt": [86, 89], "ci_upp": [70, 71], "cinelli": [93, 137, 138, 151], "circumv": 153, "citat": 152, "cite": 148, "claim": 60, "clarifi": [64, 65, 66, 67], "clash": 57, "class": [0, 4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 46, 47, 48, 49, 50, 51, 59, 60, 61, 62, 64, 65, 66, 67, 68, 70, 71, 80, 81, 83, 85, 86, 91, 92, 94, 95, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 120, 121, 136, 148, 150, 152], "class_estim": 90, "class_learn": 86, "class_learner_1": 82, "class_learner_2": 82, "classes_": [83, 91], "classic": [57, 58, 65, 84, 153], "classif": [26, 46, 50, 55, 57, 59, 60, 61, 62, 64, 65, 66, 67, 82, 83, 86, 88, 92, 94, 104, 105, 106, 108, 109, 111, 153], "classifavg": 60, "classifi": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 38, 44, 46, 50, 60, 70, 71, 83, 90, 105, 152], "classifiermixin": 91, "classmethod": [4, 5, 6, 7, 8, 9], "claudia": [151, 152], "claus": [148, 152], "clean": 152, "cleaner": 82, "cleanup": 152, "clear": [58, 70, 84], "clearli": [64, 67, 90], "clever": 82, "client": 70, "clone": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 55, 56, 60, 73, 82, 84, 86, 95, 105, 106, 120, 121, 136, 137, 143, 149, 150], "close": [57, 59, 70, 85, 91, 93, 137, 138], "cluster": [5, 6, 7, 8, 9, 12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 41, 66, 97, 98, 100, 101, 102, 151, 152], "cluster_col": [4, 5, 6, 8, 9, 58, 84, 97, 98, 100, 101, 102], "cluster_var": [4, 5, 6, 7, 8, 9, 41, 97, 98, 102], "cluster_var_i": [58, 84], "cluster_var_j": [58, 84], "cmap": 84, "cmd": 152, "co": [43, 63], "codaci": 152, "code": [22, 26, 38, 43, 55, 57, 58, 59, 60, 61, 72, 77, 85, 96, 104, 105, 106, 120, 121, 136, 149, 150, 152, 153], "codecov": 152, "coef": [12, 15, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 56, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 92, 93, 94, 95, 96, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 150, 153], "coef_": 93, "coef_df": 58, "coef_valu": 83, "coeffici": [16, 31, 32, 39, 47, 48, 51, 59, 61, 78, 79, 82, 85, 87, 88, 90, 93, 94, 104, 136, 137, 143, 153], "coefs_t": 88, "coefs_w": 88, "coffici": [137, 143], "cofid": 48, "coincid": [63, 81, 86], "col": [56, 58, 85], "col_nam": [64, 67], "collect": [60, 61, 62, 70, 84, 94], "colnam": [58, 82], "color": [13, 16, 59, 61, 63, 67, 70, 71, 73, 74, 75, 76, 83, 84, 85, 86, 87, 89, 90, 93], "color_palett": [13, 16, 64, 67, 70, 71, 73, 84, 85, 86], "colorbar": 84, "colorblind": [13, 16, 64, 67, 70, 71], "colorramppalett": 58, "colorscal": [74, 75], "colour": [56, 58], "column": [5, 6, 7, 8, 9, 62, 63, 64, 65, 66, 67, 68, 70, 71, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 90, 92, 93, 94, 97, 98, 99, 100, 101, 102, 104, 105, 106, 109, 111, 120, 150, 152, 153], "column_stack": [63, 70, 71, 78, 79, 90, 92, 93, 106], "colv": 58, "com": [43, 57, 59, 60, 65, 66, 77, 87, 93, 105, 148, 149], "comb": 77, "combin": [14, 16, 57, 58, 60, 62, 65, 66, 70, 71, 81, 82, 83, 84, 93, 105, 106, 110, 120, 137, 143, 152], "combind": 86, "combined_loss": 77, "come": [95, 105, 121, 137, 138, 148, 153], "command": [149, 152], "comment": [97, 102], "commit": 152, "common": [82, 92, 93, 104, 106, 151], "commonli": 70, "companion": 151, "compar": [56, 58, 63, 64, 67, 70, 73, 74, 75, 76, 78, 79, 81, 87, 89, 90, 91, 93, 96, 105, 106, 137, 138, 152], "comparevers": 59, "comparison": [64, 67, 71, 82, 87], "compat": [4, 55, 57, 72, 97, 102, 152], "complement": 93, "complet": [83, 96, 137, 143, 149], "complex": [26, 57, 83], "compli": [90, 106], "complianc": [89, 90, 106, 121, 130], "complic": [60, 153], "complier": [59, 85, 86, 89, 90, 104, 106], "compon": [17, 19, 50, 51, 57, 59, 65, 70, 77, 82, 83, 85, 88, 104, 105, 120, 121, 122, 123, 124, 125, 126, 128, 129, 130, 132, 133, 152], "compont": 57, "composit": 151, "comprehens": 70, "compris": 136, "comput": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 48, 52, 56, 57, 59, 60, 64, 65, 66, 67, 70, 73, 85, 86, 91, 92, 93, 120, 121, 137, 138, 139, 140, 141, 142, 143, 144, 145, 148, 151, 152, 153], "computation": [137, 138], "concat": [70, 83, 84, 85, 88, 136], "concaten": [63, 85, 136], "concentr": 136, "concern": 93, "conclud": [90, 93, 153], "cond": [106, 108, 119], "conda": [151, 152], "condit": [20, 21, 22, 24, 26, 31, 32, 38, 39, 56, 58, 59, 61, 62, 63, 64, 67, 71, 73, 80, 81, 84, 85, 88, 90, 93, 94, 96, 103, 106, 109, 110, 111, 112, 136, 137, 144, 145, 147, 150, 151, 152, 153], "conduct": [104, 106, 108, 109, 111, 153], "conf": [57, 89], "confer": 151, "confid": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 48, 57, 58, 59, 61, 62, 64, 65, 66, 67, 70, 71, 74, 75, 76, 78, 79, 81, 84, 86, 89, 90, 92, 94, 103, 104, 106, 120, 121, 137, 143, 150, 151, 152, 153], "confidenceband": 76, "confidenti": 93, "config": 87, "configur": [60, 83], "confint": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 48, 59, 61, 62, 63, 64, 65, 66, 67, 70, 71, 74, 75, 76, 78, 79, 81, 82, 86, 88, 89, 90, 91, 92, 94, 104, 120, 136, 148, 150, 153], "conflict": 149, "confound": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 37, 38, 39, 52, 55, 59, 64, 65, 67, 72, 80, 85, 89, 92, 93, 97, 102, 106, 117, 118, 136, 137, 138, 143, 146, 147, 150, 151, 152, 153], "congress": 151, "connect": [59, 85, 86], "consequ": [31, 39, 58, 80, 84, 92, 104, 106, 108, 137, 138, 144, 145, 147], "conserv": [92, 93, 137, 147], "consid": [24, 25, 26, 27, 28, 45, 56, 58, 59, 61, 62, 63, 64, 65, 66, 67, 73, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 104, 105, 106, 107, 112, 115, 116, 120, 121, 136, 137, 138, 148, 153], "consider": [93, 106], "consist": [37, 38, 47, 51, 59, 62, 83, 85, 86, 87, 93, 96, 97, 102, 106, 112, 117, 118, 150, 152], "consol": [56, 152], "constant": [17, 19, 34, 47, 51, 64, 67, 77, 88, 104, 106, 136], "constrained_layout": 73, "construct": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 37, 38, 48, 60, 63, 64, 67, 74, 75, 76, 81, 86, 91, 92, 95, 104, 121, 127, 135, 136, 152, 153], "construct_framework": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "construct_iv": 84, "constructiv": 58, "constructor": [60, 98, 102], "consum": [58, 84], "cont": 35, "cont_d": [70, 71], "contain": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 45, 46, 47, 50, 51, 56, 58, 59, 64, 65, 67, 71, 73, 74, 75, 78, 79, 82, 84, 85, 91, 96, 98, 99, 101, 102, 104, 105, 106, 107, 109, 111, 136, 137, 138, 143, 152], "context": [93, 106, 119, 153], "contin": [35, 83], "continu": [35, 55, 60, 70, 71, 72, 77, 87, 90, 106, 137, 147, 152, 153], "contour": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 77, 80, 92, 93, 137, 143], "contour_plot": 93, "contours_z": [74, 75], "contrast": [23, 62, 70, 76, 81, 106, 114], "contribut": [149, 152], "contributor": 152, "control": [14, 16, 17, 19, 40, 45, 57, 65, 66, 77, 81, 86, 88, 90, 91, 93, 97, 98, 102, 106, 107, 109, 110, 111, 112, 121, 123, 125, 137, 140, 142, 153], "control_group": [14, 16, 64, 65, 66, 67, 106, 107, 109, 111], "conveni": [97, 100, 102], "convent": [44, 59, 65, 85, 86, 90, 106, 112], "converg": [56, 73, 82, 84, 96], "convergencewarn": 84, "convers": 84, "convert": [70, 76, 84, 89], "convex": 87, "cooper": 152, "coor": [60, 105, 148, 150], "coordin": [70, 93], "copi": [67, 83, 85, 88, 93], "cor": [137, 147], "core": [17, 19, 64, 65, 66, 67, 68, 70, 71, 76, 80, 84, 85, 86, 89, 92, 97, 99, 102, 105, 150, 152], "cores_us": [76, 86, 89], "correct": [80, 81, 93, 104, 136, 152], "correctli": [46, 50, 62, 87, 92, 137, 147], "correl": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 58, 61, 77, 84, 91, 92, 94, 106, 137, 138, 147], "correpond": [64, 67, 106], "correspond": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 56, 58, 59, 60, 62, 63, 64, 65, 66, 67, 71, 73, 74, 75, 77, 81, 82, 84, 85, 86, 88, 89, 92, 93, 96, 104, 105, 106, 108, 110, 112, 113, 119, 120, 121, 123, 125, 136, 137, 138, 140, 142, 143, 145, 147, 152, 153], "correspondingli": [64, 67], "cosh": 43, "coul": 58, "could": [55, 60, 64, 65, 67, 72, 74, 75, 83, 93, 152, 153], "counfound": [31, 39, 89, 92, 104, 137, 147], "count": [70, 71, 85, 86], "counti": 65, "countour": [137, 143], "countyr": 65, "coupl": [59, 85, 86], "cournapeau": [148, 150], "cours": [59, 82, 85, 93, 136, 153], "cov": [30, 31, 45, 90], "cov_nam": [90, 106], "cov_typ": [22, 26, 38, 48, 152], "covari": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 19, 26, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 48, 49, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 68, 70, 71, 73, 74, 75, 77, 78, 79, 80, 83, 84, 85, 86, 87, 88, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 108, 109, 111, 117, 118, 119, 121, 122, 123, 124, 125, 136, 137, 138, 150, 151, 152], "cover": [57, 77, 92, 101, 102], "coverag": [64, 67, 82, 90, 91, 104, 152], "cp": [59, 60, 105], "cpu": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 70], "cpu_count": [76, 86, 89], "cran": [60, 151, 152], "creat": [13, 16, 17, 19, 32, 55, 58, 60, 64, 67, 71, 72, 73, 74, 75, 76, 78, 79, 84, 86, 88, 89, 93, 97, 102, 105, 137, 138, 143, 147, 149, 152], "create_synthetic_group_data": 88, "crictial": 120, "critic": [93, 153], "cross": [12, 14, 15, 16, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 55, 56, 57, 59, 60, 61, 65, 70, 73, 82, 83, 85, 86, 90, 93, 96, 98, 102, 103, 105, 109, 110, 112, 124, 135, 136, 139, 140, 143, 152, 153], "cross_sectional_data": [15, 18, 62, 106, 108], "crossfit": [82, 106], "crosstab": 87, "crucial": [77, 106, 153], "csail": [148, 150], "csdid": 66, "csv": [66, 77], "cuda": 70, "cumul": 106, "current": [49, 57, 62, 63, 64, 67, 81, 121, 137, 147, 148, 149, 153], "custom": [13, 16, 56, 57, 65, 73, 93, 105], "custom_measur": 57, "cut": 88, "cutoff": [44, 45, 90, 106], "cv": [60, 85, 105, 120], "cv_glmnet": [58, 59, 60, 61, 105, 106, 136, 150], "cvar": [24, 29, 103, 127, 152], "cvar_0": 76, "cvar_1": 76, "d": [4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 48, 49, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 67, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 111, 113, 115, 116, 117, 118, 119, 120, 121, 122, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 143, 144, 145, 146, 147, 148, 150, 151, 153], "d0": [76, 89, 136], "d0_true": 89, "d0cdb0ea4795": 60, "d1": [76, 87, 89, 136], "d10": 136, "d1_true": 89, "d2": [87, 136], "d21ee5775b5f": 60, "d2cml": 66, "d3": 136, "d4": 136, "d5": 136, "d5a0c70f1d98": 60, "d6": 136, "d7": 136, "d8": 136, "d9": 136, "d_": [35, 41, 58, 63, 71, 84, 106, 108, 112, 136], "d_0": [106, 113], "d_1": [87, 136], "d_2": 87, "d_col": [4, 5, 6, 7, 8, 9, 16, 55, 56, 58, 59, 60, 61, 64, 65, 66, 67, 72, 74, 75, 78, 79, 81, 84, 85, 86, 88, 90, 91, 92, 95, 96, 97, 98, 99, 101, 102, 105, 106, 107, 109, 111, 120, 121, 150, 152, 153], "d_i": [32, 33, 34, 36, 40, 42, 43, 56, 61, 62, 71, 73, 76, 87, 89, 90, 94, 96, 106, 108, 119], "d_j": [71, 106, 113, 114, 136], "d_k": [106, 114, 136], "d_l": [106, 113], "d_w": 88, "da1440": 87, "dag": [61, 93, 94, 153], "dai": 64, "dark": [56, 73], "darkblu": 58, "darkr": 58, "dash": [61, 64, 67, 70, 71], "dat": [97, 102], "data": [0, 10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 50, 51, 57, 63, 77, 82, 87, 91, 95, 97, 99, 100, 101, 103, 104, 105, 107, 109, 110, 111, 112, 120, 136, 140, 141, 142, 143, 151, 152], "data_apo": [70, 71], "data_cvar": 86, "data_dict": [44, 74, 75, 78, 79, 80, 90, 106], "data_dml": 92, "data_dml_bas": [59, 74, 75, 78, 79, 85, 86, 88], "data_dml_base_iv": [59, 85, 86], "data_dml_flex": [59, 85], "data_dml_flex_iv": 59, "data_dml_iv_flex": 85, "data_dml_new": 88, "data_fram": 153, "data_lqt": 86, "data_pq": 86, "data_qt": 86, "data_transf": [58, 84, 85], "datafram": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 44, 48, 49, 58, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 98, 99, 100, 101, 104, 105, 106, 108, 121, 136, 137, 138, 143, 150, 153], "dataset": [0, 5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 56, 57, 61, 62, 64, 67, 70, 71, 73, 74, 75, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 137, 143, 150, 153], "datatyp": [66, 152], "date": [16, 17, 19, 105], "date_format": 16, "datetim": [7, 16, 17, 19, 64, 65, 67, 99, 102], "datetime64": [64, 99, 102], "datetime_unit": [7, 16, 64, 67, 99, 102, 106, 107, 109, 111], "david": 152, "db": [59, 85, 86, 92, 153], "dbl": [57, 58, 59, 60, 97, 102, 136, 150, 153], "dc13a11076b3": 60, "ddc9": 60, "de": [55, 72, 151], "deal": [55, 72], "debias": [10, 11, 41, 42, 58, 77, 84, 91, 103, 105, 120, 148, 151, 152], "debt": [59, 85, 86], "decai": [61, 94], "decid": [59, 85, 91], "decis": [26, 55, 59, 72, 85, 86, 104, 106, 151, 153], "decision_effect": 55, "decision_impact": [55, 72], "decisiontreeclassifi": [26, 49, 85], "decisiontreeregressor": 85, "declar": 153, "decomposit": [106, 107], "decreas": 90, "dedic": [97, 101, 102], "deep": [46, 47, 50, 51, 83], "deeper": 26, "def": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 64, 67, 73, 76, 82, 83, 84, 87, 88, 89, 91, 93, 105, 121], "default": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 57, 58, 61, 62, 64, 65, 66, 67, 78, 79, 82, 84, 88, 90, 92, 93, 94, 95, 98, 102, 104, 105, 106, 120, 136, 137, 143, 144, 150, 153], "default_arg": [64, 67], "default_convert": 84, "default_jitt": 16, "defier": [90, 106], "defin": [17, 18, 19, 31, 32, 33, 34, 35, 36, 39, 40, 41, 42, 43, 44, 45, 47, 51, 56, 59, 60, 62, 65, 70, 71, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 104, 105, 106, 108, 109, 111, 112, 119, 121, 122, 124, 125, 137, 138, 143, 147, 152], "definit": [43, 64, 67, 78, 79, 81, 106, 110, 137, 144, 145], "defint": 137, "degre": [45, 59, 74, 75, 81, 84, 85, 90, 104, 137, 138], "dekel": 151, "delete_origin": 60, "deliber": 87, "delta": [16, 40, 57, 62, 64, 67, 93, 106, 108, 109, 110, 111, 112, 121, 123, 125], "delta_bench": 93, "delta_i": 57, "delta_j": 40, "delta_t": [17, 19, 64, 67], "delta_theta": [52, 71, 80, 92, 93, 137, 138], "delta_v": 93, "demand": [58, 84, 137, 138], "demir": [10, 11, 42, 58, 77, 84, 91, 96, 120, 148, 151], "demo": [65, 93], "demonstr": [56, 57, 58, 65, 70, 73, 84, 90, 93, 97, 102, 106, 136, 148, 150], "deni": 151, "denomin": [137, 138, 144, 145], "denot": [19, 37, 58, 59, 61, 62, 63, 67, 84, 85, 90, 93, 94, 104, 106, 108, 109, 111, 112, 113, 117, 121, 137, 138, 143, 145, 147], "dens_net_tfa": 59, "densiti": [27, 28, 29, 56, 61, 70, 71, 73], "dep": 68, "dep1": [60, 68, 97, 102, 150], "dep2": [60, 68, 97, 102, 150], "depend": [17, 19, 22, 24, 26, 27, 29, 32, 60, 62, 64, 67, 74, 75, 78, 79, 80, 82, 83, 88, 90, 95, 104, 105, 106, 109, 110, 111, 121, 130, 131, 137, 138, 144, 147, 150, 151], "deprec": [4, 62, 63, 64, 65, 66, 67, 95, 97, 102, 106, 120, 121, 137], "depreci": 152, "depth": [26, 49, 59, 60, 88, 95, 104, 105, 106, 120, 121, 136, 150, 153], "deriv": [12, 14, 15, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 106, 136], "describ": [13, 57, 58, 64, 67, 84, 85, 86, 93, 105, 120, 149, 152], "descript": [19, 59, 66, 68, 92, 105, 120, 121, 123, 125, 137, 138, 140, 142], "deserv": 106, "design": [8, 17, 19, 44, 45, 70, 71, 83, 100, 102, 103, 151, 152], "design_info": [74, 75], "design_matrix": [74, 75, 104], "desir": [39, 60, 88, 106, 149], "detail": [12, 14, 15, 20, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 56, 59, 60, 62, 63, 70, 71, 73, 77, 83, 86, 90, 92, 93, 96, 97, 102, 104, 105, 107, 108, 109, 110, 111, 112, 119, 121, 127, 129, 130, 131, 134, 135, 136, 137, 138, 140, 142, 147, 148, 149, 150, 152, 153], "determin": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 47, 51, 59, 67, 76, 85, 86, 89, 90, 92, 106, 136, 137, 147], "determinist": [88, 90, 104, 106], "deutsch": 148, "dev": [149, 152], "develop": [57, 58, 60, 84, 93, 106, 108, 112, 152], "deviat": [82, 106, 137, 147], "devic": 70, "dezeur": 151, "df": [5, 6, 7, 8, 9, 16, 55, 56, 58, 61, 63, 64, 67, 70, 71, 72, 74, 75, 76, 81, 84, 87, 89, 90, 92, 93, 94, 96, 98, 99, 101, 102, 104, 106, 107, 109, 111], "df_agg": 77, "df_all_apo": 70, "df_all_at": 70, "df_anticip": [64, 67], "df_apo": [70, 71], "df_apo_ci": 71, "df_apos_ci": 71, "df_ate": [70, 71], "df_bench": 93, "df_binari": 93, "df_bonu": [60, 97, 102, 150], "df_capo0": 81, "df_capo1": 81, "df_cate": [74, 75, 81], "df_causal_contrast_c": 81, "df_ci": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 48], "df_coef": 82, "df_cvar": 86, "df_fuzzi": 90, "df_lqte": 86, "df_ml_g0": 82, "df_ml_g1": 82, "df_ml_m": 82, "df_pa": [62, 94], "df_perform": 70, "df_plot": 58, "df_post_treat": [64, 67], "df_pq": 86, "df_qte": 86, "df_result": 77, "df_sharp": 90, "df_sort": [70, 71], "df_summari": 85, "df_treat": 67, "df_wide": 84, "dfg": 148, "dgp": [17, 18, 19, 58, 61, 63, 64, 67, 76, 77, 84, 87, 88, 89, 93, 94], "dgp1": [17, 18, 19], "dgp2": [17, 18, 19], "dgp3": [17, 18, 19], "dgp4": [17, 18, 19], "dgp5": [17, 18, 19], "dgp6": [17, 18, 19], "dgp_confounded_irm_data": 93, "dgp_dict": 93, "dgp_tpye": 62, "dgp_type": [17, 18, 19, 62, 64, 67], "diagnost": 65, "diagon": 93, "diagram": [55, 72, 106], "dichotom": [55, 72], "dict": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 48, 49, 50, 51, 52, 64, 67, 74, 75, 77, 83, 93, 105], "dict_kei": [137, 143], "dict_rdd": [100, 102], "dictionari": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 35, 37, 38, 39, 45, 52, 64, 67, 74, 75, 78, 79, 92, 104, 105, 106, 107, 137, 143], "dictonari": [59, 85], "did": [0, 5, 7, 56, 62, 63, 64, 65, 66, 67, 84, 98, 99, 102, 103, 107, 108, 109, 111, 112, 121, 123, 125, 152, 153], "did_aggreg": [64, 66, 67], "did_multi": [106, 107], "diff": 85, "differ": [5, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 37, 38, 39, 55, 56, 58, 59, 60, 61, 64, 66, 67, 70, 71, 72, 73, 76, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 92, 93, 94, 95, 98, 102, 103, 104, 105, 107, 108, 109, 111, 112, 120, 122, 123, 124, 125, 149, 150, 151, 152, 153], "differenti": 106, "difficult": 93, "dillon": 151, "dim": [45, 59], "dim_x": [12, 14, 15, 17, 19, 22, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 45, 56, 58, 60, 73, 81, 82, 83, 84, 96, 104, 105, 106, 137, 143], "dim_z": [37, 40, 106], "dimens": [17, 19, 32, 41, 58, 84, 88, 120], "dimension": [13, 32, 34, 37, 38, 77, 91, 104, 106, 117, 118, 120, 136, 137, 143, 150, 151], "direct": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 56, 61, 63, 73, 81, 94, 96, 106, 153], "directli": [44, 56, 57, 59, 71, 73, 82, 92, 96, 97, 102, 137, 143, 150, 153], "discontinu": [8, 44, 45, 100, 102, 103, 151, 152], "discret": [23, 35, 70, 71, 84, 106, 113, 152], "discretis": 86, "discuss": [33, 58, 59, 84, 85, 106, 107, 112, 151, 152, 153], "disjoint": [58, 78, 79, 84], "displai": [13, 58, 64, 65, 66, 67, 70, 71, 84, 93, 104, 105, 137, 143], "displot": 85, "disproportion": [59, 85], "disregard": [47, 51], "dist": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "distinct": [97, 102], "distinguish": [16, 64, 67], "distr": 105, "distribut": [45, 56, 62, 70, 71, 73, 82, 93, 96, 106, 108, 112, 137, 145, 149, 151, 152], "diverg": [44, 56, 73, 96], "divid": [64, 67, 106], "dmatrix": [74, 75, 104], "dml": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 55, 56, 60, 61, 65, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 121, 136, 137, 143, 149], "dml1": [103, 150, 152, 153], "dml2": [55, 58, 60, 61, 68, 84, 103, 106, 121, 136, 150, 152, 153], "dml_apo": 81, "dml_apo_obj": 106, "dml_apos_att": 81, "dml_apos_obj": 106, "dml_combin": 136, "dml_cover": 91, "dml_cv_predict": 152, "dml_cvar": [76, 86], "dml_cvar_0": 76, "dml_cvar_1": 76, "dml_cvar_obj": [24, 104], "dml_data": [7, 16, 57, 58, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 80, 81, 82, 84, 87, 91, 92, 93, 94, 98, 99, 100, 101, 102, 104, 105, 106, 107, 109, 111, 136, 153], "dml_data_anticip": [64, 67], "dml_data_arrai": [98, 101, 102], "dml_data_bench": 93, "dml_data_bonu": [60, 150], "dml_data_df": 153, "dml_data_fuzzi": 90, "dml_data_lasso": 68, "dml_data_sharp": 90, "dml_data_sim": [60, 150], "dml_df": [58, 84], "dml_did": [62, 63], "dml_did_obj": [12, 15, 16, 106, 107, 108, 109, 111], "dml_doc": 70, "dml_iivm": 91, "dml_iivm_boost": [59, 85], "dml_iivm_forest": [59, 85], "dml_iivm_lasso": [59, 85], "dml_iivm_obj": [25, 72, 106], "dml_iivm_tre": [59, 85], "dml_irm": [74, 78, 81, 82, 88], "dml_irm_at": 80, "dml_irm_att": 81, "dml_irm_boost": [59, 85], "dml_irm_forest": [59, 85], "dml_irm_gat": 80, "dml_irm_gatet": 80, "dml_irm_lasso": [59, 68, 85], "dml_irm_new": 88, "dml_irm_obj": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 92, 104, 105, 106], "dml_irm_obj_ext": 105, "dml_irm_rf": 68, "dml_irm_tre": [59, 85], "dml_irm_weighted_att": 81, "dml_kwarg": 81, "dml_length": 91, "dml_long": 52, "dml_lpq_0": 89, "dml_lpq_1": 89, "dml_lpq_obj": [27, 104], "dml_lqte": [86, 89], "dml_obj": [57, 64, 65, 66, 67, 70, 71, 92, 93], "dml_obj_al": [64, 67], "dml_obj_anticip": [64, 67], "dml_obj_bench": 93, "dml_obj_lasso": 65, "dml_obj_linear": [64, 67], "dml_obj_linear_logist": 65, "dml_obj_nyt": [64, 67], "dml_obj_univers": [64, 67], "dml_pliv": [58, 84], "dml_pliv_obj": [37, 58, 84, 106], "dml_plr": [75, 79, 136], "dml_plr_1": 136, "dml_plr_2": 136, "dml_plr_boost": [59, 85], "dml_plr_forest": [59, 85, 153], "dml_plr_lasso": [59, 68, 85], "dml_plr_no_split": 120, "dml_plr_obj": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 92, 95, 104, 105, 106, 120, 121, 136, 137, 138, 143], "dml_plr_obj_extern": 120, "dml_plr_obj_intern": 120, "dml_plr_obj_onfold": 83, "dml_plr_obj_untun": 83, "dml_plr_rf": 68, "dml_plr_tree": [59, 85, 153], "dml_pq_0": [86, 89], "dml_pq_1": [86, 89], "dml_pq_obj": [28, 104], "dml_procedur": [68, 95, 150, 152, 153], "dml_qte": [86, 89], "dml_qte_obj": [29, 104], "dml_robust_confset": 91, "dml_robust_length": 91, "dml_short": 52, "dml_ssm": [61, 94, 106], "dml_standard_ci": 91, "dml_tune": 152, "dmldummyclassifi": 105, "dmldummyregressor": 105, "dmlmt": 151, "dnorm": 56, "do": [57, 58, 59, 60, 64, 65, 67, 81, 82, 84, 85, 86, 87, 93, 104, 105, 120, 137, 147, 150, 153], "doabl": 121, "doc": [55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 148, 152], "docu": 152, "document": [63, 64, 65, 66, 67, 69, 74, 75, 78, 79, 81, 83, 93, 106, 112, 121, 137, 148, 152], "doe": [16, 23, 29, 57, 58, 59, 70, 71, 81, 84, 85, 87, 92, 93, 106, 121, 125, 137, 147, 153], "doesn": [55, 72], "doi": [10, 11, 17, 18, 19, 31, 33, 36, 39, 41, 42, 57, 58, 60, 77, 84, 93, 96, 105, 120, 136, 148, 150, 152], "domain": 88, "don": [57, 83], "done": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 65, 83, 86, 105, 120, 137, 138], "dosag": [70, 71], "dot": [30, 63, 64, 67, 88, 97, 102, 104, 105, 106, 110, 112, 113, 136, 150], "doubl": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 41, 42, 43, 44, 59, 65, 77, 82, 83, 85, 87, 91, 103, 105, 120, 121, 136, 137, 138, 152], "double_ml": [64, 70], "double_ml_bonus_data": 68, "double_ml_data_from_data_fram": [56, 96, 97, 102, 153], "double_ml_data_from_matrix": [57, 60, 97, 102, 105, 136, 150], "double_ml_framework": [106, 107], "double_ml_irm": 68, "double_ml_score_mixin": 0, "doubleiivm": 148, "doubleml": [0, 56, 58, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 86, 87, 88, 89, 90, 91, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 120, 121, 137, 143, 150, 151, 152], "doubleml2022python": 148, "doubleml2024r": 148, "doubleml_did_eval_linear": 57, "doubleml_did_eval_rf": 57, "doubleml_did_linear": 57, "doubleml_did_rf": 57, "doubleml_framework": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "doublemlapo": [70, 71, 81, 106, 121, 126, 152], "doublemlblp": [22, 26, 38, 74, 75, 81, 104, 152], "doublemlclusterdata": [0, 97, 102], "doublemlcvar": [76, 104, 121, 127, 152], "doublemldata": [0, 4, 7, 10, 11, 12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 55, 58, 60, 61, 62, 63, 68, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 103, 104, 105, 106, 108, 120, 121, 136, 137, 143, 152, 153], "doublemldid": [62, 63, 106, 108, 121, 124, 152], "doublemldidaggreg": [64, 65, 66, 67, 106, 107], "doublemldidc": [62, 106, 108, 121, 122, 152], "doublemldiddata": [0, 12, 15, 18, 62, 63, 98, 106, 108], "doublemldidmulti": [64, 65, 66, 67, 106, 107, 109, 110, 111, 121, 123, 125, 152], "doublemlframework": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 64, 65, 66, 67, 106, 107, 120, 136, 152], "doublemlframwork": 23, "doublemlidid": [106, 108], "doublemlididc": [106, 108], "doublemliivm": [55, 59, 72, 85, 91, 105, 106, 120, 121, 128, 152], "doublemlirm": [12, 14, 15, 22, 24, 25, 27, 28, 30, 37, 38, 57, 59, 68, 71, 74, 78, 80, 81, 82, 85, 87, 88, 92, 93, 104, 105, 106, 120, 121, 129, 148, 152], "doublemllpq": [89, 104, 121, 130, 152], "doublemlpaneldata": [0, 14, 16, 65, 66, 99, 106, 107, 109, 110, 111, 152], "doublemlpliv": [105, 106, 120, 121, 132, 148, 152], "doublemlplr": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 56, 59, 60, 68, 73, 75, 79, 83, 85, 87, 92, 95, 96, 104, 105, 106, 120, 121, 133, 136, 137, 143, 148, 150, 152, 153], "doublemlpolicytre": [26, 104], "doublemlpq": [86, 89, 104, 121, 131, 152], "doublemlqt": [76, 86, 89, 104, 136, 152], "doublemlrdddata": [0, 44, 90, 100, 106], "doublemlresampl": [81, 82], "doublemlsmm": 152, "doublemlssm": [61, 94, 106, 121, 134, 135], "doublemlssmdata": [0, 30, 36, 94, 101, 106], "doubli": [18, 31, 39, 57, 65, 91, 151], "down": 93, "download": [55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 97, 102, 149, 150], "download_fil": 65, "downward": 93, "dpg_dict": 92, "dpi": [56, 73, 87], "dr": [106, 110], "dramat": 57, "draw": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 93, 120, 152], "draw_sample_split": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 81, 82, 120], "drawn": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 45, 59, 64, 67, 85, 86, 88, 120], "drive": [56, 73, 96], "driven": [93, 153], "drop": [57, 83, 84, 87, 97, 102, 105, 121, 122, 123, 124, 125, 136], "dropna": [64, 67], "dt": [64, 121, 122, 137, 139], "dt_bonu": [97, 102], "dta": [57, 66], "dtrain": 85, "dtype": [64, 65, 66, 67, 68, 70, 71, 78, 79, 80, 84, 85, 86, 91, 92, 97, 99, 102, 104, 150], "dualiti": 84, "dubourg": [148, 150], "duchesnai": [148, 150], "due": [56, 57, 65, 73, 74, 75, 80, 92, 93, 96, 106, 119, 137, 138, 152, 153], "duflo": [10, 11, 42, 58, 77, 84, 91, 96, 120, 148, 151], "dummi": [22, 26, 38, 46, 47, 48, 65, 83, 93, 104, 105, 106, 108, 152], "dummyclassifi": [46, 65], "dummyregressor": [47, 65], "duplic": 152, "durabl": [60, 68, 97, 102, 150], "durat": 11, "dure": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 57, 58, 59, 60, 61, 83, 84, 85, 105, 120, 150, 152, 153], "dx": 33, "dynam": [57, 151], "e": [6, 7, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 35, 36, 37, 38, 39, 42, 44, 46, 47, 50, 51, 56, 57, 58, 59, 61, 62, 64, 65, 66, 67, 70, 71, 73, 74, 75, 77, 80, 82, 83, 84, 85, 86, 87, 90, 91, 92, 93, 94, 96, 99, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153], "e20ea26": 60, "e401": [59, 85, 86, 92, 153], "e4016553": 153, "e45228": 87, "e57c": 60, "each": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 46, 47, 50, 51, 58, 60, 63, 64, 65, 66, 67, 70, 71, 78, 79, 82, 83, 84, 86, 87, 88, 91, 92, 93, 95, 97, 99, 102, 105, 106, 112, 120, 136, 137, 143, 153], "earlier": [64, 67, 153], "earn": [59, 85, 86], "earner": [59, 85, 92], "easi": [60, 91, 121], "easier": 83, "easili": [60, 82, 83, 86, 152], "ec973f": 87, "ecolor": [63, 70, 71, 85, 87], "econ": 151, "econml": 151, "econom": [36, 40, 41, 43, 58, 77, 84, 87, 93, 120, 151], "econometr": [10, 11, 17, 18, 19, 31, 39, 42, 43, 57, 58, 77, 84, 91, 96, 148, 151], "econometrica": [34, 58, 84, 87, 91, 96, 151], "ecosystem": [148, 153], "ectj": [10, 11, 42, 58, 77, 84, 96, 148], "ed": 151, "edge_color": 73, "edgecolor": 73, "edit": [149, 151], "edu": [148, 150], "educ": [59, 85, 86, 92, 153], "ee97bda7": 60, "effect": [12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 45, 46, 47, 50, 51, 55, 56, 57, 58, 60, 61, 62, 63, 71, 72, 73, 77, 80, 84, 88, 90, 91, 94, 96, 103, 105, 107, 108, 110, 112, 114, 115, 116, 119, 120, 121, 129, 136, 137, 138, 150, 151, 152, 153], "effici": [70, 106, 151], "effort": 121, "eight": [58, 84], "either": [12, 14, 15, 16, 17, 19, 22, 24, 25, 26, 27, 28, 30, 32, 37, 38, 60, 63, 64, 67, 77, 88, 90, 104, 105, 106, 109, 112, 153], "eleanor": 151, "element": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 58, 61, 62, 64, 65, 66, 67, 74, 75, 76, 82, 84, 86, 89, 92, 94, 106, 109, 110, 111, 121, 122, 123, 124, 125, 126, 137, 140, 142, 143, 146, 147, 152], "element_text": [58, 59], "elementari": 151, "elif": [78, 79, 88], "elig": [86, 92, 153], "eligibl": [59, 85, 92], "ell": [56, 58, 73, 77, 84, 96, 121, 132, 133, 150], "ell_0": [25, 37, 38, 56, 73, 77, 83, 96, 106, 115, 121, 133], "ell_1": 65, "ell_2": 82, "els": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 48, 57, 58, 59, 63, 70, 78, 79, 84, 88, 93], "em": 151, "emphas": [58, 84], "empir": [20, 21, 56, 58, 65, 73, 84, 87, 93, 96, 106, 109, 110, 111, 120, 121, 136], "emploi": [58, 77, 84, 93, 121, 128], "employ": [59, 65, 85, 86], "employe": 153, "empti": 84, "emul": [137, 138], "enabl": [13, 70, 71, 88, 92, 104, 137, 138, 152], "enable_metadata_rout": [46, 47, 50, 51], "encapsul": [46, 47, 50, 51, 105], "encod": 87, "encount": [67, 70], "end": [33, 40, 41, 56, 57, 58, 59, 61, 63, 64, 67, 73, 76, 77, 82, 84, 85, 87, 88, 89, 94, 95, 97, 102, 105, 106, 109, 111, 112, 120, 121, 123, 125, 136, 150, 153], "endogen": [59, 85, 86, 153], "enet_coordinate_descent_gram": 84, "enforc": 65, "engin": [60, 151], "enrol": [59, 85, 86], "ensembl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 68, 70, 71, 73, 74, 75, 78, 79, 80, 82, 85, 88, 92, 93, 95, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 137, 143, 150, 153], "ensemble_learner_pipelin": 105, "ensemble_pipe_classif": 60, "ensemble_pipe_regr": 60, "ensur": [50, 51, 58, 70, 81, 83, 84, 88, 91, 100, 102], "entir": [16, 56, 59, 73, 85, 96, 137, 138], "entri": [12, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 56, 58, 64, 65, 66, 67, 68, 70, 71, 73, 80, 84, 85, 86, 92, 96, 97, 99, 102, 105, 148, 150, 152], "enumer": [63, 70, 71, 76, 78, 79, 82, 84, 85, 86, 89, 95, 105, 120], "env": [70, 149], "environ": 149, "ep": [70, 87], "epanechnikov": 44, "epsilon": [59, 62, 63, 76, 85, 89, 104, 106, 108, 112], "epsilon_": [58, 63, 64, 67, 84], "epsilon_0": 45, "epsilon_1": 45, "epsilon_i": [32, 76, 87, 88, 89], "epsilon_sampl": 88, "epsilon_tru": [76, 89], "eqnarrai": 59, "equal": [13, 17, 19, 22, 26, 58, 61, 64, 67, 84, 87, 91, 94, 104, 105, 106, 137, 145], "equat": [45, 58, 59, 84, 85, 93, 95, 136, 153], "equilibrium": [58, 84], "equiv": [106, 112, 121, 123, 125], "equival": [77, 81, 120], "err": [12, 15, 24, 25, 26, 27, 28, 29, 30, 37, 38, 62, 64, 65, 66, 67, 68, 70, 71, 72, 74, 75, 76, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 92, 93, 94, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 150, 153], "error": [5, 6, 7, 8, 9, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 50, 51, 55, 56, 57, 59, 60, 61, 63, 70, 73, 77, 78, 79, 82, 83, 85, 90, 93, 96, 105, 106, 117, 118, 120, 121, 136, 137, 143, 150, 152, 153], "errorbar": [63, 70, 71, 78, 79, 83, 85, 87, 90], "erstellt": [58, 59, 60], "es_linear_logist": 65, "es_rf": 65, "escap": 84, "esim": 90, "especi": [82, 83, 97, 102], "essenti": 93, "est": 90, "est_method": 57, "esther": [120, 151], "estim": [10, 12, 13, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 44, 46, 47, 48, 49, 50, 51, 56, 57, 58, 60, 63, 71, 73, 74, 75, 76, 78, 79, 81, 82, 84, 88, 90, 91, 95, 96, 103, 104, 105, 106, 107, 108, 109, 110, 111, 114, 116, 122, 123, 124, 127, 130, 131, 135, 137, 138, 143, 148, 151, 152], "estimand": 91, "estimator_list": 83, "et": [10, 11, 32, 34, 41, 42, 56, 58, 59, 60, 62, 73, 74, 75, 76, 77, 78, 79, 82, 84, 85, 86, 89, 92, 96, 106, 108, 112, 120, 121, 127, 129, 130, 131, 136, 137, 138, 147, 148, 150, 152], "eta": [20, 21, 56, 58, 59, 63, 83, 84, 85, 89, 90, 95, 104, 106, 109, 110, 111, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 147, 150, 153], "eta1": 87, "eta2": 87, "eta_": [136, 137, 147], "eta_0": [44, 95, 106, 109, 110, 111, 121, 136], "eta_d": [90, 106], "eta_i": [17, 19, 32, 63, 64, 67, 88, 89, 90, 106], "eta_sampl": 88, "eta_tru": 89, "etc": [12, 13, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 58, 82, 83, 84, 152], "ev": [56, 73, 96], "eval": [16, 60, 64, 65, 66, 67, 105, 106, 109, 110, 111, 121, 123, 125], "eval_metr": [59, 85, 153], "eval_pr": 57, "eval_predict": 57, "evalu": [12, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 34, 37, 38, 57, 60, 63, 64, 65, 66, 67, 74, 75, 76, 80, 81, 86, 89, 92, 95, 106, 110, 112, 151, 152], "evaluate_learn": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 82, 83, 105, 152], "evalut": 105, "even": [59, 60, 64, 67, 85, 87, 90, 105, 106, 153], "event": [106, 107], "eventstudi": [16, 64, 65, 66, 67, 106, 107], "eventu": [58, 84], "everi": [58, 65, 84], "everyth": 148, "evid": [80, 83], "exact": [81, 93], "exactli": [90, 93, 106], "examin": 70, "exampl": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 44, 55, 56, 59, 60, 61, 62, 63, 64, 66, 67, 68, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 94, 95, 96, 104, 105, 106, 107, 109, 110, 111, 119, 120, 121, 136, 137, 143, 148, 150, 152, 153], "example_attgt": 57, "example_attgt_dml_eval_linear": 57, "example_attgt_dml_eval_rf": 57, "example_attgt_dml_linear": 57, "example_attgt_dml_rf": 57, "except": [47, 51, 77, 93, 152], "excess": 82, "exclud": 52, "exclus": [22, 26, 38, 78, 79, 104], "execut": [60, 153], "exemplarili": 150, "exemplatori": 88, "exhaust": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 105], "exhibit": [58, 84], "exist": [46, 47, 50, 51, 81, 106, 108, 112, 137, 147], "exogen": [59, 85, 86, 106, 153], "exp": [17, 18, 19, 31, 32, 34, 35, 39, 42, 56, 63, 73, 74, 75, 78, 79, 87, 88, 96], "expect": [31, 39, 47, 51, 57, 61, 62, 64, 65, 67, 71, 80, 82, 83, 90, 93, 94, 100, 101, 102, 104, 106, 120, 136, 137, 144, 150], "experi": [11, 33, 34, 56, 59, 73, 85, 91, 93, 96, 97, 102, 120, 150, 151], "experiment": [12, 14, 15, 16, 17, 18, 19, 121, 122, 123, 124, 125, 137, 139, 140, 141, 142], "expertis": 93, "explain": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 92, 137, 138, 146, 147], "explan": [58, 62, 84, 92, 137, 146, 148, 153], "explanatori": [93, 136], "explicit": 93, "explicitli": [64, 66, 67, 80, 153], "exploit": [56, 73, 96, 106, 153], "explor": 83, "exponenti": 136, "export": [83, 152], "expos": [97, 98, 100, 101, 102, 106, 112], "exposur": [7, 17, 19, 63, 64, 65, 66, 67], "express": [58, 77, 90, 137, 147], "ext": 16, "extend": [93, 98, 101, 102, 105, 148, 152], "extendend": [137, 147], "extens": [105, 121, 148, 151, 152], "extent": 77, "extern": [56, 73, 81, 83, 103, 137, 138, 152], "external_predict": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 73, 105], "externalptr": 59, "extra": 60, "extract": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 70, 83], "extralearn": 60, "extrem": [59, 85], "ey": 77, "ezequiel": 152, "f": [59, 60, 62, 63, 64, 67, 70, 71, 73, 76, 77, 82, 84, 85, 86, 88, 89, 92, 93, 94, 105, 137, 147, 148, 150], "f00584a57972": 60, "f1718fdeb9b0": 60, "f2e7": 60, "f3d24993": 60, "f6ebc": 87, "f_": [17, 18, 19, 31, 63, 104], "f_loc": [76, 89], "f_p": 63, "f_scale": [76, 89], "f_t": [64, 67], "f_x": 106, "face_color": 73, "facet_wrap": 59, "facilit": 83, "fact": [59, 85, 86], "factor": [45, 56, 57, 58, 59, 60, 73, 82, 96, 105, 137, 140, 142, 153], "faculti": 151, "fail": 152, "fair": 82, "fake": [55, 72, 91], "fall": 70, "fallback": 105, "fals": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 44, 45, 46, 47, 48, 50, 51, 56, 59, 60, 61, 62, 64, 67, 70, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 93, 94, 97, 102, 105, 106, 109, 120, 121, 122, 123, 124, 125, 136, 137, 139, 140, 141, 142, 153], "famili": [59, 85, 105], "familiar": 91, "fanci": 57, "far": [59, 85], "farbmach": 33, "fast": [82, 88, 105], "faster": 77, "fb5c25fa": 60, "fc9e": 60, "fd8a": 60, "featur": [10, 11, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 47, 49, 51, 57, 64, 67, 68, 70, 80, 81, 82, 85, 88, 100, 102, 104, 105, 106], "featureless": [60, 105], "features_bas": [59, 85, 86, 92], "features_flex": 59, "featureunion": 60, "februari": [64, 93], "feder": 65, "femal": [60, 68, 97, 102, 150], "fern\u00e1ndez": [34, 120, 151], "fetch": [59, 84, 85, 86, 97, 102], "fetch_401k": [59, 85, 86, 92, 153], "fetch_bonu": [60, 68, 97, 102, 150], "few": [59, 85, 86], "ff7f0e": 63, "field": [58, 84, 105, 153], "fifteenth": 151, "fifth": [58, 64, 67], "fig": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 63, 64, 65, 66, 67, 70, 71, 74, 75, 76, 77, 81, 82, 83, 86, 87, 89, 90, 93], "fig_al": 73, "fig_dml": 73, "fig_non_orth": 73, "fig_orth_nosplit": 73, "fig_po_al": 73, "fig_po_dml": 73, "fig_po_nosplit": 73, "figsiz": [13, 16, 63, 64, 67, 68, 70, 71, 74, 75, 76, 78, 79, 81, 82, 83, 84, 85, 86, 87, 89, 90], "figur": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 42, 56, 58, 63, 64, 65, 67, 68, 70, 71, 73, 74, 75, 76, 77, 78, 79, 81, 83, 84, 85, 86, 89, 93, 96], "figure_format": 87, "file": [10, 11, 65, 77, 87, 151, 152], "filenam": 56, "fill": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 56, 58, 59, 61, 62, 82, 85, 94], "fill_between": [74, 75, 76, 81, 86, 89], "fill_valu": 82, "fillna": 64, "filter": 60, "filterwarn": [70, 73, 82], "final": [56, 60, 61, 63, 64, 66, 67, 71, 73, 74, 75, 76, 78, 79, 80, 86, 89, 90, 94, 96, 106, 119, 121, 123, 125, 153], "final_estim": 90, "financi": [10, 92, 153], "find": [59, 62, 63, 70, 85, 93, 104, 105, 153], "finish": 60, "finit": [56, 59], "firm": [58, 84, 92], "firmid": 84, "first": [7, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 41, 44, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 71, 72, 73, 74, 75, 78, 79, 80, 81, 82, 84, 85, 86, 88, 89, 90, 93, 94, 96, 104, 106, 107, 110, 112, 120, 136, 137, 143, 149, 150, 152, 153], "fit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 46, 47, 48, 49, 50, 51, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 103, 104, 105, 106, 107, 108, 109, 111, 112, 121, 124, 135, 136, 137, 138, 143, 148, 152, 153], "fit_arg": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38], "fit_transform": [81, 84, 85], "five": 84, "fix": [63, 64, 67, 82, 152], "flag": [18, 97, 102, 120, 149], "flake8": 152, "flamlclassifierdoubleml": 83, "flamlregressordoubleml": 83, "flatten": [83, 87], "flexibl": [44, 55, 57, 59, 60, 62, 72, 85, 106, 148, 151, 152, 153], "flexibli": [59, 65, 85, 92], "float": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 44, 45, 46, 47, 48, 50, 51, 64, 65, 66, 67, 99, 102], "float32": [85, 86, 92], "float64": [64, 65, 66, 67, 68, 70, 71, 76, 78, 79, 80, 84, 85, 92, 97, 99, 102, 105, 150], "floor": 60, "floor_divid": 84, "flt": 60, "flush": 56, "fmt": [63, 70, 71, 78, 79, 83, 85, 87, 90], "fobj": 85, "focu": [58, 59, 65, 81, 84, 85, 86, 93, 104, 106, 108, 112, 119, 153], "focus": [86, 92, 93, 153], "fold": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 55, 58, 59, 60, 61, 62, 64, 65, 66, 67, 82, 84, 85, 86, 92, 94, 95, 103, 105, 106, 108, 109, 111, 121, 124, 136, 150, 153], "follow": [17, 18, 19, 31, 32, 35, 39, 56, 58, 59, 61, 62, 63, 64, 65, 66, 67, 73, 74, 75, 76, 78, 79, 83, 84, 85, 86, 89, 90, 91, 92, 93, 94, 96, 97, 102, 104, 105, 106, 107, 109, 110, 111, 112, 120, 121, 123, 125, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 153], "font_scal": [84, 85, 86], "fontsiz": [64, 67, 76, 86, 89], "force_all_d_finit": [5, 6, 7, 8, 9, 97, 98, 102], "force_all_x_finit": [4, 5, 6, 7, 8, 9, 97, 102], "forest": [33, 55, 56, 57, 59, 60, 62, 70, 72, 73, 80, 82, 85, 92, 96, 105, 150, 153], "forest_summari": 85, "forg": [149, 151, 152], "form": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 48, 50, 51, 59, 61, 62, 63, 74, 75, 76, 78, 79, 80, 82, 85, 89, 90, 92, 94, 104, 106, 107, 108, 112, 115, 116, 117, 118, 121, 123, 125, 126, 129, 137, 138, 143, 144, 145, 146, 147, 149, 150], "format": [7, 16, 65, 70, 73, 80, 137, 143, 152], "former": [65, 91], "formula": [58, 59, 84, 85, 90, 93, 152], "formula_flex": 59, "forschungsgemeinschaft": 148, "forthcom": [93, 151], "forum": 152, "forward": [26, 49], "found": [66, 74, 75, 77, 78, 79, 83, 96, 97, 102, 105, 106, 119, 150], "foundat": [70, 148, 151], "four": [59, 70, 82, 85, 106, 109, 152], "fourth": [58, 84], "frac": [17, 18, 19, 21, 25, 31, 33, 34, 36, 39, 40, 42, 43, 47, 51, 56, 58, 60, 63, 67, 73, 77, 80, 84, 87, 90, 95, 96, 104, 106, 109, 110, 111, 115, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145, 146, 147], "fraction": [19, 60], "frame": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 48, 55, 56, 58, 59, 61, 64, 65, 66, 67, 68, 70, 71, 74, 75, 78, 79, 80, 84, 85, 86, 87, 88, 92, 96, 97, 99, 102, 150, 153], "framealpha": [64, 67], "frameon": [64, 67], "framework": [13, 16, 21, 56, 58, 60, 70, 73, 82, 83, 84, 87, 93, 96, 105, 136, 148, 150, 152, 153], "freez": 149, "fribourg": 151, "friendli": [64, 67, 70, 71], "from": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 51, 55, 56, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 104, 105, 106, 107, 108, 109, 110, 111, 112, 120, 121, 136, 137, 143, 150, 152, 153], "from_arrai": [4, 5, 6, 7, 8, 9, 30, 44, 62, 63, 73, 76, 89, 96, 97, 98, 100, 101, 102, 105, 136, 150], "from_product": 84, "front": 71, "fr\u00e9chet": [137, 147], "fs_kernel": [44, 106], "fs_specif": [44, 106], "fsize": [59, 85, 86, 92, 153], "full": [62, 63, 70, 71, 73, 76, 78, 79, 82, 85, 86, 89, 90, 91, 94, 96, 106], "fulli": [26, 59, 69, 83, 85, 91, 106, 116], "fun": 56, "func": 57, "function": [0, 4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 42, 43, 44, 55, 56, 59, 60, 61, 62, 64, 65, 66, 67, 70, 72, 73, 74, 75, 76, 81, 82, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 103, 105, 106, 107, 108, 109, 110, 111, 112, 115, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 147, 148, 151, 152, 153], "fund": [59, 85, 86, 148], "further": [13, 16, 17, 18, 19, 31, 32, 35, 39, 41, 58, 60, 61, 62, 63, 64, 65, 66, 67, 71, 74, 75, 76, 80, 81, 82, 84, 86, 88, 89, 90, 92, 93, 94, 105, 106, 108, 111, 119, 121, 127, 130, 131, 134, 135, 136, 137, 138, 143, 146, 147, 148, 150, 152, 153], "furthermor": [73, 99, 102, 121, 126, 129], "futur": [4, 64, 65, 66, 67, 106, 121, 137], "futurewarn": [64, 65, 66, 67, 70], "fuzzi": [44, 45], "g": [6, 7, 12, 14, 15, 16, 17, 19, 22, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 44, 46, 47, 50, 51, 56, 57, 60, 62, 63, 64, 65, 66, 67, 68, 73, 74, 75, 77, 80, 82, 86, 87, 88, 91, 92, 94, 96, 99, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 121, 122, 123, 124, 125, 126, 128, 129, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 153], "g0": 70, "g1": 70, "g_": [45, 71, 106, 111, 121, 122, 124, 125, 127, 130, 131, 136], "g_0": [12, 14, 15, 16, 22, 25, 26, 28, 37, 38, 42, 43, 44, 45, 56, 58, 59, 70, 73, 82, 84, 85, 96, 104, 105, 106, 113, 114, 115, 116, 117, 118, 121, 123, 125, 126, 133, 134, 135, 137, 144, 145, 147, 150, 153], "g_1": [45, 82], "g_all": [56, 59], "g_all_po": 56, "g_ci": 59, "g_d": [121, 127, 131], "g_dml": 56, "g_dml_po": 56, "g_hat": [37, 38, 56, 73, 121], "g_hat0": [25, 26], "g_hat1": [25, 26], "g_i": [17, 19, 106, 109, 111, 112, 121, 123, 125], "g_k": 104, "g_nonorth": 56, "g_nosplit": 56, "g_nosplit_po": 56, "g_valu": 14, "g_x": 63, "gain": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 52, 82, 137, 138, 145, 152], "gain_statist": 152, "galleri": [96, 104, 105, 106, 107, 109, 111, 119, 148, 152], "gama": 83, "gamma": [36, 40, 43, 58, 84, 87, 88, 90, 93, 106, 121, 127, 130], "gamma_0": [32, 61, 88, 94, 121, 127, 130], "gamma_a": [31, 39, 93], "gamma_bench": 93, "gamma_v": 93, "gap": [84, 93], "gapo": 22, "gate": [22, 26, 38, 48, 87, 88, 103, 152], "gate_obj": 104, "gatet": 104, "gaussian": [27, 28, 29, 56, 73, 96, 104, 105, 136, 151], "ge": [18, 31, 32, 80, 88, 104, 106, 110, 112], "geer": 151, "gelbach": [58, 84], "gener": [0, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 47, 51, 55, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 97, 102, 103, 104, 105, 106, 109, 111, 112, 113, 120, 121, 123, 125, 126, 129, 136, 138, 139, 140, 141, 142, 144, 145, 147, 151, 152, 153], "generate_treat": 89, "generate_weakiv_data": 91, "geom_bar": 59, "geom_dens": [59, 61], "geom_errorbar": 59, "geom_funct": 56, "geom_histogram": 56, "geom_hlin": 59, "geom_point": 59, "geom_til": 58, "geom_vlin": [56, 61], "geq": [17, 19, 90, 106], "german": 148, "get": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 50, 51, 60, 64, 66, 67, 70, 71, 82, 87, 92, 93, 137, 138, 148, 149], "get_dummi": 87, "get_feature_names_out": [81, 84, 85], "get_legend_handles_label": [70, 71], "get_level_valu": 83, "get_logg": [56, 57, 58, 59, 60, 61, 95, 105, 106, 120, 121, 136, 150], "get_metadata_rout": [46, 47, 50, 51], "get_param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 50, 51, 83, 105], "get_ylim": 81, "ggdid": 57, "ggplot": [56, 58, 59, 61], "ggplot2": [56, 58, 59, 61], "ggsave": 56, "ggtitl": 59, "gh": 152, "git": 149, "github": [57, 59, 65, 77, 83, 87, 148, 151, 152], "githubusercont": [66, 77], "give": [59, 81, 85], "given": [12, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 34, 37, 38, 39, 42, 43, 44, 45, 46, 47, 50, 51, 56, 58, 61, 63, 65, 66, 71, 73, 78, 79, 84, 86, 87, 90, 93, 94, 96, 104, 106, 109, 110, 111, 121, 126, 136, 137, 143, 144, 145, 146, 147, 150, 152], "glmnet": [59, 60, 105, 152], "global": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 50, 51, 105, 106], "globalclassifi": 90, "globallearn": 90, "globalregressor": 90, "glrn": 60, "glrn_lasso": 60, "gmm": 91, "gname": 57, "go": [74, 75, 77, 81, 83, 90, 93], "goal": [71, 78, 79, 106], "goe": 106, "goldman": 151, "good": [77, 137, 138, 153], "gpu": 70, "gradient": [59, 70, 85], "gradientboostingclassifi": 82, "gradientboostingregressor": 82, "gradual": 93, "gramfort": [148, 150], "graph": [60, 61, 94, 153], "graph_ensemble_classif": 60, "graph_ensemble_regr": 60, "graph_obj": 90, "graph_object": [74, 75, 77, 93], "graphlearn": [60, 105], "grasp": [71, 137, 138], "great": [63, 153], "greater": 153, "green": [56, 74, 75, 76, 89], "greg": 151, "grei": [59, 70, 71], "grenand": 151, "grey50": 58, "grid": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 58, 60, 64, 67, 70, 71, 74, 75, 76, 77, 81, 86, 87, 89, 93, 105, 137, 143], "grid_arrai": [74, 75], "grid_basi": 81, "grid_bound": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 93], "grid_search": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 60, 105], "grid_siz": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 74, 75], "gridextra": 58, "gridsearchcv": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 105], "grisel": [148, 150], "grob": 58, "group": [7, 14, 16, 17, 19, 22, 26, 38, 55, 57, 65, 70, 71, 72, 80, 81, 86, 87, 88, 93, 103, 106, 107, 109, 110, 111, 112, 121, 123, 125, 137, 140, 142], "group_0": 104, "group_1": [78, 79, 104], "group_2": [78, 79, 104], "group_3": [78, 79], "group_effect": 88, "group_ind": 80, "group_treat": 80, "groupbi": [64, 67, 70, 77, 85, 91], "gruber": 33, "gt": [55, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 70, 71, 72, 74, 75, 76, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 97, 102, 150], "gt_combin": [16, 64, 65, 67, 106, 107, 109, 110, 111], "gt_dict": [64, 67], "guarante": [58, 84], "guber": 33, "guess": [92, 137, 138], "guid": [20, 21, 46, 47, 50, 51, 56, 57, 58, 60, 63, 64, 65, 66, 67, 71, 73, 80, 81, 84, 90, 92, 105, 148, 150, 152], "guidelin": 152, "gunion": [60, 105], "gxidclusterperiodytreat": 57, "h": [17, 18, 19, 31, 33, 39, 41, 57, 58, 84, 90, 91, 99, 102, 106, 151], "h20": 83, "h_0": [64, 67, 71, 80, 81, 92, 93, 137, 143, 153], "h_f": [44, 106], "ha": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 48, 49, 50, 51, 56, 57, 58, 59, 65, 67, 73, 77, 82, 83, 84, 85, 86, 87, 90, 91, 92, 93, 97, 102, 104, 105, 106, 108, 112, 137, 138, 143, 144, 145, 146, 147, 148, 153], "had": 65, "half": [56, 73, 87, 96, 120], "hand": [44, 82, 83, 87, 91, 153], "handbook": 87, "handl": [57, 65, 70, 71, 82, 98, 99, 102, 105, 152], "hansen": [10, 11, 34, 40, 42, 58, 77, 84, 91, 96, 148, 151], "happend": 82, "hard": [92, 137, 138], "harold": 151, "harsh": [46, 50], "hasn": [13, 16, 64, 66, 67], "hat": [56, 58, 73, 77, 80, 84, 87, 90, 95, 96, 104, 106, 120, 121, 136, 137, 138, 143, 146], "have": [16, 22, 23, 26, 29, 32, 35, 38, 48, 49, 50, 51, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 74, 75, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 97, 102, 104, 105, 106, 110, 121, 123, 125, 136, 137, 138, 144, 147, 149, 150, 152, 153], "hazlett": [93, 137, 138], "hc": [57, 151], "hc0": [48, 152], "hdm": [58, 84], "he": [61, 94], "head": [57, 58, 60, 64, 65, 66, 67, 68, 74, 75, 78, 79, 81, 83, 84, 85, 87, 90, 93, 97, 98, 102, 104, 150], "heat": [58, 84], "heatmap": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 58, 84, 93], "heavili": 82, "hei": 151, "height": [13, 16, 56, 58, 77, 83], "help": [57, 59, 65, 76, 82, 86, 88, 93, 106, 107, 120, 153], "helper": [98, 102, 152], "henc": [57, 59, 60, 85, 93, 105, 121, 153], "here": [27, 28, 29, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 74, 75, 76, 78, 79, 80, 82, 84, 85, 86, 88, 89, 90, 93, 94, 97, 102, 105, 106, 109, 149], "heterogen": [17, 19, 26, 32, 59, 80, 85, 86, 88, 103, 106, 116, 120, 151, 152, 153], "heteroskedast": [78, 79], "heurist": [56, 73, 96], "high": [34, 37, 38, 59, 63, 77, 85, 86, 91, 95, 106, 117, 118, 136, 148, 150, 151], "higher": [57, 59, 77, 85, 86, 87, 90, 91, 152, 153], "highli": [59, 85, 148], "highlight": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 62, 81, 83, 93, 152], "highlightcolor": [74, 75], "hint": 83, "hispan": 68, "hist": [70, 71], "hist_e401": 59, "hist_p401": 59, "histogram": [70, 71], "histplot": 73, "hjust": 59, "hline": [97, 102, 136, 150, 153], "hold": [36, 58, 59, 61, 83, 84, 85, 94, 104, 105, 106, 110], "holdout": [105, 120], "holm": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38], "home": [59, 64, 66, 67, 85, 93], "homogen": 106, "hook": 152, "hopefulli": 86, "horizont": [58, 63, 70, 84], "hostedtoolcach": [64, 65, 66, 67, 84, 85], "hot": 87, "hotstart_backward": [60, 105], "hotstart_forward": [60, 105], "household": [59, 85, 86, 92], "how": [13, 17, 19, 46, 47, 50, 51, 55, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 96, 98, 102, 105, 106, 148, 149], "howev": [56, 59, 61, 73, 83, 85, 90, 93, 94, 96, 106, 153], "hown": [59, 85, 86, 92, 153], "hpwt": [58, 84], "hpwt0": 58, "hpwtairmpdspac": 58, "href": 148, "hspace": 82, "hstack": [30, 63], "html": [60, 148, 150, 152], "http": [33, 43, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 105, 148, 149, 150, 152], "huber": [36, 61, 94, 106, 119, 121, 134, 135, 151], "hue": [64, 67, 85], "huge": 82, "hugo": 151, "husd": [60, 68, 97, 102, 150], "hyperparamet": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 59, 60, 68, 70, 77, 82, 83, 85, 103, 150], "hypothes": [136, 151], "hypothesi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 59, 85, 92, 137, 143, 151], "hypothet": 93, "i": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 135, 136, 137, 138, 140, 142, 143, 144, 145, 147, 148, 149, 150, 152, 153], "i0": [62, 63, 106, 108], "i03": 148, "i1": [62, 106, 108], "i_": [40, 84, 88], "i_1": [58, 84], "i_2": [58, 84], "i_3": [58, 84], "i_4": 63, "i_est": 73, "i_fold": 58, "i_k": [58, 84, 95, 120, 136], "i_learn": 82, "i_level": 71, "i_rep": [56, 61, 62, 73, 82, 94, 96], "i_split": 84, "i_train": 73, "icp": 151, "id": [7, 16, 57, 58, 60, 64, 65, 66, 67, 84, 99, 102, 106, 107, 109, 111, 137, 142], "id_col": [7, 16, 64, 65, 66, 67, 99, 102, 106, 107, 109, 111], "id_var": 84, "idea": [59, 60, 85, 86, 93, 105, 106, 137, 138, 153], "ident": [17, 18, 19, 31, 32, 35, 39, 40, 49, 60, 65, 71, 81, 83, 90, 105, 106, 112, 121, 129, 137, 143], "identfi": 93, "identif": [64, 65, 66, 67, 90, 91, 106, 153], "identifi": [7, 58, 59, 62, 65, 70, 80, 84, 85, 86, 90, 93, 98, 99, 100, 101, 102, 104, 106, 108, 110, 112, 119, 137, 147, 152], "identifii": 104, "idnam": 57, "idx_gt_att": 16, "idx_learn": 70, "idx_tau": [76, 86, 89], "idx_treat": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 70, 71, 137, 143], "ieee": 151, "ifels": 57, "ignor": [46, 47, 50, 51, 65, 70, 73, 82, 90], "ignore_index": 70, "ii": [58, 84], "iid": [64, 67, 106, 108], "iivm": [20, 21, 25, 33, 86, 95, 104, 115, 128, 148, 152], "iivm_summari": 85, "iivmglmnet": 59, "iivmrang": 59, "iivmrpart": 59, "iivmxgboost11861": 59, "ij": [41, 58, 61, 71, 84, 94], "ilia": 151, "illustr": [56, 58, 59, 60, 61, 62, 63, 65, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 84, 85, 86, 88, 89, 92, 93, 94, 96, 105, 153], "iloc": [62, 63, 64, 65, 66, 67, 70, 71, 82, 84, 87, 91], "immedi": 149, "immun": [120, 151], "impact": [55, 72, 82, 87, 92], "implement": [12, 14, 15, 16, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 50, 51, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 73, 77, 81, 82, 84, 85, 87, 90, 92, 93, 94, 96, 103, 104, 105, 107, 108, 110, 112, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 146, 148, 150, 151, 152, 153], "impli": [31, 39, 58, 59, 64, 67, 84, 85, 86, 90, 104, 106, 112, 137, 139, 140, 141, 142, 144, 145], "implicitli": [106, 110], "implment": [63, 106, 107], "import": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 137, 143, 149, 150, 152, 153], "importlib": 77, "impos": 93, "improv": [62, 64, 67, 82, 88, 106, 152], "in_sample_norm": [12, 14, 15, 16, 62, 121, 122, 123, 124, 125, 137, 139, 140, 141, 142], "inbuild": 82, "inbuilt": 82, "inc": [59, 85, 86, 92, 153], "includ": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 52, 57, 59, 63, 64, 65, 66, 67, 70, 71, 78, 79, 81, 85, 90, 92, 93, 104, 106, 136, 137, 143, 144, 145, 147, 149, 152, 153], "include_bia": [81, 84, 85], "include_never_tr": [17, 19, 67], "include_scenario": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 93], "incom": [59, 85, 86, 88, 92, 153], "incorpor": [60, 92, 137, 143], "increas": [19, 64, 67, 80, 82, 84, 93, 153], "increment": 152, "ind": 85, "independ": [12, 14, 15, 16, 18, 31, 32, 39, 45, 58, 60, 63, 80, 84, 88, 106, 112, 119, 121, 122, 123, 124, 125, 152], "index": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 56, 58, 63, 68, 70, 73, 77, 78, 79, 83, 84, 85, 87, 88, 96, 97, 102, 120, 121, 122, 123, 124, 125, 148, 150], "index_col": 77, "india": [120, 151], "indic": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 36, 37, 38, 44, 48, 58, 59, 61, 63, 64, 65, 66, 67, 80, 84, 85, 86, 90, 91, 93, 94, 95, 97, 98, 101, 102, 104, 106, 108, 112, 113, 119, 120], "individu": [17, 19, 22, 26, 50, 51, 57, 59, 63, 64, 67, 70, 71, 78, 79, 80, 83, 85, 86, 90, 92, 104, 106, 153], "individual_df": 63, "induc": [103, 120], "industri": [58, 84], "inf": [5, 6, 7, 8, 9, 57, 64, 65, 66, 67, 91], "inf_model": 121, "infer": [34, 40, 55, 56, 58, 65, 70, 72, 73, 77, 82, 83, 84, 91, 96, 103, 106, 120, 148, 150, 151, 152], "inferenti": 153, "infinit": [5, 6, 7, 8, 9, 91, 97, 98, 102, 152], "influenc": [25, 47, 51, 70, 106], "info": [55, 60, 64, 65, 66, 67, 68, 70, 71, 80, 83, 84, 85, 86, 92, 97, 99, 102, 150, 152, 153], "inform": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 46, 47, 48, 50, 51, 55, 60, 64, 65, 66, 67, 72, 74, 75, 82, 90, 91, 92, 93, 106, 107, 137, 138, 151], "infti": [56, 73, 96, 106, 112], "inher": 93, "inherit": [87, 98, 99, 100, 101, 102, 152], "initi": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 59, 60, 61, 62, 64, 65, 66, 67, 70, 76, 85, 86, 89, 90, 92, 93, 94, 97, 99, 102, 104, 105, 106, 120, 150, 152, 153], "inlin": [68, 87], "inlinebackend": 87, "inner": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 105], "innermost": 105, "input": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 47, 51, 60, 65, 92, 95, 106, 110, 136, 137, 138, 143], "insensit": 106, "insid": [46, 47, 50, 51], "insight": [77, 93], "insignific": 92, "inspect": 150, "inspir": [31, 33, 34, 36, 64, 67, 93], "instal": [59, 70, 83, 90, 106, 152], "install_github": 149, "instanc": [50, 51, 59, 60, 70, 85, 105], "instanti": [58, 59, 84, 85, 105, 120], "instead": [4, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 50, 51, 55, 57, 59, 64, 65, 66, 67, 70, 71, 72, 80, 83, 85, 86, 97, 102, 104, 105, 106, 121, 125, 137, 139, 140, 141, 142, 145, 146, 152], "instruct": [70, 149, 152], "instrument": [5, 6, 7, 8, 9, 10, 25, 33, 37, 40, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 70, 71, 80, 84, 85, 86, 89, 92, 94, 97, 98, 99, 100, 101, 102, 105, 106, 108, 109, 111, 115, 117, 121, 130, 136, 150, 153], "instrument_effect": 55, "instrument_impact": 72, "instrument_strength": 91, "insuffienct": 83, "int": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 44, 45, 48, 49, 57, 58, 59, 62, 65, 70, 72, 76, 88, 89, 91, 93, 94, 99, 102], "int32": 65, "int64": [64, 66, 67, 68, 84, 97, 99, 102, 150], "int8": [85, 86, 92], "integ": [18, 60, 105], "integr": [70, 83, 93, 137, 147, 152], "intend": [44, 60, 93, 153], "intent": [106, 153], "inter": 105, "interact": [22, 23, 25, 26, 31, 33, 34, 35, 44, 45, 71, 93, 103, 105, 115, 116, 144, 145, 148, 152, 153], "interchang": 136, "interest": [25, 26, 31, 37, 38, 39, 56, 59, 61, 62, 73, 77, 85, 86, 90, 91, 94, 96, 104, 106, 108, 110, 113, 114, 115, 116, 117, 118, 119, 121, 136, 150, 153], "interfac": [57, 59, 60, 70, 97, 102, 105, 120, 150], "intermedi": 93, "intern": [19, 57, 59, 60, 71, 83, 86, 105, 106, 107, 151], "internet": [59, 85, 86], "interpret": [64, 65, 66, 67, 78, 79, 91, 93, 104, 137, 138, 144, 145, 146, 147, 149, 153], "intersect": [93, 137, 143, 152], "interv": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 48, 57, 58, 59, 61, 62, 64, 65, 66, 67, 70, 71, 74, 75, 76, 78, 79, 81, 84, 86, 89, 90, 92, 94, 103, 104, 120, 121, 137, 143, 150, 151, 152, 153], "intial": 90, "introduc": [56, 73, 96, 97, 102, 136, 152, 153], "introduct": [56, 58, 60, 73, 84, 86, 92, 105, 106, 108, 112, 137, 138], "introductori": [57, 93], "intrument": [61, 94], "intspecifi": 44, "intuit": 93, "inuidur1": [60, 68, 97, 102, 150], "inuidur1femaleblackothracedep1dep2q2q3q4q5q6agelt35agegt54durablelusdhusdtg": [60, 97, 102, 150], "inuidur2": [68, 97, 102, 150], "inv_sigmoid": 87, "invalid": [56, 67, 73, 84, 96], "invari": [106, 108, 112], "invers": [22, 24, 25, 26, 27, 28, 29, 30, 61, 94, 137, 144, 145], "invert": [25, 91], "invert_yaxi": 84, "investig": [77, 83, 93], "involv": [104, 105, 121, 153], "io": [87, 152], "ipw_norm": 152, "ipykernel_49813": 84, "ipynb": [55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94], "ira": [59, 85, 86], "irm": [0, 9, 12, 14, 15, 20, 21, 37, 38, 48, 49, 71, 75, 79, 80, 82, 93, 94, 95, 101, 102, 103, 105, 109, 111, 116, 129, 144, 145, 148, 152, 153], "irm_summari": 85, "irmglmnet": 59, "irmrang": 59, "irmrpart": 59, "irmxgboost8047": 59, "irrespect": 93, "irrevers": [106, 112], "is_classifi": [12, 14, 15, 22, 25, 26, 38], "is_gat": [22, 26, 38, 48], "isfinit": [64, 65, 66, 67], "isnan": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 82, 105], "isoton": 93, "isotonicregress": 93, "issn": 77, "issu": [13, 16, 93, 148, 151, 152], "ite": [64, 67, 70, 71, 78, 79, 80], "ite_lower_quantil": [64, 67], "ite_mean": [64, 67], "ite_upper_quantil": [64, 67], "item": [25, 70, 85, 95, 105, 120], "iter": [44, 55, 61, 62, 84, 85, 90, 94, 105, 136, 153], "itertool": 77, "its": [46, 47, 70, 93, 95, 104, 105, 106, 108, 109, 111, 120, 121, 136], "iv": [25, 33, 37, 38, 40, 41, 56, 58, 73, 84, 96, 97, 102, 115, 117, 132, 133, 137, 146, 148, 152, 153], "iv_2": 55, "iv_var": [58, 84], "iv\u00e1n": [120, 151], "j": [10, 11, 17, 18, 19, 31, 33, 34, 36, 39, 40, 41, 42, 43, 56, 57, 58, 60, 61, 71, 73, 77, 84, 87, 91, 94, 96, 105, 106, 113, 136, 148, 150], "j_": [58, 84], "j_0": 136, "j_1": [58, 84], "j_2": [58, 84], "j_3": [58, 84], "j_k": [58, 84], "jame": 151, "janari": [64, 67], "janni": [59, 85], "januari": 64, "jasenakova": 152, "javanmard": 151, "jbe": [58, 84], "jeconom": [17, 18, 19, 31, 39, 57], "jerzi": 151, "jia": 93, "jitter": [16, 70], "jitter_strength": 70, "jitter_valu": 16, "jk": [106, 114], "jmlr": [60, 148, 150, 152], "job": [59, 85, 86], "john": 151, "joint": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 48, 64, 65, 66, 67, 71, 74, 75, 76, 78, 79, 81, 86, 89, 91, 106, 108, 136, 152, 153], "jointli": [89, 104], "jonathan": 151, "joss": [60, 105, 148, 150], "journal": [10, 11, 17, 18, 19, 31, 36, 39, 41, 42, 57, 58, 60, 77, 84, 87, 91, 93, 96, 105, 148, 150, 151, 152], "jss": 148, "jump": [88, 90, 106], "jun": [57, 151], "jupyt": [55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94], "juraj": 151, "just": [57, 60, 62, 63, 64, 67, 71, 76, 78, 79, 80, 81, 88, 89, 106, 121, 122, 123, 124, 125, 137, 138], "justif": [120, 137, 138], "k": [10, 17, 18, 19, 33, 34, 36, 39, 40, 41, 42, 56, 58, 60, 73, 82, 83, 84, 90, 91, 95, 96, 103, 104, 106, 136, 153], "k_h": [90, 106], "kaggl": [59, 85], "kallu": [76, 86, 89, 91, 92, 121, 127, 130, 131, 151], "kappa": 106, "kato": [41, 58, 84, 136, 151], "kb": [65, 66, 70, 71, 80, 84, 85, 86, 92, 97, 99, 102, 150], "kde": [27, 28, 29, 85], "kdeplot": [62, 82, 94], "kdeunivari": [27, 28, 29], "kecsk\u00e9sov\u00e1": 152, "keel": 91, "keep": [47, 51, 57, 81, 93, 101, 102, 153], "kei": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 44, 49, 58, 59, 74, 75, 78, 79, 83, 84, 85, 86, 90, 93, 105, 106, 121, 137, 143, 152], "keith": 151, "kelz": 91, "kengo": 151, "kennedi": 91, "kept": [100, 102], "kernel": [27, 28, 29, 44, 47, 51, 90, 106], "kernel_regress": 90, "kernelreg": 90, "keyword": [17, 18, 19, 22, 26, 38, 41, 42, 43, 45, 48], "kf": 120, "kfold": [84, 120], "kind": [55, 72, 85], "kj": [17, 18, 19, 33, 34, 36, 39, 40, 41, 42, 56, 58, 73, 84, 96], "klaassen": [33, 77, 82, 83, 93, 148, 151], "klaa\u00dfen": 33, "knau": 151, "know": [62, 88, 91], "knowledg": [55, 72, 82, 87, 88], "known": [80, 82, 90, 91, 93, 105, 106, 112], "kohei": 151, "kotthof": 60, "kotthoff": [60, 105, 148, 150], "krueger": 87, "kueck": [59, 85], "kurz": [148, 151, 152], "kwarg": [17, 18, 19, 22, 26, 31, 35, 38, 39, 41, 42, 43, 44, 45, 46, 48, 83, 106], "l": [58, 60, 61, 68, 74, 75, 84, 91, 93, 94, 105, 137, 146, 148, 150], "l1": [85, 94, 106], "l_hat": [37, 38, 56, 73, 121], "lab": 61, "label": [13, 16, 46, 50, 63, 70, 71, 73, 74, 75, 76, 78, 79, 81, 83, 86, 87, 89, 90], "labor": 87, "laffer": 151, "laff\u00e9r": [36, 61, 94, 106, 119, 121, 134, 135], "lal": [87, 152], "lambda": [58, 59, 60, 61, 64, 67, 85, 87, 88, 105, 106, 121, 122, 123, 136, 150], "lambda_": 77, "lambda_0": [121, 122, 123], "lambda_t": [18, 19, 67], "land": 88, "lang": [60, 105, 148, 150], "langl": [32, 88], "lanni": 91, "lappli": 120, "larg": [56, 73, 80, 82, 83, 87, 93, 106], "larger": [26, 57, 90, 93, 137, 143], "largest": 82, "largli": 82, "lasso": [58, 59, 60, 61, 65, 85, 94, 105, 150, 151], "lasso_class": [59, 85], "lasso_pip": [60, 105], "lasso_summari": 85, "lassocv": [30, 65, 77, 84, 85, 94, 105, 106, 136, 150], "last": [18, 60, 149], "late": [25, 55, 59, 85, 91, 106, 115, 121, 128], "latent": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 92, 137, 146, 147], "later": [59, 60, 90, 93, 105, 153], "latest": 148, "latter": [50, 51, 91, 106], "layout": 77, "lbrace": [25, 26, 33, 34, 36, 58, 84, 95, 106, 113, 115, 116, 120, 121, 126, 136, 137, 144], "ldot": [37, 38, 58, 61, 84, 94, 95, 106, 117, 118, 120, 136, 150], "le": [18, 62, 88, 104, 106, 108, 121, 130, 131], "lead": [57, 93, 106], "leadsto": 136, "lear": [60, 105, 148, 150], "learn": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 41, 42, 43, 44, 55, 59, 60, 65, 68, 71, 72, 76, 77, 81, 82, 83, 85, 86, 87, 89, 90, 91, 93, 97, 102, 103, 105, 120, 121, 136, 137, 138, 152, 153], "learner": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 55, 56, 57, 58, 59, 61, 62, 64, 65, 66, 67, 70, 73, 74, 75, 77, 84, 85, 86, 92, 93, 94, 95, 96, 103, 106, 108, 109, 111, 120, 121, 136, 137, 143, 152, 153], "learner_class": [30, 152], "learner_cv": 60, "learner_dict": 70, "learner_forest_classif": 60, "learner_forest_regr": 60, "learner_l": 92, "learner_lasso": 60, "learner_list": 82, "learner_m": 92, "learner_nam": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 70, 105], "learner_pair": 70, "learner_param_v": 60, "learner_rf": 136, "learnerclassif": 60, "learnerregr": 60, "learnerregrcvglmnet": 60, "learnerregrrang": [60, 105], "learning_r": [64, 67, 73, 76, 86, 89, 90, 93, 96], "least": [55, 59, 72, 85, 86, 92, 106, 110, 120], "leav": [61, 93, 94], "left": [17, 19, 33, 34, 36, 40, 41, 56, 58, 70, 71, 73, 82, 84, 85, 86, 87, 89, 90, 96, 106, 121, 122, 123, 124, 125, 136, 137, 139, 140, 141, 142, 144, 145], "legend": [59, 63, 64, 67, 70, 71, 73, 74, 75, 76, 78, 79, 81, 82, 86, 87, 89], "lemp": 65, "len": [70, 71, 76, 82, 83, 84, 86, 89, 91], "length": [12, 13, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 56, 60, 62, 64, 67, 91, 105], "leq": [58, 84], "less": [57, 59, 85, 86, 90, 93], "lester": 151, "let": [17, 18, 19, 31, 35, 39, 56, 57, 59, 60, 61, 62, 64, 67, 70, 71, 73, 76, 78, 79, 81, 82, 85, 86, 89, 93, 94, 95, 96, 105, 106, 108, 112, 119, 137, 138, 147, 153], "level": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 44, 48, 58, 59, 61, 62, 63, 64, 65, 66, 67, 70, 71, 74, 75, 76, 78, 79, 80, 81, 84, 85, 86, 89, 91, 92, 93, 94, 105, 113, 114, 121, 126, 137, 143, 144, 153], "level_0": [60, 84], "level_1": 84, "level_bound": [70, 71], "leverag": 70, "levi": 91, "levinsohn": [58, 84], "lewi": 151, "lgbm": 70, "lgbmclassifi": [62, 63, 64, 67, 70, 76, 82, 86, 89, 90, 93], "lgbmregressor": [62, 63, 64, 67, 70, 73, 76, 82, 86, 90, 93, 96], "lgr": [56, 57, 58, 59, 60, 61, 95, 105, 106, 120, 121, 136, 150], "lib": [64, 65, 66, 67, 70, 84, 85], "liblinear": [85, 94, 106], "librari": [55, 56, 57, 58, 59, 60, 61, 70, 95, 96, 97, 102, 105, 106, 120, 121, 136, 149, 150, 153], "licens": [148, 152], "lie": 151, "lightgbm": [62, 63, 64, 67, 70, 73, 76, 82, 86, 89, 90, 93], "like": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 46, 47, 50, 51, 57, 59, 60, 64, 65, 66, 67, 77, 85, 86, 93, 105, 106, 107, 120, 150, 153], "lim": 87, "lim_": [90, 106], "limegreen": [74, 75], "limit": [87, 106, 112, 151], "limits_": 104, "lin": [90, 93, 106], "line": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 63, 65, 70, 71, 93], "line_width": 70, "linear": [17, 19, 20, 21, 22, 26, 31, 35, 37, 38, 39, 40, 41, 42, 43, 48, 55, 56, 57, 58, 60, 62, 63, 65, 70, 71, 72, 73, 74, 75, 77, 78, 81, 82, 83, 84, 91, 92, 93, 95, 96, 103, 104, 105, 109, 110, 111, 117, 118, 120, 122, 123, 124, 125, 126, 128, 129, 132, 133, 136, 143, 145, 146, 147, 148, 150, 151, 152, 153], "linear_learn": [64, 67], "linear_model": [22, 26, 30, 38, 48, 64, 65, 66, 67, 68, 70, 71, 72, 77, 81, 82, 84, 85, 90, 91, 93, 94, 105, 106, 136, 150], "linearli": [90, 106], "linearregress": [55, 64, 65, 66, 67, 70, 71, 72, 81, 82, 90, 91, 93], "linearscoremixin": [0, 121], "lineplot": [64, 67, 70, 71], "linestyl": [63, 64, 67, 70, 71, 83, 90], "linetyp": 61, "linewidth": [63, 64, 67, 70], "link": [93, 152], "linspac": [74, 75, 81, 93], "lint": 152, "linux": 149, "list": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 47, 51, 56, 57, 58, 59, 60, 64, 65, 67, 73, 74, 75, 84, 86, 88, 96, 105, 120, 121, 149, 152], "list_confset": 25, "listedcolormap": 84, "literatur": [93, 106, 108, 112], "littl": [80, 91], "ll": [60, 136, 153], "lllllllllllllllll": [97, 102, 150], "lm": [55, 57, 93], "ln_alpha_ml_l": 77, "ln_alpha_ml_m": 77, "load": [55, 57, 59, 60, 77, 85, 86, 97, 102, 149, 150], "loader": 0, "loc": [63, 64, 65, 66, 67, 70, 71, 73, 76, 77, 78, 79, 84, 87, 89, 92, 93], "local": [25, 27, 70, 91, 104, 106, 115, 151, 152], "localconvert": 84, "locat": [76, 89, 106], "log": [58, 62, 64, 65, 66, 67, 70, 77, 82, 84, 86, 87, 92, 94, 105, 106, 108, 109, 111], "log_odd": 88, "log_p": [58, 84], "log_reg": [55, 57], "logarithm": [70, 77], "logic": [25, 60, 105], "logical_not": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 82, 105], "logist": [31, 45, 55, 57, 59, 61, 65, 70, 71, 72, 85, 91, 93, 94, 153], "logisticregress": [55, 64, 65, 66, 67, 68, 70, 71, 72, 81, 90, 91, 93], "logisticregressioncv": [30, 65, 82, 85, 94, 106], "logit": [82, 87], "loglik": 60, "logloss": [59, 70, 85, 153], "logloss_m": 70, "logo": 152, "logspac": 85, "long": [7, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 56, 65, 73, 82, 92, 93, 137, 138, 147, 151], "longer": [64, 67, 97, 102, 106, 110], "look": [57, 59, 60, 62, 63, 64, 65, 66, 67, 76, 82, 85, 86, 89, 90, 92], "loop": [70, 71, 91], "loss": [62, 64, 65, 66, 67, 70, 82, 83, 86, 90, 92, 94, 105, 106, 108, 109, 111], "loss_ml_g0": 82, "loss_ml_g1": 82, "loss_ml_m": 82, "low": [63, 80, 91, 104, 151], "lower": [25, 59, 60, 63, 64, 67, 70, 71, 76, 77, 80, 81, 86, 87, 89, 90, 91, 92, 93, 105, 137, 143, 147, 153], "lower_bound": [74, 75], "lpop": 65, "lpq": [27, 29, 86, 104, 130, 152], "lpq_0": 89, "lpq_1": 89, "lqte": 104, "lr": 90, "lrn": [55, 56, 57, 58, 59, 60, 61, 95, 105, 106, 120, 121, 136, 150, 153], "lrn_0": 60, "lt": [55, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 70, 71, 80, 84, 85, 86, 88, 92, 93, 97, 102, 150], "lucien": 152, "luka": 151, "luk\u00e1\u0161": 36, "lusd": [60, 68, 97, 102, 150], "lvert": 77, "m": [7, 10, 11, 16, 30, 31, 40, 41, 42, 56, 58, 60, 64, 67, 68, 70, 73, 77, 80, 82, 83, 84, 87, 91, 96, 99, 102, 103, 104, 105, 106, 107, 109, 111, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 139, 140, 141, 142, 144, 145, 146, 147, 148, 149, 150, 151, 152], "m_": [70, 71, 106, 109, 111, 113, 121, 123, 125, 126, 130, 136], "m_0": [12, 14, 15, 16, 22, 24, 25, 26, 28, 37, 38, 42, 43, 44, 56, 58, 59, 73, 77, 80, 83, 84, 85, 96, 104, 105, 106, 115, 116, 117, 118, 121, 122, 123, 124, 125, 127, 130, 131, 133, 134, 135, 150, 153], "m_hat": [25, 26, 37, 38, 56, 73, 81, 121], "m_i": [90, 106], "ma": [41, 58, 84, 91, 106, 107, 151], "mac": 149, "machin": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 41, 42, 43, 44, 55, 59, 60, 61, 62, 64, 65, 66, 67, 68, 71, 72, 76, 77, 81, 82, 83, 85, 86, 87, 89, 90, 91, 92, 93, 94, 103, 105, 106, 108, 109, 111, 120, 121, 136, 137, 138, 152, 153], "machineri": [77, 151], "mackei": 151, "maco": 149, "made": [106, 119, 153], "mae": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 82, 105], "maggi": 151, "magnitud": [137, 138], "mai": [47, 51, 61, 62, 94], "main": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 66, 70, 77, 86, 93, 106, 136, 137, 138, 151, 153], "mainli": 93, "maintain": [57, 148, 152], "mainten": 152, "major": [60, 93, 152], "make": [55, 70, 71, 72, 82, 83, 93, 104, 105, 152, 153], "make_confounded_irm_data": [93, 152], "make_confounded_plr_data": 92, "make_did_cs2021": [7, 16, 64, 99, 102, 106, 107, 111], "make_did_cs_cs2021": [67, 106, 109], "make_did_sz2020": [5, 12, 15, 62, 98, 102, 106, 108], "make_heterogeneous_data": [74, 75, 78, 79, 80], "make_iivm_data": [25, 27, 104, 106], "make_irm_data": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 81, 82, 104, 105, 106], "make_irm_data_discrete_treat": [70, 71], "make_pipelin": 85, "make_pliv_chs2015": [37, 106], "make_pliv_multiway_cluster_ckms2021": [58, 84], "make_plr_ccddhnr2018": [6, 7, 12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 56, 73, 83, 95, 96, 104, 105, 106, 120, 121, 136, 137, 143], "make_rdd_data": 8, "make_simple_rdd_data": [44, 90, 100, 102, 106], "make_spd_matrix": 43, "make_ssm_data": [9, 61, 94, 101, 102, 106], "malt": [148, 151], "man": [55, 72], "manag": [105, 149], "mandatori": [100, 102], "mani": [20, 21, 40, 56, 57, 58, 60, 62, 73, 83, 84, 96, 121, 136, 153], "manili": 48, "manipul": [59, 60, 90, 106], "manual": [59, 81, 83, 92, 153], "mao": 151, "map": [25, 46, 47, 50, 51, 57, 58, 84, 104, 106, 115], "mapsto": [95, 104], "mar": [36, 106], "march": [77, 82, 83], "margin": [74, 75, 93], "marit": [59, 85], "marker": [64, 67, 71, 93], "markers": 87, "market": 87, "markettwo": 58, "markov": [43, 151], "marr": [59, 85, 86, 92, 153], "marshal": 105, "martin": [36, 93, 148, 151, 152], "masatoshi": 151, "masip": [91, 152], "mask": 16, "maskedarrai": [106, 107], "master": [57, 65], "mat": 58, "match": [105, 137, 146], "math": [30, 64, 65, 66, 67], "mathbb": [17, 18, 19, 20, 21, 25, 26, 31, 35, 37, 38, 39, 58, 61, 62, 63, 64, 67, 70, 71, 80, 82, 83, 84, 87, 90, 94, 104, 106, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 133, 134, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 150, 153], "mathcal": [17, 18, 19, 31, 32, 33, 34, 35, 36, 39, 40, 41, 42, 43, 45, 56, 58, 61, 63, 73, 76, 84, 88, 89, 94, 96, 106, 107, 110, 112], "mathop": 104, "mathrm": [31, 39, 64, 65, 66, 67, 90, 106, 107, 109, 110, 111, 112, 121, 123, 125, 137, 140, 142], "matia": 151, "matplotlib": [13, 16, 62, 63, 64, 65, 66, 67, 68, 70, 71, 73, 74, 75, 76, 78, 79, 81, 82, 83, 84, 85, 86, 87, 89, 90, 93, 94], "matric": [88, 152], "matrix": [17, 18, 19, 31, 33, 34, 35, 36, 39, 40, 41, 42, 43, 47, 51, 56, 58, 59, 60, 61, 73, 84, 94, 96, 97, 102, 105, 136, 150, 152, 153], "matt": 151, "matter": [82, 87], "max": [17, 19, 59, 60, 70, 81, 85, 86, 91, 95, 104, 105, 106, 120, 121, 123, 125, 127, 136, 137, 140, 142, 150, 153], "max_depth": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 68, 85, 92, 95, 104, 105, 106, 108, 120, 121, 136, 137, 143, 150, 153], "max_featur": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 68, 85, 92, 95, 104, 105, 106, 120, 121, 136, 137, 143, 150, 153], "max_it": [70, 84, 85, 93], "maxim": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 88, 104, 106], "maxima": 136, "maximum": [104, 105], "mb": [64, 67, 68, 97, 102, 150], "mb706": 152, "mea": 33, "mean": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 46, 47, 50, 51, 55, 56, 58, 59, 62, 64, 67, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 89, 91, 92, 93, 96, 105, 106, 136, 153], "mean_absolute_error": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 82, 105], "meant": [65, 104, 152], "measir": 92, "measur": [57, 60, 65, 77, 83, 91, 92, 93, 105, 106, 107, 119, 137, 138, 144, 145, 146, 147], "measure_col": 77, "measure_func": 57, "measure_pr": 57, "measures_r": 57, "mechan": [46, 47, 50, 51, 93, 106, 112], "median": [91, 93, 120], "medium": 70, "melt": 58, "membership": 93, "memori": [64, 65, 66, 67, 68, 70, 71, 80, 84, 85, 86, 92, 97, 99, 102, 150], "mention": [80, 104], "merg": [59, 85], "mert": [120, 151], "meshgrid": [74, 75, 93], "messag": [56, 57, 58, 59, 60, 61, 70, 82, 150, 152], "meta": [46, 47, 50, 51, 105, 150], "metadata": [46, 47, 50, 51], "metadata_rout": [46, 47, 50, 51], "metadatarequest": [46, 47, 50, 51], "method": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 71, 72, 73, 74, 75, 76, 78, 79, 81, 82, 84, 85, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 102, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 137, 138, 143, 148, 150, 152], "methodolog": 151, "methodologi": 93, "metric": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 46, 50, 70, 105], "michael": 151, "michaela": 152, "michel": [148, 150], "michela": [36, 151], "mid": [59, 85, 87, 90, 106, 121, 133], "mid_point": [70, 71], "might": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 58, 65, 76, 81, 82, 84, 88, 90, 92, 93, 105, 106], "mild": [56, 73, 96], "militari": 87, "miller": [58, 84], "mimic": 93, "min": [58, 59, 60, 61, 64, 65, 66, 67, 70, 76, 81, 84, 85, 86, 89, 90, 95, 105, 106, 110, 120, 121, 136, 150, 153], "min_": 104, "min_samples_leaf": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 49, 80, 85, 92, 95, 104, 105, 106, 108, 120, 121, 136, 137, 143, 153], "min_samples_split": [85, 106, 107, 109, 111], "miniconda3": 70, "minim": [26, 49, 59, 82, 85, 90, 106], "minimum": 65, "minor": [56, 65, 73, 96, 121, 152], "minsplit": 59, "minut": 83, "mirror": [97, 102], "miruna": 151, "mislead": 152, "miss": [5, 6, 7, 8, 9, 30, 60, 97, 98, 102, 105, 106, 121, 134, 152], "missing": [36, 61, 94], "misspecif": 62, "misspecifi": 62, "mit": [148, 150], "mixin": [0, 20, 21, 121], "ml": [43, 58, 59, 60, 65, 77, 83, 84, 85, 90, 91, 95, 103, 105, 106, 120, 148, 151, 152], "ml_g": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 55, 56, 57, 59, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 76, 78, 80, 81, 82, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 104, 105, 106, 107, 108, 109, 111, 152], "ml_g0": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 57, 59, 62, 64, 65, 66, 68, 82, 85, 92, 105, 106, 108, 111], "ml_g1": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 57, 59, 62, 64, 65, 66, 68, 82, 85, 92, 105, 106, 108, 111], "ml_g_d0": [94, 106], "ml_g_d0_t0": [62, 67, 106, 108, 109], "ml_g_d0_t1": [62, 67, 106, 108, 109], "ml_g_d1": [94, 106], "ml_g_d1_t0": [62, 67, 106, 108, 109], "ml_g_d1_t1": [62, 67, 106, 108, 109], "ml_g_d_lvl0": [70, 106], "ml_g_d_lvl1": [70, 106], "ml_g_sim": 30, "ml_l": [37, 38, 56, 58, 59, 60, 68, 73, 75, 79, 83, 84, 85, 87, 92, 95, 96, 105, 106, 120, 121, 136, 137, 143, 150, 152, 153], "ml_l_bonu": 150, "ml_l_forest": 60, "ml_l_forest_pip": 60, "ml_l_lasso": 60, "ml_l_lasso_pip": 60, "ml_l_rf": 153, "ml_l_sim": 150, "ml_l_tune": 105, "ml_l_xgb": 153, "ml_m": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 137, 143, 150, 152, 153], "ml_m_bench_control": 93, "ml_m_bench_treat": 93, "ml_m_bonu": 150, "ml_m_forest": 60, "ml_m_forest_pip": 60, "ml_m_lasso": 60, "ml_m_lasso_pip": 60, "ml_m_rf": 153, "ml_m_sim": [30, 150], "ml_m_tune": 105, "ml_m_xgb": 153, "ml_pi": [30, 61, 94, 106], "ml_pi_sim": 30, "ml_r": [25, 37, 55, 58, 59, 72, 84, 85, 91, 106, 152], "ml_r0": 106, "ml_r1": [59, 85, 106], "mlr": [60, 105], "mlr3": [55, 56, 57, 58, 59, 61, 95, 105, 106, 120, 121, 136, 148, 150, 152, 153], "mlr3book": [60, 105], "mlr3extralearn": [59, 105], "mlr3filter": 60, "mlr3learner": [55, 56, 57, 58, 59, 95, 105, 106, 120, 121, 136, 150, 153], "mlr3measur": 57, "mlr3pipelin": [105, 152], "mlr3tune": [60, 105, 152], "mlr3vers": 59, "mlrmeasur": 57, "mode": [93, 149], "model": [0, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 47, 48, 49, 51, 52, 55, 56, 57, 58, 60, 62, 63, 64, 65, 66, 67, 72, 73, 76, 77, 80, 82, 84, 86, 89, 92, 95, 96, 97, 99, 101, 102, 103, 105, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 122, 123, 124, 125, 126, 128, 129, 132, 133, 138, 143, 144, 145, 146, 147, 148, 151, 152], "model_data": [59, 85], "model_label": 83, "model_list": 70, "model_select": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 73, 84, 105, 120], "modellist": [70, 81], "modelmlestimatelowerupp": 59, "modern": [60, 105, 148, 150], "modul": [70, 90, 102, 106, 149], "moment": [20, 21, 58, 84, 106, 109, 110, 111, 121, 136, 137, 138, 147, 150], "monoton": 106, "mont": [31, 32, 35, 39, 74, 75, 78, 79], "montanari": 151, "month": [64, 67], "more": [26, 48, 55, 57, 59, 64, 65, 66, 67, 70, 71, 72, 74, 75, 77, 81, 82, 83, 85, 86, 90, 92, 93, 95, 104, 105, 106, 107, 108, 109, 110, 111, 112, 119, 121, 129, 136, 137, 138, 143, 147, 150, 153], "moreov": [59, 60, 65, 77, 105, 136, 153], "mortgag": [59, 85, 86], "most": [59, 76, 82, 85, 86, 89, 93, 104, 105, 106, 137, 143, 149], "motiv": [93, 96], "motivation_example_bch": 77, "mp": 57, "mpd": [58, 84], "mpdta": 65, "mpg": 84, "mse": [60, 77, 105], "mserd": 90, "msg": [64, 70], "msr": [60, 105], "mtry": [59, 60, 95, 105, 106, 120, 121, 136, 153], "mu": 63, "mu_": 63, "mu_0": 106, "mu_mean": 63, "much": [59, 60, 70, 85, 90, 91, 93, 153], "muld": [68, 97, 102, 150], "multi": [16, 46, 50, 57, 58, 74, 75, 84, 121, 123, 125, 152], "multiclass": [60, 83], "multiindex": 84, "multioutput": [47, 51], "multioutputregressor": [47, 51], "multipl": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 57, 58, 59, 61, 62, 65, 66, 70, 81, 84, 85, 92, 93, 94, 97, 102, 105, 110, 114, 117, 120, 136, 137, 138, 151, 152, 153], "multipletest": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38], "multipli": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 73, 103, 104, 121, 153], "multiprocess": [76, 86, 89], "multitest": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38], "multivariate_norm": 30, "multiwai": [41, 58, 84, 151], "music": 151, "must": [12, 13, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 97, 98, 102, 105, 106], "mutat": 60, "mutual": [22, 26, 38, 59, 78, 79, 85, 86, 104], "my_sampl": 120, "my_task": 120, "n": [12, 14, 15, 17, 18, 19, 22, 24, 25, 26, 27, 28, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 55, 56, 58, 60, 61, 63, 64, 67, 70, 71, 72, 73, 76, 77, 80, 84, 87, 88, 89, 90, 91, 94, 95, 96, 104, 105, 106, 112, 120, 136, 148, 149], "n_": [35, 63, 67, 137, 140, 142], "n_aggreg": 13, "n_coef": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 137, 143], "n_color": [64, 67], "n_complier": 89, "n_core": [76, 86, 89], "n_estim": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 62, 63, 64, 67, 68, 70, 73, 74, 75, 76, 78, 79, 80, 85, 86, 88, 89, 90, 92, 93, 95, 96, 104, 105, 106, 108, 120, 121, 136, 137, 143, 150, 153], "n_eval": [60, 105], "n_featur": [46, 47, 50, 51], "n_fold": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 56, 57, 58, 59, 62, 64, 65, 67, 68, 73, 74, 75, 76, 78, 79, 81, 82, 84, 85, 86, 87, 88, 89, 90, 92, 93, 96, 105, 120, 150, 153], "n_folds_per_clust": [58, 84], "n_folds_tun": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "n_framework": 13, "n_iter": [44, 90, 106], "n_iter_randomized_search": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 105], "n_job": [76, 85, 86, 89], "n_jobs_cv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 82], "n_jobs_model": [16, 23, 29, 76, 86, 89], "n_learner": 70, "n_level": [35, 70, 71], "n_ob": [7, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 45, 48, 49, 56, 60, 61, 62, 63, 64, 67, 70, 71, 73, 74, 75, 78, 79, 80, 81, 82, 83, 90, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 111, 120, 136, 137, 143, 150], "n_output": [46, 47, 50, 51], "n_period": [17, 19, 64, 67], "n_pre_treat_period": [17, 19, 64, 67], "n_rep": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 56, 57, 58, 61, 62, 64, 67, 68, 70, 71, 73, 80, 81, 82, 84, 90, 92, 93, 94, 96, 105, 120, 137, 143, 150, 153], "n_rep_boot": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 48, 64, 65, 66, 67, 71, 74, 75, 76, 78, 79, 81, 86, 89, 136], "n_sampl": [46, 47, 50, 51, 88, 91], "n_samples_fit": [47, 51], "n_split": 120, "n_t": 63, "n_target": [50, 51], "n_theta": 13, "n_time_period": 63, "n_true": [76, 89], "n_var": [56, 60, 73, 96, 97, 102, 105, 136, 150], "n_w": 88, "n_x": [32, 74, 75, 78, 79, 80], "na": [5, 6, 7, 8, 9, 56, 58, 61, 96, 152], "na_real_": [58, 152], "naiv": [56, 73, 96], "name": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 50, 51, 56, 57, 58, 64, 65, 67, 70, 78, 79, 80, 83, 84, 90, 92, 93, 105, 149, 152], "namespac": 57, "nan": [5, 6, 7, 8, 9, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 62, 63, 70, 71, 73, 76, 78, 79, 82, 83, 85, 86, 89, 94, 96, 105], "nanmean": 73, "narita": 151, "nat": [64, 67], "nathan": 151, "nation": [93, 120, 151], "nativ": 57, "natt": 88, "natur": 93, "nbest": 70, "ncol": [58, 59, 60, 90, 97, 102, 105, 136, 150], "ncoverag": 82, "ndarrai": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 97, 102], "nearli": 82, "necess": [58, 84], "necessari": [57, 58, 70, 83, 84, 90, 106, 149], "need": [12, 14, 15, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 55, 56, 57, 59, 61, 70, 72, 73, 83, 86, 91, 94, 105, 120, 137, 147, 152, 153], "neg": [47, 51], "neighborhood": [90, 136], "neither": [5, 6, 7, 8, 9, 58, 84, 97, 102], "neng": 151, "neq": [70, 90, 106], "nest": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 50, 51, 105, 121, 135, 137, 143], "net": [86, 92, 153], "net_tfa": [59, 85, 86, 92, 153], "network": 70, "nev": [106, 109, 111, 112], "never": [17, 19, 25, 57, 58, 64, 65, 66, 67, 84, 106, 112, 152], "never_tak": [25, 59, 85], "never_tr": [14, 16, 64, 65, 66, 67, 106, 107, 109, 111], "nevertheless": 81, "new": [55, 56, 57, 58, 59, 60, 61, 74, 75, 83, 85, 88, 95, 96, 97, 102, 104, 105, 106, 120, 121, 136, 148, 150, 151, 152, 153], "new_data": [74, 75, 88], "newei": [10, 11, 42, 58, 77, 84, 93, 96, 148, 151], "newest": 152, "next": [57, 59, 60, 74, 75, 76, 80, 82, 85, 86, 88, 89, 91, 93, 152], "neyman": [58, 84, 95, 103, 137, 147, 148, 151], "nfold": [58, 59, 61, 106], "nh": 106, "nice": [57, 70], "nifa": [85, 86, 92], "nil": 93, "nine": [58, 84], "nlogloss": 70, "nn": 90, "noack": [90, 106, 151, 152], "node": [59, 60, 95, 106, 120, 121, 136, 150, 153], "nois": [45, 87, 88], "nomin": 91, "non": [14, 17, 18, 19, 25, 41, 42, 43, 44, 55, 56, 59, 63, 64, 67, 70, 72, 73, 85, 86, 88, 90, 105, 120, 121, 123, 125, 136, 137, 140, 142], "non_orth_scor": [56, 73, 121], "nondur": 68, "none": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 44, 46, 47, 48, 50, 51, 58, 59, 62, 64, 65, 66, 67, 68, 70, 71, 72, 80, 85, 86, 91, 92, 93, 94, 97, 98, 99, 100, 101, 102, 105, 106, 108, 109, 111, 121, 136, 149, 150], "nonignor": [30, 135], "nonlinear": [17, 19, 21, 59, 64, 67, 85, 90, 106, 121, 130, 131, 152], "nonlinearscoremixin": [0, 121], "nonparametr": [27, 28, 29, 90, 93, 121, 137, 138, 144, 145, 146, 147, 151], "nop": 60, "nor": [5, 6, 7, 8, 9, 58, 84, 97, 102], "norm": 73, "normal": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 56, 61, 62, 63, 64, 66, 67, 72, 73, 76, 80, 86, 87, 88, 89, 90, 91, 94, 96, 97, 102, 105, 106, 121, 122, 123, 124, 125, 136, 150], "normalize_ipw": [22, 23, 24, 25, 26, 27, 28, 29, 30, 61, 81, 86, 94], "not_yet_tr": [14, 16, 64, 67], "notat": [58, 61, 62, 84, 94, 106, 108, 109, 110, 111, 112, 119, 121, 123, 125], "note": [5, 6, 7, 8, 9, 13, 16, 19, 20, 21, 25, 26, 30, 37, 38, 46, 47, 49, 50, 51, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 120, 121, 148, 150], "notebook": [55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 104, 105, 106, 153], "notic": [55, 72, 91], "now": [57, 58, 59, 61, 65, 66, 70, 74, 75, 82, 84, 85, 88, 91, 93, 94, 102, 150, 152], "np": [5, 6, 7, 8, 9, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 44, 62, 63, 64, 65, 66, 67, 68, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 102, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 137, 143, 150, 153], "nrmse": 70, "nround": [56, 59, 153], "nrow": [57, 58, 60, 90, 97, 102, 105, 136, 150], "nu": [18, 25, 43, 61, 94, 106, 115, 137, 138, 140, 142, 143, 146, 147], "nu2": [64, 70, 82, 137, 143], "nu_0": [137, 147], "nu_i": [61, 94], "nuis_g0": 55, "nuis_g1": 55, "nuis_l": 153, "nuis_m": [55, 153], "nuis_r0": 55, "nuis_r1": 55, "nuis_rmse_ml_l": 77, "nuis_rmse_ml_m": 77, "nuisanc": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 42, 43, 44, 56, 57, 58, 59, 60, 61, 62, 65, 70, 73, 74, 75, 76, 77, 80, 82, 84, 85, 86, 89, 91, 92, 93, 94, 95, 96, 105, 106, 109, 110, 111, 120, 121, 122, 123, 124, 125, 126, 130, 136, 137, 140, 142, 147, 148, 152, 153], "nuisance_el": [137, 139, 140, 141, 142, 144, 145, 146], "nuisance_loss": [70, 82, 105, 152], "nuisance_target": 82, "null": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 57, 92, 105, 137, 143, 152], "null_hypothesi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 92, 137, 143], "num": [59, 60, 95, 105, 106, 120, 121, 136, 150], "num_leav": [63, 76, 86, 89], "number": [12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 51, 56, 58, 63, 64, 65, 67, 73, 74, 75, 76, 77, 78, 79, 82, 84, 86, 88, 89, 90, 93, 106, 110, 120, 136, 148, 150, 153], "numer": [21, 55, 60, 81, 87, 105, 121, 137, 144, 145, 152], "numeric_onli": 77, "numpi": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 48, 49, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 99, 102, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 137, 143, 150], "nuniqu": [64, 67], "ny": 151, "nyt": [106, 109, 111, 112], "o": [63, 70, 71, 77, 78, 79, 82, 83, 85, 87, 90, 148, 150], "ob": [57, 59, 63, 67, 90, 137, 140], "obei": 121, "obj": 85, "obj_dml_data": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 55, 56, 58, 65, 66, 72, 73, 76, 81, 83, 84, 89, 95, 96, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 137, 143, 152], "obj_dml_data_bonu": [97, 102], "obj_dml_data_bonus_df": [97, 102], "obj_dml_data_from_arrai": [5, 6, 7, 8, 9], "obj_dml_data_from_df": [5, 6, 8, 9], "obj_dml_data_sim": [97, 102], "obj_dml_data_sim_clust": [97, 102], "obj_dml_plr": [56, 73, 96], "obj_dml_plr_bonu": [60, 150], "obj_dml_plr_bonus_pip": 60, "obj_dml_plr_bonus_pipe2": 60, "obj_dml_plr_bonus_pipe3": 60, "obj_dml_plr_bonus_pipe_ensembl": 60, "obj_dml_plr_fullsampl": 83, "obj_dml_plr_lesstim": 83, "obj_dml_plr_nonorth": [56, 73], "obj_dml_plr_orth_nosplit": [56, 73], "obj_dml_plr_sim": [60, 150], "obj_dml_plr_sim_pip": 60, "obj_dml_plr_sim_pipe_ensembl": 60, "obj_dml_plr_sim_pipe_tun": 60, "obj_dml_sim": 30, "object": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 55, 59, 60, 61, 62, 64, 65, 66, 67, 68, 71, 74, 75, 76, 80, 81, 83, 85, 86, 89, 90, 94, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 148, 150, 151, 152, 153], "obs_confound": [55, 72], "observ": [12, 14, 15, 16, 17, 18, 19, 22, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 48, 49, 52, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 80, 82, 83, 84, 85, 86, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 111, 112, 119, 120, 121, 122, 123, 124, 125, 136, 137, 138, 139, 140, 141, 142, 150, 151, 153], "obtain": [19, 25, 39, 55, 56, 57, 58, 61, 62, 64, 65, 66, 67, 72, 73, 74, 75, 76, 77, 82, 84, 89, 91, 93, 94, 95, 96, 104, 105, 120, 121, 136, 137, 138, 143, 149, 150], "obvious": [64, 67], "occur": [17, 19, 64, 67, 83, 152], "off": [88, 151], "offer": [17, 19, 57, 59, 85, 86, 93, 153], "offici": 149, "offset": 105, "often": 89, "oka": 151, "ol": [22, 26, 38, 48], "olma": [90, 106, 151, 152], "omega": [80, 104, 106, 107, 121, 126, 129, 137, 144, 145], "omega_": [41, 58, 84], "omega_1": [41, 58, 84], "omega_2": [41, 58, 84], "omega_epsilon": [58, 84], "omega_v": [41, 58, 84], "omega_x": [41, 58, 84], "omit": [64, 67, 92, 93, 106, 110, 121, 123, 125, 137, 138, 147, 151, 152, 153], "ommit": 93, "onc": [57, 83, 93, 106, 112, 153], "one": [13, 16, 37, 52, 55, 56, 57, 58, 59, 60, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 82, 84, 86, 87, 90, 91, 92, 93, 96, 97, 102, 104, 105, 106, 107, 110, 112, 117, 120, 121, 122, 123, 124, 125, 129, 132, 133, 136, 137, 138, 143, 144, 145, 146, 150, 152], "ones": [60, 63, 76, 83, 89, 92, 104], "ones_lik": [70, 71, 89], "onli": [5, 6, 7, 8, 9, 12, 14, 15, 16, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 46, 47, 48, 50, 51, 57, 58, 59, 64, 67, 74, 75, 78, 79, 80, 82, 83, 84, 85, 86, 90, 95, 104, 105, 106, 109, 111, 119, 121, 123, 125, 127, 130, 131, 136, 137, 138, 140, 142, 144, 145, 147, 152], "onlin": 153, "onto": 82, "oo": 83, "oob_error": [60, 105], "oop": 152, "opac": [74, 75], "open": [60, 105, 148, 150], "oper": [60, 152], "opposit": [88, 90, 106], "oprescu": [32, 74, 75, 78, 79, 151], "opt": [64, 65, 66, 67, 84, 85], "optim": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 44, 60, 74, 75, 83, 88, 104, 105, 151], "option": [12, 13, 14, 15, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 46, 47, 50, 51, 55, 56, 58, 59, 61, 64, 65, 67, 71, 74, 75, 78, 79, 80, 82, 84, 85, 86, 94, 97, 98, 100, 101, 102, 105, 106, 120, 121, 127, 130, 131, 136, 152], "oracl": [35, 45, 64, 67, 70, 71], "oracle_valu": [31, 35, 39, 45, 70, 71], "orang": 56, "orcal": [31, 39], "order": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 50, 57, 58, 59, 60, 65, 81, 84, 85, 90, 105, 106, 120, 121], "org": [33, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 105, 148, 149, 151, 152], "orient": [60, 105, 121, 148, 150, 151, 152], "origin": [46, 47, 49, 50, 51, 57, 60, 64, 65, 66, 67, 88, 92, 93, 104, 121, 129], "orign": [59, 85], "orth_sign": [48, 49, 81], "orthogon": [48, 49, 58, 59, 64, 70, 84, 85, 95, 103, 106, 136, 137, 147, 148, 151], "orthongon": [137, 147], "osx": 149, "other": [5, 6, 7, 8, 9, 37, 38, 46, 47, 50, 51, 56, 58, 59, 60, 61, 62, 64, 65, 66, 67, 71, 73, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 96, 97, 100, 102, 104, 105, 106, 117, 118, 120, 121, 129, 136, 137, 147, 148, 149, 150, 151, 152, 153], "other_ind": 84, "otherwis": [12, 14, 15, 22, 25, 26, 38, 46, 47, 50, 51, 59, 85, 86, 88, 106, 108, 121, 123, 125], "othrac": [60, 68, 97, 102, 150], "our": [56, 57, 59, 60, 62, 64, 67, 70, 73, 74, 75, 76, 82, 83, 85, 86, 89, 90, 92, 93, 96, 106, 148, 150, 152, 153], "ourselv": 82, "out": [37, 38, 58, 60, 62, 63, 64, 65, 66, 67, 68, 77, 82, 83, 84, 86, 92, 93, 94, 95, 97, 102, 103, 104, 105, 106, 107, 108, 109, 111, 121, 132, 133, 136, 137, 138, 143, 146, 148, 150, 152, 153], "outcom": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 35, 37, 38, 39, 45, 55, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 72, 77, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 97, 98, 99, 100, 101, 102, 105, 108, 109, 111, 112, 113, 117, 118, 119, 123, 125, 126, 136, 138, 143, 144, 146, 147, 150, 152, 153], "outcome_0": 72, "outcome_1": 72, "outer": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 105], "outperform": 70, "output": [57, 64, 66, 67, 82, 91, 95, 106, 109, 111, 136, 153], "output_list": 91, "outshr": 84, "outsid": 56, "over": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 37, 38, 56, 64, 65, 67, 71, 73, 77, 82, 96, 103, 105, 106, 107, 137, 143, 152], "overal": [13, 64, 65, 66, 67, 88, 93, 106, 107], "overall_aggregation_weight": [13, 64, 65, 66, 67], "overcom": [103, 121], "overfit": [83, 103, 120], "overlap": [62, 93, 106, 108, 112], "overrid": [105, 152], "overridden": 106, "overst": [59, 85, 86], "overview": [82, 136, 137, 143, 151], "overwrit": 152, "ownership": [59, 85], "p": [12, 14, 15, 16, 17, 18, 19, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 37, 38, 39, 44, 45, 56, 57, 58, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 96, 104, 105, 106, 107, 108, 109, 111, 112, 120, 121, 122, 123, 124, 125, 126, 127, 130, 131, 134, 135, 136, 137, 144, 145, 148, 149, 150, 152], "p401": [59, 85, 86], "p_0": [121, 122, 123, 124], "p_1": 136, "p_adjust": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 120, 136, 148, 150], "p_dbl": [60, 105], "p_hat": 81, "p_int": 105, "p_n": 40, "p_val": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38], "p_x": [41, 58, 84], "p_x0": 87, "p_x1": 87, "packag": [55, 56, 58, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 83, 84, 86, 88, 89, 90, 91, 92, 93, 94, 95, 96, 104, 105, 106, 108, 112, 119, 120, 121, 136, 137, 138, 148, 150, 151, 152, 153], "packagedata": 84, "packagevers": 59, "page": [93, 148, 151], "pair": [55, 72], "pake": [58, 84], "paket": [58, 59, 60], "pal": 58, "palett": [13, 16, 64, 67, 70, 71], "pand": [64, 67], "panda": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 48, 49, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 97, 99, 102, 104, 106, 137, 138, 150], "pandas2ri": 84, "panel": [6, 7, 12, 14, 16, 17, 18, 19, 65, 67, 97, 98, 99, 102, 109, 110, 112, 141, 142, 151, 152], "paper": [33, 40, 60, 83, 87, 90, 92, 93, 137, 147, 148, 150, 151, 152], "par": 68, "par_grid": [60, 105], "paradox": [60, 105, 152], "parallel": [57, 62, 63, 64, 67, 71, 76, 82, 89, 106, 108, 110, 112], "param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 50, 51, 83, 105], "param_grid": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 105], "param_nam": 57, "param_set": [60, 105], "param_v": 60, "paramet": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 56, 57, 58, 59, 61, 62, 64, 65, 67, 70, 71, 73, 74, 75, 76, 77, 80, 81, 82, 84, 85, 88, 89, 90, 91, 92, 93, 94, 95, 96, 102, 103, 104, 105, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 130, 131, 136, 137, 138, 143, 145, 147, 148, 150, 151, 152, 153], "parametr": [25, 57, 93, 96, 105, 153], "params_exact": 105, "params_nam": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 57], "parenttoc": 148, "part": [43, 56, 58, 59, 60, 66, 73, 82, 83, 84, 85, 96, 105, 120, 137, 147, 152, 153], "parti": 43, "partial": [21, 37, 38, 39, 40, 41, 42, 43, 58, 60, 68, 77, 83, 84, 92, 95, 103, 105, 117, 118, 120, 132, 133, 136, 143, 144, 145, 146, 147, 148, 150, 152, 153], "partial_": [121, 136], "partiallli": 92, "particip": [10, 86, 92, 153], "particular": [106, 148], "particularli": [70, 83], "partion": [58, 84], "partit": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 58, 84, 95, 103], "partli": 153, "pass": [22, 26, 38, 46, 47, 48, 50, 51, 57, 60, 65, 83, 105, 153], "passo": [148, 150], "past": 58, "paste0": [58, 61], "pastel": 73, "path": [105, 106], "path_to_r": 77, "patsi": [74, 75, 104], "pattern": 93, "paul": 151, "pd": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 44, 48, 62, 63, 64, 65, 66, 67, 70, 71, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 104, 106], "pdf": [73, 87], "pedregosa": [148, 150], "pedregosa11a": [148, 150], "pedro": [57, 151], "penal": [61, 65, 94], "penalti": [59, 60, 65, 72, 85, 91, 93, 94, 105, 106], "pennsylvania": [11, 97, 102, 150], "pension": [59, 85, 86, 153], "peopl": [59, 85, 86], "pep8": 152, "per": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 58, 64, 65, 66, 67, 84], "percent": 105, "percentag": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39], "perf_count": 82, "perfectli": [90, 106], "perform": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 49, 56, 58, 60, 62, 64, 65, 66, 67, 73, 77, 80, 82, 83, 84, 86, 92, 93, 94, 96, 105, 106, 108, 109, 111, 120, 121, 136, 148, 150, 151, 153], "performance_result": 70, "perfrom": 80, "perhap": 153, "period": [12, 14, 16, 17, 19, 57, 62, 63, 66, 99, 102, 107, 108, 109, 110, 111, 112, 123, 125, 151, 152], "perp": [106, 119], "perrot": [148, 150], "person": 153, "pessimist": 93, "peter": 151, "petra": 152, "petronelaj": 152, "pfister": [60, 105, 148, 150], "phi": [58, 84, 104, 136], "philipp": [93, 148, 151], "philippbach": [148, 152], "pi": [30, 34, 40, 43, 104, 106, 121, 134, 135], "pi_": [41, 58, 84], "pi_0": [121, 134, 135], "pi_i": [61, 94, 106], "pick": [90, 153], "pip": [90, 106], "pip3": 149, "pipe": 60, "pipe_forest_classif": 60, "pipe_forest_regr": 60, "pipe_lasso": 60, "pipelin": [46, 47, 50, 51, 60, 85, 152], "pipeop": 60, "pira": [59, 85, 86, 92, 153], "pivot": [70, 77, 84, 151], "pivot_logloss": 70, "pivot_rmse_g0": 70, "pivot_rmse_g1": 70, "plai": [83, 153], "plan": [10, 59, 85, 86, 153], "plausibl": [93, 137], "pleas": [46, 47, 50, 51, 57, 62, 63, 65, 71, 83, 93, 120, 148, 149], "plim": 87, "pliv": [20, 21, 37, 58, 84, 95, 104, 117, 132, 148, 152], "plm": [0, 6, 7, 12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 73, 83, 84, 92, 95, 96, 103, 104, 105, 120, 136, 143, 153], "plot": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 49, 56, 57, 59, 60, 61, 63, 64, 65, 66, 67, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 85, 86, 87, 89, 90, 92, 93, 94, 104, 137, 143], "plot_data": [64, 67], "plot_effect": [13, 16, 64, 65, 66, 67], "plot_tre": [49, 88, 104], "plotli": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 74, 75, 77, 90, 93], "plr": [20, 21, 38, 60, 83, 87, 92, 95, 105, 118, 120, 133, 136, 143, 145, 146, 147, 148, 150, 152, 153], "plr_est": 87, "plr_est1": 87, "plr_est2": 87, "plr_obj": 87, "plr_obj_1": 87, "plr_obj_2": 87, "plr_summari": 85, "plrglmnet": 59, "plrranger": 59, "plrrpart": 59, "plrxgboost8700": 59, "plt": [62, 63, 64, 67, 68, 70, 71, 73, 74, 75, 76, 78, 79, 81, 82, 83, 84, 85, 86, 87, 89, 90, 93, 94], "plt_smpl": [58, 84], "plt_smpls_cluster": [58, 84], "plug": [80, 137, 139, 140, 141, 142, 143, 144, 145], "pm": [44, 58, 84, 136, 137, 143, 147], "pmatrix": [61, 94], "pmlr": [77, 82, 83], "po": [60, 105], "poe": 151, "point": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 57, 58, 70, 78, 79, 84, 93, 104, 106, 153], "pointwis": [48, 76, 78, 79, 89], "poli": [59, 81, 84, 85], "polici": [26, 37, 38, 49, 103, 106, 117, 118, 150, 151, 152], "policy_tre": [26, 88, 104], "policy_tree_2": 88, "policy_tree_obj": 104, "policytre": 88, "polit": 87, "poly_dict": 85, "polynomi": [10, 11, 45, 59, 68, 81, 85, 90], "polynomial_featur": [10, 11, 59, 68], "polynomialfeatur": [81, 84, 85], "poor": 91, "pop": [121, 123], "popul": [93, 106, 109, 111, 121, 125], "popular": [82, 106, 137, 138], "porport": 92, "posit": [43, 59, 64, 66, 67, 70, 82, 87, 93, 153], "posixct": [60, 105], "possibl": [5, 6, 7, 8, 9, 47, 50, 51, 57, 60, 64, 66, 67, 74, 75, 78, 79, 80, 81, 82, 83, 88, 90, 91, 92, 93, 105, 106, 110, 113, 114, 136, 137, 138, 152, 153], "possibli": [91, 137, 138], "post": [16, 40, 43, 106, 108, 110, 112, 136, 151], "postdoubl": 151, "poster": 87, "potenti": [17, 19, 22, 23, 24, 27, 28, 30, 31, 35, 45, 61, 62, 64, 67, 81, 87, 90, 94, 108, 112, 113, 119, 126, 127, 136, 144, 149, 152, 153], "potential_level": [70, 71], "power": [60, 83, 91, 93, 105, 151], "pp": [57, 77, 82, 83], "pq": [27, 28, 29, 86, 131, 152], "pq_0": [86, 89], "pq_1": [86, 89], "pr": [30, 55, 58, 59, 60, 61, 105, 106, 120, 121, 136, 150, 153], "practic": [82, 93, 151], "pre": [14, 16, 17, 19, 57, 61, 62, 64, 65, 66, 67, 94, 105, 106, 108, 109, 110, 111, 121, 123, 125, 152], "precis": [57, 106, 137, 145, 153], "precomput": [47, 51], "pred": [57, 83], "pred_df": 88, "pred_dict": 105, "pred_treat": 88, "predict": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 46, 47, 48, 49, 50, 51, 56, 58, 59, 60, 65, 70, 73, 76, 77, 81, 82, 83, 84, 85, 88, 91, 93, 96, 104, 120, 137, 138, 143, 145, 152, 153], "predict_proba": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 38, 44, 46, 50, 83, 91, 105], "predictor": [22, 26, 38, 48, 49, 74, 75, 78, 79, 93, 95], "prefer": [59, 85, 86, 153], "preliminari": [24, 56, 73, 90, 121, 127, 130, 131, 135], "prepar": [57, 58, 84, 152], "preprint": [91, 151], "preprocess": [59, 65, 81, 84, 85, 86, 105], "presenc": [59, 85, 86], "present": [57, 65, 93, 105, 121, 129, 153], "prespecifi": 92, "pretreat": [12, 14, 15, 16, 57, 62], "prettenhof": [148, 150], "preval": 93, "prevent": [120, 152], "previou": [63, 80, 81, 87, 149, 153], "previous": [65, 91, 105, 153], "price": [58, 84], "priliminari": [27, 29], "primari": [70, 71], "principl": [137, 138], "print": [14, 16, 44, 56, 57, 58, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 137, 143, 149, 150, 152, 153], "print_detail": 57, "print_period": [14, 16], "prior": [70, 82, 106, 119], "privat": 152, "prob": 60, "prob_dist": 91, "prob_dist_": 91, "probabilit": 80, "probabl": [17, 18, 19, 22, 24, 25, 26, 27, 28, 29, 30, 35, 50, 56, 57, 61, 62, 64, 67, 71, 73, 80, 87, 89, 90, 93, 94, 96, 106, 121, 122, 123, 124, 125, 130, 151], "problem": [59, 65, 85, 86, 104, 105], "procedur": [56, 58, 59, 73, 82, 84, 85, 92, 93, 105, 136, 149, 152], "proceed": [40, 151], "process": [17, 18, 19, 31, 32, 33, 34, 35, 36, 39, 40, 41, 42, 43, 45, 57, 61, 62, 63, 66, 67, 74, 75, 76, 77, 78, 79, 82, 83, 88, 89, 93, 94, 103, 136, 137, 138, 151, 152], "produc": 87, "product": [74, 75, 77, 82, 93, 137, 147], "producton": 58, "program": [34, 59, 85, 86, 151, 153], "progress": 69, "project": [60, 74, 75, 104, 148, 152], "project_z": [74, 75], "prone": 121, "pronounc": 90, "propens": [17, 19, 27, 29, 31, 39, 59, 61, 62, 64, 65, 67, 70, 80, 81, 82, 85, 86, 93, 94, 104, 106, 109, 111, 113, 121, 123, 125, 137, 144], "proper": 70, "properli": [70, 83, 153], "properti": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 59, 60, 64, 65, 66, 67, 82, 85, 86, 87, 91, 92, 97, 98, 100, 101, 102, 105, 106, 137, 143, 150, 152], "proport": [92, 137, 138, 146, 147], "propos": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 58, 60, 84, 90, 137, 138, 151, 152], "provid": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 46, 47, 50, 51, 57, 58, 59, 60, 65, 70, 74, 75, 78, 79, 81, 83, 84, 85, 90, 93, 95, 96, 97, 99, 102, 103, 105, 136, 148, 150, 152, 153], "prune": [26, 49], "ps911c": 84, "ps944": 84, "pscore1": 87, "pscore2": 87, "psi": [20, 21, 56, 57, 58, 84, 95, 106, 109, 110, 111, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 147, 150], "psi_": [136, 137, 140, 142, 143, 146, 147], "psi_a": [20, 25, 26, 37, 38, 56, 58, 73, 84, 106, 109, 110, 111, 120, 121, 122, 123, 124, 125, 126, 128, 129, 132, 133, 136], "psi_b": [20, 25, 26, 37, 38, 56, 73, 104, 106, 109, 110, 111, 120, 121, 122, 123, 124, 125, 126, 128, 129, 132, 133], "psi_el": [120, 121], "psi_j": 136, "psi_nu2": [137, 143], "psi_sigma2": [137, 143], "public": [55, 72, 152], "publish": [93, 148, 152], "pull": [59, 152], "purchas": 93, "pure": 93, "purp": [74, 75], "purpos": [56, 73, 80, 92, 93, 121, 123, 125, 137, 138, 150], "pval": 136, "px": [77, 90], "py": [64, 65, 66, 67, 70, 84, 85, 93, 148, 149, 152], "py3": 149, "py_al": 73, "py_did": 62, "py_did_pretest": 63, "py_dml": 73, "py_dml_nosplit": 73, "py_dml_po": 73, "py_dml_po_nosplit": 73, "py_double_ml_apo": 71, "py_double_ml_bas": 73, "py_double_ml_basic_iv": 72, "py_double_ml_c": 74, "py_double_ml_cate_plr": 75, "py_double_ml_cvar": 76, "py_double_ml_firststag": 77, "py_double_ml_g": 78, "py_double_ml_gate_plr": 79, "py_double_ml_gate_sensit": 80, "py_double_ml_irm_vs_apo": 81, "py_double_ml_learn": 82, "py_double_ml_meets_flaml": 83, "py_double_ml_multiway_clust": 84, "py_double_ml_pens": 85, "py_double_ml_pension_qt": 86, "py_double_ml_plm_irm_hetfx": 87, "py_double_ml_policy_tre": 88, "py_double_ml_pq": 89, "py_double_ml_rdflex": 90, "py_double_ml_robust_iv": 91, "py_double_ml_sensit": 92, "py_double_ml_sensitivity_book": 93, "py_double_ml_ssm": 94, "py_non_orthogon": 73, "py_panel": 64, "py_panel_data_exampl": 65, "py_panel_simpl": 66, "py_po_al": 73, "py_rep_c": 67, "py_tabpfn": 70, "pypi": [151, 152], "pyplot": [62, 63, 64, 67, 68, 70, 71, 73, 74, 75, 76, 78, 79, 81, 82, 83, 84, 85, 86, 87, 89, 90, 93, 94], "pyproject": 152, "pyreadr": 65, "python": [43, 57, 83, 93, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 111, 120, 121, 136, 137, 138, 143, 148, 150, 151, 152, 153], "python3": [64, 65, 66, 67, 84, 85, 149], "pytorch": 70, "q": [60, 76, 89, 90, 105, 148, 150], "q2": [60, 68, 97, 102, 150], "q3": [60, 68, 97, 102, 150], "q4": [60, 68, 97, 102, 150], "q5": [60, 68, 97, 102, 150], "q6": [60, 68, 97, 102, 150], "q_i": [90, 106], "qquad": 34, "qte": [76, 86, 152], "quad": [17, 18, 19, 59, 61, 62, 85, 88, 90, 94, 104, 106, 108, 112, 119, 121, 130, 136, 137, 139, 140], "quadrat": [61, 94], "qualiti": [92, 95, 152], "quanitl": 86, "quant": 76, "quantifi": [93, 106, 112], "quantil": [16, 23, 24, 27, 28, 29, 35, 64, 67, 71, 76, 81, 92, 103, 105, 127, 130, 131, 151, 152], "quantiti": [55, 72, 93], "queri": 85, "question": [93, 153], "quick": 86, "quit": [70, 82, 88, 92, 137, 138], "r": [25, 33, 46, 47, 50, 51, 63, 64, 65, 66, 67, 73, 74, 75, 77, 84, 87, 90, 91, 93, 95, 96, 97, 102, 103, 106, 115, 120, 121, 128, 132, 136, 137, 138, 144, 145, 146, 147, 148, 150, 151, 152, 153], "r2_d": [34, 82], "r2_score": [47, 51], "r2_y": [34, 82], "r6": [60, 152], "r_0": [25, 37, 59, 85, 106, 115], "r_all": 56, "r_d": 34, "r_df": 84, "r_dml": 56, "r_dml_nosplit": 56, "r_dml_po": 56, "r_dml_po_nosplit": 56, "r_double_ml_bas": 56, "r_double_ml_basic_iv": 55, "r_double_ml_did": 57, "r_double_ml_multiway_clust": 58, "r_double_ml_pens": 59, "r_double_ml_pipelin": 60, "r_double_ml_ssm": 61, "r_hat": 37, "r_hat0": 25, "r_hat1": 25, "r_non_orthogon": 56, "r_po_al": 56, "r_y": 34, "rais": [5, 6, 7, 8, 9, 46, 47, 50, 51, 64, 65, 66, 67, 105], "randint": 87, "randn": 30, "random": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 36, 37, 38, 43, 44, 45, 55, 56, 57, 59, 60, 62, 63, 64, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 99, 102, 104, 105, 107, 108, 109, 111, 112, 120, 134, 136, 137, 143, 147, 150, 151, 153], "random_search": 105, "random_st": [35, 64, 67, 73, 80, 81, 88], "randomforest": [59, 70, 82, 85], "randomforest_class": [59, 74, 85, 88], "randomforest_reg": [74, 88], "randomforestclassifi": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 68, 70, 71, 74, 75, 78, 79, 80, 82, 85, 88, 90, 92, 93, 104, 105, 106, 107, 108, 109, 111, 153], "randomforestregressor": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 68, 70, 71, 73, 74, 75, 78, 79, 80, 82, 85, 88, 90, 92, 93, 95, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 137, 143, 150, 153], "randomized_search": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 105], "randomizedsearchcv": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 105], "randomli": [56, 58, 73, 84, 96, 120, 153], "rang": [56, 62, 63, 70, 71, 73, 76, 78, 79, 82, 83, 84, 86, 88, 89, 90, 91, 93, 94, 96, 105, 106], "rangeindex": [64, 65, 66, 67, 68, 70, 71, 80, 84, 85, 86, 92, 97, 99, 102, 150], "ranger": [57, 59, 60, 95, 105, 106, 120, 121, 136, 150, 153], "rangl": [32, 88], "rank": 152, "rate": [77, 82, 106], "rather": [90, 93, 106], "ratio": [105, 120, 137, 138], "rational": 65, "ravel": [74, 75], "raw": [59, 65, 66, 77, 85], "raw_res_manual_lasso_r_100_n_100_p_200_rho_0": 77, "rbind": 59, "rbindlist": 59, "rbinom": 55, "rbrace": [25, 26, 33, 34, 36, 58, 84, 95, 106, 113, 115, 116, 120, 121, 126, 136, 137, 144], "rcolorbrew": 58, "rcparam": [63, 68, 74, 75, 76, 78, 79, 81, 84, 85, 86, 89], "rd": [106, 152], "rda": 65, "rdbu": 58, "rdbu_r": 84, "rdbwselect": 106, "rdd": [0, 8, 100, 102, 103, 149], "rdflex": [90, 106, 152], "rdflex_fuzzi": 90, "rdflex_fuzzy_stack": 90, "rdflex_obj": [44, 106], "rdflex_sharp": 90, "rdflex_sharp_stack": 90, "rdrobust": [44, 90, 106, 149, 152], "rdrobust_fuzzi": 90, "rdrobust_fuzzy_noadj": 90, "rdrobust_sharp": 90, "rdrobust_sharp_noadj": 90, "rdt044": 77, "re": [64, 70, 84, 93, 149], "read": [65, 149], "read_csv": [66, 77], "read_r": 65, "readabl": [70, 152], "reader": [65, 91], "readili": 148, "real": [59, 85, 86, 92, 137, 138], "realat": 106, "realiz": [90, 106, 119], "reason": [5, 6, 7, 8, 9, 55, 70, 72, 77, 82, 83, 92, 93, 137, 138, 153], "recal": [68, 137, 147], "receiv": [17, 19, 64, 67, 71, 90, 106, 108, 110], "recent": [83, 106, 108, 112, 151], "recogn": [59, 85, 86], "recommend": [60, 64, 67, 70, 82, 90, 93, 95, 106, 120, 137, 149, 151, 152], "recov": [55, 57, 72, 87], "recreat": 70, "recsi": 151, "red": [58, 61, 70, 78, 79, 83, 84], "reduc": [59, 80, 83, 85, 90, 92, 93, 106, 152], "redund": 152, "reemploy": [11, 97, 102, 150], "ref": 65, "refactor": 152, "refer": [10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 59, 63, 64, 67, 70, 71, 80, 85, 86, 90, 92, 97, 102, 103, 104, 106, 107, 109, 111, 112, 121, 137, 138, 143, 151, 152], "reference_level": [23, 70, 71, 81, 106], "refin": 152, "refit": [137, 138], "reflect": [88, 93, 104], "reg": [17, 18, 19, 59, 85, 153], "reg_estim": 90, "reg_learn": 86, "reg_learner_1": 82, "reg_learner_2": 82, "regard": [93, 148], "regener": 152, "region": [58, 76, 84, 136, 151], "regr": [55, 56, 57, 58, 59, 60, 61, 95, 105, 106, 120, 121, 123, 136, 150, 153], "regravg": [60, 105], "regress": [8, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 48, 55, 57, 58, 60, 61, 62, 64, 65, 66, 67, 70, 71, 72, 77, 83, 84, 87, 91, 92, 93, 94, 95, 96, 100, 102, 103, 104, 105, 108, 109, 111, 115, 116, 117, 118, 120, 125, 136, 138, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153], "regressor": [47, 51, 56, 59, 70, 71, 73, 76, 82, 83, 85, 96], "regular": [40, 103, 105, 121, 136, 151], "reich": [60, 105], "reinforc": 151, "reject": [59, 85], "rel": [59, 65, 70, 85, 91, 106, 107, 137, 138, 144, 145], "relat": [81, 93, 153], "relationship": [55, 70, 72, 77, 93, 136], "relev": [12, 14, 15, 16, 32, 46, 47, 48, 50, 51, 64, 67, 76, 88, 89, 97, 102, 106, 137, 153], "reli": [16, 62, 63, 64, 67, 74, 75, 80, 81, 104, 105, 106, 108, 121, 125, 137, 138, 153], "reload": 59, "remain": [57, 97, 102, 136, 153], "remark": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 56, 62, 63, 64, 65, 66, 67, 71, 73, 74, 75, 76, 78, 79, 80, 81, 82, 86, 92, 104, 105, 106, 107, 108, 109, 111, 120, 121, 122, 123, 124, 125, 130, 131, 136, 137, 140, 142, 145], "remot": 149, "remov": [4, 59, 81, 93, 97, 102, 103, 106, 120, 121, 137, 152], "renam": [64, 67, 85, 152], "render": [92, 93], "reorgan": 152, "rep": [56, 61, 96, 105, 136], "repeat": [12, 14, 15, 16, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 55, 56, 58, 59, 60, 61, 64, 65, 66, 73, 80, 84, 85, 86, 87, 90, 92, 94, 96, 98, 102, 103, 105, 109, 110, 111, 112, 136, 139, 140, 150, 152, 153], "repeatedkfold": 84, "repet": 92, "repetit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 48, 65, 74, 75, 77, 78, 79, 80, 82, 103, 105, 136, 150, 152, 153], "replac": [88, 93, 152], "replic": [10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 56, 59, 65, 66, 73, 77, 91, 93], "repo": 152, "report": [59, 83, 85, 148, 152], "repositori": [64, 67, 77, 90, 152], "repr": [56, 58], "repres": [17, 19, 70, 87, 93, 106], "represent": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 64, 70, 92, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 150, 152], "reproduc": 35, "request": [46, 47, 50, 51, 152], "requir": [37, 38, 46, 50, 55, 59, 60, 64, 65, 66, 67, 70, 71, 80, 85, 86, 92, 97, 102, 106, 107, 109, 110, 111, 121, 123, 125, 136, 137, 138, 143, 149, 152, 153], "requirenamespac": 57, "rerun": 65, "res_df": 84, "res_dict": [31, 32, 35, 39, 45], "resampl": [55, 58, 60, 61, 62, 64, 65, 66, 67, 84, 86, 92, 94, 105, 106, 108, 109, 111, 120, 121, 136, 148, 150, 153], "resdat": 67, "research": [58, 60, 84, 87, 93, 120, 148, 150, 151, 153], "resembl": [61, 94], "reset": 57, "reset_index": [64, 67, 77, 84, 85], "reshap": [63, 73, 74, 75, 81, 97, 102], "reshape2": 58, "residu": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 47, 51, 92, 137, 138, 146, 147], "resolut": [60, 105], "resourc": 82, "resourcewis": 82, "respect": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 59, 64, 65, 66, 67, 71, 85, 86, 90, 104, 106, 110, 115, 119, 120, 137, 147, 153], "respons": [10, 60, 105], "rest": 106, "restart": 149, "restrict": 82, "restructur": 152, "restud": 77, "result": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 55, 56, 57, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 73, 74, 75, 77, 80, 81, 82, 88, 90, 91, 92, 93, 94, 96, 105, 120, 121, 122, 123, 124, 125, 137, 138, 143, 150, 152], "result_iivm": 59, "result_irm": 59, "result_plr": 59, "results_df": 91, "retain": [46, 47, 50, 51], "retina": 87, "retir": [59, 85, 86, 92], "return": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 56, 57, 58, 60, 61, 64, 65, 66, 67, 73, 76, 82, 83, 84, 87, 88, 89, 91, 92, 93, 94, 95, 105, 121, 137, 138, 152], "return_count": [70, 71, 82], "return_tune_r": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "return_typ": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 18, 22, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 56, 59, 60, 61, 62, 73, 81, 82, 83, 85, 86, 92, 94, 95, 96, 97, 98, 100, 101, 102, 104, 105, 106, 108, 120, 121, 136, 137, 143, 150, 153], "rev": 58, "reveal": 80, "review": [40, 77, 151], "revist": [58, 84], "reweight": [106, 107], "rf": 90, "rho": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 52, 64, 67, 71, 80, 81, 90, 92, 93, 137, 138, 143, 147, 153], "rho_val": 93, "richter": [60, 105, 148, 150], "riesz": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 64, 70, 92, 137, 138, 139, 140, 141, 142, 143, 146, 147], "riesz_rep": [137, 143], "right": [17, 19, 33, 34, 36, 40, 41, 56, 58, 73, 82, 84, 85, 86, 87, 89, 90, 93, 96, 106, 121, 122, 123, 124, 125, 136, 137, 139, 140, 141, 142, 144, 145], "rightarrow_": [56, 73, 96], "risk": [24, 103, 152], "ritov": 151, "rival": 84, "rival_ind": 84, "rmd": 57, "rmse": [57, 62, 64, 65, 66, 67, 70, 82, 83, 92, 94, 105, 106, 108, 109, 111, 121, 136, 150, 152], "rmse_dml_ml_l_fullsampl": 83, "rmse_dml_ml_l_lesstim": 83, "rmse_dml_ml_l_onfold": 83, "rmse_dml_ml_l_untun": 83, "rmse_dml_ml_m_fullsampl": 83, "rmse_dml_ml_m_lesstim": 83, "rmse_dml_ml_m_onfold": 83, "rmse_dml_ml_m_untun": 83, "rmse_g0": 70, "rmse_g1": 70, "rmse_oos_ml_l": 83, "rmse_oos_ml_m": 83, "rmse_oos_onfolds_ml_l": 83, "rmse_oos_onfolds_ml_m": 83, "rnorm": [55, 60, 97, 102, 105, 136, 150], "robin": [10, 11, 42, 58, 77, 84, 96, 148, 151], "robinson": [56, 73, 96], "robject": 84, "robu": [78, 79], "robust": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 41, 44, 57, 64, 65, 67, 71, 80, 81, 90, 92, 93, 97, 102, 106, 137, 143, 151, 152, 153], "robust_confset": [25, 91, 152], "robust_cov": 91, "robust_length": 91, "roc\u00edo": 151, "role": [4, 5, 6, 7, 8, 9, 56, 73, 83, 96, 100, 102, 153], "romano": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 136], "root": [12, 14, 15, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 70, 77, 96, 105, 121, 151], "rotat": [83, 90], "roth": [90, 106, 108, 112, 151], "rough": [93, 153], "roughli": [64, 67, 93], "round": [59, 65, 70, 71, 81, 82, 87, 93], "rout": [46, 47, 50, 51], "row": [13, 56, 59, 63, 65, 68, 74, 75, 83, 84, 88, 97, 102, 106, 109, 111, 120, 150, 153], "rownam": 58, "rowv": 58, "roxygen2": 152, "royal": [93, 151], "rpart": [59, 60, 105], "rpart_cv": 60, "rprocess": 82, "rpy2": 84, "rpy2pi": 84, "rskf": 81, "rsmp": [60, 105, 120], "rsmp_tune": [60, 105], "rssb": 93, "rtype": 23, "ruben": 151, "ruiz": [55, 72], "rule": [57, 104], "run": [8, 57, 65, 70, 90, 100, 102, 106, 149, 152], "runif": 55, "runner": [64, 66, 67, 93], "runtime_learn": 60, "runtimewarn": 67, "rv": [64, 67, 71, 80, 81, 92, 93, 137, 143, 153], "rva": [64, 67, 71, 80, 81, 92, 93, 137, 143, 153], "rvert": 77, "rvert_": 77, "s1": 83, "s2": 83, "s_": [41, 58, 84, 106, 119], "s_1": 42, "s_2": 42, "s_col": [4, 9, 61, 90, 94, 101, 102, 106], "s_i": [36, 61, 90, 94, 106], "s_x": [41, 58, 84], "safeguard": [62, 105], "sake": [59, 85, 93, 153], "same": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 35, 37, 38, 48, 56, 58, 61, 64, 65, 67, 73, 74, 75, 80, 81, 82, 84, 86, 88, 90, 91, 92, 93, 94, 105, 106, 109, 111, 121, 124, 125, 136, 137, 145, 152], "samii": 87, "sampl": [9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 36, 37, 38, 39, 41, 44, 46, 47, 50, 51, 55, 57, 58, 60, 62, 64, 65, 66, 67, 72, 78, 79, 81, 82, 84, 86, 88, 91, 92, 101, 102, 103, 105, 108, 109, 111, 112, 119, 122, 136, 137, 140, 142, 150, 151, 152], "sample_weight": [44, 46, 47, 50, 51, 90], "sant": [12, 14, 15, 17, 18, 19, 31, 35, 39, 57, 62, 64, 65, 66, 67, 106, 107, 108, 110, 112, 151], "sara": 151, "sasaki": [41, 58, 84, 151], "satisfi": [61, 65, 94, 105, 121, 136], "save": [56, 59, 65, 73, 78, 79, 82, 83, 85, 86, 105, 137, 143, 153], "savefig": 73, "saveguard": 82, "saver": [59, 85, 86], "sc": [64, 67], "scalar": 106, "scale": [17, 19, 56, 58, 63, 76, 81, 87, 89, 93, 136, 137, 140, 142, 147], "scale_color_manu": 56, "scale_fill_manu": [56, 58], "scaled_psi": 81, "scatter": [63, 70, 71, 78, 79, 87, 90, 93], "scatterplot": [70, 71], "scenario": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 64, 67, 71, 80, 81, 92, 93, 106, 137, 143, 153], "scene": [74, 75, 77], "scene_camera": 77, "schacht": [77, 82, 83], "schaefer": 87, "schedul": [97, 102, 152], "scheme": [58, 84, 105, 106, 107, 120, 148], "schneider": 60, "schratz": [60, 105, 148, 150], "scienc": [43, 55, 72, 87, 151], "scikit": [65, 82, 85, 105, 148, 150, 152, 153], "scipi": 73, "score": [0, 8, 12, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 44, 45, 46, 47, 50, 51, 55, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 70, 74, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 93, 94, 95, 100, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 143, 145, 146, 147, 148, 152, 153], "score_col": [8, 90, 100, 102, 106], "scoring_method": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "script": 149, "sd": 55, "se": [56, 58, 73, 92, 96, 105, 120, 136, 137, 143, 151, 153], "se_df": 58, "se_dml": [56, 73, 96], "se_dml_po": [56, 73, 96], "se_nonorth": [56, 73], "se_orth_nosplit": [56, 73], "se_orth_po_nosplit": [56, 73], "seaborn": [13, 16, 62, 64, 67, 68, 70, 71, 73, 82, 84, 85, 86, 93, 94], "seamlessli": 70, "search": [12, 14, 15, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 105, 121], "search_mod": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 105], "searchabl": 59, "second": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 41, 56, 58, 60, 73, 82, 83, 84, 95, 96, 120, 136, 137, 138, 147, 150], "secondari": [70, 71], "section": [15, 16, 18, 19, 57, 58, 59, 60, 63, 80, 83, 84, 86, 93, 98, 102, 107, 109, 110, 111, 112, 129, 139, 140, 152], "secur": 87, "see": [10, 11, 12, 14, 15, 19, 20, 21, 22, 24, 25, 26, 27, 28, 30, 34, 36, 37, 38, 46, 47, 48, 50, 51, 55, 57, 58, 59, 60, 62, 64, 65, 66, 67, 70, 71, 72, 74, 75, 81, 83, 84, 86, 87, 88, 90, 91, 92, 93, 105, 106, 108, 110, 112, 120, 121, 127, 129, 130, 131, 134, 135, 137, 138, 140, 142, 143, 147, 149, 150, 152], "seed": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 44, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 68, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 102, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 137, 143, 150, 153], "seek": 87, "seem": [57, 59, 80, 85, 86, 137, 153], "seen": [78, 79, 81], "sel_cols_chiang": 84, "select": [9, 17, 19, 30, 35, 36, 40, 55, 70, 77, 82, 90, 93, 95, 97, 101, 102, 103, 105, 119, 136, 150, 151, 152, 153], "selected_coef": 82, "selected_featur": [60, 105], "selected_learn": 82, "self": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 46, 47, 48, 49, 50, 51, 82, 83, 91, 153], "selfref": 59, "semenova": [74, 75, 151], "semi": 96, "semiparametr": 10, "sens": [92, 93], "sensemakr": [137, 138], "sensit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 52, 103, 104, 106, 111, 138, 143, 147, 152], "sensitivity_analysi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 64, 67, 71, 80, 81, 92, 93, 137, 143, 153], "sensitivity_benchmark": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 71, 80, 92, 93, 137, 138], "sensitivity_el": [137, 143], "sensitivity_param": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 92, 93, 137, 138, 143], "sensitivity_plot": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 71, 80, 92, 93, 137, 143], "sensitivity_summari": [64, 67, 71, 80, 81, 92, 93, 137, 143, 153], "sensitv": 81, "sensitvity_benchmark": 71, "sensiv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38], "senstiv": [137, 146], "sep": 56, "separ": [13, 70, 87, 92, 100, 101, 102, 105, 106, 120, 152], "seper": [83, 90, 92, 136, 137, 138], "seq_len": [56, 61, 96], "sequenc": 84, "sequenti": 11, "ser": [64, 65, 66, 67], "seri": [64, 65, 66, 67, 93, 151], "serv": [17, 19, 97, 99, 102, 150, 152], "serverless": [151, 152], "servic": 87, "set": [4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 41, 42, 43, 46, 49, 50, 51, 55, 56, 57, 58, 59, 61, 62, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100, 102, 104, 106, 107, 109, 110, 112, 120, 121, 122, 123, 124, 125, 126, 129, 136, 137, 138, 144, 145, 146, 149, 150, 152, 153], "set_as_param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "set_config": [46, 47, 50, 51], "set_fit_request": [50, 51], "set_fold_specif": 105, "set_index": 85, "set_ml_nuisance_param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 59, 68, 85, 105, 152], "set_param": [46, 47, 50, 51, 83, 105], "set_sample_split": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 81, 82, 120, 152], "set_score_request": [46, 47, 50, 51], "set_styl": [85, 86], "set_text": 82, "set_threshold": [56, 57, 58, 59, 60, 61, 95, 105, 106, 120, 121, 136, 150], "set_tick": 84, "set_ticklabel": 84, "set_titl": [70, 71, 81, 83, 84, 90], "set_x_d": [4, 5, 6, 7, 8, 9], "set_xlabel": [70, 71, 73, 81, 83, 84, 90], "set_xlim": 73, "set_xtick": 87, "set_xticklabel": 87, "set_ylabel": [70, 71, 81, 83, 84, 87, 90], "set_ylim": [76, 81, 83, 84, 89], "setdiff": 152, "setdiff1d": 84, "setminu": [58, 84, 136], "settings_l": 83, "settings_m": 83, "setup": [149, 152], "seven": [58, 84], "sever": [52, 59, 60, 64, 65, 66, 67, 70, 82, 83, 85, 86, 92, 93, 96, 105, 153], "shape": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 48, 49, 50, 51, 63, 64, 67, 70, 71, 74, 75, 78, 79, 82, 84, 85, 88, 90, 92, 93, 105, 106], "share": [58, 59, 84, 85], "sharma": [93, 151], "sharp": 44, "shift": [64, 67], "shock": [58, 84], "short": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 92, 93, 106, 110, 137, 138, 151, 152, 153], "shortcut": 59, "shortli": [58, 60, 84, 105], "shota": 151, "should": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 46, 47, 50, 51, 59, 61, 64, 65, 67, 71, 78, 79, 82, 85, 90, 91, 92, 94, 97, 102, 104, 105, 106, 107, 114, 136, 137, 138, 148], "show": [55, 56, 58, 61, 62, 64, 67, 68, 70, 71, 72, 73, 74, 75, 77, 80, 81, 82, 83, 84, 87, 90, 91, 93, 94, 96, 137, 146, 149], "showcas": 88, "showlabel": 93, "showlegend": 93, "shown": [55, 72, 87, 150], "showscal": [74, 75, 77], "shrink": 90, "shuffl": 120, "side": [90, 106, 137, 143], "sigma": [17, 18, 19, 30, 31, 33, 34, 35, 36, 39, 40, 41, 42, 43, 56, 58, 61, 73, 84, 94, 96, 104, 120, 136, 137, 138, 140, 142, 143, 146, 147], "sigma2": [105, 137, 143], "sigma_": [17, 18, 19, 33, 34, 36, 39, 40, 41, 42, 56, 58, 73, 84, 96], "sigma_0": [137, 147], "sigma_j": 136, "sigmoid": 87, "sign": [91, 93], "signal": [48, 49], "signatur": [25, 26, 27, 28, 29, 37, 38, 121], "signif": [55, 57, 58, 59, 60, 61, 105, 106, 120, 121, 136, 150, 153], "signific": [55, 58, 59, 60, 61, 64, 67, 71, 80, 81, 85, 88, 90, 92, 93, 105, 106, 120, 121, 136, 137, 143, 150, 153], "significantli": 70, "silverman": [27, 28, 29], "sim": [17, 18, 19, 31, 32, 33, 34, 35, 36, 39, 40, 41, 42, 43, 45, 56, 57, 58, 61, 63, 73, 76, 84, 88, 89, 94, 96, 106], "sim_data": 66, "similar": [35, 39, 57, 60, 65, 74, 75, 80, 83, 86, 90, 91, 92, 93, 106, 121, 122, 123], "similarli": [65, 83, 91], "simpl": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 50, 51, 57, 60, 74, 75, 78, 79, 80, 81, 88, 93, 103, 106, 137, 138], "simplest": 104, "simpli": [60, 62, 153], "simplic": [59, 82, 85, 88, 93], "simplif": [137, 139], "simplifi": [64, 67, 81, 87, 93, 104, 137, 146], "simul": [17, 18, 19, 31, 32, 33, 34, 35, 36, 39, 40, 42, 43, 56, 60, 61, 64, 66, 67, 73, 74, 75, 76, 77, 78, 79, 82, 83, 89, 90, 93, 94, 96, 105, 136, 150], "simul_data": 30, "simulaten": [106, 114], "simulation_run": 77, "simult": 57, "simultan": [65, 103, 153], "sin": [32, 35, 43, 63, 74, 75, 78, 79], "sinc": [31, 39, 46, 50, 59, 61, 62, 63, 71, 78, 79, 80, 82, 83, 85, 87, 94, 105, 106, 108, 137, 143, 145, 149, 152], "singl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 62, 64, 65, 66, 67, 78, 79, 86, 87, 105, 136], "single_learner_pipelin": 105, "singleton": 120, "sinh": 43, "sipp": [59, 85, 86], "site": [64, 65, 66, 67, 70, 84, 85], "situat": [58, 84], "six": [17, 19, 58], "sixth": 84, "size": [13, 16, 30, 56, 58, 59, 60, 63, 64, 65, 66, 67, 70, 73, 76, 77, 80, 82, 83, 85, 87, 88, 89, 91, 93, 95, 97, 102, 105, 106, 107, 120, 121, 136, 137, 140, 142, 150, 153], "sizeabl": 93, "skill": 151, "sklearn": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 43, 44, 46, 47, 49, 50, 51, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 90, 91, 92, 93, 94, 95, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 137, 143, 150, 153], "skotara": 93, "slide": 87, "slight": [106, 109, 111], "slightli": [12, 14, 15, 63, 64, 67, 78, 79, 80, 82, 104, 106, 110, 112, 121, 122, 123, 124, 125, 137, 138], "slow": [56, 73, 96], "slower": [56, 73, 96], "small": [32, 61, 62, 63, 70, 81, 88, 94, 106, 137, 138, 145], "smaller": [59, 62, 78, 79, 80, 83, 85, 90, 93, 106, 153], "smallest": [14, 70, 82], "smpl": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 56, 58, 73, 82, 84, 120, 121], "smpls_cluster": [58, 84], "smucler": [91, 152], "sn": [62, 64, 67, 68, 70, 71, 73, 82, 84, 85, 86, 93, 94], "so": [50, 51, 55, 59, 60, 61, 62, 64, 65, 67, 72, 83, 85, 87, 93, 94, 105, 136, 153], "social": [87, 151], "societi": [58, 84, 93, 151], "softwar": [60, 105, 148, 150, 151, 152], "solari": 152, "sole": [65, 93], "solut": [95, 104, 121], "solv": [20, 58, 84, 104, 105, 106, 109, 110, 111, 136], "solver": [85, 94, 106], "some": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 50, 51, 59, 60, 61, 62, 63, 65, 68, 82, 83, 85, 86, 90, 91, 92, 94, 104, 105, 106, 115, 149, 152], "sometim": [82, 106, 112], "sonabend": [60, 105], "sophist": 105, "sort": [13, 70, 85, 91, 106], "sort_bi": 13, "sort_valu": [70, 71], "sourc": [60, 105, 150, 152], "sourcefileload": 77, "sp": 57, "space": [58, 70, 84, 105], "spars": [77, 105, 136, 150, 151], "sparsiti": 151, "spec": 151, "special": [58, 84, 103, 106], "specialis": [100, 102], "specif": [12, 14, 15, 17, 19, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 44, 58, 59, 64, 65, 66, 67, 70, 71, 81, 82, 84, 85, 93, 97, 102, 103, 104, 105, 106, 109, 111, 120, 121, 129, 136, 143, 147, 148, 150], "specifi": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 55, 58, 59, 60, 61, 62, 64, 65, 66, 67, 70, 71, 72, 74, 75, 76, 78, 79, 81, 83, 84, 85, 86, 88, 89, 90, 92, 93, 94, 95, 97, 99, 102, 103, 104, 106, 126, 129, 149, 150, 152, 153], "specifii": 86, "speed": [16, 23, 29, 82], "speedup": 82, "spefici": 25, "spindler": [40, 77, 82, 83, 91, 93, 148, 151, 152], "spine": [85, 86], "spline": [74, 75, 104], "spline_basi": [74, 75, 104], "spline_grid": [74, 75], "split": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 55, 58, 60, 61, 62, 64, 65, 66, 67, 81, 82, 84, 86, 88, 92, 94, 103, 104, 105, 106, 108, 109, 111, 121, 136, 150, 152], "split_sampl": [81, 82], "sponsor": [59, 85, 86], "sprintf": 56, "sq_error": 77, "sqrt": [17, 18, 19, 31, 34, 35, 39, 56, 58, 60, 68, 73, 76, 84, 89, 96, 120, 136, 137, 138, 150], "squar": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 47, 51, 59, 70, 77, 85, 105, 106, 137, 147, 151], "squarederror": [59, 85, 153], "squeez": [62, 76, 89, 94], "src": 85, "ssm": [9, 36, 103, 119], "ssrn": 33, "stabil": 80, "stabl": [55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 106, 107, 148], "stack": [60, 105], "stackingclassifi": 90, "stackingregressor": 90, "stacklrn": 60, "stackrel": 106, "stage": [44, 65, 74, 75, 78, 79, 88, 90, 105, 106, 152, 153], "stagger": [106, 112], "stai": [106, 112], "standard": [16, 18, 57, 60, 64, 65, 66, 67, 76, 78, 79, 90, 91, 98, 100, 102, 106, 107, 109, 110, 111, 120, 121, 136, 137, 143, 147, 152, 153], "standard_norm": [97, 102, 105, 136, 150], "standardscal": 85, "star": 106, "start": [17, 19, 57, 59, 60, 64, 65, 66, 67, 70, 74, 75, 77, 80, 82, 83, 84, 85, 89, 93, 106, 108, 148, 153], "start_dat": [17, 19], "stat": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 73, 90, 97, 102, 105, 106, 136, 148, 151], "stat_bin": 56, "stat_dens": 59, "state": 153, "stationar": 62, "stationari": [106, 108], "statist": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 36, 37, 38, 41, 52, 58, 84, 91, 92, 93, 136, 137, 143, 148, 150, 151, 152, 153], "statsmodel": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 48, 90], "statu": [57, 59, 61, 62, 85, 87, 90, 94, 106, 112], "std": [12, 15, 24, 25, 26, 27, 28, 29, 30, 37, 38, 55, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 70, 71, 72, 74, 75, 76, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 92, 93, 94, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 150, 153], "stefan": 151, "step": [19, 56, 59, 60, 73, 78, 79, 80, 85, 88, 96, 105, 106, 136, 148, 153], "stepdown": 136, "stick": [59, 85], "still": [61, 62, 74, 75, 78, 79, 80, 86, 90, 92, 94, 105, 106, 110], "stochast": [37, 38, 106, 117, 118, 150], "stock": [59, 85, 86, 91], "store": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 70, 91, 95, 105, 120, 121, 136, 137, 143, 152], "store_model": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 83], "store_predict": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 57, 85, 88], "stori": [93, 151], "str": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 46, 47, 48, 50, 51, 59, 64, 67, 70, 71, 78, 79, 89, 90, 104, 106, 152], "straightforward": [64, 67, 78, 79, 82, 104], "strategi": [87, 93, 106, 153], "stratifi": [81, 82], "stratum": 87, "strength": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 91, 92, 93, 137, 138, 143, 146], "strftime": 64, "strictli": 106, "string": [12, 13, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 37, 38, 104, 136, 137, 143, 150, 152], "string_label": 87, "strong": [61, 91, 94, 137, 138], "stronger": [91, 106, 110, 136, 153], "structur": [10, 11, 16, 17, 19, 42, 58, 59, 61, 65, 77, 84, 85, 91, 94, 96, 105, 148, 151, 153], "student": 151, "studi": [36, 58, 59, 77, 82, 83, 84, 85, 86, 91, 92, 106, 107, 150, 153], "style": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 64, 67, 83, 152], "styler": 152, "styliz": 93, "sub": [46, 47, 50, 51, 58, 84], "subclass": [99, 102, 152], "subfold": 105, "subgroup": [25, 59, 85, 152], "subject": [58, 84], "submiss": 152, "submit": [64, 67], "submodul": 152, "subobject": [46, 47, 50, 51], "subplot": [58, 63, 70, 71, 73, 74, 75, 76, 78, 79, 81, 82, 83, 84, 85, 86, 87, 89, 90], "subplots_adjust": 82, "subpopul": [106, 119], "subsampl": [60, 82, 121, 123], "subscript": [106, 110, 121, 123, 125, 137, 138], "subsequ": [58, 70, 84], "subset": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 46, 50, 58, 82, 84, 88, 95, 104, 105, 137, 138, 140, 142], "subseteq": 104, "substanti": [59, 85, 87], "substract": 136, "subtract": 136, "sudo": 149, "suffic": 93, "suffici": [82, 83, 93], "suggest": [58, 59, 84, 85, 93, 152], "suitabl": [61, 74, 75, 94, 106, 110], "sum": [47, 51, 58, 59, 84, 85, 86, 89, 90, 104, 136], "sum_": [17, 19, 45, 56, 58, 73, 84, 90, 95, 96, 104, 106, 107, 112, 136], "sum_i": 87, "sum_oth": 84, "sum_riv": 84, "summar": [13, 57, 64, 65, 66, 67, 70, 71, 87, 93, 95, 137, 143], "summari": [12, 15, 24, 25, 26, 27, 28, 29, 30, 37, 38, 55, 57, 58, 60, 61, 62, 64, 65, 66, 67, 68, 71, 72, 74, 75, 76, 78, 79, 80, 81, 84, 86, 89, 91, 92, 93, 94, 96, 97, 99, 102, 104, 105, 106, 107, 108, 109, 111, 120, 121, 136, 137, 150, 152, 153], "summary_df": 91, "summary_result": 59, "summary_stat": 70, "superior": 70, "suppli": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 74, 75, 78, 79, 80, 88, 97, 102, 104, 137, 138, 143, 144], "support": [25, 32, 44, 57, 58, 64, 65, 66, 67, 82, 84, 88, 90, 101, 102, 105, 106, 115, 153], "support_s": [32, 74, 75, 78, 79, 88], "support_t": 88, "support_w": 88, "suppos": 93, "suppress": [57, 59, 60, 61], "suppresswarn": 56, "suprema": 136, "suptitl": [76, 82, 83, 86, 89], "supxlabel": [76, 86, 89], "supylabel": [76, 86, 89], "sure": [70, 71, 105, 152], "surfac": [74, 75, 77], "surgic": 91, "surpress": [58, 150], "survei": [59, 85, 86, 153], "susan": 151, "sven": [93, 148, 151], "svenklaassen": [148, 152], "svg": [56, 73], "switch": [56, 73, 93, 96], "symbol": 93, "symmetr": 43, "syntax": [90, 106], "syntaxwarn": 84, "synthesi": 151, "synthet": [17, 19, 32, 45, 55, 70, 72, 74, 75, 76, 78, 79, 83, 88, 89, 91], "syrgkani": [91, 93, 151], "system": 151, "szita": 151, "t": [4, 5, 7, 12, 13, 15, 16, 17, 18, 19, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 44, 46, 47, 50, 51, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 74, 75, 76, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 98, 99, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 120, 121, 122, 123, 136, 137, 139, 140, 150, 153], "t_": [16, 64, 65, 66, 67, 106, 109, 110, 111, 121, 123, 125], "t_1_start": 82, "t_1_stop": 82, "t_2_start": 82, "t_2_stop": 82, "t_3_start": 82, "t_3_stop": 82, "t_col": [4, 5, 7, 15, 16, 64, 65, 66, 67, 98, 99, 102, 106, 107, 108, 109, 111], "t_df": 88, "t_diff": 63, "t_dml": 56, "t_g": [17, 19], "t_i": [62, 88, 90, 106, 108, 109, 112, 121, 123], "t_idx": 63, "t_nonorth": 56, "t_orth_nosplit": 56, "t_sigmoid": 88, "t_stat": 136, "t_value_ev": 14, "t_value_pr": 14, "tabl": [56, 58, 59, 60, 61, 70, 71, 91, 95, 97, 102, 105, 106, 120, 121, 136, 150, 153], "tabpfnclassifi": 70, "tabpfnregressor": 70, "tabular": [70, 82, 97, 102, 136, 150, 153], "taddi": 151, "tailor": [98, 102], "takatsu": 91, "take": [25, 26, 31, 32, 37, 38, 39, 61, 62, 63, 64, 65, 66, 67, 74, 75, 76, 77, 78, 79, 82, 86, 89, 90, 91, 92, 94, 95, 104, 105, 106, 107, 112, 115, 116, 117, 118, 121, 126, 129, 137, 144, 145, 146, 150], "taken": [59, 85, 86, 153], "taker": [25, 152], "talk": 153, "target": [12, 14, 15, 20, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 50, 51, 55, 58, 59, 60, 61, 74, 75, 82, 84, 104, 105, 106, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 121, 130, 131, 136, 137, 145, 147, 148, 150, 152, 153], "task": [55, 70, 83, 97, 102, 120, 153], "task_typ": 152, "tau": [45, 63, 76, 86, 87, 89, 90, 104, 106, 121, 127, 130, 131], "tau_": [87, 90, 106], "tau_0": [90, 106], "tau_1": 87, "tau_2": 87, "tau_vec": [76, 86, 89], "tax": [59, 85, 86], "te": [57, 74, 75, 88], "techniqu": [56, 73, 96, 120, 153], "teen": 65, "templat": 152, "ten": 83, "tend": [59, 85, 86, 106], "tensor": [74, 75], "tenth": 151, "term": [7, 14, 56, 58, 59, 60, 63, 73, 77, 84, 85, 87, 93, 96, 106, 112, 148, 153], "termin": [60, 105], "terminatorev": 60, "test": [11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 37, 38, 46, 47, 50, 51, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 73, 80, 84, 91, 93, 96, 105, 106, 120, 121, 136, 150, 151, 152, 153], "test_id": [58, 120], "test_ind": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38], "test_set": 120, "test_siz": 73, "text": [16, 17, 18, 19, 31, 33, 35, 39, 44, 45, 58, 59, 64, 65, 66, 67, 70, 76, 77, 87, 88, 89, 90, 93, 104, 106, 109, 110, 111, 112, 120, 121, 123, 125, 137, 140, 142], "textbf": [95, 105, 153], "textposit": 93, "textrm": [137, 138, 144, 145, 146, 147], "tg": [60, 68, 97, 102, 150], "th": [58, 84], "than": [26, 56, 57, 59, 73, 77, 81, 82, 85, 86, 87, 90, 91, 92, 93, 96, 106, 110, 137, 143, 153], "thank": [57, 59, 60, 85, 152], "thatw": 63, "thei": [57, 59, 63, 78, 79, 85, 87, 91, 97, 102, 106, 137, 147], "them": [13, 59, 60, 74, 75, 76, 80, 83, 85, 89, 106], "theme": [58, 59], "theme_minim": [56, 59, 61], "theorem": [106, 112, 137, 147], "theoret": [82, 93, 120, 151], "theori": [104, 151], "therebi": [58, 60, 84, 153], "therefor": [64, 66, 67, 71, 87, 90, 92, 120, 121, 137, 146], "theta": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 41, 43, 56, 58, 60, 61, 62, 63, 64, 67, 70, 71, 73, 77, 80, 81, 82, 84, 90, 92, 93, 94, 95, 96, 97, 102, 104, 105, 106, 107, 109, 110, 111, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 143, 146, 147, 150, 153], "theta_": [17, 19, 64, 67, 71, 90, 93, 104, 106, 113, 114, 136, 137, 147], "theta_0": [25, 26, 32, 37, 38, 56, 58, 59, 61, 71, 73, 74, 75, 77, 78, 79, 84, 85, 93, 94, 96, 104, 106, 108, 115, 116, 117, 118, 119, 121, 130, 131, 133, 136, 137, 144, 145, 147, 150], "theta_d": 70, "theta_dml": [56, 73, 96], "theta_dml_po": [56, 73, 96], "theta_initi": 73, "theta_nonorth": [56, 73], "theta_orth_nosplit": [56, 73], "theta_orth_po_nosplit": [56, 73], "theta_resc": 56, "theta_t": 63, "thi": [4, 12, 13, 14, 15, 16, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 46, 47, 49, 50, 51, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 104, 105, 106, 110, 112, 113, 114, 115, 116, 120, 121, 122, 123, 124, 125, 127, 129, 130, 131, 136, 137, 138, 143, 144, 145, 148, 149, 150, 151, 152, 153], "think": 60, "third": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 56, 65, 66, 73, 84, 96, 120], "thirion": [148, 150], "this_df": [77, 85], "this_split_ind": 84, "those": [57, 59, 65, 85, 86, 91], "though": [55, 72, 87], "thread": [87, 105], "three": [58, 60, 78, 79, 149, 152], "threshold": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 90, 93, 105, 106], "through": [57, 65, 76, 78, 79, 89, 90, 97, 102, 105, 106], "throughout": [80, 91], "thu": [83, 90, 104, 106], "tibbl": 57, "tick": 16, "tick_param": 90, "tight": 73, "tight_layout": [64, 67, 83, 84, 90], "tighter": 90, "tild": [17, 18, 19, 31, 35, 39, 58, 84, 87, 95, 104, 120, 121, 123, 125, 130, 131, 134, 135, 136, 137, 146, 147], "tile": 91, "time": [5, 7, 12, 14, 16, 17, 19, 40, 41, 56, 57, 58, 59, 61, 62, 63, 65, 73, 77, 78, 79, 84, 85, 86, 90, 92, 93, 94, 98, 99, 102, 106, 107, 108, 109, 110, 111, 112, 121, 137, 151, 152, 153], "time_budget": 83, "time_df": 63, "time_period": 63, "time_typ": [17, 19, 64, 67], "titiunik": [106, 151], "titl": [13, 16, 58, 59, 61, 64, 65, 67, 70, 71, 74, 75, 76, 77, 78, 79, 82, 83, 84, 85, 86, 87, 89, 90, 93, 148], "title_fonts": [64, 67], "tmp": 84, "tname": 57, "tnr": [60, 105], "to_datetim": 64, "to_fram": 88, "to_numpi": [76, 80, 86, 89], "to_str": 70, "todo": [58, 68], "toeplitz": 77, "togeth": [78, 79, 101, 102, 136], "toler": 84, "tomasz": [151, 152], "toml": 152, "too": 82, "tool": [57, 60, 92, 153], "top": [58, 82, 84, 85, 86, 90, 93, 106, 148], "total": [47, 51, 59, 83, 85, 106, 107], "total_width": 70, "tpot": 83, "track": [98, 100, 102], "tracker": 148, "tradit": [70, 136], "train": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 50, 51, 56, 58, 60, 70, 73, 74, 75, 76, 78, 79, 81, 82, 84, 85, 88, 89, 95, 96, 120], "train_id": [58, 120], "train_ind": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38], "train_set": 120, "train_test_split": 73, "transact": 151, "transform": [31, 39, 45, 70, 81, 87, 93, 153], "translat": [70, 77], "transpos": 63, "treament": 88, "treat": [16, 17, 18, 19, 26, 57, 62, 63, 64, 65, 66, 67, 71, 80, 88, 90, 93, 104, 106, 108, 109, 111, 112, 116, 121, 123, 125, 136, 153], "treat1_param": 87, "treat2_param": 87, "treat_var": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 105], "treatment": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 42, 44, 55, 57, 58, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 72, 77, 80, 82, 83, 84, 88, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 122, 123, 124, 125, 126, 127, 129, 130, 131, 136, 140, 142, 143, 144, 146, 148, 150, 151, 152, 153], "treatment_df": 63, "treatment_effect": [32, 74, 75], "treatment_level": [22, 23, 70, 71, 81, 106], "treatment_var": [4, 5, 6, 7, 8, 9], "tree": [26, 49, 59, 60, 62, 63, 64, 67, 82, 85, 95, 103, 105, 106, 120, 121, 136, 150, 152], "tree_param": [26, 49], "tree_summari": 85, "trees_class": [59, 85], "trend": [57, 62, 63, 64, 67, 84, 106, 108, 110, 112, 151], "tri": [77, 137, 138], "triangular": [44, 90, 106], "trim": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 59, 85, 86, 93], "trimming_rul": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 86], "trimming_threshold": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 59, 74, 81, 85, 86, 88, 89, 93], "trm": [60, 105], "true": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 36, 37, 38, 44, 45, 46, 47, 50, 51, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 102, 105, 106, 108, 120, 121, 126, 127, 130, 131, 134, 135, 136, 137, 139, 140, 141, 142, 147, 150, 153], "true_effect": [63, 74, 75, 78, 79, 91], "true_gatet_effect": 80, "true_group_effect": 80, "true_tau": 90, "truemfunct": 91, "truncat": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 86], "try": [82, 92], "tune": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 70, 77, 82, 90, 103, 106, 148, 150, 152], "tune_on_fold": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 105], "tune_r": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "tune_set": [60, 105], "tuned_model": 83, "tuner": 105, "tunergridsearch": 60, "tupl": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 64, 67, 106, 109, 110, 111], "turn": 93, "turrel": 43, "tutori": 59, "tw": [85, 86], "twice": 106, "twinx": [70, 71], "two": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 55, 56, 59, 60, 62, 64, 65, 66, 67, 72, 73, 76, 81, 82, 83, 85, 86, 87, 88, 89, 91, 92, 93, 95, 96, 104, 105, 108, 111, 120, 130, 136, 153], "twoclass": 60, "twoearn": [59, 85, 86, 92, 153], "type": [7, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 56, 57, 58, 59, 60, 64, 67, 73, 82, 83, 84, 90, 93, 96, 103, 105, 106, 121, 132, 133, 136, 137, 146, 152, 153], "typeerror": [64, 65, 66, 67], "typic": [70, 106, 112, 148], "u": [18, 25, 26, 27, 28, 29, 31, 32, 34, 36, 39, 47, 51, 56, 57, 58, 59, 62, 63, 64, 65, 67, 70, 71, 73, 76, 78, 79, 81, 82, 84, 85, 86, 88, 89, 91, 92, 93, 96, 106, 113, 115, 116, 137, 138, 149, 153], "u_hat": [56, 73, 121], "u_i": [33, 36, 40, 43], "u_t": 18, "uehara": 151, "uhash": 60, "ulf": 151, "unambigu": 93, "uncertainti": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 78, 79, 81, 90, 92, 137, 143, 153], "unchang": [46, 47, 50, 51], "uncondit": [59, 64, 67, 85, 153], "unconfounded": [93, 151], "under": [25, 30, 56, 59, 62, 65, 73, 85, 88, 90, 93, 96, 106, 110, 112, 119, 136, 151], "underbrac": [56, 63, 73, 96, 104], "underfit": 83, "underli": [31, 35, 59, 60, 64, 67, 70, 71, 78, 79, 87, 88, 106, 112, 137, 138, 153], "underlin": [58, 84], "underset": [90, 106], "understand": [64, 65, 67, 70, 93], "undesir": 105, "unevenli": 120, "unifi": 102, "uniform": [18, 44, 45, 63, 72, 74, 75, 76, 88, 89, 136], "uniform_averag": [47, 51], "uniformli": [25, 64, 67, 76, 86, 136], "union": 25, "uniqu": [7, 55, 64, 65, 66, 67, 70, 71, 72, 82, 90, 98, 99, 102, 106, 109, 111, 121, 137, 147], "unique_label": 83, "unit": [7, 17, 19, 56, 57, 61, 62, 63, 64, 65, 66, 67, 80, 90, 94, 99, 102, 106, 108, 109, 110, 111, 112, 121, 122, 123, 124, 125, 137, 140, 142, 152], "univari": [32, 74, 75], "univers": [16, 151], "unknown": 106, "unlik": [59, 85, 86, 93], "unobserv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 55, 59, 64, 67, 72, 85, 86, 92, 93, 106, 137, 138, 147, 153], "unpen": 57, "unstabl": [137, 138], "unter": [58, 59, 60], "untest": 93, "until": [106, 108, 152], "untreat": [93, 106, 108], "untun": 70, "up": [16, 23, 29, 59, 77, 82, 83, 85, 86, 92, 93, 105, 106, 108, 120, 137, 138, 149, 152, 153], "upcom": 152, "updat": [46, 47, 50, 51, 58, 84, 85, 151, 152], "update_layout": [74, 75, 77, 90, 93], "update_trac": [74, 75], "upload": 152, "upon": [121, 152], "upper": [25, 59, 60, 63, 64, 67, 70, 71, 73, 76, 80, 81, 86, 89, 90, 92, 93, 105, 137, 143, 147, 153], "upper_bound": [74, 75], "upsilon": [61, 94], "upsilon_i": [61, 94], "upward": [59, 85, 86, 93], "upweight": 87, "url": [65, 77, 148, 151], "us": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 50, 51, 56, 58, 59, 61, 62, 63, 64, 65, 66, 67, 70, 71, 73, 74, 75, 76, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 102, 104, 106, 107, 109, 111, 120, 121, 122, 123, 124, 125, 136, 137, 138, 143, 145, 146, 147, 148, 149, 150, 152, 153], "usa": 151, "usabl": 82, "usag": [57, 64, 65, 66, 67, 68, 70, 71, 80, 84, 85, 86, 92, 97, 150, 152], "use_label_encod": [85, 153], "use_other_treat_as_covari": [4, 5, 6, 7, 8, 9, 97, 102], "use_pred_offset": 105, "use_weight": 105, "usecolormap": [74, 75], "user": [20, 21, 46, 47, 50, 51, 56, 57, 58, 59, 60, 65, 70, 71, 73, 80, 81, 82, 84, 85, 90, 92, 104, 105, 106, 121, 136, 148, 149, 150, 152, 153], "userwarn": [64, 66, 67, 70, 85, 93], "usual": [58, 62, 64, 65, 66, 67, 70, 74, 75, 82, 84, 90, 92, 93, 104, 105, 120, 137, 147], "util": [0, 21, 70, 81, 82, 83, 87, 90, 105, 106, 152], "v": [10, 11, 25, 26, 34, 36, 37, 38, 40, 41, 42, 47, 51, 56, 58, 59, 64, 67, 70, 71, 73, 80, 81, 82, 83, 84, 85, 87, 90, 91, 95, 96, 104, 106, 113, 115, 116, 117, 118, 136, 148, 150, 151, 152, 153], "v108": 148, "v12": [148, 150], "v22": 60, "v23": 148, "v_": [41, 58, 84, 106], "v_i": [33, 34, 36, 42, 43, 56, 73, 96, 106], "v_j": 136, "val": [34, 64, 65, 66, 67, 120, 151], "val_list": 77, "valid": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 33, 37, 38, 56, 57, 58, 59, 62, 65, 70, 73, 76, 82, 83, 84, 85, 86, 89, 96, 103, 104, 105, 120, 121, 127, 130, 131, 137, 138, 151, 153], "valu": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 50, 51, 52, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 90, 91, 92, 93, 95, 97, 98, 99, 102, 103, 105, 106, 107, 112, 113, 119, 120, 127, 130, 131, 134, 135, 136, 137, 138, 143, 147, 150, 152, 153], "value_count": 85, "van": 151, "vanderpla": [148, 150], "vanish": [56, 73, 96], "var": [17, 18, 19, 31, 35, 39, 58, 84, 87, 90, 137, 138, 144, 145, 146, 147], "var_ep": 93, "varepsilon": [25, 31, 39, 41, 58, 61, 84, 94, 104, 106, 115], "varepsilon_": [17, 19, 41, 58, 64, 67, 84], "varepsilon_0": 18, "varepsilon_1": 18, "varepsilon_d": [35, 39], "varepsilon_i": [35, 40, 61, 76, 89, 94], "vari": [17, 19, 59, 63, 64, 67, 82, 85, 87, 93], "variabl": [4, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 44, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 70, 71, 77, 80, 83, 84, 85, 86, 90, 92, 93, 94, 97, 98, 99, 100, 101, 102, 104, 105, 106, 108, 109, 111, 112, 113, 115, 116, 117, 118, 120, 121, 136, 137, 138, 143, 147, 150, 151, 152, 153], "varianc": [20, 21, 58, 60, 84, 90, 92, 93, 103, 106, 120, 137, 138, 143, 145, 146, 147, 150, 152], "variant": [57, 65, 81], "variat": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 81, 92, 137, 138, 147], "variou": [57, 83, 93, 105, 153], "varoquaux": [148, 150], "vasili": [93, 151], "vast": 70, "vector": [12, 14, 15, 17, 19, 22, 24, 25, 26, 27, 28, 30, 32, 33, 34, 36, 37, 38, 40, 41, 43, 55, 58, 59, 61, 62, 72, 78, 79, 80, 84, 85, 88, 91, 94, 106, 108, 117, 118, 119, 136, 150, 152], "vee": [121, 123, 125], "venv": 149, "verbos": [59, 62, 63, 64, 67, 70, 73, 76, 82, 83, 86, 89, 90, 93], "veri": [57, 58, 60, 65, 80, 82, 84, 91, 93, 121, 148], "verifi": 87, "versa": [82, 87, 137, 143], "version": [4, 31, 46, 47, 50, 51, 58, 59, 60, 62, 63, 93, 95, 97, 102, 104, 106, 109, 110, 111, 121, 136, 137, 139, 140, 141, 142, 144, 145, 148, 152], "versoin": 93, "vertic": [58, 70, 71, 84], "via": [12, 14, 15, 18, 21, 22, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 57, 61, 62, 63, 64, 65, 66, 67, 76, 77, 78, 79, 80, 81, 82, 84, 90, 92, 94, 95, 97, 102, 103, 104, 105, 106, 107, 108, 109, 111, 120, 127, 135, 136, 137, 138, 143, 147, 148, 149, 150, 151, 152, 153], "viabl": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38], "vice": [82, 87, 137, 143], "victor": [77, 93, 120, 148, 151], "vignett": [57, 152], "villa": [55, 72], "violat": [64, 67], "violet": [76, 86, 89], "vira": 151, "virtual": [65, 149], "virtualenv": 149, "visibl": [86, 90, 93], "visit": [148, 153], "visual": [13, 58, 64, 65, 66, 67, 80, 81, 83, 84, 90], "vmax": 67, "vmin": 67, "vol": 57, "volum": [93, 148], "voluntari": 87, "vv740": 84, "vv760g": 84, "w": [10, 11, 17, 18, 19, 20, 21, 31, 39, 42, 46, 47, 50, 51, 58, 77, 84, 87, 88, 91, 95, 96, 106, 109, 110, 111, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150], "w24678": 120, "w30302": 151, "w_": [17, 18, 19, 58, 84, 88, 106], "w_1": [17, 18, 19, 88], "w_2": [17, 18, 19, 88], "w_3": [17, 18, 19], "w_4": [17, 18, 19], "w_df": 88, "w_i": [36, 62, 88, 90, 95, 104, 106, 120, 121, 123, 125, 136], "wa": [58, 63, 83, 84, 93, 152], "wage": 65, "wager": 151, "wai": [59, 82, 83, 85, 91, 93, 105, 121, 149], "wander": 43, "wang": 151, "want": [55, 58, 59, 60, 62, 72, 76, 82, 84, 89, 90, 105, 106, 148, 149, 151], "warn": [13, 16, 55, 56, 57, 58, 59, 60, 61, 64, 66, 67, 70, 73, 82, 85, 93, 95, 105, 106, 120, 121, 136, 150, 152], "wayon": 58, "we": [26, 49, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 102, 104, 105, 106, 107, 109, 110, 111, 112, 116, 120, 121, 123, 125, 128, 136, 137, 138, 147, 149, 150, 152, 153], "weak": [25, 137, 138, 151, 152], "weakest": [64, 67], "wealth": [10, 92], "websit": [59, 60, 65, 105, 148], "wedg": [58, 84], "week": 152, "wei": 136, "weight": [13, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 46, 47, 50, 51, 58, 59, 60, 61, 64, 65, 66, 67, 70, 71, 80, 81, 84, 85, 90, 94, 103, 105, 106, 107, 121, 126, 129, 136, 137, 144, 145, 152], "weights_bar": [22, 26, 81], "weights_dict": 81, "weiss": [148, 150], "well": [5, 6, 7, 8, 9, 50, 51, 56, 58, 73, 77, 82, 83, 84, 91, 95, 96, 120, 149, 150], "were": [59, 61, 85, 86, 94, 153], "what": [57, 77, 82, 151], "when": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 50, 51, 59, 62, 65, 70, 81, 85, 87, 91, 106, 116, 119, 121, 136, 148, 149, 150, 152], "whenev": [59, 85], "whera": [137, 145], "where": [12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 47, 48, 49, 51, 55, 56, 58, 59, 61, 62, 63, 64, 65, 66, 67, 71, 72, 73, 76, 80, 84, 85, 87, 88, 89, 90, 91, 93, 94, 95, 96, 104, 105, 106, 107, 108, 110, 111, 112, 113, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 143, 144, 145, 147, 149, 150, 152, 153], "wherea": [32, 61, 62, 64, 67, 71, 91, 93, 94, 106, 109, 111, 121, 129, 137, 144, 153], "whether": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 36, 37, 38, 44, 48, 59, 63, 82, 85, 86, 90, 91, 93, 97, 102, 105, 106, 137, 138, 152], "which": [5, 6, 7, 8, 9, 12, 14, 15, 16, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 46, 50, 55, 56, 57, 59, 60, 61, 62, 64, 65, 66, 67, 69, 70, 71, 72, 73, 77, 80, 82, 83, 85, 86, 88, 90, 92, 93, 94, 96, 97, 102, 104, 105, 106, 110, 121, 136, 137, 138, 143, 144, 145, 147, 149, 152, 153], "while": [55, 72, 106], "white": [58, 78, 79, 84, 93], "whitegrid": [85, 86], "whitnei": [93, 151], "who": [57, 59, 85, 93], "whole": [56, 62, 73, 90, 96, 105, 121, 122, 137, 138], "whom": 106, "why": [65, 70], "widehat": [64, 65, 66, 67, 106], "width": [13, 16, 56, 58, 70, 74, 75, 77], "wiki": 152, "wiksel": 151, "wild": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 136], "window": 149, "wise": [78, 79], "wish": 149, "within": [44, 58, 64, 65, 66, 67, 70, 78, 79, 84, 88, 90], "without": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 44, 55, 56, 64, 65, 66, 67, 70, 72, 73, 82, 83, 93, 96, 103, 105, 106, 137, 138, 149, 152], "wolf": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 136], "won": 93, "word": [44, 90, 106, 152, 153], "work": [46, 47, 50, 51, 64, 65, 66, 67, 69, 70, 71, 80, 82, 87, 92, 93, 105, 106, 136, 149, 151], "workflow": [148, 152], "workspac": 85, "world": 151, "worri": 93, "wors": [47, 51], "would": [47, 51, 57, 59, 60, 64, 65, 66, 67, 74, 75, 77, 82, 85, 86, 90, 92, 93, 104, 105, 137, 147, 153], "wrapper": [4, 57, 90, 97, 102, 105], "wright": 91, "write": [56, 57, 61, 62, 73, 94, 96, 137, 147], "written": [106, 121, 137, 144, 145], "wrong": [82, 87], "wspace": 82, "wurd": [58, 59, 60], "www": [148, 149], "x": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 24, 25, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 51, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 100, 101, 102, 104, 105, 106, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 144, 145, 146, 147, 150, 153], "x0": [70, 71, 87, 90], "x1": [58, 60, 61, 62, 70, 71, 81, 83, 84, 87, 90, 92, 93, 94, 97, 100, 101, 102, 104, 105, 106, 121, 136, 137, 138, 150], "x10": [58, 60, 61, 81, 83, 84, 94, 97, 101, 102, 105, 106, 121, 136, 150], "x100": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x11": [58, 60, 61, 84, 94, 97, 101, 102, 105, 106, 121, 136, 150], "x12": [58, 60, 61, 84, 94, 97, 101, 102, 105, 106, 121, 136, 150], "x13": [58, 60, 61, 84, 94, 97, 101, 102, 105, 106, 121, 136, 150], "x14": [58, 60, 61, 84, 94, 97, 101, 102, 105, 106, 121, 136, 150], "x15": [58, 60, 61, 84, 94, 97, 101, 102, 105, 106, 121, 136, 150], "x16": [58, 60, 61, 84, 94, 97, 101, 102, 105, 106, 121, 136, 150], "x17": [58, 60, 61, 84, 94, 97, 101, 102, 105, 106, 121, 136, 150], "x18": [58, 60, 61, 84, 94, 97, 101, 102, 105, 106, 121, 136, 150], "x19": [58, 60, 61, 84, 94, 97, 101, 102, 105, 106, 121, 136, 150], "x1x2x3x4x5x6x7x8x9x10": 58, "x2": [58, 60, 61, 62, 70, 71, 81, 83, 84, 90, 92, 93, 94, 97, 100, 101, 102, 104, 105, 106, 121, 136, 150], "x20": [58, 60, 61, 84, 94, 97, 101, 102, 105, 106, 121, 136, 150], "x21": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x22": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x23": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x24": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x25": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x26": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x27": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x28": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x29": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x2_dummi": 93, "x2_preds_control": 93, "x2_preds_treat": 93, "x3": [58, 60, 61, 62, 70, 71, 81, 83, 84, 92, 93, 94, 97, 100, 101, 102, 104, 105, 106, 121, 136, 150], "x30": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x31": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x32": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x33": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x34": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x35": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x36": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x37": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x38": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x39": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x4": [58, 60, 61, 62, 70, 71, 81, 83, 84, 92, 93, 94, 97, 101, 102, 105, 106, 121, 136, 150], "x40": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x41": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x42": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x43": [58, 60, 61, 83, 84, 94, 97, 101, 102, 106, 150], "x44": [58, 60, 61, 83, 84, 94, 97, 101, 102, 106, 150], "x45": [58, 60, 61, 83, 84, 94, 97, 101, 102, 106, 150], "x46": [58, 60, 61, 83, 84, 94, 97, 101, 102, 106, 150], "x47": [58, 60, 61, 83, 84, 94, 97, 101, 102, 106, 150], "x48": [58, 60, 61, 83, 84, 94, 97, 101, 102, 106, 150], "x49": [58, 60, 61, 83, 84, 94, 97, 101, 102, 106, 150], "x5": [58, 60, 61, 81, 83, 84, 93, 94, 97, 101, 102, 105, 106, 121, 136, 150], "x50": [58, 60, 61, 83, 84, 94, 97, 101, 102, 106, 150], "x51": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x52": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x53": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x54": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x55": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x56": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x57": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x58": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x59": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x6": [58, 60, 61, 81, 83, 84, 94, 97, 101, 102, 105, 106, 121, 136, 150], "x60": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x61": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x62": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x63": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x64": [58, 60, 61, 64, 65, 66, 67, 84, 85, 94, 97, 101, 102, 106, 150], "x65": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x66": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x67": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x68": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x69": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x7": [58, 60, 61, 81, 83, 84, 94, 97, 101, 102, 105, 106, 121, 136, 150], "x70": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x71": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x72": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x73": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x74": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x75": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x76": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x77": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x78": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x79": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x8": [58, 60, 61, 81, 83, 84, 94, 97, 101, 102, 105, 106, 121, 136, 150], "x80": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x81": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x82": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x83": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x84": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x85": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x86": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x87": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x88": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x89": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x9": [58, 60, 61, 81, 83, 84, 94, 97, 101, 102, 105, 106, 121, 136, 150], "x90": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x91": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x92": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x93": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x94": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x95": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x96": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x96x97x98x99x100ydcluster_var_icluster_var_jz": 58, "x97": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x98": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x99": [58, 60, 61, 84, 94, 97, 101, 102, 106, 150], "x_": [17, 19, 41, 42, 56, 58, 63, 73, 84, 93, 96], "x_0": [63, 74, 75, 78, 79, 80], "x_1": [17, 18, 19, 31, 35, 37, 38, 39, 63, 74, 75, 76, 78, 79, 80, 89, 93, 106, 117, 118, 137, 138, 150], "x_1x_3": [76, 89], "x_2": [17, 18, 19, 31, 35, 39, 63, 74, 75, 76, 78, 79, 80, 89, 93, 137, 138], "x_3": [17, 18, 19, 31, 35, 39, 63, 74, 75, 78, 79, 80, 137, 138], "x_4": [17, 18, 19, 31, 35, 39, 74, 75, 76, 78, 79, 80, 89], "x_5": [31, 35, 39, 74, 75, 78, 79], "x_6": [74, 75, 78, 79], "x_7": [74, 75, 78, 79], "x_8": [74, 75, 78, 79], "x_9": [74, 75, 78, 79], "x_binary_control": 93, "x_binary_tr": 93, "x_center": 70, "x_col": [4, 5, 6, 7, 8, 9, 16, 55, 58, 59, 60, 64, 65, 66, 67, 72, 77, 84, 85, 86, 88, 90, 91, 92, 93, 97, 98, 99, 102, 105, 106, 107, 109, 111, 150, 152, 153], "x_cols_bench": 93, "x_cols_binari": 93, "x_cols_poli": 84, "x_conf": 89, "x_conf_tru": 89, "x_df": 63, "x_domain": 60, "x_end": 70, "x_i": [32, 33, 34, 36, 40, 42, 43, 45, 56, 61, 62, 73, 76, 78, 79, 87, 89, 90, 94, 96, 104, 106, 108, 109, 111, 112, 119, 121, 123, 125], "x_jitter": 70, "x_p": [37, 38, 106, 117, 118, 150], "x_rang": 70, "x_start": 70, "x_train": 83, "x_true": [76, 89], "x_var": 60, "xaxis_titl": [74, 75, 77, 90, 93], "xformla": 57, "xgb": 83, "xgb_untuned_l": 83, "xgb_untuned_m": 83, "xgbclassifi": [82, 85, 87, 153], "xgboost": [56, 59, 82, 85, 87, 153], "xgbregressor": [82, 83, 85, 87, 153], "xi": [17, 18, 19, 35, 106], "xi_": 136, "xi_0": [41, 58, 84], "xi_i": [61, 94], "xiaoji": 151, "xintercept": [56, 61], "xlab": [56, 58, 59], "xlabel": [63, 64, 65, 67, 70, 71, 74, 75, 76, 78, 79, 83, 85, 86, 89], "xlim": [56, 59, 70], "xmax": 70, "xmax_rel": 70, "xmin": 70, "xmin_rel": 70, "xtick": [70, 71, 83], "xval": [60, 105], "xx": 73, "y": [4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 18, 22, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 51, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 111, 113, 115, 116, 117, 118, 120, 121, 122, 123, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 143, 144, 145, 146, 147, 150, 153], "y0": [57, 64, 67, 70, 71, 76, 89], "y0_cvar": 76, "y0_quant": [76, 89], "y1": [57, 64, 67, 76, 89], "y1_cvar": 76, "y1_quant": [76, 89], "y_": [16, 17, 19, 41, 58, 61, 62, 63, 64, 67, 84, 94, 106, 108, 109, 111, 112, 119, 121, 123, 125], "y_0": [12, 14, 18, 45, 121, 124], "y_1": [12, 14, 18, 45, 121, 124], "y_col": [4, 5, 6, 7, 8, 9, 16, 55, 56, 58, 59, 60, 61, 64, 65, 66, 67, 72, 74, 75, 77, 78, 79, 81, 84, 85, 86, 88, 90, 91, 92, 95, 96, 97, 98, 99, 101, 102, 105, 106, 107, 109, 111, 120, 121, 150, 152, 153], "y_df": [63, 88], "y_diff": 63, "y_i": [32, 33, 34, 36, 40, 42, 43, 56, 61, 62, 73, 76, 87, 88, 89, 90, 94, 96, 106, 108, 119], "y_label": [13, 16], "y_lower_quantil": [64, 67], "y_mean": [64, 67], "y_pred": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 47, 51, 82, 105], "y_train": 83, "y_true": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 47, 51, 82, 105], "y_upper_quantil": [64, 67], "ya": 151, "yasui": 151, "yata": 151, "yaxis_titl": [74, 75, 77, 90, 93], "year": [65, 148], "yerr": [63, 70, 71, 78, 79, 83, 85, 87, 90], "yet": [58, 64, 66, 67, 69, 106, 109, 111, 112], "yggvpl": 84, "yield": 106, "yintercept": 59, "ylab": [56, 58, 59], "ylabel": [63, 64, 65, 67, 70, 71, 74, 75, 76, 78, 79, 83, 85, 86, 89], "ylim": 85, "ymax": 59, "ymin": 59, "yname": 57, "york": 151, "you": [46, 47, 50, 51, 55, 56, 63, 64, 65, 66, 67, 70, 72, 84, 92, 106, 148, 149, 153], "your": [82, 149], "ython": 148, "yukun": 151, "yusuk": 151, "yuya": 151, "yy": 73, "z": [4, 5, 6, 7, 8, 9, 17, 18, 19, 25, 27, 30, 31, 33, 35, 36, 37, 39, 40, 41, 55, 58, 59, 61, 64, 67, 72, 74, 75, 77, 84, 85, 89, 91, 93, 94, 104, 106, 115, 117, 121, 128, 130, 132, 135, 136, 152], "z1": [7, 16, 37, 64, 67, 98, 99, 102, 106, 107, 108, 109, 111], "z2": [7, 16, 64, 67, 98, 99, 102, 106, 107, 108, 109, 111], "z3": [7, 16, 64, 67, 98, 99, 102, 106, 107, 108, 109, 111], "z4": [7, 16, 64, 67, 98, 99, 102, 106, 107, 108, 109, 111], "z_": [41, 58, 84], "z_1": [31, 35, 39, 64, 67], "z_2": [31, 35, 39], "z_3": [31, 35, 39], "z_4": [31, 35, 39, 64, 67], "z_5": 31, "z_col": [4, 5, 6, 7, 8, 9, 25, 27, 37, 55, 58, 59, 61, 72, 84, 85, 86, 91, 94, 97, 98, 102, 104, 106, 152], "z_i": [36, 40, 61, 89, 94, 106], "z_j": [17, 18, 19, 31, 35, 39], "z_true": 89, "zadik": 151, "zaxis_titl": [74, 75, 77], "zero": [14, 18, 45, 62, 63, 64, 67, 70, 76, 81, 82, 88, 89, 92, 93, 106, 121, 123, 125, 136, 137, 140, 142], "zeros_lik": 89, "zeta": [25, 37, 38, 59, 85, 104, 106, 115, 117, 118, 150], "zeta_": [41, 58, 84], "zeta_0": [41, 58, 84], "zeta_i": [34, 40, 42, 56, 73, 96], "zeta_j": 136, "zhang": 151, "zhao": [12, 14, 15, 18, 31, 35, 39, 57, 62, 64, 67, 106, 108, 112, 151], "zimmert": [62, 106, 112, 151], "zip": [74, 75], "zorder": [70, 71], "\u03c4_x0": 87, "\u03c4_x1": 87, "\u2139": 56}, "titles": ["API Reference", "<span class=\"section-number\">1. </span>DoubleML Data Class", "<span class=\"section-number\">3. </span>Datasets", "<span class=\"section-number\">2. </span>DoubleML Models", "<span class=\"section-number\">1.2. </span>doubleml.data.DoubleMLClusterData", "<span class=\"section-number\">1.6. </span>doubleml.data.DoubleMLDIDData", "<span class=\"section-number\">1.1. </span>doubleml.data.DoubleMLData", "<span class=\"section-number\">1.3. </span>doubleml.data.DoubleMLPanelData", "<span class=\"section-number\">1.5. </span>doubleml.data.DoubleMLRDDData", "<span class=\"section-number\">1.4. </span>doubleml.data.DoubleMLSSMData", "<span class=\"section-number\">3.1.1. </span>doubleml.datasets.fetch_401K", "<span class=\"section-number\">3.1.2. </span>doubleml.datasets.fetch_bonus", "<span class=\"section-number\">2.3.4. </span>doubleml.did.DoubleMLDID", "<span class=\"section-number\">2.3.2. </span>doubleml.did.DoubleMLDIDAggregation", "<span class=\"section-number\">2.3.3. </span>doubleml.did.DoubleMLDIDBinary", "<span class=\"section-number\">2.3.5. </span>doubleml.did.DoubleMLDIDCS", "<span class=\"section-number\">2.3.1. </span>doubleml.did.DoubleMLDIDMulti", "<span class=\"section-number\">3.2.13. </span>doubleml.did.datasets.make_did_CS2021", "<span class=\"section-number\">3.2.12. </span>doubleml.did.datasets.make_did_SZ2020", "<span class=\"section-number\">3.2.14. </span>doubleml.did.datasets.make_did_cs_CS2021", "<span class=\"section-number\">5.1. </span>doubleml.double_ml_score_mixins.LinearScoreMixin", "<span class=\"section-number\">5.2. </span>doubleml.double_ml_score_mixins.NonLinearScoreMixin", "<span class=\"section-number\">2.2.2. </span>doubleml.irm.DoubleMLAPO", "<span class=\"section-number\">2.2.3. </span>doubleml.irm.DoubleMLAPOS", "<span class=\"section-number\">2.2.7. </span>doubleml.irm.DoubleMLCVAR", "<span class=\"section-number\">2.2.4. </span>doubleml.irm.DoubleMLIIVM", "<span class=\"section-number\">2.2.1. </span>doubleml.irm.DoubleMLIRM", "<span class=\"section-number\">2.2.6. </span>doubleml.irm.DoubleMLLPQ", "<span class=\"section-number\">2.2.5. </span>doubleml.irm.DoubleMLPQ", "<span class=\"section-number\">2.2.8. </span>doubleml.irm.DoubleMLQTE", "<span class=\"section-number\">2.2.9. </span>doubleml.irm.DoubleMLSSM", "<span class=\"section-number\">3.2.5. </span>doubleml.irm.datasets.make_confounded_irm_data", "<span class=\"section-number\">3.2.3. </span>doubleml.irm.datasets.make_heterogeneous_data", "<span class=\"section-number\">3.2.2. </span>doubleml.irm.datasets.make_iivm_data", "<span class=\"section-number\">3.2.1. </span>doubleml.irm.datasets.make_irm_data", "<span class=\"section-number\">3.2.4. </span>doubleml.irm.datasets.make_irm_data_discrete_treatments", "<span class=\"section-number\">3.2.6. </span>doubleml.irm.datasets.make_ssm_data", "<span class=\"section-number\">2.1.2. </span>doubleml.plm.DoubleMLPLIV", "<span class=\"section-number\">2.1.1. </span>doubleml.plm.DoubleMLPLR", "<span class=\"section-number\">3.2.11. </span>doubleml.plm.datasets.make_confounded_plr_data", "<span class=\"section-number\">3.2.9. </span>doubleml.plm.datasets.make_pliv_CHS2015", "<span class=\"section-number\">3.2.10. </span>doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021", "<span class=\"section-number\">3.2.7. </span>doubleml.plm.datasets.make_plr_CCDDHNR2018", "<span class=\"section-number\">3.2.8. </span>doubleml.plm.datasets.make_plr_turrell2018", "<span class=\"section-number\">2.4.1. </span>doubleml.rdd.RDFlex", "<span class=\"section-number\">3.2.15. </span>doubleml.rdd.datasets.make_simple_rdd_data", "<span class=\"section-number\">4.1.2. </span>doubleml.utils.DMLDummyClassifier", "<span class=\"section-number\">4.1.1. </span>doubleml.utils.DMLDummyRegressor", "<span class=\"section-number\">4.1.3. </span>doubleml.utils.DoubleMLBLP", "<span class=\"section-number\">4.1.4. </span>doubleml.utils.DoubleMLPolicyTree", "<span class=\"section-number\">4.1.6. </span>doubleml.utils.GlobalClassifier", "<span class=\"section-number\">4.1.5. </span>doubleml.utils.GlobalRegressor", "<span class=\"section-number\">4.2.1. </span>doubleml.utils.gain_statistics", "<span class=\"section-number\">5. </span>Score Mixin Classes for DoubleML Models", "<span class=\"section-number\">4. </span>Utility Classes and Functions", "R: Basic Instrumental Variables Calculation", "R: Basics of Double Machine Learning", "R: DoubleML for Difference-in-Differences", "R: Cluster Robust Double Machine Learning", "R: Impact of 401(k) on Financial Wealth", "R: Ensemble Learners and More with <code class=\"docutils literal notranslate\"><span class=\"pre\">mlr3pipelines</span></code>", "R: Sample Selection Models", "Python: Difference-in-Differences", "Python: Difference-in-Differences Pre-Testing", "Python: Panel Data with Multiple Time Periods", "Python: Real-Data Example for Multi-Period Difference-in-Differences", "Python: Panel Data Introduction", "Python: Repeated Cross-Sectional Data with Multiple Time Periods", "DML: Bonus Data", "Examples", "Python: Causal Machine Learning with TabPFN", "Python: Average Potential Outcome (APO) Models", "Python: Basic Instrumental Variables calculation", "Python: Basics of Double Machine Learning", "Python: Conditional Average Treatment Effects (CATEs) for IRM models", "Python: Conditional Average Treatment Effects (CATEs) for PLR models", "Python: Conditional Value at Risk of potential outcomes", "Python: First Stage and Causal Estimation", "Python: Group Average Treatment Effects (GATEs) for IRM models", "Python: Group Average Treatment Effects (GATEs) for PLR models", "Python: GATE Sensitivity Analysis", "Python: IRM and APO Model Comparison", "Python: Choice of learners", "DoubleML meets FLAML - How to tune learners automatically within <code class=\"docutils literal notranslate\"><span class=\"pre\">DoubleML</span></code>", "Python: Cluster Robust Double Machine Learning", "Python: Impact of 401(k) on Financial Wealth", "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)", "Python: PLM and IRM for Multiple Treatments", "Python: Policy Learning with Trees", "Python: Potential Quantiles and Quantile Treatment Effects", "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)", "Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments", "Python: Sensitivity Analysis", "Example: Sensitivity Analysis for Causal ML", "Python: Sample Selection Models", "<span class=\"section-number\">6. </span>Double machine learning algorithms", "<span class=\"section-number\">1. </span>The basics of double/debiased machine learning", "DoubleMLData from dataframes", "Key arguments", "Key arguments", "Key arguments", "Key arguments", "<span class=\"section-number\">2. </span>Data Backend", "User Guide", "<span class=\"section-number\">4. </span>Heterogeneous treatment effects", "<span class=\"section-number\">7. </span>Learners, hyperparameters and hyperparameter tuning", "<span class=\"section-number\">3. </span>Models", "&lt;no title&gt;", "Panel Data", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">9. </span>Sample-splitting, cross-fitting and repeated cross-fitting", "<span class=\"section-number\">5. </span>Score functions", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">8. </span>Variance estimation and confidence intervals", "<span class=\"section-number\">10. </span>Sensitivity analysis", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "DoubleML", "Installing DoubleML", "Getting Started", "Double Machine Learning Literature", "Release Notes", "DoubleML Workflow"], "titleterms": {"": 83, "0": 153, "1": [83, 93, 153], "2": [83, 93, 153], "2011": 93, "2023": 93, "3": [83, 93, 153], "4": [93, 153], "401": [59, 85, 86, 92], "5": [93, 153], "6": 153, "7": 153, "95": 83, "A": [58, 84], "ATE": [61, 80, 87, 94], "No": [58, 84], "One": [58, 74, 75, 84], "That": 91, "The": [59, 85, 87, 96, 150], "acknowledg": [57, 148], "acycl": [55, 72], "addit": 87, "adjust": [64, 67, 90], "advanc": [90, 105, 136], "aggreg": [64, 65, 66, 67, 106], "al": 93, "algorithm": [95, 137, 148, 150], "all": [64, 67], "altern": 121, "analysi": [64, 67, 71, 80, 81, 92, 93, 137, 153], "anticip": [64, 67], "api": [0, 83], "apo": [71, 81, 106, 121, 137], "applic": [58, 84, 92], "approach": [56, 73, 82, 96], "ar": 91, "arah": 93, "arbitrari": 87, "archiv": 69, "argument": [98, 99, 100, 101, 102], "arrai": [97, 102], "asset": [59, 85], "assumpt": 93, "att": [62, 64, 65, 66, 67], "augment": 87, "automat": 83, "automl": 83, "averag": [59, 70, 71, 74, 75, 78, 79, 81, 85, 104, 106, 121, 137], "backend": [58, 59, 84, 85, 102, 150, 153], "band": 136, "base": [60, 64, 67], "basic": [55, 56, 64, 67, 72, 73, 96], "benchmark": [92, 93, 137], "bia": [56, 73, 96], "binari": [106, 121], "bonu": 68, "bootstrap": 136, "build": 149, "calcul": [55, 72], "call": 83, "callabl": 121, "case": 69, "cate": [74, 75, 87, 104], "causal": [65, 68, 70, 71, 77, 93, 121, 150, 153], "chernozhukov": 93, "choic": 82, "citat": 148, "class": [1, 53, 54, 58, 84], "cluster": [58, 84], "code": 148, "coeffici": 83, "combin": [64, 67, 77], "compar": [82, 83], "comparison": [57, 70, 81, 83], "comput": [82, 83], "conclus": [83, 93], "conda": 149, "condit": [65, 74, 75, 76, 86, 104, 121], "confid": [83, 91, 136], "construct": 105, "contrast": 71, "control": [64, 67], "covari": [64, 67, 90], "coverag": [62, 77], "cran": 149, "creat": [70, 83], "cross": [58, 62, 67, 84, 106, 108, 120, 121, 137, 150], "custom": [82, 83], "cvar": [76, 86, 104, 121], "dag": [55, 72], "data": [1, 4, 5, 6, 7, 8, 9, 55, 56, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 83, 84, 85, 86, 88, 89, 90, 92, 93, 94, 96, 102, 106, 108, 121, 137, 150, 153], "datafram": [97, 102], "dataset": [2, 10, 11, 17, 18, 19, 31, 32, 33, 34, 35, 36, 39, 40, 41, 42, 43, 45, 68], "debias": [56, 73, 96, 150], "default": 83, "defin": [58, 84], "demo": 57, "depend": 149, "descript": [64, 67], "design": [90, 106], "detail": [57, 64, 65, 66, 67, 106], "develop": 149, "dgp": [56, 70, 71, 73], "did": [3, 12, 13, 14, 15, 16, 17, 18, 19, 57, 106], "differ": [57, 62, 63, 65, 69, 82, 106, 121, 136, 137], "dimension": [74, 75], "direct": [55, 72], "disclaim": 93, "discontinu": [90, 106], "distribut": [61, 94], "dml": [58, 68, 84, 120, 150, 153], "dml1": 95, "dml2": 95, "dmldummyclassifi": 46, "dmldummyregressor": 47, "doubl": [56, 58, 73, 84, 95, 96, 148, 150, 151], "double_ml_score_mixin": [20, 21], "doubleml": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 57, 59, 60, 70, 72, 83, 85, 92, 93, 136, 148, 149, 153], "doublemlapo": [22, 23], "doublemlblp": 48, "doublemlclusterdata": [4, 58], "doublemlcvar": 24, "doublemldata": [6, 59, 70, 84, 85, 97, 102, 150], "doublemldid": 12, "doublemldidaggreg": 13, "doublemldidbinari": 14, "doublemldidc": 15, "doublemldiddata": [5, 102], "doublemldidmulti": 16, "doublemliivm": 25, "doublemlirm": 26, "doublemllpq": 27, "doublemlpaneldata": [7, 64, 67, 102], "doublemlpliv": [37, 58, 84], "doublemlplr": 38, "doublemlpolicytre": 49, "doublemlpq": 28, "doublemlqt": 29, "doublemlrdddata": [8, 102], "doublemlssm": 30, "doublemlssmdata": [9, 102], "effect": [59, 64, 65, 66, 67, 69, 70, 74, 75, 76, 78, 79, 81, 85, 86, 87, 89, 92, 93, 104, 106], "elig": [59, 85], "empir": 77, "ensembl": [60, 90], "error": [58, 84], "estim": [55, 59, 61, 62, 64, 65, 66, 67, 68, 70, 72, 77, 80, 83, 85, 86, 87, 89, 92, 93, 94, 120, 121, 136, 150, 153], "et": 93, "evalu": [70, 82, 83, 105], "event": [64, 65, 66, 67], "exampl": [57, 58, 65, 69, 74, 75, 84, 92, 93, 98, 99, 100, 101, 102], "exploit": [57, 60], "extern": [105, 120], "featur": [60, 148], "fetch_401k": 10, "fetch_bonu": 11, "figur": 87, "file": 149, "final": 57, "financi": [59, 85, 86], "first": 77, "fit": [58, 83, 84, 120, 150], "flaml": 83, "flexibl": 90, "fold": [83, 120], "forest": 68, "formul": [93, 153], "from": [57, 60, 97, 102, 149], "full": 83, "function": [54, 57, 58, 84, 121, 150], "fuzzi": [90, 106], "gain_statist": 52, "gate": [78, 79, 80, 104], "gatet": 80, "gener": [2, 56, 69, 70, 71, 73, 83, 90, 96, 137], "get": 150, "github": 149, "global": 90, "globalclassifi": 50, "globalregressor": 51, "graph": [55, 72], "group": [64, 66, 67, 78, 79, 104], "guid": 103, "helper": [58, 84], "heterogen": [69, 81, 87, 104], "how": [60, 83], "hyperparamet": [81, 105], "identif": 93, "iivm": [59, 85, 106, 121], "impact": [59, 85, 86], "implement": [95, 106, 121, 137], "import": 70, "induc": [56, 73, 96], "infer": [136, 153], "initi": [58, 83, 84], "insight": 70, "instal": 149, "instrument": [55, 72, 91], "integr": 57, "interact": [59, 78, 85, 88, 106, 121, 137], "interv": [83, 91, 136], "introduct": 66, "invers": 87, "irm": [3, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 59, 68, 74, 78, 81, 85, 87, 88, 92, 104, 106, 121, 137], "iv": [55, 59, 72, 85, 106, 121], "k": [59, 85, 86, 92, 120], "kei": [70, 98, 99, 100, 101, 102], "lambda": 77, "lasso": [68, 77], "latest": 149, "lear": [58, 84], "learn": [56, 58, 70, 73, 84, 88, 95, 96, 104, 148, 150, 151], "learner": [60, 68, 81, 82, 83, 90, 105, 150], "less": 83, "level": 106, "linear": [59, 64, 67, 79, 85, 87, 90, 106, 121, 137], "linearscoremixin": 20, "literatur": 151, "load": [58, 68, 84, 93], "loader": 2, "local": [59, 85, 86, 89, 90, 121], "loss": 77, "lpq": [89, 121], "lqte": [86, 89], "m": 120, "machin": [56, 58, 70, 73, 84, 95, 96, 148, 150, 151], "main": 148, "mainten": 148, "make_confounded_irm_data": 31, "make_confounded_plr_data": 39, "make_did_cs2021": 17, "make_did_cs_cs2021": 19, "make_did_sz2020": 18, "make_heterogeneous_data": 32, "make_iivm_data": 33, "make_irm_data": 34, "make_irm_data_discrete_treat": 35, "make_pliv_chs2015": 40, "make_pliv_multiway_cluster_ckms2021": 41, "make_plr_ccddhnr2018": 42, "make_plr_turrell2018": 43, "make_simple_rdd_data": 45, "make_ssm_data": 36, "mar": [61, 94], "market": [58, 84], "matric": [97, 102], "meet": 83, "method": [70, 83, 153], "metric": [82, 83], "minimum": 105, "miss": [61, 94], "missing": [106, 121], "mixin": 53, "ml": [56, 57, 73, 93, 96, 153], "mlr3": 60, "mlr3extralearn": 60, "mlr3learner": 60, "mlr3pipelin": 60, "model": [3, 53, 59, 61, 68, 70, 71, 74, 75, 78, 79, 81, 83, 85, 87, 88, 91, 93, 94, 104, 106, 120, 121, 136, 137, 150, 153], "modul": 68, "more": 60, "motiv": [58, 84], "multi": 65, "multipl": [64, 67, 71, 87, 106], "multipli": 136, "naiv": [55, 72], "net": [59, 85], "neyman": [121, 150], "nonignor": [61, 94, 106, 121], "nonlinearscoremixin": 21, "nonrespons": [61, 94, 106, 121], "note": 152, "nuisanc": [83, 150], "object": [58, 70, 84, 92], "option": 149, "orthogon": [56, 73, 96, 121, 150], "out": [56, 73, 96], "outcom": [61, 62, 70, 71, 76, 94, 104, 106, 121, 137], "over": 136, "overcom": [56, 73, 96], "overfit": [56, 73, 96], "overlap": 87, "packag": [57, 59, 85, 149], "panel": [62, 64, 66, 106, 108, 121, 137], "parallel": 65, "paramet": [60, 68, 83, 106, 121], "partial": [56, 59, 73, 79, 85, 87, 96, 106, 121, 137], "particip": [59, 85], "partit": 120, "penalti": 77, "perform": [57, 70, 87], "period": [64, 65, 67, 106, 121, 137], "pip": 149, "pipelin": 105, "pliv": [106, 121], "plm": [3, 37, 38, 39, 40, 41, 42, 43, 87, 106, 121, 137], "plot": [58, 83, 84], "plr": [59, 68, 75, 79, 85, 104, 106, 121, 137], "polici": [88, 104], "potenti": [70, 71, 76, 86, 89, 104, 106, 121, 137], "pq": [89, 104, 121], "pre": 63, "predict": [57, 105], "preprocess": 60, "problem": 153, "process": [56, 58, 70, 71, 73, 84, 96], "product": [58, 84], "propens": 87, "provid": 120, "python": [62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 91, 92, 94, 105, 149], "qte": [89, 104], "qualiti": 77, "quantil": [86, 89, 104, 121], "question": 65, "r": [55, 56, 57, 58, 59, 60, 61, 69, 105, 149], "random": [61, 68, 94, 106, 121], "rank": 87, "rdd": [3, 44, 45, 90, 106], "rdflex": 44, "real": [58, 65, 84], "refer": [0, 55, 57, 58, 60, 72, 77, 82, 83, 84, 87, 91, 93, 96, 105, 120, 136, 148, 150], "regress": [59, 78, 79, 85, 88, 90, 106, 121, 137], "regular": [56, 73, 96], "releas": [149, 152], "remark": 57, "remov": [56, 73, 96], "repeat": [62, 67, 106, 108, 120, 121, 137], "repetit": 120, "requir": 105, "research": 65, "respect": [58, 84], "result": [58, 59, 84, 85, 87], "risk": [76, 86, 104, 121], "robust": [58, 84, 91], "run": 91, "sampl": [56, 61, 73, 83, 94, 96, 106, 120, 121], "sandbox": 69, "score": [53, 56, 73, 87, 96, 121, 150], "section": [62, 67, 106, 108, 121, 137], "select": [61, 64, 67, 94, 106, 121], "sensit": [64, 67, 71, 80, 81, 92, 93, 137, 153], "set": [60, 105], "setup": 70, "sharp": [90, 106], "simpl": [56, 73, 96], "simul": [55, 58, 62, 72, 84, 91, 92], "simultan": 136, "singl": 71, "small": 91, "sourc": [148, 149], "special": 102, "specif": [137, 153], "specifi": [68, 105, 121], "split": [56, 73, 96, 120], "ssm": 106, "stack": 90, "stage": 77, "standard": [58, 82, 84], "start": 150, "step": 83, "structur": 70, "studi": [64, 65, 66, 67, 69], "summari": [59, 70, 83, 85, 87], "tabpfn": 70, "takeawai": 70, "test": 63, "theori": 137, "time": [64, 66, 67, 82, 83], "train": 83, "treat": 81, "treatment": [59, 70, 74, 75, 76, 78, 79, 81, 85, 86, 87, 89, 104, 106, 121, 137], "tree": [88, 104], "trend": 65, "tune": [60, 83, 105], "two": [58, 74, 75, 84, 106, 121, 137], "type": 102, "uncondit": 65, "under": [61, 87, 94], "univers": [64, 67], "untun": 83, "up": 60, "us": [55, 57, 60, 68, 72, 83, 105], "usag": [98, 99, 100, 101, 102], "user": 103, "util": [46, 47, 48, 49, 50, 51, 52, 54], "v": 77, "valid": 136, "valu": [76, 86, 104, 121], "vanderweel": 93, "variabl": [55, 72, 91], "varianc": 136, "version": 149, "via": 121, "visual": 70, "wai": [58, 84], "weak": 91, "wealth": [59, 85, 86], "weight": [87, 104], "when": 83, "whl": 149, "within": 83, "without": [90, 120], "workflow": 153, "xgboost": 83, "zero": [58, 84]}})