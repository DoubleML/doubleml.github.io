Search.setIndex({"alltitles": {"(One-Way) Cluster Robust Double Machine Learing": [[63, "(One-Way)-Cluster-Robust-Double-Machine-Learing"], [91, "(One-Way)-Cluster-Robust-Double-Machine-Learing"]], "0. Problem Formulation": [[173, "problem-formulation"]], "1. Data Backend": [[173, "data-backend"]], "1. Formulation of Causal Model & Identification Assumptions": [[100, "1.-Formulation-of-Causal-Model-&-Identification-Assumptions"]], "2. Causal Model": [[173, "causal-model"]], "2. Estimation of Causal Effect": [[100, "2.-Estimation-of-Causal-Effect"]], "3. ML Methods": [[173, "ml-methods"]], "3. Sensitivity Analysis": [[100, "3.-Sensitivity-Analysis"]], "4. Benchmarking Analysis": [[100, "4.-Benchmarking-Analysis"]], "4. DML Specifications": [[173, "dml-specifications"]], "5. Conclusion": [[100, "5.-Conclusion"]], "5. Estimation": [[173, "estimation"]], "6. Inference": [[173, "inference"]], "7. Sensitivity Analysis": [[173, "sensitivity-analysis"]], "A Motivating Example: Two-Way Cluster Robust DML": [[63, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"], [91, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"]], "API Reference": [[0, null]], "ATE Estimation and Sensitivity": [[87, "ATE-Estimation-and-Sensitivity"]], "ATE estimates distribution": [[66, "ATE-estimates-distribution"], [66, "id3"], [101, "ATE-estimates-distribution"], [101, "id3"]], "ATT Estimation": [[69, "ATT-Estimation"], [69, "id1"], [71, "ATT-Estimation"], [72, "ATT-Estimation"], [72, "id1"]], "ATT Estimation: Conditional Parallel Trends": [[70, "ATT-Estimation:-Conditional-Parallel-Trends"]], "ATT Estimation: Unconditional Parallel Trends": [[70, "ATT-Estimation:-Unconditional-Parallel-Trends"]], "ATTE Estimation": [[67, "ATTE-Estimation"], [67, "id2"]], "Acknowledgements": [[168, "acknowledgements"]], "Acknowledgements and Final Remarks": [[62, "Acknowledgements-and-Final-Remarks"]], "Additional Results: CATE estimates": [[94, "Additional-Results:-CATE-estimates"]], "Advanced: External Predictions": [[112, "advanced-external-predictions"]], "Advanced: Global and Local Learners, Stacked Ensembles": [[97, "Advanced:-Global-and-Local-Learners,-Stacked-Ensembles"]], "Aggregated Effects": [[69, "Aggregated-Effects"], [72, "Aggregated-Effects"]], "Aggregation Details": [[69, "Aggregation-Details"], [70, "Aggregation-Details"], [71, "Aggregation-Details"], [72, "Aggregation-Details"]], "Algorithm DML1": [[102, "algorithm-dml1"]], "Algorithm DML2": [[102, "algorithm-dml2"]], "All Combinations": [[69, "All-Combinations"]], "All combinations": [[72, "All-combinations"]], "Anticipation": [[69, "Anticipation"], [72, "Anticipation"]], "Application Results": [[63, "Application-Results"], [91, "Application-Results"]], "Application: 401(k)": [[99, "Application:-401(k)"]], "AutoML with less Computation time": [[90, "AutoML-with-less-Computation-time"]], "Average Potential Outcome (APOs)": [[78, "Average-Potential-Outcome-(APOs)"]], "Average Potential Outcomes (APOs)": [[124, "average-potential-outcomes-apos"], [140, "average-potential-outcomes-apos"], [157, "average-potential-outcomes-apos"]], "Average Potential Outcomes (APOs) for Multiple Treatment Levels": [[124, "average-potential-outcomes-apos-for-multiple-treatment-levels"]], "Average Treatment Effect": [[88, "Average-Treatment-Effect"]], "Average Treatment Effect on the Treated": [[88, "Average-Treatment-Effect-on-the-Treated"]], "Basic Tuning Example": [[76, "Basic-Tuning-Example"]], "Basics": [[69, "Basics"], [72, "Basics"]], "Benchmarking": [[157, "benchmarking"]], "Benchmarking Analysis": [[99, "Benchmarking-Analysis"]], "Binary Interactive Regression Model (IRM)": [[124, "binary-interactive-regression-model-irm"], [140, "binary-interactive-regression-model-irm"]], "CATEs for IRM models": [[111, "cates-for-irm-models"]], "CATEs for PLR models": [[111, "cates-for-plr-models"]], "CVaR Treatment Effects": [[83, "CVaR-Treatment-Effects"]], "CVaR of potential outcomes": [[111, "cvar-of-potential-outcomes"]], "CVaR treatment effects": [[111, "cvar-treatment-effects"]], "Causal Analysis with DoubleML": [[100, "Causal-Analysis-with-DoubleML"]], "Causal Contrasts": [[78, "Causal-Contrasts"]], "Causal Research Question": [[70, "Causal-Research-Question"]], "Causal estimation vs. lasso penalty \\lambda": [[84, "Causal-estimation-vs.-lasso-penalty-\\lambda"]], "Chernozhukov et al. (2023): Benchmarking": [[100, "Chernozhukov-et-al.-(2023):-Benchmarking"]], "Citation": [[168, "citation"]], "Cluster Robust Cross Fitting": [[63, "Cluster-Robust-Cross-Fitting"], [91, "Cluster-Robust-Cross-Fitting"]], "Cluster Robust Standard Errors": [[63, "Cluster-Robust-Standard-Errors"], [91, "Cluster-Robust-Standard-Errors"]], "Clustering and double machine learning": [[63, "Clustering-and-double-machine-learning"], [91, "Clustering-and-double-machine-learning"]], "Combined loss vs. lasso penalty \\lambda": [[84, "Combined-loss-vs.-lasso-penalty-\\lambda"]], "Compare Metrics for Nuisance Estimation": [[90, "Compare-Metrics-for-Nuisance-Estimation"]], "Comparing different learners": [[75, "Comparing-different-learners"]], "Comparison": [[76, "Comparison"], [76, "id2"]], "Comparison and summary": [[90, "Comparison-and-summary"]], "Comparison to AutoML with less Computation time and Untuned XGBoost Learners": [[90, "Comparison-to-AutoML-with-less-Computation-time-and-Untuned-XGBoost-Learners"]], "Comparison to did package": [[62, "Comparison-to-did-package"]], "Computation time": [[75, "Computation-time"]], "Conclusion": [[90, "Conclusion"]], "Conditional Value at Risk (CVaR)": [[83, "Conditional-Value-at-Risk-(CVaR)"]], "Conditional average treatment effects (CATEs)": [[111, "conditional-average-treatment-effects-cates"]], "Conditional value at risk (CVaR)": [[111, "conditional-value-at-risk-cvar"], [140, "conditional-value-at-risk-cvar"]], "Confidence bands and multiplier bootstrap for valid simultaneous inference": [[156, "confidence-bands-and-multiplier-bootstrap-for-valid-simultaneous-inference"]], "Control Groups": [[69, "Control-Groups"], [72, "Control-Groups"]], "Coverage Simulation": [[67, "Coverage-Simulation"], [67, "id3"]], "Creating the DoubleMLData Object": [[77, "Creating-the-DoubleMLData-Object"]], "Cross-fitting with K folds": [[139, "cross-fitting-with-k-folds"]], "Cross-fitting, DML algorithms and Neyman-orthogonal score functions": [[170, "cross-fitting-dml-algorithms-and-neyman-orthogonal-score-functions"]], "Custom evaluation metrics": [[75, "Custom-evaluation-metrics"]], "DML: Bonus Data": [[73, null]], "Data": [[64, "Data"], [66, "Data"], [66, "id1"], [67, "Data"], [67, "id1"], [69, "Data"], [70, "Data"], [71, "Data"], [72, "Data"], [81, "Data"], [82, "Data"], [83, "Data"], [85, "Data"], [86, "Data"], [87, "Data"], [88, "Data"], [89, "Data"], [92, "Data"], [93, "Data"], [95, "Data"], [96, "Data"], [96, "id1"], [99, "Data"], [101, "Data"], [101, "id1"], [170, "data"]], "Data Backend": [[109, null]], "Data Description": [[69, "Data-Description"], [72, "Data-Description"]], "Data Details": [[69, "Data-Details"], [72, "Data-Details"]], "Data Generating Process (DGP)": [[61, "Data-Generating-Process-(DGP)"], [76, "Data-Generating-Process-(DGP)"], [77, "Data-Generating-Process-(DGP)"], [78, "Data-Generating-Process-(DGP)"], [80, "Data-Generating-Process-(DGP)"]], "Data Generation": [[90, "Data-Generation"]], "Data Simulation": [[60, "Data-Simulation"], [79, "Data-Simulation"]], "Data and Effect Estimation": [[99, "Data-and-Effect-Estimation"]], "Data generating process": [[103, "data-generating-process"]], "Data preprocessing": [[65, "Data-preprocessing"]], "Data with Anticipation": [[69, "Data-with-Anticipation"], [72, "Data-with-Anticipation"]], "Data-Backend for Cluster Data": [[63, "Data-Backend-for-Cluster-Data"], [91, "Data-Backend-for-Cluster-Data"]], "Dataset Generators": [[2, "dataset-generators"]], "Dataset Loaders": [[2, "dataset-loaders"]], "Datasets": [[2, null]], "Define Helper Functions for Plotting": [[63, "Define-Helper-Functions-for-Plotting"], [91, "Define-Helper-Functions-for-Plotting"]], "Define Nuisance Learners": [[76, "Define-Nuisance-Learners"], [76, "id1"]], "Demo Example from did": [[62, "Demo-Example-from-did"]], "Detailed Hyperparameter Tuning Guide": [[76, "Detailed-Hyperparameter-Tuning-Guide"]], "Detailed Tuning Result Analysis": [[76, "Detailed-Tuning-Result-Analysis"]], "Details on Predictive Performance": [[62, "Details-on-Predictive-Performance"]], "Difference-in-Differences": [[74, "difference-in-differences"]], "Difference-in-Differences Models": [[140, "difference-in-differences-models"], [157, "difference-in-differences-models"]], "Difference-in-Differences Models (DID)": [[124, "difference-in-differences-models-did"]], "Difference-in-Differences for Panel Data": [[157, "difference-in-differences-for-panel-data"]], "Difference-in-Differences for repeated cross-sections": [[157, "difference-in-differences-for-repeated-cross-sections"]], "Disclaimer": [[100, "Disclaimer"]], "Double Machine Learning Algorithm": [[168, "double-machine-learning-algorithm"]], "Double Machine Learning Literature": [[171, null]], "Double machine learning algorithms": [[102, null]], "Double/debiased machine learning": [[61, "Double/debiased-machine-learning"], [80, "Double/debiased-machine-learning"], [103, "double-debiased-machine-learning"]], "DoubleML": [[168, null]], "DoubleML Data Class": [[1, null]], "DoubleML Models": [[3, null]], "DoubleML Object": [[99, "DoubleML-Object"]], "DoubleML Workflow": [[173, null]], "DoubleML meets FLAML - How to tune learners automatically within DoubleML": [[90, null]], "DoubleML with TabPFN": [[77, "DoubleML-with-TabPFN"]], "DoubleMLAPOS Tuning Example": [[76, "DoubleMLAPOS-Tuning-Example"]], "DoubleMLDIDData": [[109, "doublemldiddata"]], "DoubleMLData": [[109, "doublemldata"]], "DoubleMLData from arrays and matrices": [[104, "doublemldata-from-arrays-and-matrices"], [109, "doublemldata-from-arrays-and-matrices"]], "DoubleMLData from dataframes": [[104, null], [109, "doublemldata-from-dataframes"]], "DoubleMLPanelData": [[69, "DoubleMLPanelData"], [72, "DoubleMLPanelData"], [109, "doublemlpaneldata"]], "DoubleMLRDDData": [[109, "doublemlrdddata"]], "DoubleMLSSMData": [[109, "doublemlssmdata"]], "Effect Aggregation": [[70, "Effect-Aggregation"], [71, "Effect-Aggregation"], [124, "effect-aggregation"]], "Effect Heterogeneity": [[74, "effect-heterogeneity"], [88, "Effect-Heterogeneity"]], "Empirical coverage vs. lasso penalty \\lambda": [[84, "Empirical-coverage-vs.-lasso-penalty-\\lambda"]], "Estimate DML models without sample-splitting": [[139, "estimate-dml-models-without-sample-splitting"]], "Estimate double/debiased machine learning models": [[170, "estimate-double-debiased-machine-learning-models"]], "Estimating Potential Quantiles and Quantile Treatment Effects": [[93, "Estimating-Potential-Quantiles-and-Quantile-Treatment-Effects"]], "Estimating local quantile treatment effects (LQTEs)": [[93, "Estimating-local-quantile-treatment-effects-(LQTEs)"]], "Estimating the Average Treatment Effect of 401(k) Eligibility on Net Financial Assets": [[64, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"], [92, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"]], "Estimating the treatment effect on the Conditional Value a Risk (CVaR)": [[93, "Estimating-the-treatment-effect-on-the-Conditional-Value-a-Risk-(CVaR)"]], "Estimation": [[66, "Estimation"], [66, "id2"], [101, "Estimation"], [101, "id2"]], "Estimation of Average Potential Outcomes": [[77, "Estimation-of-Average-Potential-Outcomes"]], "Estimation quality vs. \\lambda": [[84, "Estimation-quality-vs.-\\lambda"]], "Evaluate learners": [[112, "evaluate-learners"]], "Event Study Aggregation": [[69, "Event-Study-Aggregation"], [70, "Event-Study-Aggregation"], [71, "Event-Study-Aggregation"], [72, "Event-Study-Aggregation"]], "Example usage": [[105, "example-usage"], [106, "example-usage"], [107, "example-usage"], [108, "example-usage"], [109, "example-usage"], [109, "id6"], [109, "id8"], [109, "id10"]], "Examples": [[74, null]], "Exploiting the Functionalities of did": [[62, "Exploiting-the-Functionalities-of-did"]], "Externally provide a sample splitting / partition": [[139, "externally-provide-a-sample-splitting-partition"]], "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)": [[97, null]], "Fuzzy RDD": [[97, "Fuzzy-RDD"]], "Fuzzy RDD Without Adjustment": [[97, "Fuzzy-RDD-Without-Adjustment"]], "Fuzzy RDD with Flexible Adjustment": [[97, "Fuzzy-RDD-with-Flexible-Adjustment"]], "Fuzzy RDD with Linear Adjustment": [[97, "Fuzzy-RDD-with-Linear-Adjustment"]], "Fuzzy Regression Discontinuity Design": [[124, "fuzzy-regression-discontinuity-design"]], "GATE Estimation and Sensitivity": [[87, "GATE-Estimation-and-Sensitivity"]], "GATET Estimation and Sensitivity": [[87, "GATET-Estimation-and-Sensitivity"]], "GATEs for IRM models": [[111, "gates-for-irm-models"]], "GATEs for PLR models": [[111, "gates-for-plr-models"]], "General Examples": [[74, "general-examples"]], "General algorithm": [[157, "general-algorithm"]], "Generate Fuzzy Data": [[97, "Generate-Fuzzy-Data"]], "Generate Sharp Data": [[97, "Generate-Sharp-Data"]], "Getting Started": [[170, null]], "Group Aggregation": [[69, "Group-Aggregation"], [71, "Group-Aggregation"], [72, "Group-Aggregation"]], "Group Average Treatment Effects (GATEs)": [[85, "Group-Average-Treatment-Effects-(GATEs)"], [86, "Group-Average-Treatment-Effects-(GATEs)"]], "Group average treatment effects (GATEs)": [[111, "group-average-treatment-effects-gates"]], "Group-Time Combinations": [[69, "Group-Time-Combinations"], [72, "Group-Time-Combinations"]], "Heterogeneous treatment effects": [[111, null]], "How to exploit more features of mlr3pipelines in DoubleML": [[65, "How-to-exploit-more-features-of-mlr3pipelines-in-DoubleML"]], "Hyperparameter Tuning": [[76, "Hyperparameter-Tuning"], [76, "id4"]], "Hyperparameter Tuning with Pipelines": [[76, "Hyperparameter-Tuning-with-Pipelines"]], "Hyperparameter tuning": [[112, "hyperparameter-tuning"], [112, "r-tune-params"]], "Hyperparameter tuning (Grid Search)": [[112, "hyperparameter-tuning-grid-search"]], "Hyperparameter tuning with pipelines": [[112, "hyperparameter-tuning-with-pipelines"]], "Implementation": [[157, "implementation"]], "Implementation Details": [[124, "implementation-details"]], "Implementation of the double machine learning algorithms": [[102, "implementation-of-the-double-machine-learning-algorithms"]], "Implementation of the score function and the estimate of the causal parameter": [[140, "implementation-of-the-score-function-and-the-estimate-of-the-causal-parameter"]], "Implemented Neyman orthogonal score functions": [[140, "implemented-neyman-orthogonal-score-functions"]], "Imports and Setup": [[77, "Imports-and-Setup"]], "Initialize DoubleMLClusterData object": [[63, "Initialize-DoubleMLClusterData-object"]], "Initialize DoubleMLData object with clusters": [[91, "Initialize-DoubleMLData-object-with-clusters"]], "Initialize the objects of class DoubleMLPLIV": [[63, "Initialize-the-objects-of-class-DoubleMLPLIV"], [91, "Initialize-the-objects-of-class-DoubleMLPLIV"]], "Installing DoubleML": [[169, null]], "Instrumental Variables Directed Acyclic Graph (IV - DAG)": [[60, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"], [79, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"]], "Interactive IV Model (IIVM)": [[64, "Interactive-IV-Model-(IIVM)"], [92, "Interactive-IV-Model-(IIVM)"]], "Interactive IV model (IIVM)": [[124, "interactive-iv-model-iivm"], [140, "interactive-iv-model-iivm"]], "Interactive Regression Model (IRM)": [[64, "Interactive-Regression-Model-(IRM)"], [85, "Interactive-Regression-Model-(IRM)"], [92, "Interactive-Regression-Model-(IRM)"], [95, "Interactive-Regression-Model-(IRM)"]], "Interactive regression model (IRM)": [[157, "interactive-regression-model-irm"]], "Interactive regression models (IRM)": [[124, "interactive-regression-models-irm"], [140, "interactive-regression-models-irm"], [157, "interactive-regression-models-irm"]], "Key Takeaways": [[77, "Key-Takeaways"]], "Key arguments": [[105, null], [106, null], [107, null], [108, null], [109, "key-arguments"], [109, "id5"], [109, "id7"], [109, "id9"]], "Learners and Hyperparameters": [[88, "Learners-and-Hyperparameters"]], "Learners to estimate the nuisance models": [[170, "learners-to-estimate-the-nuisance-models"]], "Learners, hyperparameters and hyperparameter tuning": [[112, null]], "Linear Covariate Adjustment": [[69, "Linear-Covariate-Adjustment"], [72, "Linear-Covariate-Adjustment"]], "Load Data": [[100, "Load-Data"]], "Load and Process Data": [[63, "Load-and-Process-Data"], [91, "Load-and-Process-Data"]], "Load bonus data using the dml datasets module": [[73, "Load-bonus-data-using-the-dml-datasets-module"]], "Local Average Treatment Effects of 401(k) Participation on Net Financial Assets": [[64, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"], [92, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"]], "Local Potential Quantile Estimation": [[96, "Local-Potential-Quantile-Estimation"]], "Local Potential Quantiles (LPQs)": [[96, "Local-Potential-Quantiles-(LPQs)"]], "Local Quantile Treatment Effects (LQTEs)": [[96, "Local-Quantile-Treatment-Effects-(LQTEs)"]], "Local potential quantiles (LPQs)": [[140, "local-potential-quantiles-lpqs"]], "Logistic partial linear regression (LPLR)": [[140, "logistic-partial-linear-regression-lplr"]], "Logistic partially linear regression model (LPLR)": [[124, "logistic-partially-linear-regression-model-lplr"]], "Machine Learning Methods Comparison": [[77, "Machine-Learning-Methods-Comparison"]], "Main Features": [[168, "main-features"]], "Minimum requirements for learners": [[112, "minimum-requirements-for-learners"], [112, "r-learner-req"]], "Missingness at Random": [[124, "missingness-at-random"], [140, "missingness-at-random"]], "Model": [[89, "Model"]], "Model Performance Evaluation": [[77, "Model-Performance-Evaluation"]], "Model-specific implementations": [[157, "model-specific-implementations"]], "Models": [[124, null]], "Motivation": [[63, "Motivation"], [91, "Motivation"]], "Multiple Average Potential Outcome Models (APOS)": [[78, "Multiple-Average-Potential-Outcome-Models-(APOS)"]], "Naive estimation": [[60, "Naive-estimation"], [79, "Naive-estimation"]], "No Clustering / Zero-Way Clustering": [[63, "No-Clustering-/-Zero-Way-Clustering"], [91, "No-Clustering-/-Zero-Way-Clustering"]], "Nonignorable Nonresponse": [[124, "nonignorable-nonresponse"], [140, "nonignorable-nonresponse"]], "One-Way Clustering with Respect to the Market": [[63, "One-Way-Clustering-with-Respect-to-the-Market"], [91, "One-Way-Clustering-with-Respect-to-the-Market"]], "One-Way Clustering with Respect to the Product": [[63, "One-Way-Clustering-with-Respect-to-the-Product"], [91, "One-Way-Clustering-with-Respect-to-the-Product"]], "One-dimensional Example": [[81, "One-dimensional-Example"], [82, "One-dimensional-Example"]], "Outcome missing at random (MAR)": [[66, "Outcome-missing-at-random-(MAR)"], [101, "Outcome-missing-at-random-(MAR)"]], "Outcome missing under nonignorable nonresponse": [[66, "Outcome-missing-under-nonignorable-nonresponse"], [101, "Outcome-missing-under-nonignorable-nonresponse"]], "Overcoming regularization bias by orthogonalization": [[61, "Overcoming-regularization-bias-by-orthogonalization"], [80, "Overcoming-regularization-bias-by-orthogonalization"], [103, "overcoming-regularization-bias-by-orthogonalization"]], "Panel Data": [[124, "id9"], [126, null], [140, "panel-data"], [140, "id3"], [157, "panel-data"]], "Panel Data (Repeated Outcomes)": [[67, "Panel-Data-(Repeated-Outcomes)"]], "Panel data": [[124, "panel-data"]], "Parameter tuning": [[65, "Parameter-tuning"]], "Parameters & Implementation": [[124, "parameters-implementation"]], "Partialling out score": [[61, "Partialling-out-score"], [80, "Partialling-out-score"], [103, "partialling-out-score"]], "Partially Linear Regression Model (PLR)": [[64, "Partially-Linear-Regression-Model-(PLR)"], [86, "Partially-Linear-Regression-Model-(PLR)"], [92, "Partially-Linear-Regression-Model-(PLR)"]], "Partially linear IV regression model (PLIV)": [[124, "partially-linear-iv-regression-model-pliv"], [140, "partially-linear-iv-regression-model-pliv"]], "Partially linear models (PLM)": [[124, "partially-linear-models-plm"], [140, "partially-linear-models-plm"], [157, "partially-linear-models-plm"]], "Partially linear regression model (PLR)": [[124, "partially-linear-regression-model-plr"], [140, "partially-linear-regression-model-plr"], [157, "partially-linear-regression-model-plr"]], "Performance Summary and Insights": [[77, "Performance-Summary-and-Insights"]], "Plot Coefficients and 95% Confidence Intervals": [[90, "Plot-Coefficients-and-95%-Confidence-Intervals"]], "Policy Learning with Trees": [[95, "Policy-Learning-with-Trees"], [111, "policy-learning-with-trees"]], "Potential Quantile Estimation": [[96, "Potential-Quantile-Estimation"]], "Potential Quantiles (PQs)": [[96, "Potential-Quantiles-(PQs)"]], "Potential quantiles (PQs)": [[111, "potential-quantiles-pqs"], [140, "potential-quantiles-pqs"]], "Python: Average Potential Outcome (APO) Models": [[78, null]], "Python: Basic Instrumental Variables calculation": [[79, null]], "Python: Basics of Double Machine Learning": [[80, null]], "Python: Building the package from source": [[169, "python-building-the-package-from-source"]], "Python: Case studies": [[74, "python-case-studies"]], "Python: Causal Machine Learning with TabPFN": [[77, null]], "Python: Choice of learners": [[75, null]], "Python: Cluster Robust Double Machine Learning": [[91, null]], "Python: Conditional Average Treatment Effects (CATEs) for IRM models": [[81, null]], "Python: Conditional Average Treatment Effects (CATEs) for PLR models": [[82, null]], "Python: Conditional Value at Risk of potential outcomes": [[83, null]], "Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments": [[98, null]], "Python: Difference-in-Differences": [[67, null]], "Python: Difference-in-Differences Pre-Testing": [[68, null]], "Python: First Stage and Causal Estimation": [[84, null]], "Python: GATE Sensitivity Analysis": [[87, null]], "Python: Group Average Treatment Effects (GATEs) for IRM models": [[85, null]], "Python: Group Average Treatment Effects (GATEs) for PLR models": [[86, null]], "Python: Hyperparametertuning with Optuna": [[76, null]], "Python: IRM and APO Model Comparison": [[88, null]], "Python: Impact of 401(k) on Financial Wealth": [[92, null]], "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)": [[93, null]], "Python: Installing DoubleML": [[169, "python-installing-doubleml"]], "Python: Installing a released version from a .whl file": [[169, "python-installing-a-released-version-from-a-whl-file"]], "Python: Installing the latest release from pip or conda": [[169, "python-installing-the-latest-release-from-pip-or-conda"]], "Python: Learners and hyperparameters": [[112, "python-learners-and-hyperparameters"]], "Python: Log-Odds Effects for Logistic PLR models": [[89, null]], "Python: Optional Dependencies": [[169, "python-optional-dependencies"]], "Python: PLM and IRM for Multiple Treatments": [[94, null]], "Python: Panel Data Introduction": [[71, null]], "Python: Panel Data with Multiple Time Periods": [[69, null]], "Python: Policy Learning with Trees": [[95, null]], "Python: Potential Quantiles and Quantile Treatment Effects": [[96, null]], "Python: Real-Data Example for Multi-Period Difference-in-Differences": [[70, null]], "Python: Repeated Cross-Sectional Data with Multiple Time Periods": [[72, null]], "Python: Sample Selection Models": [[101, null]], "Python: Sensitivity Analysis": [[99, null]], "Python: Sensitivity Analysis for Causal ML": [[100, null]], "Quantile Treatment Effects (QTEs)": [[96, "Quantile-Treatment-Effects-(QTEs)"]], "Quantile treatment effects (QTEs)": [[111, "quantile-treatment-effects-qtes"]], "Quantiles": [[111, "quantiles"]], "R: Basic Instrumental Variables Calculation": [[60, null]], "R: Basics of Double Machine Learning": [[61, null]], "R: Case studies": [[74, "r-case-studies"]], "R: Cluster Robust Double Machine Learning": [[63, null]], "R: DoubleML for Difference-in-Differences": [[62, null]], "R: Ensemble Learners and More with mlr3pipelines": [[65, null]], "R: Impact of 401(k) on Financial Wealth": [[64, null]], "R: Installing DoubleML": [[169, "r-installing-doubleml"]], "R: Installing the development version from GitHub": [[169, "r-installing-the-development-version-from-github"]], "R: Installing the latest release from CRAN": [[169, "r-installing-the-latest-release-from-cran"]], "R: Learners and hyperparameters": [[112, "r-learners-and-hyperparameters"]], "R: Sample Selection Models": [[66, null]], "Ranking Treatment Effects under Treatment Propensity and Treatment Effect Heterogeneity": [[94, "Ranking-Treatment-Effects-under-Treatment-Propensity-and-Treatment-Effect-Heterogeneity"]], "Real-Data Application": [[63, "Real-Data-Application"], [91, "Real-Data-Application"]], "References": [[60, "References"], [62, "References"], [63, "References"], [65, "References"], [75, "References"], [79, "References"], [84, "References"], [90, "References"], [91, "References"], [94, "References"], [98, "References"], [100, "References"], [103, "references"], [112, "references"], [122, null], [139, "references"], [156, "references"], [168, "references"], [170, "references"]], "Regression Discontinuity Designs (RDD)": [[124, "regression-discontinuity-designs-rdd"]], "Regularization Bias in Simple ML-Approaches": [[61, "Regularization-Bias-in-Simple-ML-Approaches"], [80, "Regularization-Bias-in-Simple-ML-Approaches"]], "Regularization bias in simple ML-approaches": [[103, "regularization-bias-in-simple-ml-approaches"]], "Release Notes": [[172, null]], "Repeated Cross-Sectional Data": [[67, "Repeated-Cross-Sectional-Data"], [140, "repeated-cross-sectional-data"], [140, "id4"], [157, "repeated-cross-sectional-data"]], "Repeated cross-fitting with K folds and M repetitions": [[139, "repeated-cross-fitting-with-k-folds-and-m-repetitions"]], "Repeated cross-sections": [[124, "repeated-cross-sections"], [124, "id10"], [126, "repeated-cross-sections"]], "Running a small simulation": [[98, "Running-a-small-simulation"]], "Sample Selection Models": [[140, "sample-selection-models"]], "Sample Selection Models (SSM)": [[124, "sample-selection-models-ssm"]], "Sample splitting to remove bias induced by overfitting": [[61, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [80, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [103, "sample-splitting-to-remove-bias-induced-by-overfitting"]], "Sample-splitting without cross-fitting": [[139, "sample-splitting-without-cross-fitting"]], "Sample-splitting, cross-fitting and repeated cross-fitting": [[139, null]], "Sandbox/Archive": [[74, "sandbox-archive"]], "Score Mixin Classes for DoubleML Models": [[58, null]], "Score functions": [[140, null]], "Selected Combinations": [[69, "Selected-Combinations"], [72, "Selected-Combinations"]], "Sensitivity Analysis": [[69, "Sensitivity-Analysis"], [72, "Sensitivity-Analysis"], [78, "Sensitivity-Analysis"], [88, "Sensitivity-Analysis"], [99, "Sensitivity-Analysis"], [99, "id1"]], "Sensitivity Analysis with IRM": [[99, "Sensitivity-Analysis-with-IRM"]], "Sensitivity analysis": [[157, null]], "Set up learners based on mlr3pipelines": [[65, "Set-up-learners-based-on-mlr3pipelines"]], "Sharp RDD": [[97, "Sharp-RDD"]], "Sharp RDD Without Adjustment": [[97, "Sharp-RDD-Without-Adjustment"]], "Sharp RDD with Flexible Adjustment": [[97, "Sharp-RDD-with-Flexible-Adjustment"]], "Sharp RDD with Linear Adjustment": [[97, "Sharp-RDD-with-Linear-Adjustment"]], "Sharp Regression Discontinuity Design": [[124, "sharp-regression-discontinuity-design"]], "Simulate two-way cluster data": [[63, "Simulate-two-way-cluster-data"], [91, "Simulate-two-way-cluster-data"]], "Simulation Example": [[99, "Simulation-Example"]], "Simultaneous inference over different DoubleML models (advanced)": [[156, "simultaneous-inference-over-different-doubleml-models-advanced"]], "Single Average Potential Outcome Models (APO)": [[78, "Single-Average-Potential-Outcome-Models-(APO)"]], "Source code and maintenance": [[168, "source-code-and-maintenance"]], "Special Data Types": [[109, "special-data-types"]], "Specify learner and estimate causal parameter: IRM model with Lasso as learner": [[73, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: IRM model with random forest as learner": [[73, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-random-forest-as-learner"]], "Specify learner and estimate causal parameter: PLR model with Lasso as learner": [[73, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: PLR model with random forest as learner": [[73, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-random-forest-as-learner"]], "Specifying alternative score functions via callables": [[140, "specifying-alternative-score-functions-via-callables"]], "Specifying learners and set hyperparameters": [[112, "specifying-learners-and-set-hyperparameters"], [112, "r-set-params"]], "Standard approach": [[75, "Standard-approach"]], "Step 1: Custom API for FLAML Models within DoubleML": [[90, "Step-1:-Custom-API-for-FLAML-Models-within-DoubleML"]], "Step 1: Initialize and Train the AutoML Models:": [[90, "Step-1:-Initialize-and-Train-the-AutoML-Models:"]], "Step 2: Evaluate the Tuned Models": [[90, "Step-2:-Evaluate-the-Tuned-Models"]], "Step 2: Using the API when calling DoubleML\u2019s .fit() Method": [[90, "Step-2:-Using-the-API-when-calling-DoubleML's-.fit()-Method"]], "Step 3: Create and Fit DoubleML Model": [[90, "Step-3:-Create-and-Fit-DoubleML-Model"]], "Summary Figure": [[94, "Summary-Figure"]], "Summary of Results": [[64, "Summary-of-Results"], [92, "Summary-of-Results"]], "The Augmented Inverse Propensity Weighting Model estimates the ATE under arbitrary effect and propensity score heterogeneity": [[94, "The-Augmented-Inverse-Propensity-Weighting-Model-estimates-the-ATE-under-arbitrary-effect-and-propensity-score-heterogeneity"]], "The Data Backend: DoubleMLData": [[64, "The-Data-Backend:-DoubleMLData"], [92, "The-Data-Backend:-DoubleMLData"]], "The DoubleML package": [[64, "The-DoubleML-package"], [92, "The-DoubleML-package"]], "The Partially Linear Model performs overlap weighting": [[94, "The-Partially-Linear-Model-performs-overlap-weighting"]], "The basics of double/debiased machine learning": [[103, null]], "The causal model": [[170, "the-causal-model"]], "The data-backend DoubleMLData": [[170, "the-data-backend-doublemldata"]], "Theory": [[157, "theory"]], "Time Aggregation": [[69, "Time-Aggregation"], [71, "Time-Aggregation"], [72, "Time-Aggregation"]], "Tuning on the Folds": [[90, "Tuning-on-the-Folds"]], "Tuning on the full Sample": [[90, "Tuning-on-the-full-Sample"]], "Two treatment periods": [[124, "two-treatment-periods"], [140, "two-treatment-periods"], [157, "two-treatment-periods"]], "Two-Dimensional Example": [[81, "Two-Dimensional-Example"], [82, "Two-Dimensional-Example"]], "Two-Way Clustering with Respect to Product and Market": [[63, "Two-Way-Clustering-with-Respect-to-Product-and-Market"], [91, "Two-Way-Clustering-with-Respect-to-Product-and-Market"]], "Universal Base Period": [[69, "Universal-Base-Period"], [72, "Universal-Base-Period"]], "Untuned (default parameter) XGBoost": [[90, "Untuned-(default-parameter)-XGBoost"]], "Untuned Model": [[76, "Untuned-Model"], [76, "id3"]], "Untuned Model with Pipeline": [[76, "Untuned-Model-with-Pipeline"]], "Use ensemble learners based on mlr3pipelines": [[65, "Use-ensemble-learners-based-on-mlr3pipelines"]], "User Guide": [[110, null]], "Using DoubleML": [[60, "Using-DoubleML"], [79, "Using-DoubleML"]], "Using ML for DiD: Integrating DoubleML in did": [[62, "Using-ML-for-DiD:-Integrating-DoubleML-in-did"]], "Using learners from mlr3, mlr3learners and mlr3extralearners": [[65, "Using-learners-from-mlr3,-mlr3learners-and-mlr3extralearners"]], "Using pipelines to construct learners": [[112, "using-pipelines-to-construct-learners"]], "Utility Classes": [[59, "utility-classes"]], "Utility Classes and Functions": [[59, null]], "Utility Functions": [[59, "utility-functions"]], "VanderWeele and Arah (2011): Benchmarking": [[100, "VanderWeele-and-Arah-(2011):-Benchmarking"]], "Variance estimation": [[156, "variance-estimation"]], "Variance estimation and confidence intervals": [[156, null]], "Visualizations": [[89, "Visualizations"]], "Visualizing Average Potential Outcomes": [[77, "Visualizing-Average-Potential-Outcomes"]], "Visualizing Average Treatment Effects": [[77, "Visualizing-Average-Treatment-Effects"]], "Visualizing the Treatment Effect Structure": [[77, "Visualizing-the-Treatment-Effect-Structure"]], "Weighted Average Treatment Effects": [[111, "weighted-average-treatment-effects"]], "doubleml.data.DoubleMLClusterData": [[4, null]], "doubleml.data.DoubleMLDIDData": [[5, null]], "doubleml.data.DoubleMLData": [[6, null]], "doubleml.data.DoubleMLPanelData": [[7, null]], "doubleml.data.DoubleMLRDDData": [[8, null]], "doubleml.data.DoubleMLSSMData": [[9, null]], "doubleml.datasets.fetch_401K": [[10, null]], "doubleml.datasets.fetch_bonus": [[11, null]], "doubleml.did": [[3, "doubleml-did"]], "doubleml.did.DoubleMLDID": [[12, null]], "doubleml.did.DoubleMLDIDAggregation": [[13, null]], "doubleml.did.DoubleMLDIDBinary": [[14, null]], "doubleml.did.DoubleMLDIDCS": [[15, null]], "doubleml.did.DoubleMLDIDMulti": [[16, null]], "doubleml.did.datasets.make_did_CS2021": [[17, null]], "doubleml.did.datasets.make_did_SZ2020": [[18, null]], "doubleml.did.datasets.make_did_cs_CS2021": [[19, null]], "doubleml.double_ml_score_mixins.LinearScoreMixin": [[20, null]], "doubleml.double_ml_score_mixins.NonLinearScoreMixin": [[21, null]], "doubleml.irm": [[3, "doubleml-irm"]], "doubleml.irm.DoubleMLAPO": [[22, null]], "doubleml.irm.DoubleMLAPOS": [[23, null]], "doubleml.irm.DoubleMLCVAR": [[24, null]], "doubleml.irm.DoubleMLIIVM": [[25, null]], "doubleml.irm.DoubleMLIRM": [[26, null]], "doubleml.irm.DoubleMLLPQ": [[27, null]], "doubleml.irm.DoubleMLPQ": [[28, null]], "doubleml.irm.DoubleMLQTE": [[29, null]], "doubleml.irm.DoubleMLSSM": [[30, null]], "doubleml.irm.datasets.make_confounded_irm_data": [[31, null]], "doubleml.irm.datasets.make_heterogeneous_data": [[32, null]], "doubleml.irm.datasets.make_iivm_data": [[33, null]], "doubleml.irm.datasets.make_irm_data": [[34, null]], "doubleml.irm.datasets.make_irm_data_discrete_treatments": [[35, null]], "doubleml.irm.datasets.make_ssm_data": [[36, null]], "doubleml.plm": [[3, "doubleml-plm"]], "doubleml.plm.DoubleMLLPLR": [[37, null]], "doubleml.plm.DoubleMLPLIV": [[38, null]], "doubleml.plm.DoubleMLPLR": [[39, null]], "doubleml.plm.datasets.make_confounded_plr_data": [[40, null]], "doubleml.plm.datasets.make_lplr_LZZ2020": [[41, null]], "doubleml.plm.datasets.make_pliv_CHS2015": [[42, null]], "doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021": [[43, null]], "doubleml.plm.datasets.make_plr_CCDDHNR2018": [[44, null]], "doubleml.plm.datasets.make_plr_turrell2018": [[45, null]], "doubleml.rdd": [[3, "doubleml-rdd"]], "doubleml.rdd.RDFlex": [[46, null]], "doubleml.rdd.datasets.make_simple_rdd_data": [[47, null]], "doubleml.utils.DMLDummyClassifier": [[48, null]], "doubleml.utils.DMLDummyRegressor": [[49, null]], "doubleml.utils.DMLOptunaResult": [[50, null]], "doubleml.utils.DoubleMLBLP": [[51, null]], "doubleml.utils.DoubleMLPolicyTree": [[52, null]], "doubleml.utils.GlobalClassifier": [[53, null]], "doubleml.utils.GlobalRegressor": [[54, null]], "doubleml.utils.PSProcessor": [[55, null]], "doubleml.utils.PSProcessorConfig": [[56, null]], "doubleml.utils.gain_statistics": [[57, null]]}, "docnames": ["api/api", "api/data_class", "api/datasets", "api/dml_models", "api/generated/doubleml.data.DoubleMLClusterData", "api/generated/doubleml.data.DoubleMLDIDData", "api/generated/doubleml.data.DoubleMLData", "api/generated/doubleml.data.DoubleMLPanelData", "api/generated/doubleml.data.DoubleMLRDDData", "api/generated/doubleml.data.DoubleMLSSMData", "api/generated/doubleml.datasets.fetch_401K", "api/generated/doubleml.datasets.fetch_bonus", "api/generated/doubleml.did.DoubleMLDID", "api/generated/doubleml.did.DoubleMLDIDAggregation", "api/generated/doubleml.did.DoubleMLDIDBinary", "api/generated/doubleml.did.DoubleMLDIDCS", "api/generated/doubleml.did.DoubleMLDIDMulti", "api/generated/doubleml.did.datasets.make_did_CS2021", "api/generated/doubleml.did.datasets.make_did_SZ2020", "api/generated/doubleml.did.datasets.make_did_cs_CS2021", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin", "api/generated/doubleml.irm.DoubleMLAPO", "api/generated/doubleml.irm.DoubleMLAPOS", "api/generated/doubleml.irm.DoubleMLCVAR", "api/generated/doubleml.irm.DoubleMLIIVM", "api/generated/doubleml.irm.DoubleMLIRM", "api/generated/doubleml.irm.DoubleMLLPQ", "api/generated/doubleml.irm.DoubleMLPQ", "api/generated/doubleml.irm.DoubleMLQTE", "api/generated/doubleml.irm.DoubleMLSSM", "api/generated/doubleml.irm.datasets.make_confounded_irm_data", "api/generated/doubleml.irm.datasets.make_heterogeneous_data", "api/generated/doubleml.irm.datasets.make_iivm_data", "api/generated/doubleml.irm.datasets.make_irm_data", "api/generated/doubleml.irm.datasets.make_irm_data_discrete_treatments", "api/generated/doubleml.irm.datasets.make_ssm_data", "api/generated/doubleml.plm.DoubleMLLPLR", "api/generated/doubleml.plm.DoubleMLPLIV", "api/generated/doubleml.plm.DoubleMLPLR", "api/generated/doubleml.plm.datasets.make_confounded_plr_data", "api/generated/doubleml.plm.datasets.make_lplr_LZZ2020", "api/generated/doubleml.plm.datasets.make_pliv_CHS2015", "api/generated/doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021", "api/generated/doubleml.plm.datasets.make_plr_CCDDHNR2018", "api/generated/doubleml.plm.datasets.make_plr_turrell2018", "api/generated/doubleml.rdd.RDFlex", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data", "api/generated/doubleml.utils.DMLDummyClassifier", "api/generated/doubleml.utils.DMLDummyRegressor", "api/generated/doubleml.utils.DMLOptunaResult", "api/generated/doubleml.utils.DoubleMLBLP", "api/generated/doubleml.utils.DoubleMLPolicyTree", "api/generated/doubleml.utils.GlobalClassifier", "api/generated/doubleml.utils.GlobalRegressor", "api/generated/doubleml.utils.PSProcessor", "api/generated/doubleml.utils.PSProcessorConfig", "api/generated/doubleml.utils.gain_statistics", "api/mixins", "api/utility", "examples/R_double_ml_basic_iv", "examples/R_double_ml_basics", "examples/R_double_ml_did", "examples/R_double_ml_multiway_cluster", "examples/R_double_ml_pension", "examples/R_double_ml_pipeline", "examples/R_double_ml_ssm", "examples/did/py_did", "examples/did/py_did_pretest", "examples/did/py_panel", "examples/did/py_panel_data_example", "examples/did/py_panel_simple", "examples/did/py_rep_cs", "examples/double_ml_bonus_data", "examples/index", "examples/learners/py_learner", "examples/learners/py_optuna", "examples/learners/py_tabpfn", "examples/py_double_ml_apo", "examples/py_double_ml_basic_iv", "examples/py_double_ml_basics", "examples/py_double_ml_cate", "examples/py_double_ml_cate_plr", "examples/py_double_ml_cvar", "examples/py_double_ml_firststage", "examples/py_double_ml_gate", "examples/py_double_ml_gate_plr", "examples/py_double_ml_gate_sensitivity", "examples/py_double_ml_irm_vs_apo", "examples/py_double_ml_lplr", "examples/py_double_ml_meets_flaml", "examples/py_double_ml_multiway_cluster", "examples/py_double_ml_pension", "examples/py_double_ml_pension_qte", "examples/py_double_ml_plm_irm_hetfx", "examples/py_double_ml_policy_tree", "examples/py_double_ml_pq", "examples/py_double_ml_rdflex", "examples/py_double_ml_robust_iv", "examples/py_double_ml_sensitivity", "examples/py_double_ml_sensitivity_booking", "examples/py_double_ml_ssm", "guide/algorithms", "guide/basics", "guide/data/base_data", "guide/data/did_data", "guide/data/panel_data", "guide/data/rdd_data", "guide/data/ssm_data", "guide/data_backend", "guide/guide", "guide/heterogeneity", "guide/learners", "guide/learners/python/evaluate_learners", "guide/learners/python/external_preds", "guide/learners/python/minimum_req", "guide/learners/python/set_hyperparams", "guide/learners/python/tune_hyperparams", "guide/learners/python/tune_hyperparams_old", "guide/learners/r/minimum_req", "guide/learners/r/pipelines", "guide/learners/r/set_hyperparams", "guide/learners/r/tune_and_pipelines", "guide/learners/r/tune_hyperparams", "guide/models", "guide/models/did/did_aggregation", "guide/models/did/did_binary", "guide/models/did/did_cs", "guide/models/did/did_implementation", "guide/models/did/did_pa", "guide/models/did/did_setup", "guide/models/irm/apo", "guide/models/irm/apos", "guide/models/irm/iivm", "guide/models/irm/irm", "guide/models/plm/lplr", "guide/models/plm/pliv", "guide/models/plm/plr", "guide/models/ssm/ssm", "guide/resampling", "guide/scores", "guide/scores/did/did_cs_binary_score", "guide/scores/did/did_cs_score", "guide/scores/did/did_pa_binary_score", "guide/scores/did/did_pa_score", "guide/scores/irm/apo_score", "guide/scores/irm/cvar_score", "guide/scores/irm/iivm_score", "guide/scores/irm/irm_score", "guide/scores/irm/lpq_score", "guide/scores/irm/pq_score", "guide/scores/plm/lplr_score", "guide/scores/plm/pliv_score", "guide/scores/plm/plr_score", "guide/scores/ssm/mar_score", "guide/scores/ssm/nr_score", "guide/se_confint", "guide/sensitivity", "guide/sensitivity/benchmarking", "guide/sensitivity/did/did_cs_binary_sensitivity", "guide/sensitivity/did/did_cs_sensitivity", "guide/sensitivity/did/did_pa_binary_sensitivity", "guide/sensitivity/did/did_pa_sensitivity", "guide/sensitivity/implementation", "guide/sensitivity/irm/apo_sensitivity", "guide/sensitivity/irm/irm_sensitivity", "guide/sensitivity/plm/plr_sensitivity", "guide/sensitivity/theory", "index", "intro/install", "intro/intro", "literature/literature", "release/release", "workflow/workflow"], "envversion": {"nbsphinx": 4, "sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1}, "filenames": ["api/api.rst", "api/data_class.rst", "api/datasets.rst", "api/dml_models.rst", "api/generated/doubleml.data.DoubleMLClusterData.rst", "api/generated/doubleml.data.DoubleMLDIDData.rst", "api/generated/doubleml.data.DoubleMLData.rst", "api/generated/doubleml.data.DoubleMLPanelData.rst", "api/generated/doubleml.data.DoubleMLRDDData.rst", "api/generated/doubleml.data.DoubleMLSSMData.rst", "api/generated/doubleml.datasets.fetch_401K.rst", "api/generated/doubleml.datasets.fetch_bonus.rst", "api/generated/doubleml.did.DoubleMLDID.rst", "api/generated/doubleml.did.DoubleMLDIDAggregation.rst", "api/generated/doubleml.did.DoubleMLDIDBinary.rst", "api/generated/doubleml.did.DoubleMLDIDCS.rst", "api/generated/doubleml.did.DoubleMLDIDMulti.rst", "api/generated/doubleml.did.datasets.make_did_CS2021.rst", "api/generated/doubleml.did.datasets.make_did_SZ2020.rst", "api/generated/doubleml.did.datasets.make_did_cs_CS2021.rst", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin.rst", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin.rst", "api/generated/doubleml.irm.DoubleMLAPO.rst", "api/generated/doubleml.irm.DoubleMLAPOS.rst", "api/generated/doubleml.irm.DoubleMLCVAR.rst", "api/generated/doubleml.irm.DoubleMLIIVM.rst", "api/generated/doubleml.irm.DoubleMLIRM.rst", "api/generated/doubleml.irm.DoubleMLLPQ.rst", "api/generated/doubleml.irm.DoubleMLPQ.rst", "api/generated/doubleml.irm.DoubleMLQTE.rst", "api/generated/doubleml.irm.DoubleMLSSM.rst", "api/generated/doubleml.irm.datasets.make_confounded_irm_data.rst", "api/generated/doubleml.irm.datasets.make_heterogeneous_data.rst", "api/generated/doubleml.irm.datasets.make_iivm_data.rst", "api/generated/doubleml.irm.datasets.make_irm_data.rst", "api/generated/doubleml.irm.datasets.make_irm_data_discrete_treatments.rst", "api/generated/doubleml.irm.datasets.make_ssm_data.rst", "api/generated/doubleml.plm.DoubleMLLPLR.rst", "api/generated/doubleml.plm.DoubleMLPLIV.rst", "api/generated/doubleml.plm.DoubleMLPLR.rst", "api/generated/doubleml.plm.datasets.make_confounded_plr_data.rst", "api/generated/doubleml.plm.datasets.make_lplr_LZZ2020.rst", "api/generated/doubleml.plm.datasets.make_pliv_CHS2015.rst", "api/generated/doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021.rst", "api/generated/doubleml.plm.datasets.make_plr_CCDDHNR2018.rst", "api/generated/doubleml.plm.datasets.make_plr_turrell2018.rst", "api/generated/doubleml.rdd.RDFlex.rst", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data.rst", "api/generated/doubleml.utils.DMLDummyClassifier.rst", "api/generated/doubleml.utils.DMLDummyRegressor.rst", "api/generated/doubleml.utils.DMLOptunaResult.rst", "api/generated/doubleml.utils.DoubleMLBLP.rst", "api/generated/doubleml.utils.DoubleMLPolicyTree.rst", "api/generated/doubleml.utils.GlobalClassifier.rst", "api/generated/doubleml.utils.GlobalRegressor.rst", "api/generated/doubleml.utils.PSProcessor.rst", "api/generated/doubleml.utils.PSProcessorConfig.rst", "api/generated/doubleml.utils.gain_statistics.rst", "api/mixins.rst", "api/utility.rst", "examples/R_double_ml_basic_iv.ipynb", "examples/R_double_ml_basics.ipynb", "examples/R_double_ml_did.ipynb", "examples/R_double_ml_multiway_cluster.ipynb", "examples/R_double_ml_pension.ipynb", "examples/R_double_ml_pipeline.ipynb", "examples/R_double_ml_ssm.ipynb", "examples/did/py_did.ipynb", "examples/did/py_did_pretest.ipynb", "examples/did/py_panel.ipynb", "examples/did/py_panel_data_example.ipynb", "examples/did/py_panel_simple.ipynb", "examples/did/py_rep_cs.ipynb", "examples/double_ml_bonus_data.ipynb", "examples/index.rst", "examples/learners/py_learner.ipynb", "examples/learners/py_optuna.ipynb", "examples/learners/py_tabpfn.ipynb", "examples/py_double_ml_apo.ipynb", "examples/py_double_ml_basic_iv.ipynb", "examples/py_double_ml_basics.ipynb", "examples/py_double_ml_cate.ipynb", "examples/py_double_ml_cate_plr.ipynb", "examples/py_double_ml_cvar.ipynb", "examples/py_double_ml_firststage.ipynb", "examples/py_double_ml_gate.ipynb", "examples/py_double_ml_gate_plr.ipynb", "examples/py_double_ml_gate_sensitivity.ipynb", "examples/py_double_ml_irm_vs_apo.ipynb", "examples/py_double_ml_lplr.ipynb", "examples/py_double_ml_meets_flaml.ipynb", "examples/py_double_ml_multiway_cluster.ipynb", "examples/py_double_ml_pension.ipynb", "examples/py_double_ml_pension_qte.ipynb", "examples/py_double_ml_plm_irm_hetfx.ipynb", "examples/py_double_ml_policy_tree.ipynb", "examples/py_double_ml_pq.ipynb", "examples/py_double_ml_rdflex.ipynb", "examples/py_double_ml_robust_iv.ipynb", "examples/py_double_ml_sensitivity.ipynb", "examples/py_double_ml_sensitivity_booking.ipynb", "examples/py_double_ml_ssm.ipynb", "guide/algorithms.rst", "guide/basics.rst", "guide/data/base_data.rst", "guide/data/did_data.rst", "guide/data/panel_data.rst", "guide/data/rdd_data.rst", "guide/data/ssm_data.rst", "guide/data_backend.rst", "guide/guide.rst", "guide/heterogeneity.rst", "guide/learners.rst", "guide/learners/python/evaluate_learners.rst", "guide/learners/python/external_preds.rst", "guide/learners/python/minimum_req.rst", "guide/learners/python/set_hyperparams.rst", "guide/learners/python/tune_hyperparams.rst", "guide/learners/python/tune_hyperparams_old.rst", "guide/learners/r/minimum_req.rst", "guide/learners/r/pipelines.rst", "guide/learners/r/set_hyperparams.rst", "guide/learners/r/tune_and_pipelines.rst", "guide/learners/r/tune_hyperparams.rst", "guide/models.rst", "guide/models/did/did_aggregation.rst", "guide/models/did/did_binary.rst", "guide/models/did/did_cs.rst", "guide/models/did/did_implementation.rst", "guide/models/did/did_pa.rst", "guide/models/did/did_setup.rst", "guide/models/irm/apo.rst", "guide/models/irm/apos.rst", "guide/models/irm/iivm.rst", "guide/models/irm/irm.rst", "guide/models/plm/lplr.rst", "guide/models/plm/pliv.rst", "guide/models/plm/plr.rst", "guide/models/ssm/ssm.rst", "guide/resampling.rst", "guide/scores.rst", "guide/scores/did/did_cs_binary_score.rst", "guide/scores/did/did_cs_score.rst", "guide/scores/did/did_pa_binary_score.rst", "guide/scores/did/did_pa_score.rst", "guide/scores/irm/apo_score.rst", "guide/scores/irm/cvar_score.rst", "guide/scores/irm/iivm_score.rst", "guide/scores/irm/irm_score.rst", "guide/scores/irm/lpq_score.rst", "guide/scores/irm/pq_score.rst", "guide/scores/plm/lplr_score.rst", "guide/scores/plm/pliv_score.rst", "guide/scores/plm/plr_score.rst", "guide/scores/ssm/mar_score.rst", "guide/scores/ssm/nr_score.rst", "guide/se_confint.rst", "guide/sensitivity.rst", "guide/sensitivity/benchmarking.rst", "guide/sensitivity/did/did_cs_binary_sensitivity.rst", "guide/sensitivity/did/did_cs_sensitivity.rst", "guide/sensitivity/did/did_pa_binary_sensitivity.rst", "guide/sensitivity/did/did_pa_sensitivity.rst", "guide/sensitivity/implementation.rst", "guide/sensitivity/irm/apo_sensitivity.rst", "guide/sensitivity/irm/irm_sensitivity.rst", "guide/sensitivity/plm/plr_sensitivity.rst", "guide/sensitivity/theory.rst", "index.rst", "intro/install.rst", "intro/intro.rst", "literature/literature.rst", "release/release.rst", "workflow/workflow.rst"], "indexentries": {"adjust_ps() (doubleml.utils.psprocessor method)": [[55, "doubleml.utils.PSProcessor.adjust_ps", false]], "aggregate() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.aggregate", false]], "aggregate_over_splits() (doubleml.rdd.rdflex method)": [[46, "doubleml.rdd.RDFlex.aggregate_over_splits", false]], "bootstrap() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.bootstrap", false]], "bootstrap() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.bootstrap", false]], "bootstrap() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.bootstrap", false]], "capo() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.capo", false]], "cate() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.cate", false]], "cate() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.cate", false]], "causal_contrast() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.causal_contrast", false]], "confint() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.confint", false]], "confint() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.confint", false]], "confint() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.confint", false]], "confint() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.confint", false]], "confint() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.confint", false]], "confint() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.confint", false]], "confint() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.confint", false]], "confint() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.confint", false]], "confint() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.confint", false]], "confint() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.confint", false]], "confint() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.confint", false]], "confint() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.confint", false]], "confint() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.confint", false]], "confint() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.confint", false]], "confint() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.confint", false]], "confint() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.confint", false]], "confint() (doubleml.rdd.rdflex method)": [[46, "doubleml.rdd.RDFlex.confint", false]], "confint() (doubleml.utils.doublemlblp method)": [[51, "doubleml.utils.DoubleMLBLP.confint", false]], "construct_framework() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.construct_framework", false]], "construct_framework() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.construct_framework", false]], "construct_framework() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.construct_framework", false]], "dmldummyclassifier (class in doubleml.utils)": [[48, "doubleml.utils.DMLDummyClassifier", false]], "dmldummyregressor (class in doubleml.utils)": [[49, "doubleml.utils.DMLDummyRegressor", false]], "dmloptunaresult (class in doubleml.utils)": [[50, "doubleml.utils.DMLOptunaResult", false]], "doublemlapo (class in doubleml.irm)": [[22, "doubleml.irm.DoubleMLAPO", false]], "doublemlapos (class in doubleml.irm)": [[23, "doubleml.irm.DoubleMLAPOS", false]], "doublemlblp (class in doubleml.utils)": [[51, "doubleml.utils.DoubleMLBLP", false]], "doublemlclusterdata (class in doubleml.data)": [[4, "doubleml.data.DoubleMLClusterData", false]], "doublemlcvar (class in doubleml.irm)": [[24, "doubleml.irm.DoubleMLCVAR", false]], "doublemldata (class in doubleml.data)": [[6, "doubleml.data.DoubleMLData", false]], "doublemldid (class in doubleml.did)": [[12, "doubleml.did.DoubleMLDID", false]], "doublemldidaggregation (class in doubleml.did)": [[13, "doubleml.did.DoubleMLDIDAggregation", false]], "doublemldidbinary (class in doubleml.did)": [[14, "doubleml.did.DoubleMLDIDBinary", false]], "doublemldidcs (class in doubleml.did)": [[15, "doubleml.did.DoubleMLDIDCS", false]], "doublemldiddata (class in doubleml.data)": [[5, "doubleml.data.DoubleMLDIDData", false]], "doublemldidmulti (class in doubleml.did)": [[16, "doubleml.did.DoubleMLDIDMulti", false]], "doublemliivm (class in doubleml.irm)": [[25, "doubleml.irm.DoubleMLIIVM", false]], "doublemlirm (class in doubleml.irm)": [[26, "doubleml.irm.DoubleMLIRM", false]], "doublemllplr (class in doubleml.plm)": [[37, "doubleml.plm.DoubleMLLPLR", false]], "doublemllpq (class in doubleml.irm)": [[27, "doubleml.irm.DoubleMLLPQ", false]], "doublemlpaneldata (class in doubleml.data)": [[7, "doubleml.data.DoubleMLPanelData", false]], "doublemlpliv (class in doubleml.plm)": [[38, "doubleml.plm.DoubleMLPLIV", false]], "doublemlplr (class in doubleml.plm)": [[39, "doubleml.plm.DoubleMLPLR", false]], "doublemlpolicytree (class in doubleml.utils)": [[52, "doubleml.utils.DoubleMLPolicyTree", false]], "doublemlpq (class in doubleml.irm)": [[28, "doubleml.irm.DoubleMLPQ", false]], "doublemlqte (class in doubleml.irm)": [[29, "doubleml.irm.DoubleMLQTE", false]], "doublemlrdddata (class in doubleml.data)": [[8, "doubleml.data.DoubleMLRDDData", false]], "doublemlssm (class in doubleml.irm)": [[30, "doubleml.irm.DoubleMLSSM", false]], "doublemlssmdata (class in doubleml.data)": [[9, "doubleml.data.DoubleMLSSMData", false]], "draw_sample_splitting() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.draw_sample_splitting", false]], "evaluate_learners() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.evaluate_learners", false]], "fetch_401k() (in module doubleml.datasets)": [[10, "doubleml.datasets.fetch_401K", false]], "fetch_bonus() (in module doubleml.datasets)": [[11, "doubleml.datasets.fetch_bonus", false]], "fit() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.fit", false]], "fit() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.fit", false]], "fit() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.fit", false]], "fit() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.fit", false]], "fit() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.fit", false]], "fit() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.fit", false]], "fit() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.fit", false]], "fit() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.fit", false]], "fit() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.fit", false]], "fit() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.fit", false]], "fit() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.fit", false]], "fit() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.fit", false]], "fit() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.fit", false]], "fit() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.fit", false]], "fit() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.fit", false]], "fit() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.fit", false]], "fit() (doubleml.rdd.rdflex method)": [[46, "doubleml.rdd.RDFlex.fit", false]], "fit() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.fit", false]], "fit() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.fit", false]], "fit() (doubleml.utils.doublemlblp method)": [[51, "doubleml.utils.DoubleMLBLP.fit", false]], "fit() (doubleml.utils.doublemlpolicytree method)": [[52, "doubleml.utils.DoubleMLPolicyTree.fit", false]], "fit() (doubleml.utils.globalclassifier method)": [[53, "doubleml.utils.GlobalClassifier.fit", false]], "fit() (doubleml.utils.globalregressor method)": [[54, "doubleml.utils.GlobalRegressor.fit", false]], "from_arrays() (doubleml.data.doublemlclusterdata class method)": [[4, "doubleml.data.DoubleMLClusterData.from_arrays", false]], "from_arrays() (doubleml.data.doublemldata class method)": [[6, "doubleml.data.DoubleMLData.from_arrays", false]], "from_arrays() (doubleml.data.doublemldiddata class method)": [[5, "doubleml.data.DoubleMLDIDData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlpaneldata class method)": [[7, "doubleml.data.DoubleMLPanelData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlrdddata class method)": [[8, "doubleml.data.DoubleMLRDDData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlssmdata class method)": [[9, "doubleml.data.DoubleMLSSMData.from_arrays", false]], "from_config() (doubleml.utils.psprocessor class method)": [[55, "doubleml.utils.PSProcessor.from_config", false]], "gain_statistics() (in module doubleml.utils)": [[57, "doubleml.utils.gain_statistics", false]], "gapo() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.gapo", false]], "gate() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.gate", false]], "gate() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.gate", false]], "get_metadata_routing() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalclassifier method)": [[53, "doubleml.utils.GlobalClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalregressor method)": [[54, "doubleml.utils.GlobalRegressor.get_metadata_routing", false]], "get_params() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.get_params", false]], "get_params() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.get_params", false]], "get_params() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.get_params", false]], "get_params() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.get_params", false]], "get_params() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.get_params", false]], "get_params() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.get_params", false]], "get_params() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.get_params", false]], "get_params() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.get_params", false]], "get_params() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.get_params", false]], "get_params() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.get_params", false]], "get_params() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.get_params", false]], "get_params() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.get_params", false]], "get_params() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.get_params", false]], "get_params() (doubleml.utils.globalclassifier method)": [[53, "doubleml.utils.GlobalClassifier.get_params", false]], "get_params() (doubleml.utils.globalregressor method)": [[54, "doubleml.utils.GlobalRegressor.get_params", false]], "globalclassifier (class in doubleml.utils)": [[53, "doubleml.utils.GlobalClassifier", false]], "globalregressor (class in doubleml.utils)": [[54, "doubleml.utils.GlobalRegressor", false]], "linearscoremixin (class in doubleml.double_ml_score_mixins)": [[20, "doubleml.double_ml_score_mixins.LinearScoreMixin", false]], "make_confounded_irm_data() (in module doubleml.irm.datasets)": [[31, "doubleml.irm.datasets.make_confounded_irm_data", false]], "make_confounded_plr_data() (in module doubleml.plm.datasets)": [[40, "doubleml.plm.datasets.make_confounded_plr_data", false]], "make_did_cs2021() (in module doubleml.did.datasets)": [[17, "doubleml.did.datasets.make_did_CS2021", false]], "make_did_cs_cs2021() (in module doubleml.did.datasets)": [[19, "doubleml.did.datasets.make_did_cs_CS2021", false]], "make_did_sz2020() (in module doubleml.did.datasets)": [[18, "doubleml.did.datasets.make_did_SZ2020", false]], "make_heterogeneous_data() (in module doubleml.irm.datasets)": [[32, "doubleml.irm.datasets.make_heterogeneous_data", false]], "make_iivm_data() (in module doubleml.irm.datasets)": [[33, "doubleml.irm.datasets.make_iivm_data", false]], "make_irm_data() (in module doubleml.irm.datasets)": [[34, "doubleml.irm.datasets.make_irm_data", false]], "make_irm_data_discrete_treatments() (in module doubleml.irm.datasets)": [[35, "doubleml.irm.datasets.make_irm_data_discrete_treatments", false]], "make_lplr_lzz2020() (in module doubleml.plm.datasets)": [[41, "doubleml.plm.datasets.make_lplr_LZZ2020", false]], "make_pliv_chs2015() (in module doubleml.plm.datasets)": [[42, "doubleml.plm.datasets.make_pliv_CHS2015", false]], "make_pliv_multiway_cluster_ckms2021() (in module doubleml.plm.datasets)": [[43, "doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021", false]], "make_plr_ccddhnr2018() (in module doubleml.plm.datasets)": [[44, "doubleml.plm.datasets.make_plr_CCDDHNR2018", false]], "make_plr_turrell2018() (in module doubleml.plm.datasets)": [[45, "doubleml.plm.datasets.make_plr_turrell2018", false]], "make_simple_rdd_data() (in module doubleml.rdd.datasets)": [[47, "doubleml.rdd.datasets.make_simple_rdd_data", false]], "make_ssm_data() (in module doubleml.irm.datasets)": [[36, "doubleml.irm.datasets.make_ssm_data", false]], "nonlinearscoremixin (class in doubleml.double_ml_score_mixins)": [[21, "doubleml.double_ml_score_mixins.NonLinearScoreMixin", false]], "p_adjust() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.p_adjust", false]], "p_adjust() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.p_adjust", false]], "p_adjust() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.p_adjust", false]], "plot_effects() (doubleml.did.doublemldidaggregation method)": [[13, "doubleml.did.DoubleMLDIDAggregation.plot_effects", false]], "plot_effects() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.plot_effects", false]], "plot_tree() (doubleml.utils.doublemlpolicytree method)": [[52, "doubleml.utils.DoubleMLPolicyTree.plot_tree", false]], "policy_tree() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.policy_tree", false]], "predict() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.predict", false]], "predict() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.predict", false]], "predict() (doubleml.utils.doublemlpolicytree method)": [[52, "doubleml.utils.DoubleMLPolicyTree.predict", false]], "predict() (doubleml.utils.globalclassifier method)": [[53, "doubleml.utils.GlobalClassifier.predict", false]], "predict() (doubleml.utils.globalregressor method)": [[54, "doubleml.utils.GlobalRegressor.predict", false]], "predict_proba() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.predict_proba", false]], "predict_proba() (doubleml.utils.globalclassifier method)": [[53, "doubleml.utils.GlobalClassifier.predict_proba", false]], "psprocessor (class in doubleml.utils)": [[55, "doubleml.utils.PSProcessor", false]], "psprocessorconfig (class in doubleml.utils)": [[56, "doubleml.utils.PSProcessorConfig", false]], "rdflex (class in doubleml.rdd)": [[46, "doubleml.rdd.RDFlex", false]], "robust_confset() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.robust_confset", false]], "score() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.score", false]], "score() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.score", false]], "score() (doubleml.utils.globalclassifier method)": [[53, "doubleml.utils.GlobalClassifier.score", false]], "score() (doubleml.utils.globalregressor method)": [[54, "doubleml.utils.GlobalRegressor.score", false]], "sensitivity_analysis() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_analysis", false]], "sensitivity_benchmark() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_benchmark", false]], "sensitivity_plot() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_plot", false]], "set_fit_request() (doubleml.utils.globalclassifier method)": [[53, "doubleml.utils.GlobalClassifier.set_fit_request", false]], "set_fit_request() (doubleml.utils.globalregressor method)": [[54, "doubleml.utils.GlobalRegressor.set_fit_request", false]], "set_ml_nuisance_params() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.set_ml_nuisance_params", false]], "set_params() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.set_params", false]], "set_params() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.set_params", false]], "set_params() (doubleml.utils.globalclassifier method)": [[53, "doubleml.utils.GlobalClassifier.set_params", false]], "set_params() (doubleml.utils.globalregressor method)": [[54, "doubleml.utils.GlobalRegressor.set_params", false]], "set_sample_splitting() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.set_sample_splitting", false]], "set_score_request() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.set_score_request", false]], "set_score_request() (doubleml.utils.globalclassifier method)": [[53, "doubleml.utils.GlobalClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.globalregressor method)": [[54, "doubleml.utils.GlobalRegressor.set_score_request", false]], "set_x_d() (doubleml.data.doublemlclusterdata method)": [[4, "doubleml.data.DoubleMLClusterData.set_x_d", false]], "set_x_d() (doubleml.data.doublemldata method)": [[6, "doubleml.data.DoubleMLData.set_x_d", false]], "set_x_d() (doubleml.data.doublemldiddata method)": [[5, "doubleml.data.DoubleMLDIDData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlpaneldata method)": [[7, "doubleml.data.DoubleMLPanelData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlrdddata method)": [[8, "doubleml.data.DoubleMLRDDData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlssmdata method)": [[9, "doubleml.data.DoubleMLSSMData.set_x_d", false]], "tune() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.tune", false]], "tune() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.tune", false]], "tune() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.tune", false]], "tune() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.tune", false]], "tune() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.tune", false]], "tune() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.tune", false]], "tune() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.tune", false]], "tune() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.tune", false]], "tune() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.tune", false]], "tune() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.tune", false]], "tune() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.tune", false]], "tune() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.tune", false]], "tune() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.tune", false]], "tune_ml_models() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.tune_ml_models", false]], "tune_ml_models() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.tune_ml_models", false]], "tune_ml_models() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.tune_ml_models", false]], "tune_ml_models() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.tune_ml_models", false]], "tune_ml_models() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.tune_ml_models", false]], "tune_ml_models() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.tune_ml_models", false]], "tune_ml_models() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.tune_ml_models", false]], "tune_ml_models() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.tune_ml_models", false]]}, "objects": {"doubleml.data": [[4, 0, 1, "", "DoubleMLClusterData"], [5, 0, 1, "", "DoubleMLDIDData"], [6, 0, 1, "", "DoubleMLData"], [7, 0, 1, "", "DoubleMLPanelData"], [8, 0, 1, "", "DoubleMLRDDData"], [9, 0, 1, "", "DoubleMLSSMData"]], "doubleml.data.DoubleMLClusterData": [[4, 1, 1, "", "from_arrays"], [4, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLDIDData": [[5, 1, 1, "", "from_arrays"], [5, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLData": [[6, 1, 1, "", "from_arrays"], [6, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLPanelData": [[7, 1, 1, "", "from_arrays"], [7, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLRDDData": [[8, 1, 1, "", "from_arrays"], [8, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLSSMData": [[9, 1, 1, "", "from_arrays"], [9, 1, 1, "", "set_x_d"]], "doubleml.datasets": [[10, 2, 1, "", "fetch_401K"], [11, 2, 1, "", "fetch_bonus"]], "doubleml.did": [[12, 0, 1, "", "DoubleMLDID"], [13, 0, 1, "", "DoubleMLDIDAggregation"], [14, 0, 1, "", "DoubleMLDIDBinary"], [15, 0, 1, "", "DoubleMLDIDCS"], [16, 0, 1, "", "DoubleMLDIDMulti"]], "doubleml.did.DoubleMLDID": [[12, 1, 1, "", "bootstrap"], [12, 1, 1, "", "confint"], [12, 1, 1, "", "construct_framework"], [12, 1, 1, "", "draw_sample_splitting"], [12, 1, 1, "", "evaluate_learners"], [12, 1, 1, "", "fit"], [12, 1, 1, "", "get_params"], [12, 1, 1, "", "p_adjust"], [12, 1, 1, "", "sensitivity_analysis"], [12, 1, 1, "", "sensitivity_benchmark"], [12, 1, 1, "", "sensitivity_plot"], [12, 1, 1, "", "set_ml_nuisance_params"], [12, 1, 1, "", "set_sample_splitting"], [12, 1, 1, "", "tune"], [12, 1, 1, "", "tune_ml_models"]], "doubleml.did.DoubleMLDIDAggregation": [[13, 1, 1, "", "plot_effects"]], "doubleml.did.DoubleMLDIDBinary": [[14, 1, 1, "", "bootstrap"], [14, 1, 1, "", "confint"], [14, 1, 1, "", "construct_framework"], [14, 1, 1, "", "draw_sample_splitting"], [14, 1, 1, "", "evaluate_learners"], [14, 1, 1, "", "fit"], [14, 1, 1, "", "get_params"], [14, 1, 1, "", "p_adjust"], [14, 1, 1, "", "sensitivity_analysis"], [14, 1, 1, "", "sensitivity_benchmark"], [14, 1, 1, "", "sensitivity_plot"], [14, 1, 1, "", "set_ml_nuisance_params"], [14, 1, 1, "", "set_sample_splitting"], [14, 1, 1, "", "tune"], [14, 1, 1, "", "tune_ml_models"]], "doubleml.did.DoubleMLDIDCS": [[15, 1, 1, "", "bootstrap"], [15, 1, 1, "", "confint"], [15, 1, 1, "", "construct_framework"], [15, 1, 1, "", "draw_sample_splitting"], [15, 1, 1, "", "evaluate_learners"], [15, 1, 1, "", "fit"], [15, 1, 1, "", "get_params"], [15, 1, 1, "", "p_adjust"], [15, 1, 1, "", "sensitivity_analysis"], [15, 1, 1, "", "sensitivity_benchmark"], [15, 1, 1, "", "sensitivity_plot"], [15, 1, 1, "", "set_ml_nuisance_params"], [15, 1, 1, "", "set_sample_splitting"], [15, 1, 1, "", "tune"], [15, 1, 1, "", "tune_ml_models"]], "doubleml.did.DoubleMLDIDMulti": [[16, 1, 1, "", "aggregate"], [16, 1, 1, "", "bootstrap"], [16, 1, 1, "", "confint"], [16, 1, 1, "", "fit"], [16, 1, 1, "", "p_adjust"], [16, 1, 1, "", "plot_effects"], [16, 1, 1, "", "sensitivity_analysis"], [16, 1, 1, "", "sensitivity_benchmark"], [16, 1, 1, "", "sensitivity_plot"], [16, 1, 1, "", "tune_ml_models"]], "doubleml.did.datasets": [[17, 2, 1, "", "make_did_CS2021"], [18, 2, 1, "", "make_did_SZ2020"], [19, 2, 1, "", "make_did_cs_CS2021"]], "doubleml.double_ml_score_mixins": [[20, 0, 1, "", "LinearScoreMixin"], [21, 0, 1, "", "NonLinearScoreMixin"]], "doubleml.irm": [[22, 0, 1, "", "DoubleMLAPO"], [23, 0, 1, "", "DoubleMLAPOS"], [24, 0, 1, "", "DoubleMLCVAR"], [25, 0, 1, "", "DoubleMLIIVM"], [26, 0, 1, "", "DoubleMLIRM"], [27, 0, 1, "", "DoubleMLLPQ"], [28, 0, 1, "", "DoubleMLPQ"], [29, 0, 1, "", "DoubleMLQTE"], [30, 0, 1, "", "DoubleMLSSM"]], "doubleml.irm.DoubleMLAPO": [[22, 1, 1, "", "bootstrap"], [22, 1, 1, "", "capo"], [22, 1, 1, "", "confint"], [22, 1, 1, "", "construct_framework"], [22, 1, 1, "", "draw_sample_splitting"], [22, 1, 1, "", "evaluate_learners"], [22, 1, 1, "", "fit"], [22, 1, 1, "", "gapo"], [22, 1, 1, "", "get_params"], [22, 1, 1, "", "p_adjust"], [22, 1, 1, "", "sensitivity_analysis"], [22, 1, 1, "", "sensitivity_benchmark"], [22, 1, 1, "", "sensitivity_plot"], [22, 1, 1, "", "set_ml_nuisance_params"], [22, 1, 1, "", "set_sample_splitting"], [22, 1, 1, "", "tune"], [22, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLAPOS": [[23, 1, 1, "", "bootstrap"], [23, 1, 1, "", "causal_contrast"], [23, 1, 1, "", "confint"], [23, 1, 1, "", "draw_sample_splitting"], [23, 1, 1, "", "fit"], [23, 1, 1, "", "sensitivity_analysis"], [23, 1, 1, "", "sensitivity_benchmark"], [23, 1, 1, "", "sensitivity_plot"], [23, 1, 1, "", "set_sample_splitting"], [23, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLCVAR": [[24, 1, 1, "", "bootstrap"], [24, 1, 1, "", "confint"], [24, 1, 1, "", "construct_framework"], [24, 1, 1, "", "draw_sample_splitting"], [24, 1, 1, "", "evaluate_learners"], [24, 1, 1, "", "fit"], [24, 1, 1, "", "get_params"], [24, 1, 1, "", "p_adjust"], [24, 1, 1, "", "sensitivity_analysis"], [24, 1, 1, "", "sensitivity_benchmark"], [24, 1, 1, "", "sensitivity_plot"], [24, 1, 1, "", "set_ml_nuisance_params"], [24, 1, 1, "", "set_sample_splitting"], [24, 1, 1, "", "tune"], [24, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLIIVM": [[25, 1, 1, "", "bootstrap"], [25, 1, 1, "", "confint"], [25, 1, 1, "", "construct_framework"], [25, 1, 1, "", "draw_sample_splitting"], [25, 1, 1, "", "evaluate_learners"], [25, 1, 1, "", "fit"], [25, 1, 1, "", "get_params"], [25, 1, 1, "", "p_adjust"], [25, 1, 1, "", "robust_confset"], [25, 1, 1, "", "sensitivity_analysis"], [25, 1, 1, "", "sensitivity_benchmark"], [25, 1, 1, "", "sensitivity_plot"], [25, 1, 1, "", "set_ml_nuisance_params"], [25, 1, 1, "", "set_sample_splitting"], [25, 1, 1, "", "tune"], [25, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLIRM": [[26, 1, 1, "", "bootstrap"], [26, 1, 1, "", "cate"], [26, 1, 1, "", "confint"], [26, 1, 1, "", "construct_framework"], [26, 1, 1, "", "draw_sample_splitting"], [26, 1, 1, "", "evaluate_learners"], [26, 1, 1, "", "fit"], [26, 1, 1, "", "gate"], [26, 1, 1, "", "get_params"], [26, 1, 1, "", "p_adjust"], [26, 1, 1, "", "policy_tree"], [26, 1, 1, "", "sensitivity_analysis"], [26, 1, 1, "", "sensitivity_benchmark"], [26, 1, 1, "", "sensitivity_plot"], [26, 1, 1, "", "set_ml_nuisance_params"], [26, 1, 1, "", "set_sample_splitting"], [26, 1, 1, "", "tune"], [26, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLLPQ": [[27, 1, 1, "", "bootstrap"], [27, 1, 1, "", "confint"], [27, 1, 1, "", "construct_framework"], [27, 1, 1, "", "draw_sample_splitting"], [27, 1, 1, "", "evaluate_learners"], [27, 1, 1, "", "fit"], [27, 1, 1, "", "get_params"], [27, 1, 1, "", "p_adjust"], [27, 1, 1, "", "sensitivity_analysis"], [27, 1, 1, "", "sensitivity_benchmark"], [27, 1, 1, "", "sensitivity_plot"], [27, 1, 1, "", "set_ml_nuisance_params"], [27, 1, 1, "", "set_sample_splitting"], [27, 1, 1, "", "tune"], [27, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLPQ": [[28, 1, 1, "", "bootstrap"], [28, 1, 1, "", "confint"], [28, 1, 1, "", "construct_framework"], [28, 1, 1, "", "draw_sample_splitting"], [28, 1, 1, "", "evaluate_learners"], [28, 1, 1, "", "fit"], [28, 1, 1, "", "get_params"], [28, 1, 1, "", "p_adjust"], [28, 1, 1, "", "sensitivity_analysis"], [28, 1, 1, "", "sensitivity_benchmark"], [28, 1, 1, "", "sensitivity_plot"], [28, 1, 1, "", "set_ml_nuisance_params"], [28, 1, 1, "", "set_sample_splitting"], [28, 1, 1, "", "tune"], [28, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLQTE": [[29, 1, 1, "", "bootstrap"], [29, 1, 1, "", "confint"], [29, 1, 1, "", "draw_sample_splitting"], [29, 1, 1, "", "fit"], [29, 1, 1, "", "p_adjust"], [29, 1, 1, "", "set_sample_splitting"], [29, 1, 1, "", "tune_ml_models"]], "doubleml.irm.DoubleMLSSM": [[30, 1, 1, "", "bootstrap"], [30, 1, 1, "", "confint"], [30, 1, 1, "", "construct_framework"], [30, 1, 1, "", "draw_sample_splitting"], [30, 1, 1, "", "evaluate_learners"], [30, 1, 1, "", "fit"], [30, 1, 1, "", "get_params"], [30, 1, 1, "", "p_adjust"], [30, 1, 1, "", "sensitivity_analysis"], [30, 1, 1, "", "sensitivity_benchmark"], [30, 1, 1, "", "sensitivity_plot"], [30, 1, 1, "", "set_ml_nuisance_params"], [30, 1, 1, "", "set_sample_splitting"], [30, 1, 1, "", "tune"], [30, 1, 1, "", "tune_ml_models"]], "doubleml.irm.datasets": [[31, 2, 1, "", "make_confounded_irm_data"], [32, 2, 1, "", "make_heterogeneous_data"], [33, 2, 1, "", "make_iivm_data"], [34, 2, 1, "", "make_irm_data"], [35, 2, 1, "", "make_irm_data_discrete_treatments"], [36, 2, 1, "", "make_ssm_data"]], "doubleml.plm": [[37, 0, 1, "", "DoubleMLLPLR"], [38, 0, 1, "", "DoubleMLPLIV"], [39, 0, 1, "", "DoubleMLPLR"]], "doubleml.plm.DoubleMLLPLR": [[37, 1, 1, "", "bootstrap"], [37, 1, 1, "", "confint"], [37, 1, 1, "", "construct_framework"], [37, 1, 1, "", "draw_sample_splitting"], [37, 1, 1, "", "evaluate_learners"], [37, 1, 1, "", "fit"], [37, 1, 1, "", "get_params"], [37, 1, 1, "", "p_adjust"], [37, 1, 1, "", "sensitivity_analysis"], [37, 1, 1, "", "sensitivity_benchmark"], [37, 1, 1, "", "sensitivity_plot"], [37, 1, 1, "", "set_ml_nuisance_params"], [37, 1, 1, "", "set_sample_splitting"], [37, 1, 1, "", "tune"], [37, 1, 1, "", "tune_ml_models"]], "doubleml.plm.DoubleMLPLIV": [[38, 1, 1, "", "bootstrap"], [38, 1, 1, "", "confint"], [38, 1, 1, "", "construct_framework"], [38, 1, 1, "", "draw_sample_splitting"], [38, 1, 1, "", "evaluate_learners"], [38, 1, 1, "", "fit"], [38, 1, 1, "", "get_params"], [38, 1, 1, "", "p_adjust"], [38, 1, 1, "", "sensitivity_analysis"], [38, 1, 1, "", "sensitivity_benchmark"], [38, 1, 1, "", "sensitivity_plot"], [38, 1, 1, "", "set_ml_nuisance_params"], [38, 1, 1, "", "set_sample_splitting"], [38, 1, 1, "", "tune"], [38, 1, 1, "", "tune_ml_models"]], "doubleml.plm.DoubleMLPLR": [[39, 1, 1, "", "bootstrap"], [39, 1, 1, "", "cate"], [39, 1, 1, "", "confint"], [39, 1, 1, "", "construct_framework"], [39, 1, 1, "", "draw_sample_splitting"], [39, 1, 1, "", "evaluate_learners"], [39, 1, 1, "", "fit"], [39, 1, 1, "", "gate"], [39, 1, 1, "", "get_params"], [39, 1, 1, "", "p_adjust"], [39, 1, 1, "", "sensitivity_analysis"], [39, 1, 1, "", "sensitivity_benchmark"], [39, 1, 1, "", "sensitivity_plot"], [39, 1, 1, "", "set_ml_nuisance_params"], [39, 1, 1, "", "set_sample_splitting"], [39, 1, 1, "", "tune"], [39, 1, 1, "", "tune_ml_models"]], "doubleml.plm.datasets": [[40, 2, 1, "", "make_confounded_plr_data"], [41, 2, 1, "", "make_lplr_LZZ2020"], [42, 2, 1, "", "make_pliv_CHS2015"], [43, 2, 1, "", "make_pliv_multiway_cluster_CKMS2021"], [44, 2, 1, "", "make_plr_CCDDHNR2018"], [45, 2, 1, "", "make_plr_turrell2018"]], "doubleml.rdd": [[46, 0, 1, "", "RDFlex"]], "doubleml.rdd.RDFlex": [[46, 1, 1, "", "aggregate_over_splits"], [46, 1, 1, "", "confint"], [46, 1, 1, "", "fit"]], "doubleml.rdd.datasets": [[47, 2, 1, "", "make_simple_rdd_data"]], "doubleml.utils": [[48, 0, 1, "", "DMLDummyClassifier"], [49, 0, 1, "", "DMLDummyRegressor"], [50, 0, 1, "", "DMLOptunaResult"], [51, 0, 1, "", "DoubleMLBLP"], [52, 0, 1, "", "DoubleMLPolicyTree"], [53, 0, 1, "", "GlobalClassifier"], [54, 0, 1, "", "GlobalRegressor"], [55, 0, 1, "", "PSProcessor"], [56, 0, 1, "", "PSProcessorConfig"], [57, 2, 1, "", "gain_statistics"]], "doubleml.utils.DMLDummyClassifier": [[48, 1, 1, "", "fit"], [48, 1, 1, "", "get_metadata_routing"], [48, 1, 1, "", "get_params"], [48, 1, 1, "", "predict"], [48, 1, 1, "", "predict_proba"], [48, 1, 1, "", "score"], [48, 1, 1, "", "set_params"], [48, 1, 1, "", "set_score_request"]], "doubleml.utils.DMLDummyRegressor": [[49, 1, 1, "", "fit"], [49, 1, 1, "", "get_metadata_routing"], [49, 1, 1, "", "get_params"], [49, 1, 1, "", "predict"], [49, 1, 1, "", "score"], [49, 1, 1, "", "set_params"], [49, 1, 1, "", "set_score_request"]], "doubleml.utils.DoubleMLBLP": [[51, 1, 1, "", "confint"], [51, 1, 1, "", "fit"]], "doubleml.utils.DoubleMLPolicyTree": [[52, 1, 1, "", "fit"], [52, 1, 1, "", "plot_tree"], [52, 1, 1, "", "predict"]], "doubleml.utils.GlobalClassifier": [[53, 1, 1, "", "fit"], [53, 1, 1, "", "get_metadata_routing"], [53, 1, 1, "", "get_params"], [53, 1, 1, "", "predict"], [53, 1, 1, "", "predict_proba"], [53, 1, 1, "", "score"], [53, 1, 1, "", "set_fit_request"], [53, 1, 1, "", "set_params"], [53, 1, 1, "", "set_score_request"]], "doubleml.utils.GlobalRegressor": [[54, 1, 1, "", "fit"], [54, 1, 1, "", "get_metadata_routing"], [54, 1, 1, "", "get_params"], [54, 1, 1, "", "predict"], [54, 1, 1, "", "score"], [54, 1, 1, "", "set_fit_request"], [54, 1, 1, "", "set_params"], [54, 1, 1, "", "set_score_request"]], "doubleml.utils.PSProcessor": [[55, 1, 1, "", "adjust_ps"], [55, 1, 1, "", "from_config"]]}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function"}, "terms": {"": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 42, 43, 44, 45, 46, 51, 53, 54, 60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 78, 84, 85, 86, 87, 89, 91, 92, 93, 97, 99, 100, 101, 102, 104, 106, 107, 108, 109, 112, 113, 120, 122, 123, 124, 126, 127, 129, 130, 138, 140, 154, 155, 156, 157, 158, 168, 170, 171, 172, 173], "0": [4, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 51, 52, 54, 55, 56, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 165, 169, 170, 172], "00": [72, 76, 85, 86, 88, 92, 93, 139, 173], "000": [98, 156, 173], "00000": 88, "000000": [69, 71, 72, 73, 78, 88, 92, 93, 104, 109, 111, 170], "0000000": 156, "0000000000000010000100": [65, 104, 109, 170], "000000e": [72, 85, 86, 88, 92, 93], "000006": 78, "000017": 96, "000025": 91, "000034": 92, "000039": 91, "000041": 70, "000056": 111, "000057": 70, "000064": 79, "000067": 91, "000076": 124, "000091": 91, "0001": [73, 92], "000112": 70, "000128": 89, "000129": 37, "000144": 124, "000161": 70, "0002": 70, "000219": 28, "000242": 29, "000292": 69, "000320": 96, "00032016": 96, "000336": 69, "000341": 91, "000345": 72, "000346": 72, "000353": 72, "000360": 72, "000363": 72, "000393": 72, "000412": 72, "000418": 69, "000442": 91, "000443": 72, "00047580260495": 60, "000488": 91, "000494": 87, "0005": 73, "000522": 91, "000543": 69, "000565": 82, "000567": 69, "000575": 69, "000583": 69, "000595": 69, "0005a80b528f": 65, "000606": 69, "000611": 70, "000620": 72, "000662": 69, "000670": 91, "000690": 70, "000694": 71, "000743": 99, "000889": 70, "000915799": 156, "0009157990": 156, "000916": [105, 109], "000943": [81, 82], "001": [17, 19, 55, 60, 62, 63, 64, 65, 66, 76, 80, 94, 112, 120, 121, 122, 123, 124, 139, 140, 156, 170, 173], "0010": 70, "001006": 76, "001008": 76, "001027": 70, "001040": 70, "001051": 91, "001073": 76, "001080": 76, "001097": 76, "001145": 93, "0011563701553192595": 76, "001159": 76, "001168": 72, "001181": 76, "001184": 76, "001260": 70, "00133": 65, "001353": 76, "001377": 70, "001381": 69, "00138944": [102, 140], "0013978426220758982": 76, "001403": 97, "001444": 70, "001471": 88, "001487": 69, "001494": [111, 112, 114, 124], "001541": 70, "001576": 76, "0016": [64, 92], "001698": 88, "0017": 70, "001725003": 139, "001738": 76, "001758": 81, "001779": 76, "0018": [64, 92], "0019": 73, "001907": 88, "002037900301454": 90, "002149e": 82, "002169338": 156, "0021693380": 156, "0021693381": 156, "002184": 70, "002225": 81, "002290": 68, "0023": 62, "002436": 87, "002440": 70, "002567": 70, "0026": 73, "002606": 69, "00263": 139, "002728e": 82, "002779": 99, "0028": [62, 64, 70, 92], "002821": 100, "00282133350419121": 100, "002832356": 139, "002873e": 81, "002983": 91, "003": [18, 31, 40, 98], "003018": 72, "003045": 88, "003051": 81, "003074": 124, "003134": 96, "003220": 78, "003275": [124, 125], "003303": 70, "003328": 96, "003351": 70, "003367": 70, "003391": 70, "0034": [70, 84], "003404": 78, "003415": 78, "003427": 91, "003528": 70, "003528e": 81, "003765": [124, 125], "003779": 87, "003836": 96, "003867": 70, "003924": 87, "004": 94, "00409412": [102, 140], "004128": 70, "004192": 81, "0042": [64, 70, 92], "004217333": 139, "004253": 78, "004269": 82, "004392": 87, "004526": 78, "004542": 88, "0046": 70, "0047": [64, 92], "004785": 70, "004788": 70, "004846": 100, "004868": 76, "004910": 70, "005229": 70, "005316": [124, 125], "005339": [81, 82], "005427": [69, 70], "005471": 72, "005485": 76, "005734": 76, "005805345": 139, "005857": 91, "005e": 124, "006": 94, "006055": 78, "0060715124549546": 90, "006132": 70, "00634248": 139, "006593": 111, "0066": 70, "006648": 72, "0068": 70, "006816": 70, "006841": 72, "006922": 73, "006931": 72, "006958": [81, 82], "00728": 170, "0073": 73, "007421": 111, "007596": 76, "007672241": 139, "007715": 70, "00778625": 139, "007789e": 90, "008": [100, 139], "008023": 93, "008027": 69, "008223": [81, 82], "008487": 73, "008508": 70, "008674": 16, "008825": 83, "008825189994473": 83, "008883698": 140, "00888458890362062": 102, "008884589": 102, "008941": 81, "008943": 69, "008dbd": 94, "008e80": 94, "009": [94, 100, 120], "009122": 96, "009300438": 139, "009329847": 140, "0094": 70, "009428": 83, "00944171905420782": 100, "00950122695463054": 102, "009501226954630540": 102, "009501227": 102, "009645422": 63, "009656": 96, "00972": 73, "009763": [124, 127], "009790": 93, "009986": 96, "01": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 52, 55, 56, 60, 63, 64, 65, 66, 69, 72, 76, 77, 81, 82, 88, 92, 93, 94, 95, 96, 97, 112, 118, 120, 121, 122, 123, 124, 127, 129, 139, 140, 156, 170, 173], "010045": 90, "010086": 94, "010088": 72, "010133": 71, "010213": 99, "010246": 69, "010269": 91, "010364": 111, "010395": 94, "010450": 63, "0105": 70, "010694": 72, "010726": 76, "010797": 72, "010909": 72, "010940": 91, "010950": 71, "011": 94, "011016": 77, "011131": 96, "0112": 62, "011204": 88, "01127670": 139, "01128": 73, "011285": 81, "011515": 69, "011598": 96, "0118095": 63, "011823": 99, "011841": 70, "011957": 94, "011988e": 96, "012065": 72, "012134": 70, "01219": 65, "012355": 72, "0124105481660435": 90, "01274": 100, "012831": 100, "012909": 70, "013034": 100, "013088": 25, "013236": 94, "013288": 70, "013350": 81, "013450": 88, "01351638": 63, "013593": 99, "013667": 69, "013677": 97, "013848": 81, "013849": 76, "013870": 81, "013976": 82, "01398951": 63, "013990": 156, "014": 97, "01403089": 63, "014054": 70, "014080": [81, 82], "014218": 72, "014229": 70, "014431": 93, "014432": 68, "014593": 76, "014637": 91, "014681": 99, "014721": 93, "014728": 69, "014738": 81, "014899": 70, "0149": 139, "014914": 76, "015": 65, "0150": 70, "015038": 83, "015039": 70, "015138": 70, "015250": 69, "015257": 72, "015565": 96, "015698": 96, "015738": 70, "01574297": 96, "015743": 96, "016": 94, "016029": 70, "016058": 69, "016154": 91, "016200": [81, 82], "016266": 72, "016310": 70, "016375": 70, "0164": 70, "016415": 72, "01643": 171, "0164601": 69, "016518": 93, "016598": 81, "0167": 70, "016702": 85, "016786": 76, "016792": 70, "016816": 70, "017": [65, 94], "017185": 93, "017393e": 156, "017605": 81, "017660": 88, "01772": 158, "0178": 70, "017800092": 156, "0178000920": 156, "017820": 70, "017899": 70, "0179": 70, "017901": 70, "018": 65, "018006": 70, "018040": 69, "018092": 111, "018148": 96, "018187": 70, "01835199": 139, "018372": 70, "0187512020118494": 90, "019": 94, "01900305": [124, 127], "01903": [65, 112, 122, 168, 170], "019134": 70, "01916030e": 139, "01925597": 63, "019439633": 156, "0194396330": 156, "0194396331": 156, "0195": 70, "019511": 70, "019540": 70, "019596": 83, "019660": 29, "019824": 70, "019856": 70, "019858": 70, "01990373": 101, "019904": [124, 129], "019984": 70, "02": [16, 69, 72, 81, 82, 92, 93, 96, 124, 127, 129, 139], "020054": 72, "02008393": [124, 129], "02016117": 170, "020166": 96, "0202": 70, "020234": 70, "020243": 69, "020271": 91, "020272": 88, "020299": 70, "020360838": 156, "0203608380": 156, "0203608381": 156, "02052929": [102, 140], "02079162e": 139, "020837": [124, 129], "020906": 72, "02092": 170, "020925": 69, "021": 139, "021269": [85, 86], "021340513": 139, "021435": 70, "02163217": 63, "021690": 86, "021726": 94, "021728": 77, "021810": 69, "021866": 95, "021926": 83, "022032": 72, "022258": 88, "022320": 70, "022327": 70, "02247976": 63, "022485": 71, "022511": 93, "022629": 111, "022749": 111, "022768": 73, "022783": 99, "022820": 81, "022915": 91, "022950": 93, "023020e": [92, 93], "023137": 70, "0232": 70, "023256": 96, "0233": 70, "023317": 69, "023548": 70, "023563": 156, "023564641": 139, "023728": 82, "0238": 70, "024250": 81, "024266": 88, "024322": 70, "024355": 68, "024360": 70, "024364": 157, "024401": [85, 86], "024604": 91, "02467": 98, "024782": 96, "024926": 68, "024985": 77, "025": [81, 82, 85, 86, 88, 94], "025077": 156, "0253": 65, "025407": 90, "025434": 70, "025443": 73, "025463": 71, "0257": 62, "025813114": 156, "0258131140": 156, "02584": 65, "025841": 88, "02585686": 69, "0259": 70, "025937": 70, "025964": 88, "026278": 70, "026655": 70, "026723": 83, "026857": 70, "026966": 88, "02699695": 75, "02708816": [124, 127], "027186": 72, "027310": 70, "027419e": 82, "027579": 69, "027684": 70, "027838": 69, "02791": 73, "028": 173, "0281": 65, "028101": 70, "028146": 90, "02839978": 69, "028520": [81, 82], "02855711": 139, "028681": 111, "028727": 70, "028731": 111, "028868": 93, "028949": 69, "029": 94, "02900983": 96, "029010": 96, "029066": 70, "0291": 70, "029127": 70, "029192": 70, "029209": 173, "029364": [157, 163], "0294": 70, "029515": 70, "029696": 82, "029831": 96, "029910e": [92, 93], "02e": 64, "03": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 66, 69, 72, 78, 81, 82, 83, 87, 88, 92, 93, 96, 97, 99, 100, 124, 125, 127, 129, 139, 157, 163, 173], "030": 60, "030059": 111, "0301": 65, "03018": 27, "0302": 70, "030346": 170, "03045": 66, "030450": 70, "030497": 82, "0305": 70, "030516": 70, "03064181": 69, "0307": 65, "030715": 70, "030726": 71, "030817": 82, "030837": 70, "030858": 76, "030934": 96, "030955": 70, "030962": 96, "030964": 77, "0310": 70, "031075": 90, "0311": 70, "031105": 70, "03113": 101, "031134": [112, 118], "0312190390696285": 76, "031269": 73, "031323": 90, "031491": 90, "031639": 96, "031692": 90, "031712": 93, "031862": 69, "03191": 171, "03220": 172, "0323": 62, "03244552": [112, 113], "0325": 170, "032509": 69, "032602": 70, "032664": 81, "032735": 69, "032744": 70, "032871": 70, "032953": 99, "033353": 26, "033389": 117, "0334": 70, "033414": 70, "033422": 112, "033497": 70, "033655": 69, "033661": 90, "033779": 90, "033946": [85, 86], "034050": 69, "034096": 70, "03411": 170, "034217": 71, "034372": 70, "03438": 66, "0344": 70, "034609": 82, "034666": 82, "034675": 94, "034690": 83, "034801": 70, "034812763": 156, "0348127630": 156, "0348127631": 156, "034846": 92, "03488035": 139, "03489": [43, 63, 91], "0349": 70, "034935": 70, "035": 94, "035002": 69, "035033": 70, "03505": 71, "0351": 70, "035119185": 156, "0351191850": 156, "0351191851": 156, "035265": 83, "03536": 170, "03538": 65, "03539": 65, "035391": 73, "0354": 65, "035411": 170, "03545": 65, "0355": 70, "035545": 73, "035572": 73, "035579": 82, "035730": 96, "035735": 82, "03574": 73, "035757": 70, "035762": 96, "035892": 72, "0359": 65, "036129015": 156, "0361290150": 156, "0361290151": 156, "036143": 96, "036147": 96, "036240": 78, "036376": 70, "036381": 81, "0364": 70, "036577": 77, "036729": 91, "0368": 62, "036879": 70, "037008": [85, 86], "037111": 70, "037114": 88, "037271": 81, "037301": 76, "0374": 65, "037509": 101, "037529": 88, "037747": [81, 82], "037773": 69, "03783666": 96, "037837": 96, "037855e": 82, "038006": 82, "038048": 70, "038085": 82, "03809928": 139, "038103": 88, "038672": 71, "038812": 95, "038831": 90, "038847": 81, "03907122389107094": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "0391": 70, "039141": 78, "039146": 72, "039154": 93, "03917696": [140, 156], "03920960e": 139, "039302e": 83, "0394": 70, "039661": 88, "03968904": [124, 127], "039743": 70, "039895": 88, "04": [16, 40, 64, 69, 72, 78, 81, 82, 92, 93, 96, 97, 99, 124, 125, 127, 129, 139, 173], "040010": 88, "040067": 82, "040068": 70, "040112": 156, "040118": 77, "040139": [81, 82], "040180047": 139, "040253": 69, "040464": 71, "040533": [39, 140, 156], "04053339": 156, "040556": [124, 127], "040748": 72, "040784": 78, "040813": 81, "040912": 82, "0410": 70, "041091": 81, "0411": 70, "041147": 83, "041229": 70, "041242": 70, "041262": 90, "041275e": 81, "041284": 83, "0413": 70, "041381": 81, "041387": 83, "041491e": 83, "041519": 70, "04165": 124, "0418": 62, "041831": 83, "042034": 100, "042060": 90, "042219": 70, "042244": 70, "042265": 83, "0424": 70, "042445": 70, "042451": 94, "0425": [112, 122, 123], "042517": 82, "042583": 93, "0428": 101, "042804": 88, "042824": 70, "042844e": 96, "042854": 81, "042943297": 139, "04300012336462904": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "043002": 82, "043003447": 139, "043064": 70, "043126": 72, "0433": 62, "0434e374": 65, "043617": 70, "043723": 70, "043724": 70, "043820": 76, "04387": [112, 121], "043996": 82, "044113": 83, "04415": 65, "044176": 88, "044239": 88, "04424": 65, "044369194": 139, "044399": 82, "044418": 71, "044447": [105, 109], "04444978": 156, "044449780": 156, "0445": [77, 112, 121], "04465": 63, "04486": 170, "04487585": [157, 163], "04491": 124, "044929": 88, "04497975": [157, 163], "044986": 70, "04501612": 156, "04502": [112, 121, 140, 156], "045062": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "045127": 70, "045144": 91, "045172": 88, "045206": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "045302": 70, "04532923": 139, "045379": 170, "04552": 91, "045553": 83, "045621": 71, "045624": 68, "04563": [112, 123], "045751": 70, "045754": 96, "04583": [112, 120], "045848": 72, "04586": [112, 121], "045932": 96, "045962": 70, "045984": 88, "045991": 81, "045993": [112, 113, 116], "046088": 96, "04608822": 96, "046236": 94, "046238": 81, "04625": [112, 120], "046396": 72, "046451": 88, "04651494": 139, "046525": 111, "046527": 83, "046600": 71, "0466028": 63, "046728": 99, "046788": 93, "046819": 69, "04682310e": 139, "046844": 111, "046922": [112, 116], "046953": 69, "047": 94, "047123": 82, "047181": 70, "047239": 88, "047288": 111, "0473": 81, "047390": 82, "047873": 88, "047954": 91, "048": 94, "048308": 86, "048476": 88, "048699": 101, "048723": [112, 118], "049319": 70, "049450": 71, "049729": 82, "05": [16, 17, 19, 56, 60, 62, 63, 64, 65, 66, 69, 72, 76, 81, 82, 83, 84, 91, 92, 93, 94, 96, 97, 100, 112, 117, 118, 120, 121, 122, 123, 124, 125, 127, 129, 139, 140, 156, 170, 173], "05039": 99, "05041879": 139, "050966": 70, "050986": 69, "051": 65, "05110958": 139, "051186": 96, "051357": 71, "051651": 97, "051712": 95, "051821": 69, "051870e": 83, "052": 97, "052023": 88, "0520233166790431": 90, "052184": 71, "0522": 70, "052247": 70, "052488": 86, "052502": 96, "052571": 70, "052745": 83, "052811": 93, "052861644": 139, "053": [65, 94, 124], "053016": 71, "053165": 69, "053188": 81, "0533": 62, "053331": 83, "053389": 156, "053448": 72, "053465e": 139, "053530": [124, 129], "053541": 96, "053558": 83, "053659": 67, "053898": [124, 129], "054": [65, 97], "054064": 90, "054068": 91, "054162": 91, "054232": 72, "054279": 76, "054315": [124, 129], "054330": 82, "054339": 156, "054370": 83, "054403": 70, "054529": 156, "054771e": 96, "054970": 70, "055165": 99, "055280e": 81, "055410": 82, "055413": 70, "055439": 93, "055493": 100, "055680": 156, "055705": 69, "055947": 70, "056": 97, "0560": 70, "056018": 70, "056172": 77, "056238": 71, "056499": 86, "056654": 70, "056760": 72, "056915": 88, "057371": 93, "057535": 81, "0576": [64, 92], "057608": 81, "057762": 96, "057962": 83, "058042": 156, "058121": 70, "058175": 96, "058375": 78, "058383": 81, "058463": 96, "058467": 82, "058508": 101, "058538": 25, "058676e": 81, "0587": 70, "058801": 69, "058892": 81, "058903541934281406": 76, "05891": 124, "058911": 72, "0590": 62, "059058": 82, "059270": 71, "059384": 96, "059492": 70, "059630": 68, "059685": 96, "059936": 81, "059976e": 82, "059991": 70, "06": [18, 31, 40, 69, 72, 78, 81, 82, 83, 92, 93, 96, 112, 121, 139], "060": 60, "0600": 70, "060016": 78, "060068": 70, "06008533": 124, "060097": 71, "060201": 96, "060212": [92, 93], "060247": 70, "0603268864456956": 90, "060596": 70, "060672": [124, 127], "060706": 76, "060845": 156, "0611": 62, "06111111": 65, "0615": 62, "061973": 69, "061986": 69, "062": [97, 124], "062167": [124, 127], "062293": 82, "062414": 93, "06269": 98, "06270135": 75, "062774": 81, "06278811": [124, 127], "0628": 62, "062897": 71, "062964": 156, "063017": 78, "063067": 81, "063098": 81, "063155": 71, "0632": 62, "063312": 93, "063327": 88, "063428e": 93, "063464": 71, "0635": 62, "063590": 96, "0636": 62, "063641": 82, "0638": 62, "06389": 124, "06397789": 96, "063978": 96, "063979": 81, "063994": 69, "0640": 62, "064003": 71, "064164": 93, "06428": 92, "064280": 92, "064406": 69, "0645": 62, "064620": 71, "0646222": 64, "064673": 77, "0647": 62, "0649": 62, "065": [100, 120], "065103": 69, "065161": 70, "065277": 111, "0653": 62, "065356": [85, 86], "065363": 71, "0654": 62, "065453": 76, "065490": 71, "0655": 62, "065535": 30, "065621": 72, "065625": 69, "065725": 83, "065822": 71, "0659": 62, "065963": 70, "065969": 124, "065976": 88, "066": 94, "0660": 70, "066037": 70, "066039": 82, "0661": 70, "0662": 62, "066261": 72, "066295": 88, "066422": 71, "066464": 99, "066478": 90, "066553": 69, "066722": 71, "0669": 62, "06692492": 139, "0671": 62, "06719": 71, "067212": 88, "067291": 72, "0673": 62, "067436": 90, "0675": 62, "067528": 100, "067622": 71, "067639": 93, "067721": 156, "06811": 124, "068148": 96, "068221": 95, "06827": 99, "068340": 81, "068357": 71, "068377": 93, "068479": 71, "068700": 111, "068934": 78, "06895837": 63, "069038": 70, "069144": 90, "069171": 72, "069307": 72, "069443": 78, "0695854": 63, "069589": 88, "069634": 81, "07": [81, 82, 93, 96, 97, 100, 111, 124, 139], "070020": 96, "0702127": 63, "070393": 93, "0704": [62, 70], "070427": 81, "070432": 72, "070433": 88, "070497": 100, "0707": 62, "070797": 93, "07085301": 124, "070884": 96, "071": 94, "0711": 62, "071279": 156, "07136": [63, 91], "071543e": 83, "0716": 62, "07168291": 63, "071731": 93, "071771e": 81, "071777": [112, 118], "071782": 29, "07189557": 139, "0719": 62, "07202564": [85, 86], "072069": 26, "072153": 81, "07222222": 65, "072293": 95, "072320": 70, "072516": 88, "072583e": 82, "072590": 69, "072597": 70, "072605": 78, "0727": 124, "073": 97, "073013": 96, "073166": 70, "073207": 91, "0732109605604835": 76, "073293e": 81, "073317": 70, "073384": 88, "07347676": 63, "07350015": [36, 43, 63, 91], "073518": 83, "0735183279373635": 83, "073520": 83, "0736": 62, "073654": [124, 129], "07366": [65, 112, 123], "073900": 82, "074156": 69, "0743": 62, "074303": 76, "074304": 156, "07436521": 139, "074381": 69, "074426": 96, "074530": 70, "07456127": 63, "074700e": 93, "07479278": 99, "074927": 78, "07523350": 139, "075261": 68, "075384": 96, "07538443": 96, "07544271e": 139, "07561": 170, "07564554e": 139, "0758": 100, "075809": 78, "075869": [112, 118], "076017": 69, "076019": 92, "076119": 93, "076150": 156, "076179312": 156, "0761793120": 156, "076187e": 81, "076279": 81, "076283": 69, "076322": 96, "076347": 83, "076406": 70, "076432": 72, "076481": 70, "0765": 65, "076538": 70, "076684": 170, "076821": 117, "07685043": 139, "076869": 112, "07689": 65, "07691847": 139, "076953": [85, 86], "076971": 73, "077090": 90, "077215": 70, "07727773e": 139, "077319": 96, "077502": [157, 163], "077527e": 93, "077543": 81, "077592": 88, "07765383": 139, "077702": 78, "077753": 72, "0777777777777778": [112, 123], "07777778": [65, 112, 123], "07781396": 139, "077883": 96, "077920": [105, 109], "07796": 124, "078090": 156, "078095": 16, "078138007883929": 76, "078207": 73, "07828372": 156, "078382": 82, "078384e": 81, "078426": 111, "078474": 156, "078598": 70, "078810": 96, "079085": 73, "079134": 72, "07915": 65, "07919896": 139, "07942v3": 171, "079458e": 92, "07961": 99, "079710": 71, "079761": [105, 109], "07978296": 139, "08": [72, 83, 93, 96, 100], "080041e": 81, "080121": 93, "08031571": 139, "080533": [124, 127], "080633": 76, "080771": 71, "0808": 70, "080854": 93, "08091581": 139, "080947": 73, "080987": 81, "081": 65, "081014": 70, "081100": 96, "081230": [81, 82], "081396": 86, "081488": 91, "08154161": 139, "081635": 72, "081759": 70, "081788": 69, "08181827e": 139, "0820": 62, "082197": 88, "082297": 139, "082399e": 72, "08246377": 139, "082571": 70, "082643": 16, "082804": 68, "082973": 91, "083": 94, "083079": 93, "083153": 82, "083258": 156, "083312": 156, "08333333": 65, "08333617": 139, "0835771416": 63, "0836": 88, "08364": 88, "083706": 100, "083744": 70, "083926": 70, "083949": 100, "084": 63, "084007": 87, "0841": 70, "084269": 93, "084288": 81, "084590": 71, "084714": 82, "084807": 82, "084910": 70, "085279e": 85, "0853505": 63, "085372": 81, "085470": 69, "085550": 69, "085566": 83, "085592": 88, "085697": 76, "085730": 77, "086004": 88, "086013": 69, "08602774e": 139, "0862": 168, "086264": 83, "086401570476133": 83, "086402": 83, "08641773": 70, "086617264": 139, "08664208": 139, "086666": [124, 127], "086679": [112, 118], "086826": 85, "086890": 70, "087184": 38, "087192": 69, "0872": 62, "087370": [124, 127], "087566": 90, "087634": 81, "087660": 69, "087826": 82, "087947": 96, "088019": 81, "088048": 96, "088082": 90, "088282": 86, "088288": [105, 109], "088289": 69, "088346e": 81, "08835239": 70, "088357": 96, "088401": 81, "08848": [112, 123], "088482": 29, "08853": 139, "088616": 69, "088626": 69, "088792": 90, "088836": 90, "08888889": 65, "0889": 77, "089096": 69, "089191": 82, "089229": 77, "0894": 62, "089459": 124, "089647": 76, "08968939": 63, "089717": 72, "089819": 69, "08e": 64, "09": [38, 72, 81, 82, 83, 92, 93, 96], "09000000000000001": [112, 118], "090014": 69, "09015": 62, "090213": 69, "090255": 96, "090257": 71, "090331": 82, "090363": 95, "090728": 89, "091179": 69, "091267": 72, "091268": 69, "091308": 111, "091391": 156, "091406": 157, "091432": 81, "0916": 62, "09174584": [124, 127], "091905": 69, "091992": 95, "092": 94, "09218894": [124, 127], "092229": 100, "092253": 72, "092262": 124, "092336": 72, "092365": 156, "092453": 96, "09245337": 96, "092646": 96, "092919": 158, "0929369228758206": 90, "093": 60, "093043": 96, "09310496": 156, "093153": 96, "093173": 81, "0935": 124, "09351167": 124, "093548": 70, "093607": 82, "093623": 72, "093740": 156, "093950": 91, "094026": 91, "094118": 96, "09421": 139, "094276": 69, "094381": 91, "094420": 93, "09444444": 65, "094766": 95, "094829": 124, "094883": 81, "094968": 82, "094999": 96, "095104": 78, "095115": 72, "095427": 72, "095785": 78, "095835": [124, 129], "09603": 168, "096257": [124, 127], "09633319": 139, "096337": 91, "096418": 78, "096616": 24, "096806": 69, "09682314": 124, "096890e": 111, "096915": 100, "096968": 71, "097": [94, 97], "097009": 88, "097024": 69, "097140": 85, "097157": 100, "097208": 95, "097426": 69, "097468": 83, "09748405": 139, "097502": 69, "09753792": 139, "09756": 98, "09779675": 156, "097796750": 156, "098": 64, "09801463544687879": 76, "098256": 96, "09830758": 99, "098308": 99, "098319": 96, "098530": 70, "0986": 62, "098712": 96, "09879814e": 139, "098901": 88, "099": 97, "099230": 70, "09944659": 69, "09952196": 139, "09957868943595276": 76, "0996469": 70, "099647": 95, "099670": 93, "099731": [81, 82], "09980311": 156, "09988": 171, "0_": 42, "0ff823b17d45": 65, "0x": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "0x1747bdd4520": 73, "0x1747bdd6b90": 73, "0x7f028a1bb4a0": 76, "0x7f029668ba10": 76, "0x7f0e0ab180e0": 100, "0x7faf6d26ec90": 95, "0x7fc311edfad0": 163, "0x7fc312901280": 124, "0x7fc3129294f0": 124, "0x7fc313751c70": 173, "0x7fc313e28380": 157, "0x7fc31b0793d0": 124, "0x7fc31b090710": 124, "0x7fc31b4f02c0": 125, "0x7fc31b864f50": 124, "0x7fc31b8c0ef0": 114, "0x7fc31b8c13d0": 125, "0x7fc31b8e0bc0": 113, "0x7fc31b8e1790": 114, "0x7fc31bdbcaa0": 124, "0x7fc32ab314c0": 112, "0x7fc32ab31d30": 117, "0x7fc32ab3eed0": 112, "0x7fc32ab519d0": 112, "0x7fc32ac1b2c0": 112, "0x7fc32aff1e20": 156, "0x7fc32aff30b0": 156, "0x7fc32aff3dd0": 156, "0x7fc32b0be990": 156, "1": [8, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 53, 54, 55, 56, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172], "10": [10, 11, 12, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 35, 36, 37, 38, 39, 40, 41, 43, 44, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 106, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 168, 170, 171, 172, 173], "100": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 43, 45, 63, 65, 66, 67, 75, 76, 81, 82, 84, 87, 88, 91, 94, 98, 100, 101, 102, 104, 109, 111, 112, 114, 117, 122, 123, 124, 126, 139, 140, 156, 157, 163, 170, 172], "1000": [17, 19, 25, 27, 61, 67, 68, 75, 76, 77, 79, 80, 85, 86, 87, 89, 90, 92, 93, 97, 99, 100, 103, 124], "10000": [60, 68, 72, 81, 82, 92, 96], "100000e": 93, "100044": 86, "100099": 69, "100208": 111, "10035": 72, "100356": 83, "10038": 99, "10039862": [101, 124], "10043": 72, "10046": 72, "100510": 156, "100664": 81, "10074": 93, "100781": 70, "10079785": 124, "1008": 70, "100807": [81, 82], "100858": 99, "100887": 139, "10089588": 96, "100896": 96, "100923": 96, "100_000": 94, "101": [18, 31, 40, 62, 97, 111, 171, 172], "10109": 111, "10126": 93, "10127930": 156, "101279300": 156, "101373": 69, "1013902": 139, "1015": [64, 92], "101590": 70, "1016": [17, 18, 19, 31, 40, 62], "1016010": 64, "1016338581630878": 90, "1018": 93, "101805": [124, 127], "101998": 78, "102": [104, 109, 111, 170, 172], "102282": 16, "10235": 93, "102352247": 139, "102616": 83, "10277": 93, "102775": 83, "102796": 72, "102863": 76, "10299": 92, "103": [81, 91, 97, 104, 109, 111, 120, 172], "10307": 156, "1030891095588866": 90, "1031": 93, "103179163001313": 90, "103215": 16, "103259": 72, "10348": 92, "103497": 96, "103806": 83, "103863": 69, "10396": 92, "104": [64, 92, 111, 172], "1040": 93, "104016": 81, "10406": 93, "104097": 139, "1041": 62, "10414": 93, "104383": 82, "104475": 81, "10449": 98, "104492": 88, "1045303": 63, "104583": 72, "104628": 70, "1046283": 139, "10475463": 70, "104787": 91, "104913": [124, 127], "104917": [124, 127], "105": [42, 63, 88, 91, 111, 172], "105130": 69, "105141": 70, "105227e": 82, "105318": 96, "1054": 65, "105461": 111, "105494": 72, "1055": [62, 98], "105553": 72, "106": [65, 111, 172], "10607": [73, 104, 109, 170], "106116": 93, "10627": 93, "106271": 69, "1063": 77, "10637173e": 139, "106385": 156, "1065": [75, 84, 90, 124], "1065512": 139, "10658295": 70, "106595": [12, 124, 126], "106715": 87, "106883": 69, "107": [65, 70, 100, 111, 172], "107073": 83, "107156": 88, "107171": 70, "107207363": 139, "107290": 156, "1073": 93, "107467": [105, 109], "10747": [73, 104, 109, 170], "1075011": 139, "107689": 69, "107746": 96, "107809": 81, "107855": 72, "10791": 93, "107935": 96, "108": [111, 168, 171, 172], "1080": [36, 43, 62, 63, 91], "10824": [73, 104, 109, 170], "108257e": 93, "108259": 78, "10829": 93, "10831": [73, 104, 109, 170], "108323": 72, "1083826": 139, "108625": 69, "108742": 16, "108783": 83, "108783087402629": 83, "10878571": 96, "108786": 96, "108987": 71, "109": [76, 81, 111], "10903": 92, "109069": 156, "109074": 69, "109079e": 96, "109273": 91, "109277": 88, "10928": 93, "1093": [41, 84], "10930647": 139, "109316": 69, "109454": 93, "1095": 77, "1096": [62, 98], "109639": 72, "10967": 92, "1098217": 139, "109861": 170, "1099472942084532": 79, "10e": [83, 96], "11": [39, 41, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 91, 92, 93, 94, 96, 97, 99, 100, 101, 102, 104, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 124, 125, 126, 127, 139, 140, 156, 157, 163, 170, 172, 173], "110": [76, 111, 172], "110081": 88, "1101": 93, "110179": 82, "11019365749799062": 100, "110194": 100, "110205": 69, "110359": 91, "110365": 100, "110557": 90, "110666038": 139, "110681": 99, "11071087": [101, 124], "110717": 156, "110742": 90, "110784": 72, "1109": 93, "110e": 124, "111": [82, 111, 172], "111043": 85, "1111": [10, 11, 44, 61, 63, 80, 84, 91, 100, 103, 124, 157, 163, 168], "11120": 93, "11120056": 139, "11128117": 70, "111352344760325": 90, "111526": 76, "111577": 93, "1117": [75, 84, 90], "111783": 93, "1118": 64, "11199615e": 139, "112": [65, 111, 172], "1120": 92, "112078": 111, "11208236": [102, 140], "112135": 83, "1121351274811793": 83, "1121653": 139, "1122": 93, "112216": 83, "1122851": 139, "112415": 76, "11258214": 70, "112867e": 82, "113": [10, 111, 172], "113005": 93, "113022": 88, "11311": 92, "113149": 88, "113207": 96, "113270": 83, "113493": 69, "113543": 69, "11376": 93, "113780": 91, "113952": 88, "11399": 92, "114": [111, 172], "114026": 90, "114197": 94, "114258": 81, "114327": 72, "1144": 93, "114445": 69, "1144500": 63, "11447": 99, "1144949": 139, "114525": 69, "114530": 85, "11453423": 70, "1145370": 63, "114570": 82, "11458": 93, "114647": 83, "1147": 62, "114834": 93, "114877": 71, "114989": 78, "115": [111, 172], "11500": [92, 173], "115060e": 96, "1151": 93, "115192": 69, "115297e": [92, 93], "11530": 93, "11552911": 99, "11570": 92, "115785": 16, "11588": 93, "115901": 78, "116": [111, 172], "116036": 71, "1160713": 139, "11622132": 139, "116274": 83, "116382": 70, "1166": [92, 171], "1167": 92, "11673": 93, "11676": 93, "11684973": [124, 127], "117": [81, 111], "11700": 173, "117194": 95, "11720": 93, "117242": 96, "11724226": 96, "117366": 96, "11743": 173, "117457": 82, "11750": 93, "1176": 62, "117692": 71, "1177": 62, "117710": 83, "117795": 69, "11789998": 96, "117900": 96, "11792": 64, "11796": 93, "118": [111, 120], "118018e": 124, "1182": 64, "11820": 93, "11823404": 100, "118255": 96, "11850": 93, "118596": 83, "1186": 64, "118601": 91, "11861": 64, "1187": 99, "118708e": 91, "1187339840850312": 91, "118799": 93, "118952": 91, "119": [100, 111, 172], "11935": 99, "119409": 90, "119766": 96, "1198": [63, 91], "12": [4, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 55, 56, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 99, 100, 102, 104, 109, 111, 112, 113, 114, 116, 117, 118, 121, 123, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 168, 170, 171, 172, 173], "120": [66, 67, 90, 101, 111, 172], "12002": 92, "120071": 69, "1200x600": [69, 70, 72], "1200x800": [69, 70, 72], "120160": 72, "1202": 171, "1205": 70, "120548": 69, "120567": [85, 86], "120641": 72, "120721": 91, "12097": [10, 11, 44, 63, 84, 91, 93, 103, 168], "121": [93, 111, 172], "1210": 93, "12105472": 156, "121054720": 156, "1211": 93, "121297": 93, "1213405": 63, "121369": 69, "1214": 156, "121584e": 96, "121596": 72, "121738": 69, "121750": 82, "121774": 87, "12196389e": 139, "122": [18, 31, 40, 62, 69, 97, 104, 109, 111, 171, 172], "122120": 72, "12214": 64, "12223182e": 139, "122408": 83, "122421": 88, "122539": 70, "122671e": 82, "122777": 156, "123": [46, 60, 64, 65, 69, 72, 92, 100, 111, 172, 173], "1230": 93, "123192": 100, "1232": 98, "12323": 93, "1234": [60, 61, 62, 73, 79, 80, 97, 98, 103, 112, 113, 116, 118, 122, 123, 139, 156], "12348": 100, "123501e": 93, "123542": 72, "124": 111, "1240": 98, "12410": 93, "1247297280098136": 76, "124805": 92, "124836": 69, "124931": 72, "125": [111, 172], "12500": 92, "125059": 156, "12539340": 156, "125649": 111, "125673": 69, "12572": 93, "1258": [63, 93], "125998": 69, "126": [111, 172], "126023": 76, "12612": 93, "12616": 93, "126475": 69, "126494": 16, "126598": 71, "126771": 156, "126788": 69, "126802": 93, "12689": 93, "127": [31, 111, 172], "12705095": [140, 156], "12707800": 63, "127337": 88, "1275": 93, "12752825": 156, "127563": 99, "127672": 69, "1277": 94, "127759": 72, "128": [64, 111, 172], "12802": 64, "128061": 76, "128100e": 72, "12814": 93, "128158": 72, "128229": 88, "128312": 96, "128372": 82, "128393": 93, "128408": 91, "128412": [105, 109], "12842": 69, "1285": 62, "12861": 93, "12861479": 70, "128804": 69, "129": [91, 111, 172], "129012": 72, "129085": 82, "129188": 72, "129446": 82, "12945": 171, "1295": [62, 93], "12955": 92, "129586": 72, "129597": 72, "1297": 93, "12980769e": 139, "12983057": 124, "129881": 76, "13": [12, 14, 15, 17, 18, 19, 22, 24, 25, 26, 27, 28, 30, 33, 35, 37, 38, 39, 40, 41, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 76, 77, 78, 81, 82, 83, 85, 86, 87, 88, 90, 91, 92, 93, 96, 97, 98, 99, 100, 102, 104, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 124, 126, 127, 139, 140, 156, 157, 163, 170, 173], "130": [65, 91, 111, 172], "130122": 99, "130304": 69, "13030464382207196": 112, "13034980e": 139, "130370": 83, "13037270504317794": 117, "130465": 69, "1306": 99, "130609": 82, "130796": 72, "130802": 81, "130829": 96, "13084": 124, "13091": 93, "130971": 85, "130976": 71, "131": [111, 172], "13102231": 124, "131071": 76, "13119": 99, "1312": 173, "1313": [64, 173], "13137893e": 139, "1314": 70, "131483": 88, "131793": 72, "1318": 62, "131842": 88, "131928": [105, 109], "132": [65, 81, 91, 111, 172], "13208": 173, "1321": [92, 173], "132248": 111, "1324": [64, 92], "132454": 68, "132481": 111, "1325": 64, "13257": 92, "132671": 83, "1328": 99, "1328324148535658626365697071787983889092": 139, "13283241485356586263656970717879838890924101516182023343945505457667780868997981567111722262930374960687281828595992381421364244465152556174758487919496": 139, "13283241485356586263656970717879838890924101516182023343945505457667780868997989121924252731333538404347596467737693100156711172226293037496068728182859599": 139, "132832414853565862636569707178798388909241015161820233439455054576677808689979891219242527313335384043475964677376931002381421364244465152556174758487919496": 139, "132832414853565862636569707178798388909291219242527313335384043475964677376931001567111722262930374960687281828595992381421364244465152556174758487919496": 139, "132841": 70, "13288448": 70, "132941": 93, "133": [65, 104, 109, 111, 171, 172], "13306435": 139, "133202": 93, "133286": 72, "13334307": 70, "133421": 93, "133450": 70, "133458": 71, "133509": 81, "133523": 71, "13355896": [124, 127], "13357": 93, "133596": 96, "133676": 82, "133839": 88, "133882": 69, "13398": 100, "133f5a": 94, "134": [91, 101, 111, 172], "134037": 71, "1340371": 62, "1341": 64, "13412471": 70, "134145": 70, "134211": 96, "1343": [92, 93], "134533": 69, "134567": 93, "1346": 139, "1346035": 64, "134664e": 81, "134687": 93, "13474": 93, "134744": 69, "134765": 93, "134784": 124, "1348": 92, "13490": 93, "134911": 71, "135": [65, 111, 124, 172], "13505272": 63, "135309": 81, "13533463": 70, "135375": 78, "135378": 156, "135398": 82, "135596": 93, "135610": 72, "135671": 69, "135707": [112, 118], "135856": 96, "13585644": 96, "135871": 91, "1359": 93, "136": [73, 91, 100, 111, 172], "1360": [64, 77], "136010": 81, "13602": 100, "136089": 91, "136160": 69, "13642": 93, "136442": 91, "1366": 94, "136836": 91, "136885": 93, "136898": 81, "137": [31, 65, 73, 111, 172], "1371": 93, "137165": 124, "137230": 90, "137258": 70, "1373": 70, "137396": 96, "1378": 93, "137825": 69, "137999": 124, "138": [111, 172], "1380": 92, "13809": 93, "138142": 85, "138264": 100, "138378": 83, "1384": 92, "1386": 62, "13868238": 156, "138682380": 156, "138698": 156, "1387": 62, "13874651": 70, "138841": [124, 129], "13893": 93, "139": [100, 111, 170], "13937569": 70, "139491": 156, "139508": 88, "13956": 99, "139622": 93, "139659": [124, 129], "139721": 72, "1398": 93, "139811": 76, "1399": 62, "13993416": [124, 127], "14": [61, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 76, 77, 78, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 96, 97, 99, 100, 102, 104, 109, 111, 112, 113, 114, 116, 117, 118, 124, 126, 127, 139, 140, 156, 157, 163, 170, 171, 173], "140": [66, 67, 88, 93, 101, 111, 172], "1400": 93, "14000073": 139, "1401": 62, "14018": 124, "140406": [124, 127], "140530": 81, "140770": [81, 82], "140833": 83, "140861": 63, "140983": 70, "141": [93, 111, 172], "14101581": 70, "141069": 82, "141101e": 93, "14114": 99, "141179": 69, "141247": 78, "14141": 93, "141484": 81, "141546": 156, "141777": 72, "141820": 83, "141977": 72, "142": [111, 172], "14200098": 156, "142045e": 91, "142132": 81, "1422": 70, "142270": 68, "1424": [112, 122, 123], "142482": 96, "14268": 124, "142738": 70, "14281403493938022": [112, 118], "14289": 93, "143": [104, 109, 111, 172], "143274": 69, "1435": 93, "143593": 81, "14368145": 156, "143943": 76, "144": [111, 172], "14400": 92, "14405": 93, "144059": 69, "14406": 93, "144084": 83, "1441": 62, "144195": 82, "1443": 93, "14443664": 139, "144669": 96, "144800": 83, "144813": 78, "144908": 95, "144971": 92, "145": [111, 172], "145082": [124, 127], "145245": 96, "14530": 93, "14532650": 156, "14539": 72, "145625": 96, "145692": 69, "145699e": 81, "145737": 156, "14588": 93, "146": [111, 172], "146037": 96, "146087": 170, "146214": [105, 109], "14625": 93, "1465": 64, "146641": 156, "14670521": 70, "146749": 72, "1468115": 63, "146861": 111, "146956": 69, "147": [111, 172], "14702": 73, "147121": 96, "147240": 72, "147383": 82, "14744": 93, "147464": 111, "14772": 93, "1479": 93, "14790924": 156, "147909240": 156, "147927": 73, "14795": 93, "148": [111, 120, 172], "148005": 78, "14803": 93, "148134": [81, 82], "148161": 96, "14817": 72, "148210": 83, "1482102407485826": 83, "14845": 73, "148455": 83, "1484554868601506": 83, "1485": 93, "148750e": [92, 93], "148835": 93, "149": [94, 111, 172], "1492": 60, "149228": 100, "149265": 82, "149285": 96, "149392e": 111, "149446427": 81, "149472": 100, "149681": 81, "149714": 91, "149858": 28, "149897": 82, "149898": 96, "149967": 69, "15": [11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 61, 63, 64, 65, 67, 69, 72, 75, 76, 77, 78, 81, 82, 83, 87, 88, 89, 90, 91, 92, 93, 96, 97, 98, 99, 100, 104, 109, 111, 112, 113, 114, 116, 117, 118, 124, 126, 127, 139, 140, 156, 157, 163, 170, 173], "150": [42, 65, 100, 111, 172], "15000": [64, 92], "150000": 64, "15000000000000002": [83, 93, 96, 112, 118], "1502": 63, "150200": 91, "150234": 93, "150408": 63, "150435": 93, "150614": 73, "150719e": 92, "15093586": 70, "151": [111, 172], "15108": 93, "151447": 90, "15145853": 70, "151547": 72, "151636": 83, "15178237": 70, "151819": 96, "15194": 92, "152": [111, 172], "152148": [81, 82], "152264": 72, "152325": 72, "152414": 82, "15263": 72, "152706": 124, "15283304": 70, "15287": 93, "152926": 68, "153": [100, 111, 172], "153119": 83, "153164": 69, "153293": 93, "15339": 93, "153398": 93, "15339806": 139, "15354": 99, "15356203": 70, "153587": 91, "15360": 72, "153633": 73, "153639": 139, "153826": 72, "153971e": 81, "153983": 83, "1539831483813536": 83, "154": [77, 111], "15430": 173, "154415": 156, "1545": 93, "154557": 96, "154630": [124, 129], "154707": 78, "154752": 156, "154828": 83, "155": [77, 111, 172], "155000": 92, "155025": 96, "155120": 96, "155160": 78, "1554": 93, "155423": 78, "15549": 93, "15556": 93, "1557093": 63, "155892": 69, "155995": 82, "156": [111, 172], "156021": 96, "156202": [81, 82], "156205": 76, "156209": 82, "156317": [81, 82], "156328": 90, "1564": 156, "156517": 72, "156540": 156, "156554": 81, "156704": 93, "156711172226293037496068728182859599": 139, "156721": 72, "1569": 93, "156969": 83, "157": [82, 111, 172], "157080": 156, "15727": 72, "157404": 72, "157470": 95, "1576": 93, "1577657": 63, "157957": 69, "158": [111, 172], "158007": 96, "158076": 82, "15815035": 64, "158178": 83, "158198": 93, "1582": 93, "158288": 93, "1586": 93, "15867315": 70, "158682": 90, "158693": 70, "158697": 156, "158726": 111, "1589": 93, "159": 172, "15915": 71, "15916": [62, 71], "159386": 99, "159466": 96, "15946647": 96, "159592": 82, "1596": 65, "1597988": 70, "159826": 93, "159959": 93, "16": [24, 60, 61, 63, 64, 65, 66, 67, 69, 71, 72, 76, 77, 78, 81, 82, 83, 86, 87, 88, 91, 92, 93, 96, 97, 99, 100, 104, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 126, 127, 139, 140, 156, 157, 163, 170, 173], "160": [66, 67, 101, 172], "160285e": 82, "16033142": 70, "1604": 64, "160768": 72, "160825": 85, "160836": 88, "160932": 83, "161": [65, 77, 171, 172], "16109026": 70, "161141": 91, "16115851": 70, "16118746": [124, 127], "161236": 96, "161243": 96, "161305": 81, "161441": 67, "1619": 64, "16195766": 139, "162": [60, 77, 172], "16201": 93, "16211": 92, "162153": 96, "1622": 93, "162389": 81, "16241": 93, "162436": 100, "16249": 93, "1626685": 63, "162683": 100, "162710": 83, "162752": 90, "1628": 92, "162909": 91, "162930": 93, "163": 172, "163013": 16, "163194": 96, "163566": 93, "1636": 69, "16364838": 70, "16370066": 70, "163895": 83, "164": [78, 93, 97, 124, 172], "164034": 156, "164467": 92, "164473": 69, "164608": 96, "164698": 87, "1648": 62, "164801": 96, "164805": 83, "164864": 91, "164941": 16, "164943": 90, "165": [77, 100, 172], "16500": 92, "16510251": 72, "165178": 96, "1653": 93, "16536299": 156, "165362990": 156, "16539906e": 139, "165419": 96, "165499": 72, "165518": 69, "16553": 92, "165549": 170, "165668": 71, "165670e": 82, "165707": 78, "16590": 93, "166": [77, 94, 172], "166057": 72, "166088": 77, "1661": 92, "16614409": 70, "16618": 93, "166322": [105, 109], "166362": 69, "166439": 69, "166517": 88, "166804": 69, "166904": 93, "167": [64, 92, 172], "167035": 93, "167306": 69, "16747866": [124, 127], "167547": 96, "1676": 93, "1676705013704926": 76, "167930": 93, "167942": 69, "167981": 156, "168": [120, 172], "16803512": 156, "168092": 156, "1681": [62, 70, 92], "168148": 70, "168195": 99, "168614": 96, "168931": 96, "169": [65, 172], "169039": 70, "1691": [62, 93], "16910": 93, "169117": 100, "169196": 96, "169220e": 83, "16951": 93, "169677": 89, "16984": 93, "169858": 90, "17": [61, 63, 64, 65, 67, 69, 72, 76, 77, 78, 81, 82, 87, 88, 90, 91, 92, 93, 96, 97, 99, 100, 104, 109, 111, 112, 114, 116, 117, 118, 123, 124, 126, 127, 139, 140, 156, 157, 163, 170, 173], "170": 172, "1705": 93, "1706": 70, "170705": 111, "17083": 93, "171": [76, 77, 172], "1712": 171, "171255": 111, "1714": 64, "171575": 96, "1716593": 70, "171696": 111, "171815": [112, 118], "171860": 82, "1719": [69, 70, 71, 72], "17197794": 70, "172": [97, 172], "172022": 156, "172083": 82, "17221404": 70, "172307": 69, "172672": 37, "172793": 96, "172824": 70, "17291874": 70, "173": [77, 172], "17328207": 70, "173426": 71, "173504": 86, "173564e": 139, "17365976": 70, "17372": 93, "173818": 69, "173849": 69, "17385178": [112, 113], "173964": 156, "173e": 97, "174": 172, "174177": 96, "174185": 96, "174499": 156, "174516e": 96, "17453": 93, "1746": 93, "17469": 93, "174738": 72, "174968": 92, "174e": 124, "175": [77, 172], "17500": 93, "1751": 92, "175254": 92, "175284": 83, "175346": 69, "17536": 93, "175494": 71, "175564e": 82, "175635027": 63, "17576": 93, "175894": 100, "175931": [111, 112, 114, 124], "176": 69, "176056e": 111, "176069": 69, "176074": 72, "176495": 96, "17655394": 96, "176554": 96, "176929": 156, "177": [171, 172], "177007": 96, "17700723": 96, "177015": 76, "177043": [81, 82], "1773": 93, "177397": 93, "1774": 62, "177496": 96, "177611": 96, "177751": 96, "177995": 96, "178": [93, 172], "17800": 93, "17807": 93, "178169": 85, "17823": 65, "178273": 82, "178661": 156, "178751": 77, "178763": 96, "178934": 156, "179": [89, 93, 105, 109, 172], "179030": 69, "179101": 111, "179204": 82, "179276": 82, "179332": 69, "179548": [105, 109], "1795850": 63, "179588e": 96, "179810": 82, "1798913180930109556": 94, "18": [61, 63, 64, 65, 67, 69, 72, 73, 75, 76, 77, 81, 82, 85, 87, 88, 90, 91, 92, 93, 96, 97, 99, 100, 104, 109, 111, 112, 114, 117, 118, 124, 126, 127, 139, 156, 170, 173], "180": [66, 67, 69, 76, 101, 172], "180271": 88, "180296": 82, "1803": 62, "18030": 93, "180307": 82, "180474": 82, "180575": [85, 86], "1807": [62, 93], "180880": 72, "1809": 171, "180951": 96, "181": [69, 172], "18110044": 70, "18117991": 70, "1812": 93, "181330": 78, "181350": 81, "1814": 62, "18141": 93, "181432": 156, "181687": 72, "182": [69, 172], "1820": 62, "182018": 82, "182151": 111, "182288": 82, "182299": 72, "182354": 82, "182397": 94, "182399": 16, "182633": 96, "182795": 72, "182826": [124, 129], "182849": 96, "183": [65, 72, 77, 124, 172], "1832": 93, "183367": 72, "183373": 124, "183526": 83, "183553": 97, "18356413": 124, "18368": 93, "183814": 93, "183855": [112, 118], "183888": 91, "183942": 69, "183956": 117, "184": [65, 72, 171, 172], "184082": 112, "184224": 78, "184365": 71, "184449": 81, "184806": 77, "184880": 72, "185": [64, 65, 69, 76], "185035": 81, "185130": 97, "185524": 69, "185617": 82, "185753": 71, "185930": 111, "185938": 69, "185956e": 82, "186": 172, "18604": 93, "186121": 69, "186136": 82, "1862": 62, "18622": 93, "18637": 168, "186589": 78, "18666": 93, "186689": 124, "186735": 96, "18678094e": 139, "186836": 96, "186864": 90, "186868": 67, "187": 172, "187148": 156, "187617": 82, "187690": 96, "18773": 93, "187732": 81, "187782": 81, "1879": 77, "188": [69, 72, 172], "188175": 96, "1881752": 96, "188206": 71, "188223": 96, "1884318": 139, "188545": 82, "1887": [111, 112, 114, 124], "188760": 88, "1888": 93, "188838905": 139, "18888149e": 139, "188986e": 81, "189": [65, 69, 72, 93, 97, 172], "189023": 156, "189195": 93, "1895815": [43, 63, 91], "189737": 96, "189739": 78, "18976": 93, "189998": 96, "19": [15, 61, 63, 64, 65, 67, 69, 72, 76, 77, 81, 82, 88, 90, 91, 92, 93, 96, 97, 99, 100, 104, 109, 111, 112, 117, 118, 124, 126, 127, 139, 156, 170, 173], "190": [65, 172], "190000e": 93, "190090": 156, "190231": 93, "19031969": 96, "190320": 96, "19033538": 63, "19073905e": 139, "190796": [124, 125], "190809": 96, "190892": 100, "1909": [43, 63, 91], "190915": 83, "1909165": 139, "190921": 86, "190976": 97, "190982": 96, "191": [65, 72, 120, 171, 172], "191085": 69, "1912": 171, "1912705": 103, "191501": 81, "191534": 92, "191578": 25, "191773": 69, "1918": 62, "192": [69, 72, 76, 172], "192234": 156, "1923": 93, "192526": 99, "19252647": 99, "192539": 29, "19258": 72, "192817": 69, "192952": 78, "1929674": 139, "193": [69, 172], "193060": 96, "193224": 69, "193308": 29, "193375": 88, "1935259": 139, "193552": 69, "193581": 81, "193644": 93, "193649": 71, "19368": 69, "19374710e": 139, "193773": 72, "19382": 93, "193833": 93, "193849": 97, "19385": 93, "193870": 81, "193f0d909729": 65, "194": [69, 75, 172], "1941": 64, "19413": [92, 93], "194167": 81, "194588e": 81, "194945": 111, "195": [72, 76, 106, 109, 172], "19508": 100, "19508031003642456": 100, "19509680e": 139, "195101": 81, "195132": 82, "195377": 96, "195396": 96, "19550": 93, "195564": 91, "19559": [64, 92], "195816": 81, "1959": 171, "195920": 77, "195963": 88, "196": [69, 72, 76, 172], "196099": 124, "196136": 72, "196189": 96, "196378": 69, "196444": 72, "196655": 88, "19680840": 156, "196824": 96, "196835": 90, "19685": 72, "196973": 69, "197": [69, 72, 76, 105, 109, 172], "1970": 93, "19705": 93, "197223": 72, "197225": [73, 104, 109, 170], "1972250000001000100001": [65, 104, 109, 170], "197356": 81, "197424": [112, 118], "197462": 82, "197484": 156, "1975": 93, "19756": 93, "19758": 93, "197600": 68, "197756": 69, "19793": 93, "19798": 93, "198": [69, 72, 76, 172], "198221": 93, "19824": 93, "198259": 70, "198493": 88, "198503": 97, "198529e": 82, "198549": 73, "198617": 95, "198687": 64, "1988": [61, 80, 103, 124], "199": [69, 72, 76, 92, 172], "1990": [64, 92, 93], "1991": [64, 92, 93, 173], "199213": 82, "199281e": 96, "199445": 156, "199446": 70, "1995": [63, 91], "19962187": 124, "1998": 94, "199806": 69, "19983954": 101, "1999": 94, "1_": [83, 96], "1d": 13, "1e": [12, 15, 55, 56, 76, 88, 93], "1f77b4": 68, "1m": [112, 120, 121], "1mnon": [112, 120], "1x_4x_3": 68, "2": [10, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 45, 46, 47, 49, 52, 54, 55, 56, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 133, 139, 140, 156, 157, 158, 160, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172], "20": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 37, 38, 39, 40, 41, 43, 44, 45, 61, 63, 64, 65, 66, 67, 68, 69, 72, 75, 76, 77, 81, 82, 83, 87, 88, 89, 90, 91, 92, 93, 96, 99, 100, 101, 102, 103, 104, 109, 111, 112, 117, 118, 123, 124, 126, 127, 139, 140, 156, 157, 163, 170, 173], "200": [17, 19, 32, 35, 42, 62, 66, 67, 69, 72, 75, 76, 83, 84, 95, 96, 101, 103, 108, 109, 112, 117, 118, 122, 123, 171, 172], "2000": [11, 30, 64, 66, 78, 81, 82, 83, 88, 92, 93, 96, 98, 101, 111, 124], "20000": [64, 92], "20000000000000004": [83, 93, 96], "200000e": 93, "200042": 81, "2001": 70, "20010": 93, "200110": 93, "200225": 77, "2003": [10, 70, 171], "200303": 170, "2004": [70, 124, 128], "2005": [67, 70], "20055": 93, "2006": [70, 93, 124, 128], "2007": [70, 124, 128], "20074": 93, "20081047": 70, "20082661": 70, "200982": 85, "201": [65, 69, 172], "2010": [63, 91], "2011": [63, 91, 168, 170], "201166": 72, "2012522": 70, "2013": [84, 156, 171], "2014": [156, 171], "201421": 69, "2015": [42, 171], "201528": [81, 82], "201531": 72, "20158": 93, "2016": 94, "20167273": 75, "20168243": 70, "2017": [34, 171], "201767": 72, "201768": 91, "2018": [10, 11, 44, 45, 61, 63, 64, 67, 75, 80, 84, 91, 92, 93, 98, 99, 103, 124, 130, 139, 140, 148, 156, 168, 171, 172], "2019": [32, 65, 81, 82, 83, 85, 86, 93, 96, 99, 112, 122, 140, 146, 149, 150, 168, 170, 171], "201e": 97, "202": [72, 172], "2020": [12, 14, 15, 17, 18, 19, 31, 33, 35, 40, 62, 65, 67, 69, 72, 89, 100, 112, 122, 124, 126, 130, 157, 158, 171], "2020435": 63, "2021": [17, 19, 41, 43, 62, 63, 65, 69, 70, 71, 72, 81, 82, 91, 124, 125, 128, 130, 140, 151, 171, 172], "20219609": 63, "2022": [99, 100, 124, 126, 130, 157, 158, 167, 168, 171], "202298": 82, "2023": [36, 66, 98, 101, 124, 138, 140, 154, 155, 171], "2024": [60, 75, 79, 84, 90, 94, 97, 98, 100, 124, 168, 171], "2025": [16, 17, 19, 69, 72, 76, 98, 124, 125, 127, 129], "202650e": 83, "20269": 93, "2029554862": 68, "203": [64, 69, 72, 81, 92, 139, 172], "203082": 111, "2032": 77, "203284": 83, "20329": 93, "203403": 69, "203600": 69, "203828": 93, "204": [69, 72, 172], "204007": 96, "20400735": 96, "204362": 100, "204469e": 139, "204482": 96, "204495": 82, "204794": 96, "204891": 72, "204893": 78, "205": [69, 72, 97, 99, 172], "205111": 72, "205187": 83, "205224": 99, "205245": 93, "205333e": 92, "20584883": 70, "205938": 91, "206": [76, 77, 172], "206086": 72, "206138": 69, "206234": 72, "206253": [92, 93], "206256": 88, "20626766": 70, "206752": 77, "206802": 26, "207": [69, 72, 76, 94, 97, 172], "207129": 72, "207242": 82, "207337": 71, "2075": 62, "20783816": 63, "207840": 86, "207885": 92, "207912": 156, "208": [69, 72, 76, 77, 78, 172], "2080787": 63, "20823898": 63, "208300": 96, "208485": 77, "208553": 70, "2086": [70, 93], "208697": 69, "20877105": [124, 127], "208817": 72, "209": [69, 76, 77, 78, 120], "209219e": 99, "20956": 93, "209647": 70, "209894": 96, "21": [10, 11, 44, 61, 63, 64, 65, 67, 69, 72, 76, 77, 81, 82, 84, 90, 91, 92, 93, 96, 98, 99, 100, 103, 104, 109, 111, 112, 117, 118, 122, 123, 124, 126, 139, 156, 168, 170, 171, 173], "210": [17, 18, 19, 35, 40, 69, 72, 76, 77, 78, 93], "2103": 168, "2103034": 63, "210319": [81, 82], "210323": 96, "210377": 72, "2104": 172, "2107": 171, "210799": 69, "21082": 93, "210946": 70, "210949": 69, "211": [72, 76, 77, 78, 97, 120, 172], "21105": [65, 112, 122, 168, 170], "211115043": 139, "2112": 100, "2114026": 96, "211403": 96, "21142": 93, "211534": 83, "211682": 82, "212": [69, 72, 78, 172], "2121": 93, "212317": 96, "212390": 72, "212491": 96, "21257396e": 139, "212811": 78, "212844": 91, "213": [69, 72, 76, 77, 78, 97, 171, 172], "213199": 124, "213409": 72, "21342": 93, "213483": 69, "21351": 93, "2138": 77, "2139": 33, "214": [60, 69, 72, 76, 77, 172], "214012": 69, "214486": 72, "214732": 81, "214764": 99, "214769": 88, "215": [76, 78, 94, 172], "215103": 82, "215159": 93, "215171": 72, "215342": 96, "2155": 93, "21550": 93, "21562": 93, "215958": 156, "216": [68, 72, 77, 78, 172], "216113": 96, "216207": [112, 118], "216215e": 81, "216224": 82, "21624417": 63, "2163": 93, "216344": 96, "21669513e": 139, "2167": 93, "216943": 111, "217": [69, 76, 77, 78, 97, 171, 172], "21716": 93, "217179": 72, "2171802": [63, 91], "217244": 27, "2175": 93, "217684": 88, "218": [76, 77, 78, 172], "21804": [64, 92], "218223": 90, "218383": 78, "218546": 96, "2189": 93, "218924": 93, "219": [18, 31, 40, 62, 69, 72, 76, 77, 78, 105, 109, 171, 172], "2191274": 63, "219287": [124, 127], "219372": 69, "219585": 78, "219763": 72, "22": [61, 63, 64, 65, 67, 69, 72, 76, 77, 81, 82, 90, 91, 92, 96, 97, 99, 100, 111, 112, 117, 118, 124, 126, 139, 156, 170, 173], "220": [69, 72, 76, 77, 78, 172], "220211": 93, "220587": 16, "220772": 96, "221": [77, 78, 94, 172], "221127": 81, "2213": 91, "2214": 91, "2215": 91, "221578": 69, "2216": 91, "2217": [63, 91], "2218": 171, "222": [77, 172], "222119": 72, "2222": [61, 63, 80, 124], "222261": 111, "222264e": 82, "222294": 71, "222306": 90, "222430": 82, "222562": 72, "22272803e": 139, "222843": 96, "223": [69, 72, 76, 97, 172], "223095": 67, "22336235": 63, "223485956098176": [85, 86], "223625": 16, "22375856": 63, "22390": 92, "224": [69, 72, 97, 172], "224073": 77, "224137": 72, "2244": 171, "224674": 72, "224822": 82, "224897": [81, 82], "224947": 72, "224995": 70, "225": [17, 19, 62, 72, 94, 101, 171, 172], "22505965": 63, "22507006e": 139, "225175": 96, "22528": 93, "225427": 78, "225574": 91, "2256": 93, "22562": 93, "225635": 96, "22563538": 96, "225764": 90, "225766": 72, "225776": 100, "225814": 72, "225917": 90, "226": [69, 172], "226262": [124, 127], "2264": 62, "226479": 88, "226524": 96, "226598": 91, "226919": 82, "226938": 86, "227": [76, 105, 109, 172], "227018": 83, "227018245501943": 83, "227086": 90, "2271071": 36, "227184e": 82, "227567": 111, "2276": 62, "227621e": 82, "2279": 93, "227932e": 92, "228": [69, 72, 76, 172], "228035": 93, "2281": 93, "228648": [64, 124, 127], "228725": 93, "229": [64, 69, 172], "22913223": 70, "22913487": 70, "22913684": 70, "22913807": 70, "229139": 69, "22925": 93, "229266": 69, "229443": 96, "229452": [111, 112, 114, 124], "229472": 92, "2295": [72, 93], "22959": 93, "229726": 93, "229759": [112, 118], "2298": 62, "229961": [81, 82], "229994": [81, 82], "22m": [112, 120, 121], "23": [49, 54, 63, 64, 65, 67, 69, 72, 76, 81, 82, 89, 90, 91, 92, 96, 99, 100, 104, 109, 111, 112, 117, 118, 124, 139, 156, 168, 170, 171, 173], "230": [17, 19, 62, 69, 171, 172], "230009": [85, 86], "23010908": 139, "2302": 98, "230368": 81, "23036871": 70, "2307": [63, 91, 98, 103], "2308": 99, "230821": 93, "230951": [124, 127], "230956": 68, "231": [10, 105, 109, 172], "231165": 124, "23124913": [124, 127], "23129476": 70, "231307": 72, "231310": 96, "231320": 72, "231430": 156, "231467": 124, "231734": 124, "231798": 124, "231879": 77, "231967": 69, "231986": 96, "231e": 97, "232": 172, "232134": [81, 82], "232205": 72, "232277": 82, "232485": 82, "232497e": 81, "232659": 76, "232774": 81, "23286602": 70, "232959": [85, 86], "232e": 124, "233": [34, 172], "2331": 93, "233154": 173, "233415": 93, "2335": 62, "233573": 76, "23367054": 70, "23368": 93, "233734": [124, 125], "23391813": 70, "233931": 76, "234": [69, 72, 171, 172], "234137": 100, "234153": 100, "234534": 83, "234605": 73, "234683": 69, "23473028": 72, "234910": 91, "235": [171, 172], "235419": 76, "235493": 72, "2359": 173, "236": [72, 76, 172], "236008": 83, "23611": 93, "236411": 96, "236556": 69, "236734": 71, "23689448": [124, 127], "23690345e": 139, "237": [65, 172], "237158": 82, "237164": 82, "237345": 71, "237461": 99, "23751359e": 139, "237756": 77, "23797971": 72, "238": [63, 91, 172], "238012": 96, "23801203": 96, "238101": 96, "2381421364244465152556174758487919496": 139, "238213": 156, "238251": 83, "238287": 81, "238416": 81, "238449": 82, "23856": 93, "238619": 78, "238718": 82, "238766e": 72, "239": 172, "239001": 69, "239019": 88, "239352": 93, "239503": 81, "23968": 93, "239845": 93, "239952": 72, "23e": 64, "24": [41, 63, 64, 65, 69, 72, 76, 81, 82, 90, 91, 92, 96, 99, 100, 101, 111, 112, 118, 124, 139, 156, 170, 171, 172, 173], "240": 90, "240053": 82, "240127": [81, 82], "240194": 81, "240295": 99, "2403": 98, "240305364": 139, "240420": 70, "240495": 111, "240532": [81, 82], "240693": 111, "2407": 62, "24080030a4d": 65, "240813": 87, "241": [76, 172], "241049": 96, "241064": 82, "241228": 69, "241474e": 82, "241503": 111, "241516e": 72, "2416": 62, "241841": 95, "241962": 100, "24197136": [124, 127], "241973": 93, "241e": 97, "242": [171, 172], "24208611": 139, "242124": [92, 93], "242139": 156, "242158": [92, 93], "2424": 88, "242427": 88, "242559": 67, "242604": 76, "242815": 156, "242864": 93, "242902": 96, "242964": 71, "243": [93, 172], "243056": 71, "2430561": 62, "243180": 76, "24318918": 139, "243246": 96, "243263": 69, "243457": 95, "243521": 69, "2438": 93, "243e": 97, "244": [93, 94, 172], "244455": 96, "2445": 93, "244622": 156, "244627": 76, "24469564": 170, "245": [171, 172], "245062": 96, "2451": 62, "24510393": 64, "245344": 72, "245370": 91, "245512": 96, "245552": 82, "245610": 82, "245720": 68, "246": 172, "2467506": 63, "246753": 96, "2468": 93, "246862": 69, "246879": 96, "247": [97, 172], "247020": 83, "247057e": 96, "2472": 93, "247617": 111, "24774": [92, 93], "247826": 91, "248": [93, 172], "248171": 96, "248178": 81, "24835916": [124, 127], "248441": 111, "248487": 90, "248497": 72, "248635967": 139, "248638": 83, "248725": [124, 127], "2489": 77, "249": [63, 91, 94, 172], "249067": 82, "2491": 93, "249100": 91, "24917": 93, "249306": 90, "249411": 72, "249494": 81, "2499": [70, 106, 109], "249970": 76, "25": [17, 18, 19, 29, 30, 31, 35, 40, 41, 42, 43, 44, 63, 64, 65, 68, 69, 72, 76, 81, 82, 83, 84, 90, 91, 92, 93, 96, 100, 111, 112, 118, 124, 139, 156, 170, 173], "250": [94, 172], "2500": [70, 93, 106, 109, 124, 129], "25000000000000006": [83, 93, 96], "250083": 93, "2501": 93, "250210": 83, "250425": 83, "250529": 90, "2506": 98, "250758": 69, "250838": 90, "251": [92, 93, 99, 172], "251152": 93, "2515941": 139, "251892": 69, "2519555": 72, "252": [93, 172], "252133": 93, "252253": 99, "252503": 69, "252524": 96, "252601": 156, "252669": 72, "252682": 72, "252707": 69, "252849": 82, "253": 172, "253026": [81, 82], "253610": 16, "253675": 92, "253724": 96, "25374": 93, "253828": 77, "254": [93, 172], "25401679": 63, "254038": 86, "254055": 69, "25408068": 139, "254170": 69, "2543": 93, "254324": 83, "254348": 72, "254400": 156, "254814": 72, "254858": 77, "255": [93, 172], "255151e": 96, "256": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 93, 112, 118, 172], "256082": 111, "256145": 72, "256352": 82, "256416": 96, "25643": 93, "256567": 91, "25672": 93, "25692136": 72, "256944": 96, "257": 172, "257037": 93, "257069": 69, "25716613": 72, "257207": 63, "257361": 69, "257377": 68, "25742553": 70, "257490": 69, "2575": [124, 127], "257703": 69, "25776064": 70, "257762": 96, "25780736": 70, "257953": 69, "258": 172, "258158": [81, 82], "25821083": 76, "2584": 93, "25855": 67, "259": [60, 172], "259184": 77, "259230": 82, "25928861": 72, "259395": 87, "2594": [64, 92], "25943372": 70, "2596": 70, "259828": [81, 82], "25990225": 72, "25x_3": 68, "26": [63, 64, 65, 67, 69, 72, 73, 76, 77, 81, 82, 90, 91, 92, 104, 109, 111, 112, 118, 120, 124, 139, 156, 170], "260": 172, "26016": 93, "260161": 28, "260211": [81, 82], "260328": 81, "260356": 92, "260360": 96, "260762": 78, "260927": 69, "261": [97, 172], "2610": [70, 93], "261031": [124, 129], "261140": 82, "261207": 72, "261366": 81, "261624": [92, 93], "261635": 111, "261685": 93, "261686": 90, "261903": 91, "2619317": 63, "262": 172, "2620": 93, "262083": 90, "262204": 93, "262261": 81, "262829": 139, "263": [10, 93, 172], "263165": 69, "2633": 93, "26374565": 72, "263946": 69, "264": [171, 172], "264086": 68, "264255": 16, "264426": 90, "265": 172, "2651": 124, "265119": [72, 95], "2652": [65, 92, 93], "265462": 72, "265547": 93, "265562": 81, "2658": 86, "265888": 77, "265929": 97, "266": 172, "266147": 97, "2663": 77, "266552": 69, "266633": 81, "266724": 93, "266909": 156, "267": 94, "267055e": 81, "2670691": 63, "267149e": 139, "267164": 90, "267374": 69, "267444": 76, "267500": 91, "267581": 93, "267950": 96, "268": 172, "268405": 69, "268406": 76, "268499": 72, "268850e": 82, "268940": 72, "268942": 96, "268977": 93, "268998": 64, "269043": 96, "269112": 124, "269425e": 81, "269580": 72, "26971447": 72, "26bd56a6": 65, "26e": 64, "27": [17, 18, 19, 35, 40, 61, 63, 64, 65, 66, 67, 69, 72, 73, 76, 81, 82, 90, 91, 92, 104, 109, 111, 112, 118, 120, 124, 139, 156, 170, 171], "270": 172, "2700": 65, "270000e": 93, "270311": 16, "270618": 71, "270644": [81, 82], "270652": 72, "27066": 93, "270771": 69, "271": 172, "271004": [92, 93], "271183": 92, "27119017": 72, "2714838731": 91, "27176736": 72, "271867": 91, "272143": 69, "272384": 82, "272505": 90, "272643": 93, "272662": 94, "273": 65, "27310333": 72, "273131": 72, "273208": 93, "273356": 83, "273433": 76, "273603": 69, "27371": [64, 92], "27372": [64, 92], "273803e": 72, "2739": [68, 69, 72, 97, 100], "274": [65, 76, 93], "2740991": 62, "274251e": 92, "27427865": 72, "27429763": [124, 126], "27431071": [124, 127], "274646": 94, "27472": 93, "274793": 96, "274825": 29, "2754": 62, "275454": 82, "275538": [105, 109], "275596": 156, "275831": 82, "276": [65, 172], "2760": 93, "276002": 77, "276148": 96, "276189e": 91, "2762": 93, "276311": 93, "276587": 69, "2766091": 64, "276774": 76, "27695": 93, "277299": 73, "277626": 111, "277723": 70, "277808": 81, "277987": 90, "278": [85, 99, 172], "2780": 63, "278000": 91, "278035": 78, "2783319": 72, "278391": 93, "2784": 139, "278434": 85, "278454": 88, "278543": 81, "2786": 156, "278907": 96, "279": 172, "279140": 77, "27927": 93, "27951256e": 139, "279524": 96, "279595": 78, "27991": 93, "279926": 82, "28": [63, 64, 65, 69, 71, 72, 76, 81, 82, 84, 90, 91, 92, 111, 112, 113, 116, 118, 120, 124, 139, 156, 170, 172], "280122": 81, "280196": 86, "280454dd": 65, "2805": 168, "280501": 156, "280723": 69, "280766": 82, "280963": 95, "281": [97, 172], "281024": 96, "28111364": 64, "281360": 16, "2814": 70, "2815": 93, "2818": 62, "2819": 156, "281908": 81, "281980": [124, 129], "282": [97, 171, 172], "282200": 86, "28226870": 139, "282308": 76, "2825": [168, 170], "2830": [168, 170], "283168e": 81, "28326": 93, "2836": 62, "2836059": 63, "28367": 93, "283693": 72, "283856": 72, "283892": 69, "283e": 97, "284": 172, "284073": 96, "28425026": 99, "284271": 87, "284397": 173, "28448202": [124, 127], "284518": 72, "28452": [64, 92], "2849": 93, "284987": 93, "285": [97, 124, 172], "285001": 78, "285113": 69, "285160": 91, "285276": 96, "28531434": 72, "285483": 88, "285967": 71, "285e": 97, "286": 172, "2861": 77, "286203": 81, "2865": [62, 93], "286507": 83, "286563e": 93, "28685345": 72, "287": 172, "287041": 96, "287123": 111, "287227": 69, "287441": 69, "287518": 69, "287815": 99, "287926": 96, "288": [94, 172], "288105e": 81, "288788e": 82, "288798": 72, "288833": 76, "289": [171, 172], "289006": 77, "289062": [92, 93], "289081": 96, "289170": 91, "289198": 82, "289440": [81, 82], "289536": 81, "289549": 77, "289555": 88, "289706": 81, "289983": 90, "29": [63, 64, 65, 69, 72, 76, 77, 81, 82, 90, 91, 92, 99, 111, 112, 118, 120, 124, 139, 156, 170], "290": 124, "29001936": [124, 129], "290112e": 82, "290507": 82, "290848e": 72, "290901": 78, "290987": 92, "291": [97, 172], "2910": 93, "291011": [12, 124, 126], "291071": 96, "29107127": 96, "291342": 82, "291406": 96, "291500e": [92, 93], "291517": [81, 82], "291841": 71, "291963": 96, "292": [95, 172], "2920": 93, "292028": 83, "292046": 156, "2921": 77, "292105": 96, "292164": 69, "292179": 111, "292207": 70, "2925": 65, "292652e": 82, "292804": 69, "292843": [124, 129], "292997": 96, "29299726": 96, "293218": 96, "293591": 72, "293611": [124, 129], "293669": 93, "29372827": 139, "293853": 72, "293886": 69, "294": 172, "294067": [81, 82], "294113": 72, "294123": 111, "2945": 77, "294734": 72, "295": [171, 172], "295590": 81, "295837": [73, 104, 109, 170], "2958370000000100000100": [65, 104, 109, 170], "2958370001000010011100": [65, 104, 109, 170], "2958371000000010010100": [65, 104, 109, 170], "295855642811191": 83, "295856": 83, "295868": 93, "296077": 76, "296099": 78, "296171": 81, "296523": 81, "296729": 91, "29675887": 96, "296759": 96, "29678199": [102, 140], "296932": 69, "297": 172, "297102": 81, "297146": 69, "297204": 81, "297287": [81, 82], "297349": [85, 86], "29754129": 72, "297682": 96, "2977": 93, "297752": 69, "29784405": 99, "298": [34, 65], "298069": 81, "298088": 82, "298120": 83, "298132": 81, "298150": 90, "298235e": 93, "298327": 78, "298629": 111, "298995": 111, "299": [65, 97], "299372": 124, "299537": 86, "299669": 72, "299755": 90, "2999": 78, "29999": 69, "2_": [36, 66, 101, 157, 158, 167], "2_x": [36, 66, 101], "2d": [13, 140, 149], "2dx_5": [83, 96], "2e": [60, 62, 63, 64, 65, 66, 112, 120, 121, 122, 123, 124, 140, 156, 170], "2f": [77, 87], "2m": [157, 163, 167], "2n_t": 68, "2x": 96, "2x_0": [32, 81, 82, 85, 86], "2x_4": 68, "3": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 44, 47, 48, 49, 53, 54, 55, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 139, 140, 156, 157, 163, 168, 169, 170, 171, 172], "30": [32, 60, 61, 63, 65, 66, 67, 69, 71, 72, 76, 77, 78, 79, 80, 81, 82, 83, 89, 90, 91, 92, 93, 96, 97, 111, 112, 116, 118, 120, 124, 139, 156, 170], "300": [61, 80, 83, 93, 96, 103, 171], "3000": 78, "30000": 69, "30000000000000004": [83, 93, 96], "30006143": 72, "3002": 77, "300208": 72, "30031116e": 139, "300566": 76, "30093956": 99, "301": 65, "301000": 72, "301366": 156, "301371": 96, "3016": 92, "301629": 81, "30175": 93, "30186": 93, "301985": 69, "302062": 81, "30229388": 96, "302294": 96, "302295": 94, "302552": 81, "302648": 91, "302707": 69, "303115": 69, "303198": 72, "303324": 91, "303835": 91, "303f00f0bd62": 65, "304": 60, "304093": 72, "304130": 96, "304159": 96, "304201": 68, "304385": 69, "3044": 77, "304855": 69, "305142": 90, "305272": 67, "30529": 93, "305341": 96, "305612": 91, "305687": 96, "305775": 96, "305811": 111, "305b": 65, "306282e": 82, "30628774": 96, "306288": 96, "306416": 124, "30645": 93, "30672815": 63, "306738": 69, "306915": 91, "306963": 96, "307098": 82, "307407": 96, "307444": 90, "307528": 72, "307571": 72, "307923": 16, "308": [93, 173], "308011": 69, "30814389": 72, "308239": 93, "308415": 16, "308455": 71, "30861195": 72, "3087": 77, "308837": 82, "30917769": [85, 86], "309289": 69, "309464": 93, "309772": 91, "309952": 77, "31": [63, 64, 65, 66, 69, 71, 72, 76, 77, 81, 82, 90, 91, 92, 94, 111, 112, 118, 120, 124, 139, 156, 170, 173], "310097": 81, "310326": 72, "310899": 76, "310921": 71, "311": 97, "311375": 82, "311594": 81, "311740": 90, "311807": 81, "311869": 111, "311971": 70, "312030": 93, "312172": 85, "312271": 72, "3125": 93, "312652": 97, "312769e": 82, "312873": 81, "313044": 156, "313140": 69, "313209": 83, "313442": 69, "313522": 72, "313535": 96, "31378": 65, "313796": 82, "313870": 88, "314": [76, 139], "3141": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 63, 65, 66, 73, 91, 102, 104, 109, 111, 112, 117, 118, 120, 121, 122, 123, 124, 140, 156, 170], "314247": 100, "314309": 81, "31459503": 139, "3146": 30, "314625": 82, "31476": [92, 93], "314781": 85, "315": 172, "315031": 100, "315155": 82, "315290": [85, 86], "31554378": 72, "316": [65, 172], "316107": 72, "316171": 69, "316193": 96, "316197": 77, "31633": 93, "316717": [81, 82], "316723": 69, "317386": 69, "317394": 68, "317487": 96, "317607": 96, "318": [65, 172], "318571": 156, "318753": [85, 86], "318805": 72, "319": [65, 172], "319100": [85, 86], "319291": 76, "319420": 88, "319559": 77, "319634": 93, "319667": 69, "319750": 16, "319759": 96, "319771": [124, 129], "319850": 96, "319903": 67, "319965": 81, "32": [39, 63, 64, 65, 69, 71, 72, 76, 81, 82, 90, 91, 92, 93, 101, 111, 112, 118, 120, 124, 126, 139, 140, 156, 170], "320": [93, 94], "320000e": 93, "320314": 92, "320633": 83, "321": 172, "32121582": 72, "32166983": 72, "321673": 156, "321924e": 139, "322041": [124, 127], "322404": 99, "32263256": 72, "322736": [37, 72], "322764": 82, "322991": 81, "323272": 69, "3235": 93, "323636": 92, "323674": 72, "323679": 91, "324": [64, 172], "32458367": 63, "3245837": 96, "324910": 77, "325046": 93, "325056": 96, "32514089": 72, "325213": 72, "325657": 81, "325819": 93, "325825": 76, "326": 97, "326400": 72, "326740": 96, "32674263": 71, "326871": 100, "3268714482135149": 100, "327": [93, 172], "327121": 16, "327578": 82, "32789": 93, "327958": 78, "327997": 81, "32809556": 72, "328367": 93, "328478": 76, "32857912": 69, "32875335": 71, "328884": 81, "32950022e": 139, "329671": 81, "329967": 82, "33": [63, 64, 65, 69, 72, 76, 81, 82, 91, 92, 93, 111, 112, 118, 120, 121, 124, 139, 156, 170, 171], "330": [120, 172], "3300": [64, 92], "330000e": 93, "330069": 72, "330285": [81, 82], "330286": 69, "330301": [124, 125], "3304269": 63, "330615": 96, "33065771": 96, "330658": 96, "330731": 29, "331": 172, "331161": 82, "331215": 90, "3313579": 72, "331521": 96, "331538": 76, "331602": 93, "33168943": 93, "33175566": 96, "331756": 96, "331913": 85, "331955": 72, "332": 172, "332028": [124, 127], "332039": 76, "332535": [124, 127], "33261572": 69, "332782": 29, "3329": 93, "332996": 91, "333000": 81, "333250": 93, "3333": [61, 63, 80, 111, 112, 114, 124], "3333333": 65, "33333333": [69, 71, 72], "33335939e": 139, "3334": [77, 93], "333412": 94, "333581": 92, "333882": 81, "333947": 82, "333973": 82, "334": [64, 94], "334092": 72, "3341": 77, "334265": 72, "334750": 83, "335113": 69, "335125": 76, "335446": 78, "335496": 76, "335609e": 96, "335846": 96, "335869": 111, "3359": 77, "336": 172, "336461": 93, "336498": 96, "336612": 68, "336661": 72, "336716": 81, "336870": 70, "337": 172, "3371": 93, "3376": 62, "337970": [124, 125, 129], "337986": 82, "338": [99, 172], "338049": 72, "33849": 93, "3386": 139, "338775": 83, "33880517": 69, "338855": 93, "338890": 82, "3389": 77, "338908": 83, "339177": 77, "339268": 96, "339269": 99, "339348": 71, "339553": 82, "339570": 96, "339762e": 93, "339875": [85, 86], "34": [61, 62, 63, 64, 65, 66, 69, 71, 72, 76, 81, 82, 91, 92, 98, 99, 111, 112, 118, 124, 139, 156, 173], "340": [64, 93, 173], "340142": 124, "340217": 70, "340235": 88, "340274": 99, "340712": 93, "340844": 38, "341106": 81, "341336": 27, "342117": 88, "3422": 93, "342214e": 82, "342254": 72, "342362": 78, "342675": 63, "342693": 76, "34287815": 99, "342992": 91, "343": [97, 124, 172], "343117": 76, "343190": 72, "343453": 72, "34350121": 72, "343639": 111, "34375": 92, "34417449": 139, "344212": 173, "344368": 81, "344440": 111, "344463": 82, "34450402": 71, "344505": [92, 93], "344640": 96, "344748": 95, "344787": [81, 82], "344834": 68, "344845": 82, "344912": 72, "345": 172, "3454": 93, "345903": 96, "346107": 92, "346206": 96, "346238": 99, "346274": 81, "346678": 95, "346683": 83, "3466832975109777": 83, "34717753": 70, "347213": 91, "347310": 29, "34741764": 70, "347501": 89, "347770e": 82, "347793": 82, "34790788": 72, "348": 97, "348338": 93, "34838909": 70, "348617": 96, "348622": 97, "348697": 83, "3486970271639334": 83, "34875207": 70, "349213": 71, "3492131": 62, "349461": 82, "34967621": 63, "349772": 86, "34980811": 75, "349867": 82, "349900e": 93, "34m": [112, 120, 121], "34mglmnet": [112, 120], "34mmlr3": [112, 120, 121], "34mmlr3learner": [112, 120, 121], "34mmlr3pipelin": [112, 120], "34mranger": [112, 120, 121], "34mrpart": [112, 120], "35": [64, 65, 69, 72, 76, 81, 82, 83, 85, 91, 92, 93, 96, 111, 112, 118, 124, 139, 156, 157, 163, 173], "3500000000000001": [83, 93, 96], "350165": [112, 116], "350323": 93, "350491": 81, "350518": 96, "350533": 96, "35053317": 96, "350562": 72, "350712": [85, 86], "35077502": [157, 163], "351161": 69, "35155": 93, "351752": 72, "352": [64, 91, 172], "352047": [124, 127], "352246": 96, "352250e": [92, 93], "3522697": 63, "35292": 93, "353748e": 96, "3538": 62, "353813": [124, 127], "354": [93, 172], "354188": 68, "354371": 96, "354529": 82, "355": 172, "355065": 78, "355508": 72, "355564": 82, "355627": 111, "35595509": 70, "35595939": 70, "35596355": 70, "35596802": 70, "356": 172, "356136e": 93, "356167": 86, "35620768e": 139, "356345": 69, "3566": 93, "3568": 124, "356821": 111, "357": [93, 172], "35731523": [124, 126], "357450": 81, "357586986897548": 67, "357654": 93, "358": 172, "358158": [92, 173], "358288": 72, "358289": 91, "358395": 99, "358690e": 93, "35871235": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "358787": 156, "359": [97, 172, 173], "359050": 82, "359229": 78, "3593": [75, 99], "359307": 82, "359622": 77, "359770": 69, "3598": 93, "35th": 171, "36": [60, 64, 65, 69, 71, 72, 76, 77, 81, 82, 91, 92, 94, 111, 112, 118, 124, 139, 156], "360": 172, "360004": 96, "360054": 156, "360249": 87, "360403": 76, "360419": 69, "360475": [81, 82], "360682": 72, "360683": 83, "360801": 83, "360965": [105, 109], "361": [97, 172], "361220": 111, "361234": 72, "361487": 72, "361623": 88, "361663": 69, "3619201": 33, "36213": 93, "36231307e": 139, "362398": 25, "36274369": [124, 129], "362760": 93, "363": 172, "363103": 83, "3631031251500065": 83, "363215": 81, "363276": 63, "363576": 90, "363771": 81, "363990": 81, "364276": 90, "3643": 156, "364595": 63, "3647": 65, "364800": 96, "36489934": 72, "365": 172, "365150": [124, 129], "365509e": 81, "365527": 93, "36557195e": 139, "36566025e": 139, "365702": 81, "366188": [105, 109], "366718627": 63, "366912": 81, "367": [97, 172], "367017": 90, "367177": 69, "367225": 81, "367323": 96, "367366": 88, "367398": 72, "367571": 83, "367625": 96, "367664": 72, "367697": 82, "368": [69, 71, 72, 93, 172], "368152": 91, "368191": [124, 125], "3682": [64, 92, 93], "368324": 91, "368577": 111, "369": 172, "369212e": 82, "36930727": 72, "369556": 83, "369596": 72, "3696": 99, "369796": 96, "369869": 92, "369981": 91, "36m": [112, 120, 121], "37": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 64, 71, 75, 77, 81, 82, 88, 91, 92, 111, 112, 122, 123, 124, 139, 156], "370": 172, "370000e": 93, "370152": 69, "370165": 93, "370254e": 92, "3702770": 63, "370736": 91, "3707775": 63, "370889": 76, "371": 172, "371015": 70, "3711": 77, "3711317415516624": 83, "371132": 83, "3712": 93, "371357": [92, 93], "371429": 83, "371562": 69, "371587": 69, "371972": 26, "372": [171, 172], "37200": [92, 93], "372097": 83, "3722": 93, "3724": 93, "372555": 82, "3727679": 63, "373034e": 38, "373235": 93, "373451": 111, "373614": 69, "373783": 81, "3738573": 63, "373941e": 81, "374": [120, 172], "374335": 96, "37433503": 96, "3745": 93, "37477097": 139, "374812e": 93, "375081": 93, "375106": 76, "375151": 81, "375403": 67, "375465": 96, "375776": 81, "375897": 76, "375995": 90, "3766": 88, "376617": 88, "376681": 82, "376937": 69, "376972": 82, "377": 172, "377147": 111, "377246": 93, "377311": 96, "377572": 82, "377955": 96, "378133": 72, "378154": 69, "378161": 70, "378367": 96, "378471": 81, "378596": 91, "37880": 139, "378834": 96, "3788859": 63, "379": 171, "379614": 96, "379892": 81, "38": [60, 65, 76, 77, 81, 82, 92, 111, 124, 139, 156], "3800694": 63, "380442": 81, "380538": 77, "380695": 77, "380837": [92, 93], "381": 97, "381072": 96, "381247": 93, "381278": 93, "381684e": 92, "381685e": 93, "381689": 96, "382024": [105, 109], "382285": 93, "38240784": 72, "382443": 72, "382730": [124, 127], "38285613": 72, "382872": 83, "382919": 16, "383178": 69, "383297": 96, "38330262": 71, "38348": 93, "383510": 69, "383759": 69, "383921": [124, 127], "384": 93, "38402797": [124, 127], "384189": 90, "384208": 90, "384232": 77, "384677": 78, "384717": 81, "384729": 72, "384755": 82, "3848": 93, "384883": 93, "385226": 156, "38538781": 139, "385583": 69, "385917": 91, "386": [65, 93], "386102": 83, "386158137": 139, "386229": 94, "386428e": 82, "386502": 93, "386616": 82, "386813": 72, "386831": 78, "387": 65, "3871": 62, "387268": 76, "387390": 69, "387426": 96, "387673": 81, "387710": 72, "387780": 96, "388005": 93, "388071": 96, "388185": 78, "38818693": 139, "388216e": [112, 118], "38898864": 96, "388989": 96, "389": 65, "389186": 124, "389603": 81, "38973512e": 139, "38983785": 67, "38990574": 139, "39": [60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 78, 81, 82, 87, 89, 91, 92, 93, 99, 100, 101, 111, 112, 117, 124, 139, 156], "39007": 30, "39010121e": 139, "390155": 90, "390185": 69, "390266": 69, "390312": 82, "390379": 96, "390495": 82, "391377": 100, "39186467": 75, "391982": 69, "392242": 87, "392472": 93, "392623": 82, "3926871": 139, "392801": 81, "392833": 99, "392864e": [92, 93], "393441": 82, "393604": 83, "393654": 78, "3937": 77, "39425708": 63, "39454997": [124, 127], "394554": [124, 125, 129], "394757": 90, "39494900": 139, "395076e": 93, "395175": 72, "395195": 91, "395268": 111, "395558": 81, "3958": 124, "39587324": 71, "396": 124, "3961": 93, "39611477": 64, "39621961e": 139, "396272": 90, "396531": 93, "396671": 81, "396835e": 82, "396985": 91, "396992": [81, 82], "397140": 83, "39727": 93, "397313": 62, "3975117": 71, "397578": 87, "3977": 77, "397811": 99, "3979": 71, "398": [104, 109, 170], "398000e": 93, "398021": 71, "398166": 78, "398367": 77, "3985": 93, "398595": 69, "398770": 96, "398999": 24, "399": 64, "399056": 96, "399223": 68, "399355": 68, "399372": 69, "399679": 124, "399692": 96, "399858": 100, "39m": [112, 120, 121], "3cd0": 65, "3dx_1": [83, 96], "3e1c": 65, "3ec2": 65, "3f5d93": 94, "3x_": 96, "3x_4": [83, 96], "4": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 36, 37, 38, 39, 40, 41, 47, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 117, 118, 123, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170, 172], "40": [63, 66, 67, 76, 81, 82, 83, 92, 93, 96, 101, 104, 109, 111, 112, 118, 124, 139, 156, 157, 163], "400": 91, "4000": [69, 72], "4000000000000001": [112, 118], "40000000000000013": [83, 93, 96], "40001265": 139, "400113": 88, "40029364": [157, 163], "400297": 82, "400587": 90, "400823": 96, "400905": 78, "401": [10, 173], "401009": 90, "401035": 76, "401037": 76, "401247": [39, 140, 156], "40127723e": 139, "401931": [85, 86], "402": [104, 109], "4020": 77, "402022": 72, "402023": 69, "402077": 93, "402100": 156, "402301e": [112, 116], "40262617": 71, "402924": 76, "403": 97, "403113": 83, "403113188429014": 83, "403425": 96, "40359107": 67, "403626490670169": 102, "4036264906701690": 102, "403626491": 102, "403771948": 140, "4039": 62, "40391364": 139, "404300": 78, "404318": 62, "40452": 93, "404550": 95, "404824": 93, "404834": 124, "4050": 139, "405000": 93, "405203": 68, "405313": 81, "40550695": 71, "40557353": 71, "40583": 62, "405890": 29, "405892": 81, "40608": [124, 125], "406085": 111, "406285": 96, "406317": 72, "406446": 83, "4065173": 139, "406565": 81, "406693": 69, "406756": 82, "40676": 62, "407": 97, "40732": 88, "407758": 16, "407786e": 81, "40795732": 71, "408000": 82, "408135": 81, "408476": [157, 163], "40847623": [157, 163], "408539": 96, "408565": 96, "408682": 81, "409154": 62, "4093": 99, "409390": 93, "409395": 96, "409532": 81, "409551": 82, "409713": 72, "409746": 83, "409848": [81, 82], "409923": 69, "41": [76, 81, 82, 92, 93, 111, 112, 123, 124, 139, 156], "410095e": 82, "410151618202334394550545766778086899798": 139, "41015161820233439455054576677808689979891219242527313335384043475964677376931001567111722262930374960687281828595992381421364244465152556174758487919496": 139, "4101895": 76, "4103": 77, "410393": 83, "410621": 94, "410681": 68, "410795": 91, "41093655": 139, "411190": [81, 82], "411295": 96, "411304": [81, 82], "411582": 96, "41174392": 71, "412": 93, "412127": 96, "412304": 100, "412406": 93, "412477": 68, "412620": 76, "412653": 91, "412714": 83, "412864": 85, "412919": 82, "412947": 81, "413": 93, "41336": [112, 113, 116], "413376": 124, "41341040": 63, "413616": 71, "413784": 93, "414": 97, "414139": 69, "414157": 69, "415040": 82, "41525168e": 139, "415294": 96, "415556": 111, "41564189": 71, "41566": 124, "415812": 173, "416052": 78, "41605334": 71, "4166": 93, "4166667": 65, "416683": 72, "416709": 82, "416737": 90, "416899": 81, "416e": 97, "417": 120, "417619": 82, "417669": 93, "417727": 92, "417767": [85, 86], "417834": 78, "417859": 81, "41798768e": 139, "417988": 96, "418052": 81, "418056": 96, "41805621": 96, "418400": 88, "418591": 93, "41867611": 72, "418741": 78, "418806e": 83, "418969": 111, "41918406e": 139, "41931306": 71, "419371": 96, "419871": 78, "41989983e": 139, "4199952": 63, "41e5": 65, "42": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 66, 67, 68, 69, 71, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 93, 95, 96, 99, 100, 101, 106, 109, 111, 112, 118, 124, 125, 126, 127, 129, 139, 156, 171], "420308e": 93, "42071": 71, "42073312": 63, "420967": 83, "421083": 62, "4211349413": 63, "421200": 100, "421357": [85, 86], "42143": 93, "421576e": 93, "421793": 99, "4218": 77, "421919": 93, "422007": 99, "422116": 93, "422119": 93, "422244": 76, "422251": 82, "422325": 83, "422646e": 82, "42264931": 72, "4227502": 139, "422914": 72, "422933": 76, "422984": 16, "423": 60, "42303521": 71, "4232": 77, "4234": 77, "4235": 77, "4235839": [85, 86], "42365741": [124, 129], "423951": 62, "42410067": 71, "424108": 83, "424127": 139, "42412729": 63, "42419005": 71, "4242": 89, "424328": 96, "424573": 90, "424651": 124, "424682": 81, "4247": 77, "424717": 83, "424748": 100, "424847": 69, "424879": 93, "425": 91, "425000e": 93, "425103": 62, "42520897": 72, "425325": 88, "425414": [105, 109], "42543731": 71, "425493": 62, "42550": 93, "425636": 88, "426055": 62, "426133": [124, 127], "42620147": 71, "426540": 91, "426540301": 63, "426734": 90, "426736": 93, "426782": 69, "427": 93, "427486": [81, 82], "42755087": 99, "427551": 99, "427573": 91, "4276": 77, "427654": 88, "427725": 96, "428": [156, 173], "428046": 95, "42811700": 173, "428191": 81, "428255": 96, "428411": [92, 93], "428467": 96, "4284675": 96, "4286": 77, "428612": [124, 129], "428766": 81, "428769": 69, "428771": 29, "428779": 82, "428863": 72, "4290": 62, "429133": 90, "429141e": 81, "429298": 90, "429309": 93, "42934105": 101, "429438": 72, "4298455": 139, "42ba": 65, "43": [64, 71, 76, 78, 81, 82, 93, 111, 124, 139, 156], "430193": 69, "4302": 77, "430298e": [92, 93], "430345": 90, "430872": 76, "430878": 76, "4311947070055128": [112, 118], "431306": 96, "431437": 99, "431605": 81, "431639": [124, 129], "431701914": 168, "431747": 82, "431929": 93, "431998": 78, "432130e": 92, "432300e": 96, "43231359e": 139, "432707": 97, "43294": 65, "432f": 65, "433": [65, 97], "4330": 93, "433221": 83, "433473": 93, "433509": 77, "433614": [124, 129], "43361991": 96, "433620": 96, "433684e": 81, "4339": 62, "434054e": 88, "434116": 81, "434393": [124, 129], "4344": 77, "434535": 96, "43453524": 96, "434698e": 81, "4348": 77, "43482": 93, "435": [65, 139], "43503345": 124, "435126": 72, "435192": [124, 127], "435306": 76, "435345": 72, "435401": 91, "43567360": 139, "4357": 93, "435927": 93, "435967": 91, "436": [65, 93], "436327": 93, "436764": 97, "436806": 93, "43684628": 72, "436987": 71, "437134": 93, "437257": 72, "437601": 82, "437667": 92, "437924": 93, "438": 91, "43803797": 72, "438219": 96, "438293": 69, "438323": 72, "438569": 93, "43871788": 71, "43883": 86, "438960": 91, "4391": 77, "439257": [124, 129], "439541": [92, 93], "439675": 97, "439699": 78, "439732039": 139, "439858": 69, "43989": 24, "43f0": 65, "44": [71, 76, 78, 81, 82, 111, 120, 124, 126, 139, 156], "4401": 77, "440170": 81, "440320": 93, "440524": 81, "440605": [112, 116], "440790": 77, "440867": 76, "44093442": 139, "440a": 65, "441004": 93, "441153": 96, "441209": 96, "44124313": 124, "441311": 85, "441528": 81, "441567": [124, 127], "4416552": 63, "4417": 77, "4420": 77, "442976": 76, "44300848": 72, "443016": 83, "443032": 92, "44312177": 64, "44349955": 76, "443686": 96, "443782": 72, "4438": 93, "443e": 97, "444": 60, "44404522": [124, 127], "444046": 93, "44412537": 124, "4444": [61, 63, 80, 124], "444500": [92, 93], "444805": 90, "445": 97, "4450": 77, "445238": [124, 129], "4455": 77, "44563945e": 139, "445816": 81, "4458522": 139, "4460": 77, "446023": 95, "4462": 65, "44647451": 99, "446981": [124, 127], "447": 120, "44713577e": 139, "447375": 76, "447624": [81, 82], "447706": 83, "447745": 72, "447803": [124, 127], "447863": 83, "447863428811719": 83, "447871e": 72, "447897": 81, "448": 93, "448070": 82, "4481": 77, "448525": 69, "448587": 83, "4487": 93, "448745": 96, "44890536": 139, "448923": 87, "449150": 29, "449172": 76, "449345": 82, "449677": 88, "449715": 76, "449835": 72, "44fa97767be8": 65, "45": [67, 81, 82, 83, 87, 90, 93, 96, 111, 112, 120, 124, 139, 156], "4500": 92, "45000000000000007": [83, 93, 96, 112, 118], "450000e": 93, "450097": 82, "4501497883477352": 117, "450152": 91, "45020276": 72, "4506": 77, "4506262264637636": 112, "450708": 71, "450870601": 63, "451137": [124, 125, 129], "451158": 76, "4513": 77, "4516": 70, "451671": [124, 129], "452": 65, "452091": 93, "452110": 69, "452114": 111, "45244897": 72, "452488701": 63, "452489": 91, "452622": 81, "4526379": 139, "453": [65, 94], "453189": 76, "4535": 93, "4536": 93, "4539": 65, "4539275": 139, "454129": 16, "454406": 78, "45451": 93, "454513": [124, 129], "45467447": 139, "454991": 77, "455": 65, "455078": 83, "455107": 83, "4552": 65, "455262": 82, "455293": 83, "4552b8af": 65, "455448": 99, "455564": 16, "455629": 69, "455672": 93, "455688": 72, "455981": 157, "456": 94, "45613995": [124, 127], "456225": 96, "456370": 91, "456453": 16, "4566031": 156, "45660310": 156, "456617": 96, "4567": 99, "4568": 77, "456892": 83, "457088": 96, "458114": 93, "458194": 72, "458307": 158, "4584": 93, "458420": 93, "45842424": 71, "4584447": 63, "4585199": 139, "458814": 88, "458855": 64, "459098": 81, "4592": 63, "459200": 91, "459383": 83, "45957837": 139, "459597": 72, "459617": 16, "459710": 72, "459760": 93, "459812": 83, "46": [71, 77, 81, 82, 87, 94, 111, 124, 139, 156], "460013": 72, "460084": 72, "4601": 93, "460207": [81, 82], "460218": 83, "460289": 96, "460385": 81, "46045571": 72, "46062": 139, "460744": 92, "4610": 173, "4611": 77, "461162e": 82, "461396": 90, "461412": 111, "461469": 70, "461493": 90, "461629": 100, "462278": 112, "462319": 93, "462451": 83, "462473": 82, "463046": 82, "4631122": 139, "463208": 93, "463325": 96, "463369783": 139, "4634": 93, "463418": 100, "4637": 77, "463766": 86, "463803": 82, "463b": 65, "464": 60, "464076": 83, "464080": 94, "464282": 91, "46448227": 139, "464537e": 81, "464668": 27, "4649": 77, "465": [78, 124], "46507214": 99, "4652": 77, "465649": 100, "465730": 100, "4659651": 102, "465965114589023": 102, "4659651145890230": 102, "465981": 96, "46598119": 96, "466047": 96, "46618738": 139, "466269": [124, 129], "4663": 77, "466440": 83, "466452": 76, "466684": 93, "466756": 96, "46709481": 139, "46722576e": 139, "46752915": 139, "467613": 91, "467613401": 63, "467681": [81, 82], "467770": 83, "468001": 93, "468016": 76, "468035e": 81, "468072": 82, "468075": 96, "46807543": 96, "468406": 93, "468869": 82, "468907": 78, "468d": 65, "469": 65, "469170": 93, "469379": 93, "469474": 100, "469676": 78, "469825": 83, "47": [60, 64, 77, 78, 81, 82, 85, 92, 99, 111, 124, 139, 156, 172], "470023": 82, "470365": 93, "470801": 81, "470828": 81, "471": 78, "47100203": 72, "471435": 97, "471454": [105, 109], "471532": 69, "471666": [105, 109], "472255": 93, "472380": 72, "472891": 96, "472952": 82, "472e": 65, "473036": 111, "473099": 83, "47319": 124, "47419634": 170, "474214": [85, 86], "474317": 71, "474699": 82, "474709": 124, "475395": 93, "475551": 81, "475e": 97, "476270": 69, "4766": 77, "476856": 83, "47699623": [124, 129], "4770": 77, "477130": [81, 82], "477150": 96, "4772": 77, "477354": 93, "477357": 97, "477395": 93, "47743379": 72, "477474": 91, "47759584": 139, "47782": 93, "477834": [124, 129], "47805578": [124, 129], "4782": 77, "478276": 72, "478358": 72, "478399": 93, "47857478": 139, "478618": 72, "478668": 82, "478794e": 82, "478896": 93, "479119": 82, "479288": 16, "479527": 82, "47966100e": 139, "479722": 82, "479876": [85, 86], "479882": 82, "479928": 96, "47be": 65, "48": [65, 78, 81, 82, 86, 92, 93, 111, 124, 139, 156], "480": 78, "480000e": 93, "480133e": 96, "480191": 71, "480199": 88, "48029755": 99, "480579": 82, "48069071": [102, 140, 156], "480691": [39, 140, 156], "480800e": 96, "481172": 96, "481344": [124, 125, 129], "481359": 69, "481399": [92, 93], "4814": 75, "481713": 88, "481761e": 93, "482": [65, 78], "482038": 83, "482312": 71, "482349": 85, "482461": [157, 163], "48246134": [157, 163], "482483": 96, "482508": 90, "482790": 68, "48296": 99, "483": [97, 124], "4831471": 139, "48315": 99, "483165": 81, "483186": 68, "483192": [92, 93], "48331": 99, "483357": 82, "483531": 96, "48353114": 96, "483572": 69, "483642": 93, "483711": 96, "483717": 83, "483855": 93, "484": 93, "48404": 63, "484074": 82, "484111e": 81, "4842": 93, "484271": 69, "484272e": 82, "484545": [124, 127], "484640": 96, "48474275": 72, "4849": 65, "485": [65, 76, 93], "485000": 76, "485368": 82, "485468": [105, 109], "48550": 100, "485617": [92, 93], "485759": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "48583": [92, 93], "485871": 86, "486": [42, 93], "486202": 83, "486333": 117, "486342": 93, "486476": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "486532": 96, "486692": 76, "487": [78, 93], "487006": 72, "487352": 70, "487467": 93, "487524": 88, "48753207": 72, "487641e": 96, "487686": 72, "487775": 71, "487872": 79, "487934": 16, "4879383": 139, "488455": 90, "488551": 76, "488713": 72, "488759": 81, "488811": 96, "488866": 81, "488909": [92, 93], "488982e": 83, "489054": 95, "489292": 76, "489511": 76, "4895498": 96, "489550": 96, "489567": 98, "489699": 83, "489760": 72, "49": [65, 76, 78, 81, 82, 111, 139, 156], "490070931": 63, "4904": 76, "490488e": [92, 93], "490689": 90, "490882": [124, 125], "490888": 76, "490896": 78, "490898": 93, "490941": 93, "49098": 99, "491245": 91, "491249": 90, "491479": [124, 129], "4915707": [124, 126], "492": [60, 93], "492074": [124, 129], "492075": 81, "4923156": 102, "49231564722955": 102, "492315647229550": 102, "492410": 96, "492417e": 124, "492626": 87, "49270769e": 139, "492804": 72, "493": [97, 124, 171], "4931": 93, "493144": 100, "493195": 88, "493219": 96, "493426": 97, "493792": 90, "493823": 94, "494": 97, "494253e": 81, "494324": 91, "494324401": 63, "494756": 71, "495": 95, "495028": 82, "495068": 72, "49530782": 63, "495396": [124, 127], "495405": 96, "495653": 69, "495657": 83, "495752": 96, "49596416e": 139, "496": 95, "496074": 81, "496300e": 93, "4965": 77, "49650883": 99, "496551": 96, "496714": 100, "496735": 69, "496777": 173, "496787": 72, "49693": [112, 121], "497": 95, "497050": 94, "497078": 72, "4977067": 139, "497740": 72, "497915": 81, "497964": 111, "498": [93, 95], "498048": 77, "498122": 88, "498575": 76, "498836": 94, "498921": 96, "498979": 93, "498f": 65, "499": [76, 95, 104, 109, 170], "499000e": [92, 93], "49934441": 76, "499416": 82, "499451": 69, "4996": 77, "499789": 71, "49d4": 65, "4a53": 65, "4b8f": 65, "4dba": 65, "4dd2": 65, "4e": [63, 64], "4ecd": 65, "4f": 76, "4fee": 65, "4x": 96, "4x_0": [32, 81, 82, 85, 86], "4x_1": [32, 81, 82], "5": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 55, 56, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 139, 140, 148, 156, 157, 163, 169, 170, 172], "50": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 63, 65, 66, 68, 72, 75, 76, 78, 83, 86, 90, 92, 93, 94, 96, 111, 139, 156], "500": [7, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 37, 38, 39, 40, 41, 44, 51, 61, 65, 69, 70, 71, 72, 73, 75, 76, 77, 80, 81, 82, 85, 86, 92, 95, 97, 99, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 124, 125, 126, 127, 129, 140, 156, 157, 163, 170, 173], "5000": [47, 69, 70, 71, 72, 81, 82, 83, 96, 98], "50000": 91, "500000": [92, 93], "5000000000000001": [83, 93, 96], "500000e": 93, "500084": 96, "500267": 87, "5003517412": 63, "500635": 93, "50093148e": 139, "501021": 93, "5011": 77, "501164": 94, "501203": 90, "501364": 72, "501403": 96, "501769": 71, "501808": 72, "501988": [124, 129], "502005": 111, "502084": 124, "502268": 93, "502494": 83, "5025850": 63, "502612": 96, "502950": 94, "502995": 96, "503": 120, "503089": 96, "503226": 82, "503504": [112, 113, 116], "503700": 78, "503768": 82, "503830": 69, "50398782e": 139, "504076": 82, "504286": 91, "5042861": 63, "504320": 72, "504889": 69, "5050973": 63, "505712": 76, "505958": 72, "506080": 77, "506090": 69, "506126": 72, "506632": 69, "50672034": 63, "506736": 111, "506833": 81, "506900e": 96, "506903": 83, "507": 97, "507285": 88, "507683": 82, "50768b": 94, "508068": 81, "508153": 95, "50828321": 76, "508406": 95, "508433": 81, "508459": 91, "508756": [105, 109], "5089": 93, "508947": [15, 124, 126], "508988": 81, "509005": 81, "509059": 93, "509102": 93, "509196": 96, "509461": 96, "509651": 90, "50967": 100, "5097": 100, "509764": 72, "5098": [73, 104, 109, 170], "509853": 96, "509894": 76, "5099": [65, 73, 104, 109, 170], "509951": 83, "509958": 91, "51": [62, 64, 65, 71, 78, 85, 92, 111, 139, 156, 172], "510000e": [92, 93], "510257e": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "510385": 91, "510555": 78, "510586": 96, "51079110": 63, "510971": 90, "510982": 72, "5115547": 102, "5115547181877": 102, "51155471818770": 102, "511668": 100, "5116683753999616": 100, "511722": 38, "512": [60, 91], "512108": 96, "512149": 96, "51214922": 96, "51233332": [124, 127], "51243406e": 139, "512519": 91, "512566": 82, "512572": 96, "512657e": 93, "512672": [124, 157, 163], "51276711": 72, "512885": 76, "5130468": 139, "5131": 92, "513222": 88, "513658": 93, "513992": 96, "514": 65, "514024": 81, "5140249": 139, "514179": 81, "514199": 77, "514957": 82, "514958": 72, "51516233": 139, "515208": 82, "515358": 83, "5154": 93, "5154782580830215": 91, "5155": 65, "515582": 69, "516": 65, "516125": 83, "516145": 93, "516222": 96, "516255": 96, "516256": 96, "516528": 96, "516800": 69, "517": [65, 91], "5175": 93, "517510812139451": 67, "517798": 78, "517798e": 139, "517852": 71, "518175": 91, "51834137": 72, "518478": 78, "518517": 30, "518610": 124, "518616": 82, "518846": 91, "518854": 78, "518978": 90, "519625": 70, "519637": 81, "51966955": 63, "519710": 96, "519828": 82, "519898": 81, "52": [62, 65, 87, 90, 97, 111, 139, 156], "520": 93, "520012": 81, "520096": 90, "520406": 81, "520641": 99, "520670": 72, "520918": [124, 129], "520930": 83, "521002": 83, "5210215656500876": 76, "521044": 76, "521104": 93, "521118": 76, "521233": 78, "521357": 82, "521608e": 81, "521644": 76, "521990": 76, "521994": 76, "522398": 90, "522605": 93, "522835": 68, "523030": 100, "523140": 82, "523163": 83, "52343523e": 139, "523453": 76, "523794e": 96, "5238": 77, "523977545": 63, "524215": 93, "52424539": 63, "524493": 69, "524557": 72, "52465": [112, 120], "524657": 96, "524817": 90, "524934": [81, 82], "525064": 78, "52510803": 64, "5251546891842586": 100, "525316": 76, "525324": 89, "52544332": 124, "5255": 65, "52590": [64, 92], "525956686": 139, "526": 91, "526125": 81, "526532": 93, "526582": 88, "526769": [81, 82], "52678547": 139, "526991": 69, "527": 120, "527141": 81, "52714188": [124, 127], "52727602": 72, "52732": [112, 121], "527644e": 96, "527793": 72, "528000e": 99, "528075": 76, "528145": 82, "528255": [124, 127], "528272e": 82, "528381e": 101, "528553": 76, "528725": 96, "528937": [85, 86], "528991": 76, "528996901": 63, "528997": 91, "529": 91, "529178": 81, "529405": 62, "529468": 111, "529692": 82, "529782": 62, "53": [62, 65, 76, 78, 87, 91, 104, 109, 111, 124, 126, 139, 156, 168, 171], "5301": 93, "530659": 87, "530812": 69, "530830": 93, "530940": 96, "53094017": 96, "531": 65, "531030": 82, "531223": 83, "53148135": [124, 127], "531515": 111, "531594": 93, "531999": 93, "532202e": 30, "532266": 83, "532421": 85, "53248126": 72, "53257": [112, 116], "532738": 96, "53273833": 96, "5328": 93, "53291002": [124, 127], "533": 97, "533283": 124, "533316": 93, "533489": 68, "5337": 77, "533871": 93, "533900": 96, "533936": 72, "534139": 78, "5346": 65, "534759": 81, "535179": 96, "535318": 96, "535736": 69, "53606675": 96, "536067": 96, "53618908": 72, "536658": [124, 127], "536792": 93, "536798e": [92, 93], "53763958": 72, "537681": 93, "53791422": [101, 124], "538": 65, "5382": 99, "538227": 82, "538282": 93, "53849791": 96, "538498": 96, "538513": [105, 109], "538737": 69, "538937": [92, 93], "539455": 96, "539491": [85, 86], "539767": 83, "54": [62, 64, 65, 76, 77, 90, 97, 103, 111, 139, 156, 172], "540101": 96, "54010127": 96, "540240": 93, "540299": 76, "540375": 88, "5405": 88, "540549": 88, "540578": 90, "540625": 72, "54074547": 139, "5408": 62, "541": 97, "541010": 96, "541060": 88, "541118": 72, "541159": 96, "541162": 69, "54119805": 67, "54138277": 72, "54163": 99, "5416844": 102, "541684435562712": 102, "541863": 76, "542": 97, "542446": 86, "542451": 96, "542560": 100, "542647": 96, "542648": 111, "542671": 91, "542754": 82, "542769": 96, "542883": [157, 163], "5428834": [157, 163], "542919": 111, "542989": 96, "543": [91, 93], "543014": 93, "543052": 78, "543075": 83, "543136": 83, "543287": 76, "543380": 91, "5434231": 102, "543423145188043": 102, "5436005": 63, "5436876323961168": 83, "543688": 83, "543734": 93, "543764": 86, "54378": 99, "544": 94, "544058": 76, "544097": 96, "54411927": 139, "544276": 82, "544383": 100, "544396": 69, "544555": 91, "544657": 76, "544680": 82, "54483": [140, 156], "5448331": [140, 156], "54517706e": 139, "545416": 72, "545492": 78, "54550506": 97, "545605e": 96, "545672": 82, "545930": 111, "546223": 81, "546260": 90, "54631452": 72, "546438": 93, "546967": 81, "54716": 99, "547257": 82, "547306": 83, "5473064979425033": 83, "5475": 93, "547794": 16, "5479": 93, "547909": 93, "549192": 76, "5494": 93, "55": [64, 65, 76, 83, 90, 92, 93, 96, 111, 139, 156], "5500000000000002": [83, 93, 96], "550176": 90, "550196": 76, "55021688": [124, 129], "550472": 82, "551008": 71, "551010e": 93, "55122032": 72, "551317": 82, "551367": 71, "551686": 83, "55176": [112, 120], "551820": 69, "552081": 90, "552358": 72, "552727": 91, "552776": 96, "5529": 70, "553004": 78, "55307": [112, 123], "553097": [124, 127], "553154": [124, 129], "55346908": [124, 129], "553672": 93, "553754": 111, "553878": 28, "55390846": [124, 127], "554076": 83, "554793e": 139, "5548": 77, "55484": 71, "554973": 69, "554986": 83, "554986206618521": 83, "555": 91, "555356": 72, "555445": 95, "555498": 96, "5555": [61, 80], "555544": 81, "555954": 93, "556": 173, "556191": [81, 82], "556333": [124, 127], "556533e": 93, "55662098": [124, 127], "556792": 96, "5572": 77, "5574dcd4": 65, "557595": 91, "557741": 90, "557872": 72, "557999": 91, "558134": [81, 82], "55814996": 72, "55828259": 101, "558387": 82, "5584": 91, "5584610": 139, "5585": 91, "55855122": 67, "558633": 72, "55863386": 124, "558655": 83, "5589": 91, "558996": 93, "559": [41, 171, 173], "5590": 91, "559144": 83, "559186": 83, "5592": 91, "559394": 96, "559522": 96, "559680": 93, "559712": [105, 109], "559844": 82, "559952": 69, "55dc37e31fb1": 65, "55e": 64, "56": [65, 77, 103, 111, 139, 156, 168, 171, 173], "560135": [39, 140, 156], "56018481": 96, "560185": 96, "560250": 82, "560616": 76, "560689": 62, "560723": 87, "5607573": [124, 127], "561159": 69, "561309": 93, "561447": 72, "5616": 92, "561652": 82, "561785": [15, 124, 126], "562013": 96, "56223": 99, "562288": 100, "562317e": 81, "562320e": 81, "562390": 111, "562556": 71, "5625561": 62, "562712": [81, 82], "562866": 93, "563026": 70, "563067": 97, "563374e": 83, "563456": 111, "563503": 96, "563563": 88, "563673": 93, "563690": 93, "563760": 93, "5638": 93, "56387280e": 139, "56390147e": 139, "564": 124, "564045": 96, "564073": 93, "564098": 69, "564232": [81, 82], "564451": 82, "564577": 93, "564610": 81, "564722": 72, "564800e": 93, "564829": 83, "565066": 83, "56521754": [124, 127], "565253": 72, "565915": 93, "566": 100, "566024": 96, "56633984": [124, 127], "566521": 72, "566600": 96, "56670073": 96, "566701": 96, "566964": 81, "567004": 99, "567491": 82, "567695": 81, "567945": [85, 86], "568071": 93, "568143e": 81, "568287": 88, "56915009": [124, 127], "569449": 111, "569597e": 139, "569911": 63, "5699994715": 63, "57": [65, 76, 97, 111, 139, 156, 173], "57002385": [124, 127], "570278": 81, "570351": 77, "570486": 62, "570562": 62, "570722": 170, "5708": 93, "571040": 82, "571429": 83, "5714294154804167": 83, "571758": 72, "571778": 62, "5718": 93, "5722": 92, "57245066": 96, "572451": 96, "572717": 98, "573349e": 82, "573663": 69, "573679": 16, "573700": 68, "574": [65, 120], "574050": 81, "574160": 88, "57436": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "574796": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "5748": [112, 121], "57496671": 63, "575": 11, "575381": 92, "575423": 16, "57572422": 99, "57585824": 99, "57592948e": 139, "57599221": 99, "575e": 97, "576": 65, "5763996": 63, "57643609": 99, "576879": 91, "577": 65, "5770": 92, "577024": 94, "577071": 81, "57715074": 63, "577210e": 82, "577271": 91, "57751232": 93, "577634": 81, "5776971": 99, "57775704": 99, "577807": [81, 82], "577884": 93, "577e": 97, "5780788": [124, 127], "578081": 93, "578307": 96, "578353": 70, "578389": 69, "578557": 82, "578726": 72, "578847e": 83, "579": 120, "579080": 76, "579125": 92, "57914935": 64, "579197": 88, "579213": 100, "579238": 83, "579322e": 92, "579521": 95, "579804e": 72, "57e": 64, "58": [31, 64, 76, 92, 100, 111, 139, 156, 172, 173], "5800": 93, "58000": 92, "580332": 69, "5804": [65, 93], "580414": 100, "580713": [124, 125], "580751": 92, "580831": 82, "580922": 85, "581675": 76, "581880": 90, "582031": 92, "582290": 82, "582331": 93, "58241568": 139, "582705e": 81, "582754": 98, "582761": 83, "582803": 81, "582991": 92, "583076": 93, "583140": 82, "583195": [81, 82], "5833333": 65, "583534": 96, "583666": 72, "583969": 81, "583982": 72, "584007": [124, 127], "584542": 71, "584742": 86, "584849": 83, "5852": 93, "585402": 77, "585412": 81, "585426": 139, "585479": 88, "585513": 82, "585793": 83, "58631757": [124, 127], "586362": 96, "5864": 62, "586423": 81, "586508e": 72, "5867": 93, "5868472": 63, "586878": [124, 129], "586962e": 82, "587135": 82, "587594": 93, "587796": [124, 127], "588": 171, "58812": [112, 121], "5882": 92, "588364": 24, "588396": 83, "5883964619856044": 83, "58859711": [124, 127], "588824": 81, "588844": 72, "589184": 76, "59": [69, 76, 82, 111, 124, 139, 156, 173], "590": 93, "590098": 83, "590320": [68, 72], "590467244372398": 76, "5905": 92, "590670": [124, 129], "590736": 96, "590738": 96, "590813": 96, "590880": 77, "590911": 83, "590991": 83, "591": 60, "591080": 68, "591646": 72, "591652": 92, "591678": 92, "591679": 82, "59172611": [124, 127], "59199423e": 139, "592093": 85, "592124": [124, 125], "592681e": 83, "59307502e": 139, "593162": 69, "593458": 72, "593637": 82, "593648": [112, 113, 116], "593981": 111, "594": 11, "594316e": 96, "595353": 83, "595987": 82, "596": 93, "596076e": 93, "596270": [85, 86], "596367": 81, "596588e": 82, "596889": 72, "597": 64, "597051": 111, "59769369": [124, 127], "597923": 93, "59798": 93, "598122": 72, "598200": 69, "598371": 93, "59854797": 124, "5985730": 64, "598663": 69, "598753": 81, "599297": [111, 112, 114, 124], "599586e": 90, "59978": 72, "59979": 72, "5cb31a99b9cc": 65, "5d": [83, 96], "5x_2": 68, "5x_3": 68, "5z_i": 96, "6": [11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 41, 42, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 104, 106, 108, 109, 111, 112, 113, 114, 116, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 168, 170, 171, 172], "60": [63, 66, 67, 77, 83, 93, 94, 96, 101, 111, 139, 156, 171], "600": 91, "6000": 93, "6000000000000002": [83, 93, 96], "600000e": 93, "600134e": 72, "60013741": [124, 129], "600254": 95, "600354": 82, "600445": 76, "600694": 124, "600776": 78, "601": 64, "601061": 83, "601109": 77, "601158": 76, "601598": 91, "601757": 93, "602079": 86, "602168": 83, "602322": 111, "602587": 96, "602628": 83, "602870": 81, "60337339": [124, 127], "604110": 93, "6043": 77, "604532": 93, "604841": [92, 93], "605": 93, "605093": 69, "605195": 86, "60542393": 70, "6059664": 70, "606034": 96, "606129": 96, "606342": 83, "606727": 81, "606733": 93, "6068": 63, "606800": 91, "606954": 83, "60712561": [124, 127], "60725365": 70, "60730524": 70, "6075": 173, "607618": 96, "607660": 69, "607666": 69, "608": [84, 97], "608177": 77, "60857": 62, "608818": 99, "60883043": [124, 127], "60884759": 70, "60885253": 70, "60885876": 70, "6088669": 70, "609": 97, "609575": [12, 124, 126], "609947": 77, "61": [24, 69, 78, 111, 139, 156, 172], "610053e": 82, "610093e": 81, "610318": 78, "610810": 72, "611": [60, 173], "611018e": 72, "611269": 91, "61170069": 97, "611859": 86, "611995": 93, "612": 97, "612025": 69, "612073": 82, "612246": 88, "612328e": 81, "61250258": [124, 127], "612792": 93, "6133": 64, "613391": 82, "613408": 96, "613498": 93, "61357": 124, "613574": 87, "613691": [12, 124, 126], "613950": 83, "614": 97, "614188": 91, "61448202": [124, 127], "61458483": 124, "614716": 82, "615": 78, "615011": 69, "615180": 76, "615863": [85, 86], "616086": 93, "616116": 93, "616346": 82, "616372": 92, "61669761": [157, 163], "616698": [157, 163], "616828": 93, "617277": 111, "6173": 65, "617424": 72, "617481": 93, "61771229": 97, "617780": 111, "6180": 77, "618069": 92, "61810738": 64, "618625": 81, "618722": 93, "618753": 93, "618924": 72, "619": [97, 120], "619294": 78, "619351": [81, 82], "619390": [81, 82], "619454": 68, "619613": 92, "61e": [64, 173], "62": [78, 86, 87, 111, 139, 156], "620": 94, "620021": 82, "620026": 96, "620135": 82, "620156": 96, "620267": 82, "620874e": 124, "620995": 101, "621060": 77, "621094": [92, 93], "621097e": 82, "621275": 95, "621359": 111, "621490": 96, "6215": 92, "621953": 96, "62195343": 96, "622": [93, 97], "622153": 93, "622272": 78, "6224": 63, "622502": 90, "622822": 70, "622996": 69, "623024": 83, "623107": 81, "6232": 77, "6236967": [124, 127], "624": 91, "6240": 99, "6241": 93, "624206": 85, "624320": 16, "6243811": 63, "624535": [112, 116], "624798": 92, "624894": 111, "624919": 93, "62496584": 72, "625": [63, 91], "625002": 93, "625159": 87, "625165": 95, "625477": 96, "62549184": 72, "625888": 77, "625891": [85, 86], "626192": [124, 125], "62638213": 72, "626433": 96, "626530e": 81, "626765": 93, "626866": 82, "627": 120, "627319": 90, "627505": [85, 86], "627560": 96, "627564": 83, "627874": 82, "62793471": 72, "628": 97, "628069": 91, "628213": 16, "628834": 93, "62916": 76, "629319": 93, "629835": 93, "629e": 97, "63": [63, 78, 91, 94, 111, 139, 156, 171, 172], "630124": 76, "630150e": 96, "630290": [124, 127], "630880": 97, "630914": 87, "631113": 82, "63117311": 124, "631179": 82, "631231": 70, "631333": 96, "63153104": 72, "631762": 82, "6318": [92, 173], "632058": 91, "63245862e": 139, "632747e": 96, "632904": 94, "6330631": 156, "633433": 91, "634": 97, "63407762": 173, "634078": [92, 173], "634587": 156, "6349": 77, "63499": 93, "635000e": [92, 93], "635199": [92, 93], "635757": 93, "635900": 81, "63593298": [101, 124], "636": 60, "636048": 124, "636453": 27, "636639": 76, "637108": 83, "637240": 93, "637326": 96, "6379": 92, "63817859": 96, "638179": 96, "638260": 72, "638264": 96, "638488": 87, "638778e": 72, "639": 92, "639135": 91, "63916605": 64, "639585": 82, "64": [78, 86, 92, 93, 94, 97, 111, 139, 156, 170], "640": 93, "640314": 82, "640432": 81, "641109": [124, 125], "64117907": [124, 127], "641355": 76, "641528": 96, "642": 97, "642016": 96, "642096": 95, "642329": 78, "64269": 99, "6427": 93, "642768": 82, "642984": 72, "64340": 99, "643512": 83, "643623": 96, "64362337": 96, "643752": 96, "643924": 70, "643939": 78, "643988": 81, "644113": 111, "644665": 83, "64476745e": 139, "644799": 68, "644963": 82, "645": 93, "645583": 78, "64579": 62, "6458": 63, "645800": 91, "645921": 72, "645988": 69, "646318": 76, "646419": 82, "646937": 68, "646963": 30, "647": 124, "647196": 68, "64723": 99, "647679": 82, "647689": 100, "647750": 93, "647864": 111, "647873": 96, "64797": 99, "647986": [124, 129], "648": 92, "648000e": 93, "648094": 81, "648472": [124, 129], "648596": 70, "648820e": 111, "649": [124, 171], "649158": 96, "649177": 77, "649335": 81, "64959174": 72, "649969": 93, "65": [75, 78, 83, 87, 93, 96, 97, 111, 139, 156], "650": 84, "6500000000000001": [83, 93, 96], "650046": 124, "6502": 93, "65042006": 72, "650867": 83, "650893": 82, "6515": 70, "651662": 93, "65177421": 72, "651919": 96, "652025": 71, "6522": 171, "652324": 78, "652350": 91, "652450e": [92, 93], "652526": 85, "6527": 84, "652778": 91, "652940": 82, "653008e": 92, "653055": 72, "653094": 81, "653424": 85, "653634": 69, "6536993": 139, "653829": 88, "65384092": 72, "653846": 83, "653900": 111, "653901": [81, 82], "654070e": 124, "654191": 77, "654285": 72, "65436972": 72, "654755": 68, "654964": 90, "655284": 96, "6553": 173, "6554": 171, "655547": 81, "65557405e": 139, "655750": 72, "655903": 72, "655959": 88, "656073": 72, "656939": 82, "657": [60, 65, 124], "657283": 99, "657360": 72, "657431": 76, "657723": 72, "658267": 96, "658345": 82, "6586": 62, "658612": [105, 109], "658769": 81, "659": 65, "659245": [81, 82], "659387": 71, "6593871": 62, "659423": [81, 82], "659473": 100, "659755": 97, "659799": [105, 109], "66": [78, 88, 94, 111, 139, 156, 170, 172], "660": [65, 124], "660128": 83, "660320": 86, "660360": 82, "660479": [15, 124, 126], "6607402": 97, "660776": 96, "661145": 93, "661166": 37, "66133": 124, "661338e": 93, "661521": 81, "661581": 72, "662149": 69, "662347": 82, "6625": 93, "66266982": 71, "66272028": 71, "66285102": 71, "662961": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "663115": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "663177": 78, "663182": 83, "663357": 82, "6634357241067574": 100, "663529": 96, "663533": 93, "663603": 69, "663672": 88, "663859": 81, "664108": [124, 129], "664310": 70, "66433357": 139, "664846": 91, "665187e": 139, "665264": 96, "665602": 90, "665868e": 93, "66592902": 72, "665932": 72, "665974": 90, "66601815": [124, 126], "666104": 96, "666220": 81, "666307": 68, "66633949": 139, "666530": 81, "6666667": 65, "666742": 111, "666750": 93, "666959e": 88, "667": 91, "66702367": 72, "667285944503316": 83, "667286": 83, "667536": 96, "66755728": 72, "667682": 67, "667867": 72, "667985": 87, "668375": 72, "668389e": 72, "668446": 87, "668584": 68, "669130": 90, "669562": 85, "669733": 82, "669919": 82, "67": [60, 65, 77, 92, 100, 111, 139, 156, 170], "670051e": 81, "670346": 94, "67074612": 72, "670867": 29, "67091005": 72, "671168": 111, "671271": [81, 82], "671382": 81, "671420": 90, "671633": 93, "671717": 82, "67199": 93, "6722": [65, 77], "672234": [81, 82], "672266": 81, "672384": [81, 82], "67245350": 63, "673092": [81, 82], "673302": 91, "6734628878523097": 83, "673463": 83, "673581815999206": 83, "673582": 83, "67410934": 63, "674181": 93, "674507e": 81, "6745349414": 63, "67456": 100, "674609": 83, "674822": 71, "675011": 83, "675011328023803": 83, "675451": 76, "675489": 81, "675528": 82, "676093e": 85, "67618978": 67, "676242": 82, "676405": 83, "6765": [64, 92], "676536": 156, "676714": 81, "67674909": 67, "676756": 96, "676807": 92, "676972": 70, "677614": 96, "677980": 83, "678": [60, 97], "6780": 139, "678104": 77, "678117": 93, "678540": 77, "678826": 83, "678953": 77, "67913613": [124, 129], "679251": 69, "67930427": 71, "67936506": [124, 126], "679486": 77, "679539": 91, "679748": 72, "67991699": 71, "67ad635a": 65, "68": [65, 78, 98, 99, 111, 139, 156], "680": 93, "68024298": 71, "680366": 93, "6807": 77, "680958": 72, "681": 91, "6810775": 99, "681176": 91, "681261": 81, "681279": 69, "681817dcfcda": 65, "68183347": [124, 127], "68194051": 69, "682315": 93, "682410": 69, "6826": [38, 92], "68265886": 69, "682830": 82, "68283807": 69, "683331": 83, "683456": 72, "683487": 82, "683582": 124, "683637e": 88, "683785": 82, "683826": 72, "683942": 96, "684": 173, "68410364": 64, "68411700": [64, 173], "684373": 69, "684502": 96, "684657": 82, "685066": 81, "685107": 96, "685143": 90, "685496": 69, "68554404e": 139, "68562150e": 139, "685807": 96, "685816": 81, "685970": 81, "685989": 124, "686684": 82, "686694": 82, "686698": 81, "687095": 72, "687227e": 82, "687345": 96, "687647": 96, "687854": 68, "687871": 91, "6878711": 63, "688": 171, "68847939": 69, "688537": 111, "688540": 111, "688886": 111, "688918": 93, "688919": 72, "689088": [81, 82], "689188": 68, "689392": 96, "6895081": 139, "689542": [105, 109], "68993577": 69, "689967": 82, "69": [87, 111, 139, 156, 172], "690111": 69, "690536": 93, "690585764": 139, "690743": 69, "69076574": 71, "69079935": 71, "69086528": 139, "690968": 81, "69102267": 71, "69140475e": 139, "691511": 92, "691761": 93, "6921": 77, "692269": 69, "692538": 82, "692725": 96, "692768": 81, "692907": 93, "6930": 77, "693345": 93, "693359": 93, "6934117220290754": 83, "693412": 83, "693495e": 93, "693796": 91, "693837": 77, "693874": 81, "69404895": [124, 129], "694154": 83, "694561": 97, "694755": 93, "694767": 71, "694919": 91, "69505403": 124, "69520523": 67, "6955": 93, "695581": 87, "695582": 90, "69562150e": 139, "695860": 81, "695890": 72, "696011": 28, "696289": [85, 86], "696644": 72, "696826e": 82, "69684828": 124, "697": 91, "697000": 83, "697118": 96, "697226": 69, "697420": [85, 86], "697591": 81, "697900": 82, "698013": 69, "698223": 68, "698244": 68, "698302": 81, "69840389e": 139, "698450": 16, "69850969": [124, 127], "698694": 91, "699035": 96, "699082": 83, "699097": 77, "69921": 65, "699259e": 96, "699333": 83, "699980": 72, "6_design_1a": 84, "6_r2d_0": 84, "6_r2y_0": 84, "6b": 156, "6cea": 65, "7": [12, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 40, 41, 44, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 106, 108, 109, 111, 112, 113, 114, 116, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170, 171, 172], "70": [64, 66, 83, 92, 93, 96, 111, 139, 156, 172], "700": [81, 82, 84, 91], "7000000000000002": [83, 93, 96], "700015": 96, "70007159": 96, "700072": 96, "700102": 96, "7001912": 139, "700314": 78, "700596": 96, "701088": 92, "701106": 87, "701343": 72, "701413": 93, "701491": 85, "70164893": 139, "701672e": 83, "701841e": 86, "702119e": 82, "702500": 93, "702632": 82, "703049": 81, "703325": 88, "7038474": 69, "703900": 72, "7040": 93, "704039": 82, "704482": 88, "7045": 88, "704558": 88, "704613": [124, 127], "704881": 82, "704896": 88, "705170": 81, "705519": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "705581": 93, "7055958": 102, "705595810371231": 102, "7055958103712310": 102, "705620": 81, "705682": 81, "705798": 82, "705801": 82, "70583": 99, "705981": 82, "706173": 82, "706208": 93, "706385": 81, "706645": 83, "707101": 83, "707868": 96, "70793174": [124, 127], "70793652": 69, "707963e": 92, "708003": [124, 129], "708062": 82, "708154": 82, "708190": 91, "708459": 96, "708532": 81, "708762": 82, "708821": 78, "708837": 78, "708934": 81, "708992": 72, "709026": 68, "709593": 81, "709606": 29, "709743": 72, "709790122": 139, "71": [111, 139, 156, 172], "710059": 78, "710626": 70, "711051": 81, "711089": 69, "711328": 93, "711518": 93, "711716": 69, "712038": 70, "712065": 85, "712072": 95, "712095": 78, "712130": [124, 127], "712209e": 82, "712503": 99, "712592": 92, "712631": 81, "712846": 111, "712921": 93, "712960": 83, "713": 93, "713006": 72, "713137": 82, "713582": 90, "713719": 81, "714240": 91, "714261": 82, "7142787": 96, "714279": 96, "714321": 92, "71441268": 69, "715013": 93, "715179e": 93, "715407": 83, "7155": 93, "715596": 81, "7157": 93, "715764": 16, "7158581": 63, "716": 93, "716098": 78, "7161": 93, "716100": 76, "71620729": 69, "716358": 72, "716456": 96, "7166": 72, "716615": 78, "716740": 111, "71675293": 69, "716762": 83, "716793": 83, "716799": 91, "7167991": 63, "716918": 72, "716920e": 139, "717185": 96, "717339": 69, "718294": 67, "718686": 100, "7193": 93, "71967912": 69, "719831": 82, "72": [66, 88, 111, 139, 156, 172], "72017804": [124, 129], "720208": 81, "720409": 81, "720447": 77, "720502": 72, "720571": 96, "720589": 97, "720642": 69, "720664": 91, "721071": 96, "721097": 16, "721118": 90, "721243": 81, "7215093d9089": 65, "72155839e": 139, "721604": 82, "721609": 93, "722": 91, "72205324": 69, "722316": 96, "7224": 93, "722634": 96, "722786": 94, "722848": 83, "722879": 70, "723": 65, "72330647": [124, 129], "723314": 96, "723342": 111, "723345e": 96, "723414e": 82, "723689": 93, "72369688": [124, 129], "723846": 78, "7239": 93, "723958": 81, "7241399": 63, "724338": 96, "724375e": 82, "724498": 77, "7247396": 69, "724767": [85, 86], "724816": 72, "724918": 100, "725": 65, "725010": 78, "725031": 96, "725087": 93, "725142": 81, "72515122": 139, "725166": 96, "725232": 111, "725477": 81, "725662": 76, "725992": 81, "726": [65, 97], "726078": 77, "72623389": 69, "72652658": 69, "7268131": 63, "7271188": [124, 129], "727501": 93, "727543": 68, "7276": 70, "727693": 93, "727780": 93, "727976": 83, "7282094": [124, 126], "728478": 82, "728710": 96, "72875815e": 139, "728e": 97, "729364": 70, "729365": 72, "729668": 111, "72993015": [124, 129], "73": [64, 75, 78, 111, 139, 156], "730011e": 81, "730629": 90, "730773": 81, "7308": 62, "730823": 81, "73091801": 69, "731212": 69, "731754": 83, "731928": 90, "732": 97, "732162": 82, "732307": 81, "7326": [69, 93], "732706": [124, 127], "732845": 81, "73285": 27, "732983": 81, "733404": 76, "734332": 81, "7344": 77, "734948": 96, "735166": 69, "735426": 81, "735587": 77, "735694": 96, "73569431": 96, "735713": 82, "73579164": 76, "735818": 69, "735848": 111, "735961": 70, "735964": 68, "73602094": [124, 129], "736082": [81, 82], "73649108": [124, 129], "736770e": 81, "737034": 72, "737210": 82, "737440": 69, "7375615": 64, "73764317e": 139, "73766741": [124, 129], "737796": 93, "737884": 25, "737934": 69, "737951": [81, 82], "737984": 72, "738147": 93, "738315": 93, "738422": 82, "738825": 72, "738936": 82, "739": 93, "739125e": 81, "739182": 81, "739261": 82, "7393": 77, "73949306": [124, 129], "739595": 97, "739660": 81, "739720": 93, "739817": 87, "74": [31, 64, 82, 92, 111, 139, 156, 172], "740": 91, "740180e": 96, "740200": 81, "740339": 77, "740417": 92, "7405": 93, "740505": 78, "7407627053044026": 83, "740763": 83, "740869": 83, "741043": 81, "741104": 83, "741255": [124, 125], "741380": 97, "741523": 78, "7416": 69, "741702": 96, "7418": 62, "74189": 65, "742128": 96, "742365": 95, "742407": 95, "742695": 81, "742907": 96, "742999": 72, "7432": 62, "7437": 93, "744": 120, "74402577": 96, "744026": 96, "744045e": 72, "744125": 82, "744211": 93, "744236": 99, "744327": 72, "74461783e": 139, "745353": 81, "745638": 92, "745886": 69, "746361": 96, "746843": 86, "747008": 81, "747023": 76, "747057": 77, "747164": 96, "747945": 63, "747977": 82, "748284": 90, "748377": 92, "748513": 93, "748728": 94, "748945": 93, "74938952": [124, 126], "749854893": 140, "75": [18, 29, 31, 65, 68, 78, 83, 88, 92, 93, 96, 111, 139, 156, 172], "75000": 100, "7500000000000002": [83, 93, 96], "750275": 93, "750300": 69, "750701": 78, "751013": 93, "751046": 71, "751081": 93, "751194": 77, "751281": 82, "751372": 81, "751633": 93, "75171": 92, "751710": [83, 92], "751712655588833": 102, "7517126555888330": 102, "751712656": 102, "752522": 81, "752696": 78, "752909": 88, "753136": 81, "7533": 92, "753516": 82, "753523": 96, "753880": 82, "754503": 81, "75466617": [124, 129], "754692": 111, "7548": 100, "754870": 91, "755063": 70, "7554": 69, "755721": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "755885": 111, "7560824": 63, "756200": 78, "756337": 81, "756539": 77, "756585519864526": 83, "756586": 83, "756802": 69, "756805": 91, "756867e": 93, "756969": 83, "757": [97, 171], "757136": 93, "757151": [81, 82], "757559": 88, "757596": 83, "757663": 85, "757690": 96, "757819": 91, "758027": 93, "758340": 77, "758754": 82, "758852": 82, "75887": 65, "75899873": [124, 129], "759043": 69, "759373": 82, "76": [111, 139, 156, 171, 172], "760104": 96, "760134": 72, "760155": 93, "76023347": [124, 129], "7603": 62, "760386": [12, 124, 126], "760403": 82, "760778": 91, "760868": 76, "760915": 68, "761": [63, 91], "761224": 78, "761317": 77, "761337": 72, "761714": 83, "761942": 69, "762049": 77, "762284": 96, "76228406": 96, "763": 60, "764093": [81, 82], "76419024e": 139, "764315": 96, "76444177e": 139, "764859": 67, "764953": 92, "765092": [124, 129], "765363": [81, 82], "765491": 71, "765500e": [92, 93], "765557": 81, "7656": 93, "765710e": 101, "765792": 96, "76591188": 63, "7660": 62, "766005": 96, "76608187": 70, "766499": 96, "766585": 93, "7669": 93, "766940": 78, "76702611e": 139, "767027": 77, "767188": [85, 86], "767435": 100, "767486": 82, "768273": [85, 86], "768798": 78, "768837": 94, "768874": 82, "769063": 93, "769361": 96, "769555e": 76, "769805": 96, "77": [97, 111, 139, 156], "770128": 70, "7704": 69, "770556": 93, "770751": 72, "77075678": 72, "770944": [85, 86], "7710": 99, "771167": 156, "7712": 77, "771275": 93, "771383e": 93, "7714": 94, "771463014326052": 76, "771529": 90, "7716982": 64, "771876": [72, 93], "771965": 93, "771986": 67, "772157": 88, "77227783e": 139, "772373": 81, "772444": 111, "772612": 96, "77261209": 96, "772791": 93, "77289874e": 139, "773": 65, "773177": 83, "7733": 70, "773327": 70, "773339": 88, "7733697": 139, "773716": 69, "77401500e": 139, "774212": [124, 129], "774365": 82, "774671": 94, "774683e": 91, "774915": 81, "775": [65, 93], "775191": [81, 82], "775969": 99, "7763": 92, "776601": 81, "776887": 92, "777297": 26, "7776071": 63, "777728": 24, "777867": 88, "777e": 97, "778305": 81, "7784": 70, "7786": 62, "779": [97, 124], "779068": 88, "779349": 81, "779517": [81, 82], "779682": 83, "779727e": 93, "78": [85, 97, 111, 139, 156, 172], "780": 65, "780458": 96, "780856": 92, "780943": [82, 96], "781076": 93, "781131": 72, "781225": 70, "781681": 96, "782": 65, "782147": [124, 129], "782159": 93, "78240482": [124, 129], "782643": 96, "783": [65, 93], "783183": 82, "783276": 124, "7833": 62, "7838": 62, "784": 156, "784235": 69, "784238": 91, "784405": 99, "78459606": 72, "784841": 93, "784872": 78, "784910": 82, "785": 65, "785107": 83, "785258": 71, "785805": 72, "785815": 78, "785911": 96, "785979": 90, "786": 65, "786191": 87, "786369e": 139, "786429": 81, "786684": 71, "786744": 83, "786986": 78, "78711285e": 139, "78721": [124, 125], "78777": 99, "787795": 16, "787838e": 81, "788": 171, "78818": 65, "788267": 90, "788385": 111, "78847235": 72, "788896": 82, "7889255": [124, 127], "788972": 81, "789": 124, "789355e": 99, "789699": 69, "78980657": [124, 127], "79": [78, 111, 139, 172], "790": 92, "790000e": 93, "790115": 93, "790142": 90, "790261": 111, "790655": 72, "790723": [85, 86], "790885": 82, "7910908091075142": 83, "791091": 83, "791097": 92, "791241": 96, "791297": 29, "792272": 94, "79228725": [124, 129], "792396": 78, "79251664": 72, "792939": 83, "792972": 111, "793297e": 93, "793315": 111, "79338596e": 139, "793570": 96, "793618919": 139, "793735": 96, "793818": [81, 82], "794": 124, "794119": 93, "794366": 93, "79458848e": 139, "794755": 93, "794805": 85, "795": 97, "795008": 82, "7952": 70, "795647": 96, "795828": 82, "795931": 72, "795932": [112, 118], "7963": 93, "796314": 72, "796340": 93, "796e": 97, "797078": [124, 129], "797119": 72, "797280": 96, "797323": 91, "797410": 81, "797454": 124, "797617": 16, "797743": 156, "79792890e": 139, "797966": 77, "797971": 156, "797984": 82, "798308": 92, "798388": 82, "798564": 71, "798685": 26, "798783": [85, 86], "798862e": 81, "799": 60, "799199": 72, "7992264": [124, 129], "799239": 72, "799403": 96, "799698": 81, "799890": 81, "7b428990": 65, "7x": 96, "8": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 52, 55, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 104, 109, 111, 112, 113, 114, 116, 117, 118, 123, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170, 172, 173], "80": [66, 67, 76, 77, 83, 93, 96, 101, 111, 139, 172], "800": 91, "8000": [36, 66, 101], "8000000000000002": [83, 93, 96], "800000e": 93, "8001": 70, "800668": 76, "801130": 93, "801269": 124, "802": 97, "802149": 82, "802293": 69, "802322": 72, "802554": 69, "802738": 111, "803297": 72, "803300": 81, "803492e": 96, "803779": 69, "803889e": 93, "803908e": 82, "803945e": 94, "804219": 96, "804284": 99, "804316": 96, "804330": 71, "804549": 93, "8048": 64, "804828": 96, "805007": 91, "805153e": [92, 93], "8055563": 63, "805714": 81, "805720": 96, "805789": 81, "8059": 92, "806167": 93, "806220e": 93, "806288969": 139, "806356": 93, "806519": 69, "806532e": 81, "806544": 69, "806732": 87, "806736e": 81, "80696592e": 139, "80714504e": 139, "807879": 96, "808": [64, 156], "808172": 72, "808246": 92, "808347": 67, "808524": 69, "808640": 93, "808860": 82, "809": 93, "80909808": 72, "809278": 67, "809392": 81, "8095": 94, "809913": [81, 82], "80a8": 65, "81": [63, 81, 84, 87, 111, 139, 172], "810044": 92, "810118": 76, "810134": 96, "8102": [62, 92], "810306": 78, "810382": [92, 93], "810564": 81, "810650": 96, "810833": 82, "810916": 93, "810960": 76, "811155": 87, "811255": 82, "81128199": 96, "811282": 96, "811329": 76, "811458": 92, "811672": 71, "8116912": 156, "811763": 81, "811825": 91, "811901": 96, "81190107": 96, "812": 97, "812003": 81, "812658": 82, "81278125": 72, "8132463": 63, "813293": 96, "813342": 156, "813682": 93, "814351": 83, "814717": 83, "814913": 91, "815214": 69, "815224": 156, "815226": [15, 124, 126], "815696": 71, "815707": 94, "815759": 72, "815993": 96, "8160": 93, "816131": 81, "816176": 100, "816296e": 81, "816318": 91, "816412": 71, "816417": 81, "816475": 72, "817342387": 139, "817967": 124, "817977": 77, "818029": 93, "81827267": 96, "818273": 96, "818289": 96, "81828926": 96, "818380": [81, 82], "81850929": [124, 129], "81856": 65, "818662": 69, "818761": 69, "818990e": 82, "819223": 111, "819519": [124, 127], "82": [77, 100, 111, 139, 172], "8202": 64, "820366": 91, "8208": 93, "8209": 64, "820963": 78, "820993": 93, "821": 171, "8210": 64, "821009": 93, "821021": 83, "821398": 69, "821566": 96, "8221": 62, "822289": [92, 173], "82228913": 173, "822482": 83, "8226": 77, "8227": 93, "822822": 83, "823247": 96, "823273": [81, 82], "82329138": 75, "823666": 82, "82372418": [124, 127], "824140": 72, "824350": [81, 82], "824701": 83, "824750": 83, "824889": 83, "824961e": 93, "8250": 62, "825617": 91, "825749": 82, "8260": 92, "826065": [81, 82], "826074e": 91, "826366": 76, "826426": [15, 124, 126], "826492": 96, "826519": 29, "82666866e": 139, "82684324": 99, "826897": 96, "827093": 81, "827381": 96, "827414": 77, "827735": 96, "827874854703913": 83, "827875": 83, "827938162750831": [85, 86], "828": 60, "828157": 78, "828437": 69, "828618": 88, "828915": [85, 86], "829038": 37, "829543": 83, "829764": 111, "83": [76, 111, 139, 172], "830089": 69, "830102": 81, "830120": 82, "830142": 89, "830273": 78, "830301": 95, "830850": 81, "830967": 81, "831": 97, "831019": 83, "831167e": 81, "831479": 81, "831535": 72, "832086": 96, "83211823": 69, "832146": 71, "8325": 124, "8326928": 99, "832693": 99, "832875": 96, "83287529": 96, "832965": 93, "832991": 16, "833": 120, "833018": 93, "833024": 91, "833065": 78, "833096": 93, "833227e": [112, 113, 116], "833510": [124, 125], "833563": 90, "833907": 91, "834133": 88, "835": 97, "8350": [77, 93], "835344": 78, "835750": 88, "835839": 93, "836234": 124, "837022": 76, "837366e": 82, "837699": 82, "837934": 69, "838114": 96, "838235": 94, "838322": 81, "83854057": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "838883": 77, "839397e": 72, "84": [65, 87, 97, 111, 124, 139, 172], "840041": 93, "84014899": [124, 127], "840303": 96, "84030318": 96, "840718": [12, 124, 126], "840836": 96, "840995e": 92, "841": [63, 91, 94], "841132": 92, "8415": 64, "841712": 93, "841847": 93, "841920": 82, "842205": 93, "842262": 124, "842405": 83, "842625": 91, "842746": 96, "8428": 92, "842853": 96, "8429": 77, "843": 93, "843018": 111, "843296": 93, "843454": 81, "843730": 91, "844026": 111, "844308": 96, "8445": 75, "844549": [85, 86], "844663": 78, "844673": 156, "844707": 96, "844893": 91, "8452": 77, "845241": 97, "845525": 81, "84609957": [124, 129], "8461": 77, "846602": 93, "846707": 93, "847136": 83, "847372": 72, "847595": 28, "847948": 83, "84813353": [124, 127], "848578": 93, "848734": 81, "848757e": 92, "848868": 83, "848997": 93, "84930915e": 139, "849427": 111, "84945993": 139, "849747": 99, "849766": 93, "8497f641": 65, "8498": 93, "85": [34, 75, 83, 87, 93, 96, 101, 111, 139], "8500000000000002": [83, 93, 96], "850038": 78, "850101": 82, "850242": [124, 127], "850321": 91, "850575": [81, 82], "850656": 88, "851": 171, "851012": 90, "851100": 78, "8513": 65, "851366": 91, "852016": 96, "852592e": 90, "852916e": 82, "85397773": [124, 126], "85402594": 75, "854566": 72, "85460638": 139, "854713": 71, "855": 93, "855242": 81, "855780": 96, "856037": 82, "856404": 86, "856552": 78, "856758": 111, "8571": 62, "857161": 96, "857515": 111, "857544": 91, "858577": 95, "858635": 90, "858952": 78, "859": [60, 93], "85911521e": 139, "85912862": 156, "859129": [39, 140, 156], "8597": 92, "859906": 69, "85c5": 65, "85e": 64, "86": [76, 111, 139, 172], "860256e": 72, "860378": 93, "860663": 156, "860804": 96, "861019": 78, "861222": 69, "862043": [85, 86], "862274": 85, "862359": 83, "862564": 72, "862929": 69, "863337": 81, "86343": 76, "863430": 76, "863772": 92, "863949": 70, "863982270": 140, "864": [94, 97], "86415573": 64, "86424193e": 139, "864388": 71, "8644": 65, "864404": 93, "8646": 70, "864664": 78, "8648": 77, "865101": 77, "865313": 93, "865442": 90, "865562": [81, 82], "865860": [92, 93], "866102": [81, 82], "86610467": 96, "866105": 96, "866142": 82, "866179899731091": 102, "866179900": 102, "866579": 93, "866798": 93, "867187": 76, "867565": 96, "867775": 82, "8679": 93, "868": 65, "868082": 69, "8681": 75, "869": [65, 97], "869020": 83, "869301": 111, "869427": 38, "869586": 87, "869617": 111, "86975793": [124, 127], "869778": 95, "869787": [124, 129], "869912e": 82, "87": [64, 81, 87, 91, 111, 139, 172], "870": [71, 94], "8700": 64, "870099": [85, 86], "870187": 82, "870260": 96, "870288": 81, "870332": 96, "870444": 92, "870917": 72, "870943": 82, "871": 65, "871080": 77, "871972": 88, "872": 97, "872354": 96, "872768": 96, "872852": 96, "87290240e": 139, "873039": 82, "873198": 93, "873677": [85, 86], "873848": 71, "87384812361": 62, "87384812362": 62, "87430335": 156, "874303353": 156, "874702": [85, 86], "874721": 81, "87484243": [124, 127], "874926": 72, "8751": 93, "8759": 93, "876": 97, "876233": 71, "87623301": 62, "87640238": [124, 127], "876431e": 83, "87674597e": 139, "8768": 62, "8771": 93, "877153": 93, "877287": 16, "87730126": 139, "877446": 111, "877455": 95, "877833": [81, 82], "877903": 78, "878122": 90, "878281": 96, "878402": 81, "878625": 72, "878746": 78, "878802": 93, "878895": 78, "878914": 81, "879103": 83, "87e": 64, "88": [41, 64, 76, 87, 97, 111, 139], "880106": 91, "880229": 71, "880579": 96, "880591": 95, "880886": 92, "8810": 92, "881201": 93, "88125046e": 139, "881254": [124, 129], "881353": 72, "881465": 68, "881567": 82, "88173062": 63, "88173585": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "881937": 78, "882": 120, "882470": 16, "882641": 92, "882761": 112, "882822": 117, "882886": 69, "882896": 96, "882928": 78, "883224": 82, "883301": 83, "883461": 71, "883622": 96, "883914": 83, "88415932": [124, 127], "8843": 99, "8845": 62, "884821": 111, "884996": 83, "8850": 64, "885059": 69, "885065": 96, "885513": 93, "885530": 69, "885686e": 81, "885832": 97, "885978": [85, 86], "886086": [81, 82], "88629": 62, "886300": 81, "8865311": 139, "886611": 93, "88664": 65, "886764": 82, "8869": 70, "887212": 93, "887270": 82, "887556": 83, "887729": 69, "887731": 67, "887778": 93, "888146": 91, "8881461": 63, "888153": 72, "888217": 93, "888348": 82, "888352": 78, "888423": 93, "888757": 96, "888775": 86, "889261": 81, "889293": 96, "889339": 69, "889566": 99, "889638": 78, "889733": 96, "88988263e": 139, "889913": [81, 82], "88ad": 65, "89": [64, 82, 111, 139, 171, 172], "890": [63, 91], "89027368": 156, "890273683": 156, "890342": 81, "89035917": 87, "890372": [73, 104, 109, 170], "8903720000100010000010": [65, 104, 109, 170], "8904": 60, "890454": [112, 118], "890548": 76, "89058730": 139, "890665": 88, "890855": 78, "8909": [63, 92, 173], "891102": 82, "891299": 77, "891547": 76, "891593": 81, "891606": 92, "891649": 25, "891669": 69, "891757": 82, "892": 65, "892648": 96, "892796": [81, 82], "892828": 88, "893": [60, 65], "8932105": 63, "893288": 70, "893357": 69, "89338403": 69, "893461": 78, "893521": 82, "893576": 69, "893649": [81, 82], "893851": 96, "893884": 93, "894": 65, "894329e": 93, "89460366": [124, 129], "8946549": 66, "894701": 82, "894951": 72, "895106": [81, 82], "895275": 16, "895308": 93, "895333": 96, "895442": 92, "895690": [81, 82], "895717": 94, "895768e": 83, "895853e": 81, "89588295": 69, "896023": 96, "896064": 72, "896182e": 88, "896263": 88, "896333": 82, "896761": 70, "896820e": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "897115": 81, "897208": [124, 125], "897220": 96, "8974": 92, "8979": 77, "89814202": [124, 129], "898183": 78, "898501": 69, "898722": 96, "899021": 111, "899250": 78, "899460": 96, "899568": 81, "8996": 75, "899654": 78, "8bdee1a1d83d": 65, "8da924c": 65, "8e": 139, "8e3aa840": 65, "9": [12, 14, 15, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 104, 109, 111, 112, 113, 114, 116, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 169, 170, 172, 173], "90": [42, 64, 66, 67, 83, 93, 96, 101, 111, 139, 172], "9000000000000002": [83, 93, 96], "900021": [112, 118], "900387": 72, "900829": 88, "901013": 82, "901148": 96, "90136": 92, "901360": 92, "901526": 87, "901563": 16, "901797": [124, 125], "901998": 90, "902": 156, "902219": 81, "90222834": 69, "902472": 70, "902573": 83, "902920": 111, "903056e": 96, "903339": 83, "903366e": 83, "903418": 91, "903681": 96, "903731": 90, "903767": [81, 82], "903879": 82, "904356": 72, "9045": 93, "904900": 81, "904976": 81, "905494": 83, "905612": 77, "905858": 100, "905951": 99, "906051": 83, "906051023766621": 83, "9067": 93, "906716732639898": [85, 86], "906757": 79, "90678968": [124, 127], "907115": 96, "907176": 96, "907491": 83, "907801": 91, "907879": 78, "90794478": 156, "907944783": 156, "908024": 100, "908912": 82, "90918607": 69, "909304": [81, 82], "909571": 78, "90963122e": 139, "90971778": 69, "909919": 69, "909942e": 24, "909997": [92, 173], "91": [76, 98, 111, 139, 172], "910899": 70, "9109": 65, "91102953": 96, "911030": 96, "9112": 92, "91148978": 69, "911794e": 82, "911818": 82, "912033": 30, "91205214": 69, "9121924252731333538404347596467737693100": 139, "912230": [81, 82], "9126": [64, 173], "912663": 72, "9127": [64, 173], "912801": 82, "912837": 81, "913": 65, "91315015": 63, "913280": 100, "913585": 88, "913774": 83, "9142": 93, "9143065172164": 76, "91438767e": 139, "9145": 62, "914598": 78, "915": [64, 65, 92, 93, 94], "915000e": [92, 93], "915488": [85, 86], "915502": 82, "9156": 71, "916171": 95, "9162": 77, "916236": 62, "916355": 82, "916359": 78, "9166667": 65, "916913": 96, "916914": 96, "916921": 93, "916984": 96, "917": 65, "917038": 77, "917497": 85, "91771387": 96, "917714": 96, "9179": 93, "918": 97, "918044": 81, "918199e": 81, "918227": 83, "918542": 69, "919": 173, "919873": 69, "919879": 93, "91e": 64, "92": [69, 98, 111, 112, 121, 139, 172], "920283": 93, "920337": 86, "920661": 76, "920695": 93, "9209": [62, 77], "9210": 93, "921061": 100, "921198": 88, "921372": 83, "921692": 72, "921778": 88, "921913": 91, "921956": [81, 82], "921e4f0d": 65, "922232": 81, "92248736": [124, 129], "922668": 88, "922996": 91, "923033": 82, "923084e": 83, "923517": 101, "923607": 96, "92369755": 63, "923943": 173, "924002": 96, "924007": 71, "924061": 81, "924232e": 90, "9243": 93, "924396": [85, 86], "924443": 78, "924540": 90, "924634": 68, "924724": 83, "924732": 93, "9248": 65, "924821": 83, "925248": [85, 86], "925591": 69, "92566": 124, "925689": 72, "925736": 83, "925994": 92, "926260": 85, "92633461": 69, "926621": 83, "926626": [124, 127], "926685": 91, "927": 61, "927074": 96, "927232": 93, "9274": 93, "927501": 82, "927563": 82, "927956": 76, "928269": 93, "92827999": 124, "92877119": 69, "92881435e": 139, "9289008": [124, 127], "929031": 81, "92905": 63, "929374": 72, "929607": 93, "929636": 76, "929699": 82, "92972925e": 156, "929729e": [39, 140, 156], "929840": 69, "93": [64, 76, 88, 98, 111, 112, 120, 123, 139, 172], "930357": 76, "9304028": 63, "930497": 76, "931": 103, "931479": 96, "931646": 81, "931978": 170, "931988": 71, "932007": 71, "932027": 83, "932091": 70, "9321": 70, "932233": 77, "932327": 81, "932404e": 93, "9325": 62, "932571": 71, "932651": 76, "9327": 62, "932906": 82, "932973": 96, "93300": 124, "933514": 72, "933603": 82, "933677": 71, "933996": 83, "934122": 82, "934270": 91, "934433": [81, 82], "9345": 65, "934511": 156, "93458": 99, "934634": 82, "934992": 83, "935": 75, "935286": 72, "935591": 96, "935730": 96, "935989": 91, "9359891": 63, "935993": 82, "936048": 71, "936094": 81, "936124": 76, "936214": 72, "936238": 82, "93648": 101, "9365": 77, "936739": 96, "936827": 69, "937586": 93, "937967": 77, "938": 156, "938460": 81, "9386744462704798": 90, "9388": 93, "939068": [85, 86], "939390": 81, "9395": 93, "93958082416": 173, "93965493": [124, 129], "94": [75, 76, 111, 139, 172, 173], "94002553": 69, "940406": 72, "940693": [124, 125], "940758": 72, "940786": 72, "941047": 16, "941364": 77, "94140672": 69, "941440": 81, "941709": 83, "9417090334740552": 83, "942": 120, "942139": 86, "942312": 96, "942460e": 96, "9425": 62, "942550": 93, "942629": 93, "942661": 91, "942777": [124, 127], "94309994e": 139, "943548": 88, "94359066": 69, "94375039": 69, "944045e": 96, "94419857": 69, "944253e": 96, "944266": [69, 85, 86], "944280": 93, "944317": 93, "94441007e": 139, "944630": 96, "94473": 66, "94478833": 69, "944825": 69, "944959": 70, "945058": 81, "946180": 88, "94629": 101, "946297": 83, "946406": 86, "946433": 96, "946968": 83, "947194": 81, "947290": 82, "947298": 72, "947391": 76, "947440": 95, "947466": [112, 113, 116], "947855": 78, "9480": 93, "948112": 97, "948233": 71, "948644": 76, "9487425": 139, "948828": 81, "948975": 87, "94906344": 63, "949241": 156, "949370": 72, "949376": 72, "949456": 96, "949459e": 81, "94954135": 69, "949672": 72, "9497455": [124, 127], "949866": 82, "95": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 51, 62, 64, 66, 67, 68, 69, 70, 71, 72, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 92, 93, 96, 97, 99, 100, 101, 111, 124, 139, 156, 157, 163, 172, 173], "9500": 93, "950280": 96, "950545": 79, "95062986e": 139, "951": 120, "951532": 91, "951604": 82, "951920": 95, "952": [64, 97, 173], "9523": 62, "952572": 77, "952839": 96, "95305": 66, "9532": 93, "953683": 91, "953706": 81, "95372559e": 139, "953884": 124, "954": 156, "95401167e": 139, "954738": 82, "9552": 62, "955455": 71, "955541": 29, "955577": 69, "95559917": [112, 113], "955689": 81, "955928": 71, "956047": 63, "9561": 62, "956270": 72, "956272": 91, "95656893": 69, "956574": 93, "956588": 124, "956762": [124, 129], "956992": 71, "957229": 86, "957339": 91, "957375": 91, "957417": 93, "957479": 83, "9574793755564219": 83, "957745": 83, "9579": 64, "957996": 83, "958": 156, "9580": 64, "95807659": 69, "958636": 87, "95865935": 69, "959441": 82, "959613": 111, "95e": 64, "96": [64, 67, 81, 82, 94, 111, 139, 172], "960049": 78, "960236": 124, "96048672": 69, "960508": 72, "960808": 83, "960846": 93, "9609": 62, "961018": 82, "961538": 93, "961962": 83, "962": 94, "962022": 82, "962041": 82, "962228": 82, "962364": 78, "9625": 93, "962948": 77, "963340": 70, "964025e": 96, "964261e": 91, "964317": 93, "9647": 62, "964914": 16, "964940": 82, "965": 67, "965210": 70, "965376": 81, "965531": 124, "96582": [112, 118], "966": 60, "966015": 96, "966413": 69, "966566": 82, "966723": 69, "967467": 99, "968127": 78, "968134e": 96, "968288": 93, "968305e": 81, "96896106": [124, 129], "969141": [111, 112, 114, 124], "969702563412915": 83, "969703": 83, "9699": 92, "96e": 139, "97": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 111, 112, 113, 114, 116, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 170, 172, 173], "970065": 96, "970652": 82, "971058": [85, 86], "971132": 93, "971563": 71, "971617": 76, "972051": 93, "972405": 81, "97314470": 63, "97318225": 69, "973241": 96, "973293": 82, "973874": 83, "974023": 81, "974104": 82, "974202": 83, "974213": 82, "97436783": [124, 129], "97441062": [85, 86], "974414": 83, "974416": 82, "974491": 77, "97470872": 99, "97483665": 139, "9748910611": 63, "974925": 82, "97499195": 96, "974992": 96, "975": [75, 81, 82, 85, 86, 88, 94, 124, 127], "975289": 78, "9753": 65, "976": 94, "976088": 96, "976398": 82, "976562": 96, "976706": 72, "976791e": 82, "976905": 69, "977113": 82, "977280": [81, 82], "977295": 93, "9773": 93, "977489": 93, "978000e": 93, "978052": 82, "978438e": 82, "978446": 81, "9787": 93, "978977": 96, "979": [97, 173], "979135": 111, "979384": 88, "979409e": 82, "979507": 69, "979966": 88, "98": [81, 82, 111, 139, 172], "980": 94, "9802393": 63, "980407": 82, "980574": 81, "980629": 71, "980643e": 83, "980771": 81, "980838": 81, "981104": 95, "981339": 82, "981383": 69, "98155798": 69, "981622": 81, "981672": 83, "981703": 81, "982353e": 93, "982392": [124, 127], "982417": 83, "982456": 76, "982815": 93, "982948": 81, "98296754": 69, "983192": 96, "983389": 81, "983482": 93, "983483": 78, "983759": 173, "98393441": 99, "984": 94, "984083": [85, 86], "984368": 77, "984562": 96, "984786": 72, "984866": 156, "984872": [81, 82], "984937": 83, "98505871e": 139, "985207": [81, 82], "985279": 93, "985569": 93, "985971": 82, "986249": 92, "986674": 69, "986683": 81, "98673": 87, "986999": 69, "9870004": 65, "987175": 77, "9875": 62, "98750578": 67, "9880384": 65, "988081": 81, "988421": [81, 82], "988463": 96, "988935": 72, "989011": 69, "989117": 72, "989364": [124, 127], "989379": 72, "989605": 94, "989914": 94, "99": [55, 64, 76, 78, 81, 82, 93, 94, 111, 139, 172], "990210": 93, "990567e": 93, "990838": 81, "9909": 72, "991": 65, "991040": 71, "991395": 71, "9914": [92, 93, 99], "9915": [64, 92, 93, 99], "991512": 64, "991824": 81, "991915": 71, "991963": [81, 82], "991977": 93, "992": 97, "992288": 81, "99232145": 99, "992359": 16, "992541": [124, 125], "992582": [81, 82], "992794": 71, "993416": 88, "993490": 69, "994": [94, 97], "994168239": 63, "994208": 78, "994304": 93, "994315": 69, "994332": 79, "9944": [15, 124, 126], "994588": 72, "994615": 77, "9948104": 66, "994864": 93, "994936": 72, "994937": 86, "9951": 62, "995248": 96, "99549118e": 139, "99571372e": 139, "995729": 72, "9960": 72, "9961": 92, "9961392": 63, "996350": 72, "996413": 93, "996729": 81, "996934": 91, "9970": 93, "997034": 101, "997038": 82, "997494": 101, "997571": 91, "997591": 72, "997621": 83, "997934": [85, 86], "998": 94, "998050": 95, "998063": 79, "9981": 77, "9986": 72, "99864670889": 173, "99883095": [124, 129], "998855": 93, "999": [55, 68, 77, 87, 89, 99, 173], "999120": 95, "999207": 96, "9995": [68, 81, 82], "999596": 37, "9996": [68, 81, 82], "9996553": 64, "9997": [68, 81, 82], "9998": [68, 81, 82], "9999": [68, 81, 82], "99c8": 65, "A": [10, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 34, 35, 37, 38, 39, 40, 45, 46, 48, 49, 51, 53, 54, 60, 61, 62, 64, 65, 69, 70, 71, 72, 75, 77, 79, 80, 84, 90, 94, 95, 97, 98, 99, 100, 103, 104, 109, 111, 112, 113, 116, 117, 119, 122, 124, 125, 127, 129, 138, 156, 157, 158, 164, 165, 166, 167, 168, 170, 171, 173], "ATE": [26, 30, 31, 64, 73, 77, 78, 88, 92, 99, 100, 111, 124, 134, 138, 140, 148, 157, 165], "ATEs": [76, 77, 78, 94], "And": [66, 94, 101, 157, 159, 160], "As": [61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 75, 76, 77, 78, 80, 81, 82, 83, 84, 87, 88, 90, 91, 92, 93, 94, 96, 100, 101, 102, 112, 122, 123, 124, 127, 128, 129, 139, 140, 142, 144, 146, 156, 157, 158, 167, 173], "At": [18, 31, 40, 63, 67, 68, 70, 75, 76, 78, 87, 91, 93, 96, 173], "Being": 173, "But": [75, 88], "By": [62, 63, 91, 97, 100, 112, 123, 124, 127, 129, 157, 163], "For": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 49, 54, 60, 62, 63, 65, 67, 69, 70, 71, 72, 75, 76, 77, 78, 79, 87, 88, 90, 91, 93, 95, 97, 99, 100, 102, 103, 104, 105, 109, 111, 112, 113, 120, 123, 124, 125, 126, 127, 128, 129, 130, 131, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 167, 168, 169, 170, 173], "ITE": [35, 77, 78], "ITEs": [76, 77, 78], "If": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 53, 54, 55, 56, 61, 63, 67, 69, 70, 71, 72, 75, 76, 77, 80, 81, 82, 88, 91, 93, 97, 103, 104, 109, 111, 112, 123, 124, 126, 132, 140, 141, 142, 143, 144, 145, 148, 156, 157, 158, 159, 160, 161, 162, 163, 166, 167, 168, 173], "In": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 53, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 139, 140, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173], "It": [13, 62, 63, 64, 77, 81, 82, 84, 85, 86, 91, 92, 93, 97, 98, 100, 105, 108, 109, 112, 119, 121, 124, 125, 139, 168, 172], "No": [33, 56, 60, 62, 64, 65, 66, 67, 69, 70, 71, 72, 73, 76, 77, 78, 87, 89, 92, 93, 97, 99, 101, 104, 106, 107, 108, 109, 112, 113, 124, 126, 127, 129, 140, 156, 170, 171], "Not": [124, 130], "Of": [75, 156, 173], "On": [61, 76, 80, 90, 94, 98, 103, 171], "One": [41, 64, 69, 72, 92, 93, 100, 111, 156], "Or": 46, "Such": [100, 112], "That": [46, 173], "The": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 124, 125, 126, 128, 130, 133, 135, 136, 137, 138, 139, 140, 145, 148, 153, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 167, 168, 169, 171, 172, 173], "Then": [18, 83, 96, 98, 124, 156, 157, 167, 168, 169], "There": [64, 92, 100, 124, 130, 169, 173], "These": [25, 64, 65, 70, 74, 76, 90, 92, 95, 97, 99, 111, 124, 173], "To": [12, 14, 15, 16, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 61, 63, 64, 65, 66, 67, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 88, 91, 92, 93, 95, 96, 97, 98, 99, 100, 101, 103, 104, 109, 111, 112, 113, 114, 116, 117, 118, 119, 121, 123, 124, 128, 139, 156, 157, 158, 163, 167, 168, 169, 170, 173], "Will": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30], "With": [34, 81, 82, 112, 114, 171], "_": [9, 12, 14, 15, 17, 19, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 61, 63, 67, 68, 69, 71, 72, 80, 81, 82, 83, 84, 85, 86, 91, 92, 93, 95, 96, 97, 98, 102, 103, 108, 109, 111, 124, 127, 130, 139, 140, 142, 156, 157, 158, 163], "_0": [61, 63, 80, 84, 91, 102, 103, 139, 140, 154, 155, 156, 157, 167], "_1": [17, 18, 19, 31, 35, 40, 66, 94, 101, 140, 154, 155], "_2": [17, 18, 19, 31, 35, 40, 94], "_3": [17, 18, 19, 31, 35, 40], "_4": [17, 18, 19, 31, 35, 40], "_5": [31, 35], "__": [53, 54], "__init__": [90, 98], "__version__": 169, "_a": [140, 142, 144], "_all_coef": 139, "_all_s": 139, "_b": [140, 142, 144], "_compute_scor": 21, "_compute_score_deriv": 21, "_coordinate_desc": 91, "_d": [77, 97, 124], "_est_causal_pars_and_s": 172, "_estimator_typ": 90, "_h": [97, 124], "_i": [61, 66, 80, 96, 101, 103, 124, 130], "_id": 139, "_j": [17, 18, 19, 31, 35, 40, 43, 63, 91, 156], "_l": [112, 121, 123], "_lower_quantil": [69, 72], "_m": [112, 121, 123, 139], "_mean": [69, 72], "_n": [140, 141, 142, 143, 144, 148, 156, 157, 163, 166], "_n_folds_per_clust": 91, "_offset": [112, 123], "_pred": [112, 123], "_rmse": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "_upper_quantil": [69, 72], "_x": 47, "_y": [97, 124], "a0": 90, "a09a": 65, "a09b": 65, "a1": 90, "a3d9": 65, "a4a147": 94, "a5e6": 65, "a5e7": 65, "a6ba": 65, "a79359d2da46": 65, "a840": 65, "a_": [66, 101], "a_0": [41, 44, 140, 151], "a_1": 44, "a_j": [124, 131], "ab": [62, 98, 168], "ab71": 65, "abadi": [10, 67], "abb0fd28": 65, "abdt": [73, 104, 109, 170], "abl": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 61, 75, 80, 93, 94, 112, 115, 157, 158, 167], "about": [14, 16, 64, 67, 68, 75, 77, 92, 98, 124, 168, 170, 173], "abov": [61, 64, 70, 75, 78, 80, 81, 82, 85, 86, 90, 92, 94, 95, 96, 97, 100, 103, 111, 112, 122, 124, 128, 130, 169], "absolut": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 75, 112, 113], "abstract": [21, 62, 63, 91, 140, 168, 172], "abus": [69, 72, 124, 127, 129, 130], "acc": [23, 62], "acceler": 77, "accept": [17, 19, 105, 109, 111, 112, 116, 121], "access": [48, 49, 62, 64, 69, 70, 71, 72, 75, 76, 77, 85, 86, 87, 99, 107, 109, 112, 121, 157, 163, 173], "accompani": 168, "accord": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 64, 66, 67, 70, 76, 78, 80, 83, 92, 96, 97, 100, 101, 112, 123, 124, 125, 130, 156, 157, 159, 160, 161, 162, 164, 165, 173], "accordingli": [66, 67, 75, 90, 92, 97, 101], "account": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 63, 64, 69, 72, 91, 92, 93, 99, 100, 157, 163, 167, 173], "accross": 89, "accumul": [64, 92, 93, 99], "accur": 77, "accuraci": [48, 53, 62, 124], "acemoglu": 171, "achiev": [50, 63, 77, 88, 91, 95, 100, 124, 156], "acic_2024_post": 94, "acknowledg": [64, 65, 92], "acm": 171, "acov": 171, "across": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 64, 76, 77, 92, 94, 173], "action": 172, "activ": [4, 5, 6, 7, 8, 9, 169, 172], "actual": [46, 69, 72, 76, 87, 100], "acycl": [66, 101, 173], "ad": [5, 6, 7, 8, 9, 10, 11, 21, 48, 49, 53, 54, 87, 104, 109, 112, 114, 124, 156, 157, 158, 172], "adapt": [25, 89, 92, 172], "add": [62, 63, 66, 67, 68, 73, 76, 77, 78, 85, 86, 87, 94, 96, 97, 99, 100, 101, 112, 115, 119, 124, 171, 172], "add_trac": 100, "addit": [12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 43, 44, 45, 47, 51, 69, 70, 71, 72, 84, 89, 100, 105, 106, 107, 108, 109, 112, 115, 124, 125, 140, 149, 157, 164, 165, 167, 171, 172], "addition": [31, 40, 72, 76, 78, 83, 93, 99, 112, 115, 124, 139, 156, 157, 163, 170], "additional_inform": 13, "additional_paramet": 13, "address": 100, "adel": 171, "adj": [97, 100], "adj_coef_bench": 100, "adj_est": 100, "adj_vanderweelearah": 100, "adjust": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 52, 55, 63, 68, 88, 89, 91, 93, 99, 100, 111, 124, 130, 156, 157, 163, 171, 172, 173], "adjust_p": 55, "adopt": [67, 124, 126, 130], "advanc": [90, 110, 139, 171], "advantag": [61, 62, 64, 78, 80, 92, 93, 103, 169], "advers": [157, 158], "adversari": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 99, 157, 163, 167], "ae": [61, 63, 64, 66], "ae56": 65, "ae89": 65, "aesthet": 61, "aeturrel": 45, "afd9e4": 94, "affect": [77, 78, 84, 124, 172, 173], "after": [55, 56, 62, 64, 65, 66, 67, 84, 92, 93, 100, 101, 111, 112, 116, 119, 121, 124, 130, 157, 159, 163, 169, 173], "after_stat": 61, "afterward": 76, "ag": [64, 92, 93, 95, 99, 173], "again": [61, 62, 63, 64, 66, 67, 69, 72, 76, 78, 80, 87, 90, 91, 92, 97, 98, 99, 100, 101, 103, 157, 159, 160], "against": [67, 75, 77, 87, 95, 112, 114], "agebra": 111, "agegt54": [65, 73, 104, 109, 170], "agelt35": [65, 73, 104, 109, 170], "agg": [62, 69, 72, 98], "agg_df": [69, 72], "agg_df_anticip": [69, 72], "agg_dict": [69, 72], "agg_dictionari": [69, 72], "agg_did_obj": [124, 125], "aggrag": [124, 125], "aggreg": [13, 16, 62, 102, 125, 139, 172], "aggregate_over_split": 46, "aggregated_eventstudi": [69, 70, 71, 72], "aggregated_framework": [69, 70, 71, 72, 124, 125], "aggregated_group": [69, 72], "aggregated_tim": [69, 71, 72], "aggregation_0": 13, "aggregation_1": 13, "aggregation_color_idx": 13, "aggregation_method_nam": 13, "aggregation_nam": 13, "aggregation_weight": [13, 69, 70, 71, 72], "aggt": 62, "ai": [71, 98, 171], "aim": 97, "aipw": 94, "aipw_est_1": 94, "aipw_est_2": 94, "aipw_obj_1": 94, "aipw_obj_2": 94, "air": [63, 91], "al": [10, 11, 32, 34, 41, 43, 44, 61, 63, 64, 65, 67, 75, 80, 81, 82, 83, 84, 85, 86, 89, 91, 92, 93, 96, 99, 103, 124, 126, 130, 139, 140, 146, 148, 149, 150, 151, 156, 157, 158, 167, 168, 170, 172], "alexandr": [84, 171], "algebra": 124, "algorithm": [60, 62, 63, 65, 66, 67, 69, 70, 71, 72, 75, 78, 80, 83, 88, 91, 93, 96, 99, 101, 110, 112, 113, 122, 123, 124, 126, 127, 129, 139, 140, 156, 172, 173], "alia": [48, 49, 53, 54], "align": [41, 61, 63, 66, 68, 69, 72, 75, 80, 83, 89, 91, 92, 94, 95, 96, 101, 124, 127, 129, 130, 140, 142, 144, 172], "all": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 49, 53, 54, 57, 61, 62, 63, 64, 66, 67, 75, 76, 77, 78, 80, 87, 88, 89, 90, 91, 92, 93, 95, 97, 100, 101, 103, 104, 106, 109, 111, 112, 113, 115, 121, 123, 124, 125, 126, 128, 130, 139, 140, 142, 144, 156, 157, 167, 168, 169, 172], "all_coef": 139, "all_dml1_coef": 102, "all_s": 139, "all_smpl": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 88], "all_smpls_clust": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "all_z_col": [63, 91], "allow": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 53, 54, 64, 69, 72, 77, 78, 92, 93, 97, 109, 111, 112, 113, 124, 139, 140, 156, 168, 172, 173], "almqvist": 171, "along": [77, 112, 118], "alongsid": [105, 107, 109], "alpha": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 42, 44, 61, 63, 64, 66, 69, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 84, 88, 89, 90, 91, 92, 93, 96, 102, 103, 111, 112, 113, 116, 117, 118, 120, 121, 123, 124, 139, 140, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167], "alpha_": [43, 63, 91, 112, 118], "alpha_0": [157, 167], "alpha_ml_l": 84, "alpha_ml_m": 84, "alpha_x": [25, 33, 124], "alreadi": [18, 66, 67, 69, 72, 76, 98, 101, 112, 113, 124, 126], "also": [12, 14, 15, 16, 22, 23, 25, 26, 39, 60, 61, 62, 63, 64, 65, 67, 69, 70, 72, 75, 76, 77, 78, 79, 80, 81, 82, 85, 86, 87, 90, 91, 92, 93, 95, 97, 99, 100, 103, 111, 112, 113, 116, 118, 119, 120, 121, 122, 123, 124, 139, 140, 156, 157, 158, 169, 170, 172, 173], "alter": [63, 91], "altern": [37, 62, 64, 65, 70, 92, 95, 110, 112, 116, 121, 122, 123, 156, 168, 170], "although": 100, "alwai": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 49, 54, 62, 89, 97, 172], "always_tak": [25, 64, 92], "alyssa": 171, "amamb": 91, "american": [42, 94], "amgrem": 91, "amhorn": 91, "amit": [100, 171], "amjavl": 91, "ammata": 91, "among": [64, 84, 92, 93, 99, 100], "amount": [16, 64, 90, 92, 93, 173], "amp": [60, 63, 65, 66, 67, 69, 70, 71, 72, 91, 93, 99, 101], "an": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 41, 48, 49, 53, 54, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 75, 76, 78, 80, 81, 82, 84, 87, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108, 109, 111, 112, 114, 118, 119, 122, 123, 124, 125, 128, 130, 131, 139, 140, 156, 157, 158, 163, 168, 170, 171, 172, 173], "analog": [20, 21, 63, 69, 70, 72, 91, 93, 99, 111, 124, 126, 140, 141, 142, 143, 144, 156, 157, 163], "analys": [104, 109, 173], "analysi": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 47, 61, 63, 64, 80, 91, 92, 93, 98, 103, 110, 111, 124, 129, 158, 163, 167, 168, 172], "analyst": 98, "analyt": [94, 96], "analyz": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 64, 92, 93, 99, 173], "ancillari": 100, "andrea": 171, "angl": 64, "angrist": 94, "ani": [60, 61, 62, 65, 66, 67, 77, 79, 80, 98, 100, 101, 103, 124, 169, 173], "anna": [12, 14, 15, 17, 18, 19, 31, 35, 40, 62, 67, 69, 70, 71, 72, 124, 125, 126, 128, 130, 171], "annal": [156, 171], "anneal": [112, 123], "annot": 61, "annual": 171, "anoth": [61, 62, 63, 64, 75, 80, 90, 91, 98, 103, 112, 123, 124, 132], "anticip": [14, 16, 17, 19, 62, 70, 71, 124, 125, 127, 128, 129, 130], "anticipation_period": [14, 16, 17, 19, 69, 72], "anymor": [63, 91], "aos1161": 156, "aos1230": 156, "aos1671": 156, "ap": [64, 92], "ape_e401_uncond": 64, "ape_p401_uncond": 64, "api": [77, 89, 104, 109, 168, 172], "apo": [22, 23, 76, 77, 131, 145, 164], "apo_result": 77, "apoorva": 172, "apoorva__l": 94, "apoorval": 94, "app": 172, "appeal": 100, "append": [75, 77, 80, 98, 103], "appendix": [34, 36, 66, 69, 72, 99, 101, 157, 158], "appli": [11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 55, 56, 60, 61, 63, 64, 65, 66, 68, 69, 71, 72, 75, 80, 88, 91, 92, 93, 97, 98, 100, 101, 103, 120, 124, 139, 140, 156, 168, 170, 171, 172, 173], "applic": [41, 61, 67, 77, 80, 94, 100, 103, 105, 109, 111, 139, 171, 173], "applicatoin": 70, "apply_along_axi": 95, "apply_cross_fit": [61, 139], "apply_crossfit": 172, "appreci": 168, "approach": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 62, 63, 77, 78, 91, 97, 99, 100, 110, 112, 122, 124, 139, 156, 157, 158, 169, 171, 173], "appropri": [64, 84, 92, 124, 139, 173], "approx": [111, 124, 127, 129], "approxim": [61, 72, 75, 80, 81, 82, 83, 96, 100, 103, 111, 124, 156, 172, 173], "april": 69, "apt": 169, "ar": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 37, 38, 39, 40, 41, 44, 45, 46, 48, 49, 51, 52, 53, 54, 55, 56, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 78, 80, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 109, 111, 112, 113, 114, 116, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 152, 153, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173], "arang": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 68, 80, 83, 89, 93, 95, 96, 99, 100, 104, 109, 112, 118], "arbitrarili": [49, 54], "architectur": [77, 140, 171], "arellano": 171, "arg": [90, 97, 111, 124], "argmax": 98, "argmin": 75, "argu": [61, 64, 80, 92, 93, 99, 103, 173], "argument": [12, 14, 15, 17, 18, 19, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 43, 44, 45, 46, 47, 51, 64, 67, 69, 70, 71, 72, 75, 76, 81, 82, 87, 91, 92, 93, 98, 102, 104, 111, 112, 113, 114, 117, 121, 123, 124, 125, 172, 173], "aris": [61, 62, 63, 70, 80, 91, 100, 103, 173], "aronow": 94, "around": [62, 64, 77, 92, 93, 97, 124, 140], "arr": 95, "arrai": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 48, 49, 51, 52, 53, 54, 55, 66, 67, 69, 70, 71, 72, 75, 76, 77, 78, 80, 81, 82, 83, 91, 94, 95, 98, 99, 100, 101, 102, 103, 105, 107, 108, 111, 112, 113, 114, 117, 118, 139, 156, 157, 163, 170, 172, 173], "arrang": 63, "array_lik": 29, "articl": [45, 168], "arxiv": [43, 62, 63, 91, 98, 100, 168, 171, 172], "as_learn": [65, 112, 120, 122], "asarrai": [81, 82], "aspect": [64, 92, 93], "assert": [112, 121], "assess": 62, "asset": [93, 99, 173], "assign": [4, 5, 6, 7, 8, 9, 17, 19, 55, 64, 69, 72, 76, 77, 89, 92, 97, 104, 109, 111, 112, 123, 124, 125, 138, 157, 173], "assmput": [124, 138], "associ": [64, 84, 92, 124, 156, 171], "assum": [60, 63, 67, 79, 91, 94, 95, 98, 100, 124, 125, 126, 128, 130, 140, 141, 142, 143, 144, 156, 157, 167, 173], "assumpt": [62, 63, 64, 66, 67, 68, 69, 70, 72, 75, 91, 92, 94, 97, 101, 124, 125, 126, 128, 130, 138, 156, 173], "assur": 172, "astyp": [69, 71, 79, 97, 100], "asymptot": [20, 21, 61, 63, 77, 80, 91, 103, 139, 156, 171], "ate": [77, 78], "ate_estim": [66, 101], "ates": [77, 78], "athei": 171, "att": [16, 17, 19, 26, 31, 62, 68, 87, 88, 95, 100, 111, 124, 125, 126, 127, 128, 129, 130, 134, 140, 148, 157, 165, 172], "att_": [124, 128], "att_gt": [62, 70, 71], "attach": 62, "atte_estim": 67, "attempt": [48, 49], "attenu": [64, 92], "attr": 64, "attribut": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 50, 51, 52, 53, 54, 55, 56, 75, 90, 102, 106, 109, 112, 113, 114, 139, 140, 156], "attributeerror": [48, 49], "attrict": 124, "attrit": [30, 66, 101, 124, 138], "au": [65, 112, 122, 168, 170], "auc": 62, "author": [62, 100, 168], "auto_ml": 90, "autodoubleml": 90, "autom": 90, "automat": [13, 16, 61, 69, 71, 72, 76, 80, 87, 103, 111, 157, 163], "automl": 172, "automl_l": 90, "automl_l_lesstim": 90, "automl_m": 90, "automl_m_lesstim": 90, "automobil": [63, 91], "autos": 84, "autosklearn": 90, "auxiliari": [37, 61, 80, 103], "avail": [33, 62, 64, 65, 67, 69, 70, 72, 75, 77, 78, 84, 92, 93, 94, 95, 97, 100, 103, 104, 109, 111, 112, 113, 120, 122, 124, 125, 126, 127, 129, 157, 167, 168, 169, 172, 173], "avaiv": 52, "aver": [76, 78], "averag": [17, 18, 19, 22, 23, 25, 26, 31, 39, 40, 50, 60, 62, 65, 66, 67, 68, 69, 71, 72, 76, 79, 87, 93, 94, 95, 97, 98, 99, 100, 101, 110, 125, 126, 130, 131, 132, 133, 134, 138, 145, 148, 156, 164, 165, 171, 172, 173], "average_it": [76, 77, 78], "avoid": [56, 61, 62, 80, 97, 124, 139, 169, 172], "awai": 99, "ax": [13, 16, 68, 69, 70, 71, 72, 75, 77, 78, 80, 81, 82, 83, 85, 86, 90, 91, 92, 93, 94, 96, 97], "ax1": [77, 78, 83, 88, 93, 96], "ax2": [77, 78, 83, 88, 93, 96], "axhlin": [68, 76, 77, 90, 97], "axi": [13, 16, 63, 64, 75, 77, 78, 84, 88, 91, 92, 94, 95, 97], "axvlin": [77, 78, 80], "b": [12, 14, 15, 16, 17, 19, 45, 61, 63, 65, 80, 81, 82, 91, 94, 96, 97, 100, 103, 111, 112, 122, 124, 130, 156, 157, 167, 168, 170, 171], "b208": 65, "b371": 65, "b5d34a6f42b": 65, "b5d7": 65, "b_": 124, "b_0": 44, "b_1": 44, "b_j": 45, "bach": [75, 84, 90, 100, 168, 171, 172], "back": 77, "backbon": 75, "backend": [5, 6, 7, 8, 9, 62, 93, 99, 100, 104, 106, 108, 110, 112, 117, 172], "backward": [4, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 104, 109, 172], "bad": 94, "balanc": [41, 64, 69, 70, 72, 89, 92, 93], "balanced_r0": 41, "band": [62, 110, 173], "bandwidth": [27, 28, 29, 46, 97, 124, 172], "bar": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 87, 90, 92, 111, 124, 140, 145, 148, 157, 164], "base": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 37, 38, 39, 47, 50, 52, 56, 61, 62, 63, 64, 66, 67, 68, 70, 71, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 107, 109, 111, 112, 115, 121, 122, 123, 124, 125, 128, 130, 138, 139, 140, 156, 157, 158, 163, 168, 170, 171, 172, 173], "base_classifi": 76, "base_estim": [53, 54, 97], "base_regressor": 76, "baseestim": 98, "baselin": [14, 35, 64, 90, 92], "basesampl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "basi": [22, 26, 39, 51, 72, 81, 82, 88, 111], "basic": [62, 63, 64, 67, 71, 91, 92, 93, 94, 97, 99, 100, 110, 112, 122], "basis_df": 88, "basis_matrix": 88, "batch": 65, "battocchi": 171, "batuhan": 172, "bay": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 156], "bayesian": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "bb2913dc": 65, "bbotk": [65, 112, 122, 123, 172], "bbox_inch": 80, "bbox_to_anchor": [80, 89], "bcallaway11": [62, 70], "bd929a9e": 65, "bde4": 65, "becam": [64, 92, 93], "becaus": [49, 54, 60, 61, 62, 63, 79, 80, 87, 91, 94, 98, 100, 103, 173], "becker": [65, 112, 122], "becom": [63, 90, 91, 111, 139], "bee": 68, "been": [13, 16, 63, 64, 69, 71, 72, 90, 91, 92, 93, 99, 100, 111, 112, 119, 120, 124, 130, 168, 172], "befor": [17, 19, 55, 56, 62, 64, 68, 69, 71, 72, 76, 78, 87, 92, 96, 100, 124, 126, 173], "begin": [33, 41, 42, 43, 61, 63, 64, 65, 66, 68, 69, 72, 75, 80, 83, 91, 92, 94, 95, 96, 101, 102, 104, 109, 112, 121, 123, 124, 127, 129, 130, 139, 140, 142, 144, 156, 170, 173], "behav": [69, 71, 72, 98], "behavior": [64, 94, 112, 114], "behind": [70, 124], "being": [17, 19, 20, 21, 35, 36, 47, 50, 53, 54, 63, 72, 91, 97, 100, 124, 129, 130, 139, 140, 146, 156, 157, 163, 168], "belloni": [34, 84, 156, 171], "below": [55, 56, 60, 64, 70, 79, 92, 94, 98, 124, 169, 170], "bench_x1": 100, "bench_x2": 100, "benchmark": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 57, 77, 78, 87, 158, 172], "benchmark_dict": [57, 99], "benchmark_inc": 99, "benchmark_pira": 99, "benchmark_result": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39], "benchmark_twoearn": 99, "benchmarking_set": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 78, 87, 99, 100, 157, 158], "benchmarking_vari": 87, "benefit": [61, 64, 80, 92, 103], "bernoulli": [33, 41], "berri": [63, 91], "besid": 170, "best": [22, 26, 39, 49, 50, 51, 54, 69, 72, 76, 77, 81, 82, 85, 86, 90, 169], "best_estim": 50, "best_loss": 90, "best_param": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50], "best_scor": 50, "beta": [30, 33, 34, 36, 42, 64, 66, 92, 95, 97, 101, 124, 140, 151], "beta_": [66, 101], "beta_0": [32, 66, 89, 95, 101, 111, 124, 135, 140, 151], "beta_a": [31, 40, 100], "beta_j": [33, 34, 36, 42], "better": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 69, 72, 75, 77, 78, 100, 124, 130], "between": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 56, 60, 66, 68, 69, 72, 77, 78, 79, 83, 84, 90, 94, 96, 98, 99, 100, 101, 124, 132, 140, 141, 142, 143, 144, 148, 151, 152, 153, 156, 157, 167, 170, 172], "betwen": [60, 79], "beyond": 171, "bia": [36, 60, 66, 79, 84, 97, 100, 101, 110, 124, 138, 139, 140, 154, 155, 157, 167, 171, 172], "bias": [60, 64, 69, 72, 79, 92, 93, 99, 173], "bias_bench": 100, "bibtex": 168, "big": [84, 102, 139, 140, 141, 142, 149, 156, 157, 159, 160, 161, 162, 165, 166, 167], "bigg": [63, 91, 140, 147, 148, 157, 160, 165], "bilia": 11, "bilinski": 171, "bin": [61, 77, 78, 80, 169], "binari": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 32, 37, 39, 41, 47, 60, 62, 64, 65, 67, 75, 79, 87, 88, 89, 92, 94, 95, 100, 111, 112, 119, 123, 126, 130, 133, 134, 135, 138, 157, 164, 165, 172, 173], "binary_outcom": 47, "binary_treat": [32, 81, 85, 87], "binary_unbalanc": 41, "bind": 172, "binder": [65, 112, 122, 168, 170, 172], "binomi": [79, 94, 95, 96, 98], "bischl": [65, 112, 122, 168, 170], "black": [61, 65, 73, 104, 109, 170], "blob": 62, "blog": 45, "blondel": [168, 170], "blp": [51, 63, 91], "blp_data": [63, 91], "blp_model": [85, 86], "blue": [61, 63, 66, 91], "blueprint": 76, "bodori": 171, "bond": [64, 92, 93], "bonferroni": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 156], "bonu": [11, 65, 104, 109, 170], "book": [65, 98, 100, 112, 119, 122, 123, 171], "bool": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 41, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 87, 97], "boolean": [36, 85, 86, 104, 109, 139], "boost": [60, 64, 67, 69, 72, 75, 76, 77, 79, 92], "boost_class": [64, 92], "boost_summari": 92, "boostrap": [83, 172], "bootstrap": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 51, 69, 70, 71, 72, 78, 81, 82, 83, 85, 86, 93, 96, 110, 111, 124, 125, 139, 140, 168, 170, 172, 173], "both": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 40, 62, 64, 65, 67, 68, 75, 76, 77, 88, 89, 90, 92, 93, 95, 97, 98, 99, 100, 104, 105, 109, 112, 114, 123, 124, 127, 128, 129, 156, 157, 158, 163, 166, 167, 172, 173], "bottom": [63, 64, 75, 91, 92, 93], "bound": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 35, 37, 38, 39, 55, 56, 64, 69, 72, 77, 78, 87, 88, 92, 99, 100, 157, 158, 163, 167, 172, 173], "branch": 65, "brantli": [62, 171], "break": [61, 172], "breve": [140, 151], "breviti": 173, "brew": 169, "brewer": 63, "bridg": 100, "brief": 103, "briefli": 70, "bring": [60, 79], "brucher": [168, 170], "bsd": [168, 172], "bst": 92, "budget": [90, 112, 123], "bug": [168, 172], "build": [63, 75, 91, 95], "build_design_matric": [81, 82], "build_sim_dataset": 62, "built": [52, 90, 112, 123, 168], "bureau": [100, 139, 171], "busi": [36, 43, 63, 91, 100, 171], "b\u00fchlmann": 171, "c": [10, 11, 17, 18, 19, 34, 40, 42, 44, 60, 61, 62, 63, 64, 65, 68, 73, 79, 80, 84, 85, 86, 91, 92, 94, 97, 98, 103, 104, 109, 112, 122, 123, 124, 130, 140, 142, 144, 157, 160, 162, 168, 169, 170, 171, 173], "c1": [10, 11, 44, 63, 84, 91, 98, 103, 168, 171], "c68": [10, 11, 44, 63, 84, 91, 98, 103, 168, 171], "c895": 65, "c_": [16, 124, 127, 129, 130, 140, 142, 144, 156], "c_d": [34, 157, 165, 166, 167], "c_i": [124, 130], "c_y": [34, 157, 167], "ca1af7be64b2": 65, "caac5a95": 65, "calcualt": 95, "calcul": [22, 26, 39, 62, 64, 69, 72, 75, 77, 78, 81, 82, 83, 85, 86, 90, 92, 96, 99, 157, 163, 167], "calendar": [69, 70, 71, 72], "calibr": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 55, 56, 75, 90, 100], "calibration_method": [55, 56], "call": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 49, 53, 54, 60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 76, 77, 79, 81, 82, 83, 85, 86, 92, 93, 95, 96, 97, 99, 100, 101, 104, 109, 112, 114, 123, 139, 140, 156, 157, 163, 167, 170, 172, 173], "callabl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 61, 75, 76, 80, 81, 82, 110, 112, 113, 168], "callawai": [17, 19, 62, 69, 70, 71, 72, 124, 125, 128, 130, 171], "callback": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "camera": 84, "cameron": [63, 91], "can": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 46, 49, 52, 54, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 109, 111, 112, 113, 114, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 130, 131, 138, 139, 140, 141, 142, 143, 144, 145, 148, 151, 152, 153, 156, 157, 158, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173], "candid": 100, "cannot": [75, 97, 100, 124, 173], "capabl": [5, 6, 7, 8, 9, 60, 79], "capo": [22, 88], "capo0": 88, "capo1": 88, "capsiz": [76, 77, 78, 90, 94, 97], "capthick": [76, 77, 78, 97], "cardin": [63, 91], "care": [70, 98, 112, 114], "carlo": [31, 32, 35, 40, 81, 82, 85, 86, 100, 171], "casalicchio": [65, 112, 122, 168, 170], "case": [4, 5, 6, 7, 8, 9, 11, 14, 22, 23, 25, 26, 32, 46, 60, 63, 64, 69, 70, 72, 79, 81, 82, 83, 84, 87, 88, 89, 90, 91, 95, 96, 97, 98, 99, 100, 104, 109, 111, 112, 114, 115, 116, 118, 119, 121, 124, 126, 130, 138, 139, 140, 142, 144, 156, 157, 163, 170, 172, 173], "cat": [61, 172], "catboost": 75, "cate": [26, 39, 51, 88, 110, 172], "cate_obj": 111, "categor": [17, 19], "categori": 77, "cattaneo": [124, 171], "caus": [61, 80, 97, 103], "causal": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 60, 61, 63, 64, 65, 66, 75, 79, 80, 88, 90, 91, 92, 94, 98, 99, 101, 102, 103, 104, 107, 109, 110, 124, 130, 139, 156, 157, 163, 171], "causal_contrast": [23, 76, 77, 78, 88, 124], "causal_contrast_att": 88, "causal_contrast_c": 88, "causal_contrast_model": [77, 78, 124], "causal_contrast_result": 77, "causaldml": 171, "causalml": [98, 171], "causalweight": 171, "caution": 156, "caveat": 100, "cbind": 63, "cbook": [69, 70, 71, 72], "cc": 92, "ccp_alpha": [26, 52, 92], "cd": 169, "cd_fast": 91, "cda85647": 65, "cdf": 111, "cdid": [63, 91], "cdot": [17, 18, 19, 31, 35, 40, 41, 47, 63, 68, 69, 72, 83, 87, 91, 94, 96, 97, 100, 111, 124, 125, 127, 129, 130, 139, 140, 142, 144, 145, 148, 149, 151, 154, 155, 156, 157, 160, 162, 164], "cdot1": 87, "cell": [76, 90], "center": [69, 70, 72, 77, 84, 89], "central": [139, 172], "certain": [124, 140, 142, 144], "cexcol": 63, "cexrow": 63, "cf_d": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 57, 69, 72, 78, 87, 88, 99, 100, 157, 158, 163, 164, 165, 166, 167, 173], "cf_y": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 57, 69, 72, 78, 87, 88, 99, 100, 157, 158, 163, 164, 165, 166, 167, 173], "cff": 172, "chad": 100, "challeng": [63, 91, 157, 158], "chanc": 72, "chang": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 53, 54, 64, 66, 67, 71, 89, 93, 99, 100, 101, 124, 129, 130, 140, 144, 148, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 169, 171, 172], "channel": 173, "chapter": [20, 21, 65, 98, 112, 119, 120, 122, 123, 157, 167], "charact": [64, 65, 112, 120, 121, 123, 172], "characterist": [99, 173], "chart": 90, "check": [48, 49, 53, 54, 61, 64, 67, 68, 69, 72, 75, 80, 90, 92, 93, 98, 102, 103, 124, 125, 168, 169, 172], "check_data": 172, "check_scor": 172, "checkmat": 172, "chernozhukov": [10, 11, 34, 42, 44, 61, 63, 64, 75, 80, 84, 90, 91, 92, 93, 98, 99, 103, 139, 140, 148, 156, 157, 158, 167, 168, 171, 172], "chetverikov": [10, 11, 44, 63, 84, 91, 98, 103, 156, 168, 171], "chiang": [43, 63, 91, 171], "chieh": 171, "choic": [5, 6, 7, 8, 9, 14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 64, 69, 70, 71, 72, 76, 84, 92, 95, 111, 112, 113, 121, 123, 124, 128, 140, 142, 144, 157, 158, 163, 167, 172], "cholecyst": 98, "choos": [60, 64, 70, 75, 76, 79, 80, 84, 92, 93, 102, 124, 128, 139, 140, 141, 142, 143, 144, 148, 151, 152, 153, 156, 170, 173], "chosen": [22, 35, 40, 75, 76, 112, 123, 124], "chou": 94, "chr": 64, "christian": [84, 171], "christoph": 171, "chunk": [112, 123], "ci": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 67, 68, 69, 70, 71, 72, 76, 77, 78, 81, 82, 83, 85, 86, 87, 88, 90, 92, 93, 96, 97, 99, 100, 111, 124, 157, 163, 172, 173], "ci_at": [77, 78], "ci_bound": 16, "ci_cvar": [83, 93], "ci_cvar_0": 83, "ci_cvar_1": 83, "ci_joint": [69, 70, 71, 72, 78], "ci_joint_cvar": 83, "ci_joint_lqt": 96, "ci_joint_qt": 96, "ci_length": 67, "ci_low": [76, 77, 78], "ci_lpq_0": 96, "ci_lpq_1": 96, "ci_lqt": [93, 96], "ci_pointwis": [77, 78], "ci_pq_0": [93, 96], "ci_pq_1": [93, 96], "ci_qt": [93, 96], "ci_tun": 76, "ci_tuned_pipelin": 76, "ci_untun": 76, "ci_untuned_pipelin": 76, "ci_upp": [76, 77, 78], "cinelli": [100, 157, 158, 171], "circumv": 173, "citat": 172, "cite": 168, "cl": 68, "claim": 65, "clarifi": [69, 70, 71, 72], "clash": 62, "class": [0, 4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 64, 65, 66, 67, 69, 70, 71, 72, 73, 76, 77, 78, 87, 88, 89, 90, 92, 93, 98, 99, 101, 102, 104, 105, 106, 107, 108, 109, 111, 112, 114, 115, 116, 119, 121, 123, 124, 125, 139, 140, 156, 168, 170, 172], "class_estim": 97, "class_learn": 93, "class_learner_1": 75, "class_learner_2": 75, "classes_": [90, 98], "classic": [62, 63, 70, 91, 173], "classif": [26, 48, 53, 60, 62, 64, 65, 66, 67, 69, 70, 71, 72, 75, 90, 93, 95, 99, 101, 111, 112, 113, 119, 123, 124, 126, 127, 129, 173], "classifavg": 65, "classifi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 39, 46, 48, 53, 65, 77, 78, 89, 90, 97, 98, 112, 115, 119, 172], "classifiermixin": 98, "classmethod": [4, 5, 6, 7, 8, 9, 55], "claudia": [171, 172], "claus": [168, 172], "clean": 172, "cleaner": 75, "cleanup": 172, "clear": [63, 77, 91], "clearli": [69, 72, 97], "clever": 75, "client": 77, "clip": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 55, 56], "clipping_threshold": [12, 15, 55, 56, 81], "clone": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 61, 65, 75, 80, 91, 93, 102, 112, 115, 121, 123, 124, 139, 140, 156, 157, 163, 169, 170], "close": [56, 62, 64, 92, 98, 100, 157, 158], "cluster": [5, 6, 7, 8, 9, 12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 43, 71, 104, 105, 107, 108, 109, 171, 172], "cluster_col": [4, 5, 6, 8, 9, 63, 91, 104, 105, 107, 108, 109], "cluster_var": [4, 5, 6, 7, 8, 9, 43, 104, 105, 109], "cluster_var_i": [63, 91], "cluster_var_j": [63, 91], "cmap": 91, "cmd": 172, "co": [41, 45, 68], "codaci": 172, "code": [22, 26, 39, 45, 60, 62, 63, 64, 65, 66, 79, 84, 92, 103, 111, 112, 120, 121, 122, 123, 124, 139, 140, 156, 169, 170, 172, 173], "codecov": 172, "coef": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 99, 100, 101, 102, 103, 111, 112, 113, 114, 116, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 170, 173], "coef_": 100, "coef_df": 63, "coef_valu": 90, "coeffici": [16, 31, 32, 40, 49, 51, 54, 64, 66, 75, 76, 85, 86, 89, 92, 94, 95, 97, 100, 101, 111, 156, 157, 163, 173], "coefs_t": 95, "coefs_w": 95, "coffici": [157, 163], "cofid": 51, "coincid": [68, 88, 93], "col": [61, 63, 92], "col_nam": [69, 72], "collect": [65, 66, 67, 76, 77, 91, 101], "colnam": [63, 75], "color": [13, 16, 64, 66, 68, 72, 76, 77, 78, 80, 81, 82, 83, 90, 91, 92, 93, 94, 96, 97, 100], "color_palett": [13, 16, 69, 72, 76, 77, 78, 80, 91, 92, 93], "colorbar": 91, "colorblind": [13, 16, 69, 72, 76, 77, 78], "colorramppalett": 63, "colorscal": [81, 82], "colour": [61, 63], "column": [5, 6, 7, 8, 9, 16, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 97, 99, 100, 101, 104, 105, 106, 107, 108, 109, 111, 112, 114, 124, 127, 129, 139, 170, 172, 173], "column_stack": [8, 68, 76, 77, 78, 85, 86, 97, 99, 100, 124], "colv": 63, "com": [45, 62, 64, 65, 70, 71, 84, 94, 100, 112, 122, 168, 169], "comb": 84, "combin": [14, 16, 62, 63, 65, 67, 70, 71, 75, 76, 77, 78, 88, 90, 91, 100, 112, 120, 124, 128, 139, 157, 163, 172], "combind": 93, "combined_loss": 84, "come": [102, 112, 115, 140, 157, 158, 168, 173], "command": [169, 172], "comment": [104, 109], "commit": 172, "common": [75, 99, 100, 111, 124, 171], "commonli": 77, "companion": 171, "compar": [61, 63, 68, 69, 72, 76, 77, 80, 81, 82, 83, 85, 86, 88, 89, 94, 96, 97, 98, 100, 103, 112, 113, 124, 157, 158, 172], "comparevers": 64, "comparison": [69, 72, 75, 78, 94], "comparison_data": 76, "compat": [4, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 60, 62, 79, 104, 109, 172], "complement": 100, "complet": [50, 76, 90, 103, 157, 163, 169], "complex": [26, 62, 76, 90], "compli": [97, 124], "complianc": [96, 97, 124, 140, 149], "complic": [65, 76, 173], "complier": [64, 92, 93, 96, 97, 111, 124], "compon": [17, 19, 53, 54, 62, 64, 70, 75, 77, 84, 90, 92, 95, 111, 112, 123, 139, 140, 141, 142, 143, 144, 145, 147, 148, 149, 152, 153, 172], "compont": 62, "composit": 171, "comprehens": 77, "compris": 156, "comput": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 51, 57, 61, 62, 64, 65, 69, 70, 71, 72, 77, 80, 92, 93, 98, 99, 100, 139, 140, 151, 157, 158, 159, 160, 161, 162, 163, 164, 165, 168, 171, 172, 173], "computation": [157, 158], "concat": [77, 90, 91, 92, 95, 156], "concaten": [68, 76, 92, 156], "concentr": 156, "concern": 100, "conclud": [97, 100, 173], "cond": [124, 126, 138], "conda": [171, 172], "condit": [20, 21, 22, 24, 26, 31, 32, 39, 40, 61, 63, 64, 66, 67, 68, 69, 72, 78, 80, 87, 88, 91, 92, 95, 97, 100, 101, 103, 110, 124, 127, 128, 129, 130, 156, 157, 164, 165, 167, 170, 171, 172, 173], "conduct": [111, 124, 126, 127, 129, 173], "conf": [62, 96], "confer": 171, "confid": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 51, 62, 63, 64, 66, 67, 69, 70, 71, 72, 76, 77, 78, 81, 82, 83, 85, 86, 88, 91, 93, 96, 97, 99, 101, 110, 111, 124, 139, 140, 157, 163, 170, 171, 172, 173], "confidenceband": 83, "confidenti": 100, "config": [55, 56, 94], "configur": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 56, 65, 90], "confint": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 51, 64, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 81, 82, 83, 85, 86, 88, 93, 95, 96, 97, 98, 99, 101, 111, 139, 156, 168, 170, 173], "conflict": 169, "confound": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 37, 38, 39, 40, 57, 60, 64, 69, 70, 72, 79, 87, 89, 92, 96, 99, 100, 104, 109, 124, 135, 136, 137, 156, 157, 158, 163, 166, 167, 170, 171, 172, 173], "congress": 171, "connect": [64, 92, 93], "consequ": [31, 40, 63, 76, 87, 91, 99, 111, 124, 126, 157, 158, 164, 165, 167], "conserv": [99, 100, 157, 167], "consid": [24, 25, 26, 27, 28, 47, 55, 56, 61, 63, 64, 66, 67, 68, 69, 70, 71, 72, 75, 80, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 111, 112, 116, 121, 124, 125, 130, 133, 134, 139, 140, 156, 157, 158, 168, 173], "consider": [100, 124], "consist": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 49, 54, 64, 67, 89, 90, 92, 93, 94, 100, 103, 104, 109, 124, 130, 135, 136, 137, 170, 172], "consol": [61, 172], "constant": [17, 19, 34, 49, 54, 69, 72, 77, 84, 89, 95, 111, 124, 156], "constrained_layout": 80, "construct": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 51, 65, 68, 69, 72, 76, 81, 82, 83, 88, 93, 98, 99, 102, 111, 119, 120, 140, 146, 155, 156, 172, 173], "construct_framework": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "construct_iv": 91, "constructiv": 63, "constructor": [65, 105, 109], "consum": [63, 91], "cont": 35, "cont_d": [76, 77, 78], "contain": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 47, 48, 49, 50, 53, 54, 61, 63, 64, 69, 70, 72, 75, 76, 78, 80, 81, 82, 85, 86, 91, 92, 98, 103, 105, 106, 108, 109, 111, 112, 123, 124, 125, 127, 129, 156, 157, 158, 163, 172], "context": [100, 124, 138, 173], "contin": [35, 90], "continu": [35, 37, 41, 60, 65, 76, 77, 78, 79, 84, 89, 94, 97, 124, 157, 167, 172, 173], "contour": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 84, 87, 99, 100, 157, 163], "contour_plot": 100, "contours_z": [81, 82], "contrast": [23, 67, 77, 83, 88, 124, 132], "contribut": [169, 172], "contributor": 172, "control": [14, 16, 17, 19, 42, 47, 55, 62, 70, 71, 76, 84, 88, 93, 95, 97, 98, 100, 104, 105, 109, 124, 125, 127, 128, 129, 130, 140, 142, 144, 157, 160, 162, 173], "control_group": [14, 16, 69, 70, 71, 72, 124, 125, 127, 129], "conveni": [104, 107, 109], "convent": [64, 70, 92, 93, 97, 124, 130], "converg": [37, 61, 75, 80, 91, 103], "convergencewarn": 91, "convers": 91, "convert": [17, 19, 77, 83, 91, 96], "convex": 94, "cooper": 172, "coor": [65, 112, 122, 168, 170], "coordin": [77, 100], "copi": [72, 76, 90, 92, 95, 100], "cor": [157, 167], "core": [17, 19, 69, 70, 71, 72, 73, 76, 77, 78, 83, 87, 89, 91, 92, 93, 96, 99, 104, 106, 109, 112, 119, 170, 172], "cores_us": [83, 93, 96], "correct": [87, 88, 89, 100, 111, 156, 172], "correctli": [48, 53, 67, 76, 94, 99, 157, 167], "correl": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 63, 66, 84, 91, 98, 99, 101, 124, 157, 158, 167], "correpond": [69, 72, 124], "correspond": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 61, 63, 64, 65, 67, 68, 69, 70, 71, 72, 75, 76, 78, 80, 81, 82, 84, 88, 91, 92, 93, 95, 96, 99, 100, 103, 111, 112, 113, 114, 121, 123, 124, 126, 128, 130, 131, 138, 139, 140, 142, 144, 156, 157, 158, 160, 162, 163, 165, 167, 172, 173], "correspondingli": [69, 72], "cosh": 45, "coul": 63, "could": [60, 65, 69, 70, 72, 76, 79, 81, 82, 90, 100, 172, 173], "counfound": [31, 40, 96, 99, 111, 157, 167], "count": [76, 77, 78, 92, 93], "counti": 70, "countour": [157, 163], "countyr": 70, "coupl": [64, 92, 93], "cournapeau": [168, 170], "cours": [64, 75, 92, 100, 156, 173], "cov": [30, 31, 47, 97], "cov_nam": [97, 124], "cov_typ": [22, 26, 39, 51, 172], "covari": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 19, 26, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 51, 52, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 73, 76, 77, 78, 80, 81, 82, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 124, 126, 127, 129, 135, 136, 137, 138, 140, 141, 142, 143, 144, 156, 157, 158, 170, 171, 172], "cover": [62, 84, 99, 108, 109], "coverag": [69, 72, 75, 97, 98, 111, 172], "cp": [64, 65, 112, 120], "cpu": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 77], "cpu_count": [83, 93, 96], "cran": [65, 171, 172], "creat": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 55, 60, 63, 65, 69, 72, 76, 78, 79, 80, 81, 82, 83, 85, 86, 89, 91, 93, 95, 96, 100, 104, 109, 112, 120, 157, 158, 163, 167, 169, 172], "create_default_for_vers": 77, "create_synthetic_group_data": 95, "creation": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "crictial": 139, "critic": [100, 173], "cross": [12, 14, 15, 16, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 55, 56, 60, 61, 62, 64, 65, 66, 70, 75, 77, 80, 90, 92, 93, 97, 100, 103, 105, 109, 110, 112, 113, 114, 123, 127, 128, 130, 143, 155, 156, 159, 160, 163, 172, 173], "cross_sectional_data": [15, 18, 67, 124, 126], "crossfit": [75, 124], "crosstab": 94, "crucial": [84, 124, 173], "csail": [168, 170], "csdid": 71, "csv": [71, 84], "cuda": 77, "cumul": 124, "current": [52, 56, 62, 67, 68, 69, 72, 88, 140, 157, 167, 168, 169, 173], "custom": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 62, 70, 76, 80, 100, 112, 113], "custom_measur": 62, "cut": 95, "cutoff": [46, 47, 97, 124], "cv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 55, 65, 92, 112, 122, 123, 139], "cv_calibr": [55, 56], "cv_glmnet": [63, 64, 65, 66, 112, 120, 123, 124, 156, 170], "cvar": [24, 29, 110, 146, 172], "cvar_0": 83, "cvar_1": 83, "d": [4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 51, 52, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 163, 164, 165, 166, 167, 168, 170, 171, 173], "d0": [83, 96, 156], "d0_true": 96, "d0cdb0ea4795": 65, "d1": [83, 94, 96, 156], "d10": 156, "d1_true": 96, "d2": [94, 156], "d21ee5775b5f": 65, "d2cml": 71, "d3": 156, "d4": 156, "d5": 156, "d5a0c70f1d98": 65, "d6": 156, "d7": 156, "d8": 156, "d9": 156, "d_": [35, 43, 63, 68, 78, 91, 124, 126, 130, 156], "d_0": [124, 131], "d_1": [94, 156], "d_2": 94, "d_col": [4, 5, 6, 7, 8, 9, 16, 60, 61, 63, 64, 65, 66, 69, 70, 71, 72, 79, 81, 82, 85, 86, 88, 91, 92, 93, 95, 97, 98, 99, 102, 103, 104, 105, 106, 108, 109, 112, 120, 121, 124, 125, 127, 129, 139, 140, 170, 172, 173], "d_i": [32, 33, 34, 36, 41, 42, 44, 45, 61, 66, 67, 78, 80, 83, 94, 96, 97, 101, 103, 124, 126, 138], "d_j": [78, 124, 131, 132, 156], "d_k": [124, 132, 156], "d_l": [124, 131], "d_w": 95, "da1440": 94, "dag": [66, 100, 101, 173], "dai": [69, 76], "dark": [61, 80], "darkblu": 63, "darkr": 63, "dash": [66, 69, 72, 77, 78], "dat": [104, 109], "data": [0, 10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 51, 53, 54, 62, 68, 75, 84, 94, 98, 102, 104, 106, 107, 108, 110, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 125, 127, 128, 129, 130, 139, 156, 160, 161, 162, 163, 171, 172], "data_apo": [76, 77, 78], "data_cvar": 93, "data_dict": [46, 81, 82, 85, 86, 87, 97, 124], "data_dml": 99, "data_dml_bas": [64, 81, 82, 85, 86, 92, 93, 95], "data_dml_base_iv": [64, 92, 93], "data_dml_flex": [64, 92], "data_dml_flex_iv": 64, "data_dml_iv_flex": 92, "data_dml_new": 95, "data_fram": 173, "data_lqt": 93, "data_pq": 93, "data_qt": 93, "data_transf": [63, 91, 92], "dataclass": [50, 56], "datafram": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 51, 52, 63, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 105, 106, 107, 108, 111, 112, 113, 114, 116, 124, 126, 140, 156, 157, 158, 163, 170, 173], "dataset": [0, 5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 50, 61, 62, 66, 67, 69, 72, 75, 76, 77, 78, 80, 81, 82, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170, 172, 173], "datatyp": [71, 172], "date": [16, 17, 19, 112, 120], "date_format": 16, "datetim": [7, 16, 17, 19, 69, 70, 72, 106, 109], "datetime64": [69, 106, 109], "datetime_complet": 76, "datetime_start": 76, "datetime_unit": [7, 16, 69, 72, 106, 109, 124, 125, 127, 129], "david": 172, "db": [64, 92, 93, 99, 173], "dbl": [62, 63, 64, 65, 104, 109, 156, 170, 173], "dc13a11076b3": 65, "ddc9": 65, "de": [60, 79, 171], "deal": [60, 79], "debias": [10, 11, 41, 43, 44, 63, 84, 91, 98, 110, 112, 139, 168, 171, 172], "debt": [64, 92, 93], "decai": [66, 101], "decid": [64, 92, 98], "decis": [26, 60, 64, 79, 92, 93, 111, 124, 171, 173], "decision_effect": 60, "decision_impact": [60, 79], "decisiontreeclassifi": [26, 52, 92], "decisiontreeregressor": 92, "declar": 173, "decomposit": [124, 125], "decreas": 97, "dedic": [104, 108, 109], "deep": [48, 49, 53, 54, 90], "deeper": 26, "def": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 69, 72, 75, 76, 80, 83, 90, 91, 94, 95, 96, 98, 100, 112, 113, 117, 140], "default": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 51, 52, 53, 54, 55, 56, 62, 63, 66, 67, 69, 70, 71, 72, 75, 76, 85, 86, 91, 95, 97, 99, 100, 101, 102, 105, 109, 111, 112, 113, 116, 121, 123, 124, 139, 156, 157, 163, 164, 170, 173], "default_arg": [69, 72], "default_convert": 91, "default_jitt": 16, "defier": [97, 124], "defin": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 54, 61, 64, 65, 67, 70, 75, 77, 78, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 92, 93, 95, 96, 97, 98, 99, 100, 111, 112, 113, 117, 120, 122, 124, 126, 127, 129, 130, 138, 140, 141, 143, 144, 157, 158, 163, 167, 172], "definit": [45, 69, 72, 76, 85, 86, 88, 124, 128, 157, 164, 165], "degre": [47, 64, 81, 82, 88, 91, 92, 97, 111, 157, 158], "dekel": 171, "delete_origin": 65, "deliber": 94, "delta": [16, 42, 62, 67, 69, 72, 100, 124, 126, 127, 128, 129, 130, 140, 142, 144], "delta_bench": 100, "delta_i": 62, "delta_j": 42, "delta_t": [17, 19, 69, 72], "delta_theta": [57, 78, 87, 99, 100, 157, 158], "delta_v": 100, "demand": [63, 91, 157, 158], "demir": [10, 11, 44, 63, 84, 91, 98, 103, 139, 168, 171], "demo": [70, 100], "demonstr": [61, 62, 63, 70, 77, 80, 91, 97, 100, 104, 109, 124, 156, 168, 170], "deni": 171, "denomin": [157, 158, 164, 165], "denot": [19, 38, 63, 64, 66, 67, 68, 72, 91, 92, 97, 100, 101, 111, 124, 126, 127, 129, 130, 131, 136, 140, 157, 158, 163, 165, 167], "dens_net_tfa": 64, "densiti": [27, 28, 29, 61, 66, 77, 78, 80], "dep": 73, "dep1": [65, 73, 104, 109, 170], "dep2": [65, 73, 104, 109, 170], "depend": [17, 19, 22, 23, 24, 26, 27, 29, 32, 41, 65, 67, 69, 72, 75, 81, 82, 85, 86, 87, 89, 90, 95, 97, 102, 111, 112, 121, 124, 127, 128, 129, 140, 149, 150, 157, 158, 164, 167, 170, 171], "deprec": [4, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 67, 68, 69, 70, 71, 72, 81, 102, 104, 109, 112, 124, 139, 140, 157], "deprecationwarn": [68, 81], "depreci": 172, "depth": [26, 52, 64, 65, 95, 102, 111, 112, 123, 124, 139, 140, 156, 170, 173], "deriv": [12, 14, 15, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 124, 156], "describ": [13, 62, 63, 69, 72, 91, 92, 93, 100, 112, 139, 140, 151, 169, 172], "descript": [19, 64, 71, 73, 76, 99, 112, 117, 121, 123, 139, 140, 142, 144, 157, 158, 160, 162], "deserv": 124, "design": [8, 17, 19, 41, 46, 47, 76, 77, 78, 90, 107, 109, 110, 171, 172], "design_info": [81, 82], "design_matrix": [81, 82, 111], "desir": [40, 65, 95, 124, 169], "detail": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 64, 65, 67, 68, 77, 78, 80, 84, 90, 93, 97, 99, 100, 103, 104, 109, 111, 112, 113, 117, 120, 122, 125, 126, 127, 128, 129, 130, 138, 140, 146, 148, 149, 150, 154, 155, 156, 157, 158, 160, 162, 167, 168, 169, 170, 172, 173], "determin": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 41, 49, 54, 64, 72, 83, 92, 93, 96, 97, 99, 124, 156, 157, 167], "determinist": [95, 97, 111, 124], "deutsch": 168, "dev": [169, 172], "develop": [62, 63, 65, 91, 100, 124, 126, 130, 172], "deviat": [75, 124, 157, 167], "devic": 77, "dezeur": 171, "df": [5, 6, 7, 8, 9, 16, 60, 61, 63, 66, 68, 69, 72, 76, 77, 78, 79, 81, 82, 83, 88, 91, 94, 96, 97, 99, 100, 101, 103, 105, 106, 108, 109, 111, 124, 125, 127, 129], "df_agg": 84, "df_all_apo": 77, "df_all_at": 77, "df_anticip": [69, 72], "df_apo": [76, 77, 78], "df_apo_ci": 78, "df_apo_plot": 76, "df_apos_ci": 78, "df_ate": [77, 78], "df_bench": 100, "df_binari": 100, "df_bonu": [65, 104, 109, 170], "df_capo0": 88, "df_capo1": 88, "df_cate": [81, 82, 88], "df_causal_contrast_c": 88, "df_ci": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 51], "df_coef": 75, "df_comparison": 76, "df_cvar": 93, "df_fuzzi": 97, "df_lqte": 93, "df_ml_g0": 75, "df_ml_g1": 75, "df_ml_m": 75, "df_pa": [67, 101], "df_perform": 77, "df_plot": 63, "df_post_treat": [69, 72], "df_pq": 93, "df_qte": 93, "df_result": 84, "df_sharp": 97, "df_sort": [77, 78], "df_summari": 92, "df_treat": 72, "df_wide": 91, "dfg": 168, "dgp": [17, 18, 19, 63, 66, 68, 69, 72, 83, 84, 91, 94, 95, 96, 100, 101], "dgp1": [17, 18, 19], "dgp2": [17, 18, 19], "dgp3": [17, 18, 19], "dgp4": [17, 18, 19], "dgp5": [17, 18, 19], "dgp6": [17, 18, 19], "dgp_confounded_irm_data": 100, "dgp_dict": 100, "dgp_tpye": 67, "dgp_type": [17, 18, 19, 67, 69, 72], "diagnost": 70, "diagon": 100, "diagram": [60, 79, 124], "dichotom": [60, 79], "dict": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 50, 51, 52, 53, 54, 57, 69, 72, 81, 82, 84, 90, 100, 112, 116], "dict_kei": [76, 157, 163], "dict_rdd": [107, 109], "dictionari": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 47, 57, 69, 72, 76, 81, 82, 85, 86, 99, 111, 112, 114, 117, 124, 125, 157, 163], "dictonari": [64, 92], "did": [0, 5, 7, 61, 67, 68, 69, 70, 71, 72, 76, 77, 91, 105, 106, 109, 110, 125, 126, 127, 129, 130, 140, 142, 144, 172, 173], "did_aggreg": [69, 71, 72], "did_data": 68, "did_multi": [124, 125], "diff": 92, "differ": [5, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 37, 38, 39, 40, 60, 61, 63, 64, 65, 66, 69, 71, 72, 76, 77, 78, 79, 80, 83, 85, 86, 87, 88, 90, 91, 92, 93, 94, 96, 97, 99, 100, 101, 102, 105, 109, 110, 111, 112, 113, 116, 120, 121, 125, 126, 127, 129, 130, 139, 141, 142, 143, 144, 169, 170, 171, 172, 173], "differenti": 124, "difficult": 100, "dillon": 171, "dim": [47, 64], "dim_x": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 47, 61, 63, 65, 75, 80, 88, 89, 90, 91, 103, 111, 112, 114, 124, 157, 163], "dim_z": [38, 42, 124], "dimens": [17, 19, 32, 43, 63, 91, 95, 139], "dimension": [13, 32, 34, 37, 38, 39, 84, 98, 111, 124, 135, 136, 137, 139, 156, 157, 163, 170, 171], "direct": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 66, 68, 80, 88, 101, 103, 124, 173], "directli": [46, 61, 62, 64, 75, 76, 78, 80, 99, 103, 104, 109, 157, 163, 170, 173], "discontinu": [8, 46, 47, 107, 109, 110, 171, 172], "discret": [23, 35, 76, 77, 78, 91, 124, 131, 172], "discretis": 93, "discuss": [33, 63, 64, 91, 92, 124, 125, 130, 171, 172, 173], "disjoint": [63, 85, 86, 91], "displai": [13, 63, 69, 70, 71, 72, 76, 77, 78, 91, 100, 111, 112, 113, 157, 163], "displot": 92, "disproportion": [64, 92], "disregard": [49, 54], "dist": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "distinct": [104, 109], "distinguish": [16, 69, 72], "distr": [112, 120], "distribut": [17, 19, 47, 61, 67, 75, 77, 78, 80, 100, 103, 124, 126, 130, 157, 165, 169, 171, 172], "diverg": [46, 61, 80, 103], "divid": [69, 72, 124], "dmatrix": [81, 82, 111], "dml": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 60, 61, 65, 66, 70, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 95, 96, 97, 98, 99, 100, 101, 102, 103, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117, 118, 124, 125, 126, 127, 129, 140, 156, 157, 163, 169], "dml1": [110, 170, 172, 173], "dml2": [60, 63, 65, 66, 73, 91, 110, 124, 140, 156, 170, 172, 173], "dml_apo": 88, "dml_apo_obj": 124, "dml_apos_att": 88, "dml_apos_obj": 124, "dml_apos_tun": 76, "dml_apos_untun": 76, "dml_combin": 156, "dml_cover": 98, "dml_cv_predict": 172, "dml_cvar": [83, 93], "dml_cvar_0": 83, "dml_cvar_1": 83, "dml_cvar_obj": [24, 111], "dml_data": [7, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 63, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 87, 88, 91, 94, 98, 99, 100, 101, 105, 106, 107, 108, 109, 111, 112, 117, 118, 122, 123, 124, 125, 127, 129, 156, 173], "dml_data_anticip": [69, 72], "dml_data_arrai": [108, 109], "dml_data_bench": 100, "dml_data_bonu": [65, 170], "dml_data_df": 173, "dml_data_fuzzi": 97, "dml_data_lasso": 73, "dml_data_sharp": 97, "dml_data_sim": [65, 170], "dml_df": [63, 91], "dml_did": [67, 68], "dml_did_obj": [12, 15, 16, 124, 125, 126, 127, 129], "dml_iivm": 98, "dml_iivm_boost": [64, 92], "dml_iivm_forest": [64, 92], "dml_iivm_lasso": [64, 92], "dml_iivm_obj": [25, 79, 124], "dml_iivm_tre": [64, 92], "dml_irm": [75, 81, 85, 88, 95], "dml_irm_at": 87, "dml_irm_att": 88, "dml_irm_boost": [64, 92], "dml_irm_forest": [64, 92], "dml_irm_gat": 87, "dml_irm_gatet": 87, "dml_irm_lasso": [64, 73, 92], "dml_irm_new": 95, "dml_irm_obj": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 99, 111, 112, 114, 124], "dml_irm_obj_ext": [112, 114], "dml_irm_rf": 73, "dml_irm_tre": [64, 92], "dml_irm_weighted_att": 88, "dml_kwarg": 88, "dml_length": 98, "dml_long": 57, "dml_lplr": 89, "dml_lplr_obj": [37, 124], "dml_lpq_0": 96, "dml_lpq_1": 96, "dml_lpq_obj": [27, 111], "dml_lqte": [93, 96], "dml_obj": [62, 69, 70, 71, 72, 77, 78, 99, 100], "dml_obj_al": [69, 72], "dml_obj_anticip": [69, 72], "dml_obj_bench": 100, "dml_obj_lasso": 70, "dml_obj_linear": [69, 72], "dml_obj_linear_logist": 70, "dml_obj_nyt": [69, 72], "dml_obj_tun": 76, "dml_obj_tuned_pipelin": 76, "dml_obj_univers": [69, 72], "dml_obj_untun": 76, "dml_obj_untuned_pipelin": 76, "dml_pliv": [63, 91], "dml_pliv_obj": [38, 63, 91, 124], "dml_plr": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 82, 86, 156], "dml_plr_1": 156, "dml_plr_2": 156, "dml_plr_boost": [64, 92], "dml_plr_forest": [64, 92, 173], "dml_plr_lasso": [64, 73, 92], "dml_plr_no_split": 139, "dml_plr_obj": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 99, 102, 111, 112, 113, 116, 117, 118, 120, 121, 122, 123, 124, 139, 140, 156, 157, 158, 163], "dml_plr_obj_extern": 139, "dml_plr_obj_intern": 139, "dml_plr_obj_onfold": 90, "dml_plr_obj_untun": 90, "dml_plr_rf": 73, "dml_plr_tree": [64, 92, 173], "dml_pq_0": [93, 96], "dml_pq_1": [93, 96], "dml_pq_obj": [28, 111], "dml_procedur": [73, 102, 170, 172, 173], "dml_qte": [93, 96], "dml_qte_obj": [29, 111], "dml_robust_confset": 98, "dml_robust_length": 98, "dml_short": 57, "dml_ssm": [66, 101, 124], "dml_standard_ci": 98, "dml_tune": 172, "dmldummyclassifi": [112, 114], "dmldummyregressor": [112, 114], "dmlmt": 171, "dmloptunaresult": 76, "dnorm": 61, "do": [62, 63, 64, 65, 69, 70, 72, 75, 88, 91, 92, 93, 94, 100, 111, 112, 114, 116, 139, 157, 167, 170, 173], "doabl": 140, "doc": [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 168, 172], "docu": 172, "documen": 76, "document": [68, 69, 70, 71, 72, 74, 76, 77, 81, 82, 85, 86, 88, 89, 90, 100, 124, 130, 140, 157, 168, 172], "doe": [16, 23, 29, 62, 63, 64, 68, 69, 72, 76, 77, 78, 88, 91, 92, 94, 97, 99, 100, 124, 140, 144, 157, 167, 173], "doesn": [60, 79], "doi": [10, 11, 17, 18, 19, 31, 33, 36, 40, 41, 43, 44, 62, 63, 65, 84, 91, 100, 103, 112, 122, 139, 156, 168, 170, 172], "domain": 95, "don": [62, 90], "done": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 70, 90, 93, 112, 117, 118, 123, 139, 157, 158], "dosag": [77, 78], "dot": [30, 68, 69, 72, 95, 104, 109, 111, 112, 117, 118, 124, 128, 130, 131, 156, 170], "doubl": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 41, 43, 44, 45, 46, 64, 70, 75, 84, 90, 92, 94, 98, 110, 112, 139, 140, 156, 157, 158, 172], "double_ml": [69, 98], "double_ml_bonus_data": 73, "double_ml_data_from_data_fram": [61, 103, 104, 109, 173], "double_ml_data_from_matrix": [62, 65, 104, 109, 112, 122, 123, 156, 170], "double_ml_framework": [124, 125], "double_ml_irm": 73, "double_ml_score_mixin": 0, "doubleiivm": 168, "doubleml": [0, 61, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 93, 94, 95, 96, 97, 98, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 139, 140, 157, 163, 170, 171, 172], "doubleml2022python": 168, "doubleml2024r": 168, "doubleml_did_eval_linear": 62, "doubleml_did_eval_rf": 62, "doubleml_did_linear": 62, "doubleml_did_rf": 62, "doubleml_framework": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "doublemlapo": [77, 78, 88, 124, 140, 145, 172], "doublemlblp": [22, 26, 39, 81, 82, 88, 111, 172], "doublemlclusterdata": [0, 104, 109], "doublemlcvar": [83, 111, 140, 146, 172], "doublemldata": [0, 4, 7, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 60, 63, 65, 66, 67, 68, 73, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 105, 106, 107, 108, 110, 111, 112, 113, 114, 116, 117, 118, 120, 121, 124, 126, 139, 140, 156, 157, 163, 172, 173], "doublemldid": [67, 68, 124, 126, 140, 143, 172], "doublemldidaggreg": [69, 70, 71, 72, 124, 125], "doublemldidbinari": 68, "doublemldidc": [67, 124, 126, 140, 141, 172], "doublemldiddata": [0, 12, 15, 18, 67, 68, 105, 124, 126], "doublemldidmulti": [69, 70, 71, 72, 76, 124, 125, 127, 128, 129, 140, 142, 144, 172], "doublemlframework": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 69, 70, 71, 72, 124, 125, 139, 156, 172], "doublemlframwork": 23, "doublemlidid": [124, 126], "doublemlididc": [124, 126], "doublemliivm": [60, 64, 79, 92, 98, 112, 115, 116, 119, 121, 124, 139, 140, 147, 172], "doublemlirm": [12, 14, 15, 22, 24, 25, 27, 28, 30, 37, 38, 39, 62, 64, 73, 75, 76, 78, 81, 85, 87, 88, 92, 94, 95, 99, 100, 111, 112, 114, 115, 116, 119, 121, 124, 139, 140, 148, 168, 172], "doublemllplr": [89, 124, 140, 151, 172], "doublemllpq": [96, 111, 140, 149, 172], "doublemlpaneldata": [0, 14, 16, 68, 70, 71, 106, 124, 125, 127, 128, 129, 172], "doublemlpliv": [112, 116, 121, 124, 139, 140, 152, 168, 172], "doublemlplr": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 61, 64, 65, 73, 80, 82, 86, 90, 92, 94, 99, 102, 103, 111, 112, 113, 116, 117, 118, 119, 120, 121, 122, 123, 124, 139, 140, 153, 156, 157, 163, 168, 170, 172, 173], "doublemlpolicytre": [26, 111], "doublemlpq": [93, 96, 111, 140, 150, 172], "doublemlqt": [83, 93, 96, 111, 156, 172], "doublemlrdddata": [0, 46, 97, 107, 124, 172], "doublemlresampl": [75, 88], "doublemlsmm": 172, "doublemlssm": [66, 101, 124, 140, 154, 155, 172], "doublemlssmdata": [0, 30, 36, 101, 108, 124, 172], "doubli": [18, 31, 40, 62, 70, 98, 171], "doudou": [41, 171], "down": [89, 100], "download": [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 104, 109, 169, 170], "download_fil": 70, "downward": 100, "dpg_dict": 99, "dpi": [61, 80, 94], "dr": [124, 128], "dramat": 62, "draw": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 100, 139, 172], "draw_sample_split": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 75, 88, 139], "drawn": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 47, 64, 69, 72, 92, 93, 95, 139], "drive": [61, 80, 103], "driven": [100, 173], "drop": [62, 90, 91, 94, 104, 109, 112, 122, 123, 140, 141, 142, 143, 144, 156], "dropna": [69, 72], "dt": [69, 140, 141, 157, 159], "dt_bonu": [104, 109], "dta": [62, 71], "dtrain": 92, "dtype": [69, 70, 71, 72, 73, 76, 77, 78, 85, 86, 87, 89, 91, 92, 93, 98, 99, 104, 106, 109, 111, 170], "dualiti": 91, "dubourg": [168, 170], "duchesnai": [168, 170], "due": [61, 62, 70, 80, 81, 82, 87, 89, 99, 100, 103, 124, 138, 157, 158, 172, 173], "duflo": [10, 11, 44, 63, 84, 91, 98, 103, 139, 168, 171], "dummi": [22, 26, 39, 48, 49, 51, 70, 90, 100, 111, 112, 114, 124, 126, 172], "dummyclassifi": [48, 70], "dummyregressor": [49, 70], "duplic": 172, "durabl": [65, 73, 104, 109, 170], "durat": [11, 76], "dure": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 62, 63, 64, 65, 66, 90, 91, 92, 112, 116, 121, 123, 139, 170, 172, 173], "dx": 33, "dynam": [62, 171], "e": [6, 7, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 36, 37, 38, 39, 40, 44, 46, 48, 49, 50, 53, 54, 61, 62, 63, 64, 66, 67, 69, 70, 71, 72, 75, 76, 77, 78, 80, 81, 82, 84, 87, 89, 90, 91, 92, 93, 94, 97, 98, 99, 100, 101, 103, 106, 109, 111, 112, 113, 114, 116, 121, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173], "e20ea26": 65, "e401": [64, 92, 93, 99, 173], "e4016553": 173, "e45228": 94, "e57c": 65, "each": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 48, 49, 53, 54, 63, 65, 68, 69, 70, 71, 72, 75, 76, 77, 78, 85, 86, 89, 90, 91, 93, 94, 95, 98, 99, 100, 102, 104, 106, 109, 112, 113, 116, 121, 123, 124, 130, 139, 156, 157, 163, 173], "earlier": [69, 72, 173], "earn": [64, 92, 93], "earner": [64, 92, 99], "easi": [65, 98, 140], "easier": 90, "easili": [65, 75, 90, 93, 172], "ec973f": 94, "ecolor": [68, 76, 77, 78, 92, 94], "econ": 171, "econml": 171, "econom": [36, 42, 43, 45, 63, 84, 91, 94, 100, 139, 171], "econometr": [10, 11, 17, 18, 19, 31, 40, 41, 44, 45, 62, 63, 84, 91, 98, 103, 168, 171], "econometrica": [34, 63, 91, 94, 98, 103, 171], "ecosystem": [168, 173], "ectj": [10, 11, 41, 44, 63, 84, 91, 103, 168], "ed": 171, "edge_color": 80, "edgecolor": 80, "edit": [169, 171], "edu": [168, 170], "educ": [64, 92, 93, 99, 173], "ee97bda7": 65, "effect": [12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 47, 48, 49, 53, 54, 60, 61, 62, 63, 65, 66, 67, 68, 76, 78, 79, 80, 84, 87, 91, 95, 97, 98, 101, 103, 110, 112, 120, 121, 122, 123, 125, 126, 128, 130, 132, 133, 134, 138, 139, 140, 148, 156, 157, 158, 170, 171, 172, 173], "effici": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 77, 124, 171], "effort": 140, "eight": [63, 91], "either": [12, 14, 15, 16, 17, 19, 22, 24, 25, 26, 27, 28, 30, 32, 37, 38, 39, 65, 68, 69, 72, 84, 95, 97, 111, 112, 113, 118, 119, 123, 124, 127, 130, 173], "elapsed_tim": 77, "eleanor": 171, "element": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 63, 66, 67, 69, 70, 71, 72, 75, 81, 82, 83, 91, 93, 96, 99, 101, 124, 127, 128, 129, 140, 141, 142, 143, 144, 145, 151, 157, 160, 162, 163, 166, 167, 172], "element_text": [63, 64], "elementari": 171, "elif": [85, 86, 95], "elig": [93, 99, 173], "eligibl": [64, 92, 99], "ell": [61, 63, 80, 84, 91, 103, 140, 152, 153, 170], "ell_0": [25, 38, 39, 61, 80, 84, 90, 103, 124, 133, 140, 153], "ell_1": 70, "ell_2": 75, "els": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 51, 62, 63, 64, 68, 76, 77, 85, 86, 91, 95, 100], "em": 171, "emphas": [63, 91], "empir": [20, 21, 61, 63, 70, 80, 91, 94, 100, 103, 124, 127, 128, 129, 139, 140, 156], "emploi": [63, 76, 84, 91, 100, 140, 147], "employ": [64, 70, 92, 93], "employe": 173, "empti": 91, "emul": [157, 158], "enabl": [13, 76, 77, 78, 95, 99, 111, 157, 158, 172], "enable_metadata_rout": [48, 49, 53, 54], "encapsul": [48, 49, 53, 54, 112, 120, 121], "encod": 94, "encount": [72, 77], "end": [33, 41, 42, 43, 61, 62, 63, 64, 66, 68, 69, 72, 75, 80, 83, 84, 91, 92, 94, 95, 96, 101, 102, 104, 109, 112, 121, 123, 124, 127, 129, 130, 139, 140, 142, 144, 156, 170, 173], "end_tim": 77, "endogen": [64, 92, 93, 173], "enet_coordinate_descent_gram": 91, "enforc": 70, "engin": [65, 171], "enrol": [64, 92, 93], "ensembl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 73, 75, 76, 77, 78, 80, 81, 82, 85, 86, 87, 89, 92, 95, 99, 100, 102, 111, 112, 113, 114, 115, 116, 120, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170, 173], "ensemble_learner_pipelin": [112, 120], "ensemble_pipe_classif": 65, "ensemble_pipe_regr": 65, "ensur": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 53, 54, 63, 77, 88, 90, 91, 95, 98, 107, 109], "enter": 124, "entir": [16, 61, 64, 80, 92, 103, 157, 158], "entri": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 47, 61, 63, 69, 70, 71, 72, 73, 76, 77, 78, 80, 87, 89, 91, 92, 93, 99, 103, 104, 106, 109, 112, 121, 123, 168, 170, 172], "enumer": [68, 75, 76, 77, 78, 83, 85, 86, 91, 92, 93, 96, 102, 112, 121, 123, 139], "env": 169, "environ": [76, 169], "ep": 94, "epanechnikov": 46, "epsilon": [64, 67, 68, 83, 92, 96, 111, 124, 126, 130], "epsilon_": [63, 68, 69, 72, 91], "epsilon_0": 47, "epsilon_1": 47, "epsilon_i": [32, 83, 94, 95, 96], "epsilon_sampl": 95, "epsilon_tru": [83, 96], "eqnarrai": 64, "equal": [13, 17, 19, 22, 23, 26, 63, 66, 69, 72, 91, 94, 98, 101, 111, 112, 123, 124, 157, 165], "equat": [47, 63, 64, 91, 92, 100, 102, 156, 173], "equilibrium": [63, 91], "equiv": [124, 130, 140, 142, 144], "equival": [84, 88, 139], "err": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 67, 69, 70, 71, 72, 73, 76, 77, 78, 79, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 99, 100, 101, 111, 112, 113, 114, 116, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 170, 173], "error": [5, 6, 7, 8, 9, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 53, 54, 60, 61, 62, 64, 65, 66, 68, 75, 76, 77, 80, 84, 85, 86, 90, 92, 97, 100, 103, 112, 113, 114, 120, 121, 122, 123, 124, 136, 137, 139, 140, 156, 157, 163, 170, 172, 173], "error_on_convergence_failur": 37, "errorbar": [68, 76, 77, 78, 85, 86, 90, 92, 94, 97], "erstellt": [63, 64, 65], "es_linear_logist": 70, "es_rf": 70, "escap": 91, "esim": 97, "especi": [75, 90, 104, 109], "essenti": 100, "est": 97, "est_bound": 16, "est_method": 62, "esther": [139, 171], "estim": [10, 12, 13, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 46, 48, 49, 50, 51, 52, 53, 54, 61, 62, 63, 65, 68, 75, 76, 78, 80, 81, 82, 83, 85, 86, 88, 89, 91, 95, 97, 98, 102, 103, 110, 111, 112, 114, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 132, 134, 141, 142, 143, 146, 149, 150, 151, 155, 157, 158, 163, 168, 171, 172], "estimand": 98, "estimator_list": 90, "et": [10, 11, 32, 34, 41, 43, 44, 61, 63, 64, 65, 67, 75, 80, 81, 82, 83, 84, 85, 86, 89, 91, 92, 93, 96, 99, 103, 124, 126, 130, 139, 140, 146, 148, 149, 150, 151, 156, 157, 158, 167, 168, 170, 172], "eta": [20, 21, 61, 63, 64, 68, 90, 91, 92, 96, 97, 102, 111, 124, 127, 128, 129, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 167, 170, 173], "eta1": 94, "eta2": 94, "eta_": [156, 157, 167], "eta_0": [46, 102, 124, 127, 128, 129, 140, 156], "eta_d": [97, 124], "eta_i": [17, 19, 32, 68, 69, 72, 95, 96, 97, 124], "eta_sampl": 95, "eta_tru": 96, "etc": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 63, 75, 90, 91, 172], "ev": [61, 80, 103], "eval": [16, 65, 69, 70, 71, 72, 112, 122, 123, 124, 127, 128, 129, 140, 142, 144], "eval_metr": [64, 92, 173], "eval_pr": 62, "eval_predict": 62, "evalu": [12, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 34, 37, 38, 39, 62, 65, 68, 69, 70, 71, 72, 81, 82, 83, 87, 88, 93, 96, 99, 102, 113, 114, 123, 124, 128, 130, 171, 172], "evaluate_learn": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 75, 76, 90, 112, 113, 172], "evalut": [112, 113], "even": [64, 65, 69, 72, 76, 92, 94, 97, 112, 120, 124, 173], "event": [124, 125], "eventstudi": [16, 69, 70, 71, 72, 124, 125], "eventu": [63, 91], "everi": [63, 70, 91], "everyth": 168, "evid": [87, 90], "exact": [88, 100], "exactli": [97, 100, 124], "examin": 77, "exampl": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 46, 55, 56, 60, 61, 64, 65, 66, 67, 68, 69, 71, 72, 73, 75, 77, 78, 79, 80, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 103, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 127, 128, 129, 138, 139, 140, 156, 157, 163, 168, 170, 172, 173], "example_attgt": 62, "example_attgt_dml_eval_linear": 62, "example_attgt_dml_eval_rf": 62, "example_attgt_dml_linear": 62, "example_attgt_dml_rf": 62, "except": [49, 54, 84, 100, 172], "excess": 75, "exclud": 57, "exclus": [22, 26, 39, 85, 86, 111], "execut": [65, 173], "exemplarili": 170, "exemplatori": 95, "exhaust": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 112, 118], "exhibit": [63, 91], "exist": [48, 49, 53, 54, 88, 124, 126, 130, 157, 167], "exogen": [64, 92, 93, 124, 173], "exp": [17, 18, 19, 31, 32, 34, 35, 40, 41, 44, 61, 68, 80, 81, 82, 85, 86, 94, 95, 103], "expect": [31, 37, 40, 49, 54, 62, 66, 67, 69, 70, 72, 75, 78, 87, 89, 90, 97, 100, 101, 107, 108, 109, 111, 124, 139, 156, 157, 164, 170], "experi": [11, 33, 34, 61, 64, 80, 92, 98, 100, 103, 104, 109, 139, 170, 171], "experiment": [12, 14, 15, 16, 17, 18, 19, 140, 141, 142, 143, 144, 157, 159, 160, 161, 162], "expertis": 100, "expit": [37, 89, 124, 135, 140, 151], "explain": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 76, 99, 157, 158, 166, 167], "explan": [63, 67, 91, 99, 157, 166, 168, 173], "explanatori": [100, 156], "explicit": 100, "explicitli": [69, 71, 72, 87, 173], "exploit": [61, 80, 103, 124, 173], "explor": [76, 90], "exponenti": 156, "export": [90, 172], "expos": [104, 105, 107, 108, 109, 124, 130], "exposur": [7, 17, 19, 68, 69, 70, 71, 72], "express": [63, 84, 97, 157, 167], "ext": [16, 17, 19], "extend": [89, 100, 105, 108, 109, 112, 119, 168, 172], "extendend": [157, 167], "extens": [112, 114, 119, 140, 168, 171, 172], "extent": 84, "extern": [61, 80, 88, 90, 110, 114, 118, 121, 123, 157, 158, 172], "external_predict": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 80, 112, 114], "externalptr": 64, "extra": 65, "extract": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 77, 90], "extralearn": 65, "extrem": [55, 56, 64, 92], "extreme_threshold": [55, 56], "ey": 84, "ezequiel": 172, "f": [64, 65, 67, 68, 69, 72, 75, 76, 77, 78, 80, 83, 84, 91, 92, 93, 95, 96, 98, 99, 100, 101, 112, 122, 157, 167, 168, 170], "f00584a57972": 65, "f1718fdeb9b0": 65, "f2e7": 65, "f3d24993": 65, "f6ebc": 94, "f_": [17, 18, 19, 31, 68, 111], "f_loc": [83, 96], "f_p": 68, "f_scale": [83, 96], "f_t": [69, 72], "f_x": 124, "face_color": 80, "facet_wrap": 64, "facilit": 90, "fact": [64, 92, 93], "factor": [47, 61, 62, 63, 64, 65, 75, 80, 103, 112, 120, 121, 157, 160, 162, 173], "faculti": 171, "fail": 172, "failur": 37, "fair": 75, "fake": [60, 79, 98], "fall": 77, "fallback": [112, 120, 121], "fals": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 41, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 61, 64, 65, 66, 67, 69, 72, 75, 76, 77, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 95, 96, 97, 98, 100, 101, 104, 109, 112, 121, 122, 123, 124, 127, 139, 140, 141, 142, 143, 144, 156, 157, 159, 160, 161, 162, 173], "famili": [64, 92, 112, 120, 123], "familiar": 98, "fanci": 62, "far": [64, 92], "farbmach": 33, "fast": [75, 76, 95, 112, 119], "faster": 84, "fb5c25fa": 65, "fc9e": 65, "fd8a": 65, "featur": [10, 11, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 49, 52, 54, 62, 68, 69, 72, 73, 75, 77, 87, 88, 92, 95, 97, 100, 107, 109, 111, 112, 114, 120, 121, 124], "featureless": [65, 112, 120], "features_bas": [64, 92, 93, 99], "features_flex": 64, "featureunion": 65, "februari": [69, 100], "feder": 70, "femal": [65, 73, 104, 109, 170], "fern\u00e1ndez": [34, 139, 171], "fetch": [64, 91, 92, 93, 104, 109], "fetch_401k": [64, 92, 93, 99, 173], "fetch_bonu": [65, 73, 104, 109, 170], "few": [64, 92, 93], "ff7f0e": 68, "field": [63, 91, 112, 121, 173], "fifteenth": 171, "fifth": [63, 69, 72], "fig": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 68, 69, 70, 71, 72, 75, 76, 77, 78, 81, 82, 83, 84, 88, 90, 93, 94, 96, 97, 100], "fig_al": 80, "fig_dml": 80, "fig_non_orth": 80, "fig_orth_nosplit": 80, "fig_po_al": 80, "fig_po_dml": 80, "fig_po_nosplit": 80, "figsiz": [13, 16, 68, 69, 72, 73, 75, 76, 77, 78, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 96, 97], "figur": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 44, 61, 63, 68, 69, 70, 72, 73, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 96, 100, 103], "figure_format": 94, "file": [10, 11, 70, 84, 94, 171, 172], "filenam": 61, "fill": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 61, 63, 64, 66, 67, 75, 92, 101], "fill_between": [81, 82, 83, 88, 93, 96], "fill_valu": 75, "fillna": 69, "filter": 65, "filterwarn": [75, 76, 77, 80], "final": [61, 65, 66, 68, 69, 71, 72, 76, 78, 80, 81, 82, 83, 85, 86, 87, 93, 96, 97, 101, 103, 124, 138, 140, 142, 144, 173], "final_estim": [76, 97], "final_estimatorridg": 76, "financi": [10, 99, 173], "find": [64, 67, 68, 77, 92, 100, 111, 112, 122, 123, 173], "finish": 65, "finit": [61, 64], "firm": [63, 91, 99], "firmid": 91, "first": [7, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 43, 46, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 78, 79, 80, 81, 82, 85, 86, 87, 88, 89, 91, 92, 93, 95, 96, 97, 100, 101, 103, 111, 124, 125, 128, 130, 139, 156, 157, 163, 169, 170, 172, 173], "fit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 50, 51, 52, 53, 54, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 140, 143, 155, 156, 157, 158, 163, 168, 172, 173], "fit_arg": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39], "fit_transform": [88, 91, 92], "fittedpipelin": 76, "five": 91, "fix": [68, 69, 72, 75, 76, 172], "flag": [18, 104, 109, 139, 169], "flake8": 172, "flamlclassifierdoubleml": 90, "flamlregressordoubleml": 90, "flatten": [90, 94], "flexibl": [46, 60, 62, 64, 65, 67, 79, 92, 124, 168, 171, 172, 173], "flexibli": [64, 70, 92, 99], "float": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 41, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 69, 70, 71, 72, 106, 109], "float32": [92, 93, 99], "float64": [69, 70, 71, 72, 73, 76, 77, 78, 83, 85, 86, 87, 89, 91, 92, 99, 104, 106, 109, 112, 118, 170], "floor": 65, "floor_divid": 91, "flt": 65, "flush": 61, "fmt": [68, 76, 77, 78, 85, 86, 90, 92, 94, 97], "fobj": 92, "focu": [63, 64, 70, 76, 88, 91, 92, 93, 100, 111, 124, 126, 130, 138, 173], "focus": [76, 93, 99, 100, 173], "fold": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 60, 63, 64, 65, 66, 67, 69, 70, 71, 72, 75, 76, 91, 92, 93, 99, 101, 102, 110, 112, 113, 116, 121, 122, 123, 124, 126, 127, 129, 140, 143, 156, 170, 173], "follow": [17, 18, 19, 31, 32, 35, 40, 41, 61, 63, 64, 66, 67, 68, 69, 70, 71, 72, 76, 80, 81, 82, 83, 85, 86, 89, 90, 91, 92, 93, 96, 97, 98, 99, 100, 101, 103, 104, 109, 111, 112, 113, 114, 120, 121, 123, 124, 125, 127, 128, 129, 130, 139, 140, 142, 144, 151, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173], "font_scal": [91, 92, 93], "fontsiz": [69, 72, 83, 89, 93, 96], "force_all_d_finit": [5, 6, 7, 8, 9, 104, 105, 109], "force_all_x_finit": [4, 5, 6, 7, 8, 9, 104, 109], "forest": [33, 60, 61, 62, 64, 65, 67, 75, 77, 79, 80, 87, 92, 99, 103, 112, 120, 121, 123, 170, 173], "forest_summari": 92, "forg": [169, 171, 172], "form": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 40, 51, 53, 54, 64, 66, 67, 68, 75, 81, 82, 83, 85, 86, 87, 92, 96, 97, 99, 101, 111, 124, 125, 126, 130, 133, 134, 135, 136, 137, 140, 142, 144, 145, 148, 157, 158, 163, 164, 165, 166, 167, 169, 170], "format": [7, 16, 41, 70, 77, 80, 87, 157, 163, 172], "former": [70, 98], "formula": [63, 64, 91, 92, 97, 100, 172], "formula_flex": 64, "forschungsgemeinschaft": 168, "forthcom": [100, 171], "forum": 172, "forward": [26, 52], "found": [50, 71, 81, 82, 84, 85, 86, 89, 90, 103, 104, 109, 112, 117, 119, 124, 138, 170], "foundat": [77, 168, 171], "four": [64, 75, 77, 92, 124, 127, 172], "fourth": [63, 91], "frac": [17, 18, 19, 21, 25, 31, 33, 34, 36, 40, 41, 42, 44, 45, 49, 54, 61, 63, 65, 68, 72, 80, 84, 87, 89, 91, 94, 97, 102, 103, 111, 124, 127, 128, 129, 133, 135, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167], "fraction": [19, 65], "frame": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 51, 60, 61, 63, 64, 66, 69, 70, 71, 72, 73, 76, 77, 78, 81, 82, 85, 86, 87, 89, 91, 92, 93, 94, 95, 99, 103, 104, 106, 109, 170, 173], "framealpha": [69, 72], "frameon": [69, 72, 89], "framework": [13, 16, 21, 61, 63, 65, 75, 77, 80, 90, 91, 94, 100, 103, 112, 122, 156, 168, 170, 172, 173], "freez": 169, "fribourg": 171, "friendli": [69, 72, 77, 78], "from": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 51, 54, 55, 56, 60, 61, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 105, 106, 107, 108, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 139, 140, 156, 157, 163, 170, 172, 173], "from_arrai": [4, 5, 6, 7, 8, 9, 30, 46, 67, 68, 80, 83, 96, 103, 104, 105, 107, 108, 109, 112, 117, 118, 156, 170], "from_config": [55, 56], "from_product": 91, "front": 78, "fr\u00e9chet": [157, 167], "fs_kernel": [46, 124], "fs_specif": [46, 124], "fsize": [64, 92, 93, 99, 173], "full": [50, 67, 68, 75, 76, 77, 78, 80, 83, 85, 86, 92, 93, 96, 97, 98, 101, 103, 124], "fulli": [26, 64, 74, 90, 92, 98, 124, 134], "fun": 61, "func": 62, "function": [0, 4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 44, 45, 46, 60, 61, 64, 65, 66, 67, 69, 70, 71, 72, 75, 77, 79, 80, 81, 82, 83, 88, 89, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 110, 112, 113, 114, 115, 116, 117, 118, 119, 121, 123, 124, 125, 126, 127, 128, 129, 130, 133, 135, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 162, 167, 168, 171, 172, 173], "fund": [64, 92, 93, 168], "further": [13, 16, 17, 18, 19, 31, 32, 35, 40, 43, 63, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 78, 81, 82, 83, 87, 88, 89, 91, 93, 95, 96, 97, 99, 100, 101, 112, 113, 114, 115, 116, 121, 124, 126, 129, 138, 140, 146, 149, 150, 151, 154, 155, 156, 157, 158, 163, 166, 167, 168, 170, 172, 173], "furthermor": [80, 106, 109, 140, 145, 148], "futur": [4, 14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 69, 70, 71, 72, 81, 112, 124, 140, 157], "futurewarn": [68, 69, 70, 71, 72], "fuzzi": [46, 47], "g": [6, 7, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 46, 48, 49, 50, 53, 54, 61, 62, 65, 67, 68, 69, 70, 71, 72, 73, 75, 80, 81, 82, 84, 87, 93, 94, 95, 98, 99, 101, 103, 106, 109, 111, 112, 113, 114, 122, 124, 125, 126, 127, 128, 129, 130, 140, 141, 142, 143, 144, 145, 147, 148, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173], "g0": 77, "g1": 77, "g_": [47, 78, 124, 129, 140, 141, 143, 144, 146, 149, 150, 156], "g_0": [12, 14, 15, 16, 22, 23, 25, 26, 28, 38, 39, 44, 45, 46, 47, 61, 63, 64, 75, 77, 80, 91, 92, 103, 111, 112, 116, 121, 124, 131, 132, 133, 134, 136, 137, 140, 142, 144, 145, 153, 154, 155, 157, 164, 165, 167, 170, 173], "g_1": [47, 75], "g_all": [61, 64], "g_all_po": 61, "g_ci": 64, "g_d": [140, 146, 150], "g_dml": 61, "g_dml_po": 61, "g_hat": [38, 39, 61, 80, 140], "g_hat0": [25, 26], "g_hat1": [25, 26], "g_i": [17, 19, 124, 127, 129, 130, 140, 142, 144], "g_k": 111, "g_nonorth": 61, "g_nosplit": 61, "g_nosplit_po": 61, "g_valu": 14, "g_x": 68, "gain": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 57, 75, 157, 158, 165, 172], "gain_statist": 172, "galleri": [75, 76, 103, 111, 112, 113, 117, 120, 124, 125, 127, 129, 138, 168, 172], "gama": 90, "gamma": [36, 42, 45, 63, 91, 94, 95, 97, 100, 124, 140, 146, 149], "gamma_0": [32, 66, 95, 101, 140, 146, 149], "gamma_a": [31, 40, 100], "gamma_bench": 100, "gamma_v": 100, "gap": [91, 100], "gapo": 22, "gate": [22, 26, 39, 51, 94, 95, 110, 172], "gate_obj": 111, "gatet": 111, "gaussian": [27, 28, 29, 61, 80, 103, 111, 112, 120, 123, 156, 171], "ge": [18, 31, 32, 87, 95, 111, 124, 128, 130], "geer": 171, "gelbach": [63, 91], "gener": [0, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 49, 54, 55, 60, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 75, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 102, 104, 109, 110, 111, 112, 114, 117, 118, 120, 122, 123, 124, 127, 129, 130, 131, 139, 140, 142, 144, 145, 148, 156, 158, 159, 160, 161, 162, 164, 165, 167, 171, 172, 173], "generate_treat": 96, "generate_weakiv_data": 98, "geom_bar": 64, "geom_dens": [64, 66], "geom_errorbar": 64, "geom_funct": 61, "geom_histogram": 61, "geom_hlin": 64, "geom_point": 64, "geom_til": 63, "geom_vlin": [61, 66], "geq": [17, 19, 97, 124], "german": 168, "get": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 53, 54, 65, 69, 71, 72, 75, 76, 77, 78, 94, 99, 100, 157, 158, 168, 169], "get_dummi": 94, "get_feature_names_out": [88, 91, 92], "get_legend_handles_label": [77, 78], "get_level_valu": 90, "get_logg": [61, 62, 63, 64, 65, 66, 102, 112, 121, 122, 123, 124, 139, 140, 156, 170], "get_metadata_rout": [48, 49, 53, 54], "get_n_split": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "get_param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 53, 54, 90, 112, 115], "get_ylim": 88, "ggdid": 62, "ggplot": [61, 63, 64, 66], "ggplot2": [61, 63, 64, 66], "ggsave": 61, "ggtitl": 64, "gh": 172, "git": 169, "github": [62, 64, 70, 76, 84, 90, 94, 168, 171, 172], "githubusercont": [71, 84], "give": [64, 88, 92], "given": [12, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 34, 37, 38, 39, 40, 44, 45, 46, 47, 48, 49, 53, 54, 61, 63, 66, 68, 70, 71, 76, 78, 80, 85, 86, 91, 93, 94, 97, 100, 101, 103, 111, 124, 127, 128, 129, 140, 145, 156, 157, 163, 164, 165, 166, 167, 170, 172], "glmnet": [64, 65, 112, 122, 123, 172], "global": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 53, 54, 112, 121, 123, 124], "globalclassifi": 97, "globallearn": 97, "globalregressor": 97, "glrn": 65, "glrn_lasso": 65, "gmm": 98, "gname": 62, "go": [81, 82, 84, 88, 90, 97, 100], "goal": [76, 78, 85, 86, 124], "goe": 124, "goldman": 171, "good": [84, 89, 157, 158, 173], "gpu": 77, "gradient": [64, 92], "gradientboostingclassifi": 75, "gradientboostingregressor": 75, "gradual": 100, "gramfort": [168, 170], "grant": 168, "graph": [65, 66, 101, 173], "graph_ensemble_classif": 65, "graph_ensemble_regr": 65, "graph_obj": 97, "graph_object": [81, 82, 84, 100], "graphlearn": [65, 112, 120], "grasp": [78, 157, 158], "great": [68, 173], "greater": 173, "green": [61, 81, 82, 83, 96], "greg": 171, "grei": [64, 77, 78], "grenand": 171, "grey50": 63, "grid": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 63, 65, 69, 72, 76, 77, 78, 81, 82, 83, 84, 88, 89, 93, 94, 96, 100, 122, 123, 157, 163], "grid_arrai": [81, 82], "grid_basi": 88, "grid_bound": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 100], "grid_search": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 65, 112, 118, 122, 123], "grid_siz": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 81, 82], "gridextra": 63, "gridsearchcv": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 112, 118], "grisel": [168, 170], "grk": 168, "grob": 63, "group": [7, 14, 16, 17, 19, 22, 26, 39, 60, 62, 70, 76, 77, 78, 79, 87, 88, 93, 94, 95, 100, 110, 124, 125, 127, 128, 129, 130, 140, 142, 144, 157, 160, 162], "group_0": 111, "group_1": [85, 86, 111], "group_2": [85, 86, 111], "group_3": [85, 86], "group_effect": 95, "group_ind": 87, "group_treat": 87, "groupbi": [69, 72, 77, 84, 92, 98], "gruber": 33, "gt": [60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 76, 77, 78, 79, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 104, 109, 170], "gt_combin": [16, 69, 70, 72, 124, 125, 127, 128, 129], "gt_dict": [69, 72], "guarante": [63, 91], "guber": 33, "guess": [99, 157, 158], "guid": [20, 21, 48, 49, 53, 54, 61, 62, 63, 65, 68, 69, 70, 71, 72, 78, 80, 87, 88, 91, 97, 99, 112, 119, 168, 170, 172], "guidelin": 172, "gunion": [65, 112, 120], "gxidclusterperiodytreat": 62, "h": [17, 18, 19, 31, 33, 40, 43, 62, 63, 91, 97, 98, 106, 109, 124, 171], "h20": 90, "h_0": [69, 72, 78, 87, 88, 99, 100, 157, 163, 173], "h_f": [46, 124], "ha": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 51, 52, 53, 54, 61, 62, 63, 64, 70, 72, 75, 76, 80, 84, 90, 91, 92, 93, 94, 97, 98, 99, 100, 104, 109, 111, 112, 113, 114, 124, 126, 130, 157, 158, 163, 164, 165, 166, 167, 168, 173], "had": 70, "half": [61, 80, 94, 103, 139], "hand": [46, 75, 90, 94, 98, 173], "handbook": 94, "handl": [62, 70, 75, 76, 77, 78, 105, 106, 109, 112, 113, 114, 172], "hansen": [10, 11, 34, 42, 44, 63, 84, 91, 98, 103, 168, 171], "happend": 75, "hard": [99, 157, 158], "harold": 171, "harsh": [48, 53], "hasn": [13, 16, 69, 71, 72], "hat": [61, 63, 80, 84, 87, 91, 94, 97, 102, 103, 111, 124, 139, 140, 156, 157, 158, 163, 166], "have": [16, 22, 23, 26, 29, 32, 35, 39, 51, 52, 53, 54, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 81, 82, 87, 88, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 104, 109, 111, 112, 114, 119, 120, 121, 124, 128, 140, 142, 144, 156, 157, 158, 164, 167, 169, 170, 172, 173], "hazlett": [100, 157, 158], "hc": [62, 171], "hc0": [51, 172], "hdm": [63, 91], "he": [66, 101], "head": [62, 63, 65, 69, 70, 71, 72, 73, 81, 82, 85, 86, 88, 90, 91, 92, 94, 97, 100, 104, 105, 109, 111, 170], "heat": [63, 91], "heatmap": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 63, 91, 100], "heavili": 75, "hei": 171, "height": [13, 16, 61, 63, 84, 90], "help": [62, 64, 70, 75, 83, 93, 95, 100, 124, 125, 139, 173], "helper": [105, 109, 172], "henc": [62, 64, 65, 92, 100, 112, 123, 140, 173], "here": [27, 28, 29, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 81, 82, 83, 85, 86, 87, 89, 91, 92, 93, 95, 96, 97, 100, 101, 104, 109, 112, 114, 117, 124, 127, 169], "herzig": 172, "heterogen": [17, 19, 26, 32, 41, 64, 87, 92, 93, 95, 110, 124, 134, 139, 171, 172, 173], "heteroskedast": [85, 86], "heurist": [61, 80, 103], "high": [34, 37, 38, 39, 64, 68, 84, 92, 93, 98, 102, 124, 135, 136, 137, 156, 168, 170, 171], "higher": [62, 64, 84, 92, 93, 94, 97, 98, 172, 173], "highli": [64, 92, 168], "highlight": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 67, 75, 76, 88, 90, 100, 172], "highlightcolor": [81, 82], "hint": 90, "hispan": 73, "hist": [77, 78], "hist_e401": 64, "hist_p401": 64, "histogram": [77, 78], "histori": 50, "histplot": 80, "hjust": 64, "hline": [104, 109, 156, 170, 173], "hold": [36, 50, 56, 63, 64, 66, 89, 90, 91, 92, 101, 111, 112, 123, 124, 128], "holdout": [112, 123, 139], "holm": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "home": [64, 68, 69, 71, 72, 92, 98, 100], "homogen": 124, "hook": 172, "hopefulli": 93, "horizont": [63, 68, 76, 77, 91], "hostedtoolcach": [68, 69, 70, 71, 72, 91, 92, 97, 100], "hot": 94, "hotstart_backward": [65, 112, 120, 121], "hotstart_forward": [65, 112, 120], "household": [64, 92, 93, 99], "how": [13, 17, 19, 41, 48, 49, 53, 54, 60, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 95, 96, 97, 98, 99, 100, 101, 103, 105, 109, 112, 119, 120, 121, 123, 124, 168, 169], "howev": [61, 64, 66, 80, 89, 90, 92, 97, 100, 101, 103, 124, 173], "hown": [64, 92, 93, 99, 173], "hpwt": [63, 91], "hpwt0": 63, "hpwtairmpdspac": 63, "href": 168, "hspace": 75, "hstack": [30, 68], "html": [65, 76, 168, 170, 172], "http": [33, 45, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 112, 122, 168, 169, 170, 172], "huber": [36, 66, 101, 124, 138, 140, 154, 155, 171], "hue": [69, 72, 92], "huge": 75, "hugo": 171, "husd": [65, 73, 104, 109, 170], "hyperparamet": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 64, 65, 73, 75, 77, 84, 90, 92, 110, 114, 116, 117, 118, 119, 120, 121, 123, 170, 172], "hypothes": [156, 171], "hypothesi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 64, 92, 99, 157, 163, 171], "hypothet": 100, "i": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 52, 53, 54, 55, 56, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 155, 156, 157, 158, 160, 162, 163, 164, 165, 167, 168, 169, 170, 172, 173], "i0": [67, 68, 124, 126], "i03": 168, "i1": [67, 124, 126], "i_": [42, 91, 95], "i_1": [63, 91], "i_2": [63, 91], "i_3": [63, 91], "i_4": 68, "i_est": 80, "i_fold": 63, "i_k": [63, 91, 102, 139, 156], "i_learn": 75, "i_level": 78, "i_rep": [61, 66, 67, 75, 80, 101, 103], "i_split": 91, "i_train": 80, "icp": 171, "id": [7, 16, 62, 63, 65, 69, 70, 71, 72, 91, 106, 109, 124, 125, 127, 129, 157, 162], "id_col": [7, 16, 69, 70, 71, 72, 106, 109, 124, 125, 127, 129], "id_var": 91, "idea": [64, 65, 76, 92, 93, 100, 112, 122, 124, 157, 158, 173], "ideal": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "ident": [17, 18, 19, 31, 32, 35, 40, 42, 52, 65, 70, 76, 78, 88, 90, 97, 112, 114, 124, 130, 140, 148, 157, 163], "identfi": 100, "identif": [69, 70, 71, 72, 97, 98, 124, 173], "identifi": [7, 63, 64, 67, 70, 77, 87, 91, 92, 93, 97, 100, 105, 106, 107, 108, 109, 111, 124, 126, 128, 130, 138, 157, 167, 172], "identifii": 111, "idnam": 62, "idx_gt_att": 16, "idx_learn": 77, "idx_tau": [83, 93, 96], "idx_treat": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 77, 78, 157, 163], "ieee": 171, "ifels": 62, "ignor": [48, 49, 53, 54, 70, 75, 76, 77, 80, 97], "ignore_index": 77, "ii": [63, 91], "iid": [69, 72, 124, 126], "iivm": [20, 21, 25, 33, 93, 102, 111, 133, 147, 168, 172], "iivm_summari": 92, "iivmglmnet": 64, "iivmrang": 64, "iivmrpart": 64, "iivmxgboost11861": 64, "ij": [43, 63, 66, 78, 91, 101], "ilia": 171, "illustr": [61, 63, 64, 65, 66, 67, 68, 70, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 95, 96, 99, 100, 101, 103, 112, 113, 114, 117, 118, 121, 123, 173], "iloc": [67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 91, 94, 98], "immedi": 169, "immun": [139, 171], "impact": [60, 75, 79, 94, 99], "implement": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 53, 54, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 80, 84, 88, 91, 92, 94, 97, 99, 100, 101, 103, 110, 111, 112, 115, 118, 119, 123, 125, 126, 128, 130, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 170, 171, 172, 173], "impli": [31, 40, 63, 64, 69, 72, 91, 92, 93, 97, 111, 124, 130, 157, 159, 160, 161, 162, 164, 165], "implicitli": [124, 128], "implment": [68, 76, 124, 125], "import": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 55, 56, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 169, 170, 172, 173], "importlib": 84, "impos": 100, "improv": [67, 69, 72, 75, 95, 124, 172], "in_sample_norm": [12, 14, 15, 16, 67, 140, 141, 142, 143, 144, 157, 159, 160, 161, 162], "inbuild": 75, "inbuilt": 75, "inc": [64, 92, 93, 99, 173], "includ": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 50, 57, 62, 64, 68, 69, 70, 71, 72, 76, 77, 78, 85, 86, 88, 92, 97, 99, 100, 111, 124, 156, 157, 163, 164, 165, 167, 169, 172, 173], "include_bia": [88, 91, 92], "include_never_tr": [17, 19, 72], "include_scenario": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 100], "incom": [64, 92, 93, 95, 99, 173], "incorpor": [65, 99, 157, 163], "increas": [19, 69, 72, 75, 87, 89, 91, 100, 173], "increment": 172, "ind": 92, "independ": [12, 14, 15, 16, 18, 31, 32, 40, 47, 63, 65, 68, 87, 89, 91, 95, 124, 130, 138, 140, 141, 142, 143, 144, 172], "index": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 61, 63, 68, 73, 76, 77, 80, 84, 85, 86, 90, 91, 92, 94, 95, 103, 104, 109, 139, 140, 141, 142, 143, 144, 168, 170], "index_col": 84, "india": [139, 171], "indic": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 36, 37, 38, 39, 46, 50, 51, 63, 64, 66, 68, 69, 70, 71, 72, 87, 91, 92, 93, 97, 98, 100, 101, 102, 104, 105, 108, 109, 111, 124, 126, 130, 131, 138, 139], "individu": [17, 19, 22, 23, 26, 53, 54, 62, 64, 68, 69, 72, 76, 77, 78, 85, 86, 87, 90, 92, 93, 97, 99, 111, 124, 173], "individual_df": 68, "induc": [110, 139], "industri": [63, 91], "inf": [5, 6, 7, 8, 9, 62, 69, 70, 71, 72, 98], "inf_model": 140, "infer": [34, 42, 60, 61, 63, 70, 75, 77, 79, 80, 84, 90, 91, 98, 103, 110, 124, 139, 168, 170, 171, 172], "inferenti": 173, "infinit": [5, 6, 7, 8, 9, 98, 104, 105, 109, 172], "influenc": [25, 49, 54, 77, 124], "info": [60, 65, 69, 70, 71, 72, 73, 76, 77, 78, 87, 89, 90, 91, 92, 93, 99, 104, 106, 109, 120, 170, 172, 173], "inform": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 51, 53, 54, 60, 65, 69, 70, 71, 72, 75, 79, 81, 82, 97, 98, 99, 100, 124, 125, 157, 158, 171], "infti": [61, 80, 103, 124, 130], "inher": 100, "inherit": [94, 105, 106, 107, 108, 109, 172], "initi": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 64, 65, 66, 67, 69, 70, 71, 72, 76, 77, 83, 92, 93, 96, 97, 99, 100, 101, 104, 106, 109, 111, 112, 114, 116, 121, 123, 124, 139, 170, 172, 173], "inlin": [73, 94], "inlinebackend": 94, "inner": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 112, 114, 116, 121], "innermost": [112, 121], "input": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 49, 54, 65, 70, 76, 99, 102, 112, 117, 120, 124, 128, 156, 157, 158, 163], "insensit": 124, "insid": [48, 49, 53, 54], "insight": [84, 100], "insignific": 99, "inspect": [76, 170], "inspir": [31, 33, 34, 36, 69, 72, 100], "instabl": 56, "instal": [64, 77, 90, 97, 124, 172], "install_github": 169, "instanc": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 53, 54, 64, 65, 77, 92, 112, 120], "instanti": [63, 64, 91, 92, 112, 119, 122, 139], "instead": [4, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 53, 54, 60, 62, 64, 68, 69, 70, 71, 72, 76, 77, 78, 79, 81, 87, 90, 92, 93, 104, 109, 111, 112, 115, 124, 140, 144, 157, 159, 160, 161, 162, 165, 166, 172], "instruct": [169, 172], "instrument": [5, 6, 7, 8, 9, 10, 25, 33, 37, 38, 42, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 76, 77, 78, 87, 89, 91, 92, 93, 96, 99, 101, 104, 106, 107, 108, 109, 112, 113, 124, 126, 127, 129, 133, 136, 140, 149, 151, 156, 170, 173], "instrument_effect": 60, "instrument_impact": 79, "instrument_strength": 98, "insuffienct": 90, "int": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 41, 46, 47, 51, 52, 55, 62, 63, 64, 67, 70, 76, 77, 79, 83, 95, 96, 98, 100, 101, 106, 109], "int32": 70, "int64": [69, 71, 72, 73, 91, 104, 106, 109, 170], "int8": [92, 93, 99], "integ": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 65, 112, 120, 121], "integr": [77, 90, 100, 157, 167, 172], "intend": [46, 65, 100, 173], "intent": [124, 173], "inter": [112, 114], "interact": [22, 23, 25, 26, 31, 33, 34, 35, 46, 47, 76, 78, 100, 110, 112, 119, 133, 134, 164, 165, 168, 172, 173], "interchang": 156, "interest": [25, 26, 31, 37, 38, 39, 40, 61, 64, 66, 67, 80, 84, 89, 92, 93, 97, 98, 101, 103, 111, 124, 126, 128, 131, 132, 133, 134, 135, 136, 137, 138, 140, 156, 170, 173], "interfac": [37, 62, 64, 65, 77, 104, 109, 112, 123, 139, 170], "intermedi": 100, "intern": [19, 37, 62, 64, 65, 78, 90, 93, 112, 114, 117, 123, 124, 125, 171], "internet": [64, 92, 93], "interpret": [69, 70, 71, 72, 85, 86, 89, 98, 100, 111, 157, 158, 164, 165, 166, 167, 169, 173], "intersect": [100, 157, 163, 172], "interv": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 51, 62, 63, 64, 66, 67, 69, 70, 71, 72, 76, 77, 78, 81, 82, 83, 85, 86, 88, 91, 93, 96, 97, 99, 101, 110, 111, 139, 140, 157, 163, 170, 171, 172, 173], "intial": 97, "introduc": [61, 80, 103, 104, 109, 156, 172, 173], "introduct": [61, 63, 65, 80, 91, 93, 99, 112, 119, 120, 123, 124, 126, 130, 157, 158], "introductori": [62, 100], "intrument": [66, 101], "intspecifi": 46, "intuit": 100, "inuidur1": [65, 73, 104, 109, 170], "inuidur1femaleblackothracedep1dep2q2q3q4q5q6agelt35agegt54durablelusdhusdtg": [65, 104, 109, 170], "inuidur2": [73, 104, 109, 170], "inv_sigmoid": 94, "invalid": [61, 72, 80, 91, 98, 103], "invari": [124, 126, 130], "invers": [22, 23, 24, 25, 26, 27, 28, 29, 30, 66, 101, 157, 164, 165], "invert": [25, 98], "invert_yaxi": 91, "investig": [84, 90, 100], "involv": [111, 112, 122, 140, 173], "io": [76, 94, 172], "ipw_norm": 172, "ipykernel_14607": 68, "ipykernel_17365": 81, "ipykernel_18995": 91, "ipynb": [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101], "ira": [64, 92, 93], "irm": [0, 9, 12, 14, 15, 16, 20, 21, 37, 38, 39, 51, 52, 75, 76, 77, 78, 82, 86, 87, 100, 101, 102, 108, 109, 110, 112, 113, 114, 127, 129, 134, 148, 164, 165, 168, 172, 173], "irm_summari": 92, "irmglmnet": 64, "irmrang": 64, "irmrpart": 64, "irmxgboost8047": 64, "irrespect": 100, "irrevers": [124, 130], "is_classifi": [12, 14, 15, 22, 23, 25, 26, 39], "is_gat": [22, 26, 39, 51], "isfinit": [69, 70, 71, 72], "isnan": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 75, 112, 113], "isoton": [55, 56, 100], "isotonicregress": 100, "issn": 84, "issu": [13, 16, 100, 168, 171, 172], "ite": [69, 72, 76, 77, 78, 85, 86, 87], "ite_lower_quantil": [69, 72], "ite_mean": [69, 72], "ite_upper_quantil": [69, 72], "item": [25, 77, 92, 102, 112, 121, 123, 139], "iter": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 60, 66, 67, 91, 92, 97, 101, 112, 118, 120, 156, 173], "itertool": 84, "its": [48, 49, 77, 100, 102, 111, 112, 119, 124, 126, 127, 129, 139, 140, 156], "iv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 37, 38, 39, 42, 43, 61, 63, 80, 91, 103, 104, 109, 133, 136, 152, 153, 157, 166, 168, 172, 173], "iv_2": 60, "iv_var": [63, 91], "iv\u00e1n": [139, 171], "j": [10, 11, 17, 18, 19, 31, 33, 34, 36, 40, 41, 42, 43, 44, 45, 61, 62, 63, 65, 66, 78, 80, 84, 91, 94, 98, 101, 103, 112, 122, 124, 131, 156, 168, 170], "j_": [63, 91], "j_0": 156, "j_1": [63, 91], "j_2": [63, 91], "j_3": [63, 91], "j_k": [63, 91], "jame": 171, "jan": 172, "janari": [69, 72], "janni": [64, 92], "januari": 69, "jasenakova": 172, "javanmard": 171, "jbe": [63, 91], "jeconom": [17, 18, 19, 31, 40, 62], "jerzi": 171, "jia": 100, "jitter": [16, 76, 77], "jitter_strength": [76, 77], "jitter_valu": 16, "jk": [124, 132], "jmlr": [65, 168, 170, 172], "job": [64, 92, 93], "john": 171, "joint": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 51, 69, 70, 71, 72, 78, 81, 82, 83, 85, 86, 88, 93, 96, 98, 124, 126, 156, 172, 173], "joint_prob": 89, "jointli": [96, 111], "jonathan": 171, "joss": [65, 112, 122, 168, 170], "journal": [10, 11, 17, 18, 19, 31, 36, 40, 41, 43, 44, 62, 63, 65, 84, 91, 94, 98, 100, 103, 112, 122, 168, 170, 171, 172], "jss": 168, "juliu": 172, "jump": [95, 97, 124], "jun": [62, 171], "jupyt": [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101], "juraj": 171, "just": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 65, 67, 68, 69, 72, 76, 78, 83, 85, 86, 87, 88, 95, 96, 124, 140, 141, 142, 143, 144, 157, 158], "justif": [139, 157, 158], "k": [10, 17, 18, 19, 33, 34, 36, 40, 41, 42, 43, 44, 61, 63, 65, 75, 80, 90, 91, 97, 98, 102, 103, 110, 111, 124, 156, 173], "k_h": [97, 124], "kaggl": [64, 92], "kallu": [83, 93, 96, 98, 99, 140, 146, 149, 150, 171], "kappa": 124, "kato": [43, 63, 91, 156, 171], "kb": [70, 71, 76, 77, 78, 87, 89, 91, 92, 93, 99, 104, 106, 109, 170], "kde": [27, 28, 29, 92], "kdeplot": [67, 75, 101], "kdeunivari": [27, 28, 29], "kecsk\u00e9sov\u00e1": 172, "keel": 98, "keep": [49, 54, 62, 76, 88, 100, 108, 109, 173], "kei": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 52, 63, 64, 76, 81, 82, 85, 86, 90, 91, 92, 93, 97, 100, 112, 114, 123, 124, 140, 157, 163, 172], "keith": 171, "kelz": 98, "kengo": 171, "kennedi": 98, "kept": [107, 109], "kernel": [27, 28, 29, 46, 49, 54, 97, 124], "kernel_regress": 97, "kernelreg": 97, "keyword": [17, 18, 19, 22, 26, 39, 43, 44, 45, 47, 51], "kf": 139, "kfold": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 91, 139], "kind": [60, 79, 92], "kj": [17, 18, 19, 33, 34, 36, 40, 41, 42, 43, 44, 61, 63, 80, 91, 103], "klaassen": [33, 75, 84, 90, 100, 168, 171], "klaa\u00dfen": 33, "kluge": 172, "knau": 171, "know": [67, 95, 98], "knowledg": [60, 75, 79, 94, 95], "known": [75, 87, 97, 98, 100, 112, 113, 124, 130], "kohei": 171, "kotthof": 65, "kotthoff": [65, 112, 122, 168, 170], "krueger": 94, "kueck": [64, 92], "kurz": [168, 171, 172], "kwarg": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 43, 44, 45, 46, 47, 48, 51, 90, 124], "l": [63, 65, 66, 73, 81, 82, 91, 98, 100, 101, 112, 122, 157, 166, 168, 170], "l1": [92, 101, 124], "l_hat": [38, 39, 61, 80, 140], "lab": 66, "label": [13, 16, 48, 53, 68, 76, 77, 78, 80, 81, 82, 83, 85, 86, 88, 89, 90, 93, 94, 96, 97], "labor": 94, "laffer": 171, "laff\u00e9r": [36, 66, 101, 124, 138, 140, 154, 155], "lal": [94, 172], "lambda": [63, 64, 65, 66, 69, 72, 92, 94, 95, 112, 120, 122, 123, 124, 140, 141, 142, 156, 170], "lambda_": 84, "lambda_0": [140, 141, 142], "lambda_l1": [76, 77], "lambda_l2": [76, 77], "lambda_t": [18, 19, 72], "land": 95, "lang": [65, 112, 122, 168, 170], "langl": [32, 95], "lanni": 98, "lappli": 139, "larg": [61, 75, 80, 87, 90, 94, 100, 124], "larger": [26, 41, 62, 97, 100, 157, 163], "largest": 75, "largli": 75, "lasso": [63, 64, 65, 66, 70, 92, 101, 112, 117, 118, 122, 123, 170, 171], "lasso_class": [64, 92], "lasso_pip": [65, 112, 122], "lasso_summari": 92, "lassocv": [30, 70, 84, 91, 92, 101, 112, 118, 124, 156, 170], "last": [18, 65, 169], "late": [25, 60, 64, 92, 98, 124, 133, 140, 147], "latent": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 99, 157, 166, 167], "later": [64, 65, 76, 97, 100, 112, 123, 173], "latest": 168, "latter": [53, 54, 98, 124], "layout": 84, "lbrace": [25, 26, 33, 34, 36, 63, 91, 102, 124, 131, 133, 134, 139, 140, 145, 156, 157, 164], "ldot": [37, 38, 39, 63, 66, 89, 91, 101, 102, 124, 135, 136, 137, 139, 156, 170], "le": [18, 67, 95, 111, 124, 126, 140, 149, 150], "lead": [62, 100, 124], "leadsto": 156, "lear": [65, 112, 122, 168, 170], "learn": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 41, 43, 44, 45, 46, 60, 64, 65, 70, 73, 75, 76, 78, 79, 83, 84, 88, 90, 92, 93, 94, 96, 97, 98, 100, 104, 109, 110, 112, 115, 139, 140, 156, 157, 158, 172, 173], "learner": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 50, 55, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 72, 77, 80, 81, 82, 84, 89, 91, 92, 93, 98, 99, 100, 101, 102, 103, 110, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 126, 127, 129, 139, 140, 156, 157, 163, 172, 173], "learner_class": [30, 172], "learner_cv": 65, "learner_dict": 77, "learner_forest_classif": 65, "learner_forest_regr": 65, "learner_l": 99, "learner_lasso": 65, "learner_list": 75, "learner_m": 99, "learner_nam": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 55, 76, 77, 112, 123], "learner_pair": 77, "learner_param_v": 65, "learner_rf": 156, "learnerclassif": 65, "learnerregr": 65, "learnerregrcvglmnet": 65, "learnerregrrang": [65, 112, 121], "learning_r": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 69, 72, 76, 77, 80, 83, 93, 96, 97, 100, 103], "least": [60, 64, 79, 92, 93, 99, 124, 128, 139], "leav": [66, 100, 101], "left": [17, 19, 33, 34, 36, 42, 43, 61, 63, 75, 77, 78, 80, 91, 92, 93, 94, 96, 97, 103, 124, 140, 141, 142, 143, 144, 156, 157, 159, 160, 161, 162, 164, 165], "legend": [64, 68, 69, 72, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 88, 89, 93, 94, 96], "lemp": 70, "len": [75, 76, 77, 78, 83, 90, 91, 93, 96, 98], "length": [12, 13, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 65, 67, 69, 72, 98, 112, 116, 121], "leq": [63, 91], "less": [62, 64, 92, 93, 97, 100], "lester": 171, "let": [17, 18, 19, 31, 35, 40, 61, 62, 64, 65, 66, 67, 69, 72, 75, 76, 77, 78, 80, 83, 85, 86, 88, 92, 93, 96, 100, 101, 102, 103, 112, 116, 121, 124, 126, 130, 138, 157, 158, 167, 173], "level": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 46, 51, 63, 64, 66, 67, 68, 69, 70, 71, 72, 76, 77, 78, 81, 82, 83, 85, 86, 87, 88, 91, 92, 93, 96, 98, 99, 100, 101, 112, 114, 131, 132, 140, 145, 157, 163, 164, 173], "level_0": [65, 91], "level_1": 91, "level_bound": [76, 77, 78], "leverag": 77, "levi": 98, "levinsohn": [63, 91], "lewi": 171, "lgbm": [76, 77], "lgbm_argument": 77, "lgbmclassifi": [67, 68, 69, 72, 75, 76, 77, 83, 93, 96, 97, 100], "lgbmlgbmregressorlgbmregressor": 76, "lgbmregressor": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 67, 68, 69, 72, 75, 76, 77, 80, 83, 93, 97, 100, 103], "lgr": [61, 62, 63, 64, 65, 66, 102, 112, 121, 122, 123, 124, 139, 140, 156, 170], "lib": [68, 69, 70, 71, 72, 91, 92, 97, 100], "liblinear": [92, 101, 124], "librari": [60, 61, 62, 63, 64, 65, 66, 77, 102, 103, 104, 109, 112, 120, 121, 122, 123, 124, 139, 140, 156, 169, 170, 173], "licens": [168, 172], "lie": 171, "lightgbm": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 67, 68, 69, 72, 75, 76, 77, 80, 83, 93, 96, 97, 100], "like": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 53, 54, 62, 64, 65, 69, 70, 71, 72, 76, 84, 92, 93, 100, 112, 115, 116, 118, 124, 125, 139, 170, 173], "lim": 94, "lim_": [97, 124], "limegreen": [81, 82], "limit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 94, 124, 130, 171], "limits_": 111, "lin": [97, 100, 124], "line": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 68, 70, 76, 77, 78, 89, 100], "line_width": [76, 77], "linear": [17, 19, 20, 21, 22, 26, 31, 35, 38, 39, 40, 41, 42, 43, 44, 45, 51, 60, 61, 62, 63, 65, 67, 68, 70, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 88, 89, 90, 91, 98, 99, 100, 102, 103, 110, 111, 112, 116, 117, 118, 121, 123, 127, 128, 129, 135, 136, 137, 139, 141, 142, 143, 144, 145, 147, 148, 152, 153, 156, 163, 165, 166, 167, 168, 170, 171, 172, 173], "linear_learn": [69, 72], "linear_model": [22, 26, 30, 39, 51, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 84, 88, 91, 92, 97, 98, 100, 101, 112, 117, 118, 124, 156, 170], "linear_regress": 76, "linear_regressionlinearregress": 76, "linearli": [97, 124], "linearregress": [60, 69, 70, 71, 72, 75, 76, 77, 78, 79, 88, 97, 98, 100], "linearregressionlinearregress": 76, "linearscoremixin": [0, 140], "lineplot": [69, 72, 77, 78], "linestyl": [68, 69, 72, 76, 77, 78, 89, 90, 97], "linetyp": 66, "linewidth": [68, 69, 72, 76, 77], "link": [89, 100, 124, 135, 172], "linspac": [81, 82, 88, 100], "lint": 172, "linux": 169, "list": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 49, 54, 55, 61, 62, 63, 64, 65, 69, 70, 72, 76, 80, 81, 82, 91, 93, 95, 103, 112, 116, 119, 120, 121, 122, 123, 139, 140, 169, 172], "list_confset": 25, "listedcolormap": 91, "literatur": [100, 124, 126, 130], "littl": [87, 98], "liu": [41, 89, 140, 151, 171], "ll": [65, 156, 173], "lllllllllllllllll": [104, 109, 170], "lm": [60, 62, 100], "ln_alpha_ml_l": 84, "ln_alpha_ml_m": 84, "load": [60, 62, 64, 65, 76, 84, 92, 93, 104, 109, 169, 170], "loader": 0, "loc": [68, 69, 70, 71, 72, 76, 77, 78, 80, 83, 84, 85, 86, 89, 91, 94, 96, 99, 100], "local": [25, 27, 76, 98, 111, 124, 133, 171, 172], "localconvert": 91, "locat": [83, 96, 124], "log": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 63, 67, 69, 70, 71, 72, 75, 76, 77, 84, 91, 93, 94, 99, 101, 112, 113, 117, 124, 126, 127, 129], "log_odd": 95, "log_p": [63, 91], "log_reg": [60, 62], "logarithm": [77, 84], "logic": [25, 65, 112, 120, 121], "logical_not": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 75, 112, 113], "logist": [31, 37, 41, 47, 60, 62, 64, 66, 70, 78, 79, 92, 98, 100, 101, 135, 171, 172, 173], "logistic_regress": 76, "logisticregress": [60, 69, 70, 71, 72, 73, 76, 77, 78, 79, 88, 97, 98, 100], "logisticregressioncv": [30, 70, 75, 92, 101, 124], "logit": [17, 19, 37, 75, 94, 140, 151], "loglik": 65, "logloss": [64, 77, 92, 173], "logloss_m": 77, "logo": 172, "logspac": 92, "long": [7, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 61, 70, 75, 80, 99, 100, 157, 158, 167, 171], "longer": [69, 72, 104, 109, 124, 128], "look": [62, 64, 65, 67, 68, 69, 70, 71, 72, 75, 76, 83, 89, 92, 93, 96, 97, 99], "loop": [78, 98], "loss": [67, 69, 70, 71, 72, 75, 76, 77, 90, 93, 97, 99, 101, 112, 113, 124, 126, 127, 129], "loss_ml_g0": 75, "loss_ml_g1": 75, "loss_ml_m": 75, "low": [68, 87, 98, 111, 171], "lower": [25, 64, 65, 68, 69, 72, 77, 78, 83, 84, 87, 88, 93, 94, 96, 97, 98, 99, 100, 112, 122, 123, 157, 163, 167, 173], "lower_bound": [81, 82], "lplr": [89, 135, 151], "lpop": 70, "lpq": [27, 29, 93, 111, 149, 172], "lpq_0": 96, "lpq_1": 96, "lqte": 111, "lr": 97, "lrn": [60, 61, 62, 63, 64, 65, 66, 102, 112, 119, 120, 121, 122, 123, 124, 139, 140, 156, 170, 173], "lrn_0": 65, "lt": [60, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 76, 77, 78, 87, 89, 91, 92, 93, 95, 99, 100, 104, 109, 170], "lucien": 172, "luka": 171, "luk\u00e1\u0161": 36, "lusd": [65, 73, 104, 109, 170], "lvert": 84, "m": [7, 10, 11, 16, 30, 31, 37, 42, 43, 44, 61, 63, 65, 69, 72, 73, 75, 77, 80, 84, 87, 90, 91, 94, 98, 103, 106, 109, 110, 111, 112, 122, 124, 125, 127, 129, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 157, 159, 160, 161, 162, 164, 165, 166, 167, 168, 169, 170, 171, 172], "m_": [77, 78, 124, 127, 129, 131, 140, 142, 144, 145, 149, 156], "m_0": [12, 14, 15, 16, 22, 23, 24, 25, 26, 28, 37, 38, 39, 44, 45, 46, 61, 63, 64, 80, 84, 87, 90, 91, 92, 103, 111, 112, 116, 121, 124, 133, 134, 136, 137, 140, 141, 142, 143, 144, 146, 149, 150, 151, 153, 154, 155, 170, 173], "m_hat": [25, 26, 38, 39, 61, 80, 88, 140], "m_i": [97, 124], "ma": [43, 63, 91, 98, 124, 125, 171], "mac": 169, "machin": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 41, 43, 44, 45, 46, 60, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 78, 79, 83, 84, 88, 90, 92, 93, 94, 96, 97, 98, 99, 100, 101, 110, 112, 113, 122, 124, 126, 127, 129, 139, 140, 156, 157, 158, 172, 173], "machineri": [84, 171], "mackei": 171, "maco": 169, "made": [124, 138, 173], "mae": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 75, 112, 113], "maggi": 171, "magnitud": [41, 157, 158], "mai": [49, 54, 66, 67, 101], "main": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 71, 76, 84, 93, 100, 124, 156, 157, 158, 171, 173], "mainli": [75, 76, 100], "maintain": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 62, 168, 172], "mainten": 172, "major": [65, 100, 172], "make": [60, 75, 77, 78, 79, 90, 100, 111, 112, 119, 172, 173], "make_confounded_irm_data": [100, 172], "make_confounded_plr_data": 99, "make_did_cs2021": [7, 16, 69, 106, 109, 124, 125, 129], "make_did_cs_cs2021": [72, 124, 127], "make_did_sz2020": [5, 12, 15, 67, 105, 109, 124, 126], "make_heterogeneous_data": [81, 82, 85, 86, 87], "make_iivm_data": [25, 27, 111, 124], "make_irm_data": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 75, 88, 111, 112, 114, 124], "make_irm_data_discrete_treat": [76, 77, 78], "make_lplr_lzz2020": [37, 89, 124], "make_pipelin": 92, "make_pliv_chs2015": [38, 124], "make_pliv_multiway_cluster_ckms2021": [63, 91], "make_plr_ccddhnr2018": [6, 7, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 80, 90, 102, 103, 111, 112, 113, 116, 120, 121, 123, 124, 139, 140, 156, 157, 163, 172], "make_simple_rdd_data": [8, 46, 97, 107, 109, 124], "make_spd_matrix": 45, "make_ssm_data": [9, 66, 101, 108, 109, 124], "malt": [168, 171], "man": [60, 79], "manag": [112, 123, 169], "mandatori": [107, 109], "mani": [20, 21, 42, 61, 62, 63, 65, 67, 80, 90, 91, 103, 140, 156, 173], "manili": 51, "manipul": [64, 65, 97, 124], "manual": [64, 88, 90, 99, 173], "mao": 171, "map": [25, 48, 49, 53, 54, 62, 63, 91, 111, 124, 133], "mapsto": [102, 111], "mar": [36, 124], "march": [75, 84, 90], "margin": [81, 82, 100], "marit": [64, 92], "marker": [69, 72, 78, 100], "markers": [76, 94], "market": 94, "markettwo": 63, "markov": [45, 171], "marr": [64, 92, 93, 99, 173], "marshal": [112, 120], "martin": [36, 100, 168, 171, 172], "masatoshi": 171, "masip": [98, 172], "mask": 16, "maskedarrai": [124, 125], "master": [62, 70], "mat": 63, "match": [112, 123, 157, 166], "math": [30, 69, 70, 71, 72], "mathbb": [17, 18, 19, 20, 21, 25, 26, 31, 35, 38, 39, 40, 41, 63, 66, 67, 68, 69, 72, 75, 76, 77, 78, 87, 89, 90, 91, 94, 97, 101, 111, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166, 167, 170, 173], "mathcal": [17, 18, 19, 31, 32, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 47, 61, 63, 66, 68, 80, 83, 91, 95, 96, 101, 103, 124, 125, 128, 130], "mathop": 111, "mathrm": [31, 40, 69, 70, 71, 72, 97, 124, 125, 127, 128, 129, 130, 140, 142, 144, 157, 160, 162], "matia": 171, "matplotlib": [13, 16, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 96, 97, 100, 101], "matric": [95, 172], "matrix": [17, 18, 19, 31, 33, 34, 35, 36, 40, 42, 43, 44, 45, 49, 54, 61, 63, 64, 65, 66, 80, 91, 101, 103, 104, 109, 112, 122, 123, 156, 170, 172, 173], "matt": 171, "matter": [75, 94], "max": [17, 19, 64, 65, 76, 77, 88, 92, 93, 98, 102, 111, 112, 123, 124, 139, 140, 142, 144, 146, 156, 157, 160, 162, 170, 173], "max_depth": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 73, 76, 77, 92, 99, 102, 111, 112, 114, 124, 126, 139, 140, 156, 157, 163, 170, 173], "max_featur": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 73, 92, 99, 102, 111, 112, 114, 124, 139, 140, 156, 157, 163, 170, 173], "max_it": [76, 77, 91, 92, 100], "maxim": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 95, 111, 124], "maxima": 156, "maximum": [55, 56, 111, 112, 123], "mb": [69, 72, 73, 104, 109, 170], "mb706": 172, "mea": 33, "mean": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 48, 49, 53, 54, 60, 61, 63, 64, 67, 69, 72, 75, 76, 77, 78, 79, 80, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 96, 98, 99, 100, 103, 112, 113, 123, 124, 156, 173], "mean_absolute_error": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 75, 112, 113], "meant": [70, 111, 172], "measir": 99, "measur": [62, 65, 70, 84, 90, 98, 99, 100, 112, 122, 123, 124, 125, 138, 157, 158, 164, 165, 166, 167], "measure_col": 84, "measure_func": 62, "measure_pr": 62, "measures_r": 62, "mechan": [48, 49, 53, 54, 100, 124, 130], "median": [98, 100, 139], "medium": 77, "melt": 63, "membership": 100, "memori": [69, 70, 71, 72, 73, 76, 77, 78, 87, 89, 91, 92, 93, 99, 104, 106, 109, 170], "mention": [87, 111], "merg": [64, 92], "mert": [139, 171], "meshgrid": [81, 82, 100], "messag": [61, 62, 63, 64, 65, 66, 75, 77, 170, 172], "meta": [48, 49, 53, 54, 112, 170], "metadata": [48, 49, 53, 54], "metadata_rout": [48, 49, 53, 54], "metadatarequest": [48, 49, 53, 54], "method": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 76, 78, 79, 80, 81, 82, 83, 85, 86, 88, 91, 92, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 123, 124, 125, 126, 127, 129, 139, 140, 156, 157, 158, 163, 168, 170, 172], "methodolog": 171, "methodologi": 100, "metric": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 53, 77, 112, 113], "michael": 171, "michaela": 172, "michel": [168, 170], "michela": [36, 171], "mid": [64, 92, 94, 97, 124, 140, 153], "mid_point": [76, 77, 78], "might": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 63, 70, 75, 83, 88, 91, 95, 97, 99, 100, 112, 114, 124], "mild": [61, 80, 103], "militari": 94, "miller": [63, 91], "mimic": 100, "min": [17, 19, 63, 64, 65, 66, 69, 70, 71, 72, 76, 77, 83, 88, 91, 92, 93, 96, 97, 102, 112, 120, 123, 124, 128, 139, 140, 156, 170, 173], "min_": 111, "min_child_sampl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76], "min_data_in_leaf": 77, "min_samples_leaf": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 52, 77, 87, 89, 92, 99, 102, 111, 112, 114, 124, 126, 139, 140, 156, 157, 163, 173], "min_samples_split": [92, 124, 125, 127, 129], "minim": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 52, 64, 75, 92, 97, 124], "minimum": [55, 56, 70, 115, 119, 123], "minor": [61, 70, 80, 103, 140, 172], "minsplit": 64, "minut": 90, "mirror": [104, 109], "miruna": 171, "mislead": 172, "miss": [5, 6, 7, 8, 9, 30, 65, 104, 105, 109, 112, 120, 121, 124, 140, 154, 172], "missing": [36, 66, 101], "misspecif": 67, "misspecifi": 67, "mit": [168, 170], "mix": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "mixin": [0, 20, 21, 140], "ml": [45, 63, 64, 65, 70, 84, 90, 91, 92, 97, 98, 102, 110, 112, 121, 123, 124, 139, 168, 171, 172], "ml_a": 37, "ml_g": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 50, 60, 61, 62, 64, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 83, 85, 87, 88, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 111, 112, 114, 120, 122, 124, 125, 126, 127, 129, 172], "ml_g0": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 62, 64, 67, 69, 70, 71, 73, 75, 92, 99, 112, 114, 124, 126, 129], "ml_g1": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 64, 67, 69, 70, 71, 73, 75, 92, 99, 112, 114, 124, 126, 129], "ml_g_d0": [101, 124], "ml_g_d0_t0": [67, 72, 124, 126, 127], "ml_g_d0_t1": [67, 72, 124, 126, 127], "ml_g_d1": [101, 124], "ml_g_d1_t0": [67, 72, 124, 126, 127], "ml_g_d1_t1": [67, 72, 124, 126, 127], "ml_g_d_lvl0": [76, 77, 124], "ml_g_d_lvl1": [76, 77, 124], "ml_g_param": 76, "ml_g_params_pipelin": 76, "ml_g_pipelin": 76, "ml_g_sim": 30, "ml_l": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 63, 64, 65, 73, 80, 82, 86, 90, 91, 92, 94, 99, 102, 103, 112, 113, 116, 117, 118, 120, 121, 123, 124, 139, 140, 156, 157, 163, 170, 172, 173], "ml_l_bonu": 170, "ml_l_forest": 65, "ml_l_forest_pip": 65, "ml_l_lasso": 65, "ml_l_lasso_pip": 65, "ml_l_param": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 112, 117], "ml_l_rf": 173, "ml_l_sim": 170, "ml_l_tune": [112, 118], "ml_l_xgb": 173, "ml_m": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170, 172, 173], "ml_m_bench_control": 100, "ml_m_bench_treat": 100, "ml_m_bonu": 170, "ml_m_forest": 65, "ml_m_forest_pip": 65, "ml_m_lasso": 65, "ml_m_lasso_pip": 65, "ml_m_param": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76, 112, 117], "ml_m_params_pipelin": 76, "ml_m_pipelin": 76, "ml_m_rf": 173, "ml_m_sim": [30, 170], "ml_m_studi": 76, "ml_m_tune": [112, 118], "ml_m_xgb": 173, "ml_param_spac": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76, 112, 117], "ml_pi": [30, 66, 101, 124], "ml_pi_sim": 30, "ml_r": [25, 38, 60, 63, 64, 79, 91, 92, 98, 124, 172], "ml_r0": 124, "ml_r1": [64, 92, 124], "ml_t": [37, 89, 124], "mlr": [65, 112, 122], "mlr3": [60, 61, 62, 63, 64, 66, 102, 112, 119, 120, 121, 122, 123, 124, 139, 140, 156, 168, 170, 172, 173], "mlr3book": [65, 112, 120, 122, 123], "mlr3extralearn": [64, 112, 119], "mlr3filter": 65, "mlr3learner": [60, 61, 62, 63, 64, 102, 112, 119, 120, 121, 123, 124, 139, 140, 156, 170, 173], "mlr3measur": 62, "mlr3pipelin": [112, 119, 120, 122, 172], "mlr3tune": [65, 112, 122, 123, 172], "mlr3vers": 64, "mlrmeasur": 62, "mode": [100, 169], "model": [0, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 49, 51, 52, 54, 57, 60, 61, 62, 63, 65, 67, 68, 69, 70, 71, 72, 75, 79, 80, 83, 84, 87, 91, 93, 96, 99, 102, 103, 104, 106, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 123, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 141, 142, 143, 144, 145, 147, 148, 151, 152, 153, 158, 163, 164, 165, 166, 167, 168, 171, 172], "model_data": [64, 92], "model_label": 90, "model_list": 77, "model_select": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 80, 91, 112, 118, 139], "modellist": [77, 88], "modelmlestimatelowerupp": 64, "modelvers": 77, "modern": [65, 112, 122, 168, 170], "modul": [77, 97, 109, 124, 169], "molei": [41, 171], "moment": [20, 21, 63, 91, 124, 127, 128, 129, 140, 156, 157, 158, 167, 170], "monoton": 124, "mont": [31, 32, 35, 40, 81, 82, 85, 86], "montanari": 171, "month": [69, 72], "more": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 41, 51, 60, 62, 64, 69, 70, 71, 72, 75, 76, 77, 78, 79, 81, 82, 84, 88, 90, 92, 93, 97, 99, 100, 102, 111, 112, 113, 117, 118, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 138, 140, 148, 156, 157, 158, 163, 167, 170, 173], "moreov": [64, 65, 70, 84, 112, 120, 121, 156, 173], "mortgag": [64, 92, 93], "most": [64, 75, 83, 92, 93, 96, 100, 111, 112, 115, 117, 124, 157, 163, 169], "motiv": [100, 103], "motivation_example_bch": 84, "mp": 62, "mpd": [63, 91], "mpdta": 70, "mpg": 91, "mse": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 65, 84, 112, 122, 123], "mserd": 97, "msg": 69, "msr": [65, 112, 122, 123], "mtry": [64, 65, 102, 112, 123, 124, 139, 140, 156, 173], "mu": 68, "mu_": 68, "mu_0": 124, "mu_mean": 68, "much": [64, 65, 76, 77, 92, 97, 98, 100, 173], "muld": [73, 104, 109, 170], "multi": [16, 48, 53, 62, 63, 81, 82, 91, 140, 142, 144, 172], "multiclass": [65, 90], "multiindex": 91, "multioutput": [49, 54], "multioutputregressor": [49, 54], "multipl": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 62, 63, 64, 66, 67, 70, 71, 76, 77, 88, 91, 92, 99, 100, 101, 104, 109, 112, 116, 121, 122, 128, 132, 136, 139, 156, 157, 158, 171, 172, 173], "multipletest": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "multipli": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 80, 110, 111, 140, 173], "multiprocess": [83, 93, 96], "multitest": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "multivariate_norm": 30, "multiwai": [43, 63, 91, 171], "music": 171, "must": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 56, 89, 104, 105, 109, 112, 121, 123, 124], "mutat": 65, "mutual": [22, 26, 39, 64, 85, 86, 92, 93, 111], "my_sampl": 139, "my_task": 139, "n": [12, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 60, 61, 63, 65, 66, 68, 69, 72, 76, 77, 78, 79, 80, 83, 84, 87, 91, 94, 95, 96, 97, 98, 101, 102, 103, 111, 112, 122, 124, 130, 139, 156, 168, 169], "n_": [35, 68, 72, 157, 160, 162], "n_aggreg": 13, "n_coef": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 157, 163], "n_color": [69, 72], "n_complier": 96, "n_core": [83, 93, 96], "n_estim": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 67, 68, 69, 72, 73, 76, 77, 80, 81, 82, 83, 85, 86, 87, 92, 93, 95, 96, 97, 99, 100, 102, 103, 111, 112, 114, 116, 124, 126, 139, 140, 156, 157, 163, 170, 173], "n_eval": [65, 112, 122, 123], "n_featur": [48, 49, 53, 54], "n_fold": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 61, 62, 63, 64, 67, 69, 70, 72, 73, 75, 80, 81, 82, 83, 85, 86, 88, 89, 91, 92, 93, 94, 95, 96, 97, 99, 100, 103, 112, 116, 120, 121, 123, 139, 170, 173], "n_folds_inn": 37, "n_folds_per_clust": [63, 91], "n_folds_tun": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "n_framework": 13, "n_iter": [46, 97, 124], "n_iter_randomized_search": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 112, 118], "n_job": [83, 92, 93, 96], "n_jobs_cv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 75], "n_jobs_model": [16, 23, 29, 83, 93, 96], "n_jobs_optuna": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "n_learner": 77, "n_level": [35, 76, 77, 78], "n_model": 76, "n_ob": [7, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 47, 51, 52, 61, 65, 66, 67, 68, 69, 72, 75, 76, 77, 78, 80, 81, 82, 85, 86, 87, 88, 89, 90, 97, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 111, 112, 114, 117, 118, 122, 123, 124, 125, 126, 127, 129, 139, 156, 157, 163, 170], "n_output": [48, 49, 53, 54], "n_period": [17, 19, 69, 72], "n_pre_treat_period": [17, 19, 69, 72], "n_rep": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 61, 62, 63, 66, 67, 69, 72, 73, 75, 77, 78, 80, 87, 88, 91, 97, 99, 100, 101, 103, 112, 113, 114, 116, 120, 121, 139, 157, 163, 170, 173], "n_rep_boot": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 51, 69, 70, 71, 72, 78, 81, 82, 83, 85, 86, 88, 93, 96, 156], "n_sampl": [48, 49, 53, 54, 95, 98], "n_samples_fit": [49, 54], "n_split": 139, "n_t": 68, "n_target": [53, 54], "n_theta": 13, "n_time_period": 68, "n_trial": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76, 112, 117], "n_true": [83, 96], "n_var": [61, 65, 80, 103, 104, 109, 112, 117, 118, 122, 123, 156, 170], "n_w": 95, "n_x": [32, 81, 82, 85, 86, 87], "na": [5, 6, 7, 8, 9, 61, 63, 66, 103, 172], "na_real_": [63, 172], "naiv": [61, 80, 103], "name": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 50, 53, 54, 55, 61, 62, 63, 68, 69, 70, 72, 76, 77, 85, 86, 87, 90, 91, 97, 99, 100, 112, 114, 121, 123, 169, 172], "namespac": 62, "nan": [5, 6, 7, 8, 9, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 67, 68, 75, 76, 77, 78, 80, 83, 85, 86, 90, 92, 93, 96, 101, 103, 112, 113], "nanmean": 80, "narita": 171, "nat": [69, 72], "nathan": 171, "nation": [100, 139, 171], "nativ": 62, "natt": 95, "natur": 100, "nbest": 77, "nbviewer": 76, "ncol": [63, 64, 65, 89, 97, 104, 109, 112, 122, 123, 156, 170], "ncoverag": 75, "ndarrai": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 55, 104, 109], "nearli": 75, "necess": [63, 91], "necessari": [62, 63, 77, 90, 91, 97, 124, 169], "need": [12, 14, 15, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 61, 62, 64, 66, 77, 79, 80, 90, 93, 98, 101, 112, 115, 116, 121, 139, 140, 151, 157, 167, 172, 173], "neg": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 49, 54], "neg_log_loss": 76, "neg_mean_squared_error": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "neighborhood": [97, 156], "neither": [5, 6, 7, 8, 9, 63, 91, 98, 104, 109], "neng": 171, "neq": [77, 97, 124], "nest": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 53, 54, 112, 114, 116, 121, 140, 155, 157, 163], "net": [93, 99, 173], "net_tfa": [64, 92, 93, 99, 173], "network": 77, "nev": [124, 127, 129, 130], "never": [17, 19, 25, 62, 63, 69, 70, 71, 72, 91, 124, 130, 172], "never_tak": [25, 64, 92], "never_tr": [14, 16, 69, 70, 71, 72, 124, 125, 127, 129], "nevertheless": 88, "new": [60, 61, 62, 63, 64, 65, 66, 81, 82, 90, 92, 95, 102, 103, 104, 109, 111, 112, 120, 121, 122, 123, 124, 139, 140, 156, 168, 170, 171, 172, 173], "new_data": [81, 82, 95], "newei": [10, 11, 44, 63, 84, 91, 100, 103, 168, 171], "newest": 172, "next": [62, 64, 65, 75, 81, 82, 83, 87, 89, 92, 93, 95, 96, 98, 100, 172], "neyman": [63, 91, 102, 110, 157, 167, 168, 171], "nfold": [63, 64, 66, 124], "nh": 124, "nice": [62, 77], "nifa": [92, 93, 99], "nil": 100, "nine": [63, 91], "nlogloss": 77, "nn": 97, "noack": [97, 124, 171, 172], "node": [64, 65, 102, 124, 139, 140, 156, 170, 173], "nois": [47, 94, 95], "nomin": 98, "non": [14, 17, 18, 19, 25, 43, 44, 45, 46, 60, 61, 64, 68, 69, 72, 76, 79, 80, 92, 93, 95, 97, 112, 123, 139, 140, 142, 144, 156, 157, 160, 162], "non_orth_scor": [61, 80, 140], "nondur": 73, "none": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 46, 48, 49, 51, 53, 54, 55, 56, 63, 64, 67, 69, 70, 71, 72, 73, 76, 77, 78, 79, 87, 89, 92, 93, 98, 99, 100, 101, 104, 106, 107, 108, 109, 112, 113, 120, 121, 124, 126, 127, 129, 140, 156, 169, 170], "nonignor": [30, 155], "nonlinear": [17, 19, 21, 64, 69, 72, 92, 97, 124, 140, 149, 150, 172], "nonlinearscoremixin": [0, 140], "nonparametr": [27, 28, 29, 97, 100, 140, 157, 158, 164, 165, 166, 167, 171], "nop": 65, "nor": [5, 6, 7, 8, 9, 63, 91, 98, 104, 109], "norm": 80, "normal": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 66, 67, 68, 69, 71, 72, 79, 80, 83, 87, 93, 94, 95, 96, 97, 98, 101, 103, 104, 109, 112, 117, 118, 124, 140, 141, 142, 143, 144, 156, 170], "normalize_ipw": [22, 23, 24, 25, 26, 27, 28, 29, 30, 66, 88, 93, 101], "not_yet_tr": [14, 16, 69, 72], "notat": [63, 66, 67, 91, 101, 124, 126, 127, 128, 129, 130, 138, 140, 142, 144], "note": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 52, 53, 54, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 139, 140, 168, 170], "notebook": [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 111, 112, 113, 120, 124, 172, 173], "notic": [60, 79, 98], "now": [62, 63, 64, 66, 70, 71, 75, 76, 77, 81, 82, 91, 92, 95, 98, 100, 101, 109, 170, 172], "np": [5, 6, 7, 8, 9, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 55, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 109, 111, 112, 113, 114, 116, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170, 173], "nrmse": 77, "nround": [61, 64, 173], "nrow": [62, 63, 65, 97, 104, 109, 112, 122, 123, 156, 170], "ntrue": 76, "nu": [18, 25, 45, 66, 101, 124, 133, 157, 158, 160, 162, 163, 166, 167], "nu2": [69, 75, 157, 163], "nu_0": [157, 167], "nu_i": [66, 101], "nuis_g0": 60, "nuis_g1": 60, "nuis_l": [120, 173], "nuis_m": [60, 120, 173], "nuis_r0": 60, "nuis_r1": 60, "nuis_rmse_ml_l": 84, "nuis_rmse_ml_m": 84, "nuisanc": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 44, 45, 46, 50, 61, 62, 63, 64, 65, 66, 67, 70, 75, 77, 80, 81, 82, 83, 84, 87, 89, 91, 92, 93, 96, 98, 99, 100, 101, 102, 103, 112, 114, 115, 116, 117, 118, 119, 121, 123, 124, 127, 128, 129, 139, 140, 141, 142, 143, 144, 145, 149, 151, 156, 157, 160, 162, 167, 168, 172, 173], "nuisance_el": [157, 159, 160, 161, 162, 164, 165, 166], "nuisance_loss": [75, 77, 112, 113, 172], "nuisance_spac": [37, 140, 151], "nuisance_target": 75, "null": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 62, 99, 112, 120, 157, 163, 172], "null_hypothesi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 99, 157, 163], "num": [64, 65, 102, 112, 120, 121, 123, 124, 139, 140, 156, 170], "num_leav": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 68, 83, 93, 96], "number": [12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 51, 52, 54, 61, 63, 68, 69, 70, 72, 75, 76, 80, 81, 82, 83, 84, 85, 86, 91, 93, 95, 96, 97, 100, 124, 128, 139, 156, 168, 170, 173], "numer": [17, 19, 21, 56, 60, 65, 88, 94, 112, 120, 121, 140, 157, 164, 165, 172], "numeric_onli": 84, "numpi": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 46, 51, 52, 55, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 106, 109, 111, 112, 114, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170], "nuniqu": [69, 72], "ny": 171, "nyt": [124, 127, 129, 130], "o": [68, 75, 76, 77, 78, 84, 85, 86, 90, 92, 94, 97, 168, 170], "ob": [62, 64, 68, 72, 97, 157, 160], "obei": 140, "obj": 92, "obj_dml_data": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 60, 61, 63, 70, 71, 79, 80, 83, 88, 90, 91, 96, 102, 103, 111, 112, 113, 114, 116, 120, 121, 123, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 172], "obj_dml_data_bonu": [104, 109], "obj_dml_data_bonus_df": [104, 109], "obj_dml_data_from_arrai": [5, 6, 7, 8, 9], "obj_dml_data_from_df": [5, 6, 8, 9], "obj_dml_data_sim": [104, 109], "obj_dml_data_sim_clust": [104, 109], "obj_dml_plr": [61, 80, 103], "obj_dml_plr_bonu": [65, 170], "obj_dml_plr_bonus_pip": 65, "obj_dml_plr_bonus_pipe2": 65, "obj_dml_plr_bonus_pipe3": 65, "obj_dml_plr_bonus_pipe_ensembl": 65, "obj_dml_plr_fullsampl": 90, "obj_dml_plr_lesstim": 90, "obj_dml_plr_nonorth": [61, 80], "obj_dml_plr_orth_nosplit": [61, 80], "obj_dml_plr_sim": [65, 170], "obj_dml_plr_sim_pip": 65, "obj_dml_plr_sim_pipe_ensembl": 65, "obj_dml_plr_sim_pipe_tun": 65, "obj_dml_sim": 30, "object": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 60, 64, 65, 66, 67, 69, 70, 71, 72, 73, 76, 78, 81, 82, 83, 87, 88, 89, 90, 92, 93, 96, 97, 101, 104, 106, 107, 108, 109, 111, 112, 113, 117, 119, 122, 123, 124, 125, 126, 127, 129, 139, 140, 156, 168, 170, 171, 172, 173], "obs_confound": [60, 79], "observ": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 51, 52, 57, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 87, 89, 90, 91, 92, 93, 96, 97, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 111, 112, 113, 123, 124, 125, 126, 127, 129, 130, 138, 139, 140, 141, 142, 143, 144, 156, 157, 158, 159, 160, 161, 162, 170, 171, 173], "obtain": [19, 25, 40, 60, 61, 62, 63, 66, 67, 69, 70, 71, 72, 75, 79, 80, 81, 82, 83, 84, 91, 96, 98, 100, 101, 102, 103, 111, 112, 123, 139, 140, 151, 156, 157, 158, 163, 169, 170], "obvious": [69, 72], "occur": [17, 19, 69, 72, 90, 172], "odd": 37, "off": [95, 171], "offer": [17, 19, 62, 64, 92, 93, 100, 173], "offici": 169, "offset": [112, 120], "often": 96, "oka": 171, "ol": [22, 26, 39, 51], "olma": [97, 124, 171, 172], "omega": [87, 111, 124, 125, 140, 145, 148, 157, 164, 165], "omega_": [43, 63, 91], "omega_1": [43, 63, 91], "omega_2": [43, 63, 91], "omega_epsilon": [63, 91], "omega_v": [43, 63, 91], "omega_x": [43, 63, 91], "omit": [69, 72, 99, 100, 124, 128, 140, 142, 144, 157, 158, 167, 171, 172, 173], "ommit": 100, "onc": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 90, 100, 124, 130, 173], "one": [13, 16, 38, 57, 60, 61, 62, 63, 64, 65, 69, 70, 71, 72, 75, 78, 79, 80, 81, 82, 89, 91, 93, 94, 97, 98, 99, 100, 103, 104, 109, 111, 112, 113, 122, 123, 124, 125, 128, 130, 136, 139, 140, 141, 142, 143, 144, 148, 151, 152, 153, 156, 157, 158, 163, 164, 165, 166, 170, 172], "ones": [65, 68, 83, 90, 96, 99, 111], "ones_lik": [77, 78, 96], "onli": [5, 6, 7, 8, 9, 12, 14, 15, 16, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 51, 53, 54, 55, 56, 62, 63, 64, 69, 72, 75, 76, 81, 82, 85, 86, 87, 89, 90, 91, 92, 93, 97, 102, 111, 112, 121, 122, 124, 127, 129, 138, 140, 142, 144, 146, 149, 150, 156, 157, 158, 160, 162, 164, 165, 167, 172], "onlin": 173, "onto": 75, "oo": 90, "oob_error": [65, 112, 120, 121], "oop": 172, "opac": [81, 82], "open": [65, 112, 122, 168, 170], "oper": [65, 172], "opposit": [95, 97, 124], "oprescu": [32, 81, 82, 85, 86, 171], "opt": [68, 69, 70, 71, 72, 91, 92, 97, 100], "optim": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 65, 81, 82, 90, 95, 111, 112, 122, 123, 171], "optimize_kwarg": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "option": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 53, 54, 55, 56, 60, 61, 63, 64, 66, 69, 70, 72, 75, 76, 78, 81, 82, 85, 86, 87, 91, 92, 93, 101, 104, 105, 107, 108, 109, 112, 113, 121, 123, 124, 139, 140, 146, 149, 150, 156, 172], "optuna": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 112, 117, 172], "optuna_set": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76, 112, 117], "optuna_sett": 76, "optuna_settings_pipelin": 76, "oracl": [35, 47, 69, 72, 76, 77, 78], "oracle_valu": [31, 35, 40, 47, 76, 77, 78], "orang": 61, "orcal": [31, 40], "order": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 53, 62, 63, 64, 65, 70, 88, 91, 92, 97, 112, 115, 120, 121, 124, 139, 140], "org": [33, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 112, 122, 168, 169, 171, 172], "orient": [65, 112, 122, 140, 168, 170, 171, 172], "origin": [48, 49, 52, 53, 54, 62, 65, 69, 70, 71, 72, 95, 99, 100, 111, 140, 148], "orign": [64, 92], "orth_sign": [51, 52, 88], "orthogon": [51, 52, 63, 64, 69, 91, 92, 102, 110, 124, 156, 157, 167, 168, 171], "orthongon": [157, 167], "osx": 169, "other": [5, 6, 7, 8, 9, 37, 38, 39, 48, 49, 53, 54, 61, 63, 64, 65, 66, 67, 69, 70, 71, 72, 75, 76, 78, 80, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 104, 107, 109, 111, 112, 120, 121, 122, 124, 136, 137, 139, 140, 148, 156, 157, 167, 168, 169, 170, 171, 172, 173], "other_ind": 91, "otherwis": [12, 14, 15, 22, 23, 25, 26, 39, 48, 49, 53, 54, 64, 92, 93, 95, 124, 126, 140, 142, 144], "othrac": [65, 73, 104, 109, 170], "our": [61, 62, 64, 65, 67, 69, 72, 75, 76, 77, 80, 81, 82, 83, 90, 92, 93, 96, 97, 99, 100, 103, 124, 168, 170, 172, 173], "ourselv": 75, "out": [38, 39, 63, 65, 67, 68, 69, 70, 71, 72, 73, 75, 84, 90, 91, 93, 99, 100, 101, 102, 104, 109, 110, 111, 112, 113, 114, 116, 117, 123, 124, 125, 126, 127, 129, 140, 152, 153, 156, 157, 158, 163, 166, 168, 170, 172, 173], "outcom": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 35, 37, 38, 39, 40, 41, 47, 60, 62, 63, 64, 65, 68, 69, 70, 71, 72, 73, 76, 79, 84, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 104, 106, 107, 108, 109, 112, 113, 123, 126, 127, 129, 130, 131, 135, 136, 137, 138, 142, 144, 145, 156, 158, 163, 164, 166, 167, 170, 172, 173], "outcome_0": 79, "outcome_1": 79, "outer": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 112, 114, 116, 121], "outperform": 77, "output": [62, 69, 71, 72, 75, 98, 102, 112, 120, 124, 127, 129, 156, 173], "output_list": 98, "outshr": 91, "outsid": 61, "over": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 61, 69, 70, 72, 75, 78, 80, 84, 103, 110, 112, 117, 118, 123, 124, 125, 157, 163, 172], "overal": [13, 69, 70, 71, 72, 89, 95, 100, 124, 125], "overall_aggregation_weight": [13, 69, 70, 71, 72], "overcom": [110, 140], "overfit": [90, 110, 139], "overlap": [67, 89, 100, 124, 126, 130], "overrid": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76, 112, 121, 172], "overridden": 124, "overst": [64, 92, 93], "overview": [75, 156, 157, 163, 171], "overwrit": 172, "overwritten": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "ownership": [64, 92], "p": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 37, 38, 39, 40, 41, 47, 55, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 103, 111, 112, 113, 114, 116, 117, 118, 122, 123, 124, 125, 126, 127, 129, 130, 135, 139, 140, 141, 142, 143, 144, 145, 146, 149, 150, 151, 154, 155, 156, 157, 164, 165, 168, 169, 170, 172], "p401": [64, 92, 93], "p_": [17, 19], "p_0": [140, 141, 142, 143], "p_1": 156, "p_adjust": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 139, 156, 168, 170], "p_dbl": [65, 112, 122, 123], "p_hat": 88, "p_i": 41, "p_int": [112, 123], "p_n": 42, "p_val": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "p_x": [43, 63, 91], "p_x0": 94, "p_x1": 94, "packag": [60, 61, 63, 65, 66, 67, 68, 69, 70, 71, 72, 74, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 89, 90, 91, 93, 95, 96, 97, 98, 99, 100, 101, 102, 103, 111, 112, 115, 119, 120, 121, 122, 123, 124, 126, 130, 138, 139, 140, 156, 157, 158, 168, 170, 171, 172, 173], "packagedata": 91, "packagevers": 64, "pad": 89, "page": [76, 100, 168, 171], "pair": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 79], "pake": [63, 91], "paket": [63, 64, 65], "pal": 63, "palett": [13, 16, 69, 72, 76, 77, 78], "pand": [69, 72], "panda": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 41, 51, 52, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 104, 106, 109, 111, 124, 157, 158, 170], "pandas2ri": 91, "panel": [6, 7, 12, 14, 16, 17, 18, 19, 70, 72, 104, 105, 106, 109, 127, 128, 130, 161, 162, 171, 172], "paper": [33, 42, 65, 90, 94, 97, 99, 100, 157, 167, 168, 170, 171, 172], "par": 73, "par_grid": [65, 112, 118, 122, 123], "paradox": [65, 112, 123, 172], "parallel": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 67, 68, 69, 72, 75, 78, 83, 96, 124, 126, 128, 130], "param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 53, 54, 76, 90, 112, 116, 117, 118, 121, 123], "param_grid": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 112, 123], "param_nam": [62, 76], "param_set": [65, 112, 122, 123], "param_spac": [76, 112, 117], "param_space_pipelin": 76, "param_v": 65, "paramet": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 61, 62, 63, 64, 66, 67, 69, 70, 72, 75, 76, 77, 78, 80, 81, 82, 83, 84, 87, 88, 91, 92, 95, 96, 97, 98, 99, 100, 101, 102, 103, 109, 110, 111, 112, 113, 116, 117, 118, 120, 121, 122, 123, 126, 127, 128, 129, 130, 131, 132, 133, 134, 138, 139, 149, 150, 156, 157, 158, 163, 165, 167, 168, 170, 171, 172, 173], "parametr": [25, 62, 76, 100, 103, 112, 116, 121, 173], "params_exact": [112, 121], "params_nam": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 62, 76], "params_stacking__final_estimator__c": 76, "params_stacking__lgbm__lambda_l1": 76, "params_stacking__lgbm__lambda_l2": 76, "params_stacking__lgbm__learning_r": 76, "params_stacking__lgbm__min_child_sampl": 76, "parenttoc": 168, "part": [45, 61, 63, 64, 65, 71, 75, 80, 90, 91, 92, 103, 112, 123, 139, 157, 167, 172, 173], "parti": 45, "partial": [21, 37, 38, 39, 40, 41, 42, 43, 44, 45, 63, 65, 73, 84, 89, 90, 91, 99, 102, 110, 112, 113, 116, 117, 118, 121, 123, 135, 136, 137, 139, 152, 153, 156, 163, 164, 165, 166, 167, 168, 170, 171, 172, 173], "partial_": [140, 156], "partiallli": 99, "particip": [10, 93, 99, 173], "particular": [124, 168], "particularli": [77, 90], "partion": [63, 91], "partit": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 63, 91, 102, 110], "partli": 173, "pass": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 50, 51, 53, 54, 62, 65, 70, 76, 90, 112, 114, 119, 121, 123, 173], "passo": [168, 170], "past": 63, "paste0": [63, 66], "pastel": 80, "path": [112, 118, 123, 124], "path_to_r": 84, "patsi": [81, 82, 111], "pattern": 100, "paul": 171, "pd": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 51, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 111, 124], "pdf": [80, 94], "pedregosa": [168, 170], "pedregosa11a": [168, 170], "pedro": [62, 171], "penal": [66, 70, 101], "penalti": [64, 65, 70, 79, 92, 98, 100, 101, 112, 123, 124], "pennsylvania": [11, 104, 109, 170], "pension": [64, 92, 93, 173], "peopl": [64, 92, 93], "pep8": 172, "per": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 63, 69, 70, 71, 72, 76, 91], "percent": [112, 123], "percentag": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40], "perf_count": 75, "perfectli": [89, 97, 124], "perform": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 52, 61, 63, 65, 67, 69, 70, 71, 72, 75, 76, 80, 84, 87, 90, 91, 93, 99, 100, 101, 103, 112, 113, 117, 118, 120, 122, 123, 124, 126, 127, 129, 139, 140, 156, 168, 170, 171, 173], "performance_result": 77, "perfrom": 87, "perhap": 173, "period": [12, 14, 16, 17, 19, 62, 67, 68, 71, 106, 109, 125, 126, 127, 128, 129, 130, 142, 144, 171, 172], "perp": [124, 138], "perrot": [168, 170], "person": 173, "pessimist": 100, "peter": 171, "petra": 172, "petronelaj": 172, "pfister": [65, 112, 122, 168, 170], "phi": [63, 91, 111, 156], "philipp": [100, 168, 171], "philippbach": [168, 172], "pi": [30, 34, 42, 45, 111, 124, 140, 154, 155], "pi_": [43, 63, 91], "pi_0": [140, 154, 155], "pi_i": [66, 101, 124], "pick": [97, 173], "pip": [97, 124], "pip3": 169, "pipe": 65, "pipe_forest_classif": 65, "pipe_forest_regr": 65, "pipe_lasso": 65, "pipelin": [48, 49, 53, 54, 65, 92, 119, 120, 122, 172], "pipelineinot": 76, "pipeop": 65, "pira": [64, 92, 93, 99, 173], "pivot": [77, 84, 91, 171], "pivot_logloss": 77, "pivot_rmse_g0": 77, "pivot_rmse_g1": 77, "place": 172, "plai": [90, 173], "plan": [10, 64, 92, 93, 173], "plausibl": [100, 157], "pleas": [48, 49, 53, 54, 62, 67, 68, 70, 76, 78, 90, 100, 112, 139, 168, 169], "plim": 94, "pliv": [20, 21, 38, 63, 91, 102, 111, 136, 152, 168, 172], "plm": [0, 6, 7, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 80, 89, 90, 91, 99, 102, 103, 110, 111, 112, 113, 116, 117, 139, 156, 163, 172, 173], "plot": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 52, 61, 62, 64, 65, 66, 68, 69, 70, 71, 72, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 96, 97, 99, 100, 101, 111, 157, 163], "plot_data": [69, 72], "plot_effect": [13, 16, 69, 70, 71, 72], "plot_optimization_histori": 76, "plot_parallel_coordin": 76, "plot_param_import": 76, "plot_tre": [52, 95, 111], "plotli": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 76, 81, 82, 84, 97, 100], "plr": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 65, 90, 94, 99, 102, 112, 113, 117, 137, 139, 153, 156, 163, 165, 166, 167, 168, 170, 172, 173], "plr_est": 94, "plr_est1": 94, "plr_est2": 94, "plr_obj": 94, "plr_obj_1": 94, "plr_obj_2": 94, "plr_summari": 92, "plrglmnet": 64, "plrranger": 64, "plrrpart": 64, "plrxgboost8700": 64, "plt": [67, 68, 69, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 96, 97, 100, 101], "plt_smpl": [63, 91], "plt_smpls_cluster": [63, 91], "plug": [87, 157, 159, 160, 161, 162, 163, 164, 165], "pm": [46, 63, 91, 156, 157, 163, 167], "pmatrix": [66, 101], "pmlr": [75, 84, 90], "po": [65, 112, 120, 122], "poe": 171, "point": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 62, 63, 77, 85, 86, 91, 100, 111, 124, 173], "pointwis": [51, 83, 85, 86, 96], "poli": [64, 88, 91, 92], "polici": [26, 37, 38, 39, 52, 89, 110, 124, 135, 136, 137, 170, 171, 172], "policy_tre": [26, 95, 111], "policy_tree_2": 95, "policy_tree_obj": 111, "policytre": 95, "polit": 94, "poly_dict": 92, "polynomi": [10, 11, 47, 64, 73, 88, 92, 97], "polynomial_featur": [10, 11, 64, 73], "polynomialfeatur": [88, 91, 92], "poor": 98, "pop": [140, 142], "popul": [100, 124, 127, 129, 140, 144], "popular": [75, 124, 157, 158], "porport": 99, "posit": [45, 64, 69, 71, 72, 75, 77, 94, 100, 173], "posixct": [65, 112, 120], "possibl": [5, 6, 7, 8, 9, 49, 53, 54, 62, 65, 69, 71, 72, 75, 76, 81, 82, 85, 86, 87, 88, 90, 95, 97, 98, 99, 100, 112, 113, 114, 117, 119, 122, 124, 128, 131, 132, 156, 157, 158, 172, 173], "possibli": [98, 157, 158], "post": [16, 42, 45, 124, 126, 128, 130, 156, 171], "postdoubl": 171, "poster": 94, "potenti": [17, 19, 22, 23, 24, 27, 28, 30, 31, 35, 37, 47, 66, 67, 69, 72, 76, 88, 94, 97, 101, 126, 130, 131, 138, 145, 146, 156, 164, 169, 172, 173], "potential_level": [76, 77, 78], "power": [65, 90, 98, 100, 112, 123, 171], "pp": [62, 75, 84, 90], "pq": [27, 28, 29, 93, 150, 172], "pq_0": [93, 96], "pq_1": [93, 96], "pr": [30, 60, 63, 64, 65, 66, 112, 120, 121, 122, 123, 124, 139, 140, 156, 170, 173], "practic": [75, 100, 171], "pre": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 66, 67, 69, 70, 71, 72, 101, 112, 116, 121, 124, 126, 127, 128, 129, 140, 142, 144, 172], "precis": [62, 124, 157, 165, 173], "precomput": [49, 54], "pred": [62, 90], "pred_df": 95, "pred_dict": [112, 114], "pred_treat": 95, "predict": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 51, 52, 53, 54, 55, 61, 63, 64, 65, 70, 75, 76, 77, 80, 83, 84, 88, 90, 91, 92, 95, 98, 100, 103, 111, 114, 115, 120, 121, 123, 139, 157, 158, 163, 165, 172, 173], "predict_proba": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 39, 46, 48, 53, 90, 98, 112, 115], "predictor": [22, 26, 39, 51, 52, 81, 82, 85, 86, 100, 102], "prefer": [64, 92, 93, 173], "preliminari": [24, 61, 80, 97, 140, 146, 149, 150, 151, 155], "prepar": [62, 63, 76, 91, 172], "preprint": [98, 171], "preprocess": [64, 70, 76, 88, 91, 92, 93, 112, 120, 122], "presenc": [64, 92, 93], "present": [23, 62, 70, 100, 112, 122, 140, 148, 173], "prespecifi": 99, "pretreat": [12, 14, 15, 16, 62, 67], "prettenhof": [168, 170], "preval": 100, "prevent": [139, 172], "previou": [68, 87, 88, 94, 169, 173], "previous": [70, 98, 112, 121, 122, 173], "price": [63, 91], "priliminari": [27, 29], "primari": [77, 78], "principl": [76, 157, 158], "print": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 55, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 169, 170, 172, 173], "print_detail": 62, "print_period": [14, 16], "prior": [75, 77, 124, 138], "privat": 172, "prob": 65, "prob_dist": 98, "prob_dist_": 98, "probabilit": [87, 89], "probability_from_treat": 89, "probabl": [17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 53, 61, 62, 66, 67, 69, 72, 78, 80, 87, 89, 94, 96, 97, 98, 100, 101, 103, 124, 140, 141, 142, 143, 144, 149, 171], "problem": [64, 70, 92, 93, 111, 112, 123], "proce": 76, "procedur": [61, 63, 64, 75, 76, 80, 91, 92, 99, 100, 112, 114, 156, 169, 172], "proceed": [42, 171], "process": [14, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 47, 56, 62, 66, 67, 68, 71, 72, 75, 81, 82, 83, 84, 85, 86, 89, 90, 95, 96, 100, 101, 110, 156, 157, 158, 171, 172], "processor": [55, 56], "produc": 94, "product": [75, 81, 82, 84, 100, 157, 167], "producton": 63, "program": [34, 64, 92, 93, 171, 173], "progress": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 74], "project": [65, 81, 82, 111, 168, 172], "project_z": [81, 82], "prone": 140, "pronounc": 97, "propens": [14, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 40, 41, 55, 56, 64, 66, 67, 69, 70, 72, 75, 76, 77, 87, 88, 92, 93, 100, 101, 111, 124, 127, 129, 131, 140, 142, 144, 157, 164, 172], "propensity_scor": 55, "proper": 77, "properli": [77, 90, 173], "properti": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 64, 65, 69, 70, 71, 72, 75, 76, 92, 93, 94, 98, 99, 104, 105, 107, 108, 109, 112, 120, 121, 124, 157, 163, 170, 172], "proport": [99, 157, 158, 166, 167], "propos": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 63, 65, 91, 97, 157, 158, 171, 172], "provid": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 53, 54, 55, 56, 62, 63, 64, 65, 70, 77, 81, 82, 85, 86, 88, 90, 91, 92, 97, 98, 100, 102, 103, 104, 106, 109, 110, 112, 114, 116, 119, 120, 121, 122, 123, 156, 168, 170, 172, 173], "prune": [26, 52], "ps911c": 91, "ps944": 91, "ps_processor_config": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 81], "pscore1": 94, "pscore2": 94, "psi": [20, 21, 61, 62, 63, 91, 102, 124, 127, 128, 129, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 167, 170], "psi_": [156, 157, 160, 162, 163, 166, 167], "psi_a": [20, 25, 26, 38, 39, 61, 63, 80, 91, 124, 127, 128, 129, 139, 140, 141, 142, 143, 144, 145, 147, 148, 152, 153, 156], "psi_b": [20, 25, 26, 38, 39, 61, 80, 111, 124, 127, 128, 129, 139, 140, 141, 142, 143, 144, 145, 147, 148, 152, 153], "psi_el": [89, 139, 140], "psi_j": 156, "psi_nu2": [157, 163], "psi_sigma2": [157, 163], "psprocessor": [56, 172], "psprocessorconfig": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 55], "public": [60, 79, 172], "publish": [100, 168, 172], "pull": [64, 172], "purchas": 100, "pure": 100, "purp": [81, 82], "purpos": [61, 80, 87, 99, 100, 140, 142, 144, 157, 158, 170], "pval": 156, "px": [84, 97], "py": [68, 69, 70, 71, 72, 81, 91, 92, 97, 98, 100, 168, 169, 172], "py3": 169, "py_al": 80, "py_did": 67, "py_did_pretest": 68, "py_dml": 80, "py_dml_nosplit": 80, "py_dml_po": 80, "py_dml_po_nosplit": 80, "py_double_ml_apo": 78, "py_double_ml_bas": 80, "py_double_ml_basic_iv": 79, "py_double_ml_c": 81, "py_double_ml_cate_plr": 82, "py_double_ml_cvar": 83, "py_double_ml_firststag": 84, "py_double_ml_g": 85, "py_double_ml_gate_plr": 86, "py_double_ml_gate_sensit": 87, "py_double_ml_irm_vs_apo": 88, "py_double_ml_lplr": 89, "py_double_ml_meets_flaml": 90, "py_double_ml_multiway_clust": 91, "py_double_ml_pens": 92, "py_double_ml_pension_qt": 93, "py_double_ml_plm_irm_hetfx": 94, "py_double_ml_policy_tre": 95, "py_double_ml_pq": 96, "py_double_ml_rdflex": 97, "py_double_ml_robust_iv": 98, "py_double_ml_sensit": 99, "py_double_ml_sensitivity_book": 100, "py_double_ml_ssm": 101, "py_learn": 75, "py_non_orthogon": 80, "py_optuna": 76, "py_panel": 69, "py_panel_data_exampl": 70, "py_panel_simpl": 71, "py_po_al": 80, "py_rep_c": 72, "py_tabpfn": 77, "pypi": [171, 172], "pyplot": [67, 68, 69, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 96, 97, 100, 101], "pyproject": 172, "pyreadr": 70, "python": [45, 62, 90, 97, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 116, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 157, 158, 163, 168, 170, 171, 172, 173], "python3": [68, 69, 70, 71, 72, 91, 92, 97, 100, 169], "pytorch": 77, "q": [65, 83, 96, 97, 112, 122, 168, 170], "q2": [65, 73, 104, 109, 170], "q3": [65, 73, 104, 109, 170], "q4": [65, 73, 104, 109, 170], "q5": [65, 73, 104, 109, 170], "q6": [65, 73, 104, 109, 170], "q_i": [97, 124], "qquad": 34, "qte": [83, 93, 172], "quad": [17, 18, 19, 41, 64, 66, 67, 92, 95, 97, 101, 111, 124, 126, 130, 138, 140, 149, 156, 157, 159, 160], "quadrat": [66, 101], "qualiti": [99, 102, 172], "quanitl": 93, "quant": 83, "quantifi": [100, 124, 130], "quantil": [16, 23, 24, 27, 28, 29, 35, 69, 72, 76, 78, 83, 88, 99, 110, 112, 120, 121, 146, 149, 150, 171, 172], "quantiti": [60, 79, 100], "queri": 92, "question": [100, 173], "quick": 93, "quit": [75, 76, 77, 95, 99, 157, 158], "r": [25, 33, 48, 49, 53, 54, 68, 69, 70, 71, 72, 80, 81, 82, 84, 91, 94, 97, 98, 100, 102, 103, 104, 109, 110, 120, 121, 122, 123, 124, 133, 139, 140, 147, 151, 152, 156, 157, 158, 164, 165, 166, 167, 168, 170, 171, 172, 173], "r2_d": [34, 75], "r2_score": [49, 54], "r2_y": [34, 75], "r6": [65, 172], "r_0": [25, 37, 38, 41, 64, 89, 92, 124, 133, 135, 140, 151], "r_all": 61, "r_d": 34, "r_df": 91, "r_dml": 61, "r_dml_nosplit": 61, "r_dml_po": 61, "r_dml_po_nosplit": 61, "r_double_ml_bas": 61, "r_double_ml_basic_iv": 60, "r_double_ml_did": 62, "r_double_ml_multiway_clust": 63, "r_double_ml_pens": 64, "r_double_ml_pipelin": 65, "r_double_ml_ssm": 66, "r_hat": [38, 89], "r_hat0": 25, "r_hat1": 25, "r_non_orthogon": 61, "r_po_al": 61, "r_y": 34, "rais": [5, 6, 7, 8, 9, 37, 48, 49, 53, 54, 69, 70, 71, 72, 112, 114], "randint": 94, "randn": 30, "random": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 36, 37, 38, 39, 45, 46, 47, 60, 61, 62, 64, 65, 67, 68, 69, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 106, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 123, 125, 126, 127, 129, 130, 139, 154, 156, 157, 163, 167, 170, 171, 173], "random_search": [112, 123], "random_st": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 69, 72, 76, 77, 80, 87, 88, 95], "randomforest": [64, 75, 77, 92], "randomforest_class": [64, 81, 89, 92, 95], "randomforest_reg": [81, 89, 95], "randomforestclassifi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 73, 75, 77, 78, 81, 82, 85, 86, 87, 89, 92, 95, 97, 99, 100, 111, 112, 114, 115, 124, 125, 126, 127, 129, 173], "randomforestregressor": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 73, 75, 77, 78, 80, 81, 82, 85, 86, 87, 89, 92, 95, 97, 99, 100, 102, 111, 112, 113, 114, 115, 116, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170, 173], "randomized_search": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 112, 118], "randomizedsearchcv": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 112, 118], "randomli": [61, 63, 80, 91, 103, 139, 173], "rang": [8, 17, 19, 61, 67, 68, 75, 76, 77, 78, 80, 83, 85, 86, 89, 90, 91, 93, 95, 96, 97, 98, 100, 101, 103, 112, 123, 124], "rangeindex": [69, 70, 71, 72, 73, 76, 77, 78, 87, 89, 91, 92, 93, 99, 104, 106, 109, 170], "ranger": [62, 64, 65, 102, 112, 120, 121, 123, 124, 139, 140, 156, 170, 173], "rangl": [32, 95], "rank": 172, "rate": [75, 84, 124], "rather": [97, 100, 124], "ratio": [112, 123, 139, 157, 158], "rational": 70, "ravel": [81, 82], "raw": [55, 64, 70, 71, 84, 92], "raw_res_manual_lasso_r_100_n_100_p_200_rho_0": 84, "rbind": 64, "rbindlist": 64, "rbinom": 60, "rbrace": [25, 26, 33, 34, 36, 63, 91, 102, 124, 131, 133, 134, 139, 140, 145, 156, 157, 164], "rcolorbrew": 63, "rcparam": [68, 73, 81, 82, 83, 85, 86, 88, 91, 92, 93, 96], "rd": [124, 172], "rda": 70, "rdbu": 63, "rdbu_r": 91, "rdbwselect": 124, "rdd": [0, 8, 107, 109, 110, 169], "rdflex": [97, 124, 172], "rdflex_fuzzi": 97, "rdflex_fuzzy_stack": 97, "rdflex_obj": [46, 124], "rdflex_sharp": 97, "rdflex_sharp_stack": 97, "rdrobust": [46, 97, 124, 169, 172], "rdrobust_fuzzi": 97, "rdrobust_fuzzy_noadj": 97, "rdrobust_sharp": 97, "rdrobust_sharp_noadj": 97, "rdt044": 84, "re": [69, 91, 100, 169], "read": [70, 169], "read_csv": [71, 84], "read_r": 70, "readabl": [77, 172], "reader": [70, 98], "readili": 168, "real": [64, 92, 93, 99, 157, 158], "realat": 124, "realiz": [97, 124, 138], "reason": [5, 6, 7, 8, 9, 60, 75, 77, 79, 84, 90, 99, 100, 157, 158, 173], "recal": [73, 157, 167], "receiv": [17, 19, 69, 72, 78, 97, 124, 126, 128], "recent": [90, 124, 126, 130, 171], "recogn": [64, 92, 93], "recommend": [65, 69, 72, 75, 76, 77, 97, 100, 102, 124, 139, 157, 169, 171, 172], "recov": [60, 62, 79, 94], "recsi": 171, "red": [63, 66, 76, 77, 85, 86, 90, 91], "reduc": [64, 76, 87, 90, 92, 97, 99, 100, 124, 172], "redund": 172, "reemploy": [11, 104, 109, 170], "ref": 70, "refactor": 172, "refer": [10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 64, 68, 69, 72, 77, 78, 87, 89, 92, 93, 97, 99, 104, 109, 110, 111, 120, 124, 125, 127, 129, 130, 140, 157, 158, 163, 171, 172], "reference_level": [23, 76, 77, 78, 88, 124], "refin": 172, "refit": [157, 158], "reflect": [95, 100, 111], "reg": [17, 18, 19, 64, 92, 173], "reg_estim": 97, "reg_learn": 93, "reg_learner_1": 75, "reg_learner_2": 75, "regard": [100, 168], "regener": 172, "region": [63, 83, 91, 156, 171], "regr": [60, 61, 62, 63, 64, 65, 66, 102, 112, 120, 121, 122, 123, 124, 139, 140, 142, 156, 170, 173], "regravg": [65, 112, 120], "regress": [8, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 51, 56, 60, 62, 63, 65, 66, 67, 69, 70, 71, 72, 76, 78, 79, 84, 89, 90, 91, 94, 98, 99, 100, 101, 102, 103, 107, 109, 110, 111, 112, 113, 116, 117, 118, 119, 121, 123, 126, 127, 129, 133, 134, 135, 136, 137, 139, 144, 156, 158, 163, 164, 165, 166, 167, 168, 170, 171, 172, 173], "regressor": [37, 49, 54, 61, 64, 75, 77, 78, 80, 83, 89, 90, 92, 98, 103], "regular": [42, 110, 112, 118, 140, 156, 171], "reich": [65, 112, 122], "reinforc": 171, "reject": [64, 92], "rel": [64, 70, 76, 77, 92, 98, 124, 125, 157, 158, 164, 165], "relat": [88, 100, 173], "relationship": [60, 77, 79, 84, 100, 156], "relev": [12, 14, 15, 16, 32, 48, 49, 51, 53, 54, 69, 72, 83, 95, 96, 104, 109, 124, 157, 173], "reli": [16, 67, 68, 69, 72, 81, 82, 87, 88, 111, 112, 114, 115, 124, 126, 140, 144, 157, 158, 173], "reload": 64, "remain": [62, 104, 109, 156, 173], "remark": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 61, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 93, 99, 111, 112, 114, 124, 125, 126, 127, 129, 139, 140, 141, 142, 143, 144, 149, 150, 156, 157, 160, 162, 165], "remot": 169, "remov": [4, 14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 64, 68, 81, 88, 100, 104, 109, 110, 112, 124, 139, 140, 157, 172], "renam": [69, 72, 92, 172], "render": [76, 99, 100], "renorm": [17, 19], "reorgan": 172, "rep": [61, 66, 103, 112, 121, 156], "repeat": [12, 14, 15, 16, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 61, 63, 64, 65, 66, 69, 70, 71, 76, 80, 87, 91, 92, 93, 94, 97, 99, 101, 103, 105, 109, 110, 112, 113, 122, 127, 128, 129, 130, 156, 159, 160, 170, 172, 173], "repeatedkfold": 91, "repet": 99, "repetit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 51, 70, 75, 81, 82, 84, 85, 86, 87, 110, 112, 113, 156, 170, 172, 173], "replac": [95, 100, 172], "replic": [10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 64, 70, 71, 80, 84, 98, 100], "repo": 172, "report": [64, 90, 92, 168, 172], "repositori": [69, 72, 84, 97, 172], "repr": [61, 63], "repres": [17, 19, 77, 94, 100, 124], "represent": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 69, 76, 99, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 170, 172], "reproduc": 35, "request": [48, 49, 53, 54, 172], "requir": [38, 39, 48, 53, 56, 60, 64, 65, 69, 70, 71, 72, 76, 77, 78, 87, 92, 93, 99, 104, 109, 114, 115, 119, 121, 124, 125, 127, 128, 129, 140, 142, 144, 156, 157, 158, 163, 169, 172, 173], "requirenamespac": 62, "rerun": [70, 76], "res_df": 91, "res_dict": [31, 32, 35, 40, 47], "resampl": [37, 60, 63, 65, 66, 67, 69, 70, 71, 72, 91, 93, 99, 101, 112, 113, 123, 124, 126, 127, 129, 139, 140, 156, 168, 170, 173], "resdat": 72, "research": [63, 65, 91, 94, 100, 139, 168, 170, 171, 173], "resembl": [66, 101], "reset": 62, "reset_index": [69, 72, 84, 91, 92], "reshap": [68, 80, 81, 82, 88, 104, 109], "reshape2": 63, "residu": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 49, 54, 99, 157, 158, 166, 167], "resolut": [65, 112, 122, 123], "resourc": 75, "resourcewis": 75, "respect": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 64, 69, 70, 71, 72, 76, 78, 92, 93, 97, 111, 124, 128, 133, 138, 139, 157, 167, 173], "respons": [10, 65, 76, 112, 120, 121], "rest": 124, "restart": 169, "restrict": 75, "restructur": 172, "restud": 84, "result": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 60, 61, 62, 65, 66, 67, 68, 69, 70, 71, 72, 75, 77, 78, 80, 81, 82, 84, 87, 88, 95, 97, 98, 99, 100, 101, 103, 112, 123, 139, 140, 141, 142, 143, 144, 157, 158, 163, 170, 172], "result_iivm": 64, "result_irm": 64, "result_plr": 64, "result_typ": 16, "results_df": 98, "retain": [48, 49, 53, 54], "retina": 94, "retir": [64, 92, 93, 99], "return": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 52, 53, 54, 55, 57, 61, 62, 63, 65, 66, 68, 69, 70, 71, 72, 75, 76, 80, 83, 90, 91, 94, 95, 96, 98, 99, 100, 101, 102, 112, 113, 117, 140, 157, 158, 172], "return_count": [75, 76, 77, 78], "return_tune_r": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76], "return_typ": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 61, 64, 65, 66, 67, 75, 80, 88, 90, 92, 93, 99, 101, 102, 103, 104, 105, 107, 108, 109, 111, 112, 113, 114, 116, 120, 121, 124, 126, 139, 140, 156, 157, 163, 170, 173], "rev": 63, "reveal": 87, "review": [42, 84, 171], "revist": [63, 91], "reweight": [124, 125], "rf": 97, "rf_argument": 77, "rho": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 57, 69, 72, 78, 87, 88, 97, 99, 100, 157, 158, 163, 167, 173], "rho_val": 100, "richter": [65, 112, 122, 168, 170], "ridg": 76, "ridgeridg": 76, "riesz": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 69, 99, 157, 158, 159, 160, 161, 162, 163, 166, 167], "riesz_rep": [157, 163], "right": [17, 19, 33, 34, 36, 42, 43, 61, 63, 75, 76, 80, 91, 92, 93, 94, 96, 97, 100, 103, 124, 140, 141, 142, 143, 144, 156, 157, 159, 160, 161, 162, 164, 165], "rightarrow_": [61, 80, 103], "risk": [24, 110, 172], "ritov": 171, "rival": 91, "rival_ind": 91, "rmd": 62, "rmse": [62, 67, 69, 70, 71, 72, 75, 77, 90, 99, 101, 112, 113, 124, 126, 127, 129, 140, 156, 170, 172], "rmse_dml_ml_l_fullsampl": 90, "rmse_dml_ml_l_lesstim": 90, "rmse_dml_ml_l_onfold": 90, "rmse_dml_ml_l_untun": 90, "rmse_dml_ml_m_fullsampl": 90, "rmse_dml_ml_m_lesstim": 90, "rmse_dml_ml_m_onfold": 90, "rmse_dml_ml_m_untun": 90, "rmse_g0": 77, "rmse_g1": 77, "rmse_oos_ml_l": 90, "rmse_oos_ml_m": 90, "rmse_oos_onfolds_ml_l": 90, "rmse_oos_onfolds_ml_m": 90, "rnorm": [60, 65, 104, 109, 112, 122, 123, 156, 170], "robin": [10, 11, 44, 63, 84, 91, 103, 168, 171], "robinson": [61, 80, 103], "robject": 91, "robu": [85, 86], "robust": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 43, 62, 69, 70, 72, 78, 87, 88, 97, 99, 100, 104, 109, 124, 157, 163, 171, 172, 173], "robust_confset": [25, 98, 172], "robust_cov": 98, "robust_length": 98, "robustscal": 76, "robustscalerrobustscal": 76, "roc\u00edo": 171, "role": [4, 5, 6, 7, 8, 9, 61, 80, 90, 103, 107, 109, 173], "romano": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 156], "root": [12, 14, 15, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 77, 84, 103, 112, 113, 140, 171], "rotat": [76, 90, 97], "roth": [97, 124, 126, 130, 171], "rough": [100, 173], "roughli": [69, 72, 100], "round": [55, 64, 70, 75, 76, 77, 78, 88, 94, 100], "rout": [48, 49, 53, 54], "row": [13, 16, 61, 64, 68, 70, 73, 76, 81, 82, 90, 91, 95, 104, 109, 124, 127, 129, 139, 170, 173], "rownam": 63, "rowv": 63, "roxygen2": 172, "royal": [100, 171], "rpart": [64, 65, 112, 120], "rpart_cv": 65, "rprocess": 75, "rpy2": 91, "rpy2pi": 91, "rskf": 88, "rsmp": [65, 112, 122, 123, 139], "rsmp_tune": [65, 112, 122, 123], "rssb": 100, "rtype": 23, "ruben": 171, "ruiz": [60, 79], "rule": [62, 111], "run": [8, 62, 70, 77, 97, 107, 109, 124, 169, 172], "runif": 60, "runner": [68, 69, 71, 72, 98, 100], "runtime_learn": 65, "runtimewarn": 72, "rv": [16, 69, 72, 78, 87, 88, 99, 100, 157, 163, 173], "rva": [69, 72, 78, 87, 88, 99, 100, 157, 163, 173], "rvert": 84, "rvert_": 84, "s1": 90, "s2": 90, "s_": [43, 63, 91, 124, 138], "s_1": 44, "s_2": 44, "s_col": [4, 9, 66, 97, 101, 108, 109, 124], "s_i": [36, 66, 97, 101, 124], "s_x": [43, 63, 91], "safeguard": [67, 112, 114], "sake": [64, 92, 100, 173], "same": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 51, 61, 63, 66, 69, 70, 72, 75, 76, 80, 81, 82, 87, 88, 91, 93, 95, 97, 98, 99, 100, 101, 112, 116, 121, 124, 127, 129, 140, 143, 144, 156, 157, 165, 172], "samii": 94, "sampl": [9, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 36, 37, 38, 39, 40, 43, 46, 48, 49, 53, 54, 60, 62, 63, 65, 67, 69, 70, 71, 72, 75, 79, 85, 86, 88, 91, 93, 95, 98, 99, 108, 109, 110, 112, 113, 123, 126, 127, 129, 130, 138, 141, 156, 157, 160, 162, 170, 171, 172], "sample_weight": [46, 48, 49, 53, 54, 97], "sampler": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76], "sant": [12, 14, 15, 17, 18, 19, 31, 35, 40, 62, 67, 69, 70, 71, 72, 124, 125, 126, 128, 130, 171], "sara": 171, "sasaki": [43, 63, 91, 171], "satisfi": [66, 70, 101, 112, 115, 119, 140, 156], "save": [61, 64, 70, 75, 76, 80, 85, 86, 90, 92, 93, 112, 114, 157, 163, 173], "savefig": 80, "saveguard": 75, "saver": [64, 92, 93], "sc": [69, 72], "scalar": 124, "scale": [17, 19, 61, 63, 68, 83, 88, 94, 96, 100, 156, 157, 160, 162, 167], "scale_color_manu": 61, "scale_fill_manu": [61, 63], "scaled_psi": 88, "scaler": 76, "scatter": [68, 77, 78, 85, 86, 89, 94, 97, 100], "scatterplot": [77, 78], "scenario": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 69, 72, 78, 87, 88, 99, 100, 124, 157, 163, 173], "scene": [81, 82, 84], "scene_camera": 84, "schacht": [75, 84, 90], "schaefer": 94, "schedul": [104, 109, 172], "scheme": [63, 91, 112, 123, 124, 125, 139, 168], "schneider": 65, "schratz": [65, 112, 122, 168, 170], "scienc": [45, 60, 79, 94, 171], "scikit": [70, 75, 92, 112, 115, 168, 170, 172, 173], "scipi": [80, 89], "score": [0, 8, 12, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 41, 46, 47, 48, 49, 50, 53, 54, 55, 56, 60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 81, 82, 83, 84, 87, 88, 90, 91, 92, 93, 96, 97, 98, 99, 100, 101, 102, 107, 109, 110, 111, 112, 113, 124, 125, 126, 127, 128, 129, 130, 131, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 165, 166, 167, 168, 172, 173], "score_col": [8, 97, 107, 109, 124], "scorer": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "scoring_method": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50], "script": 169, "sd": 60, "se": [61, 63, 76, 80, 99, 103, 112, 120, 121, 139, 156, 157, 163, 171, 173], "se_aggr": [112, 120], "se_df": 63, "se_dml": [61, 80, 103], "se_dml_po": [61, 80, 103], "se_nonorth": [61, 80], "se_orth_nosplit": [61, 80], "se_orth_po_nosplit": [61, 80], "seaborn": [13, 16, 67, 69, 72, 73, 75, 76, 77, 78, 80, 91, 92, 93, 100, 101], "seamlessli": 77, "search": [12, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 76, 117, 118, 123, 140], "search_mod": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 112, 118], "searchabl": 64, "second": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 43, 61, 63, 65, 75, 76, 77, 80, 90, 91, 102, 103, 139, 156, 157, 158, 167, 170], "secondari": [77, 78], "section": [15, 16, 18, 19, 62, 63, 64, 65, 68, 76, 87, 90, 91, 93, 100, 105, 109, 125, 127, 128, 129, 130, 148, 159, 160, 172], "secur": 94, "see": [10, 11, 12, 14, 15, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 34, 36, 37, 38, 39, 48, 49, 51, 53, 54, 60, 62, 63, 64, 65, 67, 69, 70, 71, 72, 75, 76, 77, 78, 79, 81, 82, 88, 89, 90, 91, 93, 94, 95, 97, 98, 99, 100, 112, 115, 123, 124, 126, 128, 130, 139, 140, 146, 148, 149, 150, 154, 155, 157, 158, 160, 162, 163, 167, 169, 170, 172], "seed": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 46, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 91, 92, 93, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170, 173], "seek": 94, "seem": [62, 64, 87, 92, 93, 157, 173], "seen": [85, 86, 88], "sel_cols_chiang": 91, "select": [9, 17, 19, 30, 35, 36, 42, 60, 75, 84, 97, 100, 102, 104, 108, 109, 110, 112, 120, 138, 156, 170, 171, 172, 173], "selected_coef": 75, "selected_featur": [65, 112, 120, 121], "selected_learn": 75, "self": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 51, 52, 53, 54, 75, 90, 98, 173], "selfref": 64, "semenova": [81, 82, 171], "semi": 103, "semiparametr": 10, "sens": [99, 100], "sensemakr": [157, 158], "sensit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 57, 110, 111, 124, 129, 158, 163, 167, 172], "sensitivity_analysi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 69, 72, 78, 87, 88, 99, 100, 157, 163, 173], "sensitivity_benchmark": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 78, 87, 99, 100, 157, 158], "sensitivity_el": [157, 163], "sensitivity_param": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 99, 100, 157, 158, 163], "sensitivity_plot": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 78, 87, 99, 100, 157, 163], "sensitivity_summari": [69, 72, 78, 87, 88, 99, 100, 157, 163, 173], "sensitv": 88, "sensitvity_benchmark": 78, "sensiv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39], "senstiv": [157, 166], "sep": 61, "separ": [13, 76, 77, 94, 99, 107, 108, 109, 112, 121, 124, 139, 172], "seper": [90, 97, 99, 156, 157, 158], "seq_len": [61, 66, 103], "sequenc": 91, "sequenti": [11, 112, 120], "ser": [69, 70, 71, 72], "seri": [69, 70, 71, 72, 100, 171], "serv": [17, 19, 104, 106, 109, 170, 172], "serverless": [171, 172], "servic": 94, "set": [4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 43, 44, 45, 48, 50, 52, 53, 54, 56, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 85, 86, 87, 88, 90, 91, 92, 93, 94, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 109, 111, 116, 118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 130, 139, 140, 141, 142, 143, 144, 145, 148, 156, 157, 158, 164, 165, 166, 169, 170, 172, 173], "set_as_param": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "set_config": [48, 49, 53, 54], "set_fit_request": [53, 54], "set_fold_specif": [112, 121], "set_index": 92, "set_ml_nuisance_param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 64, 73, 92, 112, 115, 116, 118, 119, 121, 123, 172], "set_param": [48, 49, 53, 54, 90, 112, 115], "set_sample_split": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 75, 88, 139, 172], "set_score_request": [48, 49, 53, 54], "set_styl": [92, 93], "set_text": 75, "set_threshold": [61, 62, 63, 64, 65, 66, 102, 112, 121, 122, 123, 124, 139, 140, 156, 170], "set_tick": 91, "set_ticklabel": 91, "set_titl": [77, 78, 88, 90, 91, 97], "set_x_d": [4, 5, 6, 7, 8, 9], "set_xlabel": [77, 78, 80, 88, 90, 91, 97], "set_xlim": 80, "set_xtick": 94, "set_xticklabel": 94, "set_ylabel": [77, 78, 88, 90, 91, 94, 97], "set_ylim": [83, 88, 90, 91, 96], "setdiff": 172, "setdiff1d": 91, "setminu": [63, 91, 156], "settings_l": 90, "settings_m": 90, "setup": [169, 172], "seven": [63, 91], "sever": [57, 64, 65, 69, 70, 71, 72, 75, 76, 77, 90, 92, 93, 99, 100, 103, 112, 173], "shadow": 89, "shape": [8, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 51, 52, 53, 54, 68, 69, 72, 75, 76, 77, 78, 81, 82, 85, 86, 91, 92, 95, 97, 99, 100, 112, 114, 124], "share": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 63, 64, 91, 92], "sharma": [100, 171], "sharp": 46, "shift": [69, 72, 89], "shock": [63, 91], "short": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 99, 100, 124, 128, 157, 158, 171, 172, 173], "shortcut": 64, "shortli": [63, 65, 91, 112, 122], "shota": 171, "should": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 53, 54, 64, 66, 69, 70, 72, 75, 76, 78, 85, 86, 92, 97, 98, 99, 101, 104, 109, 111, 112, 116, 118, 121, 124, 125, 132, 156, 157, 158, 168], "show": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 61, 63, 66, 67, 69, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 84, 87, 88, 90, 91, 94, 97, 98, 100, 101, 103, 157, 166, 169], "show_progress_bar": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76], "showcas": 95, "showlabel": 100, "showlegend": 100, "shown": [60, 79, 94, 170], "showscal": [81, 82, 84], "shrink": 97, "shuffl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 139], "side": [97, 124, 157, 163], "sigma": [17, 18, 19, 30, 31, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 61, 63, 66, 80, 91, 101, 103, 111, 139, 156, 157, 158, 160, 162, 163, 166, 167], "sigma2": [112, 120, 121, 123, 157, 163], "sigma_": [17, 18, 19, 33, 34, 36, 40, 41, 42, 43, 44, 61, 63, 80, 91, 103], "sigma_0": [157, 167], "sigma_j": 156, "sigmoid": [41, 94], "sign": [98, 100], "signal": [51, 52], "signatur": [25, 26, 27, 28, 29, 38, 39, 140], "signif": [60, 62, 63, 64, 65, 66, 112, 120, 121, 122, 123, 124, 139, 140, 156, 170, 173], "signific": [60, 63, 64, 65, 66, 69, 72, 78, 87, 88, 92, 95, 97, 99, 100, 112, 120, 121, 122, 123, 124, 139, 140, 156, 157, 163, 170, 173], "significantli": 77, "silverman": [27, 28, 29], "sim": [17, 18, 19, 31, 32, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 47, 61, 62, 63, 66, 68, 80, 83, 91, 95, 96, 101, 103, 124], "sim_data": 71, "similar": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 40, 62, 65, 70, 81, 82, 87, 90, 93, 97, 98, 99, 100, 124, 140, 141, 142], "similarli": [70, 90, 98], "simpl": [32, 53, 54, 62, 65, 76, 81, 82, 85, 86, 87, 88, 89, 95, 100, 110, 124, 157, 158], "simplest": 111, "simpli": [65, 67, 173], "simplic": [64, 75, 92, 95, 100], "simplif": [157, 159], "simplifi": [69, 72, 88, 94, 100, 111, 157, 166], "simul": [17, 18, 19, 31, 32, 33, 34, 35, 36, 40, 41, 42, 44, 45, 61, 65, 66, 69, 71, 72, 75, 80, 81, 82, 83, 84, 85, 86, 90, 96, 97, 100, 101, 103, 112, 116, 121, 123, 156, 170], "simul_data": 30, "simulaten": [124, 132], "simulation_run": 84, "simult": 62, "simultan": [70, 110, 173], "sin": [32, 35, 41, 45, 68, 81, 82, 85, 86], "sinc": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 48, 53, 64, 66, 67, 68, 75, 76, 78, 85, 86, 87, 89, 90, 92, 94, 101, 112, 114, 124, 126, 157, 163, 165, 169, 172], "singl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 67, 69, 70, 71, 72, 76, 85, 86, 93, 94, 112, 120, 156], "single_learner_pipelin": [112, 120], "singleton": 139, "sinh": 45, "sipp": [64, 92, 93], "site": [68, 69, 70, 71, 72, 91, 92, 97, 100], "situat": [63, 91], "six": [17, 19, 63], "sixth": 91, "size": [13, 16, 30, 61, 63, 64, 65, 68, 69, 70, 71, 72, 75, 77, 80, 83, 84, 87, 90, 92, 94, 95, 96, 98, 100, 102, 104, 109, 112, 117, 118, 124, 125, 139, 140, 156, 157, 160, 162, 170, 173], "sizeabl": 100, "skill": 171, "skip": 50, "sklearn": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 45, 46, 48, 49, 52, 53, 54, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 97, 98, 99, 100, 101, 102, 111, 112, 113, 114, 115, 116, 117, 118, 124, 125, 126, 127, 129, 139, 140, 156, 157, 163, 170, 173], "skotara": 100, "slide": 94, "slight": [124, 127, 129], "slightli": [12, 14, 15, 68, 69, 72, 75, 85, 86, 87, 111, 124, 128, 130, 140, 141, 142, 143, 144, 157, 158], "slow": [61, 80, 103], "slower": [61, 80, 103], "small": [32, 66, 67, 68, 76, 77, 88, 95, 101, 124, 157, 158, 165], "smaller": [41, 64, 67, 85, 86, 87, 90, 92, 97, 100, 124, 173], "smallest": [14, 75, 77], "smpl": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 63, 75, 80, 91, 139, 140], "smpls_cluster": [63, 91], "smucler": [98, 172], "sn": [67, 69, 72, 73, 75, 76, 77, 78, 80, 91, 92, 93, 100, 101], "so": [17, 19, 53, 54, 60, 64, 65, 66, 67, 69, 70, 72, 79, 90, 92, 94, 100, 101, 112, 116, 156, 173], "social": [94, 171], "societi": [63, 91, 100, 171], "softmax": [17, 19], "softwar": [65, 112, 122, 168, 170, 171, 172], "solari": 172, "sole": [70, 100], "solut": [102, 111, 140], "solv": [20, 63, 91, 111, 112, 123, 124, 127, 128, 129, 156], "solver": [92, 101, 124], "some": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 53, 54, 64, 65, 66, 67, 68, 70, 73, 75, 76, 90, 92, 93, 97, 98, 99, 101, 111, 112, 113, 114, 115, 116, 119, 121, 124, 133, 169, 172], "sometim": [75, 124, 130], "sonabend": [65, 112, 122], "sophist": [112, 118, 123], "sort": [13, 77, 92, 98, 124], "sort_bi": 13, "sort_valu": [77, 78], "sourc": [65, 112, 122, 170, 172], "sourcefileload": 84, "sp": 62, "space": [63, 76, 77, 91, 112, 117, 123], "spars": [84, 112, 117, 118, 123, 156, 170, 171], "sparsiti": 171, "spec": 171, "special": [63, 89, 91, 110, 124], "specialis": [107, 109], "specif": [12, 14, 15, 16, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 46, 63, 64, 69, 70, 71, 72, 75, 76, 77, 78, 88, 91, 92, 100, 104, 109, 110, 111, 112, 114, 116, 119, 121, 124, 127, 129, 139, 140, 148, 156, 163, 167, 168, 170, 172], "specifi": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 46, 55, 56, 60, 63, 64, 65, 66, 67, 69, 70, 71, 72, 76, 77, 78, 79, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 95, 96, 97, 99, 100, 101, 102, 104, 106, 109, 110, 111, 116, 118, 119, 120, 121, 123, 124, 145, 148, 169, 170, 172, 173], "specifii": 93, "speed": [16, 23, 29, 75], "speedup": 75, "spefici": 25, "spindler": [42, 75, 84, 90, 98, 100, 168, 171, 172], "spine": [92, 93], "spline": [81, 82, 111], "spline_basi": [81, 82, 111], "spline_grid": [81, 82], "split": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 60, 63, 65, 66, 67, 69, 70, 71, 72, 75, 88, 91, 93, 95, 99, 101, 110, 111, 112, 113, 123, 124, 126, 127, 129, 140, 156, 170, 172], "split_sampl": [75, 88], "splitter": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "sponsor": [64, 92, 93], "sprintf": 61, "sq_error": 84, "sqrt": [17, 18, 19, 31, 34, 35, 40, 61, 63, 65, 73, 80, 83, 91, 96, 103, 139, 156, 157, 158, 170], "squar": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 49, 54, 64, 77, 84, 92, 112, 113, 123, 124, 157, 167, 171], "squarederror": [64, 92, 173], "squeez": [67, 83, 89, 96, 101], "src": 92, "ssm": [9, 36, 110, 138], "ssrn": 33, "stabil": [17, 19, 87], "stabl": [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 124, 125, 168], "stack": [65, 76, 112, 120], "stacking__final_estimator__alpha": 76, "stacking__final_estimator__c": 76, "stacking__final_estimator__max_it": 76, "stacking__lgbm__lambda_l1": 76, "stacking__lgbm__lambda_l2": 76, "stacking__lgbm__learning_r": 76, "stacking__lgbm__max_depth": 76, "stacking__lgbm__min_child_sampl": 76, "stacking__lgbm__n_estim": 76, "stacking_classifi": 76, "stacking_regressor": 76, "stackingclassifi": [76, 97], "stackingregressor": [76, 97], "stackingregressorstackingregressor": 76, "stacklrn": 65, "stackrel": 124, "stage": [46, 70, 81, 82, 85, 86, 89, 95, 97, 112, 123, 124, 172, 173], "stagger": [124, 130], "stai": [124, 130], "standard": [16, 18, 62, 65, 69, 70, 71, 72, 76, 83, 85, 86, 97, 98, 105, 107, 109, 124, 125, 127, 128, 129, 139, 140, 156, 157, 163, 167, 172, 173], "standard_norm": [104, 109, 112, 117, 118, 156, 170], "standardscal": 92, "star": 124, "start": [17, 19, 62, 64, 65, 69, 70, 71, 72, 75, 76, 77, 81, 82, 84, 87, 90, 91, 92, 96, 100, 124, 126, 168, 173], "start_dat": [17, 19], "start_tim": 77, "stat": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 80, 97, 104, 109, 112, 122, 123, 124, 156, 168, 171], "stat_bin": 61, "stat_dens": 64, "state": [76, 173], "stationar": 67, "stationari": [124, 126], "statist": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 36, 37, 38, 39, 43, 57, 63, 91, 98, 99, 100, 156, 157, 163, 168, 170, 171, 172, 173], "statsmodel": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 51, 97], "statu": [62, 64, 66, 67, 92, 94, 97, 101, 124, 130], "std": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 76, 77, 78, 79, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 99, 100, 101, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 139, 140, 156, 170, 173], "stefan": 171, "step": [12, 14, 15, 16, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 64, 65, 76, 80, 85, 86, 87, 92, 95, 103, 112, 123, 124, 156, 168, 173], "stepdown": 156, "stick": [64, 92], "still": [66, 67, 81, 82, 85, 86, 87, 93, 97, 99, 101, 112, 113, 124, 128], "stochast": [38, 39, 124, 136, 137, 170], "stock": [64, 92, 93, 98], "store": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 77, 98, 102, 112, 113, 139, 140, 156, 157, 163, 172], "store_model": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 90], "store_predict": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 92, 95], "stori": [100, 171], "str": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 46, 48, 49, 50, 51, 53, 54, 55, 56, 64, 69, 72, 76, 77, 78, 85, 86, 96, 97, 98, 111, 124, 172], "straightforward": [69, 72, 75, 85, 86, 111], "strategi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 55, 94, 100, 124, 173], "stratifi": [75, 88], "stratum": 94, "strength": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 98, 99, 100, 157, 158, 163, 166], "strftime": 69, "strictli": 124, "string": [12, 13, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 111, 156, 157, 163, 170, 172], "string_label": 94, "strong": [66, 98, 101, 157, 158], "stronger": [98, 124, 128, 156, 173], "structur": [10, 11, 16, 17, 19, 44, 63, 64, 66, 70, 76, 84, 91, 92, 98, 101, 103, 112, 121, 168, 171, 173], "student": 171, "studi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 50, 63, 64, 75, 76, 84, 90, 91, 92, 93, 98, 99, 124, 125, 170, 173], "study_kwarg": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "style": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 69, 72, 90, 172], "styler": 172, "styliz": 100, "sub": [48, 49, 53, 54, 63, 91], "subclass": [106, 109, 172], "subfold": [112, 123], "subgroup": [25, 64, 92, 172], "subject": [63, 91], "submiss": 172, "submit": [69, 72], "submodel": 76, "submodul": [76, 172], "subobject": [48, 49, 53, 54], "subplot": [63, 68, 75, 77, 78, 80, 81, 82, 83, 85, 86, 88, 90, 91, 92, 93, 94, 96, 97], "subplots_adjust": 75, "subpopul": [124, 138], "subsampl": [37, 65, 75, 140, 142], "subscript": [124, 128, 140, 142, 144, 157, 158], "subsequ": [63, 77, 91], "subset": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 53, 63, 75, 76, 91, 95, 102, 111, 112, 113, 157, 158, 160, 162], "subseteq": 111, "substanti": [64, 92, 94], "substract": 156, "subtract": 156, "sudo": 169, "suffic": 100, "suffici": [75, 90, 100], "suggest": [63, 64, 76, 91, 92, 100, 172], "suggest_float": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76, 112, 117], "suggest_int": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76], "suitabl": [66, 81, 82, 101, 112, 117, 124, 128], "sum": [17, 19, 49, 54, 63, 64, 91, 92, 93, 96, 97, 111, 156], "sum_": [17, 19, 47, 61, 63, 80, 91, 97, 102, 103, 111, 124, 125, 130, 156], "sum_i": 94, "sum_oth": 91, "sum_riv": 91, "summar": [13, 62, 69, 70, 71, 72, 77, 78, 94, 100, 102, 157, 163], "summari": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 62, 63, 65, 66, 67, 69, 70, 71, 72, 73, 76, 78, 79, 81, 82, 83, 85, 86, 87, 88, 89, 91, 93, 96, 98, 99, 100, 101, 103, 104, 106, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 139, 140, 156, 157, 170, 172, 173], "summary_df": 98, "summary_result": 64, "summary_stat": 77, "superior": 77, "suppli": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 76, 81, 82, 85, 86, 87, 95, 104, 109, 111, 157, 158, 163, 164], "support": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 46, 56, 62, 63, 69, 70, 71, 72, 75, 89, 91, 95, 97, 108, 109, 112, 114, 116, 121, 123, 124, 133, 173], "support_s": [32, 81, 82, 85, 86, 95], "support_t": 95, "support_w": 95, "suppos": 100, "suppress": [62, 64, 65, 66, 76], "suppresswarn": 61, "suprema": 156, "suptitl": [75, 83, 90, 93, 96], "supxlabel": [83, 93, 96], "supylabel": [83, 93, 96], "sure": [77, 78, 112, 119, 172], "surfac": [76, 81, 82, 84], "surgic": 98, "surpress": [63, 170], "survei": [64, 92, 93, 173], "susan": 171, "sven": [100, 168, 171], "svenklaassen": [168, 172], "svg": [61, 80], "switch": [61, 80, 100, 103], "symbol": 100, "symmetr": 45, "syntax": [97, 124], "syntaxwarn": 91, "synthesi": 171, "synthet": [17, 19, 32, 41, 47, 60, 77, 79, 81, 82, 83, 85, 86, 89, 90, 95, 96, 98], "syrgkani": [98, 100, 171], "system": 171, "szita": 171, "t": [4, 5, 7, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 48, 49, 53, 54, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 76, 77, 78, 79, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 105, 106, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 139, 140, 141, 142, 156, 157, 159, 160, 170, 173], "t_": [16, 69, 70, 71, 72, 124, 127, 128, 129, 140, 142, 144], "t_0": [37, 140, 151], "t_1_start": 75, "t_1_stop": 75, "t_2_start": 75, "t_2_stop": 75, "t_3_start": 75, "t_3_stop": 75, "t_col": [4, 5, 7, 15, 16, 69, 70, 71, 72, 105, 106, 109, 124, 125, 126, 127, 129], "t_df": 95, "t_diff": 68, "t_dml": 61, "t_g": [17, 19], "t_i": [67, 95, 97, 124, 126, 127, 130, 140, 142], "t_idx": 68, "t_nonorth": 61, "t_orth_nosplit": 61, "t_sigmoid": 95, "t_stat": 156, "t_value_ev": 14, "t_value_pr": 14, "tabl": [61, 63, 64, 65, 66, 77, 78, 98, 102, 104, 109, 112, 120, 121, 123, 124, 139, 140, 156, 170, 173], "tabpfn": 172, "tabpfnclassifi": 77, "tabpfnregressor": 77, "tabular": [75, 77, 104, 109, 156, 170, 173], "taddi": 171, "tailor": [105, 109], "takatsu": 98, "take": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 40, 66, 67, 68, 69, 70, 71, 72, 75, 76, 81, 82, 83, 84, 85, 86, 93, 96, 97, 98, 99, 101, 102, 111, 112, 114, 124, 125, 130, 133, 134, 135, 136, 137, 140, 145, 148, 157, 164, 165, 166, 170], "taken": [64, 92, 93, 173], "taker": [25, 172], "talk": 173, "target": [12, 14, 15, 20, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 53, 54, 60, 63, 64, 65, 66, 75, 81, 82, 91, 111, 112, 113, 120, 121, 122, 123, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 138, 139, 140, 149, 150, 156, 157, 165, 167, 168, 170, 172, 173], "task": [60, 77, 90, 104, 109, 120, 139, 173], "task_typ": 172, "tau": [47, 68, 83, 93, 94, 96, 97, 111, 124, 140, 146, 149, 150], "tau_": [94, 97, 124], "tau_0": [97, 124], "tau_1": 94, "tau_2": 94, "tau_vec": [83, 93, 96], "tax": [64, 92, 93], "te": [62, 81, 82, 95], "techniqu": [61, 80, 103, 139, 173], "teen": 70, "teichert": 172, "templat": 172, "ten": 90, "tend": [64, 92, 93, 124], "tensor": [81, 82], "tenth": 171, "term": [7, 14, 61, 63, 64, 65, 68, 80, 84, 91, 92, 94, 100, 103, 124, 130, 168, 173], "termin": [65, 112, 122, 123], "terminatorev": 65, "test": [11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 37, 38, 39, 48, 49, 53, 54, 60, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 80, 87, 91, 98, 100, 103, 112, 120, 121, 122, 123, 124, 139, 140, 156, 170, 171, 172, 173], "test_id": [63, 139], "test_ind": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "test_indic": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "test_set": 139, "test_siz": 80, "text": [16, 17, 18, 19, 31, 33, 35, 37, 40, 41, 46, 47, 63, 64, 69, 70, 71, 72, 77, 83, 84, 89, 94, 95, 96, 97, 100, 111, 124, 127, 128, 129, 130, 135, 139, 140, 142, 144, 151, 157, 160, 162], "textbf": [102, 112, 121, 123, 173], "textposit": 100, "textrm": [157, 158, 164, 165, 166, 167], "tg": [65, 73, 104, 109, 170], "th": [63, 91], "than": [26, 61, 62, 64, 75, 80, 84, 88, 92, 93, 94, 97, 98, 99, 100, 103, 124, 128, 157, 163, 173], "thank": [62, 64, 65, 92, 172], "thatw": 68, "thei": [17, 19, 62, 64, 68, 85, 86, 92, 94, 98, 104, 109, 124, 157, 167], "them": [13, 64, 65, 81, 82, 83, 87, 90, 92, 96, 124], "theme": [63, 64], "theme_minim": [61, 64, 66], "theorem": [124, 130, 157, 167], "theoret": [75, 100, 139, 171], "theori": [111, 171], "therebi": [63, 65, 91, 173], "therefor": [69, 71, 72, 78, 94, 97, 99, 139, 140, 157, 166], "thereof": 41, "theta": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 43, 45, 61, 63, 65, 66, 67, 68, 69, 72, 75, 76, 77, 78, 80, 84, 87, 88, 91, 97, 99, 100, 101, 102, 103, 104, 109, 111, 112, 114, 117, 118, 122, 123, 124, 125, 127, 128, 129, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 152, 153, 154, 155, 156, 157, 158, 163, 166, 167, 170, 173], "theta_": [17, 19, 69, 72, 78, 97, 100, 111, 124, 131, 132, 156, 157, 167], "theta_0": [25, 26, 32, 37, 38, 39, 61, 63, 64, 66, 76, 78, 80, 81, 82, 84, 85, 86, 91, 92, 100, 101, 103, 111, 124, 126, 133, 134, 136, 137, 138, 140, 149, 150, 153, 156, 157, 164, 165, 167, 170], "theta_d": 77, "theta_dml": [61, 80, 103], "theta_dml_po": [61, 80, 103], "theta_initi": 80, "theta_nonorth": [61, 80], "theta_orth_nosplit": [61, 80], "theta_orth_po_nosplit": [61, 80], "theta_resc": 61, "theta_t": 68, "thi": [4, 12, 13, 14, 15, 16, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 48, 49, 50, 52, 53, 54, 56, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 111, 112, 114, 117, 118, 122, 123, 124, 128, 130, 131, 132, 133, 134, 139, 140, 141, 142, 143, 144, 146, 148, 149, 150, 156, 157, 158, 163, 164, 165, 168, 169, 170, 171, 172, 173], "think": 65, "third": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 70, 71, 80, 91, 103, 139], "thirion": [168, 170], "this_df": [84, 92], "this_split_ind": 91, "those": [62, 64, 70, 92, 93, 98], "though": [60, 79, 94], "thread": [94, 112, 120, 121, 123], "three": [63, 65, 76, 85, 86, 169, 172], "threshold": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 55, 56, 97, 100, 112, 120, 121, 123, 124], "through": [62, 70, 83, 85, 86, 96, 97, 104, 109, 112, 123, 124], "throughout": [87, 98], "thu": [90, 97, 111, 124], "tibbl": 62, "tick": 16, "tick_param": 97, "tight": 80, "tight_layout": [69, 72, 76, 89, 90, 91, 97], "tighter": [76, 97], "tild": [17, 18, 19, 31, 35, 40, 63, 91, 94, 102, 111, 139, 140, 142, 144, 149, 150, 154, 155, 156, 157, 166, 167], "tile": [76, 98], "time": [5, 7, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 42, 43, 61, 62, 63, 64, 66, 67, 68, 70, 76, 77, 80, 84, 85, 86, 91, 92, 93, 97, 99, 100, 101, 105, 106, 109, 124, 125, 126, 127, 128, 129, 130, 140, 157, 171, 172, 173], "time_budget": 90, "time_df": 68, "time_period": 68, "time_typ": [17, 19, 69, 72], "timeout": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "titiunik": [124, 171], "titl": [13, 16, 63, 64, 66, 69, 70, 72, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 93, 94, 96, 97, 100, 168], "title_fonts": [69, 72], "tmp": [68, 81, 91], "tname": 62, "tnr": [65, 112, 122, 123], "to_datetim": 69, "to_fram": 95, "to_numpi": [83, 87, 93, 96], "to_str": [76, 77], "todo": [63, 73], "toeplitz": 84, "togeth": [85, 86, 108, 109, 156], "toler": 91, "tomasz": [171, 172], "toml": 172, "tongarlak": 172, "too": [56, 75], "tool": [62, 65, 99, 173], "top": [63, 75, 91, 92, 93, 97, 100, 124, 168], "total": [49, 54, 64, 90, 92, 124, 125], "total_width": [76, 77], "tpe": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "tpesampl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76], "tpot": 90, "track": [105, 107, 109], "tracker": 168, "tradit": [77, 156], "train": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 53, 54, 61, 63, 65, 75, 77, 80, 81, 82, 83, 85, 86, 88, 89, 91, 92, 95, 96, 102, 103, 139], "train_id": [63, 139], "train_ind": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "train_indic": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "train_set": 139, "train_test_split": 80, "transact": 171, "transform": [31, 40, 41, 47, 77, 88, 94, 100, 112, 117, 173], "translat": [77, 84], "transpos": 68, "treament": 95, "treat": [16, 17, 18, 19, 26, 55, 62, 67, 68, 69, 70, 71, 72, 76, 78, 87, 89, 95, 97, 100, 111, 124, 126, 127, 129, 130, 134, 140, 142, 144, 156, 173], "treat1_param": 94, "treat2_param": 94, "treat_var": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 112, 116, 121], "treatment": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 41, 44, 46, 55, 60, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 78, 79, 84, 87, 89, 90, 91, 95, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 112, 113, 114, 116, 119, 121, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 138, 139, 141, 142, 143, 144, 145, 146, 148, 149, 150, 156, 160, 162, 163, 164, 166, 168, 170, 171, 172, 173], "treatment_df": 68, "treatment_effect": [32, 81, 82], "treatment_level": [22, 23, 76, 77, 78, 88, 124], "treatment_levels_plot": [76, 77], "treatment_lvl": 76, "treatment_var": [4, 5, 6, 7, 8, 9], "tree": [26, 52, 64, 65, 67, 68, 69, 72, 75, 76, 77, 92, 102, 110, 112, 120, 121, 124, 139, 140, 156, 170, 172], "tree_param": [26, 52], "tree_summari": 92, "trees_class": [64, 92], "trend": [62, 67, 68, 69, 72, 91, 124, 126, 128, 130, 171], "tri": [84, 157, 158], "trial": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76, 112, 117], "trials_datafram": 76, "triangular": [46, 97, 124], "trim": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 64, 92, 93, 100], "trimming_rul": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 93], "trimming_threshold": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 64, 81, 88, 92, 93, 95, 96, 100], "trm": [65, 112, 122, 123], "true": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 36, 37, 38, 39, 41, 46, 47, 48, 49, 50, 53, 54, 56, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 109, 112, 120, 121, 122, 123, 124, 126, 139, 140, 145, 146, 149, 150, 154, 155, 156, 157, 159, 160, 161, 162, 167, 170, 173], "true_effect": [68, 81, 82, 85, 86, 98], "true_gatet_effect": 87, "true_group_effect": 87, "true_tau": 97, "truemfunct": 98, "truncat": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 93], "trust": 76, "try": [75, 76, 99], "tune": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 75, 77, 84, 97, 110, 113, 114, 117, 118, 120, 122, 123, 124, 168, 170, 172], "tune_ml_model": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 76, 112, 117, 172], "tune_on_fold": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 112, 122, 123], "tune_r": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "tune_set": [65, 112, 122, 123], "tuned_model": 90, "tuner": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 112, 123], "tunergridsearch": 65, "tuning_result": 76, "tupl": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 69, 72, 124, 127, 128, 129], "turn": 100, "turrel": 45, "tutori": 64, "tw": [92, 93], "twice": 124, "twinx": [77, 78], "two": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 60, 61, 64, 65, 67, 69, 70, 71, 72, 75, 76, 79, 80, 83, 88, 90, 92, 93, 94, 95, 96, 98, 99, 100, 102, 103, 111, 112, 120, 123, 126, 129, 139, 149, 156, 173], "twoclass": 65, "twoearn": [64, 92, 93, 99, 173], "type": [7, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 41, 46, 47, 48, 49, 51, 52, 53, 54, 55, 57, 61, 62, 63, 64, 65, 69, 72, 75, 80, 89, 90, 91, 97, 100, 103, 110, 112, 120, 121, 124, 140, 152, 153, 156, 157, 166, 172, 173], "typeerror": [69, 70, 71, 72], "typic": [76, 77, 124, 130, 168], "u": [18, 25, 26, 27, 28, 29, 31, 32, 34, 36, 40, 49, 54, 61, 62, 63, 64, 67, 68, 69, 70, 72, 75, 76, 77, 78, 80, 83, 85, 86, 88, 91, 92, 93, 95, 96, 98, 99, 100, 103, 124, 131, 133, 134, 157, 158, 169, 173], "u_hat": [61, 80, 140], "u_i": [33, 36, 42, 45], "u_t": 18, "uehara": 171, "uhash": 65, "ulf": 171, "unabl": 76, "unambigu": 100, "unbalanc": 41, "uncap": [17, 19], "uncertainti": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 85, 86, 88, 97, 99, 157, 163, 173], "unchang": [48, 49, 53, 54], "uncondit": [64, 69, 72, 92, 173], "unconfounded": [100, 171], "under": [25, 30, 61, 64, 67, 70, 80, 92, 95, 97, 100, 103, 124, 128, 130, 138, 156, 171], "underbrac": [61, 68, 80, 103, 111], "underfit": 90, "underli": [31, 35, 64, 65, 69, 72, 76, 77, 78, 85, 86, 94, 95, 124, 130, 157, 158, 173], "underlin": [63, 91], "underset": [97, 124], "understand": [69, 70, 72, 77, 100], "undesir": [112, 114], "unevenli": 139, "unifi": 109, "uniform": [18, 46, 47, 68, 79, 81, 82, 83, 95, 96, 156], "uniform_averag": [49, 54], "uniformli": [25, 69, 72, 83, 93, 156], "union": [25, 41], "uniqu": [7, 60, 69, 70, 71, 72, 75, 76, 77, 78, 79, 97, 105, 106, 109, 124, 127, 129, 140, 157, 167], "unique_label": 90, "unit": [7, 17, 19, 61, 62, 66, 67, 68, 69, 70, 71, 72, 87, 89, 97, 101, 106, 109, 124, 126, 127, 128, 129, 130, 140, 141, 142, 143, 144, 157, 160, 162, 172], "univari": [32, 81, 82], "univers": [16, 171], "unknown": 124, "unlik": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 64, 92, 93, 100], "unobserv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 60, 64, 69, 72, 79, 92, 93, 99, 100, 124, 157, 158, 167, 173], "unpen": 62, "unstabl": [157, 158], "unter": [63, 64, 65], "untest": 100, "until": [124, 126, 172], "untreat": [89, 100, 124, 126], "untun": 77, "up": [16, 23, 29, 64, 75, 84, 89, 90, 92, 93, 99, 100, 112, 121, 123, 124, 126, 139, 157, 158, 169, 172, 173], "upcom": 172, "updat": [48, 49, 53, 54, 63, 91, 92, 171, 172], "update_layout": [81, 82, 84, 97, 100], "update_trac": [81, 82], "upload": 172, "upon": [140, 172], "upper": [25, 64, 65, 68, 69, 72, 77, 78, 80, 83, 87, 88, 89, 93, 96, 97, 99, 100, 112, 122, 123, 157, 163, 167, 173], "upper_bound": [81, 82], "upsilon": [66, 101], "upsilon_i": [66, 101], "upward": [64, 92, 93, 100], "upweight": 94, "url": [70, 84, 168, 171], "us": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 53, 54, 55, 56, 61, 63, 64, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 109, 111, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 129, 139, 140, 141, 142, 143, 144, 156, 157, 158, 163, 165, 166, 167, 168, 169, 170, 172, 173], "usa": 171, "usabl": 75, "usag": [62, 69, 70, 71, 72, 73, 76, 77, 78, 87, 89, 91, 92, 93, 99, 104, 170, 172], "use_label_encod": [92, 173], "use_other_treat_as_covari": [4, 5, 6, 7, 8, 9, 104, 109], "use_pred_offset": [112, 120, 123], "use_weight": [112, 120, 121], "usecolormap": [81, 82], "user": [20, 21, 48, 49, 53, 54, 61, 62, 63, 64, 65, 70, 75, 78, 80, 87, 88, 91, 92, 97, 99, 111, 112, 114, 119, 120, 123, 124, 140, 156, 168, 169, 170, 172, 173], "userwarn": [68, 69, 71, 72, 77, 92, 97, 98, 100], "usual": [63, 67, 69, 70, 71, 72, 75, 76, 77, 81, 82, 91, 97, 99, 100, 111, 112, 114, 139, 157, 167], "utab019": 41, "util": [0, 21, 68, 69, 72, 75, 88, 90, 94, 97, 100, 112, 114, 124, 172], "v": [10, 11, 25, 26, 34, 36, 38, 39, 42, 43, 44, 49, 54, 61, 63, 64, 69, 72, 75, 76, 77, 78, 80, 87, 88, 90, 91, 92, 94, 97, 98, 102, 103, 111, 124, 131, 133, 134, 136, 137, 156, 168, 170, 171, 172, 173], "v108": 168, "v12": [168, 170], "v2": 77, "v22": 65, "v23": 168, "v_": [43, 63, 91, 124], "v_i": [33, 34, 36, 44, 45, 61, 80, 103, 124], "v_j": 156, "val": [34, 69, 70, 71, 72, 139, 171], "val_list": 84, "valid": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 37, 38, 39, 50, 55, 56, 61, 62, 63, 64, 67, 68, 69, 70, 72, 75, 77, 80, 83, 90, 91, 92, 93, 96, 97, 100, 103, 110, 111, 112, 120, 123, 139, 140, 146, 149, 150, 157, 158, 171, 173], "valu": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 53, 54, 56, 57, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 97, 98, 99, 100, 102, 104, 105, 106, 109, 110, 112, 113, 114, 116, 118, 120, 121, 122, 123, 124, 125, 130, 131, 138, 139, 146, 149, 150, 154, 155, 156, 157, 158, 163, 167, 170, 172, 173], "value_count": 92, "van": 171, "vanderpla": [168, 170], "vanish": [61, 80, 103], "var": [17, 18, 19, 31, 35, 40, 63, 91, 94, 97, 157, 158, 164, 165, 166, 167], "var_ep": 100, "varepsilon": [25, 31, 40, 43, 63, 66, 91, 101, 111, 124, 133], "varepsilon_": [17, 19, 43, 63, 69, 72, 91], "varepsilon_0": 18, "varepsilon_1": 18, "varepsilon_d": [35, 40], "varepsilon_i": [35, 42, 66, 83, 96, 101], "vari": [17, 19, 64, 68, 69, 72, 75, 92, 94, 100], "variabl": [4, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 41, 46, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 76, 77, 78, 84, 87, 89, 90, 91, 92, 93, 97, 99, 100, 101, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 119, 120, 121, 122, 123, 124, 126, 127, 129, 130, 131, 133, 134, 135, 136, 137, 139, 140, 156, 157, 158, 163, 167, 170, 171, 172, 173], "varianc": [20, 21, 63, 65, 91, 97, 99, 100, 110, 124, 139, 157, 158, 163, 165, 166, 167, 170, 172], "variant": [62, 70, 88], "variat": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 88, 99, 157, 158, 167], "variou": [62, 90, 100, 112, 123, 173], "varoquaux": [168, 170], "vasili": [100, 171], "vast": 77, "vector": [12, 14, 15, 17, 19, 22, 24, 25, 26, 27, 28, 30, 32, 33, 34, 36, 37, 38, 39, 42, 43, 45, 60, 63, 64, 66, 67, 79, 85, 86, 87, 89, 91, 92, 95, 98, 101, 124, 126, 135, 136, 137, 138, 156, 170, 172], "vee": [140, 142, 144], "venv": 169, "verbos": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 64, 67, 68, 69, 72, 75, 76, 77, 80, 83, 90, 93, 96, 97, 100, 112, 117], "veri": [62, 63, 65, 70, 75, 76, 87, 91, 98, 100, 140, 168], "verifi": 94, "versa": [75, 94, 157, 163], "version": [4, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 48, 49, 53, 54, 63, 64, 65, 67, 68, 81, 100, 102, 104, 109, 111, 112, 124, 127, 128, 129, 140, 156, 157, 159, 160, 161, 162, 164, 165, 168, 172], "versoin": 100, "versu": 89, "vertic": [63, 77, 78, 91], "via": [12, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 55, 62, 66, 67, 68, 69, 70, 71, 72, 75, 76, 83, 84, 85, 86, 87, 88, 91, 97, 99, 101, 102, 104, 109, 110, 111, 112, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 139, 146, 155, 156, 157, 158, 163, 167, 168, 169, 170, 171, 172, 173], "viabl": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "vice": [75, 94, 157, 163], "victor": [84, 100, 139, 168, 171], "vignett": [62, 172], "villa": [60, 79], "violat": [69, 72], "violet": [83, 93, 96], "vira": 171, "virtual": [70, 169], "virtualenv": 169, "visibl": [93, 97, 100], "visit": [168, 173], "visual": [13, 63, 69, 70, 71, 72, 76, 87, 88, 90, 91, 97], "vmax": 72, "vmin": 72, "vol": 62, "volum": [100, 168], "voluntari": 94, "vv740": 91, "vv760g": 91, "w": [10, 11, 17, 18, 19, 20, 21, 31, 37, 40, 44, 48, 49, 53, 54, 63, 84, 91, 94, 95, 98, 102, 103, 124, 127, 128, 129, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170], "w24678": 139, "w30302": 171, "w_": [17, 18, 19, 63, 91, 95, 124], "w_1": [17, 18, 19, 95], "w_2": [17, 18, 19, 95], "w_3": [17, 18, 19], "w_4": [17, 18, 19], "w_df": 95, "w_i": [36, 67, 95, 97, 102, 111, 124, 139, 140, 142, 144, 156], "wa": [50, 63, 68, 69, 72, 90, 91, 97, 100, 172], "wage": 70, "wager": 171, "wai": [64, 75, 90, 92, 98, 100, 112, 119, 140, 169], "wander": 45, "wang": 171, "want": [60, 63, 64, 65, 67, 75, 76, 79, 83, 91, 96, 97, 112, 114, 124, 168, 169, 171], "warn": [13, 16, 55, 60, 61, 62, 63, 64, 65, 66, 68, 69, 71, 72, 75, 76, 77, 80, 92, 98, 100, 102, 112, 117, 121, 122, 123, 124, 139, 140, 156, 170, 172], "warn_msg_prefix": 98, "wayon": 63, "we": [26, 52, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 109, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 134, 139, 140, 142, 144, 147, 151, 156, 157, 158, 167, 169, 170, 172, 173], "weak": [25, 157, 158, 171, 172], "weakest": [69, 72], "wealth": [10, 99], "websit": [64, 65, 70, 112, 119, 168], "wedg": [63, 91], "week": 172, "wei": 156, "weight": [13, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 48, 49, 53, 54, 63, 64, 65, 66, 69, 70, 71, 72, 77, 78, 87, 88, 91, 92, 97, 101, 110, 112, 120, 121, 124, 125, 140, 145, 148, 156, 157, 164, 165, 172], "weights_bar": [22, 23, 26, 88], "weights_dict": 88, "weiss": [168, 170], "well": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 53, 54, 61, 63, 75, 76, 80, 84, 89, 90, 91, 98, 102, 103, 139, 169, 170], "were": [64, 66, 76, 92, 93, 101, 173], "what": [62, 75, 84, 171], "when": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 53, 54, 56, 64, 67, 70, 77, 88, 92, 94, 98, 124, 134, 138, 140, 156, 168, 169, 170, 172], "whenev": [64, 92], "whera": [157, 165], "where": [12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 49, 51, 52, 54, 60, 61, 63, 64, 66, 67, 68, 69, 70, 71, 72, 78, 79, 80, 83, 87, 89, 91, 92, 94, 95, 96, 97, 98, 100, 101, 102, 103, 111, 112, 114, 116, 124, 125, 126, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 163, 164, 165, 167, 169, 170, 172, 173], "wherea": [32, 66, 67, 69, 72, 78, 98, 100, 101, 124, 127, 129, 140, 148, 157, 164, 173], "whether": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 36, 37, 38, 39, 46, 50, 51, 55, 56, 64, 68, 75, 92, 93, 97, 98, 100, 104, 109, 112, 121, 124, 157, 158, 172], "which": [5, 6, 7, 8, 9, 12, 14, 15, 16, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 48, 53, 55, 56, 60, 61, 62, 64, 65, 66, 67, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 84, 87, 90, 92, 93, 95, 97, 99, 100, 101, 103, 104, 109, 111, 112, 114, 115, 122, 123, 124, 128, 140, 156, 157, 158, 163, 164, 165, 167, 169, 172, 173], "while": [60, 79, 124], "white": [63, 85, 86, 91, 100], "whitegrid": [92, 93], "whitnei": [100, 171], "who": [62, 64, 92, 100], "whole": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 67, 80, 97, 103, 112, 123, 140, 141, 157, 158], "whom": 124, "why": [70, 77], "widehat": [69, 70, 71, 72, 124], "width": [13, 16, 61, 63, 77, 81, 82, 84], "wiki": 172, "wiksel": 171, "wild": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 156], "window": 169, "wise": [85, 86], "wish": 169, "within": [46, 63, 69, 70, 71, 72, 77, 85, 86, 91, 95, 97], "without": [25, 35, 46, 60, 61, 69, 70, 71, 72, 75, 76, 77, 79, 80, 90, 100, 103, 110, 112, 116, 121, 124, 157, 158, 169, 172], "wolf": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 156], "won": 100, "word": [46, 97, 124, 172, 173], "work": [48, 49, 53, 54, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 87, 94, 98, 99, 100, 112, 113, 114, 124, 156, 169, 171], "workflow": [76, 168, 172], "workspac": 92, "world": 171, "worri": 100, "wors": [49, 54], "would": [49, 54, 62, 64, 65, 69, 70, 71, 72, 75, 76, 81, 82, 84, 92, 93, 97, 99, 100, 111, 112, 123, 157, 167, 173], "wrapper": [4, 62, 97, 104, 109, 112, 123], "wright": 98, "write": [61, 62, 66, 67, 80, 101, 103, 157, 167], "written": [124, 140, 157, 164, 165], "wrong": [75, 94], "wspace": 75, "wurd": [63, 64, 65], "www": [168, 169], "x": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 53, 54, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 107, 108, 109, 111, 112, 116, 117, 118, 121, 122, 123, 124, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 162, 164, 165, 166, 167, 170, 173], "x0": [76, 77, 78, 94, 97], "x1": [63, 65, 66, 67, 76, 77, 78, 88, 89, 90, 91, 94, 97, 99, 100, 101, 104, 107, 108, 109, 111, 112, 113, 124, 140, 156, 157, 158, 170], "x10": [63, 65, 66, 88, 89, 90, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x100": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x11": [63, 65, 66, 89, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x12": [63, 65, 66, 89, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x13": [63, 65, 66, 89, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x14": [63, 65, 66, 89, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x15": [63, 65, 66, 89, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x16": [63, 65, 66, 89, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x17": [63, 65, 66, 89, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x18": [63, 65, 66, 89, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x19": [63, 65, 66, 89, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x1x2x3x4x5x6x7x8x9x10": 63, "x2": [63, 65, 66, 67, 76, 77, 78, 88, 89, 90, 91, 97, 99, 100, 101, 104, 107, 108, 109, 111, 112, 113, 124, 140, 156, 170], "x20": [63, 65, 66, 89, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x21": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x22": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x23": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x24": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x25": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x26": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x27": [63, 65, 66, 76, 91, 101, 104, 108, 109, 124, 170], "x28": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x29": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x2_dummi": 100, "x2_preds_control": 100, "x2_preds_treat": 100, "x3": [63, 65, 66, 67, 76, 77, 78, 88, 89, 90, 91, 99, 100, 101, 104, 107, 108, 109, 111, 112, 113, 124, 140, 156, 170], "x30": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x31": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x32": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x33": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x34": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x35": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x36": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x37": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x38": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x39": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x4": [63, 65, 66, 67, 76, 77, 78, 88, 89, 90, 91, 99, 100, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x40": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x41": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x42": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x43": [63, 65, 66, 90, 91, 101, 104, 108, 109, 124, 170], "x44": [63, 65, 66, 90, 91, 101, 104, 108, 109, 124, 170], "x45": [63, 65, 66, 90, 91, 101, 104, 108, 109, 124, 170], "x46": [63, 65, 66, 90, 91, 101, 104, 108, 109, 124, 170], "x47": [63, 65, 66, 90, 91, 101, 104, 108, 109, 124, 170], "x48": [63, 65, 66, 90, 91, 101, 104, 108, 109, 124, 170], "x49": [63, 65, 66, 90, 91, 101, 104, 108, 109, 124, 170], "x5": [63, 65, 66, 88, 89, 90, 91, 100, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x50": [63, 65, 66, 90, 91, 101, 104, 108, 109, 124, 170], "x51": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x52": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x53": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x54": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x55": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x56": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x57": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x58": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x59": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x6": [63, 65, 66, 88, 89, 90, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x60": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x61": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x62": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x63": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x64": [63, 65, 66, 68, 69, 70, 71, 72, 91, 92, 97, 100, 101, 104, 108, 109, 124, 170], "x65": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x66": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x67": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x68": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x69": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x7": [63, 65, 66, 88, 89, 90, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x70": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x71": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x72": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x73": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x74": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x75": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x76": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x77": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x78": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x79": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x8": [63, 65, 66, 88, 89, 90, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x80": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x81": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x82": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x83": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x84": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x85": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x86": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x87": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x88": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x89": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x9": [63, 65, 66, 88, 89, 90, 91, 101, 104, 108, 109, 112, 113, 124, 140, 156, 170], "x90": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x91": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x92": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x93": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x94": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x95": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x96": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x96x97x98x99x100ydcluster_var_icluster_var_jz": 63, "x97": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x98": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x99": [63, 65, 66, 91, 101, 104, 108, 109, 124, 170], "x_": [17, 19, 41, 43, 44, 61, 63, 68, 80, 91, 100, 103], "x_0": [68, 81, 82, 85, 86, 87], "x_1": [17, 18, 19, 31, 35, 37, 38, 39, 40, 68, 81, 82, 83, 85, 86, 87, 89, 96, 100, 124, 135, 136, 137, 157, 158, 170], "x_1x_3": [83, 96], "x_2": [17, 18, 19, 31, 35, 40, 68, 81, 82, 83, 85, 86, 87, 96, 100, 157, 158], "x_3": [17, 18, 19, 31, 35, 40, 68, 81, 82, 85, 86, 87, 157, 158], "x_4": [17, 18, 19, 31, 35, 40, 81, 82, 83, 85, 86, 87, 96], "x_5": [31, 35, 40, 81, 82, 85, 86], "x_6": [81, 82, 85, 86], "x_7": [81, 82, 85, 86], "x_8": [81, 82, 85, 86], "x_9": [81, 82, 85, 86], "x_binary_control": 100, "x_binary_tr": 100, "x_center": [76, 77], "x_col": [4, 5, 6, 7, 8, 9, 16, 60, 63, 64, 65, 69, 70, 71, 72, 79, 84, 91, 92, 93, 95, 97, 98, 99, 100, 104, 105, 106, 109, 112, 114, 124, 125, 127, 129, 170, 172, 173], "x_cols_bench": 100, "x_cols_binari": 100, "x_cols_poli": 91, "x_conf": 96, "x_conf_tru": 96, "x_df": 68, "x_domain": 65, "x_end": [76, 77], "x_i": [32, 33, 34, 36, 41, 42, 44, 45, 47, 61, 66, 67, 80, 83, 85, 86, 94, 96, 97, 101, 103, 111, 124, 126, 127, 129, 130, 138, 140, 142, 144], "x_jitter": [76, 77], "x_p": [37, 38, 39, 89, 124, 135, 136, 137, 170], "x_rang": [76, 77], "x_start": [76, 77], "x_train": 90, "x_true": [83, 96], "x_var": 65, "xaxis_titl": [81, 82, 84, 97, 100], "xformla": 62, "xgb": 90, "xgb_untuned_l": 90, "xgb_untuned_m": 90, "xgbclassifi": [75, 92, 94, 173], "xgboost": [61, 64, 75, 92, 94, 173], "xgbregressor": [75, 90, 92, 94, 173], "xi": [17, 18, 19, 35, 124], "xi_": 156, "xi_0": [43, 63, 91], "xi_i": [66, 101], "xiaoji": 171, "xintercept": [61, 66], "xlab": [61, 63, 64], "xlabel": [68, 69, 70, 72, 76, 77, 78, 81, 82, 83, 85, 86, 89, 90, 92, 93, 96], "xlim": [61, 64, 76, 77], "xmax": [76, 77], "xmax_rel": [76, 77], "xmin": [76, 77], "xmin_rel": [76, 77], "xtick": [76, 77, 78, 90], "xval": [65, 112, 120], "xx": 80, "y": [4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 53, 54, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 129, 131, 133, 134, 135, 136, 137, 139, 140, 141, 142, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 163, 164, 165, 166, 167, 170, 173], "y0": [62, 69, 72, 76, 77, 78, 83, 96], "y0_cvar": 83, "y0_quant": [83, 96], "y1": [62, 69, 72, 83, 96], "y1_cvar": 83, "y1_quant": [83, 96], "y_": [16, 17, 19, 43, 63, 66, 67, 68, 69, 72, 91, 101, 124, 126, 127, 129, 130, 138, 140, 142, 144], "y_0": [12, 14, 18, 47, 140, 143], "y_1": [12, 14, 18, 47, 140, 143], "y_col": [4, 5, 6, 7, 8, 9, 16, 60, 61, 63, 64, 65, 66, 69, 70, 71, 72, 79, 81, 82, 84, 85, 86, 88, 91, 92, 93, 95, 97, 98, 99, 102, 103, 104, 105, 106, 108, 109, 112, 120, 121, 124, 125, 127, 129, 139, 140, 170, 172, 173], "y_df": [68, 95], "y_diff": 68, "y_i": [32, 33, 34, 36, 41, 42, 44, 45, 61, 66, 67, 80, 83, 94, 95, 96, 97, 101, 103, 124, 126, 138], "y_label": [13, 16], "y_lower_quantil": [69, 72], "y_mean": [69, 72], "y_pred": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 49, 54, 75, 112, 113], "y_train": 90, "y_true": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 49, 54, 75, 112, 113], "y_upper_quantil": [69, 72], "ya": 171, "yasui": 171, "yata": 171, "yaxis_titl": [81, 82, 84, 97, 100], "year": [70, 168], "yerr": [68, 76, 77, 78, 85, 86, 90, 92, 94, 97], "yet": [63, 69, 71, 72, 74, 124, 127, 129, 130], "yggvpl": 91, "yi": [41, 171], "yield": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 124], "yintercept": 64, "ylab": [61, 63, 64], "ylabel": [68, 69, 70, 72, 76, 77, 78, 81, 82, 83, 85, 86, 89, 90, 92, 93, 96], "ylim": 92, "ymax": 64, "ymin": 64, "yname": 62, "york": 171, "you": [48, 49, 53, 54, 60, 61, 68, 69, 70, 71, 72, 76, 77, 79, 91, 99, 124, 168, 169, 173], "your": [75, 169], "ython": 168, "yukun": 171, "yusuk": 171, "yuya": 171, "yy": 80, "z": [4, 5, 6, 7, 8, 9, 17, 18, 19, 25, 27, 30, 31, 33, 35, 36, 38, 40, 42, 43, 60, 63, 64, 66, 69, 72, 79, 81, 82, 84, 91, 92, 96, 98, 100, 101, 111, 124, 133, 136, 140, 147, 149, 152, 155, 156, 172], "z1": [7, 16, 38, 69, 72, 105, 106, 109, 124, 125, 126, 127, 129], "z2": [7, 16, 69, 72, 105, 106, 109, 124, 125, 126, 127, 129], "z3": [7, 16, 69, 72, 105, 106, 109, 124, 125, 126, 127, 129], "z4": [7, 16, 69, 72, 105, 106, 109, 124, 125, 126, 127, 129], "z_": [43, 63, 91], "z_0": [140, 151], "z_1": [31, 35, 40, 69, 72], "z_2": [31, 35, 40], "z_3": [31, 35, 40], "z_4": [31, 35, 40, 69, 72], "z_5": 31, "z_col": [4, 5, 6, 7, 8, 9, 25, 27, 38, 60, 63, 64, 66, 79, 91, 92, 93, 98, 101, 104, 105, 109, 111, 124, 172], "z_i": [36, 42, 66, 96, 101, 124], "z_j": [17, 18, 19, 31, 35, 40], "z_true": 96, "zadik": 171, "zaxis_titl": [81, 82, 84], "zero": [14, 18, 47, 67, 68, 69, 72, 75, 83, 88, 95, 96, 99, 100, 124, 140, 142, 144, 156, 157, 160, 162], "zeros_lik": 96, "zeta": [25, 38, 39, 64, 92, 111, 124, 133, 136, 137, 170], "zeta_": [43, 63, 91], "zeta_0": [43, 63, 91], "zeta_i": [34, 42, 44, 61, 80, 103], "zeta_j": 156, "zhang": [41, 171], "zhao": [12, 14, 15, 18, 31, 35, 40, 62, 67, 69, 72, 124, 126, 130, 171], "zhou": [41, 171], "zimmert": [67, 124, 130, 171], "zip": [81, 82], "zorder": [76, 77, 78], "\u03c4_x0": 94, "\u03c4_x1": 94, "\u2139": 61}, "titles": ["API Reference", "<span class=\"section-number\">1. </span>DoubleML Data Class", "<span class=\"section-number\">3. </span>Datasets", "<span class=\"section-number\">2. </span>DoubleML Models", "<span class=\"section-number\">1.2. </span>doubleml.data.DoubleMLClusterData", "<span class=\"section-number\">1.6. </span>doubleml.data.DoubleMLDIDData", "<span class=\"section-number\">1.1. </span>doubleml.data.DoubleMLData", "<span class=\"section-number\">1.3. </span>doubleml.data.DoubleMLPanelData", "<span class=\"section-number\">1.5. </span>doubleml.data.DoubleMLRDDData", "<span class=\"section-number\">1.4. </span>doubleml.data.DoubleMLSSMData", "<span class=\"section-number\">3.1.1. </span>doubleml.datasets.fetch_401K", "<span class=\"section-number\">3.1.2. </span>doubleml.datasets.fetch_bonus", "<span class=\"section-number\">2.3.4. </span>doubleml.did.DoubleMLDID", "<span class=\"section-number\">2.3.2. </span>doubleml.did.DoubleMLDIDAggregation", "<span class=\"section-number\">2.3.3. </span>doubleml.did.DoubleMLDIDBinary", "<span class=\"section-number\">2.3.5. </span>doubleml.did.DoubleMLDIDCS", "<span class=\"section-number\">2.3.1. </span>doubleml.did.DoubleMLDIDMulti", "<span class=\"section-number\">3.2.14. </span>doubleml.did.datasets.make_did_CS2021", "<span class=\"section-number\">3.2.13. </span>doubleml.did.datasets.make_did_SZ2020", "<span class=\"section-number\">3.2.15. </span>doubleml.did.datasets.make_did_cs_CS2021", "<span class=\"section-number\">5.1. </span>doubleml.double_ml_score_mixins.LinearScoreMixin", "<span class=\"section-number\">5.2. </span>doubleml.double_ml_score_mixins.NonLinearScoreMixin", "<span class=\"section-number\">2.2.2. </span>doubleml.irm.DoubleMLAPO", "<span class=\"section-number\">2.2.3. </span>doubleml.irm.DoubleMLAPOS", "<span class=\"section-number\">2.2.7. </span>doubleml.irm.DoubleMLCVAR", "<span class=\"section-number\">2.2.4. </span>doubleml.irm.DoubleMLIIVM", "<span class=\"section-number\">2.2.1. </span>doubleml.irm.DoubleMLIRM", "<span class=\"section-number\">2.2.6. </span>doubleml.irm.DoubleMLLPQ", "<span class=\"section-number\">2.2.5. </span>doubleml.irm.DoubleMLPQ", "<span class=\"section-number\">2.2.8. </span>doubleml.irm.DoubleMLQTE", "<span class=\"section-number\">2.2.9. </span>doubleml.irm.DoubleMLSSM", "<span class=\"section-number\">3.2.5. </span>doubleml.irm.datasets.make_confounded_irm_data", "<span class=\"section-number\">3.2.3. </span>doubleml.irm.datasets.make_heterogeneous_data", "<span class=\"section-number\">3.2.2. </span>doubleml.irm.datasets.make_iivm_data", "<span class=\"section-number\">3.2.1. </span>doubleml.irm.datasets.make_irm_data", "<span class=\"section-number\">3.2.4. </span>doubleml.irm.datasets.make_irm_data_discrete_treatments", "<span class=\"section-number\">3.2.6. </span>doubleml.irm.datasets.make_ssm_data", "<span class=\"section-number\">2.1.2. </span>doubleml.plm.DoubleMLLPLR", "<span class=\"section-number\">2.1.3. </span>doubleml.plm.DoubleMLPLIV", "<span class=\"section-number\">2.1.1. </span>doubleml.plm.DoubleMLPLR", "<span class=\"section-number\">3.2.12. </span>doubleml.plm.datasets.make_confounded_plr_data", "<span class=\"section-number\">3.2.9. </span>doubleml.plm.datasets.make_lplr_LZZ2020", "<span class=\"section-number\">3.2.10. </span>doubleml.plm.datasets.make_pliv_CHS2015", "<span class=\"section-number\">3.2.11. </span>doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021", "<span class=\"section-number\">3.2.7. </span>doubleml.plm.datasets.make_plr_CCDDHNR2018", "<span class=\"section-number\">3.2.8. </span>doubleml.plm.datasets.make_plr_turrell2018", "<span class=\"section-number\">2.4.1. </span>doubleml.rdd.RDFlex", "<span class=\"section-number\">3.2.16. </span>doubleml.rdd.datasets.make_simple_rdd_data", "<span class=\"section-number\">4.1.2. </span>doubleml.utils.DMLDummyClassifier", "<span class=\"section-number\">4.1.1. </span>doubleml.utils.DMLDummyRegressor", "<span class=\"section-number\">4.1.3. </span>doubleml.utils.DMLOptunaResult", "<span class=\"section-number\">4.1.4. </span>doubleml.utils.DoubleMLBLP", "<span class=\"section-number\">4.1.5. </span>doubleml.utils.DoubleMLPolicyTree", "<span class=\"section-number\">4.1.7. </span>doubleml.utils.GlobalClassifier", "<span class=\"section-number\">4.1.6. </span>doubleml.utils.GlobalRegressor", "<span class=\"section-number\">4.1.9. </span>doubleml.utils.PSProcessor", "<span class=\"section-number\">4.1.8. </span>doubleml.utils.PSProcessorConfig", "<span class=\"section-number\">4.2.1. </span>doubleml.utils.gain_statistics", "<span class=\"section-number\">5. </span>Score Mixin Classes for DoubleML Models", "<span class=\"section-number\">4. </span>Utility Classes and Functions", "R: Basic Instrumental Variables Calculation", "R: Basics of Double Machine Learning", "R: DoubleML for Difference-in-Differences", "R: Cluster Robust Double Machine Learning", "R: Impact of 401(k) on Financial Wealth", "R: Ensemble Learners and More with <code class=\"docutils literal notranslate\"><span class=\"pre\">mlr3pipelines</span></code>", "R: Sample Selection Models", "Python: Difference-in-Differences", "Python: Difference-in-Differences Pre-Testing", "Python: Panel Data with Multiple Time Periods", "Python: Real-Data Example for Multi-Period Difference-in-Differences", "Python: Panel Data Introduction", "Python: Repeated Cross-Sectional Data with Multiple Time Periods", "DML: Bonus Data", "Examples", "Python: Choice of learners", "Python: Hyperparametertuning with Optuna", "Python: Causal Machine Learning with TabPFN", "Python: Average Potential Outcome (APO) Models", "Python: Basic Instrumental Variables calculation", "Python: Basics of Double Machine Learning", "Python: Conditional Average Treatment Effects (CATEs) for IRM models", "Python: Conditional Average Treatment Effects (CATEs) for PLR models", "Python: Conditional Value at Risk of potential outcomes", "Python: First Stage and Causal Estimation", "Python: Group Average Treatment Effects (GATEs) for IRM models", "Python: Group Average Treatment Effects (GATEs) for PLR models", "Python: GATE Sensitivity Analysis", "Python: IRM and APO Model Comparison", "Python: Log-Odds Effects for Logistic PLR models", "DoubleML meets FLAML - How to tune learners automatically within <code class=\"docutils literal notranslate\"><span class=\"pre\">DoubleML</span></code>", "Python: Cluster Robust Double Machine Learning", "Python: Impact of 401(k) on Financial Wealth", "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)", "Python: PLM and IRM for Multiple Treatments", "Python: Policy Learning with Trees", "Python: Potential Quantiles and Quantile Treatment Effects", "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)", "Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments", "Python: Sensitivity Analysis", "Python: Sensitivity Analysis for Causal ML", "Python: Sample Selection Models", "<span class=\"section-number\">6. </span>Double machine learning algorithms", "<span class=\"section-number\">1. </span>The basics of double/debiased machine learning", "DoubleMLData from dataframes", "Key arguments", "Key arguments", "Key arguments", "Key arguments", "<span class=\"section-number\">2. </span>Data Backend", "User Guide", "<span class=\"section-number\">4. </span>Heterogeneous treatment effects", "<span class=\"section-number\">7. </span>Learners, hyperparameters and hyperparameter tuning", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "References", "&lt;no title&gt;", "<span class=\"section-number\">3. </span>Models", "&lt;no title&gt;", "Panel Data", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">9. </span>Sample-splitting, cross-fitting and repeated cross-fitting", "<span class=\"section-number\">5. </span>Score functions", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">8. </span>Variance estimation and confidence intervals", "<span class=\"section-number\">10. </span>Sensitivity analysis", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "DoubleML", "Installing DoubleML", "Getting Started", "Double Machine Learning Literature", "Release Notes", "DoubleML Workflow"], "titleterms": {"": 90, "0": 173, "1": [90, 100, 173], "2": [90, 100, 173], "2011": 100, "2023": 100, "3": [90, 100, 173], "4": [100, 173], "401": [64, 92, 93, 99], "5": [100, 173], "6": 173, "7": 173, "95": 90, "A": [63, 91], "ATE": [66, 87, 94, 101], "No": [63, 91], "One": [63, 81, 82, 91], "That": 98, "The": [64, 92, 94, 103, 170], "acknowledg": [62, 168], "acycl": [60, 79], "addit": 94, "adjust": [69, 72, 97], "advanc": [97, 112, 156], "aggreg": [69, 70, 71, 72, 124], "al": 100, "algorithm": [102, 157, 168, 170], "all": [69, 72], "altern": 140, "analysi": [69, 72, 76, 78, 87, 88, 99, 100, 157, 173], "anticip": [69, 72], "api": [0, 90], "apo": [78, 88, 124, 140, 157], "applic": [63, 91, 99], "approach": [61, 75, 80, 103], "ar": 98, "arah": 100, "arbitrari": 94, "archiv": 74, "argument": [105, 106, 107, 108, 109], "arrai": [104, 109], "asset": [64, 92], "assumpt": 100, "att": [67, 69, 70, 71, 72], "augment": 94, "automat": 90, "automl": 90, "averag": [64, 77, 78, 81, 82, 85, 86, 88, 92, 111, 124, 140, 157], "backend": [63, 64, 91, 92, 109, 170, 173], "band": 156, "base": [65, 69, 72], "basic": [60, 61, 69, 72, 76, 79, 80, 103], "benchmark": [99, 100, 157], "bia": [61, 80, 103], "binari": [124, 140], "bonu": 73, "bootstrap": 156, "build": 169, "calcul": [60, 79], "call": 90, "callabl": 140, "case": 74, "cate": [81, 82, 94, 111], "causal": [70, 73, 77, 78, 84, 100, 140, 170, 173], "chernozhukov": 100, "choic": 75, "citat": 168, "class": [1, 58, 59, 63, 91], "cluster": [63, 91], "code": 168, "coeffici": 90, "combin": [69, 72, 84], "compar": [75, 90], "comparison": [62, 76, 77, 88, 90], "comput": [75, 90], "conclus": [90, 100], "conda": 169, "condit": [70, 81, 82, 83, 93, 111, 140], "confid": [90, 98, 156], "construct": 112, "contrast": 78, "control": [69, 72], "covari": [69, 72, 97], "coverag": [67, 84], "cran": 169, "creat": [77, 90], "cross": [63, 67, 72, 91, 124, 126, 139, 140, 157, 170], "custom": [75, 90], "cvar": [83, 93, 111, 140], "dag": [60, 79], "data": [1, 4, 5, 6, 7, 8, 9, 60, 61, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 99, 100, 101, 103, 109, 124, 126, 140, 157, 170, 173], "datafram": [104, 109], "dataset": [2, 10, 11, 17, 18, 19, 31, 32, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 47, 73], "debias": [61, 80, 103, 170], "default": 90, "defin": [63, 76, 91], "demo": 62, "depend": 169, "descript": [69, 72], "design": [97, 124], "detail": [62, 69, 70, 71, 72, 76, 124], "develop": 169, "dgp": [61, 76, 77, 78, 80], "did": [3, 12, 13, 14, 15, 16, 17, 18, 19, 62, 124], "differ": [62, 67, 68, 70, 74, 75, 124, 140, 156, 157], "dimension": [81, 82], "direct": [60, 79], "disclaim": 100, "discontinu": [97, 124], "distribut": [66, 101], "dml": [63, 73, 91, 139, 170, 173], "dml1": 102, "dml2": 102, "dmldummyclassifi": 48, "dmldummyregressor": 49, "dmloptunaresult": 50, "doubl": [61, 63, 80, 91, 102, 103, 168, 170, 171], "double_ml_score_mixin": [20, 21], "doubleml": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 62, 64, 65, 77, 79, 90, 92, 99, 100, 156, 168, 169, 173], "doublemlapo": [22, 23, 76], "doublemlblp": 51, "doublemlclusterdata": [4, 63], "doublemlcvar": 24, "doublemldata": [6, 64, 77, 91, 92, 104, 109, 170], "doublemldid": 12, "doublemldidaggreg": 13, "doublemldidbinari": 14, "doublemldidc": 15, "doublemldiddata": [5, 109], "doublemldidmulti": 16, "doublemliivm": 25, "doublemlirm": 26, "doublemllplr": 37, "doublemllpq": 27, "doublemlpaneldata": [7, 69, 72, 109], "doublemlpliv": [38, 63, 91], "doublemlplr": 39, "doublemlpolicytre": 52, "doublemlpq": 28, "doublemlqt": 29, "doublemlrdddata": [8, 109], "doublemlssm": 30, "doublemlssmdata": [9, 109], "effect": [64, 69, 70, 71, 72, 74, 77, 81, 82, 83, 85, 86, 88, 89, 92, 93, 94, 96, 99, 100, 111, 124], "elig": [64, 92], "empir": 84, "ensembl": [65, 97], "error": [63, 91], "estim": [60, 64, 66, 67, 69, 70, 71, 72, 73, 77, 79, 84, 87, 90, 92, 93, 94, 96, 99, 100, 101, 139, 140, 156, 170, 173], "et": 100, "evalu": [75, 77, 90, 112], "event": [69, 70, 71, 72], "exampl": [62, 63, 70, 74, 76, 81, 82, 91, 99, 105, 106, 107, 108, 109], "exploit": [62, 65], "extern": [112, 139], "featur": [65, 168], "fetch_401k": 10, "fetch_bonu": 11, "figur": 94, "file": 169, "final": 62, "financi": [64, 92, 93], "first": 84, "fit": [63, 90, 91, 139, 170], "flaml": 90, "flexibl": 97, "fold": [90, 139], "forest": 73, "formul": [100, 173], "from": [62, 65, 104, 109, 169], "full": 90, "function": [59, 62, 63, 91, 140, 170], "fuzzi": [97, 124], "gain_statist": 57, "gate": [85, 86, 87, 111], "gatet": 87, "gener": [2, 61, 74, 76, 77, 78, 80, 90, 97, 103, 157], "get": 170, "github": 169, "global": 97, "globalclassifi": 53, "globalregressor": 54, "graph": [60, 79], "grid": 112, "group": [69, 71, 72, 85, 86, 111], "guid": [76, 110], "helper": [63, 91], "heterogen": [74, 88, 94, 111], "how": [65, 90], "hyperparamet": [76, 88, 112], "hyperparametertun": 76, "identif": 100, "iivm": [64, 92, 124, 140], "impact": [64, 92, 93], "implement": [102, 124, 140, 157], "import": 77, "induc": [61, 80, 103], "infer": [156, 173], "initi": [63, 90, 91], "insight": 77, "instal": 169, "instrument": [60, 79, 98], "integr": 62, "interact": [64, 85, 92, 95, 124, 140, 157], "interv": [90, 98, 156], "introduct": 71, "invers": 94, "irm": [3, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 64, 73, 81, 85, 88, 92, 94, 95, 99, 111, 124, 140, 157], "iv": [60, 64, 79, 92, 124, 140], "k": [64, 92, 93, 99, 139], "kei": [77, 105, 106, 107, 108, 109], "lambda": 84, "lasso": [73, 84], "latest": 169, "lear": [63, 91], "learn": [61, 63, 77, 80, 91, 95, 102, 103, 111, 168, 170, 171], "learner": [65, 73, 75, 76, 88, 90, 97, 112, 170], "less": 90, "level": 124, "linear": [64, 69, 72, 86, 92, 94, 97, 124, 140, 157], "linearscoremixin": 20, "literatur": 171, "load": [63, 73, 91, 100], "loader": 2, "local": [64, 92, 93, 96, 97, 140], "log": 89, "logist": [89, 124, 140], "loss": 84, "lplr": [124, 140], "lpq": [96, 140], "lqte": [93, 96], "m": 139, "machin": [61, 63, 77, 80, 91, 102, 103, 168, 170, 171], "main": 168, "mainten": 168, "make_confounded_irm_data": 31, "make_confounded_plr_data": 40, "make_did_cs2021": 17, "make_did_cs_cs2021": 19, "make_did_sz2020": 18, "make_heterogeneous_data": 32, "make_iivm_data": 33, "make_irm_data": 34, "make_irm_data_discrete_treat": 35, "make_lplr_lzz2020": 41, "make_pliv_chs2015": 42, "make_pliv_multiway_cluster_ckms2021": 43, "make_plr_ccddhnr2018": 44, "make_plr_turrell2018": 45, "make_simple_rdd_data": 47, "make_ssm_data": 36, "mar": [66, 101], "market": [63, 91], "matric": [104, 109], "meet": 90, "method": [77, 90, 173], "metric": [75, 90], "minimum": 112, "miss": [66, 101], "missing": [124, 140], "mixin": 58, "ml": [61, 62, 80, 100, 103, 173], "mlr3": 65, "mlr3extralearn": 65, "mlr3learner": 65, "mlr3pipelin": 65, "model": [3, 58, 64, 66, 73, 76, 77, 78, 81, 82, 85, 86, 88, 89, 90, 92, 94, 95, 98, 100, 101, 111, 124, 139, 140, 156, 157, 170, 173], "modul": 73, "more": 65, "motiv": [63, 91], "multi": 70, "multipl": [69, 72, 78, 94, 124], "multipli": 156, "naiv": [60, 79], "net": [64, 92], "neyman": [140, 170], "nonignor": [66, 101, 124, 140], "nonlinearscoremixin": 21, "nonrespons": [66, 101, 124, 140], "note": 172, "nuisanc": [76, 90, 170], "object": [63, 77, 91, 99], "odd": 89, "option": 169, "optuna": 76, "orthogon": [61, 80, 103, 140, 170], "out": [61, 80, 103], "outcom": [66, 67, 77, 78, 83, 101, 111, 124, 140, 157], "over": 156, "overcom": [61, 80, 103], "overfit": [61, 80, 103], "overlap": 94, "packag": [62, 64, 92, 169], "panel": [67, 69, 71, 124, 126, 140, 157], "parallel": 70, "paramet": [65, 73, 90, 124, 140], "partial": [61, 64, 80, 86, 92, 94, 103, 124, 140, 157], "particip": [64, 92], "partit": 139, "penalti": 84, "perform": [62, 77, 94], "period": [69, 70, 72, 124, 140, 157], "pip": 169, "pipelin": [76, 112], "pliv": [124, 140], "plm": [3, 37, 38, 39, 40, 41, 42, 43, 44, 45, 94, 124, 140, 157], "plot": [63, 90, 91], "plr": [64, 73, 82, 86, 89, 92, 111, 124, 140, 157], "polici": [95, 111], "potenti": [77, 78, 83, 93, 96, 111, 124, 140, 157], "pq": [96, 111, 140], "pre": 68, "predict": [62, 112], "preprocess": 65, "problem": 173, "process": [61, 63, 76, 77, 78, 80, 91, 103], "product": [63, 91], "propens": 94, "provid": 139, "psprocessor": 55, "psprocessorconfig": 56, "python": [67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 112, 169], "qte": [96, 111], "qualiti": 84, "quantil": [93, 96, 111, 140], "question": 70, "r": [60, 61, 62, 63, 64, 65, 66, 74, 112, 169], "random": [66, 73, 101, 124, 140], "rank": 94, "rdd": [3, 46, 47, 97, 124], "rdflex": 46, "real": [63, 70, 91], "refer": [0, 60, 62, 63, 65, 75, 79, 84, 90, 91, 94, 98, 100, 103, 112, 122, 139, 156, 168, 170], "regress": [64, 85, 86, 92, 95, 97, 124, 140, 157], "regular": [61, 80, 103], "releas": [169, 172], "remark": 62, "remov": [61, 80, 103], "repeat": [67, 72, 124, 126, 139, 140, 157], "repetit": 139, "requir": 112, "research": 70, "respect": [63, 91], "result": [63, 64, 76, 91, 92, 94], "risk": [83, 93, 111, 140], "robust": [63, 91, 98], "run": 98, "sampl": [61, 66, 80, 90, 101, 103, 124, 139, 140], "sandbox": 74, "score": [58, 61, 80, 94, 103, 140, 170], "search": 112, "section": [67, 72, 124, 126, 140, 157], "select": [66, 69, 72, 101, 124, 140], "sensit": [69, 72, 78, 87, 88, 99, 100, 157, 173], "set": [65, 112], "setup": 77, "sharp": [97, 124], "simpl": [61, 80, 103], "simul": [60, 63, 67, 79, 91, 98, 99], "simultan": 156, "singl": 78, "small": 98, "sourc": [168, 169], "special": 109, "specif": [157, 173], "specifi": [73, 112, 140], "split": [61, 80, 103, 139], "ssm": 124, "stack": 97, "stage": 84, "standard": [63, 75, 91], "start": 170, "step": 90, "structur": 77, "studi": [69, 70, 71, 72, 74], "summari": [64, 77, 90, 92, 94], "tabpfn": 77, "takeawai": 77, "test": 68, "theori": 157, "time": [69, 71, 72, 75, 90], "train": 90, "treat": 88, "treatment": [64, 77, 81, 82, 83, 85, 86, 88, 92, 93, 94, 96, 111, 124, 140, 157], "tree": [95, 111], "trend": 70, "tune": [65, 76, 90, 112], "two": [63, 81, 82, 91, 124, 140, 157], "type": 109, "uncondit": 70, "under": [66, 94, 101], "univers": [69, 72], "untun": [76, 90], "up": 65, "us": [60, 62, 65, 73, 79, 90, 112], "usag": [105, 106, 107, 108, 109], "user": 110, "util": [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59], "v": 84, "valid": 156, "valu": [83, 93, 111, 140], "vanderweel": 100, "variabl": [60, 79, 98], "varianc": 156, "version": 169, "via": 140, "visual": [77, 89], "wai": [63, 91], "weak": 98, "wealth": [64, 92, 93], "weight": [94, 111], "when": 90, "whl": 169, "within": 90, "without": [97, 139], "workflow": 173, "xgboost": 90, "zero": [63, 91]}})