Search.setIndex({"alltitles": {"(One-Way) Cluster Robust Double Machine Learing": [[60, "(One-Way)-Cluster-Robust-Double-Machine-Learing"], [86, "(One-Way)-Cluster-Robust-Double-Machine-Learing"]], "0. Problem Formulation": [[155, "problem-formulation"]], "1. Data Backend": [[155, "data-backend"]], "1. Formulation of Causal Model & Identification Assumptions": [[95, "1.-Formulation-of-Causal-Model-&-Identification-Assumptions"]], "2. Causal Model": [[155, "causal-model"]], "2. Estimation of Causal Effect": [[95, "2.-Estimation-of-Causal-Effect"]], "3. ML Methods": [[155, "ml-methods"]], "3. Sensitivity Analysis": [[95, "3.-Sensitivity-Analysis"]], "4. Benchmarking Analysis": [[95, "4.-Benchmarking-Analysis"]], "4. DML Specifications": [[155, "dml-specifications"]], "5. Conclusion": [[95, "5.-Conclusion"]], "5. Estimation": [[155, "estimation"]], "6. Inference": [[155, "inference"]], "7. Sensitivity Analysis": [[155, "sensitivity-analysis"]], "A Motivating Example: Two-Way Cluster Robust DML": [[60, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"], [86, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"]], "API Reference": [[0, null]], "ATE Estimation and Sensitivity": [[82, "ATE-Estimation-and-Sensitivity"]], "ATE estimates distribution": [[63, "ATE-estimates-distribution"], [63, "id3"], [96, "ATE-estimates-distribution"], [96, "id3"]], "ATT Estimation": [[66, "ATT-Estimation"], [66, "id1"], [68, "ATT-Estimation"], [69, "ATT-Estimation"], [69, "id1"]], "ATT Estimation: Conditional Parallel Trends": [[67, "ATT-Estimation:-Conditional-Parallel-Trends"]], "ATT Estimation: Unconditional Parallel Trends": [[67, "ATT-Estimation:-Unconditional-Parallel-Trends"]], "ATTE Estimation": [[64, "ATTE-Estimation"], [64, "id2"]], "Acknowledgements": [[150, "acknowledgements"]], "Acknowledgements and Final Remarks": [[59, "Acknowledgements-and-Final-Remarks"]], "Additional Results: CATE estimates": [[89, "Additional-Results:-CATE-estimates"]], "Advanced: External Predictions": [[107, "advanced-external-predictions"]], "Advanced: Global and Local Learners, Stacked Ensembles": [[92, "Advanced:-Global-and-Local-Learners,-Stacked-Ensembles"]], "Aggregated Effects": [[66, "Aggregated-Effects"], [69, "Aggregated-Effects"]], "Aggregation Details": [[66, "Aggregation-Details"], [67, "Aggregation-Details"], [68, "Aggregation-Details"], [69, "Aggregation-Details"]], "Algorithm DML1": [[97, "algorithm-dml1"]], "Algorithm DML2": [[97, "algorithm-dml2"]], "All Combinations": [[66, "All-Combinations"]], "All combinations": [[69, "All-combinations"]], "Anticipation": [[66, "Anticipation"], [69, "Anticipation"]], "Application Results": [[60, "Application-Results"], [86, "Application-Results"]], "Application: 401(k)": [[94, "Application:-401(k)"]], "AutoML with less Computation time": [[85, "AutoML-with-less-Computation-time"]], "Average Potential Outcome (APOs)": [[73, "Average-Potential-Outcome-(APOs)"]], "Average Potential Outcomes (APOs)": [[108, "average-potential-outcomes-apos"], [123, "average-potential-outcomes-apos"], [139, "average-potential-outcomes-apos"]], "Average Potential Outcomes (APOs) for Multiple Treatment Levels": [[108, "average-potential-outcomes-apos-for-multiple-treatment-levels"]], "Average Treatment Effect": [[83, "Average-Treatment-Effect"]], "Average Treatment Effect on the Treated": [[83, "Average-Treatment-Effect-on-the-Treated"]], "Basics": [[66, "Basics"], [69, "Basics"]], "Benchmarking": [[139, "benchmarking"]], "Benchmarking Analysis": [[94, "Benchmarking-Analysis"]], "Binary Interactive Regression Model (IRM)": [[108, "binary-interactive-regression-model-irm"], [123, "binary-interactive-regression-model-irm"]], "CATEs for IRM models": [[106, "cates-for-irm-models"]], "CATEs for PLR models": [[106, "cates-for-plr-models"]], "CVaR Treatment Effects": [[78, "CVaR-Treatment-Effects"]], "CVaR of potential outcomes": [[106, "cvar-of-potential-outcomes"]], "CVaR treatment effects": [[106, "cvar-treatment-effects"]], "Causal Analysis with DoubleML": [[95, "Causal-Analysis-with-DoubleML"]], "Causal Contrasts": [[73, "Causal-Contrasts"]], "Causal Research Question": [[67, "Causal-Research-Question"]], "Causal estimation vs. lasso penalty \\lambda": [[79, "Causal-estimation-vs.-lasso-penalty-\\lambda"]], "Chernozhukov et al. (2023): Benchmarking": [[95, "Chernozhukov-et-al.-(2023):-Benchmarking"]], "Citation": [[150, "citation"]], "Cluster Robust Cross Fitting": [[60, "Cluster-Robust-Cross-Fitting"], [86, "Cluster-Robust-Cross-Fitting"]], "Cluster Robust Standard Errors": [[60, "Cluster-Robust-Standard-Errors"], [86, "Cluster-Robust-Standard-Errors"]], "Clustering and double machine learning": [[60, "Clustering-and-double-machine-learning"], [86, "Clustering-and-double-machine-learning"]], "Combined loss vs. lasso penalty \\lambda": [[79, "Combined-loss-vs.-lasso-penalty-\\lambda"]], "Compare Metrics for Nuisance Estimation": [[85, "Compare-Metrics-for-Nuisance-Estimation"]], "Comparing different learners": [[84, "Comparing-different-learners"]], "Comparison and summary": [[85, "Comparison-and-summary"]], "Comparison to AutoML with less Computation time and Untuned XGBoost Learners": [[85, "Comparison-to-AutoML-with-less-Computation-time-and-Untuned-XGBoost-Learners"]], "Comparison to did package": [[59, "Comparison-to-did-package"]], "Computation time": [[84, "Computation-time"]], "Conclusion": [[85, "Conclusion"]], "Conditional Value at Risk (CVaR)": [[78, "Conditional-Value-at-Risk-(CVaR)"]], "Conditional average treatment effects (CATEs)": [[106, "conditional-average-treatment-effects-cates"]], "Conditional value at risk (CVaR)": [[106, "conditional-value-at-risk-cvar"], [123, "conditional-value-at-risk-cvar"]], "Confidence bands and multiplier bootstrap for valid simultaneous inference": [[138, "confidence-bands-and-multiplier-bootstrap-for-valid-simultaneous-inference"]], "Control Groups": [[66, "Control-Groups"], [69, "Control-Groups"]], "Coverage Simulation": [[64, "Coverage-Simulation"], [64, "id3"]], "Creating the DoubleMLData Object": [[72, "Creating-the-DoubleMLData-Object"]], "Cross-fitting with K folds": [[122, "cross-fitting-with-k-folds"]], "Cross-fitting, DML algorithms and Neyman-orthogonal score functions": [[152, "cross-fitting-dml-algorithms-and-neyman-orthogonal-score-functions"]], "Custom evaluation metrics": [[84, "Custom-evaluation-metrics"]], "DML: Bonus Data": [[70, null]], "Data": [[61, "Data"], [63, "Data"], [63, "id1"], [64, "Data"], [64, "id1"], [66, "Data"], [67, "Data"], [68, "Data"], [69, "Data"], [76, "Data"], [77, "Data"], [78, "Data"], [80, "Data"], [81, "Data"], [82, "Data"], [83, "Data"], [87, "Data"], [88, "Data"], [90, "Data"], [91, "Data"], [91, "id1"], [94, "Data"], [96, "Data"], [96, "id1"], [152, "data"]], "Data Backend": [[104, null]], "Data Description": [[66, "Data-Description"], [69, "Data-Description"]], "Data Details": [[66, "Data-Details"], [69, "Data-Details"]], "Data Generating Process (DGP)": [[58, "Data-Generating-Process-(DGP)"], [72, "Data-Generating-Process-(DGP)"], [73, "Data-Generating-Process-(DGP)"], [75, "Data-Generating-Process-(DGP)"]], "Data Generation": [[85, "Data-Generation"]], "Data Simulation": [[57, "Data-Simulation"], [74, "Data-Simulation"]], "Data and Effect Estimation": [[94, "Data-and-Effect-Estimation"]], "Data generating process": [[98, "data-generating-process"]], "Data preprocessing": [[62, "Data-preprocessing"]], "Data with Anticipation": [[66, "Data-with-Anticipation"], [69, "Data-with-Anticipation"]], "Data-Backend for Cluster Data": [[60, "Data-Backend-for-Cluster-Data"], [86, "Data-Backend-for-Cluster-Data"]], "Dataset Generators": [[2, "dataset-generators"]], "Dataset Loaders": [[2, "dataset-loaders"]], "Datasets": [[2, null]], "Define Helper Functions for Plotting": [[60, "Define-Helper-Functions-for-Plotting"], [86, "Define-Helper-Functions-for-Plotting"]], "Demo Example from did": [[59, "Demo-Example-from-did"]], "Details on Predictive Performance": [[59, "Details-on-Predictive-Performance"]], "Difference-in-Differences": [[71, "difference-in-differences"]], "Difference-in-Differences Models": [[123, "difference-in-differences-models"], [139, "difference-in-differences-models"]], "Difference-in-Differences Models (DID)": [[108, "difference-in-differences-models-did"]], "Difference-in-Differences for Panel Data": [[139, "difference-in-differences-for-panel-data"]], "Difference-in-Differences for repeated cross-sections": [[139, "difference-in-differences-for-repeated-cross-sections"]], "Disclaimer": [[95, "Disclaimer"]], "Double Machine Learning Algorithm": [[150, "double-machine-learning-algorithm"]], "Double Machine Learning Literature": [[153, null]], "Double machine learning algorithms": [[97, null]], "Double/debiased machine learning": [[58, "Double/debiased-machine-learning"], [75, "Double/debiased-machine-learning"], [98, "double-debiased-machine-learning"]], "DoubleML": [[150, null]], "DoubleML Data Class": [[1, null]], "DoubleML Models": [[3, null]], "DoubleML Object": [[94, "DoubleML-Object"]], "DoubleML Workflow": [[155, null]], "DoubleML meets FLAML - How to tune learners automatically within DoubleML": [[85, null]], "DoubleML with TabPFN": [[72, "DoubleML-with-TabPFN"]], "DoubleMLDIDData": [[104, "doublemldiddata"]], "DoubleMLData": [[104, "doublemldata"]], "DoubleMLData from arrays and matrices": [[99, "doublemldata-from-arrays-and-matrices"], [104, "doublemldata-from-arrays-and-matrices"]], "DoubleMLData from dataframes": [[99, null], [104, "doublemldata-from-dataframes"]], "DoubleMLPanelData": [[66, "DoubleMLPanelData"], [69, "DoubleMLPanelData"], [104, "doublemlpaneldata"]], "DoubleMLRDDData": [[104, "doublemlrdddata"]], "DoubleMLSSMData": [[104, "doublemlssmdata"]], "Effect Aggregation": [[67, "Effect-Aggregation"], [68, "Effect-Aggregation"], [108, "effect-aggregation"]], "Effect Heterogeneity": [[71, "effect-heterogeneity"], [83, "Effect-Heterogeneity"]], "Empirical coverage vs. lasso penalty \\lambda": [[79, "Empirical-coverage-vs.-lasso-penalty-\\lambda"]], "Estimate DML models without sample-splitting": [[122, "estimate-dml-models-without-sample-splitting"]], "Estimate double/debiased machine learning models": [[152, "estimate-double-debiased-machine-learning-models"]], "Estimating Potential Quantiles and Quantile Treatment Effects": [[88, "Estimating-Potential-Quantiles-and-Quantile-Treatment-Effects"]], "Estimating local quantile treatment effects (LQTEs)": [[88, "Estimating-local-quantile-treatment-effects-(LQTEs)"]], "Estimating the Average Treatment Effect of 401(k) Eligibility on Net Financial Assets": [[61, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"], [87, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"]], "Estimating the treatment effect on the Conditional Value a Risk (CVaR)": [[88, "Estimating-the-treatment-effect-on-the-Conditional-Value-a-Risk-(CVaR)"]], "Estimation": [[63, "Estimation"], [63, "id2"], [96, "Estimation"], [96, "id2"]], "Estimation of Average Potential Outcomes": [[72, "Estimation-of-Average-Potential-Outcomes"]], "Estimation quality vs. \\lambda": [[79, "Estimation-quality-vs.-\\lambda"]], "Evaluate learners": [[107, "evaluate-learners"]], "Event Study Aggregation": [[66, "Event-Study-Aggregation"], [67, "Event-Study-Aggregation"], [68, "Event-Study-Aggregation"], [69, "Event-Study-Aggregation"]], "Example usage": [[100, "example-usage"], [101, "example-usage"], [102, "example-usage"], [103, "example-usage"], [104, "example-usage"], [104, "id6"], [104, "id8"], [104, "id10"]], "Examples": [[71, null]], "Exploiting the Functionalities of did": [[59, "Exploiting-the-Functionalities-of-did"]], "Externally provide a sample splitting / partition": [[122, "externally-provide-a-sample-splitting-partition"]], "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)": [[92, null]], "Fuzzy RDD": [[92, "Fuzzy-RDD"]], "Fuzzy RDD Without Adjustment": [[92, "Fuzzy-RDD-Without-Adjustment"]], "Fuzzy RDD with Flexible Adjustment": [[92, "Fuzzy-RDD-with-Flexible-Adjustment"]], "Fuzzy RDD with Linear Adjustment": [[92, "Fuzzy-RDD-with-Linear-Adjustment"]], "Fuzzy Regression Discontinuity Design": [[108, "fuzzy-regression-discontinuity-design"]], "GATE Estimation and Sensitivity": [[82, "GATE-Estimation-and-Sensitivity"]], "GATET Estimation and Sensitivity": [[82, "GATET-Estimation-and-Sensitivity"]], "GATEs for IRM models": [[106, "gates-for-irm-models"]], "GATEs for PLR models": [[106, "gates-for-plr-models"]], "General Examples": [[71, "general-examples"]], "General algorithm": [[139, "general-algorithm"]], "Generate Fuzzy Data": [[92, "Generate-Fuzzy-Data"]], "Generate Sharp Data": [[92, "Generate-Sharp-Data"]], "Getting Started": [[152, null]], "Group Aggregation": [[66, "Group-Aggregation"], [68, "Group-Aggregation"], [69, "Group-Aggregation"]], "Group Average Treatment Effects (GATEs)": [[80, "Group-Average-Treatment-Effects-(GATEs)"], [81, "Group-Average-Treatment-Effects-(GATEs)"]], "Group average treatment effects (GATEs)": [[106, "group-average-treatment-effects-gates"]], "Group-Time Combinations": [[66, "Group-Time-Combinations"], [69, "Group-Time-Combinations"]], "Heterogeneous treatment effects": [[106, null]], "How to exploit more features of mlr3pipelines in DoubleML": [[62, "How-to-exploit-more-features-of-mlr3pipelines-in-DoubleML"]], "Hyperparameter tuning": [[107, "hyperparameter-tuning"], [107, "id16"]], "Hyperparameter tuning with pipelines": [[107, "hyperparameter-tuning-with-pipelines"]], "Implementation": [[139, "implementation"]], "Implementation Details": [[108, "implementation-details"]], "Implementation of the double machine learning algorithms": [[97, "implementation-of-the-double-machine-learning-algorithms"]], "Implementation of the score function and the estimate of the causal parameter": [[123, "implementation-of-the-score-function-and-the-estimate-of-the-causal-parameter"]], "Implemented Neyman orthogonal score functions": [[123, "implemented-neyman-orthogonal-score-functions"]], "Imports and Setup": [[72, "Imports-and-Setup"]], "Initialize DoubleMLClusterData object": [[60, "Initialize-DoubleMLClusterData-object"]], "Initialize DoubleMLData object with clusters": [[86, "Initialize-DoubleMLData-object-with-clusters"]], "Initialize the objects of class DoubleMLPLIV": [[60, "Initialize-the-objects-of-class-DoubleMLPLIV"], [86, "Initialize-the-objects-of-class-DoubleMLPLIV"]], "Installing DoubleML": [[151, null]], "Instrumental Variables Directed Acyclic Graph (IV - DAG)": [[57, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"], [74, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"]], "Interactive IV Model (IIVM)": [[61, "Interactive-IV-Model-(IIVM)"], [87, "Interactive-IV-Model-(IIVM)"]], "Interactive IV model (IIVM)": [[108, "interactive-iv-model-iivm"], [123, "interactive-iv-model-iivm"]], "Interactive Regression Model (IRM)": [[61, "Interactive-Regression-Model-(IRM)"], [80, "Interactive-Regression-Model-(IRM)"], [87, "Interactive-Regression-Model-(IRM)"], [90, "Interactive-Regression-Model-(IRM)"]], "Interactive regression model (IRM)": [[139, "interactive-regression-model-irm"]], "Interactive regression models (IRM)": [[108, "interactive-regression-models-irm"], [123, "interactive-regression-models-irm"], [139, "interactive-regression-models-irm"]], "Key Takeaways": [[72, "Key-Takeaways"]], "Key arguments": [[100, null], [101, null], [102, null], [103, null], [104, "key-arguments"], [104, "id5"], [104, "id7"], [104, "id9"]], "Learners and Hyperparameters": [[83, "Learners-and-Hyperparameters"]], "Learners to estimate the nuisance models": [[152, "learners-to-estimate-the-nuisance-models"]], "Learners, hyperparameters and hyperparameter tuning": [[107, null]], "Linear Covariate Adjustment": [[66, "Linear-Covariate-Adjustment"], [69, "Linear-Covariate-Adjustment"]], "Load Data": [[95, "Load-Data"]], "Load and Process Data": [[60, "Load-and-Process-Data"], [86, "Load-and-Process-Data"]], "Load bonus data using the dml datasets module": [[70, "Load-bonus-data-using-the-dml-datasets-module"]], "Local Average Treatment Effects of 401(k) Participation on Net Financial Assets": [[61, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"], [87, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"]], "Local Potential Quantile Estimation": [[91, "Local-Potential-Quantile-Estimation"]], "Local Potential Quantiles (LPQs)": [[91, "Local-Potential-Quantiles-(LPQs)"]], "Local Quantile Treatment Effects (LQTEs)": [[91, "Local-Quantile-Treatment-Effects-(LQTEs)"]], "Local potential quantiles (LPQs)": [[123, "local-potential-quantiles-lpqs"]], "Machine Learning Methods Comparison": [[72, "Machine-Learning-Methods-Comparison"]], "Main Features": [[150, "main-features"]], "Minimum requirements for learners": [[107, "minimum-requirements-for-learners"], [107, "id2"]], "Missingness at Random": [[108, "missingness-at-random"], [123, "missingness-at-random"]], "Model Performance Evaluation": [[72, "Model-Performance-Evaluation"]], "Model-specific implementations": [[139, "model-specific-implementations"]], "Models": [[108, null]], "Motivation": [[60, "Motivation"], [86, "Motivation"]], "Multiple Average Potential Outcome Models (APOS)": [[73, "Multiple-Average-Potential-Outcome-Models-(APOS)"]], "Naive estimation": [[57, "Naive-estimation"], [74, "Naive-estimation"]], "No Clustering / Zero-Way Clustering": [[60, "No-Clustering-/-Zero-Way-Clustering"], [86, "No-Clustering-/-Zero-Way-Clustering"]], "Nonignorable Nonresponse": [[108, "nonignorable-nonresponse"], [123, "nonignorable-nonresponse"]], "One-Way Clustering with Respect to the Market": [[60, "One-Way-Clustering-with-Respect-to-the-Market"], [86, "One-Way-Clustering-with-Respect-to-the-Market"]], "One-Way Clustering with Respect to the Product": [[60, "One-Way-Clustering-with-Respect-to-the-Product"], [86, "One-Way-Clustering-with-Respect-to-the-Product"]], "One-dimensional Example": [[76, "One-dimensional-Example"], [77, "One-dimensional-Example"]], "Outcome missing at random (MAR)": [[63, "Outcome-missing-at-random-(MAR)"], [96, "Outcome-missing-at-random-(MAR)"]], "Outcome missing under nonignorable nonresponse": [[63, "Outcome-missing-under-nonignorable-nonresponse"], [96, "Outcome-missing-under-nonignorable-nonresponse"]], "Overcoming regularization bias by orthogonalization": [[58, "Overcoming-regularization-bias-by-orthogonalization"], [75, "Overcoming-regularization-bias-by-orthogonalization"], [98, "overcoming-regularization-bias-by-orthogonalization"]], "Panel Data": [[108, "id9"], [110, null], [123, "panel-data"], [123, "id3"], [139, "panel-data"]], "Panel Data (Repeated Outcomes)": [[64, "Panel-Data-(Repeated-Outcomes)"]], "Panel data": [[108, "panel-data"]], "Parameter tuning": [[62, "Parameter-tuning"]], "Parameters & Implementation": [[108, "parameters-implementation"]], "Partialling out score": [[58, "Partialling-out-score"], [75, "Partialling-out-score"], [98, "partialling-out-score"]], "Partially Linear Regression Model (PLR)": [[61, "Partially-Linear-Regression-Model-(PLR)"], [81, "Partially-Linear-Regression-Model-(PLR)"], [87, "Partially-Linear-Regression-Model-(PLR)"]], "Partially linear IV regression model (PLIV)": [[108, "partially-linear-iv-regression-model-pliv"], [123, "partially-linear-iv-regression-model-pliv"]], "Partially linear models (PLM)": [[108, "partially-linear-models-plm"], [123, "partially-linear-models-plm"], [139, "partially-linear-models-plm"]], "Partially linear regression model (PLR)": [[108, "partially-linear-regression-model-plr"], [123, "partially-linear-regression-model-plr"], [139, "partially-linear-regression-model-plr"]], "Performance Summary and Insights": [[72, "Performance-Summary-and-Insights"]], "Plot Coefficients and 95% Confidence Intervals": [[85, "Plot-Coefficients-and-95%-Confidence-Intervals"]], "Policy Learning with Trees": [[90, "Policy-Learning-with-Trees"], [106, "policy-learning-with-trees"]], "Potential Quantile Estimation": [[91, "Potential-Quantile-Estimation"]], "Potential Quantiles (PQs)": [[91, "Potential-Quantiles-(PQs)"]], "Potential quantiles (PQs)": [[106, "potential-quantiles-pqs"], [123, "potential-quantiles-pqs"]], "Python: Average Potential Outcome (APO) Models": [[73, null]], "Python: Basic Instrumental Variables calculation": [[74, null]], "Python: Basics of Double Machine Learning": [[75, null]], "Python: Building the package from source": [[151, "python-building-the-package-from-source"]], "Python: Case studies": [[71, "python-case-studies"]], "Python: Causal Machine Learning with TabPFN": [[72, null]], "Python: Choice of learners": [[84, null]], "Python: Cluster Robust Double Machine Learning": [[86, null]], "Python: Conditional Average Treatment Effects (CATEs) for IRM models": [[76, null]], "Python: Conditional Average Treatment Effects (CATEs) for PLR models": [[77, null]], "Python: Conditional Value at Risk of potential outcomes": [[78, null]], "Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments": [[93, null]], "Python: Difference-in-Differences": [[64, null]], "Python: Difference-in-Differences Pre-Testing": [[65, null]], "Python: First Stage and Causal Estimation": [[79, null]], "Python: GATE Sensitivity Analysis": [[82, null]], "Python: Group Average Treatment Effects (GATEs) for IRM models": [[80, null]], "Python: Group Average Treatment Effects (GATEs) for PLR models": [[81, null]], "Python: IRM and APO Model Comparison": [[83, null]], "Python: Impact of 401(k) on Financial Wealth": [[87, null]], "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)": [[88, null]], "Python: Installing DoubleML": [[151, "python-installing-doubleml"]], "Python: Installing a released version from a .whl file": [[151, "python-installing-a-released-version-from-a-whl-file"]], "Python: Installing the latest release from pip or conda": [[151, "python-installing-the-latest-release-from-pip-or-conda"]], "Python: Learners and hyperparameters": [[107, "python-learners-and-hyperparameters"]], "Python: Optional Dependencies": [[151, "python-optional-dependencies"]], "Python: PLM and IRM for Multiple Treatments": [[89, null]], "Python: Panel Data Introduction": [[68, null]], "Python: Panel Data with Multiple Time Periods": [[66, null]], "Python: Policy Learning with Trees": [[90, null]], "Python: Potential Quantiles and Quantile Treatment Effects": [[91, null]], "Python: Real-Data Example for Multi-Period Difference-in-Differences": [[67, null]], "Python: Repeated Cross-Sectional Data with Multiple Time Periods": [[69, null]], "Python: Sample Selection Models": [[96, null]], "Python: Sensitivity Analysis": [[94, null]], "Python: Sensitivity Analysis for Causal ML": [[95, null]], "Quantile Treatment Effects (QTEs)": [[91, "Quantile-Treatment-Effects-(QTEs)"]], "Quantile treatment effects (QTEs)": [[106, "quantile-treatment-effects-qtes"]], "Quantiles": [[106, "quantiles"]], "R: Basic Instrumental Variables Calculation": [[57, null]], "R: Basics of Double Machine Learning": [[58, null]], "R: Case studies": [[71, "r-case-studies"]], "R: Cluster Robust Double Machine Learning": [[60, null]], "R: DoubleML for Difference-in-Differences": [[59, null]], "R: Ensemble Learners and More with mlr3pipelines": [[62, null]], "R: Impact of 401(k) on Financial Wealth": [[61, null]], "R: Installing DoubleML": [[151, "r-installing-doubleml"]], "R: Installing the development version from GitHub": [[151, "r-installing-the-development-version-from-github"]], "R: Installing the latest release from CRAN": [[151, "r-installing-the-latest-release-from-cran"]], "R: Learners and hyperparameters": [[107, "r-learners-and-hyperparameters"]], "R: Sample Selection Models": [[63, null]], "Ranking Treatment Effects under Treatment Propensity and Treatment Effect Heterogeneity": [[89, "Ranking-Treatment-Effects-under-Treatment-Propensity-and-Treatment-Effect-Heterogeneity"]], "Real-Data Application": [[60, "Real-Data-Application"], [86, "Real-Data-Application"]], "References": [[57, "References"], [59, "References"], [60, "References"], [62, "References"], [74, "References"], [79, "References"], [84, "References"], [85, "References"], [86, "References"], [89, "References"], [93, "References"], [95, "References"], [98, "references"], [107, "references"], [122, "references"], [138, "references"], [150, "references"], [152, "references"]], "Regression Discontinuity Designs (RDD)": [[108, "regression-discontinuity-designs-rdd"]], "Regularization Bias in Simple ML-Approaches": [[58, "Regularization-Bias-in-Simple-ML-Approaches"], [75, "Regularization-Bias-in-Simple-ML-Approaches"]], "Regularization bias in simple ML-approaches": [[98, "regularization-bias-in-simple-ml-approaches"]], "Release Notes": [[154, null]], "Repeated Cross-Sectional Data": [[64, "Repeated-Cross-Sectional-Data"], [123, "repeated-cross-sectional-data"], [123, "id4"], [139, "repeated-cross-sectional-data"]], "Repeated cross-fitting with K folds and M repetitions": [[122, "repeated-cross-fitting-with-k-folds-and-m-repetitions"]], "Repeated cross-sections": [[108, "repeated-cross-sections"], [108, "id10"], [110, "repeated-cross-sections"]], "Running a small simulation": [[93, "Running-a-small-simulation"]], "Sample Selection Models": [[123, "sample-selection-models"]], "Sample Selection Models (SSM)": [[108, "sample-selection-models-ssm"]], "Sample splitting to remove bias induced by overfitting": [[58, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [75, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [98, "sample-splitting-to-remove-bias-induced-by-overfitting"]], "Sample-splitting without cross-fitting": [[122, "sample-splitting-without-cross-fitting"]], "Sample-splitting, cross-fitting and repeated cross-fitting": [[122, null]], "Sandbox/Archive": [[71, "sandbox-archive"]], "Score Mixin Classes for DoubleML Models": [[55, null]], "Score functions": [[123, null]], "Selected Combinations": [[66, "Selected-Combinations"], [69, "Selected-Combinations"]], "Sensitivity Analysis": [[66, "Sensitivity-Analysis"], [69, "Sensitivity-Analysis"], [73, "Sensitivity-Analysis"], [83, "Sensitivity-Analysis"], [94, "Sensitivity-Analysis"], [94, "id1"]], "Sensitivity Analysis with IRM": [[94, "Sensitivity-Analysis-with-IRM"]], "Sensitivity analysis": [[139, null]], "Set up learners based on mlr3pipelines": [[62, "Set-up-learners-based-on-mlr3pipelines"]], "Sharp RDD": [[92, "Sharp-RDD"]], "Sharp RDD Without Adjustment": [[92, "Sharp-RDD-Without-Adjustment"]], "Sharp RDD with Flexible Adjustment": [[92, "Sharp-RDD-with-Flexible-Adjustment"]], "Sharp RDD with Linear Adjustment": [[92, "Sharp-RDD-with-Linear-Adjustment"]], "Sharp Regression Discontinuity Design": [[108, "sharp-regression-discontinuity-design"]], "Simulate two-way cluster data": [[60, "Simulate-two-way-cluster-data"], [86, "Simulate-two-way-cluster-data"]], "Simulation Example": [[94, "Simulation-Example"]], "Simultaneous inference over different DoubleML models (advanced)": [[138, "simultaneous-inference-over-different-doubleml-models-advanced"]], "Single Average Potential Outcome Models (APO)": [[73, "Single-Average-Potential-Outcome-Models-(APO)"]], "Source code and maintenance": [[150, "source-code-and-maintenance"]], "Special Data Types": [[104, "special-data-types"]], "Specify learner and estimate causal parameter: IRM model with Lasso as learner": [[70, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: IRM model with random forest as learner": [[70, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-random-forest-as-learner"]], "Specify learner and estimate causal parameter: PLR model with Lasso as learner": [[70, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: PLR model with random forest as learner": [[70, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-random-forest-as-learner"]], "Specifying alternative score functions via callables": [[123, "specifying-alternative-score-functions-via-callables"]], "Specifying learners and set hyperparameters": [[107, "specifying-learners-and-set-hyperparameters"], [107, "id9"]], "Standard approach": [[84, "Standard-approach"]], "Step 1: Custom API for FLAML Models within DoubleML": [[85, "Step-1:-Custom-API-for-FLAML-Models-within-DoubleML"]], "Step 1: Initialize and Train the AutoML Models:": [[85, "Step-1:-Initialize-and-Train-the-AutoML-Models:"]], "Step 2: Evaluate the Tuned Models": [[85, "Step-2:-Evaluate-the-Tuned-Models"]], "Step 2: Using the API when calling DoubleML\u2019s .fit() Method": [[85, "Step-2:-Using-the-API-when-calling-DoubleML's-.fit()-Method"]], "Step 3: Create and Fit DoubleML Model": [[85, "Step-3:-Create-and-Fit-DoubleML-Model"]], "Summary Figure": [[89, "Summary-Figure"]], "Summary of Results": [[61, "Summary-of-Results"], [87, "Summary-of-Results"]], "The Augmented Inverse Propensity Weighting Model estimates the ATE under arbitrary effect and propensity score heterogeneity": [[89, "The-Augmented-Inverse-Propensity-Weighting-Model-estimates-the-ATE-under-arbitrary-effect-and-propensity-score-heterogeneity"]], "The Data Backend: DoubleMLData": [[61, "The-Data-Backend:-DoubleMLData"], [87, "The-Data-Backend:-DoubleMLData"]], "The DoubleML package": [[61, "The-DoubleML-package"], [87, "The-DoubleML-package"]], "The Partially Linear Model performs overlap weighting": [[89, "The-Partially-Linear-Model-performs-overlap-weighting"]], "The basics of double/debiased machine learning": [[98, null]], "The causal model": [[152, "the-causal-model"]], "The data-backend DoubleMLData": [[152, "the-data-backend-doublemldata"]], "Theory": [[139, "theory"]], "Time Aggregation": [[66, "Time-Aggregation"], [68, "Time-Aggregation"], [69, "Time-Aggregation"]], "Tuning on the Folds": [[85, "Tuning-on-the-Folds"]], "Tuning on the full Sample": [[85, "Tuning-on-the-full-Sample"]], "Two treatment periods": [[108, "two-treatment-periods"], [123, "two-treatment-periods"], [139, "two-treatment-periods"]], "Two-Dimensional Example": [[76, "Two-Dimensional-Example"], [77, "Two-Dimensional-Example"]], "Two-Way Clustering with Respect to Product and Market": [[60, "Two-Way-Clustering-with-Respect-to-Product-and-Market"], [86, "Two-Way-Clustering-with-Respect-to-Product-and-Market"]], "Universal Base Period": [[66, "Universal-Base-Period"], [69, "Universal-Base-Period"]], "Untuned (default parameter) XGBoost": [[85, "Untuned-(default-parameter)-XGBoost"]], "Use ensemble learners based on mlr3pipelines": [[62, "Use-ensemble-learners-based-on-mlr3pipelines"]], "User Guide": [[105, null]], "Using DoubleML": [[57, "Using-DoubleML"], [74, "Using-DoubleML"]], "Using ML for DiD: Integrating DoubleML in did": [[59, "Using-ML-for-DiD:-Integrating-DoubleML-in-did"]], "Using learners from mlr3, mlr3learners and mlr3extralearners": [[62, "Using-learners-from-mlr3,-mlr3learners-and-mlr3extralearners"]], "Using pipelines to construct learners": [[107, "using-pipelines-to-construct-learners"]], "Utility Classes": [[56, "utility-classes"]], "Utility Classes and Functions": [[56, null]], "Utility Functions": [[56, "utility-functions"]], "VanderWeele and Arah (2011): Benchmarking": [[95, "VanderWeele-and-Arah-(2011):-Benchmarking"]], "Variance estimation": [[138, "variance-estimation"]], "Variance estimation and confidence intervals": [[138, null]], "Visualizing Average Potential Outcomes": [[72, "Visualizing-Average-Potential-Outcomes"]], "Visualizing Average Treatment Effects": [[72, "Visualizing-Average-Treatment-Effects"]], "Visualizing the Treatment Effect Structure": [[72, "Visualizing-the-Treatment-Effect-Structure"]], "Weighted Average Treatment Effects": [[106, "weighted-average-treatment-effects"]], "doubleml.data.DoubleMLClusterData": [[4, null]], "doubleml.data.DoubleMLDIDData": [[5, null]], "doubleml.data.DoubleMLData": [[6, null]], "doubleml.data.DoubleMLPanelData": [[7, null]], "doubleml.data.DoubleMLRDDData": [[8, null]], "doubleml.data.DoubleMLSSMData": [[9, null]], "doubleml.datasets.fetch_401K": [[10, null]], "doubleml.datasets.fetch_bonus": [[11, null]], "doubleml.did": [[3, "doubleml-did"]], "doubleml.did.DoubleMLDID": [[12, null]], "doubleml.did.DoubleMLDIDAggregation": [[13, null]], "doubleml.did.DoubleMLDIDBinary": [[14, null]], "doubleml.did.DoubleMLDIDCS": [[15, null]], "doubleml.did.DoubleMLDIDMulti": [[16, null]], "doubleml.did.datasets.make_did_CS2021": [[17, null]], "doubleml.did.datasets.make_did_SZ2020": [[18, null]], "doubleml.did.datasets.make_did_cs_CS2021": [[19, null]], "doubleml.double_ml_score_mixins.LinearScoreMixin": [[20, null]], "doubleml.double_ml_score_mixins.NonLinearScoreMixin": [[21, null]], "doubleml.irm": [[3, "doubleml-irm"]], "doubleml.irm.DoubleMLAPO": [[22, null]], "doubleml.irm.DoubleMLAPOS": [[23, null]], "doubleml.irm.DoubleMLCVAR": [[24, null]], "doubleml.irm.DoubleMLIIVM": [[25, null]], "doubleml.irm.DoubleMLIRM": [[26, null]], "doubleml.irm.DoubleMLLPQ": [[27, null]], "doubleml.irm.DoubleMLPQ": [[28, null]], "doubleml.irm.DoubleMLQTE": [[29, null]], "doubleml.irm.DoubleMLSSM": [[30, null]], "doubleml.irm.datasets.make_confounded_irm_data": [[31, null]], "doubleml.irm.datasets.make_heterogeneous_data": [[32, null]], "doubleml.irm.datasets.make_iivm_data": [[33, null]], "doubleml.irm.datasets.make_irm_data": [[34, null]], "doubleml.irm.datasets.make_irm_data_discrete_treatments": [[35, null]], "doubleml.irm.datasets.make_ssm_data": [[36, null]], "doubleml.plm": [[3, "doubleml-plm"]], "doubleml.plm.DoubleMLPLIV": [[37, null]], "doubleml.plm.DoubleMLPLR": [[38, null]], "doubleml.plm.datasets.make_confounded_plr_data": [[39, null]], "doubleml.plm.datasets.make_pliv_CHS2015": [[40, null]], "doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021": [[41, null]], "doubleml.plm.datasets.make_plr_CCDDHNR2018": [[42, null]], "doubleml.plm.datasets.make_plr_turrell2018": [[43, null]], "doubleml.rdd": [[3, "doubleml-rdd"]], "doubleml.rdd.RDFlex": [[44, null]], "doubleml.rdd.datasets.make_simple_rdd_data": [[45, null]], "doubleml.utils.DMLDummyClassifier": [[46, null]], "doubleml.utils.DMLDummyRegressor": [[47, null]], "doubleml.utils.DoubleMLBLP": [[48, null]], "doubleml.utils.DoubleMLPolicyTree": [[49, null]], "doubleml.utils.GlobalClassifier": [[50, null]], "doubleml.utils.GlobalRegressor": [[51, null]], "doubleml.utils.PSProcessor": [[52, null]], "doubleml.utils.PSProcessorConfig": [[53, null]], "doubleml.utils.gain_statistics": [[54, null]]}, "docnames": ["api/api", "api/data_class", "api/datasets", "api/dml_models", "api/generated/doubleml.data.DoubleMLClusterData", "api/generated/doubleml.data.DoubleMLDIDData", "api/generated/doubleml.data.DoubleMLData", "api/generated/doubleml.data.DoubleMLPanelData", "api/generated/doubleml.data.DoubleMLRDDData", "api/generated/doubleml.data.DoubleMLSSMData", "api/generated/doubleml.datasets.fetch_401K", "api/generated/doubleml.datasets.fetch_bonus", "api/generated/doubleml.did.DoubleMLDID", "api/generated/doubleml.did.DoubleMLDIDAggregation", "api/generated/doubleml.did.DoubleMLDIDBinary", "api/generated/doubleml.did.DoubleMLDIDCS", "api/generated/doubleml.did.DoubleMLDIDMulti", "api/generated/doubleml.did.datasets.make_did_CS2021", "api/generated/doubleml.did.datasets.make_did_SZ2020", "api/generated/doubleml.did.datasets.make_did_cs_CS2021", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin", "api/generated/doubleml.irm.DoubleMLAPO", "api/generated/doubleml.irm.DoubleMLAPOS", "api/generated/doubleml.irm.DoubleMLCVAR", "api/generated/doubleml.irm.DoubleMLIIVM", "api/generated/doubleml.irm.DoubleMLIRM", "api/generated/doubleml.irm.DoubleMLLPQ", "api/generated/doubleml.irm.DoubleMLPQ", "api/generated/doubleml.irm.DoubleMLQTE", "api/generated/doubleml.irm.DoubleMLSSM", "api/generated/doubleml.irm.datasets.make_confounded_irm_data", "api/generated/doubleml.irm.datasets.make_heterogeneous_data", "api/generated/doubleml.irm.datasets.make_iivm_data", "api/generated/doubleml.irm.datasets.make_irm_data", "api/generated/doubleml.irm.datasets.make_irm_data_discrete_treatments", "api/generated/doubleml.irm.datasets.make_ssm_data", "api/generated/doubleml.plm.DoubleMLPLIV", "api/generated/doubleml.plm.DoubleMLPLR", "api/generated/doubleml.plm.datasets.make_confounded_plr_data", "api/generated/doubleml.plm.datasets.make_pliv_CHS2015", "api/generated/doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021", "api/generated/doubleml.plm.datasets.make_plr_CCDDHNR2018", "api/generated/doubleml.plm.datasets.make_plr_turrell2018", "api/generated/doubleml.rdd.RDFlex", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data", "api/generated/doubleml.utils.DMLDummyClassifier", "api/generated/doubleml.utils.DMLDummyRegressor", "api/generated/doubleml.utils.DoubleMLBLP", "api/generated/doubleml.utils.DoubleMLPolicyTree", "api/generated/doubleml.utils.GlobalClassifier", "api/generated/doubleml.utils.GlobalRegressor", "api/generated/doubleml.utils.PSProcessor", "api/generated/doubleml.utils.PSProcessorConfig", "api/generated/doubleml.utils.gain_statistics", "api/mixins", "api/utility", "examples/R_double_ml_basic_iv", "examples/R_double_ml_basics", "examples/R_double_ml_did", "examples/R_double_ml_multiway_cluster", "examples/R_double_ml_pension", "examples/R_double_ml_pipeline", "examples/R_double_ml_ssm", "examples/did/py_did", "examples/did/py_did_pretest", "examples/did/py_panel", "examples/did/py_panel_data_example", "examples/did/py_panel_simple", "examples/did/py_rep_cs", "examples/double_ml_bonus_data", "examples/index", "examples/learners/py_tabpfn", "examples/py_double_ml_apo", "examples/py_double_ml_basic_iv", "examples/py_double_ml_basics", "examples/py_double_ml_cate", "examples/py_double_ml_cate_plr", "examples/py_double_ml_cvar", "examples/py_double_ml_firststage", "examples/py_double_ml_gate", "examples/py_double_ml_gate_plr", "examples/py_double_ml_gate_sensitivity", "examples/py_double_ml_irm_vs_apo", "examples/py_double_ml_learner", "examples/py_double_ml_meets_flaml", "examples/py_double_ml_multiway_cluster", "examples/py_double_ml_pension", "examples/py_double_ml_pension_qte", "examples/py_double_ml_plm_irm_hetfx", "examples/py_double_ml_policy_tree", "examples/py_double_ml_pq", "examples/py_double_ml_rdflex", "examples/py_double_ml_robust_iv", "examples/py_double_ml_sensitivity", "examples/py_double_ml_sensitivity_booking", "examples/py_double_ml_ssm", "guide/algorithms", "guide/basics", "guide/data/base_data", "guide/data/did_data", "guide/data/panel_data", "guide/data/rdd_data", "guide/data/ssm_data", "guide/data_backend", "guide/guide", "guide/heterogeneity", "guide/learners", "guide/models", "guide/models/did/did_aggregation", "guide/models/did/did_binary", "guide/models/did/did_cs", "guide/models/did/did_implementation", "guide/models/did/did_pa", "guide/models/did/did_setup", "guide/models/irm/apo", "guide/models/irm/apos", "guide/models/irm/iivm", "guide/models/irm/irm", "guide/models/plm/pliv", "guide/models/plm/plr", "guide/models/ssm/ssm", "guide/resampling", "guide/scores", "guide/scores/did/did_cs_binary_score", "guide/scores/did/did_cs_score", "guide/scores/did/did_pa_binary_score", "guide/scores/did/did_pa_score", "guide/scores/irm/apo_score", "guide/scores/irm/cvar_score", "guide/scores/irm/iivm_score", "guide/scores/irm/irm_score", "guide/scores/irm/lpq_score", "guide/scores/irm/pq_score", "guide/scores/plm/pliv_score", "guide/scores/plm/plr_score", "guide/scores/ssm/mar_score", "guide/scores/ssm/nr_score", "guide/se_confint", "guide/sensitivity", "guide/sensitivity/benchmarking", "guide/sensitivity/did/did_cs_binary_sensitivity", "guide/sensitivity/did/did_cs_sensitivity", "guide/sensitivity/did/did_pa_binary_sensitivity", "guide/sensitivity/did/did_pa_sensitivity", "guide/sensitivity/implementation", "guide/sensitivity/irm/apo_sensitivity", "guide/sensitivity/irm/irm_sensitivity", "guide/sensitivity/plm/plr_sensitivity", "guide/sensitivity/theory", "index", "intro/install", "intro/intro", "literature/literature", "release/release", "workflow/workflow"], "envversion": {"nbsphinx": 4, "sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1}, "filenames": ["api/api.rst", "api/data_class.rst", "api/datasets.rst", "api/dml_models.rst", "api/generated/doubleml.data.DoubleMLClusterData.rst", "api/generated/doubleml.data.DoubleMLDIDData.rst", "api/generated/doubleml.data.DoubleMLData.rst", "api/generated/doubleml.data.DoubleMLPanelData.rst", "api/generated/doubleml.data.DoubleMLRDDData.rst", "api/generated/doubleml.data.DoubleMLSSMData.rst", "api/generated/doubleml.datasets.fetch_401K.rst", "api/generated/doubleml.datasets.fetch_bonus.rst", "api/generated/doubleml.did.DoubleMLDID.rst", "api/generated/doubleml.did.DoubleMLDIDAggregation.rst", "api/generated/doubleml.did.DoubleMLDIDBinary.rst", "api/generated/doubleml.did.DoubleMLDIDCS.rst", "api/generated/doubleml.did.DoubleMLDIDMulti.rst", "api/generated/doubleml.did.datasets.make_did_CS2021.rst", "api/generated/doubleml.did.datasets.make_did_SZ2020.rst", "api/generated/doubleml.did.datasets.make_did_cs_CS2021.rst", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin.rst", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin.rst", "api/generated/doubleml.irm.DoubleMLAPO.rst", "api/generated/doubleml.irm.DoubleMLAPOS.rst", "api/generated/doubleml.irm.DoubleMLCVAR.rst", "api/generated/doubleml.irm.DoubleMLIIVM.rst", "api/generated/doubleml.irm.DoubleMLIRM.rst", "api/generated/doubleml.irm.DoubleMLLPQ.rst", "api/generated/doubleml.irm.DoubleMLPQ.rst", "api/generated/doubleml.irm.DoubleMLQTE.rst", "api/generated/doubleml.irm.DoubleMLSSM.rst", "api/generated/doubleml.irm.datasets.make_confounded_irm_data.rst", "api/generated/doubleml.irm.datasets.make_heterogeneous_data.rst", "api/generated/doubleml.irm.datasets.make_iivm_data.rst", "api/generated/doubleml.irm.datasets.make_irm_data.rst", "api/generated/doubleml.irm.datasets.make_irm_data_discrete_treatments.rst", "api/generated/doubleml.irm.datasets.make_ssm_data.rst", "api/generated/doubleml.plm.DoubleMLPLIV.rst", "api/generated/doubleml.plm.DoubleMLPLR.rst", "api/generated/doubleml.plm.datasets.make_confounded_plr_data.rst", "api/generated/doubleml.plm.datasets.make_pliv_CHS2015.rst", "api/generated/doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021.rst", "api/generated/doubleml.plm.datasets.make_plr_CCDDHNR2018.rst", "api/generated/doubleml.plm.datasets.make_plr_turrell2018.rst", "api/generated/doubleml.rdd.RDFlex.rst", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data.rst", "api/generated/doubleml.utils.DMLDummyClassifier.rst", "api/generated/doubleml.utils.DMLDummyRegressor.rst", "api/generated/doubleml.utils.DoubleMLBLP.rst", "api/generated/doubleml.utils.DoubleMLPolicyTree.rst", "api/generated/doubleml.utils.GlobalClassifier.rst", "api/generated/doubleml.utils.GlobalRegressor.rst", "api/generated/doubleml.utils.PSProcessor.rst", "api/generated/doubleml.utils.PSProcessorConfig.rst", "api/generated/doubleml.utils.gain_statistics.rst", "api/mixins.rst", "api/utility.rst", "examples/R_double_ml_basic_iv.ipynb", "examples/R_double_ml_basics.ipynb", "examples/R_double_ml_did.ipynb", "examples/R_double_ml_multiway_cluster.ipynb", "examples/R_double_ml_pension.ipynb", "examples/R_double_ml_pipeline.ipynb", "examples/R_double_ml_ssm.ipynb", "examples/did/py_did.ipynb", "examples/did/py_did_pretest.ipynb", "examples/did/py_panel.ipynb", "examples/did/py_panel_data_example.ipynb", "examples/did/py_panel_simple.ipynb", "examples/did/py_rep_cs.ipynb", "examples/double_ml_bonus_data.ipynb", "examples/index.rst", "examples/learners/py_tabpfn.ipynb", "examples/py_double_ml_apo.ipynb", "examples/py_double_ml_basic_iv.ipynb", "examples/py_double_ml_basics.ipynb", "examples/py_double_ml_cate.ipynb", "examples/py_double_ml_cate_plr.ipynb", "examples/py_double_ml_cvar.ipynb", "examples/py_double_ml_firststage.ipynb", "examples/py_double_ml_gate.ipynb", "examples/py_double_ml_gate_plr.ipynb", "examples/py_double_ml_gate_sensitivity.ipynb", "examples/py_double_ml_irm_vs_apo.ipynb", "examples/py_double_ml_learner.ipynb", "examples/py_double_ml_meets_flaml.ipynb", "examples/py_double_ml_multiway_cluster.ipynb", "examples/py_double_ml_pension.ipynb", "examples/py_double_ml_pension_qte.ipynb", "examples/py_double_ml_plm_irm_hetfx.ipynb", "examples/py_double_ml_policy_tree.ipynb", "examples/py_double_ml_pq.ipynb", "examples/py_double_ml_rdflex.ipynb", "examples/py_double_ml_robust_iv.ipynb", "examples/py_double_ml_sensitivity.ipynb", "examples/py_double_ml_sensitivity_booking.ipynb", "examples/py_double_ml_ssm.ipynb", "guide/algorithms.rst", "guide/basics.rst", "guide/data/base_data.rst", "guide/data/did_data.rst", "guide/data/panel_data.rst", "guide/data/rdd_data.rst", "guide/data/ssm_data.rst", "guide/data_backend.rst", "guide/guide.rst", "guide/heterogeneity.rst", "guide/learners.rst", "guide/models.rst", "guide/models/did/did_aggregation.rst", "guide/models/did/did_binary.rst", "guide/models/did/did_cs.rst", "guide/models/did/did_implementation.rst", "guide/models/did/did_pa.rst", "guide/models/did/did_setup.rst", "guide/models/irm/apo.rst", "guide/models/irm/apos.rst", "guide/models/irm/iivm.rst", "guide/models/irm/irm.rst", "guide/models/plm/pliv.rst", "guide/models/plm/plr.rst", "guide/models/ssm/ssm.rst", "guide/resampling.rst", "guide/scores.rst", "guide/scores/did/did_cs_binary_score.rst", "guide/scores/did/did_cs_score.rst", "guide/scores/did/did_pa_binary_score.rst", "guide/scores/did/did_pa_score.rst", "guide/scores/irm/apo_score.rst", "guide/scores/irm/cvar_score.rst", "guide/scores/irm/iivm_score.rst", "guide/scores/irm/irm_score.rst", "guide/scores/irm/lpq_score.rst", "guide/scores/irm/pq_score.rst", "guide/scores/plm/pliv_score.rst", "guide/scores/plm/plr_score.rst", "guide/scores/ssm/mar_score.rst", "guide/scores/ssm/nr_score.rst", "guide/se_confint.rst", "guide/sensitivity.rst", "guide/sensitivity/benchmarking.rst", "guide/sensitivity/did/did_cs_binary_sensitivity.rst", "guide/sensitivity/did/did_cs_sensitivity.rst", "guide/sensitivity/did/did_pa_binary_sensitivity.rst", "guide/sensitivity/did/did_pa_sensitivity.rst", "guide/sensitivity/implementation.rst", "guide/sensitivity/irm/apo_sensitivity.rst", "guide/sensitivity/irm/irm_sensitivity.rst", "guide/sensitivity/plm/plr_sensitivity.rst", "guide/sensitivity/theory.rst", "index.rst", "intro/install.rst", "intro/intro.rst", "literature/literature.rst", "release/release.rst", "workflow/workflow.rst"], "indexentries": {"adjust_ps() (doubleml.utils.psprocessor method)": [[52, "doubleml.utils.PSProcessor.adjust_ps", false]], "aggregate() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.aggregate", false]], "aggregate_over_splits() (doubleml.rdd.rdflex method)": [[44, "doubleml.rdd.RDFlex.aggregate_over_splits", false]], "bootstrap() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.bootstrap", false]], "bootstrap() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.bootstrap", false]], "capo() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.capo", false]], "cate() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.cate", false]], "cate() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.cate", false]], "causal_contrast() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.causal_contrast", false]], "confint() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.confint", false]], "confint() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.confint", false]], "confint() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.confint", false]], "confint() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.confint", false]], "confint() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.confint", false]], "confint() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.confint", false]], "confint() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.confint", false]], "confint() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.confint", false]], "confint() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.confint", false]], "confint() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.confint", false]], "confint() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.confint", false]], "confint() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.confint", false]], "confint() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.confint", false]], "confint() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.confint", false]], "confint() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.confint", false]], "confint() (doubleml.rdd.rdflex method)": [[44, "doubleml.rdd.RDFlex.confint", false]], "confint() (doubleml.utils.doublemlblp method)": [[48, "doubleml.utils.DoubleMLBLP.confint", false]], "construct_framework() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.construct_framework", false]], "construct_framework() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.construct_framework", false]], "dmldummyclassifier (class in doubleml.utils)": [[46, "doubleml.utils.DMLDummyClassifier", false]], "dmldummyregressor (class in doubleml.utils)": [[47, "doubleml.utils.DMLDummyRegressor", false]], "doublemlapo (class in doubleml.irm)": [[22, "doubleml.irm.DoubleMLAPO", false]], "doublemlapos (class in doubleml.irm)": [[23, "doubleml.irm.DoubleMLAPOS", false]], "doublemlblp (class in doubleml.utils)": [[48, "doubleml.utils.DoubleMLBLP", false]], "doublemlclusterdata (class in doubleml.data)": [[4, "doubleml.data.DoubleMLClusterData", false]], "doublemlcvar (class in doubleml.irm)": [[24, "doubleml.irm.DoubleMLCVAR", false]], "doublemldata (class in doubleml.data)": [[6, "doubleml.data.DoubleMLData", false]], "doublemldid (class in doubleml.did)": [[12, "doubleml.did.DoubleMLDID", false]], "doublemldidaggregation (class in doubleml.did)": [[13, "doubleml.did.DoubleMLDIDAggregation", false]], "doublemldidbinary (class in doubleml.did)": [[14, "doubleml.did.DoubleMLDIDBinary", false]], "doublemldidcs (class in doubleml.did)": [[15, "doubleml.did.DoubleMLDIDCS", false]], "doublemldiddata (class in doubleml.data)": [[5, "doubleml.data.DoubleMLDIDData", false]], "doublemldidmulti (class in doubleml.did)": [[16, "doubleml.did.DoubleMLDIDMulti", false]], "doublemliivm (class in doubleml.irm)": [[25, "doubleml.irm.DoubleMLIIVM", false]], "doublemlirm (class in doubleml.irm)": [[26, "doubleml.irm.DoubleMLIRM", false]], "doublemllpq (class in doubleml.irm)": [[27, "doubleml.irm.DoubleMLLPQ", false]], "doublemlpaneldata (class in doubleml.data)": [[7, "doubleml.data.DoubleMLPanelData", false]], "doublemlpliv (class in doubleml.plm)": [[37, "doubleml.plm.DoubleMLPLIV", false]], "doublemlplr (class in doubleml.plm)": [[38, "doubleml.plm.DoubleMLPLR", false]], "doublemlpolicytree (class in doubleml.utils)": [[49, "doubleml.utils.DoubleMLPolicyTree", false]], "doublemlpq (class in doubleml.irm)": [[28, "doubleml.irm.DoubleMLPQ", false]], "doublemlqte (class in doubleml.irm)": [[29, "doubleml.irm.DoubleMLQTE", false]], "doublemlrdddata (class in doubleml.data)": [[8, "doubleml.data.DoubleMLRDDData", false]], "doublemlssm (class in doubleml.irm)": [[30, "doubleml.irm.DoubleMLSSM", false]], "doublemlssmdata (class in doubleml.data)": [[9, "doubleml.data.DoubleMLSSMData", false]], "draw_sample_splitting() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.draw_sample_splitting", false]], "evaluate_learners() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.evaluate_learners", false]], "fetch_401k() (in module doubleml.datasets)": [[10, "doubleml.datasets.fetch_401K", false]], "fetch_bonus() (in module doubleml.datasets)": [[11, "doubleml.datasets.fetch_bonus", false]], "fit() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.fit", false]], "fit() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.fit", false]], "fit() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.fit", false]], "fit() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.fit", false]], "fit() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.fit", false]], "fit() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.fit", false]], "fit() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.fit", false]], "fit() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.fit", false]], "fit() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.fit", false]], "fit() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.fit", false]], "fit() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.fit", false]], "fit() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.fit", false]], "fit() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.fit", false]], "fit() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.fit", false]], "fit() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.fit", false]], "fit() (doubleml.rdd.rdflex method)": [[44, "doubleml.rdd.RDFlex.fit", false]], "fit() (doubleml.utils.dmldummyclassifier method)": [[46, "doubleml.utils.DMLDummyClassifier.fit", false]], "fit() (doubleml.utils.dmldummyregressor method)": [[47, "doubleml.utils.DMLDummyRegressor.fit", false]], "fit() (doubleml.utils.doublemlblp method)": [[48, "doubleml.utils.DoubleMLBLP.fit", false]], "fit() (doubleml.utils.doublemlpolicytree method)": [[49, "doubleml.utils.DoubleMLPolicyTree.fit", false]], "fit() (doubleml.utils.globalclassifier method)": [[50, "doubleml.utils.GlobalClassifier.fit", false]], "fit() (doubleml.utils.globalregressor method)": [[51, "doubleml.utils.GlobalRegressor.fit", false]], "from_arrays() (doubleml.data.doublemlclusterdata class method)": [[4, "doubleml.data.DoubleMLClusterData.from_arrays", false]], "from_arrays() (doubleml.data.doublemldata class method)": [[6, "doubleml.data.DoubleMLData.from_arrays", false]], "from_arrays() (doubleml.data.doublemldiddata class method)": [[5, "doubleml.data.DoubleMLDIDData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlpaneldata class method)": [[7, "doubleml.data.DoubleMLPanelData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlrdddata class method)": [[8, "doubleml.data.DoubleMLRDDData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlssmdata class method)": [[9, "doubleml.data.DoubleMLSSMData.from_arrays", false]], "from_config() (doubleml.utils.psprocessor class method)": [[52, "doubleml.utils.PSProcessor.from_config", false]], "gain_statistics() (in module doubleml.utils)": [[54, "doubleml.utils.gain_statistics", false]], "gapo() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.gapo", false]], "gate() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.gate", false]], "gate() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.gate", false]], "get_metadata_routing() (doubleml.utils.dmldummyclassifier method)": [[46, "doubleml.utils.DMLDummyClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.dmldummyregressor method)": [[47, "doubleml.utils.DMLDummyRegressor.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalclassifier method)": [[50, "doubleml.utils.GlobalClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalregressor method)": [[51, "doubleml.utils.GlobalRegressor.get_metadata_routing", false]], "get_params() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.get_params", false]], "get_params() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.get_params", false]], "get_params() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.get_params", false]], "get_params() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.get_params", false]], "get_params() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.get_params", false]], "get_params() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.get_params", false]], "get_params() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.get_params", false]], "get_params() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.get_params", false]], "get_params() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.get_params", false]], "get_params() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.get_params", false]], "get_params() (doubleml.utils.dmldummyclassifier method)": [[46, "doubleml.utils.DMLDummyClassifier.get_params", false]], "get_params() (doubleml.utils.dmldummyregressor method)": [[47, "doubleml.utils.DMLDummyRegressor.get_params", false]], "get_params() (doubleml.utils.globalclassifier method)": [[50, "doubleml.utils.GlobalClassifier.get_params", false]], "get_params() (doubleml.utils.globalregressor method)": [[51, "doubleml.utils.GlobalRegressor.get_params", false]], "globalclassifier (class in doubleml.utils)": [[50, "doubleml.utils.GlobalClassifier", false]], "globalregressor (class in doubleml.utils)": [[51, "doubleml.utils.GlobalRegressor", false]], "linearscoremixin (class in doubleml.double_ml_score_mixins)": [[20, "doubleml.double_ml_score_mixins.LinearScoreMixin", false]], "make_confounded_irm_data() (in module doubleml.irm.datasets)": [[31, "doubleml.irm.datasets.make_confounded_irm_data", false]], "make_confounded_plr_data() (in module doubleml.plm.datasets)": [[39, "doubleml.plm.datasets.make_confounded_plr_data", false]], "make_did_cs2021() (in module doubleml.did.datasets)": [[17, "doubleml.did.datasets.make_did_CS2021", false]], "make_did_cs_cs2021() (in module doubleml.did.datasets)": [[19, "doubleml.did.datasets.make_did_cs_CS2021", false]], "make_did_sz2020() (in module doubleml.did.datasets)": [[18, "doubleml.did.datasets.make_did_SZ2020", false]], "make_heterogeneous_data() (in module doubleml.irm.datasets)": [[32, "doubleml.irm.datasets.make_heterogeneous_data", false]], "make_iivm_data() (in module doubleml.irm.datasets)": [[33, "doubleml.irm.datasets.make_iivm_data", false]], "make_irm_data() (in module doubleml.irm.datasets)": [[34, "doubleml.irm.datasets.make_irm_data", false]], "make_irm_data_discrete_treatments() (in module doubleml.irm.datasets)": [[35, "doubleml.irm.datasets.make_irm_data_discrete_treatments", false]], "make_pliv_chs2015() (in module doubleml.plm.datasets)": [[40, "doubleml.plm.datasets.make_pliv_CHS2015", false]], "make_pliv_multiway_cluster_ckms2021() (in module doubleml.plm.datasets)": [[41, "doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021", false]], "make_plr_ccddhnr2018() (in module doubleml.plm.datasets)": [[42, "doubleml.plm.datasets.make_plr_CCDDHNR2018", false]], "make_plr_turrell2018() (in module doubleml.plm.datasets)": [[43, "doubleml.plm.datasets.make_plr_turrell2018", false]], "make_simple_rdd_data() (in module doubleml.rdd.datasets)": [[45, "doubleml.rdd.datasets.make_simple_rdd_data", false]], "make_ssm_data() (in module doubleml.irm.datasets)": [[36, "doubleml.irm.datasets.make_ssm_data", false]], "nonlinearscoremixin (class in doubleml.double_ml_score_mixins)": [[21, "doubleml.double_ml_score_mixins.NonLinearScoreMixin", false]], "p_adjust() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.p_adjust", false]], "p_adjust() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.p_adjust", false]], "plot_effects() (doubleml.did.doublemldidaggregation method)": [[13, "doubleml.did.DoubleMLDIDAggregation.plot_effects", false]], "plot_effects() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.plot_effects", false]], "plot_tree() (doubleml.utils.doublemlpolicytree method)": [[49, "doubleml.utils.DoubleMLPolicyTree.plot_tree", false]], "policy_tree() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.policy_tree", false]], "predict() (doubleml.utils.dmldummyclassifier method)": [[46, "doubleml.utils.DMLDummyClassifier.predict", false]], "predict() (doubleml.utils.dmldummyregressor method)": [[47, "doubleml.utils.DMLDummyRegressor.predict", false]], "predict() (doubleml.utils.doublemlpolicytree method)": [[49, "doubleml.utils.DoubleMLPolicyTree.predict", false]], "predict() (doubleml.utils.globalclassifier method)": [[50, "doubleml.utils.GlobalClassifier.predict", false]], "predict() (doubleml.utils.globalregressor method)": [[51, "doubleml.utils.GlobalRegressor.predict", false]], "predict_proba() (doubleml.utils.dmldummyclassifier method)": [[46, "doubleml.utils.DMLDummyClassifier.predict_proba", false]], "predict_proba() (doubleml.utils.globalclassifier method)": [[50, "doubleml.utils.GlobalClassifier.predict_proba", false]], "psprocessor (class in doubleml.utils)": [[52, "doubleml.utils.PSProcessor", false]], "psprocessorconfig (class in doubleml.utils)": [[53, "doubleml.utils.PSProcessorConfig", false]], "rdflex (class in doubleml.rdd)": [[44, "doubleml.rdd.RDFlex", false]], "robust_confset() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.robust_confset", false]], "score() (doubleml.utils.dmldummyclassifier method)": [[46, "doubleml.utils.DMLDummyClassifier.score", false]], "score() (doubleml.utils.dmldummyregressor method)": [[47, "doubleml.utils.DMLDummyRegressor.score", false]], "score() (doubleml.utils.globalclassifier method)": [[50, "doubleml.utils.GlobalClassifier.score", false]], "score() (doubleml.utils.globalregressor method)": [[51, "doubleml.utils.GlobalRegressor.score", false]], "sensitivity_analysis() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.sensitivity_analysis", false]], "sensitivity_benchmark() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.sensitivity_benchmark", false]], "sensitivity_plot() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.sensitivity_plot", false]], "set_fit_request() (doubleml.utils.globalclassifier method)": [[50, "doubleml.utils.GlobalClassifier.set_fit_request", false]], "set_fit_request() (doubleml.utils.globalregressor method)": [[51, "doubleml.utils.GlobalRegressor.set_fit_request", false]], "set_ml_nuisance_params() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.set_ml_nuisance_params", false]], "set_params() (doubleml.utils.dmldummyclassifier method)": [[46, "doubleml.utils.DMLDummyClassifier.set_params", false]], "set_params() (doubleml.utils.dmldummyregressor method)": [[47, "doubleml.utils.DMLDummyRegressor.set_params", false]], "set_params() (doubleml.utils.globalclassifier method)": [[50, "doubleml.utils.GlobalClassifier.set_params", false]], "set_params() (doubleml.utils.globalregressor method)": [[51, "doubleml.utils.GlobalRegressor.set_params", false]], "set_sample_splitting() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.set_sample_splitting", false]], "set_score_request() (doubleml.utils.dmldummyclassifier method)": [[46, "doubleml.utils.DMLDummyClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.dmldummyregressor method)": [[47, "doubleml.utils.DMLDummyRegressor.set_score_request", false]], "set_score_request() (doubleml.utils.globalclassifier method)": [[50, "doubleml.utils.GlobalClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.globalregressor method)": [[51, "doubleml.utils.GlobalRegressor.set_score_request", false]], "set_x_d() (doubleml.data.doublemlclusterdata method)": [[4, "doubleml.data.DoubleMLClusterData.set_x_d", false]], "set_x_d() (doubleml.data.doublemldata method)": [[6, "doubleml.data.DoubleMLData.set_x_d", false]], "set_x_d() (doubleml.data.doublemldiddata method)": [[5, "doubleml.data.DoubleMLDIDData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlpaneldata method)": [[7, "doubleml.data.DoubleMLPanelData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlrdddata method)": [[8, "doubleml.data.DoubleMLRDDData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlssmdata method)": [[9, "doubleml.data.DoubleMLSSMData.set_x_d", false]], "tune() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.tune", false]], "tune() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.tune", false]], "tune() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.tune", false]], "tune() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.tune", false]], "tune() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.tune", false]], "tune() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.tune", false]], "tune() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.tune", false]], "tune() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.tune", false]], "tune() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.tune", false]], "tune() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.tune", false]], "tune() (doubleml.plm.doublemlpliv method)": [[37, "doubleml.plm.DoubleMLPLIV.tune", false]], "tune() (doubleml.plm.doublemlplr method)": [[38, "doubleml.plm.DoubleMLPLR.tune", false]]}, "objects": {"doubleml.data": [[4, 0, 1, "", "DoubleMLClusterData"], [5, 0, 1, "", "DoubleMLDIDData"], [6, 0, 1, "", "DoubleMLData"], [7, 0, 1, "", "DoubleMLPanelData"], [8, 0, 1, "", "DoubleMLRDDData"], [9, 0, 1, "", "DoubleMLSSMData"]], "doubleml.data.DoubleMLClusterData": [[4, 1, 1, "", "from_arrays"], [4, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLDIDData": [[5, 1, 1, "", "from_arrays"], [5, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLData": [[6, 1, 1, "", "from_arrays"], [6, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLPanelData": [[7, 1, 1, "", "from_arrays"], [7, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLRDDData": [[8, 1, 1, "", "from_arrays"], [8, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLSSMData": [[9, 1, 1, "", "from_arrays"], [9, 1, 1, "", "set_x_d"]], "doubleml.datasets": [[10, 2, 1, "", "fetch_401K"], [11, 2, 1, "", "fetch_bonus"]], "doubleml.did": [[12, 0, 1, "", "DoubleMLDID"], [13, 0, 1, "", "DoubleMLDIDAggregation"], [14, 0, 1, "", "DoubleMLDIDBinary"], [15, 0, 1, "", "DoubleMLDIDCS"], [16, 0, 1, "", "DoubleMLDIDMulti"]], "doubleml.did.DoubleMLDID": [[12, 1, 1, "", "bootstrap"], [12, 1, 1, "", "confint"], [12, 1, 1, "", "construct_framework"], [12, 1, 1, "", "draw_sample_splitting"], [12, 1, 1, "", "evaluate_learners"], [12, 1, 1, "", "fit"], [12, 1, 1, "", "get_params"], [12, 1, 1, "", "p_adjust"], [12, 1, 1, "", "sensitivity_analysis"], [12, 1, 1, "", "sensitivity_benchmark"], [12, 1, 1, "", "sensitivity_plot"], [12, 1, 1, "", "set_ml_nuisance_params"], [12, 1, 1, "", "set_sample_splitting"], [12, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDAggregation": [[13, 1, 1, "", "plot_effects"]], "doubleml.did.DoubleMLDIDBinary": [[14, 1, 1, "", "bootstrap"], [14, 1, 1, "", "confint"], [14, 1, 1, "", "construct_framework"], [14, 1, 1, "", "draw_sample_splitting"], [14, 1, 1, "", "evaluate_learners"], [14, 1, 1, "", "fit"], [14, 1, 1, "", "get_params"], [14, 1, 1, "", "p_adjust"], [14, 1, 1, "", "sensitivity_analysis"], [14, 1, 1, "", "sensitivity_benchmark"], [14, 1, 1, "", "sensitivity_plot"], [14, 1, 1, "", "set_ml_nuisance_params"], [14, 1, 1, "", "set_sample_splitting"], [14, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDCS": [[15, 1, 1, "", "bootstrap"], [15, 1, 1, "", "confint"], [15, 1, 1, "", "construct_framework"], [15, 1, 1, "", "draw_sample_splitting"], [15, 1, 1, "", "evaluate_learners"], [15, 1, 1, "", "fit"], [15, 1, 1, "", "get_params"], [15, 1, 1, "", "p_adjust"], [15, 1, 1, "", "sensitivity_analysis"], [15, 1, 1, "", "sensitivity_benchmark"], [15, 1, 1, "", "sensitivity_plot"], [15, 1, 1, "", "set_ml_nuisance_params"], [15, 1, 1, "", "set_sample_splitting"], [15, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDMulti": [[16, 1, 1, "", "aggregate"], [16, 1, 1, "", "bootstrap"], [16, 1, 1, "", "confint"], [16, 1, 1, "", "fit"], [16, 1, 1, "", "p_adjust"], [16, 1, 1, "", "plot_effects"], [16, 1, 1, "", "sensitivity_analysis"], [16, 1, 1, "", "sensitivity_benchmark"], [16, 1, 1, "", "sensitivity_plot"]], "doubleml.did.datasets": [[17, 2, 1, "", "make_did_CS2021"], [18, 2, 1, "", "make_did_SZ2020"], [19, 2, 1, "", "make_did_cs_CS2021"]], "doubleml.double_ml_score_mixins": [[20, 0, 1, "", "LinearScoreMixin"], [21, 0, 1, "", "NonLinearScoreMixin"]], "doubleml.irm": [[22, 0, 1, "", "DoubleMLAPO"], [23, 0, 1, "", "DoubleMLAPOS"], [24, 0, 1, "", "DoubleMLCVAR"], [25, 0, 1, "", "DoubleMLIIVM"], [26, 0, 1, "", "DoubleMLIRM"], [27, 0, 1, "", "DoubleMLLPQ"], [28, 0, 1, "", "DoubleMLPQ"], [29, 0, 1, "", "DoubleMLQTE"], [30, 0, 1, "", "DoubleMLSSM"]], "doubleml.irm.DoubleMLAPO": [[22, 1, 1, "", "bootstrap"], [22, 1, 1, "", "capo"], [22, 1, 1, "", "confint"], [22, 1, 1, "", "construct_framework"], [22, 1, 1, "", "draw_sample_splitting"], [22, 1, 1, "", "evaluate_learners"], [22, 1, 1, "", "fit"], [22, 1, 1, "", "gapo"], [22, 1, 1, "", "get_params"], [22, 1, 1, "", "p_adjust"], [22, 1, 1, "", "sensitivity_analysis"], [22, 1, 1, "", "sensitivity_benchmark"], [22, 1, 1, "", "sensitivity_plot"], [22, 1, 1, "", "set_ml_nuisance_params"], [22, 1, 1, "", "set_sample_splitting"], [22, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLAPOS": [[23, 1, 1, "", "bootstrap"], [23, 1, 1, "", "causal_contrast"], [23, 1, 1, "", "confint"], [23, 1, 1, "", "draw_sample_splitting"], [23, 1, 1, "", "fit"], [23, 1, 1, "", "sensitivity_analysis"], [23, 1, 1, "", "sensitivity_benchmark"], [23, 1, 1, "", "sensitivity_plot"], [23, 1, 1, "", "set_sample_splitting"]], "doubleml.irm.DoubleMLCVAR": [[24, 1, 1, "", "bootstrap"], [24, 1, 1, "", "confint"], [24, 1, 1, "", "construct_framework"], [24, 1, 1, "", "draw_sample_splitting"], [24, 1, 1, "", "evaluate_learners"], [24, 1, 1, "", "fit"], [24, 1, 1, "", "get_params"], [24, 1, 1, "", "p_adjust"], [24, 1, 1, "", "sensitivity_analysis"], [24, 1, 1, "", "sensitivity_benchmark"], [24, 1, 1, "", "sensitivity_plot"], [24, 1, 1, "", "set_ml_nuisance_params"], [24, 1, 1, "", "set_sample_splitting"], [24, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLIIVM": [[25, 1, 1, "", "bootstrap"], [25, 1, 1, "", "confint"], [25, 1, 1, "", "construct_framework"], [25, 1, 1, "", "draw_sample_splitting"], [25, 1, 1, "", "evaluate_learners"], [25, 1, 1, "", "fit"], [25, 1, 1, "", "get_params"], [25, 1, 1, "", "p_adjust"], [25, 1, 1, "", "robust_confset"], [25, 1, 1, "", "sensitivity_analysis"], [25, 1, 1, "", "sensitivity_benchmark"], [25, 1, 1, "", "sensitivity_plot"], [25, 1, 1, "", "set_ml_nuisance_params"], [25, 1, 1, "", "set_sample_splitting"], [25, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLIRM": [[26, 1, 1, "", "bootstrap"], [26, 1, 1, "", "cate"], [26, 1, 1, "", "confint"], [26, 1, 1, "", "construct_framework"], [26, 1, 1, "", "draw_sample_splitting"], [26, 1, 1, "", "evaluate_learners"], [26, 1, 1, "", "fit"], [26, 1, 1, "", "gate"], [26, 1, 1, "", "get_params"], [26, 1, 1, "", "p_adjust"], [26, 1, 1, "", "policy_tree"], [26, 1, 1, "", "sensitivity_analysis"], [26, 1, 1, "", "sensitivity_benchmark"], [26, 1, 1, "", "sensitivity_plot"], [26, 1, 1, "", "set_ml_nuisance_params"], [26, 1, 1, "", "set_sample_splitting"], [26, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLLPQ": [[27, 1, 1, "", "bootstrap"], [27, 1, 1, "", "confint"], [27, 1, 1, "", "construct_framework"], [27, 1, 1, "", "draw_sample_splitting"], [27, 1, 1, "", "evaluate_learners"], [27, 1, 1, "", "fit"], [27, 1, 1, "", "get_params"], [27, 1, 1, "", "p_adjust"], [27, 1, 1, "", "sensitivity_analysis"], [27, 1, 1, "", "sensitivity_benchmark"], [27, 1, 1, "", "sensitivity_plot"], [27, 1, 1, "", "set_ml_nuisance_params"], [27, 1, 1, "", "set_sample_splitting"], [27, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLPQ": [[28, 1, 1, "", "bootstrap"], [28, 1, 1, "", "confint"], [28, 1, 1, "", "construct_framework"], [28, 1, 1, "", "draw_sample_splitting"], [28, 1, 1, "", "evaluate_learners"], [28, 1, 1, "", "fit"], [28, 1, 1, "", "get_params"], [28, 1, 1, "", "p_adjust"], [28, 1, 1, "", "sensitivity_analysis"], [28, 1, 1, "", "sensitivity_benchmark"], [28, 1, 1, "", "sensitivity_plot"], [28, 1, 1, "", "set_ml_nuisance_params"], [28, 1, 1, "", "set_sample_splitting"], [28, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLQTE": [[29, 1, 1, "", "bootstrap"], [29, 1, 1, "", "confint"], [29, 1, 1, "", "draw_sample_splitting"], [29, 1, 1, "", "fit"], [29, 1, 1, "", "p_adjust"], [29, 1, 1, "", "set_sample_splitting"]], "doubleml.irm.DoubleMLSSM": [[30, 1, 1, "", "bootstrap"], [30, 1, 1, "", "confint"], [30, 1, 1, "", "construct_framework"], [30, 1, 1, "", "draw_sample_splitting"], [30, 1, 1, "", "evaluate_learners"], [30, 1, 1, "", "fit"], [30, 1, 1, "", "get_params"], [30, 1, 1, "", "p_adjust"], [30, 1, 1, "", "sensitivity_analysis"], [30, 1, 1, "", "sensitivity_benchmark"], [30, 1, 1, "", "sensitivity_plot"], [30, 1, 1, "", "set_ml_nuisance_params"], [30, 1, 1, "", "set_sample_splitting"], [30, 1, 1, "", "tune"]], "doubleml.irm.datasets": [[31, 2, 1, "", "make_confounded_irm_data"], [32, 2, 1, "", "make_heterogeneous_data"], [33, 2, 1, "", "make_iivm_data"], [34, 2, 1, "", "make_irm_data"], [35, 2, 1, "", "make_irm_data_discrete_treatments"], [36, 2, 1, "", "make_ssm_data"]], "doubleml.plm": [[37, 0, 1, "", "DoubleMLPLIV"], [38, 0, 1, "", "DoubleMLPLR"]], "doubleml.plm.DoubleMLPLIV": [[37, 1, 1, "", "bootstrap"], [37, 1, 1, "", "confint"], [37, 1, 1, "", "construct_framework"], [37, 1, 1, "", "draw_sample_splitting"], [37, 1, 1, "", "evaluate_learners"], [37, 1, 1, "", "fit"], [37, 1, 1, "", "get_params"], [37, 1, 1, "", "p_adjust"], [37, 1, 1, "", "sensitivity_analysis"], [37, 1, 1, "", "sensitivity_benchmark"], [37, 1, 1, "", "sensitivity_plot"], [37, 1, 1, "", "set_ml_nuisance_params"], [37, 1, 1, "", "set_sample_splitting"], [37, 1, 1, "", "tune"]], "doubleml.plm.DoubleMLPLR": [[38, 1, 1, "", "bootstrap"], [38, 1, 1, "", "cate"], [38, 1, 1, "", "confint"], [38, 1, 1, "", "construct_framework"], [38, 1, 1, "", "draw_sample_splitting"], [38, 1, 1, "", "evaluate_learners"], [38, 1, 1, "", "fit"], [38, 1, 1, "", "gate"], [38, 1, 1, "", "get_params"], [38, 1, 1, "", "p_adjust"], [38, 1, 1, "", "sensitivity_analysis"], [38, 1, 1, "", "sensitivity_benchmark"], [38, 1, 1, "", "sensitivity_plot"], [38, 1, 1, "", "set_ml_nuisance_params"], [38, 1, 1, "", "set_sample_splitting"], [38, 1, 1, "", "tune"]], "doubleml.plm.datasets": [[39, 2, 1, "", "make_confounded_plr_data"], [40, 2, 1, "", "make_pliv_CHS2015"], [41, 2, 1, "", "make_pliv_multiway_cluster_CKMS2021"], [42, 2, 1, "", "make_plr_CCDDHNR2018"], [43, 2, 1, "", "make_plr_turrell2018"]], "doubleml.rdd": [[44, 0, 1, "", "RDFlex"]], "doubleml.rdd.RDFlex": [[44, 1, 1, "", "aggregate_over_splits"], [44, 1, 1, "", "confint"], [44, 1, 1, "", "fit"]], "doubleml.rdd.datasets": [[45, 2, 1, "", "make_simple_rdd_data"]], "doubleml.utils": [[46, 0, 1, "", "DMLDummyClassifier"], [47, 0, 1, "", "DMLDummyRegressor"], [48, 0, 1, "", "DoubleMLBLP"], [49, 0, 1, "", "DoubleMLPolicyTree"], [50, 0, 1, "", "GlobalClassifier"], [51, 0, 1, "", "GlobalRegressor"], [52, 0, 1, "", "PSProcessor"], [53, 0, 1, "", "PSProcessorConfig"], [54, 2, 1, "", "gain_statistics"]], "doubleml.utils.DMLDummyClassifier": [[46, 1, 1, "", "fit"], [46, 1, 1, "", "get_metadata_routing"], [46, 1, 1, "", "get_params"], [46, 1, 1, "", "predict"], [46, 1, 1, "", "predict_proba"], [46, 1, 1, "", "score"], [46, 1, 1, "", "set_params"], [46, 1, 1, "", "set_score_request"]], "doubleml.utils.DMLDummyRegressor": [[47, 1, 1, "", "fit"], [47, 1, 1, "", "get_metadata_routing"], [47, 1, 1, "", "get_params"], [47, 1, 1, "", "predict"], [47, 1, 1, "", "score"], [47, 1, 1, "", "set_params"], [47, 1, 1, "", "set_score_request"]], "doubleml.utils.DoubleMLBLP": [[48, 1, 1, "", "confint"], [48, 1, 1, "", "fit"]], "doubleml.utils.DoubleMLPolicyTree": [[49, 1, 1, "", "fit"], [49, 1, 1, "", "plot_tree"], [49, 1, 1, "", "predict"]], "doubleml.utils.GlobalClassifier": [[50, 1, 1, "", "fit"], [50, 1, 1, "", "get_metadata_routing"], [50, 1, 1, "", "get_params"], [50, 1, 1, "", "predict"], [50, 1, 1, "", "predict_proba"], [50, 1, 1, "", "score"], [50, 1, 1, "", "set_fit_request"], [50, 1, 1, "", "set_params"], [50, 1, 1, "", "set_score_request"]], "doubleml.utils.GlobalRegressor": [[51, 1, 1, "", "fit"], [51, 1, 1, "", "get_metadata_routing"], [51, 1, 1, "", "get_params"], [51, 1, 1, "", "predict"], [51, 1, 1, "", "score"], [51, 1, 1, "", "set_fit_request"], [51, 1, 1, "", "set_params"], [51, 1, 1, "", "set_score_request"]], "doubleml.utils.PSProcessor": [[52, 1, 1, "", "adjust_ps"], [52, 1, 1, "", "from_config"]]}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function"}, "terms": {"": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 44, 48, 50, 51, 57, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 72, 73, 79, 80, 81, 82, 84, 86, 87, 88, 92, 94, 95, 96, 97, 99, 101, 102, 103, 104, 107, 108, 110, 111, 113, 114, 121, 123, 136, 137, 138, 139, 140, 150, 152, 153, 154, 155], "0": [4, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 51, 52, 53, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 147, 151, 152, 154], "00": [66, 69, 80, 81, 83, 87, 88, 122], "000": [89, 93, 138, 155], "00000": 83, "000000": [66, 68, 70, 73, 83, 87, 88, 99, 104, 106, 152], "0000000": 138, "0000000000000010000100": [62, 99, 104, 152], "000000e": [66, 69, 80, 81, 83, 87, 88], "000006": 73, "000017": 91, "000025": 86, "000034": 87, "000039": 86, "000056": 106, "000059": 67, "000064": 74, "000067": 86, "000076": 108, "000091": 86, "0001": [70, 87], "000113": 67, "000128": 122, "000145": 67, "000154": 67, "0001770168": 122, "0002": 67, "000219": 28, "000242": 29, "000320": 91, "00032016": 91, "000330": 67, "000340": 69, "000341": 86, "000361": 69, "000382": [66, 67], "000392": 69, "000407": 66, "000421": 66, "000426": 66, "000432": 66, "000442": 86, "000466": 69, "00047580260495": 57, "000476": 69, "000488": 86, "000494": 82, "0005": 70, "000522": 86, "0005231": 122, "000528": 67, "000541": 69, "000546": 66, "000565": 77, "000578": 69, "000582": 67, "000587": 66, "0005a80b528f": 62, "000622": 69, "000638": 66, "000670": 86, "000743": 94, "0007916619": 122, "000846": 67, "000892": 69, "000897": 67, "000915799": 138, "0009157990": 138, "000916": [100, 104], "000943": [76, 77], "000984": 67, "001": [17, 19, 52, 57, 59, 60, 61, 62, 63, 75, 107, 108, 122, 123, 138, 152, 155], "0010": 67, "001013": 66, "001022": 69, "001024": 67, "001049": [108, 113], "001051": 86, "001145": 88, "001152337": 122, "00133": 62, "001379": 69, "00138944": [97, 123], "001403": 92, "001446": 67, "001471": 83, "001472": 67, "001474": 67, "001494": [106, 107, 108], "0015": 67, "0016": [61, 87], "001603": [108, 113], "001698": 83, "001756127": 122, "001758": 76, "0018": [61, 87], "0019": 70, "001907": 83, "002037": 67, "002037900301454": 85, "002050": 68, "002149e": 77, "002169338": 138, "0021693380": 138, "0021693381": 138, "002225": 76, "002290": 65, "0023": 59, "002424": 67, "002436": 82, "002521": 67, "0026": 70, "002601": [108, 109], "002728e": 77, "002779": 94, "0028": [59, 61, 67, 87], "002821": 95, "00282133350419121": 95, "002873e": 76, "002906787": 122, "00290774": [108, 111], "002983": 86, "003": [18, 31, 39, 93, 122], "003045": 83, "003051": 76, "003074": 108, "003113": 66, "003134": 91, "003220": 73, "003259": 67, "003328": 91, "003333": 67, "003390": 67, "0034": [67, 79], "003404": 73, "003415": 73, "003427": 86, "003528e": 76, "003779": 82, "003801": 67, "003836": 91, "0039": 67, "003924": 82, "004004": 66, "00409412": [97, 123], "004143": 67, "004192": 76, "0042": [61, 87], "004214": 66, "004220": 67, "004253": 73, "004269": 77, "004392": 82, "004420": 67, "004526": 73, "004542": 83, "004579577": 122, "0046": 67, "004617902": 122, "0047": [61, 87], "004846": 95, "004963": 67, "00518448": [108, 113], "005339": [76, 77], "005670": 67, "005698": 67, "005857": 86, "005e": 108, "006055": 73, "0060715124549546": 85, "0065": 67, "006593": 106, "0066": 67, "006922": 70, "006958": [76, 77], "0072": 122, "0072075356": 122, "00728": 152, "0073": 70, "007421": 106, "007448": 69, "00757042": 122, "007684": 66, "00778625": 122, "007789e": 85, "007873885": 122, "007947": 66, "008": [89, 95], "008023": 88, "008056": 67, "008223": [76, 77], "008307": 69, "008387": 66, "008487": 70, "008674": 16, "008825": 78, "008825189994473": 78, "008883698": 123, "00888458890362062": 97, "008884589": 97, "008941": 76, "008dbd": 89, "008e80": 89, "009": [89, 95], "009080": 69, "009122": 91, "009171": [108, 111], "0093": 67, "009307": 67, "009312": 66, "009329847": 123, "009428": 78, "00944171905420782": 95, "009464": 69, "00950122695463054": 97, "009501226954630540": 97, "009501227": 97, "009516": 69, "009535": 66, "009562": 89, "009570": 69, "009583": 69, "009645422": 60, "009656": 91, "00972": 70, "009727": [108, 109], "009790": 88, "009986": 91, "01": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 49, 52, 53, 57, 60, 61, 62, 63, 66, 69, 76, 77, 83, 87, 88, 89, 90, 91, 92, 107, 108, 111, 113, 122, 123, 138, 152, 155], "010": 89, "010045": 85, "010213": 94, "010269": 86, "010364": 106, "010400": 89, "010450": 60, "0105": 67, "010502": 69, "010515": 68, "010758": 66, "010834": 67, "01086958": 122, "01091843": [108, 111], "010940": 86, "011131": 91, "011161": 69, "0112": 59, "011204": 83, "01128": 70, "011285": 76, "011323": [108, 109], "011420": 67, "011598": 91, "011758": 66, "0118095": 60, "011816": 67, "011823": 94, "011988e": 91, "012": 89, "012051": 89, "012140": 69, "01219": 62, "0124105481660435": 85, "012711": 67, "01274": 95, "012831": 95, "013028": 66, "013034": 95, "013088": 25, "013175": 69, "013280": 67, "013350": 76, "013355": 89, "013450": 83, "013484": 66, "01351638": 60, "013593": 94, "013677": 92, "013848": 76, "013870": 76, "013976": 77, "01398951": 60, "013990": 138, "014": 92, "01403089": 60, "014080": [76, 77], "014175": 67, "014193": 67, "014431": 88, "014432": 65, "014525": 72, "014637": 86, "014681": [66, 94], "014721": 88, "014738": 76, "015": 62, "0150": 67, "015024": 67, "015035": 69, "015037": 67, "015038": [69, 78], "015565": 91, "015692": 67, "015698": 91, "015704": 67, "01574297": 91, "015743": 91, "016154": 86, "016200": [76, 77], "016313": 67, "016365": 67, "0164": 67, "01643": 153, "016497": 67, "016518": 88, "016589": 66, "016598": 76, "016627": 66, "016646": 66, "0167": 67, "016702": 80, "016706": 67, "016709": 67, "016875": 67, "017": 62, "017009": 67, "017185": [67, 88], "017393e": 138, "01760337": 122, "017605": 76, "017660": 83, "01772": 140, "0178": 67, "017800092": 138, "0178000920": 138, "017840": 67, "0179": 67, "017901": 67, "017908": 67, "018": 62, "018092": 106, "018148": 91, "018327": 67, "018355": 67, "0184989": [108, 111], "018542": 67, "0187512020118494": 85, "01903": [62, 107, 150, 152], "019100889": 122, "01916030e": 122, "019163": 67, "01925597": 60, "019439633": 138, "0194396330": 138, "0194396331": 138, "019596": 78, "0196": 67, "019643": 67, "019660": 29, "019699": 66, "019752": 67, "019821": 69, "019899": 67, "01990373": 96, "019997": 67, "02": [16, 66, 69, 76, 77, 87, 88, 91, 108, 111, 113, 122], "020001": 67, "020123": 72, "020156": 67, "02016117": 152, "020166": 91, "020271": 86, "020272": 83, "020281": 69, "0203": 67, "020312": 67, "020360838": 138, "0203608380": 138, "0203608381": 138, "02042944": 122, "02052929": [97, 123], "02079162e": 122, "020808": 67, "02092": 152, "021063": 66, "021269": [80, 81], "02163217": 60, "021690": 81, "021814": 66, "021821": 67, "021866": 90, "02187543": 122, "021926": 78, "02200763": 122, "022258": 83, "022282": 67, "022289": 67, "02247976": 60, "022511": 88, "022532": 67, "02260304": [108, 113], "022629": 106, "022749": 106, "022768": 70, "022783": 94, "022820": 76, "022914": 67, "022915": [69, 86], "022950": 88, "023020e": [87, 88], "023147": 68, "023187": 69, "023256": 91, "0233": 67, "023445": 67, "023563": 138, "023728": 77, "02379149": 122, "023839": 89, "0240": 67, "024250": 76, "024266": 83, "024355": 65, "024364": 139, "024401": [80, 81], "024486": 67, "024604": 86, "02461520": 122, "02467": 93, "024782": 91, "024926": 65, "025": [76, 77, 80, 81, 83, 89], "025077": 138, "025180": 69, "0253": 62, "025387": 67, "0254": 67, "025407": 85, "025443": 70, "025446": 67, "025670": 66, "025674": 66, "0257": 59, "025793": 67, "025813114": 138, "0258131140": 138, "02584": 62, "025841": 83, "025945": 66, "025964": 83, "0260": 67, "026152": 66, "02617451": 69, "026292": 72, "026560": 69, "026579": 66, "026609": 67, "026723": 78, "026759": 69, "026936": 67, "026966": 83, "02699695": 84, "027": 89, "027419e": 77, "027651e": 69, "027657": 66, "02791": 70, "027920": 67, "027932": 67, "027981": 67, "028": 89, "0281": 62, "028146": 85, "028329": 67, "028509": 69, "028520": [76, 77], "028609": 67, "028630": [108, 109], "028681": 106, "028731": 106, "028868": 88, "028919": 66, "028962": 67, "02900983": 91, "029010": 91, "029065": 67, "029209": 155, "029364": [139, 145], "0294": 67, "029550": 67, "029696": 77, "029792": 66, "029831": 91, "029910e": [87, 88], "029993": 68, "02e": 61, "03": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 63, 66, 69, 73, 76, 77, 78, 82, 83, 87, 88, 89, 91, 92, 94, 95, 108, 109, 111, 113, 122, 139, 145, 155], "030059": 106, "0301": 62, "030123": [108, 111], "03018": 27, "030346": 152, "0304": 67, "03045": 63, "030457": 67, "030497": 77, "0305": 67, "030672": 67, "0307": [62, 72], "030746": 67, "030803": 68, "030817": 77, "030934": 91, "030962": 91, "031": 57, "031029": 69, "031075": 85, "0311": 67, "03113": 96, "031134": 107, "031144": 67, "0312": 67, "031269": 70, "031298": 67, "031323": 85, "031460e": 69, "031491": 85, "031639": 91, "031692": 85, "031712": 88, "03191": 153, "032165": 66, "03220": 154, "0323": 59, "032398": 89, "0324": 67, "03244552": 107, "0325": 152, "032664": 76, "032675": [108, 111], "032715": 67, "032802": 67, "032945": 69, "032953": 94, "033127": 67, "0333": 67, "033344": 67, "033353": 26, "033430": 67, "033661": 85, "033671": 67, "033727": 69, "033779": 85, "033946": [80, 81], "034075": 67, "03411": 152, "034301": 68, "034371115": 122, "034377": 67, "03438": 63, "0344": 67, "034609": 77, "034666": 77, "034690": 78, "034812763": 138, "0348127630": 138, "0348127631": 138, "034846": 87, "03489": [41, 60, 86], "0349": 67, "034990": 67, "035091": 68, "035119185": 138, "0351191850": 138, "0351191851": 138, "0352": 67, "035265": 78, "03534806": [108, 111], "03536": 152, "03538": 62, "03539": 62, "035391": 70, "0354": 62, "035411": 152, "03545": 62, "035545": 70, "035572": 70, "035579": 77, "0356": 67, "035651": 66, "035689": [108, 113], "035719": 67, "035730": 91, "035735": 77, "03574": 70, "035762": 91, "0359": 62, "036129015": 138, "0361290150": 138, "0361290151": 138, "036143": 91, "036147": 91, "036197": 67, "036217": 67, "036240": 73, "036275": 67, "036298": 66, "0363": 67, "036381": 76, "036729": 86, "036739e": 66, "0368": 59, "037008": [80, 81], "037114": 83, "037271": 76, "0373": 72, "0374": 62, "037509": 96, "037529": 83, "037577": [108, 109], "037747": [76, 77], "03783666": 91, "037837": 91, "037855e": 77, "038": 89, "038006": 77, "03802": 67, "03803676": 122, "038066": 66, "038085": [66, 77], "03809559": 122, "038103": 83, "03832792": 69, "038383": 67, "038386": 66, "03872": 68, "038812": 90, "038831": 85, "038847": 76, "039043": 67, "0391": 67, "039141": 73, "039154": 88, "03917696": [123, 138], "039186": 67, "03920960e": 122, "039302e": 78, "039401": 69, "039450": 68, "0395": 67, "039522": 69, "03966": 89, "039661": 83, "039709": 66, "039813": 89, "039895": 83, "04": [16, 39, 61, 66, 69, 73, 76, 77, 87, 88, 91, 92, 94, 108, 109, 111, 113, 122, 155], "040010": 83, "040067": 77, "040112": 138, "040139": [76, 77], "040278": 67, "040509": 68, "040533": [38, 123, 138], "04053339": 138, "040676": 67, "040784": 73, "040793": 67, "040813": 76, "040912": 77, "041091": 76, "0411": 67, "041147": 78, "0412": 67, "041256": 67, "041262": 85, "041275e": 76, "041276": 66, "041284": 78, "041381": 76, "041387": 78, "041491e": 78, "04165": 108, "0418": 59, "041831": 78, "041861": 66, "041985": 67, "041996": 69, "042": 89, "042012": 67, "042034": 95, "042060": 85, "0421": 67, "042240": 67, "042265": 78, "0425": 107, "042517": 77, "042583": 88, "042617": 69, "042775": 67, "0428": [67, 96], "042804": 83, "042828": 67, "042844e": 91, "042854": 76, "043002": 77, "043057": 67, "043082": [108, 113], "0433": 59, "043313": 67, "0434e374": 62, "043760": 66, "043769": 72, "043806": 67, "04387": 107, "043996": 77, "044": 89, "044031": 66, "044113": 78, "04415": 62, "044176": 83, "044198": 67, "044239": 83, "04424": 62, "044294": 67, "044303": 66, "044399": 77, "044447": [100, 104], "04444978": 138, "044449780": 138, "0445": 107, "044570": 67, "04465": 60, "04486": 152, "04487585": [139, 145], "04491": 108, "044929": 83, "04497975": [139, 145], "04501612": 138, "04502": [107, 123, 138], "045144": 86, "045172": 83, "04527644": 122, "045379": 152, "045422": 67, "04552": 86, "045553": 78, "045612": 68, "045624": 65, "04563": 107, "045754": 91, "04586": 107, "045932": 91, "045984": 83, "045991": 76, "045993": 107, "046088": 91, "04608822": 91, "046238": 76, "04625": 107, "046451": 83, "046479": 67, "046507": [108, 113], "046525": 106, "046527": 78, "0466028": 60, "046611": 67, "046642": 68, "046728": 94, "046788": 88, "04682310e": 122, "046844": 106, "046922": 107, "047123": 77, "047239": 83, "047288": 106, "0473": 76, "047390": 77, "047562": 68, "0477": 72, "047873": 83, "047954": 86, "048090": 72, "048209": 69, "048308": 81, "048476": 83, "048699": 96, "048723": 107, "04873094": 122, "048865": 69, "049573": [108, 113], "049729": 77, "049800": 67, "05": [16, 53, 57, 59, 60, 61, 62, 63, 66, 69, 76, 77, 78, 79, 86, 87, 88, 89, 91, 92, 95, 107, 108, 109, 111, 113, 122, 123, 138, 152, 155], "05039": 94, "050409": 66, "050863": 69, "050975": 67, "051": 62, "051169": 66, "051186": 91, "051287": 68, "051651": 92, "051712": 90, "051870e": 78, "052": 92, "052023": 83, "0520233166790431": 85, "0522": 67, "052222": 68, "052304": 67, "052318": 69, "052488": 81, "052502": 91, "05258823": 122, "052745": 78, "052811": 88, "053": [62, 108], "053046": 67, "053188": 76, "0533": 59, "053331": 78, "053370": 67, "053389": 138, "053541": 91, "053558": 78, "053659": 64, "053815": 68, "054": [62, 92], "054064": 85, "054068": 86, "054092724": 122, "054162": 86, "054219": 69, "054330": 77, "054339": 138, "054370": 78, "054529": 138, "054718": 67, "054740": 66, "054771e": 91, "05490695": 69, "055165": 94, "055280e": 76, "055410": 77, "055439": 88, "055493": 95, "055680": 138, "055744": 67, "055787": 67, "056": 92, "056148": 66, "056192": 68, "0562": 67, "056389": 67, "056499": 81, "056915": 83, "057371": 88, "057400": 67, "057535": 76, "057593": 69, "0576": [61, 87], "057608": 76, "057762": 91, "057873": 68, "057904": 68, "057962": 78, "058042": 138, "058175": 91, "058202": 69, "058375": 73, "058383": 76, "058463": 91, "058467": 77, "058508": 96, "058510": 69, "058512": 67, "058538": 25, "058676e": 76, "0587": 67, "058742": 68, "058884": 67, "058892": 76, "05891": 108, "059": 57, "0590": 59, "05903051": 122, "059058": 77, "059384": 91, "059390": 67, "059630": 65, "059685": 91, "0599": 67, "059930": 67, "059936": 76, "059976e": 77, "06": [18, 31, 39, 66, 69, 73, 76, 77, 78, 87, 88, 91, 107], "060016": 73, "06008533": 108, "060201": 91, "060212": [87, 88], "0603268864456956": 85, "060330": 67, "060845": 138, "061": 89, "061078": 67, "0611": 59, "06111111": 62, "0614448384": 122, "0615": 59, "061523": 67, "062": [92, 108], "062293": 77, "062402": 66, "062414": 88, "06269": 93, "06270135": 84, "062774": 76, "0628": 59, "062964": 138, "063017": 73, "063067": 76, "063098": 76, "063191": 68, "0632": 59, "06329768": [108, 111], "063312": 88, "063327": 83, "063428e": 88, "0635": 59, "063590": 91, "06359032": 122, "0636": 59, "063641": 77, "0638": 59, "06389": 108, "063896": 67, "06397789": 91, "063978": 91, "063979": 76, "0640": 59, "064053": 68, "064164": 88, "06428": 87, "064280": 87, "064289": 69, "0645": 59, "064564": 68, "0646222": 61, "064695": 68, "0647": 59, "064767": 66, "06487253": 122, "0649": 59, "065": 95, "065229": 66, "065277": 106, "0653": 59, "065304": 68, "065306702": 122, "065356": [80, 81], "065387": 68, "0654": 59, "06546364": 122, "0655": 59, "065535": 30, "065725": 78, "0659": 59, "065969": 108, "065976": 83, "066": 89, "066039": 77, "066042": 68, "0661": 67, "066162": 67, "0662": [59, 67], "066295": 83, "06632532": 122, "066397": 67, "06642119": 69, "066464": 94, "066465": 68, "066478": 85, "066559": 67, "066573883": 122, "0669": 59, "06692492": 122, "06701": 68, "0671": 59, "067102": 66, "067212": 83, "0673": 59, "067436": 85, "0675": 59, "067528": 95, "067639": 88, "06766766": 66, "067721": 138, "067789": 68, "067975": 69, "068005": 69, "06811": 108, "068148": 91, "068221": 90, "06827": 94, "068334": 68, "068340": 76, "068377": 88, "068569": 66, "068613": 66, "068685": 68, "068700": 106, "068881": 68, "068934": 73, "06895837": 60, "069144": 85, "069443": 73, "0695854": 60, "069589": 83, "069634": 76, "07": [57, 69, 76, 77, 88, 91, 92, 95, 106, 122], "070020": 91, "070080": 67, "0702127": 60, "070393": 88, "0704": [59, 67], "070427": 76, "070433": 83, "070497": 95, "070594": 69, "0707": 59, "070797": 88, "07085301": 108, "070884": 91, "071": 89, "0711": 59, "071279": 138, "07136": [60, 86], "071543e": 78, "0716": 59, "07168291": 60, "071731": 88, "071771e": 76, "071777": 107, "071782": 29, "0718": 72, "0719": 59, "0719410822": 122, "071947": 69, "071983": 67, "07202564": [80, 81], "072069": 26, "07215001": 122, "072153": 76, "07220707": 122, "07222222": 62, "072293": 90, "07229774": [108, 113], "072364": 68, "072516": 83, "072583e": 77, "072605": 73, "0727": 108, "072826": 69, "0729199942": 122, "073": 92, "073010": 66, "073013": 91, "073207": 86, "073261": 67, "073293e": 76, "073384": 83, "073390": 67, "073411": 69, "073447": [108, 113], "07347676": 60, "07350015": [36, 41, 60, 86], "073518": 78, "0735183279373635": 78, "073520": 78, "0736": 59, "073624": 67, "07366": [62, 107], "073840": 67, "073900": 77, "074271": 66, "0743": 59, "074304": 138, "07436521": 122, "074400": 67, "074426": 91, "07456127": 60, "074700e": 88, "07479278": 94, "074927": 73, "075261": 65, "075384": 91, "07538443": 91, "07544271e": 122, "07561": 152, "07564554e": 122, "0758": 95, "075809": 73, "075869": 107, "076019": 87, "076119": 88, "076150": 138, "076179312": 138, "0761793120": 138, "076187e": 76, "0762": 72, "076279": 76, "076322": 91, "076347": 78, "076448": 69, "0765": 62, "076653": [108, 113], "076684": 152, "076720": 66, "076770": 66, "07685043": 122, "07689": 62, "07691847": 122, "076953": [80, 81], "076971": 70, "077": 89, "077090": 85, "077214": 67, "07727773e": 122, "077319": 91, "077391": 67, "077502": [139, 145], "077527e": 88, "077543": 76, "077592": 83, "077702": 73, "0777777777777778": 107, "07777778": [62, 107], "07781396": 122, "077883": 91, "07788588": [108, 113], "077920": [100, 104], "07796": 108, "078090": 138, "078095": 16, "078207": 70, "07828372": 138, "078382": 77, "078384e": 76, "078426": 106, "078474": 138, "078810": 91, "079085": 70, "079122828": 122, "07915": 62, "07919896": 122, "07942v3": 153, "079458e": 87, "079524": 69, "07961": 94, "079689": 66, "079761": [100, 104], "07978296": 122, "079878": 67, "08": [66, 78, 88, 91, 95, 108], "080041e": 76, "080121": 88, "08031571": 122, "080509": 67, "080622": 68, "080729": 72, "080854": 88, "08091581": 122, "080947": 70, "080978": 66, "080987": 76, "081": 62, "0810": 67, "081066": 67, "081100": 91, "081230": [76, 77], "081396": 81, "081488": 86, "0815": 67, "08154161": 122, "08181827e": 122, "0820": 59, "082197": 83, "082297": 122, "082397": 67, "082468": 66, "082643": 16, "082787": 69, "082804": 65, "082905": [108, 113], "082973": 86, "083079": 88, "083153": 77, "083251": 66, "083258": 138, "083312": 138, "08333333": 62, "08333617": 122, "0835771416": 60, "0836": 83, "08364": 83, "083706": 95, "083949": 95, "084": 60, "084007": 82, "084049": 67, "084269": 88, "084288": 76, "084337": 108, "084497": 67, "084714": 77, "084719": 69, "084807": 77, "085270": 66, "085279e": 80, "0853505": 60, "085372": 76, "085466": 69, "085566": 78, "085592": 83, "08581494": 122, "086004": 83, "08602774e": 122, "0862": 150, "086264": 78, "086401570476133": 78, "086402": 78, "086559": 68, "08664208": 122, "086679": 107, "08668149": 67, "086826": 80, "087184": 37, "0872": 59, "08739261": 67, "087538": 66, "087566": 85, "087634": 76, "087640": 69, "087826": 77, "087947": 91, "087985": 69, "088019": 76, "088048": 91, "088082": 85, "088282": 81, "088288": [100, 104], "088346e": 76, "088357": 91, "088401": 76, "08848": 107, "088482": 29, "088494": 66, "088792": 85, "088836": 85, "08888889": 62, "088891": 66, "088935": [108, 111], "089045": 66, "0891236611": 122, "089191": 77, "089328": 69, "0894": 59, "08968939": 60, "089912": 68, "089989": 68, "089998": 67, "08e": 61, "09": [37, 69, 76, 77, 78, 87, 88, 91], "09000000000000001": 107, "09015": 59, "090255": 91, "090331": 77, "090363": 90, "091093": 69, "091308": 106, "091391": 138, "091406": 139, "091432": 76, "091587": 69, "0916": 59, "091992": 90, "09201089": 69, "092029": 66, "092150": 67, "092229": 95, "092262": 108, "092365": 138, "092453": 91, "09245337": 91, "092646": 91, "092919": 140, "092925": 66, "0929369228758206": 85, "093043": 91, "09310496": 138, "093153": 91, "093173": 76, "093418": 66, "093470": 69, "0935": 108, "09351167": 108, "093607": 77, "093727": 69, "093740": 138, "093950": 86, "094026": 86, "094118": 91, "094381": 86, "094420": 88, "09444444": 62, "094587": 66, "094595": 72, "094766": 90, "094829": 108, "094883": 76, "094968": 77, "094999": 91, "095": 155, "095094361": 122, "095104": 73, "095540": 66, "095785": 73, "095837": 68, "09603": 150, "096243": 69, "096337": 86, "096418": 73, "096616": 24, "09682314": 108, "096890e": 106, "096915": 95, "096944": 66, "097": 92, "097009": 83, "097140": 80, "097151": 66, "097157": 95, "097208": 90, "097468": 78, "09756": 93, "09779675": 138, "097796750": 138, "098": 61, "098202": 72, "098256": 91, "09830758": 94, "098308": 94, "098319": 91, "098521e": 66, "0986": 59, "098712": 91, "09879814e": 122, "0987985": 69, "098901": 83, "099": 92, "099485": [108, 109], "099493": 67, "099647": 90, "099670": 88, "099731": [76, 77], "09980311": 138, "099862": 66, "09988": 153, "0_": 40, "0ff823b17d45": 62, "0x": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38], "0x1747bdd4520": 70, "0x1747bdd6b90": 70, "0x7f100ef87dd0": 90, "0x7f16eaa1bce0": 95, "0x7f3fcaf2a510": 138, "0x7f3fcaf2bbc0": 138, "0x7f3fcb33c500": 108, "0x7f3fcb392360": 109, "0x7f3fd60a04d0": 108, "0x7f3fd60d14c0": 108, "0x7f3fd6315b50": 108, "0x7f3fd64325a0": 109, "0x7f3fd6458d70": 108, "0x7f3fd7260110": 155, "0x7f3fd79de720": 107, "0x7f3fd79df320": 108, "0x7f3fddcb6f60": 139, "0x7f3fdddee0f0": 107, "0x7f3fde4a9b50": 138, "0x7f3fde4a9eb0": 138, "0x7f3fde4b7ce0": 145, "0x7f3fde56cbf0": 107, "1": [8, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 51, 52, 53, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 117, 118, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154], "10": [10, 11, 12, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 35, 36, 37, 38, 39, 41, 42, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 101, 104, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 139, 145, 150, 152, 153, 154, 155], "100": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 41, 43, 60, 62, 63, 64, 76, 77, 79, 82, 83, 84, 86, 89, 93, 95, 96, 97, 99, 104, 106, 107, 108, 110, 122, 123, 138, 139, 145, 152, 154], "1000": [17, 19, 25, 27, 58, 64, 65, 72, 74, 75, 80, 81, 82, 84, 85, 87, 88, 92, 94, 95, 98, 108], "10000": [57, 65, 69, 76, 77, 87, 91], "100000e": 88, "100010": 69, "100044": 81, "100208": 106, "1003": 72, "100356": 78, "10038": 94, "10039862": [96, 108], "100510": 138, "100664": 76, "1007": 67, "100721": 67, "10074": 88, "10079785": 108, "100807": [76, 77], "100858": 94, "10089588": 91, "100896": 91, "100923": 91, "100_000": 89, "101": [18, 31, 39, 59, 92, 106, 153, 154], "10109": 106, "101140": 66, "10117516": 67, "10126": 88, "10127930": 138, "101279300": 138, "101284": 66, "1015": [61, 87], "1015192026283036374250576171747778808591": 122, "101519202628303637425057617174777880859157914222431343848596570798283849298996121718273539404149515460626668879396100123413162947525355586369737581868995": 122, "10151920262830363742505761717477788085915791422243134384859657079828384929899811212325323343444546566467727688909497123413162947525355586369737581868995": 122, "101519202628303637425057617174777880859157914222431343848596570798283849298998112123253233434445465664677276889094976121718273539404149515460626668879396100": 122, "10151920262830363742505761717477788085918112123253233434445465664677276889094976121718273539404149515460626668879396100123413162947525355586369737581868995": 122, "10157383": 67, "101574e": 69, "1016": [17, 18, 19, 31, 39, 59], "1016010": 61, "1016338581630878": 85, "1017": 93, "1018": 88, "101998": 73, "102": [99, 104, 106, 152, 154], "102058": 69, "102282": 16, "10234": 69, "10235": 88, "1025": 93, "102616": 78, "10277": 88, "102775": 78, "10299": 87, "103": [76, 86, 92, 99, 104, 106, 154], "10307": 138, "1030891095588866": 85, "1031": 88, "103159": 89, "103171": 66, "103179163001313": 85, "103215": 16, "10348": 87, "103497": 91, "103563": 66, "103806": 78, "10396": 87, "104": [61, 87, 106, 154], "1040": 88, "104016": 76, "10406": 88, "1041": 59, "10414": 88, "104341": 69, "104383": 77, "104475": 76, "104486": 69, "10449": 93, "104492": 83, "1045303": 60, "104787": 86, "104935": 69, "104982": 66, "104991": 66, "105": [40, 60, 83, 86, 106, 154], "105227e": 77, "105318": 91, "105341": 66, "1054": 62, "105461": 106, "1055": [59, 93], "105796": 66, "105930": 66, "106": [62, 106, 154], "10607": [70, 99, 104, 152], "106116": 88, "1062524": 122, "10627": 88, "106315": 67, "10637173e": 122, "106385": 138, "1065": [79, 84, 85], "106595": [12, 108, 110], "106715": 82, "107": [62, 67, 95, 106, 154], "107073": 78, "107156": 83, "107211": 68, "107290": 138, "1073": 88, "107433": 67, "107467": [100, 104], "10747": [70, 99, 104, 152], "107746": 91, "107809": 76, "107847": 66, "107851": 69, "107857": 69, "10791": 88, "107935": 91, "108": [106, 150, 153, 154], "1080": [36, 41, 59, 60, 86], "108019": 89, "10824": [70, 99, 104, 152], "108257e": 88, "108259": 73, "10829": 88, "10831": [70, 99, 104, 152], "108373": 122, "108742": 16, "108783": 78, "108783087402629": 78, "10878571": 91, "108786": 91, "108870": [108, 113], "109": [76, 106], "10903": 87, "109069": 138, "109079e": 91, "109267": 66, "109273": 86, "109277": 83, "10928": 88, "1093": 79, "109454": 88, "1096": [59, 93], "10967": 87, "10973372": 67, "10973400": 122, "109861": 152, "1099472942084532": 74, "10e": [78, 91], "11": [38, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 91, 92, 94, 95, 96, 97, 99, 104, 106, 107, 108, 109, 110, 113, 122, 123, 138, 139, 145, 152, 155], "110": [106, 154], "110081": 83, "1101": 88, "110179": 77, "11019365749799062": 95, "110194": 95, "11026688": 67, "110359": 86, "110365": 95, "110557": 85, "110681": 94, "11071087": [96, 108], "110717": 138, "110742": 85, "1109": 88, "1109237": 122, "110932": 69, "110960": 69, "110e": 108, "111": [77, 106, 154], "111043": 80, "1111": [10, 11, 42, 58, 60, 75, 79, 86, 95, 98, 108, 139, 145, 150], "11120": 88, "111352344760325": 85, "11140326": 67, "111577": 88, "1117": [79, 84, 85], "111783": 88, "1118": 61, "11199615e": 122, "112": [62, 106, 154], "1120": 87, "112055": 66, "112078": 106, "11208236": [97, 123], "112135": 78, "1121351274811793": 78, "1122": 88, "112216": 78, "112219": 122, "112545": 68, "1127845": 122, "112867e": 77, "113": [10, 106, 154], "113005": 88, "113022": 83, "11311": 87, "113149": 83, "113207": 91, "113270": 78, "1135226": 122, "113545": 66, "11369492": 67, "11376": 88, "113780": 86, "1138761431": 122, "113952": 83, "11399": 87, "114": [106, 154], "114026": 85, "1140976": 122, "1142": 122, "114258": 76, "114312": 68, "1144": 88, "114410": 66, "1144500": 60, "11447": 94, "114530": 80, "1145370": 60, "114570": 77, "11458": 88, "114647": 78, "1147": 59, "114834": 88, "114989": 73, "115": [106, 154], "11500": [87, 155], "115060e": 91, "1151": 88, "1152": 122, "115297e": [87, 88], "11530": 88, "115339": 69, "11552911": 94, "11563951": 69, "11570": 87, "115785": 16, "11588": 88, "115901": 73, "116": [89, 106, 154], "1160623": 122, "116090": [108, 111], "116112": 122, "116209": 66, "116274": 78, "116408": 67, "1166": [87, 153], "1167": 87, "11673": 88, "11676": 88, "117": [76, 106], "11700": 155, "117194": 90, "11720": 88, "117242": 91, "11724226": 91, "117366": 91, "11743": 155, "117457": 77, "11750": 88, "1176": 59, "1177": 59, "117710": 78, "11789998": 91, "117900": 91, "11792": 61, "11796": 88, "118": 106, "1182": 61, "11820": 88, "11823404": 95, "118255": 91, "1185": 67, "11850": 88, "118596": 78, "1186": 61, "118601": 86, "11861": 61, "1187": 94, "118708e": 86, "1187339840850312": 86, "118799": 88, "118938e": 108, "118952": 86, "119": [95, 106, 154], "11935": 94, "119409": 85, "11942111": [108, 113], "119766": 91, "1198": [60, 86], "1198076": 122, "119820": [108, 111], "119938": 66, "119993": 66, "12": [4, 13, 16, 17, 19, 52, 53, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 94, 95, 97, 99, 104, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 139, 145, 150, 152, 153, 154, 155], "120": [63, 64, 85, 96, 106, 154], "12002": 87, "1200x600": [66, 67, 69], "1200x800": [66, 67, 69], "1202": 153, "120567": [80, 81], "120721": 86, "12080467": [108, 111], "12097": [10, 11, 42, 60, 79, 86, 88, 98, 150], "121": [88, 106, 154], "1210": 88, "121022": 66, "12105472": 138, "121054720": 138, "12105768": 122, "1211": 88, "121161": 66, "121297": 88, "1213405": 60, "121395": 69, "1214": 138, "121584e": 91, "121750": 77, "121774": 82, "121959": 66, "12196389e": 122, "122": [18, 31, 39, 59, 66, 92, 99, 104, 106, 153, 154], "12214": 61, "1221652": 122, "12223182e": 122, "122408": 78, "122421": 83, "122456": 68, "122671e": 77, "122777": 138, "123": [44, 61, 62, 66, 69, 87, 95, 106, 154, 155], "1230": 88, "123040": 69, "123192": 95, "12323": 88, "1234": [57, 58, 59, 70, 74, 75, 92, 93, 98, 107, 122, 138], "123413162947525355586369737581868995": 122, "12348": 95, "123501e": 88, "12381627": 69, "123950": [108, 109, 113], "124": 106, "12410": 88, "124368": 66, "124805": 87, "124945": 66, "125": [106, 154], "12500": 87, "125059": 138, "125346": 69, "12539340": 138, "125649": 106, "12572": 88, "1258": [60, 88], "126": [106, 154], "126096": 66, "12612": 88, "12616": 88, "126494": 16, "126771": 138, "126802": 88, "12689": 88, "127": [31, 106, 154], "12705095": [123, 138], "12707800": 60, "127337": 83, "127420": [108, 111], "1275": 88, "12752825": 138, "127563": 94, "127578624": 122, "1277": 89, "1277749": 122, "127961": 66, "128": [57, 61, 106, 154], "12802": 61, "12814": 88, "128229": 83, "128236": 67, "128247": 68, "128312": 91, "128372": 77, "128393": 88, "128408": 86, "128412": [100, 104], "1285": 59, "12861": 88, "129": [86, 106, 154], "129085": 77, "12911038": 67, "129446": 77, "12945": 153, "12947879": [108, 111], "1295": [59, 88], "12955": 87, "1297": 88, "12980769e": 122, "12983057": 108, "12989506": 122, "13": [17, 18, 19, 33, 35, 39, 57, 58, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 91, 92, 93, 94, 95, 97, 99, 104, 106, 107, 108, 110, 111, 122, 123, 138, 139, 145, 152, 155], "130": [62, 86, 106, 154], "130025": 67, "13003274": [108, 111], "130122": 94, "13022829": 67, "13034980e": 122, "130370": 78, "130473": 67, "1306": 94, "130609": 77, "130802": 76, "130829": 91, "13084": 108, "13091": 88, "130956": 69, "130971": 80, "131": [106, 154], "13102231": 108, "131047": 69, "13119": 94, "1312": 155, "1313": [61, 155], "13137893e": 122, "131483": 83, "131505": 69, "1316": 67, "1318": 59, "131842": 83, "131882": 68, "131909": 69, "131928": [100, 104], "132": [62, 76, 86, 106, 154], "13208": 155, "1321": [87, 155], "132248": 106, "1324": [61, 87], "132454": 65, "132481": 106, "1325": 61, "13257": 87, "132671": 78, "1328": 94, "132941": 88, "133": [62, 99, 104, 106, 153, 154], "133202": 88, "133421": 88, "133509": 76, "13357": 88, "133596": 91, "133676": 77, "13369246": 67, "133839": 83, "13398": 95, "133f5a": 89, "134": [86, 96, 106, 154], "134037": 68, "1340371": 59, "1341": 61, "13413077": 67, "134211": 91, "1343": [87, 88], "134344": 66, "134382": 66, "134567": 88, "1346035": 61, "134664e": 76, "134667": 69, "134687": 88, "13474": 88, "134765": 88, "134784": 108, "1348": 87, "13490": 88, "135": [62, 106, 108, 154], "13505272": 60, "135289": 69, "135309": 76, "13536219": 67, "135375": 73, "135378": 138, "135398": 77, "135596": 88, "135707": 107, "135856": 91, "13585644": 91, "135871": 86, "1359": 88, "135981": 68, "136": [70, 86, 95, 106, 154], "1360": 61, "136010": 76, "13602": 95, "136089": 86, "13642": 88, "136442": 86, "136457": 66, "1366": 89, "136613e": 69, "136836": 86, "136885": 88, "136898": 76, "137": [31, 62, 70, 106, 154], "137094e": 66, "1371": 88, "137165": 108, "137230": 85, "137260": 67, "1373": 67, "137396": 91, "137503": 67, "137649": 68, "1378": 88, "13784555": 67, "137999": 108, "138": [106, 154], "1380": 87, "13809": 88, "138142": 80, "138264": 95, "138378": 78, "1384": 87, "138534": 66, "1386": 59, "138611": 69, "13868238": 138, "138682380": 138, "138698": 138, "1387": 59, "1388944": 67, "13893": 88, "138967": 66, "139": [95, 106, 152], "139491": 138, "139508": 83, "13956": 94, "139622": 88, "139762": 68, "1398": 88, "139830": [108, 113], "1399": 59, "14": [58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 72, 73, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 91, 92, 94, 95, 97, 99, 104, 106, 107, 108, 110, 111, 122, 123, 138, 139, 145, 152, 153, 155], "140": [63, 64, 83, 88, 96, 106, 154], "1400": 88, "14000073": 122, "1401": 59, "14018": 108, "140320": 66, "14045": 69, "140530": 76, "140563": 67, "140770": [76, 77], "140833": 78, "140861": 60, "140863": 66, "14092991": 67, "141": [88, 106, 154], "1410": 67, "141069": 77, "1411": 69, "141101e": 88, "14114": 94, "141210": 69, "141247": 73, "14141": 88, "141484": 76, "141546": 138, "141758": 66, "141820": 78, "141984": 69, "142": [106, 154], "14200098": 138, "142045e": 86, "142132": 76, "142134": 66, "142270": 65, "1424": 107, "142482": 91, "14268": 108, "14281403493938022": 107, "14289": 88, "143": [99, 104, 106, 154], "1435": 88, "143593": 76, "14368145": 138, "144": [106, 154], "14400": 87, "144013e": 66, "144015": 66, "14405": 88, "14406": 88, "144084": 78, "1441": 59, "144195": 77, "1443": 88, "144669": 91, "144800": 78, "144813": 73, "14484": 69, "144908": 90, "144971": 87, "145": [106, 154], "145245": 91, "14530": 88, "145317": 66, "14532650": 138, "14546": 69, "145625": 91, "145699e": 76, "145737": 138, "14588": 88, "14595677": 69, "146": [106, 154], "146037": 91, "146087": 152, "146214": [100, 104], "14625": 88, "146495": 66, "1465": 61, "146641": 138, "1468115": 60, "146861": 106, "147": [106, 154], "14702": 70, "147121": 91, "147383": 77, "14744": 88, "147464": 106, "147595": 66, "14772": 88, "147887": 69, "1479": [72, 88], "14790924": 138, "147909240": 138, "147927": 70, "14795": 88, "148": [89, 106, 154], "148005": 73, "14803": 88, "148134": [76, 77], "148161": 91, "148210": 78, "1482102407485826": 78, "148317": 66, "148412": 66, "14845": 70, "148455": 78, "1484554868601506": 78, "1485": 88, "148750e": [87, 88], "148811": 69, "148835": 88, "149": [106, 154], "1492": 57, "149228": [69, 95], "149265": 77, "149285": 91, "149392e": 106, "149446427": 76, "149472": 95, "149681": 76, "149714": 86, "149763": 66, "149858": 28, "149897": 77, "149898": 91, "15": [11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 58, 60, 61, 62, 64, 66, 69, 72, 73, 76, 77, 78, 82, 83, 84, 85, 86, 87, 88, 91, 92, 93, 94, 95, 99, 104, 106, 107, 108, 110, 111, 122, 123, 138, 139, 145, 152, 155], "150": [40, 62, 95, 106, 154], "15000": [61, 87], "150000": 61, "15000000000000002": [78, 88, 91, 107], "1502": 60, "150200": 86, "15021839": 67, "150234": 88, "150408": 60, "150435": 88, "150614": 70, "150719e": 87, "1509": 72, "151": [89, 106, 154], "15108": 88, "15135781": 67, "15137153": 67, "151447": 85, "151636": 78, "15173764": 67, "15180": 69, "151819": 91, "15185645": 67, "15194": 87, "152": [106, 154], "152148": [76, 77], "152414": 77, "152706": 108, "15287": 88, "152926": 65, "153": [95, 106, 154], "153119": 78, "153293": 88, "15339": 88, "153398": 88, "153464": 66, "15354": 94, "153587": 86, "153633": 70, "153639": 122, "153773": 69, "153971e": 76, "153983": 78, "1539831483813536": 78, "154": [72, 106], "15430": 155, "154415": 138, "1545": 88, "154557": 91, "15463229": 122, "154707": 73, "154752": 138, "154821": [108, 111], "154828": 78, "155": [106, 154], "155000": 87, "155025": 91, "155034": 66, "155120": 91, "155160": 73, "1554": 88, "155423": 73, "15549": 88, "155558": 66, "15556": 88, "1557093": 60, "155995": 77, "156": [106, 154], "15601656": 67, "156021": 91, "156202": [76, 77], "156209": 77, "156317": [76, 77], "156328": 85, "1564": 138, "156540": 138, "156554": 76, "156704": 88, "156817": 66, "1569": 88, "156969": 78, "156988": 72, "15699": 69, "157": [77, 106, 154], "157080": 138, "157470": 90, "1576": 88, "1577657": 60, "157774": 69, "157818508": 122, "158": [106, 154], "158007": 91, "158076": 77, "15815035": 61, "158178": 78, "158198": 88, "1582": 88, "158288": 88, "1586": 88, "158682": 85, "158697": 138, "158726": 106, "15889579": 67, "1589": 88, "159": 154, "15909534": 67, "15915": 68, "15916": [59, 68], "159386": 94, "159466": 91, "15946647": 91, "159592": 77, "1596": 62, "159826": 88, "159959": 88, "15e": 122, "16": [24, 57, 58, 60, 61, 62, 63, 64, 66, 68, 69, 72, 73, 76, 77, 78, 81, 82, 83, 86, 87, 88, 91, 92, 94, 95, 99, 104, 106, 107, 108, 110, 111, 122, 123, 138, 139, 145, 152, 155], "160": [63, 64, 96, 154], "160046": 67, "160102": 66, "160285e": 77, "160315": 66, "1604": 61, "160646": 69, "160825": 80, "160836": 83, "16083947": 67, "16089597": 67, "160932": 78, "161": [62, 72, 153, 154], "161141": 86, "161236": 91, "161243": 91, "161305": 76, "16138102": 67, "161441": 64, "1619": 61, "16191767": 67, "162": [72, 154], "16201": 88, "16211": 87, "162153": 91, "1622": 88, "162389": 76, "16241": 88, "162436": 95, "16249": 88, "162634": 69, "1626685": 60, "162683": 95, "162710": 78, "162752": 85, "1628": 87, "162909": 86, "162930": 88, "162972": 68, "163": 154, "163013": 16, "163194": 91, "163249": 69, "163465": 66, "163519": [108, 111], "163566": 88, "16363956": 67, "16374141": 67, "163895": 78, "164": [72, 73, 88, 92, 108, 154], "164034": 138, "164133": 66, "16428146": 69, "164467": 87, "164522": 69, "164596": 69, "164608": 91, "164698": 82, "1648": 59, "164801": 91, "164805": 78, "164864": 86, "164941": 16, "164943": 85, "165": [72, 95, 154], "16500": 87, "165178": 91, "1653": 88, "16536299": 138, "165362990": 138, "16539906e": 122, "165419": 91, "16553": 87, "165549": 152, "165670e": 77, "165707": 73, "16590": 88, "166": 154, "1661": 87, "16618": 88, "166322": [100, 104], "166454": 69, "166517": 83, "166904": 88, "167": [61, 87, 154], "167035": 88, "167547": 91, "1676": 88, "167916": 69, "167930": 88, "167981": 138, "168": 154, "1680": 67, "16803512": 138, "168092": 138, "168097": 67, "1681": [59, 67, 87], "168195": 94, "168614": 91, "168889": 68, "168931": 91, "169": [62, 154], "1691": [59, 88], "16910": 88, "169117": 95, "169196": 91, "169220e": 78, "16951": 88, "169533": 69, "16984": 88, "169858": 85, "17": [58, 60, 61, 62, 64, 66, 69, 72, 73, 76, 77, 82, 83, 85, 86, 87, 88, 91, 92, 94, 95, 99, 104, 106, 107, 108, 110, 122, 123, 138, 139, 145, 152, 155], "170": 154, "170253": 69, "170458": 66, "1705": 88, "170705": 106, "17083": 88, "170832": 69, "170870": 68, "170933": [108, 113], "171": [72, 154], "1712": 153, "171255": 106, "1714": 61, "171552": 67, "171575": 91, "171696": 106, "171815": 107, "171838": 67, "171860": 77, "1719": [66, 67, 68, 69], "172": [72, 92, 154], "172022": 138, "172083": 77, "17222356": 67, "17256895": 67, "1727805": 67, "172793": 91, "17282191": 67, "17290841": 67, "173": [72, 154], "173309": 66, "173400": 72, "173504": 81, "17354207": 67, "17372": 88, "17385178": 107, "173964": 138, "173e": 92, "174": 154, "174177": 91, "174185": 91, "1742964": 122, "174499": 138, "174516e": 91, "17453": 88, "1746": 88, "174653e": 69, "17469": 88, "174968": 87, "174e": 108, "175": [72, 154], "17500": 88, "17508262": 69, "1751": [72, 87], "175254": 87, "175284": 78, "17536": 88, "1755": 72, "175564e": 77, "175635027": 60, "17576": 88, "175894": 95, "175931": [106, 107, 108], "176056e": 106, "17635027": 122, "176495": 91, "17655394": 91, "176554": 91, "17658072": 122, "176929": 138, "177": [153, 154], "177007": 91, "17700723": 91, "177043": [76, 77], "177094": 72, "1773": 88, "177397": 88, "1774": 59, "177496": 91, "177611": 91, "177751": 91, "177995": 91, "178": [88, 154], "17800": 88, "17807": 88, "17809279": 122, "178169": 80, "17823": 62, "178273": 77, "178661": 138, "178763": 91, "178934": 138, "179": [57, 88, 100, 104, 154], "179101": 106, "179204": 77, "179276": 77, "1795": 72, "179548": [100, 104], "1795850": 60, "179588e": 91, "179623": 66, "179810": 77, "1798913180930109556": 89, "18": [58, 60, 61, 62, 64, 66, 69, 70, 72, 76, 77, 80, 82, 83, 84, 85, 86, 87, 88, 91, 92, 94, 95, 99, 104, 106, 107, 108, 110, 122, 138, 152, 155], "180": [63, 64, 96, 154], "180195": 67, "180271": 83, "180296": 77, "1803": 59, "18030": 88, "180307": 77, "180474": 77, "180575": [80, 81], "1807": [59, 88], "1809": 153, "180951": 91, "181": 154, "1812": 88, "181330": 73, "181350": 76, "1814": 59, "18141": 88, "181432": 138, "181467": 66, "181854": 66, "182": [69, 89, 154], "1820": 59, "18200073": 67, "182018": 77, "182151": 106, "182288": 77, "182354": 77, "182355": 69, "182399": 16, "182633": 91, "18273096": [108, 111], "182810": 69, "182849": 91, "183": [62, 72, 108, 154], "1832": 88, "183373": 108, "183382": 68, "183444": 66, "18348965": 67, "183526": 78, "183553": 92, "18356413": 108, "18368": 88, "183704": 66, "183814": 88, "183855": 107, "183888": 86, "184": [62, 66, 153, 154], "184224": 73, "184303": [108, 113], "184449": 76, "184593": 69, "1849": 122, "184980": 66, "185": [61, 62], "185035": 76, "185130": 92, "18516129": [108, 113], "185617": 77, "185930": 106, "185956e": 77, "185993": 66, "186": [69, 72, 154], "18604": 88, "186136": 77, "1862": 59, "18622": 88, "18637": 150, "186589": 73, "18666": 88, "186689": 108, "186735": [68, 91], "186775": [108, 109, 113], "18678094e": 122, "186836": 91, "186864": 85, "186868": 64, "187": 154, "187109": 69, "187148": 138, "187482": 68, "187617": 77, "187690": 91, "18773": 88, "187732": 76, "187782": 76, "187894": 66, "188": 154, "188120": 66, "188173": 68, "188175": 91, "1881752": 91, "188223": 91, "188298": 66, "188541": [108, 109], "188545": 77, "1887": [106, 107, 108], "188760": 83, "1888": 88, "18888149e": 122, "188965": 66, "188986e": 76, "189": [62, 66, 69, 88, 92, 154], "189023": 138, "189195": 88, "1895815": [41, 60, 86], "189737": 91, "189739": 73, "18976": 88, "189998": 91, "19": [15, 58, 60, 61, 62, 64, 66, 69, 72, 76, 77, 83, 85, 86, 87, 88, 91, 92, 94, 95, 99, 104, 106, 107, 108, 110, 111, 122, 138, 152, 155], "190": [62, 154], "190000e": 88, "190072": 66, "190090": 138, "190231": 88, "19031969": 91, "190320": 91, "19033538": 60, "190534": [108, 111], "19073905e": 122, "190742": 89, "190756": 69, "190809": 91, "190892": 95, "1909": [41, 60, 86], "190915": 78, "190921": 81, "190952": 69, "190976": 92, "190982": 91, "191": [62, 66, 153, 154], "1912": 153, "1912705": 98, "191501": 76, "191534": 87, "191578": 25, "19176": 66, "1918": 59, "19188": 69, "192": [69, 154], "192234": 138, "1923": 88, "192400": 66, "192526": 94, "19252647": 94, "192539": 29, "192865": 66, "192952": 73, "193": [66, 69, 154], "193060": 91, "193308": 29, "193375": 83, "193581": 76, "193644": 88, "19374710e": 122, "19382": 88, "193833": 88, "193849": 92, "19385": 88, "193870": 76, "193887": 69, "193f0d909729": 62, "194": [72, 84, 154], "1941": 61, "19413": [87, 88], "194160": 66, "194167": 76, "194588e": 76, "194786": [108, 109], "194945": 106, "195": [69, 101, 104, 154], "19508": 95, "19508031003642456": 95, "19509680e": 122, "195101": 76, "195132": 77, "195377": 91, "195396": 91, "19550": 88, "195564": 86, "19559": [61, 87], "195816": 76, "1959": 153, "195963": 83, "196": [66, 154], "196189": 91, "196247": 69, "1963315": 122, "19662": 69, "196655": 83, "19680840": 138, "196824": 91, "196835": 85, "196967": 69, "197": [100, 104, 154], "1970": 88, "197004": 69, "19705": 88, "197225": [70, 99, 104, 152], "1972250000001000100001": [62, 99, 104, 152], "197356": 76, "197424": 107, "19743683": 69, "197462": 77, "197484": 138, "1975": 88, "19756": 88, "19758": 88, "197600": 65, "197787": 66, "197839": 69, "19793": 88, "19798": 88, "198": [66, 69, 154], "198221": 88, "19824": 88, "198493": 83, "198503": 92, "198529e": 77, "198549": 70, "198617": 90, "198687": 61, "1988": [58, 75, 98, 108], "198944": 66, "199": [66, 87, 154], "1990": [61, 87, 88], "1991": [61, 87, 88, 155], "199213": 77, "199237": 68, "199281e": 91, "199404": 67, "199445": 138, "1995": [60, 86], "1998": 89, "19983954": 96, "199873": 66, "1999": 89, "1_": [78, 91], "1d": 13, "1e": [12, 15, 52, 53, 72, 83, 88], "1f77b4": 65, "1m": 107, "1mnon": 107, "1x_4x_3": 65, "2": [10, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 47, 49, 51, 52, 53, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 117, 122, 123, 138, 139, 140, 142, 144, 145, 146, 147, 148, 149, 151, 152, 153, 154], "20": [12, 14, 15, 17, 18, 19, 22, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 37, 38, 39, 41, 42, 43, 58, 60, 61, 62, 63, 64, 65, 66, 69, 72, 76, 77, 78, 82, 83, 84, 85, 86, 87, 88, 91, 94, 95, 96, 97, 98, 99, 104, 106, 107, 108, 110, 122, 123, 138, 139, 145, 152, 155], "200": [17, 19, 32, 35, 40, 59, 63, 64, 66, 69, 78, 79, 84, 90, 91, 96, 98, 103, 104, 107, 153, 154], "2000": [11, 30, 61, 63, 73, 76, 77, 78, 83, 87, 88, 91, 93, 96, 106, 108], "20000": [61, 87], "20000000000000004": [78, 88, 91], "200000e": 88, "200042": 76, "2001": 67, "20010": 88, "200110": 88, "2003": [10, 67, 153], "200303": 152, "2004": [67, 108, 112], "2005": [64, 67], "20055": 88, "200556": 69, "2006": [67, 88, 108, 112], "2007": [67, 108, 112], "20074": 88, "20088096": 67, "200982": 80, "201": [62, 154], "2010": [60, 86], "2011": [60, 86, 150, 152], "201213": 69, "20125374": 67, "2013": [79, 138, 153], "2014": [138, 153], "2015": [40, 153], "201514": [108, 111], "201528": [76, 77], "20158": 88, "2016": 89, "20167273": 84, "2017": [34, 153], "201768": 86, "2018": [10, 11, 42, 43, 58, 60, 61, 64, 75, 79, 84, 86, 87, 88, 93, 94, 98, 108, 114, 122, 123, 131, 138, 150, 153, 154], "2019": [32, 62, 76, 77, 78, 80, 81, 88, 91, 94, 107, 123, 129, 132, 133, 150, 152, 153], "20195463": 67, "20196493": 67, "201e": 92, "202": [66, 69, 154], "2020": [12, 14, 15, 17, 18, 19, 31, 33, 35, 39, 59, 62, 64, 66, 69, 95, 107, 108, 110, 114, 139, 140, 153], "2020435": 60, "2021": [17, 19, 41, 59, 60, 62, 66, 67, 68, 69, 76, 77, 86, 108, 109, 112, 114, 153, 154], "20219609": 60, "2022": [94, 95, 108, 110, 114, 139, 140, 149, 150, 153], "202298": 77, "2023": [36, 63, 93, 96, 108, 121, 123, 136, 137, 153], "2024": [57, 74, 79, 84, 85, 89, 92, 93, 95, 108, 150, 153], "2025": [16, 17, 19, 66, 69, 93, 108, 109, 111, 113], "202650e": 78, "20269": 88, "2029554862": 65, "203": [61, 76, 87, 154], "203016": 66, "203082": 106, "203284": 78, "20329": 88, "203828": 88, "203943": 66, "204": [66, 69, 154], "204007": 91, "20400735": 91, "204362": 95, "204482": 91, "204495": 77, "204592": 69, "204794": 91, "204893": 73, "205": [92, 94, 154], "205187": 78, "205224": 94, "205245": 88, "205333e": 87, "2055": 72, "20572847": 67, "205812e": 69, "20592902": 67, "205938": 86, "206": 154, "206156": 66, "206253": [87, 88], "206256": 83, "2066986": 122, "206802": 26, "207": [72, 89, 92, 154], "207242": 77, "207378": 69, "207479": 69, "2075": 59, "20783816": 60, "207840": 81, "207885": 87, "207912": 138, "208": [66, 69, 72, 73, 154], "2080787": 60, "20823898": 60, "208300": 91, "208357": 67, "2084": 67, "208447": 67, "2086": [72, 88], "208840": 72, "209": [66, 72, 73], "2091": 72, "209143": 67, "209219e": 94, "20956": 88, "209894": 91, "21": [10, 11, 42, 58, 60, 61, 62, 64, 66, 69, 72, 76, 77, 79, 85, 86, 87, 88, 91, 93, 94, 95, 98, 99, 104, 106, 107, 108, 110, 122, 138, 150, 152, 153, 155], "210": [17, 18, 19, 35, 39, 66, 69, 72, 73, 88], "2103": 150, "2103034": 60, "210319": [76, 77], "210323": 91, "2104": 154, "2107": 153, "21082": 88, "211": [66, 69, 72, 73, 92, 154], "211002": [108, 113], "21105": [62, 107, 150, 152], "2112": 95, "2113427": 122, "2114026": 91, "211403": 91, "21142": 88, "211534": 78, "2116": 72, "211682": 77, "211715": 69, "211939": 72, "212": [66, 69, 73, 154], "21200916": 69, "2121": 88, "21218961": 122, "212317": 91, "212330": 66, "212491": 91, "21257396e": 122, "2125884": 122, "212811": 73, "212844": 86, "213": [66, 69, 72, 73, 92, 153, 154], "213199": 108, "21342": 88, "21351": 88, "2139": 33, "214": [66, 69, 72, 154], "214437": 69, "214732": 76, "214764": 94, "214769": 83, "215": [66, 73, 89, 154], "215071": 69, "215103": 77, "215159": 88, "21529018": 122, "215342": 91, "2155": 88, "21550": 88, "215551": 69, "215619": 68, "21562": 88, "215958": 138, "216": [65, 66, 72, 73, 154], "216038": 66, "216113": 91, "216207": 107, "216215e": 76, "216224": 77, "216228": 66, "21624417": 60, "2163": 88, "216344": 91, "21669513e": 122, "2167": 88, "216943": 106, "217": [72, 73, 92, 153, 154], "21716": 88, "2171802": [60, 86], "217244": 27, "217461": 66, "2175": 88, "217660": 66, "217684": 83, "218": [66, 69, 72, 73, 154], "21804": [61, 87], "218223": 85, "218383": 73, "218477": 66, "218546": 91, "218662": 68, "218787": 66, "2189": 88, "218919": 72, "218924": 88, "219": [18, 31, 39, 59, 72, 73, 100, 104, 153, 154], "2191274": 60, "219585": 73, "22": [58, 60, 61, 62, 64, 66, 69, 76, 77, 85, 86, 87, 91, 92, 94, 95, 106, 107, 108, 110, 122, 138, 152, 155], "220": [66, 69, 72, 73, 154], "220171": [108, 111], "220211": 88, "220446e": 69, "220568": [108, 111], "220587": 16, "220772": 91, "220811": 69, "220897": 89, "221": [72, 73, 154], "221127": 76, "2213": 86, "2214": 86, "2215": 86, "2216": 86, "2217": [60, 86], "2218": 153, "222": [69, 72, 154], "2222": [58, 60, 75, 108], "222261": 106, "222264e": 77, "2223": 72, "222306": 85, "222350": 69, "222430": 77, "22272803e": 122, "222758": 66, "222843": 91, "223": [57, 66, 92, 154], "223095": 64, "22336235": 60, "223485956098176": [80, 81], "223625": 16, "22375856": 60, "22390": 87, "224": [66, 69, 89, 92, 154], "224146": 89, "224183": 66, "2244": 153, "224822": 77, "22483561": 122, "224897": [76, 77], "225": [17, 19, 59, 96, 153, 154], "22505965": 60, "22507006e": 122, "22515044": [108, 111], "225175": 91, "22528": 88, "225427": 73, "225511": 69, "225574": 86, "2256": 88, "22562": 88, "225635": 91, "22563538": 91, "225764": 85, "225776": 95, "225899": [108, 113], "225917": 85, "226": [66, 69, 154], "2264": 59, "226479": 83, "226524": 91, "226598": 86, "226919": 77, "226938": 81, "227": [66, 100, 104, 154], "227018": 78, "227018245501943": 78, "227086": 85, "2271071": 36, "227184e": 77, "227243e": 66, "22738944": 122, "227567": 106, "2276": 59, "227621e": 77, "227641": 69, "2279": 88, "227932e": 87, "228": [66, 69, 154], "228035": 88, "2281": 88, "228648": 61, "22867019": 67, "2287": 72, "228725": 88, "229": [61, 66, 69, 154], "22913346": 67, "22913748": 67, "2291407": 67, "2291444": 67, "22925": 88, "229443": 91, "229452": [106, 107, 108], "229472": 87, "2295": [69, 88], "22959": 88, "229726": 88, "229745": 69, "229759": 107, "2298": 59, "229961": [76, 77], "229994": [76, 77], "22m": 107, "23": [47, 51, 60, 61, 62, 64, 66, 69, 76, 77, 85, 86, 87, 91, 94, 95, 99, 104, 106, 107, 108, 122, 138, 150, 152, 153, 155], "230": [17, 19, 59, 69, 153, 154], "230009": [80, 81], "230055": [108, 111], "23006668": 67, "2302": 93, "230368": 76, "230532736": 122, "2306273": 67, "2307": [60, 86, 93, 98], "2308": 94, "230806": 69, "230821": 88, "230956": 65, "231": [10, 66, 100, 104, 154], "231165": 108, "231281": 72, "231310": 91, "23135865": 67, "231430": 138, "231467": 108, "231734": 108, "231798": 108, "231986": 91, "231e": 92, "232": [66, 69, 154], "232068": 66, "232134": [76, 77], "232277": 77, "232485": 77, "232497e": 76, "232774": 76, "232959": [80, 81], "232e": 108, "233": [34, 69, 154], "2331": 88, "233154": 155, "233415": 88, "2335": 59, "233527": 66, "23368": 88, "23391813": 67, "234": [66, 69, 153, 154], "23404391": [108, 111], "234137": 95, "234152": 66, "234153": 95, "234534": 78, "234605": 70, "234752": 66, "234910": 86, "235": [66, 153, 154], "235154": 66, "235368": 68, "235384": 66, "23549489": [108, 111], "2359": 155, "235952": 68, "236": [66, 154], "236008": 78, "23611": 88, "236411": 91, "236562": 67, "23690345e": 122, "237": [62, 69, 154], "237109e": 122, "237158": 77, "237164": 77, "237171": 69, "237461": 94, "23751359e": 122, "237599": 66, "237743": 69, "238": [60, 86, 154], "238012": 91, "23801203": 91, "238071": 66, "238101": 91, "238176e": 122, "238213": 138, "238251": 78, "238265": 68, "238287": 76, "238295": 66, "238416": 76, "238449": 77, "23856": 88, "238619": 73, "238718": 77, "239": 154, "239019": 83, "239069": 66, "239192": 66, "239352": 88, "239426": 69, "239471": 72, "239503": 76, "23968": 88, "239845": 88, "23e": 61, "24": [60, 61, 62, 66, 69, 76, 77, 85, 86, 87, 91, 94, 95, 96, 106, 107, 108, 122, 138, 152, 153, 154, 155], "240": 85, "240053": 77, "240127": [76, 77], "240131": 69, "240194": 76, "240254": 66, "240295": 94, "2403": 93, "240495": 106, "240532": [76, 77], "240601": [108, 111], "240693": 106, "2407": 59, "24080030a4d": 62, "240813": 82, "241": [69, 154], "241026018": 122, "241049": 91, "241064": 77, "241145e": 66, "241302": 66, "241393": 67, "241474e": 77, "241503": 106, "2416": 59, "241841": 90, "241962": 95, "241973": 88, "241e": 92, "242": [153, 154], "242001": 69, "242124": [87, 88], "242139": 138, "242158": [87, 88], "242175": 69, "2422": 72, "24227286": 122, "2424": 83, "242427": 83, "242559": 64, "242815": 138, "242864": 88, "242902": 91, "243": [88, 154], "243056": 68, "2430561": 59, "243246": 91, "243457": 90, "2438": 88, "243e": 92, "244": [88, 154], "244365": 69, "244455": 91, "2445": 88, "244571": 69, "244622": 138, "24469564": 152, "245": [153, 154], "245027": 72, "245062": 91, "2451": 59, "24510393": 61, "24514687": 69, "245370": 86, "245512": 91, "245552": 77, "245610": 77, "245673": 69, "245720": 65, "246": [69, 154], "246028": 66, "246136": 66, "246625": 66, "2467506": 60, "246753": 91, "2468": 88, "246820": 66, "246879": 91, "247": [92, 154], "247020": 78, "247057e": 91, "2472": 88, "247617": 106, "24774": [87, 88], "247817": 69, "247826": 86, "248": 88, "248171": 91, "248178": 76, "248441": 106, "248487": 85, "248491": 69, "248638": 78, "249": [60, 86, 89, 154], "249067": 77, "2491": 88, "249100": 86, "24917": 88, "249306": 85, "249441": 66, "249494": 76, "249601": [108, 109, 113], "2499": [67, 101, 104], "25": [17, 18, 19, 29, 30, 31, 35, 39, 40, 41, 42, 60, 61, 62, 65, 66, 69, 72, 76, 77, 78, 79, 85, 86, 87, 88, 91, 95, 106, 107, 108, 122, 138, 152, 155], "250": [89, 154], "2500": [67, 88, 101, 104, 108, 113], "25000000000000006": [78, 88, 91], "250083": 88, "25009262": 122, "2501": 88, "250210": 78, "250341": [108, 111], "250425": 78, "250529": 85, "2506": 93, "250838": 85, "251": [87, 88, 94], "251152": 88, "251187": 68, "251205": 69, "252": 88, "252133": 88, "252253": 94, "25240463": 108, "252524": 91, "252601": 138, "252849": 77, "253026": [76, 77], "253146": 67, "253610": 16, "253675": 87, "253724": 91, "25374": 88, "253747": 69, "254": [88, 154], "25401679": 60, "254038": 81, "2543": 88, "254324": 78, "254400": 138, "254720": 69, "255": [88, 154], "255115": 66, "255151e": 91, "256": [88, 107], "256082": 106, "256229": 69, "256352": 77, "256416": 91, "25643": 88, "256567": 86, "256663": 69, "2567": 67, "25672": 88, "256944": 91, "257037": 88, "257207": 60, "2573": 67, "257377": 65, "2575": [108, 111], "257762": 91, "25783377": 122, "25804293": 67, "258107": 66, "25812517": 67, "258158": [76, 77], "258278": 69, "2584": 88, "25855": 64, "25859046": 67, "2588703": 122, "259230": 77, "259325": 66, "259395": 82, "2594": [61, 87], "25953346": 67, "259580": 68, "259828": [76, 77], "259864": 69, "25x_3": 65, "26": [60, 61, 62, 64, 66, 69, 70, 72, 76, 77, 85, 86, 87, 99, 104, 106, 107, 108, 122, 138, 152], "26016": 88, "260161": 28, "260211": [76, 77], "260328": 76, "260356": 87, "260360": 91, "260449": 69, "260461": 69, "260554": 66, "260580": 66, "260738": [108, 113], "260762": 73, "261": 92, "2610": 88, "261140": 77, "26134878": 69, "261366": 76, "2615": 72, "261624": [87, 88], "261635": 106, "261685": 88, "261686": 85, "261903": 86, "2619317": 60, "2620": 88, "262083": 85, "262204": 88, "262261": 76, "262829": 122, "263": [10, 88, 154], "2633": 88, "2635209": 122, "263701": 66, "264": [153, 154], "264040": 69, "264086": 65, "264255": 16, "264426": 85, "265": 154, "2651": 108, "265119": 90, "265165": 66, "2652": [62, 87, 88], "265547": 88, "265562": 76, "265669": 72, "2658": 81, "265929": 92, "266": [57, 154], "266147": 92, "266633": 76, "266724": 88, "266909": 138, "267": 89, "267055e": 76, "2670691": 60, "267164": 85, "267500": 86, "267544": 66, "267581": 88, "267950": 91, "268": 154, "268850e": 77, "268942": 91, "268977": 88, "268998": 61, "269043": 91, "269112": 108, "269425e": 76, "269924": 69, "26bd56a6": 62, "26e": 61, "27": [17, 18, 19, 35, 39, 58, 60, 61, 62, 63, 64, 66, 69, 70, 72, 76, 77, 85, 86, 87, 99, 104, 106, 107, 108, 122, 138, 152, 153], "270": 154, "2700": 62, "270000e": 88, "270248": [108, 113], "270311": 16, "270452": 69, "270644": [76, 77], "27066": 88, "271": 154, "271004": [87, 88], "271183": 87, "271266": 69, "271383": 66, "2714838731": 86, "271556": [108, 109], "271797": 66, "271867": 86, "272236": 69, "272384": 77, "272505": 85, "272610": [108, 111], "272643": 88, "273": 62, "273208": 88, "273356": 78, "273454": 66, "273479": 66, "27371": [61, 87], "27372": [61, 87], "273825": 69, "2739": [65, 66, 69, 92, 95], "274": [62, 88], "2740991": 59, "274251e": 87, "27429763": [108, 110], "274576": 69, "27461519": [108, 113], "27469381": 122, "27472": 88, "274793": 91, "274825": 29, "27516596": 69, "2754": 59, "275428": 69, "275454": 77, "275538": [100, 104], "275596": 138, "275831": 77, "276": [62, 154], "2760": 88, "276148": 91, "276189e": 86, "2762": 88, "276311": 88, "276529": 69, "2766091": 61, "27695": 88, "277179": 72, "277299": 70, "277626": 106, "277788": 66, "277808": 76, "27794072": [108, 111], "277987": 85, "278": [80, 94, 154], "2780": 60, "278000": 86, "278035": 73, "278060": 69, "278391": 88, "278434": 80, "278454": 83, "278543": 76, "2786": 138, "278724": 72, "278907": 91, "278910": 69, "279": 154, "27927": 88, "27951256e": 122, "279524": 91, "279595": 73, "279824": 72, "27991": 88, "279926": 77, "28": [60, 61, 62, 66, 68, 69, 76, 77, 79, 85, 86, 87, 106, 107, 108, 122, 138, 152, 154], "280122": 76, "280196": 81, "280347": 69, "280389": 69, "2804": 67, "280405": 66, "280454dd": 62, "280501": 138, "280694": 69, "280766": 77, "280950": 66, "280963": 90, "281": [92, 154], "281024": 91, "28111364": 61, "281225": 66, "281360": 16, "2815": 88, "281629": 69, "2818": 59, "2819": 138, "281908": 76, "282": [92, 153, 154], "282194": 66, "282200": 81, "2825": [150, 152], "2830": [150, 152], "283168e": 76, "28326": 88, "2836": 59, "2836059": 60, "28367": 88, "2838546": [108, 111], "283939": 67, "283e": 92, "284": 154, "284073": 91, "28425026": 94, "284271": 82, "284397": 155, "284489": 66, "28452": [61, 87], "284647": 68, "284676": 89, "28487885": 122, "2849": 88, "284987": 88, "285": [92, 108, 154], "285001": 73, "285160": 86, "285276": 91, "285348": 69, "285483": 83, "285504": 69, "285783": 72, "285831": 66, "285928": 72, "285e": 92, "286": 154, "286203": 76, "2865": [59, 88], "286507": 78, "286563e": 88, "287": 154, "287041": 91, "287123": 106, "287486": 69, "287815": 94, "287926": 91, "288": [89, 154], "288105e": 76, "288788e": 77, "289": [153, 154], "289062": [87, 88], "289081": 91, "289170": 86, "289198": 77, "289440": [76, 77], "289536": 76, "289555": 83, "289706": 76, "289983": 85, "29": [60, 61, 62, 66, 69, 76, 77, 85, 86, 87, 94, 106, 107, 108, 122, 138, 152], "290": 108, "290037": 69, "290076": 67, "290112e": 77, "290253": 66, "290507": 77, "290871": 66, "290901": 73, "290987": 87, "291": [92, 154], "2910": 88, "291011": [12, 108, 110], "291071": 91, "29107127": 91, "291342": 77, "291406": 91, "291500e": [87, 88], "291517": [76, 77], "29168951": [108, 113], "291963": 91, "292": [90, 154], "2920": 88, "292028": 78, "292046": 138, "292105": 91, "292137": 69, "292179": 106, "2925": 62, "292652e": 77, "292997": 91, "29299726": 91, "293218": 91, "293669": 88, "293685e": 69, "294": 154, "294067": [76, 77], "294123": 106, "294989084": 122, "295": [153, 154], "295590": 76, "295837": [70, 99, 104, 152], "2958370000000100000100": [62, 99, 104, 152], "2958370001000010011100": [62, 99, 104, 152], "2958371000000010010100": [62, 99, 104, 152], "295855642811191": 78, "295856": 78, "295868": 88, "296099": 73, "296171": 76, "2962": 72, "296523": 76, "296729": 86, "29675887": 91, "296759": 91, "29678199": [97, 123], "297": 154, "297102": 76, "297173": 66, "297204": 76, "297276": [108, 113], "297287": [76, 77], "297349": [80, 81], "297350": 66, "297682": 91, "2977": 88, "29784405": 94, "298": [34, 62], "298069": 76, "298088": 77, "298120": 78, "298132": 76, "298150": 85, "298235e": 88, "298327": 73, "298479": 66, "298629": 106, "29872427": 69, "298995": 106, "299": [62, 92], "299398": 72, "299416314": 122, "299537": 81, "299755": 85, "299797": 69, "2999": 73, "29999": 66, "2_": [36, 63, 96, 139, 140, 149], "2_x": [36, 63, 96], "2d": [13, 123, 132], "2dx_5": [78, 91], "2e": [57, 59, 60, 61, 62, 63, 107, 108, 123, 138, 152], "2f": 82, "2m": [139, 145, 149], "2n_t": 65, "2x": 91, "2x_0": [32, 76, 77, 80, 81], "2x_4": 65, "3": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 45, 46, 47, 50, 51, 52, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 113, 114, 122, 123, 138, 139, 145, 150, 151, 152, 153, 154], "30": [32, 57, 58, 60, 62, 63, 64, 66, 68, 69, 72, 73, 74, 75, 76, 77, 78, 85, 86, 87, 88, 91, 92, 106, 107, 108, 122, 138, 152], "300": [58, 75, 78, 88, 91, 98, 153], "3000": 73, "30000": 66, "30000000000000004": [78, 88, 91], "30031116e": 122, "300376": 66, "300511": 69, "30093956": 94, "301": 62, "301366": 138, "301371": 91, "3016": 87, "301629": 76, "30175": 88, "30186": 88, "301918": 66, "301933": 66, "302062": 76, "30227357": [108, 111], "30229388": 91, "302294": 91, "302552": 76, "302648": 86, "303324": 86, "303835": 86, "303f00f0bd62": 62, "304130": 91, "304159": 91, "304201": 65, "305142": 85, "30523245": 69, "305272": 64, "30529": 88, "305341": 91, "305612": 86, "305687": 91, "305774": 67, "305775": 91, "305811": 106, "305b": 62, "306282e": 77, "30628774": 91, "306288": 91, "30645": 88, "30672815": 60, "306915": 86, "306963": 91, "306975": 68, "307098": 77, "307407": 91, "307444": 85, "307627": 66, "307923": 16, "308": 88, "308239": 88, "308415": 16, "308837": 77, "30917769": [80, 81], "309464": 88, "309772": 86, "3098979": 69, "309901": 72, "31": [60, 61, 62, 63, 66, 68, 69, 72, 76, 77, 85, 86, 87, 106, 107, 108, 122, 138, 152, 155], "310097": 76, "310743": 69, "311": 92, "311375": 77, "311594": 76, "311740": 85, "311807": 76, "311869": 106, "312030": 88, "312172": 80, "312315": 66, "3125": 88, "312652": 92, "312769e": 77, "312873": 76, "313044": 138, "313209": 78, "313535": 91, "31378": 62, "313796": 77, "313851": 66, "313870": 83, "314": 122, "3141": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 60, 62, 63, 70, 86, 97, 99, 104, 106, 107, 108, 123, 138, 152], "314247": 95, "314309": 76, "3146": 30, "314625": 77, "31476": [87, 88], "314781": 80, "315": 154, "315031": 95, "315155": 77, "315276": 66, "315290": [80, 81], "315648": 69, "316": [62, 154], "316193": 91, "316258": 67, "31633": 88, "316407": 108, "316717": [76, 77], "316772": 66, "317394": 65, "317487": 91, "317607": 91, "317946": 72, "318": [62, 154], "318134": 66, "318571": 138, "318753": [80, 81], "319": [62, 154], "319100": [80, 81], "31910229": [108, 113], "319197": 69, "319217e": 66, "319420": 83, "319634": 88, "319750": 16, "319759": 91, "319812": 69, "319850": 91, "319903": 64, "319965": 76, "32": [38, 60, 61, 62, 66, 68, 69, 76, 77, 85, 86, 87, 88, 96, 106, 107, 108, 110, 122, 123, 138, 152], "320": 88, "320000e": 88, "320314": 87, "320633": 78, "321": [89, 154], "321052": 66, "321241": 69, "321673": 138, "321954": 66, "322403": 66, "322404": 94, "32241444": 69, "322536": 69, "322764": 77, "322991": 76, "323494e": 122, "3235": 88, "323636": 87, "32366503": [108, 111], "323679": 86, "323685": 66, "324": [61, 154], "324186": 66, "32458367": 60, "3245837": 91, "325046": 88, "325056": 91, "3251347": 122, "325370": 72, "325657": 76, "32570223": 122, "325706": 69, "325819": 88, "326": 92, "3264": 72, "326463": 69, "326740": 91, "32674263": 68, "326871": 95, "3268714482135149": 95, "32695418": 66, "327": [88, 154], "327121": 16, "327578": 77, "32761819": 69, "32789": 88, "327958": 73, "327997": 76, "328367": 88, "32875335": 68, "328884": 76, "32902058": 69, "329348": 69, "329423e": 122, "32950022e": 122, "329671": 76, "329743": 69, "329850": 72, "3299": 72, "329967": 77, "33": [60, 61, 62, 66, 69, 76, 77, 86, 87, 88, 106, 107, 108, 122, 138, 152, 153, 155], "330": 154, "3300": [61, 87], "330000e": 88, "330285": [76, 77], "3304269": 60, "330615": 91, "33065771": 91, "330658": 91, "330669e": 66, "330731": 29, "330812": 66, "330822": 89, "330910": 69, "330999": 69, "331161": 77, "331215": 85, "331521": 91, "331602": 88, "33168943": 88, "33175566": 91, "331756": 91, "331898": 66, "331913": 80, "332554": 66, "332782": 29, "3329": 88, "332996": 86, "333000": 76, "333250": 88, "3333": [58, 60, 75, 106, 107, 108], "3333333": 62, "33333333": [66, 68, 69], "33335939e": 122, "3334": [72, 88], "333581": 87, "333882": 76, "333914": 72, "333947": 77, "33396226": 66, "333973": 77, "334": 61, "334717": 72, "334750": 78, "335": 89, "335446": 73, "335609e": 91, "335846": 91, "335869": 106, "336": 154, "33613": [108, 109], "336395": 69, "336461": 88, "336498": 91, "336612": 65, "336716": 76, "336870": 67, "3371": 88, "3376": 59, "337986": 77, "338": 94, "33849": 88, "3386": 122, "338775": 78, "338855": 88, "338890": [66, 77], "338908": 78, "33908356": 66, "339268": 91, "339269": 94, "339553": 77, "339570": 91, "339762e": 88, "339875": [80, 81], "34": [58, 59, 60, 61, 62, 63, 66, 68, 69, 76, 77, 86, 87, 89, 93, 94, 106, 107, 108, 122, 138, 155], "340": [61, 88], "340142": 108, "340217": 67, "340235": 83, "340274": 94, "340712": 88, "340844": 37, "341106": 76, "341336": 27, "342117": 83, "3422": 88, "342214e": 77, "342362": 73, "342675": 60, "34287815": 94, "342921": 69, "342992": 86, "343": [92, 108, 154], "34310054": 69, "34336123": 69, "343520": 66, "343639": 106, "34375": 87, "344128": 69, "344212": 155, "344368": 76, "344440": 106, "344463": 77, "34450402": 68, "344505": [87, 88], "344579": [108, 111], "344640": 91, "344748": 90, "344787": [76, 77], "344834": 65, "344845": 77, "345": 154, "3454": 88, "345903": 91, "346107": 87, "346206": 91, "346238": 94, "346274": 76, "34644161": 69, "346678": 90, "346683": 78, "3466832975109777": 78, "347213": 86, "347310": 29, "347770e": 77, "34778763": 67, "347793": 77, "348": 92, "34813348": 69, "34817475": 67, "348338": 88, "348617": 91, "348622": 92, "348697": 78, "3486970271639334": 78, "34876395": 67, "349213": 68, "3492131": 59, "349461": 77, "349611": 66, "34967621": 60, "349747": 66, "349772": 81, "34980811": 84, "34983702": 67, "349867": 77, "349900e": 88, "349970": 69, "34m": 107, "34mglmnet": 107, "34mmlr3": 107, "34mmlr3learner": 107, "34mmlr3pipelin": 107, "34mranger": 107, "34mrpart": 107, "35": [61, 62, 66, 69, 76, 77, 78, 80, 86, 87, 88, 91, 106, 107, 108, 122, 138, 139, 145, 155], "3500000000000001": [78, 88, 91], "350165": 107, "35021805": 69, "350317": 69, "350323": 88, "350425": 69, "350491": 76, "350518": 91, "350533": 91, "35053317": 91, "350712": [80, 81], "35077502": [139, 145], "35155": 88, "352": [61, 86], "352149": 69, "352246": 91, "352250e": [87, 88], "3522697": 60, "35292": 88, "352965": 66, "35321374": 69, "353748e": 91, "3538": 59, "354": 88, "354188": 65, "354371": 91, "354529": 77, "3546197197": 122, "354646": 69, "355065": 73, "355184": 69, "35536148": 69, "355562": 68, "355564": 77, "355627": 106, "355768": 66, "3559564": 67, "35595699": 67, "35596577": 67, "35596815": 67, "356136e": 88, "356167": 81, "35620768e": 122, "3566": 88, "3568": 108, "356821": 106, "357": 88, "357228": 69, "35731523": [108, 110], "357450": 76, "357586986897548": 64, "357612": 69, "357654": 88, "358158": [87, 155], "358289": 86, "358395": 94, "3586782": 122, "358690e": 88, "35871235": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "358787": 138, "359": [92, 155], "359050": 77, "359083": 72, "359229": 73, "3593": [84, 94], "359307": 77, "359413": 69, "359446": 69, "3598": 88, "35th": 153, "36": [61, 62, 66, 68, 69, 72, 76, 77, 86, 87, 106, 107, 108, 122, 138], "360004": 91, "360054": 138, "360249": 82, "360475": [76, 77], "360683": 78, "360801": 78, "360965": [100, 104], "361": 92, "361220": 106, "361374": [108, 111], "361623": 83, "3619201": 33, "36213": 88, "36231307e": 122, "362398": 25, "362518": 66, "362760": 88, "363103": 78, "3631031251500065": 78, "363215": 76, "363276": 60, "363555": 66, "363576": 85, "363771": 76, "363990": 76, "364276": 85, "3643": 138, "364340": [108, 111], "364526": 69, "364595": 60, "3647": 62, "364800": 91, "365": 89, "365275": 72, "365509e": 76, "365527": 88, "36557195e": 122, "36566025e": 122, "365702": 76, "366088": 69, "366127": 69, "366188": [100, 104], "3663": 72, "366718627": 60, "366912": 76, "366943": 69, "36696349": [108, 113], "367": 92, "367017": 85, "367225": 76, "367323": 91, "367366": 83, "367425": 72, "367571": 78, "367625": 91, "367697": 77, "368": [66, 68, 69, 88], "368152": 86, "3682": [61, 87, 88], "368324": 86, "368577": 106, "369212e": 77, "369556": 78, "3696": 94, "369796": 91, "369869": 87, "369981": 86, "36m": 107, "37": [61, 68, 72, 76, 77, 83, 84, 86, 87, 106, 107, 108, 122, 138, 155], "370000e": 88, "370165": 88, "370254e": 87, "3702770": 60, "370736": 86, "3707775": 60, "3711317415516624": 78, "371132": 78, "3712": 88, "371357": [87, 88], "371429": 78, "37191227": [108, 111], "371972": 26, "372": 153, "37200": [87, 88], "372094": 66, "372097": 78, "3722": 88, "3724": 88, "372483": 69, "372555": 77, "372726": 69, "3727679": 60, "372844": 66, "373034e": 37, "373235": 88, "373451": 106, "373783": 76, "3738573": 60, "373941e": 76, "374335": 91, "37433503": 91, "3745": 88, "374812e": 88, "375081": 88, "375151": 76, "375403": 64, "375465": 91, "375776": 76, "375995": 85, "376399": [108, 111], "3766": 83, "376617": 83, "376681": 77, "376750": 69, "376972": 77, "377147": 106, "377246": 88, "377311": 91, "377572": 77, "377955": 91, "378161": 67, "378367": 91, "378471": 76, "378596": 86, "378834": 91, "3788859": 60, "379": 153, "379614": 91, "379892": 76, "38": [62, 76, 77, 87, 89, 106, 107, 108, 122, 138, 155], "3800694": 60, "380442": 76, "380837": [87, 88], "381": 92, "381072": 91, "381247": 88, "381278": 88, "381684e": 87, "381685e": 88, "381689": 91, "382024": [100, 104], "382285": 88, "382872": 78, "382919": 16, "382948": 66, "383297": 91, "383426": 69, "38348": 88, "383641": 68, "384": 88, "384189": 85, "38420547": 68, "384208": 85, "384677": 73, "38470495": [108, 113], "384717": 76, "384755": 77, "3848": 88, "384883": 88, "385": 57, "3852": 72, "385226": 138, "385353": 66, "3855080": 122, "385917": 86, "386": [62, 88], "386102": 78, "386428e": 77, "386498": 69, "386502": 88, "386616": 77, "386831": 73, "387": 62, "3871": 59, "387426": 91, "387673": 76, "387708": 66, "387780": 91, "388005": 88, "388071": 91, "388185": 73, "38818693": 122, "388216e": 107, "38839915": 69, "38898864": 91, "388989": 91, "389": 62, "389186": 108, "389261": 66, "389489": [108, 109], "389588": 69, "389603": 76, "38973512e": 122, "38983785": 64, "38990574": 122, "389915": 66, "39": [57, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 72, 73, 76, 77, 82, 84, 86, 87, 88, 94, 95, 96, 106, 107, 108, 122, 138, 155], "39007": 30, "39010121e": 122, "390155": 85, "390304": [108, 111], "390312": 77, "390379": 91, "390495": 77, "391377": 95, "391389": 72, "39186467": 84, "392242": 82, "392472": 88, "39255938": [108, 111], "392623": 77, "392801": 76, "392833": 94, "392864e": [87, 88], "393441": 77, "393604": 78, "393654": 73, "39422054": 69, "39425708": 60, "394527": 68, "394757": 85, "395076e": 88, "395195": 86, "395268": 106, "39545153": 68, "395480": 72, "395558": 76, "3958": 108, "396": 108, "3961": 88, "39611477": 61, "39621961e": 122, "396272": 85, "396531": 88, "396671": 76, "396835e": 77, "39690776": 68, "396985": 86, "396992": [76, 77], "397140": 78, "39727": 88, "397313": 59, "39747241": 69, "397578": 82, "39761734": 69, "397716": 69, "397811": 94, "3979": 68, "398": [99, 104, 152], "398000e": 88, "398166": 73, "3985": 88, "398770": 91, "398999": 24, "399": 61, "399056": 91, "399223": 65, "399355": 65, "399470": 72, "399679": 108, "399692": 91, "399858": 95, "39m": 107, "3cd0": 62, "3dx_1": [78, 91], "3e1c": 62, "3ec2": 62, "3f5d93": 89, "3x_": 91, "3x_4": [78, 91], "4": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 36, 37, 38, 39, 45, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 139, 145, 152, 154], "40": [60, 63, 64, 76, 77, 78, 87, 88, 91, 96, 99, 104, 106, 107, 108, 122, 138, 139, 145, 155], "400": 86, "4000": [66, 69], "4000000000000001": 107, "40000000000000013": [78, 88, 91], "400069": 68, "400113": 83, "40029364": [139, 145], "400297": 77, "400587": 85, "400666": 66, "400823": 91, "400905": 73, "401": [10, 155], "401009": 85, "40117934": 69, "401247": [38, 123, 138], "40127723e": 122, "401606": 66, "401931": [80, 81], "402": [99, 104], "402077": 88, "402100": 138, "402233": 69, "402301e": 107, "402539": 66, "403": 92, "403113": 78, "403113188429014": 78, "40312587": 68, "4034241": 68, "403425": 91, "40359107": 64, "403626490670169": 97, "4036264906701690": 97, "403626491": 97, "403771948": 123, "4039": 59, "40415035": 68, "404300": 73, "404318": 59, "40452": 88, "404550": 90, "404824": 88, "405000": 88, "405203": 65, "405313": 76, "405677": 72, "40583": 59, "40586967": 69, "405890": 29, "405892": 76, "406": 57, "406085": 106, "406107": 66, "406285": 91, "406446": 78, "4065173": 122, "406565": 76, "406756": 77, "40676": 59, "407": 92, "407089": 69, "407317": 89, "40732": 83, "407509": 69, "40756483": 69, "407758": 16, "407786e": 76, "40781584": 68, "408000": 77, "408135": 76, "408476": [139, 145], "40847623": [139, 145], "408539": 91, "408565": 91, "408682": 76, "409154": 59, "4093": 94, "409390": 88, "409395": 91, "409532": 76, "409551": 77, "409746": 78, "409848": [76, 77], "41": [76, 77, 87, 88, 106, 107, 108, 122, 138], "41008867": 68, "410095e": 77, "410393": 78, "410481": 69, "410681": 65, "4107": 72, "410795": 86, "41093655": 122, "411190": [76, 77], "411264": 72, "411295": 91, "411304": [76, 77], "411358": 69, "411582": 91, "411977": 72, "412": 88, "412127": 91, "412304": 95, "412406": 88, "412477": 65, "412603": 68, "412653": 86, "412714": 78, "412864": 80, "412919": 77, "412947": 76, "413": 88, "41306066": 69, "41336": 107, "413376": 108, "413409": 69, "41341040": 60, "413784": 88, "414": 92, "414005": 69, "4142233": 122, "41428195": [108, 111], "415040": 77, "41525168e": 122, "415294": 91, "415556": 106, "41566": 108, "41581151": 68, "415812": 155, "416052": 73, "41637957": 68, "4166": 88, "4166667": 62, "416709": 77, "416737": 85, "416899": 76, "416e": 92, "417502": 72, "417619": 77, "417669": 88, "417727": 87, "417767": [80, 81], "417834": 73, "417859": 76, "41798768e": 122, "417988": 91, "418052": 76, "418056": 91, "41805621": 91, "418168e": 66, "418400": 83, "418591": 88, "41865738": 68, "418741": 73, "418806e": 78, "418969": 106, "41918406e": 122, "419371": 91, "419871": 73, "41989983e": 122, "4199952": 60, "41e5": 62, "42": [12, 15, 16, 35, 63, 64, 65, 66, 68, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 88, 90, 91, 94, 95, 96, 101, 104, 106, 107, 108, 109, 110, 111, 113, 122, 138, 153], "420308e": 88, "42035525": 69, "420467": 69, "42048682": 69, "42073312": 60, "42094064": [108, 113], "420967": 78, "421083": 59, "4211349413": 60, "421200": 95, "421357": [80, 81], "42143": 88, "421433": 68, "421576e": 88, "421793": 94, "421919": 88, "422007": 94, "422116": 88, "422119": 88, "422251": 77, "422325": 78, "422646e": 77, "422984": 16, "42303": 68, "42337951": 68, "423580": 66, "4235839": [80, 81], "42373553": 68, "423951": 59, "424108": 78, "424127": 122, "42412729": 60, "424328": 91, "424573": 85, "424651": 108, "424682": 76, "424717": 78, "424748": 95, "424879": 88, "42498489": 69, "425": [57, 86, 155], "425000e": 88, "425103": 59, "4252": 72, "425325": 83, "425414": [100, 104], "425493": 59, "42550": 88, "425636": 83, "42592885": 68, "426055": 59, "42636971": 68, "426540": 86, "426540301": 60, "426734": 85, "426736": 88, "427": 88, "427486": [76, 77], "42755087": 94, "427551": 94, "427573": 86, "42758841": 69, "427654": 83, "427725": 91, "428": [138, 155], "428046": 90, "42811700": 155, "428191": 76, "428255": 91, "428411": [87, 88], "428467": 91, "4284675": 91, "428766": 76, "428771": 29, "428779": 77, "4290": 59, "429133": 85, "429141e": 76, "429298": 85, "429309": 88, "42931855": 68, "42934105": 96, "42ba": 62, "43": [61, 68, 73, 76, 77, 88, 106, 107, 108, 122, 138], "430298e": [87, 88], "430345": 85, "430465": [108, 113], "4305986": 122, "4311947070055128": 107, "431306": 91, "431437": 94, "43159775": 69, "431605": 76, "431701914": 150, "431747": 77, "431929": 88, "431998": 73, "4320": 72, "4320612": 122, "432130e": 87, "432300e": 91, "43231359e": 122, "432707": 92, "43294": 62, "432f": 62, "433": [62, 92], "4330": 88, "433134": 66, "433221": 78, "433418": 69, "433473": 88, "43361991": 91, "433620": 91, "433630": [108, 111], "433684e": 76, "4339": 59, "434054e": 83, "434116": 76, "434487": 69, "434535": 91, "43453524": 91, "4345792": 122, "434698e": 76, "4347": 72, "43482": 88, "434871": 69, "435": 62, "43503345": 108, "4352": 72, "435232": 66, "435401": 86, "4357": 88, "435790": 69, "435927": 88, "435967": 86, "436": [62, 88], "43604985": 68, "436327": 88, "4364": 72, "436588": 69, "436764": 92, "436806": 88, "437": 155, "437013": 66, "437134": 88, "437601": 77, "437667": 87, "437924": 88, "438": 86, "438103": 66, "438219": 91, "438569": 88, "43883": 81, "438960": 86, "43907861": 122, "439446": 66, "439541": [87, 88], "439675": 92, "439699": 73, "43971471": 69, "43989": 24, "43f0": 62, "44": [68, 73, 76, 77, 106, 107, 108, 110, 122, 138], "440170": 76, "440179": 69, "440320": 88, "440524": 76, "440605": 107, "440892e": 69, "440a": 62, "441004": 88, "441153": 91, "441209": 91, "441231": 66, "44124313": 108, "4413": 122, "441311": [69, 80], "441528": 76, "4416552": 60, "442172": 66, "4425106": 122, "442839": 69, "442847": [108, 109], "443016": 78, "443032": 87, "44312177": 61, "443672": [108, 113], "443686": 91, "4438": 88, "443802": 69, "443e": 92, "444046": 88, "44411636": 122, "4444": [58, 60, 75, 108], "444500": [87, 88], "444805": 85, "445": 92, "445241": 69, "44563945e": 122, "445816": 76, "4460": 72, "446023": 90, "446197": 68, "4462": [62, 72], "44647451": 94, "44713577e": 122, "447624": [76, 77], "447706": 78, "447863": 78, "447863428811719": 78, "447897": 76, "448": 88, "448070": 77, "448319": 68, "448587": 78, "4487": 88, "448745": 91, "44890536": 122, "448923": 82, "449150": 29, "449345": 77, "449677": 83, "449790": 66, "44fa97767be8": 62, "45": [64, 76, 77, 78, 82, 85, 88, 91, 106, 107, 108, 122, 138], "4500": 87, "45000000000000007": [78, 88, 91, 107], "450000e": 88, "450097": 77, "450152": 86, "4505648": [108, 111], "4507": 67, "450870601": 60, "4509": 72, "451": 57, "4510": 72, "452": 62, "452091": 88, "452114": 106, "452150": 69, "452488701": 60, "452489": 86, "452501": 66, "452622": 76, "453": 62, "4535": 88, "453562": 66, "4536": [88, 122], "4539": 62, "454093": 69, "454129": 16, "454243": 69, "454406": 73, "45451": 88, "45467447": 122, "454810": 69, "455": 62, "455078": 78, "455107": 78, "4552": 62, "455262": 77, "455293": 78, "4552b8af": 62, "455448": 94, "455564": 16, "455635": 66, "455672": 88, "45594267": [108, 111], "455981": 139, "456043": 69, "456225": 91, "456326": 66, "456370": 86, "456453": 16, "4566031": 138, "45660310": 138, "456617": 91, "4567": 94, "456892": 78, "457088": 91, "457245": 69, "45762811": 68, "457843": 69, "457860e": 69, "458114": 88, "458196": [108, 111], "458307": 140, "4584": 88, "458420": 88, "4584447": 60, "458814": 83, "458855": 61, "459098": 76, "4592": 60, "459200": 86, "459383": 78, "45957837": 122, "459617": 16, "459760": 88, "459812": 78, "45989179": 69, "46": [57, 68, 72, 76, 77, 82, 106, 107, 108, 122, 138], "4601": 88, "460207": [76, 77], "460218": 78, "460289": 91, "460385": 76, "460744": 87, "4610": 155, "461138": 69, "461162e": 77, "461396": 85, "461412": 106, "461469": 67, "461493": 85, "461629": 95, "462319": 88, "462451": 78, "462473": 77, "463046": 77, "463208": 88, "46328686": [108, 111], "463325": 91, "4634": 88, "463406": 69, "463418": 95, "463766": 81, "46380000": 122, "463803": 77, "463816": [108, 113], "463819": 69, "463b": 62, "464076": 78, "4642": 72, "464282": 86, "46448227": 122, "4645": 72, "464537e": 76, "464668": 27, "465": [73, 108], "46507214": 94, "465158": 69, "4651878": 122, "465299": 66, "465476": 72, "465649": 95, "465730": 95, "4659651": 97, "465965114589023": 97, "4659651145890230": 97, "465981": 91, "46598119": 91, "4660": 72, "466047": 91, "46618738": 122, "466421": 66, "466440": 78, "466684": 88, "466756": 91, "46709481": 122, "46722576e": 122, "46747748": 69, "467613": 86, "467613401": 60, "467681": [76, 77], "467770": 78, "467858": 69, "468001": 88, "468035e": 76, "468072": 77, "468075": 91, "46807543": 91, "468406": 88, "468449": [108, 113], "468869": 77, "468907": 73, "468d": 62, "469": 62, "469170": 88, "469379": 88, "469474": 95, "469676": 73, "469825": 78, "47": [57, 61, 72, 73, 76, 77, 80, 87, 94, 106, 107, 108, 122, 138, 154], "470023": 77, "470365": 88, "470467": 69, "4705717": 122, "470801": 76, "470828": 76, "470838": 69, "471": [57, 73], "471435": 92, "471454": [100, 104], "4716": 72, "471666": [100, 104], "4720818": 122, "472255": 88, "472480": 69, "472835": 67, "472843": 68, "472891": 91, "472952": 77, "472e": 62, "473036": 106, "4730897": 122, "473099": 78, "47319": 108, "473263": 66, "474190": 69, "47419634": 152, "474214": [80, 81], "474324": 69, "474699": 77, "474762e": 122, "475395": 88, "475551": 76, "4757550": 122, "475e": 92, "476121": [108, 111], "476304": [108, 111], "476856": 78, "477130": [76, 77], "477150": 91, "477354": 88, "477357": 92, "477395": 88, "477474": 86, "47759584": 122, "4776": 72, "47782": 88, "478399": 88, "47857478": 122, "478668": 77, "47878617": 69, "478794e": 77, "4788": 72, "478896": 88, "479119": 77, "479288": 16, "479527": 77, "47956571": 69, "47966100e": 122, "479722": 77, "479789": 69, "479876": [80, 81], "479882": 77, "479928": 91, "479959": [108, 113], "47be": 62, "48": [57, 62, 73, 76, 77, 81, 87, 88, 89, 106, 107, 108, 122, 138], "480": 73, "480000e": 88, "480133e": 91, "480199": 83, "48029755": 94, "480408": 66, "480579": 77, "48069071": [97, 123, 138], "480691": [38, 123, 138], "480800e": 91, "4809916": 122, "4810129": 122, "481071": 67, "481172": 91, "48138": 68, "481399": [87, 88], "4814": 84, "481705": 108, "481713": 83, "481761e": 88, "482": [62, 73], "482038": 78, "482070": 69, "4822": 72, "482349": 80, "482363e": 89, "482461": [139, 145], "48246134": [139, 145], "482483": 91, "482508": 85, "482624": 66, "482790": 65, "48296": 94, "483": [92, 108], "48315": 94, "483165": 76, "483186": 65, "483192": [87, 88], "48331": 94, "483357": 77, "483531": 91, "48353114": 91, "483642": 88, "483711": 91, "483717": 78, "483855": 88, "48390784": 108, "484": 88, "48404": 60, "484074": 77, "484111e": 76, "4842": 88, "484272e": 77, "484640": 91, "48489102": [108, 111], "4849": [62, 72], "485": [62, 88], "485348": 89, "485368": 77, "485468": [100, 104], "48550": 95, "485617": [87, 88], "48583": [87, 88], "485871": 81, "485953": 66, "486": [40, 88], "4860": 72, "48617524": 69, "486202": 78, "486342": 88, "486532": 91, "486981": 69, "487": [73, 88], "487053": 69, "487352": 67, "487467": 88, "487524": 83, "487641e": 91, "48767460": 122, "487872": 74, "487934": 16, "488455": 85, "488662": 68, "488759": 76, "488811": 91, "488866": 76, "488893": 69, "488909": [87, 88], "488982e": 78, "489054": 90, "489488": [108, 113], "4895498": 91, "489550": 91, "489567": 93, "489699": 78, "49": [62, 73, 76, 77, 106, 107, 122, 138], "490070931": 60, "490488e": [87, 88], "490689": 85, "490827": 69, "490896": 73, "490898": 88, "490941": 88, "49098": 94, "491105e": 69, "491114": 69, "491245": 86, "491249": 85, "4915707": [108, 110], "492": 88, "492020": 69, "492033": 66, "492075": 76, "492243": 69, "4923156": 97, "49231564722955": 97, "492315647229550": 97, "492410": 91, "492417e": 108, "492626": 82, "49270769e": 122, "493": [92, 108, 153], "4931": 88, "493144": 95, "493195": 83, "493219": 91, "493426": 92, "493650e": 69, "493792": 85, "494": 92, "494253e": 76, "494324": 86, "494324401": 60, "495": 90, "495028": 77, "49530782": 60, "495405": 91, "495657": 78, "495752": 91, "49596416e": 122, "496": 90, "496074": 76, "496300e": 88, "496401": 69, "49650883": 94, "496551": 91, "496573": 69, "496591": [108, 113], "4966886": 122, "496714": 95, "496777": 155, "49693": 107, "497": 90, "4970": 122, "49712298": 69, "497395": 89, "4976": 72, "497915": 76, "497964": 106, "498": [88, 90], "498122": 83, "498802": 89, "498921": 91, "498979": 88, "498992": 66, "498f": 62, "499": [90, 99, 104, 152], "499000e": [87, 88], "499416": 77, "499563": 66, "499857": 69, "49d4": 62, "4a53": 62, "4b8f": 62, "4dba": 62, "4dd2": 62, "4e": [60, 61], "4ecd": 62, "4fee": 62, "4x": 91, "4x_0": [32, 76, 77, 80, 81], "4x_1": [32, 76, 77], "5": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 113, 122, 123, 131, 138, 139, 145, 151, 152, 154], "50": [29, 60, 62, 63, 65, 69, 72, 73, 78, 81, 84, 85, 87, 88, 89, 91, 106, 107, 122, 138], "500": [7, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 37, 38, 39, 42, 48, 58, 62, 66, 67, 68, 69, 70, 75, 76, 77, 80, 81, 84, 87, 90, 92, 94, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 113, 123, 138, 139, 145, 152, 155], "5000": [45, 66, 67, 68, 69, 76, 77, 78, 91, 93], "50000": 86, "500000": [87, 88], "5000000000000001": [78, 88, 91], "500000e": 88, "500084": 91, "500267": 82, "5003517412": 60, "500635": 88, "50080779": 69, "50093148e": 122, "501021": 88, "501198": 89, "501203": 85, "501403": 91, "502005": 106, "502084": 108, "502268": 88, "502494": 78, "5025850": 60, "502605": 89, "502612": 91, "502845": 69, "502942": 69, "502995": 91, "503089": 91, "503226": 77, "503504": 107, "50355018": 69, "503700": 73, "503768": 77, "50398782e": 122, "504076": 77, "504286": 86, "5042861": 60, "504786": 66, "504913": 69, "5050973": 60, "505795": [108, 111], "506274": 66, "506544": 69, "50672034": 60, "506736": 106, "50679256": 69, "506833": 76, "506900e": 91, "506903": 78, "507": 92, "507285": 83, "507683": 77, "50768b": 89, "508068": 76, "508153": 90, "508406": 90, "508433": 76, "508459": 86, "508756": [100, 104], "5089": 88, "508947": [15, 108, 110], "508988": 76, "509005": 76, "509059": 88, "509102": 88, "509196": 91, "509461": 91, "509651": 85, "50967": 95, "5097": 95, "5098": [70, 99, 104, 152], "509853": 91, "5099": [62, 70, 99, 104, 152], "509951": 78, "509958": 86, "51": [59, 61, 62, 68, 73, 80, 106, 107, 122, 138, 154], "510000e": [87, 88], "5101": 72, "510385": 86, "510555": 73, "510586": 91, "51079110": 60, "510971": 85, "5109914": 122, "5114651322": 122, "5115547": 97, "5115547181877": 97, "51155471818770": 97, "511668": 95, "5116683753999616": 95, "511722": 37, "5118": 72, "511856": 66, "512": 86, "512108": 91, "512149": 91, "51214922": 91, "51243406e": 122, "512519": 86, "512566": 77, "512572": 91, "512657e": 88, "512672": [108, 139, 145], "5131": 87, "513222": 83, "513409": 69, "5135": 72, "513658": 88, "513992": 91, "514": 62, "514024": 76, "514179": 76, "514550": 66, "51494845": [108, 113], "514957": 77, "515208": 77, "515358": 78, "5154": 88, "5154782580830215": 86, "5155": 62, "516": 62, "516125": 78, "516145": 88, "516189": 66, "516222": 91, "516255": 91, "516256": 91, "516339e": 69, "516528": 91, "5166": 72, "517": [62, 86], "5175": 88, "517510812139451": 64, "517798": 73, "518104": 66, "518175": 86, "518478": 73, "518517": 30, "518610": 108, "518616": 77, "518846": 86, "518854": 73, "518978": 85, "519598": 69, "519637": 76, "51966955": 60, "519710": 91, "519828": 77, "519898": 76, "52": [59, 62, 82, 85, 92, 106, 107, 122, 138], "520": 88, "520012": 76, "520096": 85, "520406": 76, "520641": 94, "520930": 78, "521002": 78, "521094": 67, "521104": 88, "521233": 73, "5213": 72, "521357": 77, "521608e": 76, "522395": 69, "522398": 85, "522406": 69, "522605": 88, "522835": 65, "523030": 95, "523140": 77, "523163": 78, "52343523e": 122, "523612": 69, "523794e": 91, "523977545": 60, "524215": 88, "52424539": 60, "524657": 91, "524817": 85, "524934": [76, 77], "525064": 73, "52510803": 61, "5251546891842586": 95, "5255": 62, "525718": 68, "52590": [61, 87], "526": 86, "526125": 76, "526413": 66, "526532": 88, "526582": 83, "526769": [76, 77], "527141": 76, "52732": 107, "527644e": 91, "527656": 66, "5277": 72, "528000e": 94, "528145": 77, "528272e": 77, "528381e": 96, "528568": 66, "528573": 72, "528639": 66, "528725": 91, "528937": [80, 81], "528996901": 60, "528997": 86, "529": 86, "529163e": 122, "529178": 76, "529405": 59, "529468": 106, "529576": 67, "529692": 77, "529782": 59, "529960": 69, "53": [59, 62, 73, 82, 86, 99, 104, 106, 107, 108, 110, 122, 138, 150, 153], "5301": 88, "530659": 82, "530830": 88, "530940": 91, "53094017": 91, "531": 62, "531030": 77, "531223": 78, "531515": 106, "531594": 88, "531619e": 69, "531642": 69, "531682": 72, "531999": 88, "53209683": 108, "532202e": 30, "532266": 78, "532421": 80, "53257": 107, "532671": 122, "532738": 91, "53273833": 91, "5328": 88, "533": 92, "533283": 108, "533316": 88, "533489": 65, "533593": 69, "533871": 88, "533900": 91, "534139": 73, "5346": 62, "534759": 76, "535105": 69, "535179": 91, "535318": 91, "53606675": 91, "536067": 91, "536636": 66, "536792": 88, "536798e": [87, 88], "537438": [108, 109], "537681": 88, "53781638": 69, "53791422": [96, 108], "538": 62, "5382": 94, "538227": 77, "538282": 88, "538410": 72, "53849791": 91, "538498": 91, "538513": [100, 104], "538937": [87, 88], "539455": 91, "539491": [80, 81], "539767": 78, "54": [59, 61, 62, 72, 85, 87, 92, 98, 106, 107, 122, 138, 154], "540101": 91, "54010127": 91, "54014493": [108, 111], "540240": 88, "540375": 83, "5405": 83, "540549": 83, "540578": 85, "5408": 59, "541": 92, "541010": 91, "541060": 83, "541159": 91, "54119805": 64, "54163": 94, "5416844": 97, "541684435562712": 97, "542": 92, "542268": [108, 109, 113], "5424": 72, "542446": 81, "542451": 91, "542560": 95, "542647": 91, "542648": 106, "542671": 86, "542754": 77, "542762203": 122, "542769": 91, "542883": [139, 145], "5428834": [139, 145], "542919": 106, "542989": 91, "543": [86, 88], "543014": 88, "543052": 73, "543075": 78, "543136": 78, "543380": 86, "5434231": 97, "543423145188043": 97, "5436005": 60, "5436876323961168": 78, "543688": 78, "543734": 88, "54373574": [108, 111], "543764": 81, "54378": 94, "544097": 91, "544276": 77, "544383": 95, "544555": 86, "544680": 77, "54483": [123, 138], "5448331": [123, 138], "545026": 69, "54517706e": 122, "545492": 73, "54550506": 92, "545605e": 91, "545672": 77, "545930": 106, "546146": 69, "546223": 76, "546260": 85, "546438": 88, "546730": 69, "546928": [108, 111], "546967": 76, "547101": 66, "54716": 94, "547257": 77, "547306": 78, "5473064979425033": 78, "547482": 72, "5475": 88, "547794": 16, "5479": 88, "547909": 88, "548": 57, "548841": 68, "5494": 88, "55": [61, 62, 78, 85, 87, 88, 91, 106, 107, 122, 138], "5500000000000002": [78, 88, 91], "550156": 68, "550176": 85, "550275": 68, "550472": 77, "551010e": 88, "551317": 77, "551686": 78, "55176": 107, "551791": 69, "551793": 66, "552081": 85, "552240": 69, "552632": 66, "552727": 86, "552776": 91, "553004": 73, "55307": 107, "553266": 66, "553322886": 122, "553672": 88, "553754": 106, "553878": 28, "554076": 78, "554793e": 122, "554937": [108, 111], "554986": 78, "554986206618521": 78, "555": 86, "5550": 72, "555397": 66, "555445": 90, "555498": 91, "5555": [58, 75], "555544": 76, "55589": 68, "555954": 88, "556191": [76, 77], "55642562": [108, 111], "556533e": 88, "556792": 91, "55690745": 122, "5574dcd4": 62, "557595": 86, "557741": 85, "557999": 86, "558134": [76, 77], "55820564": [108, 111], "55828259": 96, "558387": 77, "5584": 86, "5585": 86, "55855122": 64, "55863386": 108, "558655": 78, "5589": 86, "558996": 88, "559": 155, "5590": 86, "559144": 78, "559186": 78, "5592": 86, "559394": 91, "559522": 91, "559680": 88, "559712": [100, 104], "559844": 77, "55dc37e31fb1": 62, "55e": 61, "56": [62, 72, 98, 106, 107, 122, 138, 150, 153], "560135": [38, 123, 138], "56018481": 91, "560185": 91, "560250": 77, "560440": 72, "560689": 59, "560723": 82, "5610": 67, "561309": 88, "5614534": [108, 111], "561518": 89, "5616": 87, "561652": 77, "561785": [15, 108, 110], "562013": 91, "56223": 94, "5622446": 122, "562288": 95, "562317e": 76, "562320e": 76, "562390": 106, "562556": 68, "5625561": 59, "562712": [76, 77], "562866": 88, "563067": 92, "563123": 66, "563374e": 78, "56338": 89, "563456": 106, "563503": 91, "563563": 83, "563673": 88, "563690": 88, "563760": 88, "5638": 88, "56387280e": 122, "56390147e": 122, "564": 108, "56403823": [108, 111], "564045": 91, "564060": 69, "564073": 88, "564212e": 66, "564232": [76, 77], "564244e": 122, "56425415": [108, 111], "5644": 72, "564451": 77, "564577": 88, "564610": 76, "564800e": 88, "564829": 78, "5650": 72, "565066": 78, "565589": 69, "565915": 88, "566": 95, "566024": 91, "56611407": [108, 111], "566600": 91, "566642": 69, "56670073": 91, "566701": 91, "566723": 66, "566964": 76, "567004": 94, "567339": 69, "567491": 77, "567695": 76, "567945": [80, 81], "568": 57, "568004": 89, "568071": 88, "568143e": 76, "568287": 83, "56876517": [108, 111], "568932": [108, 113], "569449": 106, "569815e": 69, "569911": 60, "5699994715": 60, "57": [62, 92, 106, 107, 122, 138, 155], "57015458": [108, 111], "5702": 72, "570278": 76, "570486": 59, "570562": 59, "570722": 152, "5708": 88, "571040": 77, "571429": 78, "5714294154804167": 78, "571778": 59, "5718": 88, "5722": 87, "57245066": 91, "572451": 91, "572717": 93, "572730": 72, "5732": 72, "573349e": 77, "573679": 16, "573689": 72, "573700": 65, "574": 62, "574050": 76, "574160": 83, "574596": 66, "5748": 107, "574956": 69, "57496671": 60, "575": 11, "575218": 69, "575329": 72, "575381": 87, "575423": 16, "575513": 66, "57572422": 94, "57585824": 94, "57592948e": 122, "57599221": 94, "575e": 92, "576": 62, "576207719": 122, "5763996": 60, "57643609": 94, "576879": 86, "577": 62, "5770": 87, "577071": 76, "57715074": 60, "577210e": 77, "577271": 86, "57751232": 88, "577634": 76, "5776971": 94, "57775704": 94, "577807": [76, 77], "577884": 88, "577922": 69, "577e": 92, "578081": 88, "578307": 91, "57843836": [108, 111], "578557": 77, "578847e": 78, "579125": 87, "5791422243134384859657079828384929899": 122, "57914222431343848596570798283849298998112123253233434445465664677276889094976121718273539404149515460626668879396100123413162947525355586369737581868995": 122, "57914935": 61, "579197": 83, "579213": 95, "579238": 78, "579322e": 87, "579521": 90, "57e": 61, "58": [31, 61, 87, 95, 106, 107, 122, 138, 154], "5800": 88, "58000": 87, "5804": [62, 88], "580414": 95, "580611": 66, "580751": 87, "580831": 77, "580922": 80, "581231": 69, "581880": 85, "582031": 87, "582290": 77, "582331": 88, "58241568": 122, "5825085": [108, 111], "582705e": 76, "582754": 93, "582761": 78, "582803": 76, "582854": 66, "582977": 68, "582991": 87, "583076": 88, "583140": 77, "583195": [76, 77], "5833333": 62, "583534": 91, "583969": 76, "5840": 72, "584309": 66, "584742": 81, "584849": 78, "5852": 88, "585394": [108, 109], "585412": 76, "585426": 122, "585479": 83, "585513": 77, "585793": 78, "585895": 66, "586362": 91, "5864": 59, "586423": 76, "5867": 88, "5868472": 60, "586962e": 77, "587135": 77, "587550": 69, "587594": 88, "58775574": 69, "587801": 72, "58812": 107, "5882": 87, "588364": 24, "588396": 78, "5883964619856044": 78, "588824": 76, "589059": 69, "589349": 69, "59": [66, 77, 106, 107, 108, 122, 138], "590": 88, "590098": 78, "590320": 65, "5905": 87, "590530": [108, 111], "590736": 91, "590738": 91, "590813": 91, "590911": 78, "590991": 78, "591080": 65, "591652": 87, "591678": 87, "591679": 77, "59199423e": 122, "592": 57, "592093": 80, "592185": 66, "592681e": 78, "59300411": [108, 111], "59307502e": 122, "5931003": [108, 111], "593637": 77, "593648": 107, "593981": 106, "594": 11, "594316e": 91, "594317": 72, "595353": 78, "59563003": [108, 113], "595987": 77, "596": 88, "596076e": 88, "596270": [80, 81], "596367": 76, "596588e": 77, "597": 61, "597051": 106, "597348": 68, "5979": 72, "597923": 88, "59798": 88, "598149": 69, "59827652": [108, 111], "598371": 88, "59854797": 108, "5985730": 61, "598753": 76, "59908": 69, "59909": 69, "599297": [106, 107, 108], "599334": [108, 113], "599586e": 85, "5cb31a99b9cc": 62, "5d": [78, 91], "5x_2": 65, "5x_3": 65, "5z_i": 91, "6": [11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 99, 101, 103, 104, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 139, 145, 150, 152, 153, 154], "60": [60, 63, 64, 72, 78, 88, 89, 91, 96, 106, 107, 122, 138, 153], "600": 86, "6000": 88, "6000000000000002": [78, 88, 91], "600000e": 88, "600254": 90, "600354": 77, "600580": 67, "600694": 108, "600776": 73, "601": 61, "601061": 78, "60113395": [108, 111], "601598": 86, "601757": 88, "602079": 81, "602168": 78, "602322": 106, "602587": 91, "602628": 78, "602870": 76, "604110": 88, "60416318": 122, "604532": 88, "60458433": [108, 111], "60481177": [108, 111], "604841": [87, 88], "60488374": [108, 111], "605": 88, "60516153": 69, "605195": 81, "606034": 91, "606129": 91, "606342": 78, "60642288": 67, "60648182": 67, "606485": 69, "606727": 76, "606733": 88, "6068": 60, "606800": 86, "606954": 78, "60724018": 67, "60737162": 67, "6075": 155, "60753412": [108, 111], "607618": 91, "60795263": [108, 111], "608": [79, 92], "60853785": 69, "60857": 59, "608818": 94, "60884781": 67, "60884784": 67, "60885899": 67, "6088742": 67, "609": 92, "60905021": 69, "60908297": 69, "609115": 66, "609575": [12, 108, 110], "61": [24, 66, 73, 106, 107, 122, 138, 154], "610053e": 77, "610093e": 76, "610134": 69, "610318": 73, "61051519": 69, "611": 155, "611269": 86, "61170069": 92, "611859": 81, "611995": 88, "612": [57, 92], "612073": 77, "6121718273539404149515460626668879396100": 122, "612246": 83, "612328e": 76, "612554": 66, "612792": 88, "6133": 61, "613391": 77, "613408": 91, "613498": 88, "613574": 82, "613691": [12, 108, 110], "613950": 78, "614": 92, "614003": 66, "614188": 86, "6144": 72, "61458483": 108, "614662": 69, "614716": 77, "615": 73, "615863": [80, 81], "615967": 69, "616086": 88, "616116": 88, "616346": 77, "616372": 87, "61669761": [139, 145], "616698": [139, 145], "616828": 88, "617277": 106, "6173": 62, "617481": 88, "61771229": 92, "617780": 106, "618069": 87, "61810738": 61, "618625": 76, "618722": 88, "618753": 88, "618830": 69, "61899753": 122, "619": 92, "619294": 73, "619351": [76, 77], "619390": [76, 77], "619454": 65, "619613": 87, "61e": [61, 155], "62": [73, 81, 82, 106, 107, 122, 138], "620021": 77, "620026": 91, "620135": 77, "620156": 91, "620267": 77, "620874e": 108, "620995": 96, "62108072": [108, 111], "621094": [87, 88], "621097e": 77, "621275": 90, "621359": 106, "621490": 91, "6215": 87, "621953": 91, "62195343": 91, "621969": 66, "622": [88, 92], "622153": 88, "622272": 73, "6224": 60, "622502": 85, "622948": 66, "623024": 78, "623107": 76, "624": 86, "6240": 94, "6241": 88, "624206": 80, "624320": 16, "624370": 69, "6243811": 60, "624535": 107, "624798": 87, "624894": 106, "624919": 88, "625": [60, 86], "625002": 88, "625159": 82, "625165": 90, "625477": 91, "625891": [80, 81], "626433": 91, "626530e": 76, "626765": 88, "626866": 77, "626998": 72, "627319": 85, "627442": 69, "627505": [80, 81], "627560": 91, "627564": 78, "627754": 72, "627874": 77, "628": 92, "628069": 86, "628213": 16, "628834": 88, "629319": 88, "629835": 88, "629936": 69, "629e": 92, "63": [60, 73, 86, 106, 107, 122, 138, 153, 154], "630150e": 91, "630880": 92, "630914": 82, "631113": 77, "63117311": 108, "631179": 77, "631333": 91, "631762": 77, "6318": [87, 155], "632": 57, "632058": 86, "632407": 72, "63245862e": 122, "632747e": 91, "6330631": 138, "633433": 86, "634": 92, "63407762": 155, "634078": [87, 155], "634095": 69, "634587": 138, "63499": 88, "635000e": [87, 88], "635199": [87, 88], "635318": 66, "635757": 88, "635900": 76, "63593298": [96, 108], "636048": 108, "636331": 67, "636453": 27, "637108": 78, "637240": 88, "637326": 91, "6379": 87, "63817859": 91, "638179": 91, "638264": 91, "638488": 82, "639": 87, "63911972": 69, "639135": 86, "63916605": 61, "639585": 77, "64": [73, 81, 87, 88, 92, 106, 107, 122, 138, 152], "640": 88, "640314": 77, "640432": 76, "641528": 91, "64158767": 69, "64165449": 69, "641959": 66, "642": 92, "642016": 91, "642096": 90, "64223166": 69, "642329": 73, "642352": 69, "642648": [108, 109], "64269": 94, "6427": 88, "642768": 77, "64340": 94, "643512": 78, "643623": 91, "64362337": 91, "643679": [108, 113], "643752": 91, "643939": 73, "643988": 76, "644113": 106, "64444268": 69, "644665": 78, "64476745e": 122, "644799": 65, "644963": 77, "645": 88, "645583": 73, "64579": 59, "6458": 60, "645800": 86, "646134": 72, "646419": 77, "646937": 65, "646963": 30, "647": 108, "647004": 108, "647196": 65, "64723": 94, "64749976": 122, "647679": 77, "647689": 95, "647750": 88, "647864": 106, "647873": 91, "64797": 94, "648": 87, "648000e": 88, "648094": 76, "648333": 66, "648820e": 106, "649": [108, 153], "649158": 91, "649335": 76, "649362": 68, "649969": 88, "65": [73, 78, 82, 84, 88, 91, 92, 106, 107, 122, 138], "650": 79, "6500000000000001": [78, 88, 91], "6502": 88, "650867": 78, "650893": 77, "651662": 88, "651919": 91, "6522": [67, 153], "652324": 73, "652350": 86, "652450e": [87, 88], "652526": 80, "6527": 79, "652778": 86, "652940": 77, "653008e": 87, "653094": 76, "653424": 80, "653829": 83, "653846": 78, "653900": 106, "653901": [76, 77], "654070e": 108, "654755": 65, "654964": 85, "655284": 91, "6553": 155, "6554": 153, "655547": 76, "65557405e": 122, "655959": 83, "656802": 69, "656939": 77, "657": [62, 108], "657283": 94, "657691": 69, "65780247": 69, "658021": [108, 113], "658267": 91, "658345": 77, "658378": 69, "6586": 59, "658612": [100, 104], "658769": 76, "659": 62, "659245": [76, 77], "659387": 68, "6593871": 59, "659423": [76, 77], "659473": 95, "659755": 92, "659799": [100, 104], "66": [73, 83, 89, 106, 107, 122, 138, 152, 154], "660": [62, 108], "66006736": 69, "660128": 78, "660320": 81, "660360": 77, "660479": [15, 108, 110], "6607402": 92, "660776": 91, "660788": [108, 111], "661145": 88, "66124132": 69, "66133": 108, "661338e": 88, "661521": 76, "6616956": 69, "661790": 69, "662004798": 122, "66220205": 68, "66223141": 66, "662347": 77, "662403": 69, "6625": 88, "66284988": 68, "66302379": 68, "663177": 73, "663182": 78, "663357": 77, "6634357241067574": 95, "663529": 91, "663533": 88, "663672": 83, "663859": 76, "664846": 86, "66485158": 69, "665264": 91, "665602": 85, "665653": [108, 113], "665868e": 88, "665974": 85, "66601815": [108, 110], "666104": 91, "666220": 76, "666307": 65, "666530": 76, "6666667": 62, "666742": 106, "666750": 88, "666959e": 83, "667": 86, "667251": 69, "66725916": 66, "667285944503316": 78, "667286": 78, "667536": 91, "667682": 64, "667727": 66, "6678": 72, "667985": 82, "668446": 82, "668584": 65, "669130": 85, "669242": 69, "669562": 80, "669733": 77, "669919": 77, "67": [57, 62, 72, 87, 89, 95, 106, 107, 122, 138, 152], "670051e": 76, "67040281": 66, "670867": 29, "67095958": [108, 111], "671168": 106, "671271": [76, 77], "671382": 76, "671420": 85, "671573": 66, "671633": 88, "671717": 77, "671883": 69, "67199": 88, "6722": 62, "672234": [76, 77], "672266": 76, "672384": [76, 77], "67245350": 60, "673092": [76, 77], "673302": 86, "6734628878523097": 78, "673463": 78, "673471e": 122, "673581815999206": 78, "673582": 78, "67410934": 60, "674181": 88, "674267": 66, "674479": 69, "674507e": 76, "6745349414": 60, "67456": 95, "674609": 78, "675011": 78, "675011328023803": 78, "675489": 76, "675528": 77, "675653": [108, 109], "67571091": 66, "676093e": 80, "67618978": 64, "676242": 77, "676405": 78, "67642481": 66, "6765": [61, 87], "676536": 138, "676714": 76, "67674909": 64, "676756": 91, "676807": 87, "677614": 91, "677980": 78, "678": 92, "678117": 88, "678826": 78, "67936506": [108, 110], "67940544": 68, "679539": 86, "67963834": 68, "67965379": 68, "67ad635a": 62, "68": [62, 73, 93, 94, 106, 107, 122, 138], "680": 88, "680366": 88, "680677": [108, 111], "681": 86, "6810775": 94, "681176": 86, "681261": 76, "681817dcfcda": 62, "682315": 88, "6826": [37, 87], "682830": 77, "683331": 78, "683353": 69, "683487": 77, "683582": 108, "683637e": 83, "683785": 77, "683942": 91, "684": 155, "68410364": 61, "68411700": [61, 155], "684165": 67, "684502": 91, "684657": 77, "684899": 72, "685066": 76, "685107": 91, "685143": 85, "685250": 66, "68554404e": 122, "68562150e": 122, "685807": 91, "685816": 76, "685970": 76, "685989": 108, "686684": 77, "686694": 77, "686698": 76, "687": 122, "687227e": 77, "687278": 66, "687345": 91, "687647": 91, "687854": 65, "687871": 86, "6878711": 60, "688": 153, "6882": 72, "688537": 106, "688540": 106, "688886": 106, "688918": 88, "689072": [108, 111], "689088": [76, 77], "689188": 65, "689392": 91, "689542": [100, 104], "689772": 69, "689967": 77, "69": [82, 89, 106, 107, 122, 138, 154], "690536": 88, "69057268": 68, "69066135": 68, "690968": 76, "69103116": 68, "69140475e": 122, "691511": 87, "691641": 66, "691761": 88, "692538": 77, "692725": 91, "692768": 76, "692907": 88, "693111": 68, "693345": 88, "693359": 88, "6934117220290754": 78, "693412": 78, "693495e": 88, "693796": 86, "693874": 76, "694154": 78, "694561": 92, "694755": 88, "694919": 86, "69505403": 108, "69520523": 64, "695408": 67, "695485": 66, "6955": 88, "695581": 82, "695582": 85, "69562150e": 122, "69572427": [108, 113], "695860": 76, "696011": 28, "696289": [80, 81], "696633": 69, "696826e": 77, "69684828": 108, "697": 86, "697000": 78, "697021": 66, "697118": 91, "697420": [80, 81], "697591": 76, "697900": 77, "698223": 65, "698244": 65, "698279": 69, "698302": 76, "69840389e": 122, "698450": 16, "698642": [108, 113], "698694": 86, "699035": 91, "699082": 78, "699172": 66, "69921": 62, "69921393": 66, "699259e": 91, "699333": 78, "69955672": 66, "6_design_1a": 79, "6_r2d_0": 79, "6_r2y_0": 79, "6b": 138, "6cea": 62, "7": [12, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 42, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 101, 103, 104, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 139, 145, 152, 153, 154], "70": [61, 63, 78, 87, 88, 91, 106, 107, 122, 138, 154], "700": [76, 77, 79, 86], "7000000000000002": [78, 88, 91], "700015": 91, "70007159": 91, "700072": 91, "700102": 91, "700314": 73, "700596": 91, "700643": [108, 111], "701088": 87, "701106": 82, "701413": 88, "701491": 80, "701672e": 78, "701841e": 81, "702119e": 77, "702248": 72, "702345": 66, "702500": 88, "702632": 77, "702827": 69, "703049": 76, "70305686": [108, 113], "703325": 83, "70344386": [108, 113], "7040": 88, "704039": 77, "704482": 83, "7045": 83, "704526": 66, "704558": 83, "704581": 67, "704881": 77, "704896": 83, "705170": 76, "70557077": [108, 113], "705581": 88, "7055958": 97, "705595810371231": 97, "7055958103712310": 97, "705620": 76, "705682": 76, "70578261": 66, "705798": 77, "705801": 77, "70583": 94, "705896": 69, "705981": 77, "706173": 77, "706208": 88, "70626021": 66, "706385": 76, "706407": 69, "706645": 78, "706992": 69, "707101": 78, "70774361": [108, 113], "707868": 91, "707963e": 87, "708047": 69, "708062": 77, "708065e": 69, "708142": 69, "708154": 77, "708190": 86, "708459": 91, "708532": 76, "708762": 77, "708821": 73, "708837": 73, "708934": 76, "709026": 65, "709593": 76, "709606": 29, "71": [106, 107, 122, 138, 154], "710059": 73, "711051": 76, "711328": 88, "711518": 88, "711638": 108, "712065": 80, "712072": 90, "712095": 73, "712209e": 77, "712503": 94, "712592": 87, "712631": 76, "712846": 106, "712921": 88, "712960": 78, "713": [57, 88, 89], "71311622": 66, "713137": 77, "713582": 85, "713719": 76, "714240": 86, "714261": 77, "7142787": 91, "714279": 91, "714321": 87, "714679": 66, "715013": 88, "715179e": 88, "715407": 78, "7155": 88, "715596": 76, "7157": 88, "715764": 16, "7158581": 60, "716": 88, "716098": 73, "7161": 88, "716456": 91, "716615": 73, "716740": 106, "716762": 78, "716793": 78, "716799": 86, "7167991": 60, "717185": 91, "717916": 72, "718159": 66, "718294": 64, "718686": 95, "718848": 66, "7193": 88, "719831": 77, "72": [63, 83, 106, 107, 122, 138, 154], "720": 89, "720208": 76, "720409": 76, "7204309": [108, 113], "720571": 91, "720589": 92, "720664": 86, "721071": 91, "721097": 16, "721118": 85, "721243": 76, "7215093d9089": 62, "72155839e": 122, "721604": 77, "721609": 88, "722": 86, "72228103": [108, 111], "722316": 91, "7224": 88, "722634": 91, "72269685": [108, 113], "722848": 78, "722920": 69, "723": 62, "723314": 91, "723342": 106, "723345e": 91, "723414e": 77, "723689": 88, "723846": 73, "7239": 88, "723958": 76, "7241399": 60, "724338": 91, "724375e": 77, "724767": [80, 81], "724918": 95, "725": 62, "725010": 73, "725031": 91, "725087": 88, "725142": 76, "725166": 91, "725168": 67, "725232": 106, "725240": 72, "725456": 66, "725477": 76, "725992": 76, "726": [62, 92], "72628532": 66, "72636643": 66, "7268131": 60, "727501": 88, "727543": 65, "727693": 88, "727780": 88, "7278": 66, "727976": 78, "7282094": [108, 110], "728478": 77, "728589": 66, "728710": 91, "72875815e": 122, "728e": 92, "729668": 106, "72987186": [108, 113], "73": [61, 73, 84, 106, 107, 122, 138], "730011e": 76, "730629": 85, "730773": 76, "7308": 59, "730823": 76, "731754": 78, "731928": 85, "732": 92, "732162": 77, "732307": 76, "73232164": 66, "7326": 88, "732650": 66, "732845": 76, "73285": 27, "732983": 76, "733093": 66, "7339": 72, "73421407": 66, "734332": 76, "734948": 91, "735426": 76, "735694": [69, 91], "73569431": 91, "735713": 77, "735848": 106, "735964": 65, "735973": 66, "736082": [76, 77], "736751": 66, "736770e": 76, "7370": 72, "737210": 77, "737375": 66, "73750654": [108, 111], "7375615": 61, "73764317e": 122, "737796": 88, "737884": 25, "737951": [76, 77], "738147": 88, "738315": 88, "738405": 66, "738422": 77, "738936": 77, "739": 88, "739089": [108, 113], "739125e": 76, "739182": 76, "739261": 77, "739595": 92, "739660": 76, "739720": 88, "739817": 82, "739848": 89, "74": [31, 61, 77, 87, 106, 107, 122, 138, 154], "740": 86, "740180e": 91, "740200": 76, "740417": 87, "7405": 88, "740505": 73, "74054516": 66, "7407627053044026": 78, "740763": 78, "740869": 78, "741043": 76, "741104": 78, "741380": 92, "741523": 73, "741702": 91, "7418": 59, "74189": 62, "742128": 91, "7422": 67, "742365": 90, "742407": 90, "742695": 76, "742907": 91, "7432": 59, "743203": 72, "7434": 66, "7437": 88, "743711": 66, "74402577": 91, "744026": 91, "744125": 77, "744211": 88, "744236": 94, "74461783e": 122, "74475816": [108, 113], "74483479": 122, "745353": 76, "745536": 72, "745638": 87, "746361": 91, "746843": 81, "747": 57, "747008": 76, "747073": 69, "747164": 91, "747538": 72, "747738": 67, "747945": 60, "747977": 77, "748202": 68, "748284": 85, "748377": 87, "748513": 88, "748945": 88, "74938952": [108, 110], "749540": 72, "749854893": 123, "75": [18, 29, 31, 62, 65, 73, 78, 83, 87, 88, 91, 106, 107, 122, 138, 154], "75000": 95, "7500000000000002": [78, 88, 91], "750275": 88, "750701": 73, "751013": 88, "751081": 88, "751281": 77, "751372": 76, "751633": 88, "75171": 87, "751710": [78, 87], "751712655588833": 97, "7517126555888330": 97, "751712656": 97, "751806": 67, "752522": 76, "752696": 73, "752848e": 122, "752871": [108, 111], "752909": 83, "753136": 76, "75329595": [108, 111], "7533": 87, "753516": 77, "753523": 91, "753811": 69, "753880": 77, "754503": 76, "754692": 106, "7548": [66, 95], "754870": 86, "755383": 69, "755885": 106, "7560824": 60, "756200": 73, "756337": 76, "756585519864526": 78, "756586": 78, "756805": 86, "756867e": 88, "756969": 78, "757": [92, 153], "757136": 88, "757151": [76, 77], "757559": 83, "757596": 78, "757663": 80, "757690": 91, "757819": 86, "758027": 88, "758695": [108, 111], "758754": 77, "758852": 77, "75887": 62, "759373": 77, "759777": 67, "76": [106, 107, 122, 138, 153, 154], "760104": 91, "760155": 88, "7603": 59, "760386": [12, 108, 110], "760403": 77, "760517": 72, "760778": 86, "760915": 65, "761": [60, 86], "761224": 73, "761714": 78, "762284": 91, "76228406": 91, "762299": [108, 113], "7628744": [108, 111], "762911": 66, "764093": [76, 77], "76419024e": 122, "764315": 91, "76444177e": 122, "764859": 64, "764953": 87, "76535102": [108, 113], "765363": [76, 77], "765500e": [87, 88], "765557": 76, "7656": 88, "765679": 66, "765710e": 96, "765792": 91, "76591188": 60, "7660": 59, "766005": 91, "766024": 89, "76608187": 67, "766104": 69, "76631273": 69, "766499": 91, "766585": 88, "766709": 89, "7669": 88, "766940": 73, "76702611e": 122, "767188": [80, 81], "767435": 95, "767486": 77, "768273": [80, 81], "768798": 73, "768874": 77, "769063": 88, "769361": 91, "769743": 68, "769805": 91, "77": [92, 106, 107, 122, 138], "770556": 88, "770655": 66, "770838": 72, "770902": 69, "770944": [80, 81], "7710": 94, "771167": 138, "771275": 88, "771383e": 88, "7714": 89, "771529": 85, "7716982": 61, "771758": 67, "771876": 88, "771965": 88, "771986": 64, "772157": 83, "772275": 66, "77227783e": 122, "772373": 76, "772444": 106, "772612": 91, "77261209": 91, "772791": 88, "77289874e": 122, "773": 62, "773177": 78, "77329414": [108, 111], "773339": 83, "7740": 66, "77401500e": 122, "774365": 77, "774683e": 86, "774915": 76, "775": [62, 88], "775191": [76, 77], "775969": 94, "776": 57, "776169": 69, "7763": 87, "77655335": 122, "776601": 76, "776887": 87, "777199": 72, "777297": 26, "77746575": [108, 113], "777570": 66, "7776071": 60, "777728": 24, "777867": 83, "777e": 92, "778014": 69, "778211": 69, "778305": 76, "7786": 59, "779": [92, 108], "779068": 83, "7792": 67, "779349": 76, "779517": [76, 77], "779682": 78, "779727e": 88, "78": [80, 92, 106, 107, 122, 138, 154], "780": 62, "780458": 91, "780856": 87, "780943": [77, 91], "781076": 88, "781681": 91, "782": 62, "782159": 88, "782643": 91, "783": [62, 88], "783183": 77, "783276": 108, "7833": 59, "7838": 59, "78386025": [108, 113], "783888": 67, "7839": 67, "784": 138, "784238": 86, "784405": 94, "784841": 88, "784872": 73, "784910": 77, "784975": 66, "785": 62, "785107": 78, "785291": 68, "785458e": 122, "785815": 73, "785908": 66, "785911": 91, "785979": 85, "786": 62, "786191": 82, "786429": 76, "786568": 66, "786744": 78, "786986": 73, "78711285e": 122, "787668": 68, "787695": [108, 111], "78777": 94, "787795": 16, "787838e": 76, "788": 153, "78818": 62, "788267": 85, "788385": 106, "788896": 77, "788972": 76, "789": 108, "789355e": 94, "789544": 69, "79": [73, 106, 107, 122, 154], "790": 87, "790000e": 88, "790115": 88, "790142": 85, "790261": 106, "790273": 69, "790328": 89, "790631": 68, "790723": [80, 81], "790885": 77, "7910908091075142": 78, "791091": 78, "791095": 66, "791097": 87, "79110906": 66, "791241": 91, "791297": 29, "791337": 66, "791504": 89, "79164735": [108, 111], "7919965": [108, 111], "792200": 89, "792396": 73, "792939": 78, "792964": 66, "792972": 106, "793011e": 66, "793297e": 88, "79330022": [108, 113], "793315": 106, "79338596e": 122, "793570": 91, "793735": 91, "793818": [76, 77], "794": 108, "794119": 88, "794366": 88, "79458848e": 122, "794755": 88, "794805": 80, "795": 92, "795008": 77, "795188": 68, "79543857": 69, "795647": 91, "795828": 77, "795932": 107, "7963": 88, "796340": 88, "796477": 68, "79649206": 122, "79694309": 69, "796e": 92, "797": 89, "7970": 67, "797189": [108, 113], "797280": 91, "797323": 86, "797410": 76, "797454": 108, "797617": 16, "79765991": 122, "797743": 138, "79792890e": 122, "797971": 138, "797984": 77, "79820711": 69, "798308": 87, "798388": 77, "798685": 26, "798765": 67, "798783": [80, 81], "798862e": 76, "799403": 91, "79953099": [108, 113], "799698": 76, "799890": 76, "79997756": 69, "7b428990": 62, "7x": 91, "8": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 49, 52, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 99, 104, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 139, 145, 152, 154, 155], "80": [63, 64, 72, 78, 88, 91, 96, 106, 107, 122, 154], "800": 86, "8000": [36, 63, 96], "8000000000000002": [78, 88, 91], "800000e": 88, "8001": 67, "800156": 67, "801130": 88, "802": 92, "802149": 77, "802738": 106, "802992": 89, "803300": 76, "803492e": 91, "803889e": 88, "803908e": 77, "804219": 91, "804284": 94, "804316": 91, "804549": 88, "8048": 61, "804828": 91, "805007": 86, "805153e": [87, 88], "805521": 66, "8055563": 60, "805714": 76, "805720": 91, "805789": 76, "8059": 87, "806": 57, "806167": 88, "806220e": 88, "806356": 88, "806532e": 76, "806732": 82, "806736e": 76, "80696592e": 122, "80714504e": 122, "807879": 91, "808": [61, 138], "808246": 87, "808347": 64, "808475": 66, "808640": 88, "808860": 77, "809": 88, "809278": 64, "809392": 76, "80945": 69, "8095": 89, "809913": [76, 77], "80a8": 62, "81": [60, 76, 79, 82, 106, 107, 122, 154], "810044": 87, "810134": 91, "8102": [59, 87], "810306": 73, "810382": [87, 88], "810564": 76, "810650": 91, "810833": 77, "810916": 88, "811155": 82, "811212325323343444546566467727688909497": 122, "811255": 77, "81128199": 91, "811282": 91, "811393": 67, "811458": 87, "8116912": 138, "811763": 76, "811825": 86, "811901": 91, "81190107": 91, "812": 92, "812003": 76, "812028": [108, 109], "812567": 68, "812658": 77, "812996": 67, "813": 89, "8130": 72, "8132463": 60, "813293": 91, "813342": 138, "813426": 69, "813682": 88, "813948": 89, "814351": 78, "814369": 66, "814401": 66, "814717": 78, "814913": 86, "814954": 66, "815224": 138, "815226": [15, 108, 110], "815993": 91, "8160": 88, "816126": 69, "816131": 76, "816176": 95, "816296e": 76, "816318": 86, "816417": 76, "816896": 72, "817967": 108, "818029": 88, "81827267": 91, "818273": 91, "818289": 91, "81828926": 91, "818380": [76, 77], "81856": 62, "818990e": 77, "819223": 106, "82": [95, 106, 107, 122, 154], "8202": 61, "820366": 86, "8208": 88, "820833": 68, "8209": 61, "820963": 73, "820993": 88, "821": 153, "8210": 61, "821009": 88, "821021": 78, "821566": 91, "8221": 59, "822289": [87, 155], "82228913": 155, "822482": 78, "8227": 88, "822822": 78, "823247": 91, "823273": [76, 77], "82329138": 84, "823617": 69, "823666": 77, "823772": [108, 111], "824140": 72, "824350": [76, 77], "824701": 78, "824750": 78, "824889": 78, "824961e": 88, "8250": 59, "825617": 86, "825749": 77, "825920": 69, "825955": 66, "8260": 87, "826065": [76, 77], "826074e": 86, "82628529": 69, "826426": [15, 108, 110], "826492": 91, "826519": 29, "82666866e": 122, "82684324": 94, "826897": 91, "827093": 76, "827240": 66, "827381": 91, "827477": 69, "827735": 91, "827874854703913": 78, "827875": 78, "827938162750831": [80, 81], "828157": 73, "828618": 83, "828915": [80, 81], "829": 122, "829543": 78, "829764": 106, "83": [89, 106, 107, 122, 154], "830102": 76, "830120": 77, "830273": 73, "830301": 90, "830850": 76, "830967": 76, "831": 92, "831019": 78, "831167e": 76, "831479": 76, "831565": 69, "83160417": 66, "831833": [108, 113], "832086": 91, "8325": 108, "8326928": 94, "832693": 94, "832875": 91, "83287529": 91, "832916": 68, "832965": 88, "832991": 16, "833018": 88, "833024": 86, "833065": 73, "833096": 88, "833181": 66, "833227e": 107, "833563": 85, "833907": 86, "834133": 83, "83436056": [108, 111], "834916": [108, 111], "835": [89, 92], "8350": 88, "835239": [108, 113], "835344": 73, "835491": 69, "835750": 83, "835839": 88, "835997": 69, "836234": 108, "836623": 69, "837366e": 77, "837699": 77, "838114": 91, "838195e": 69, "838235": 89, "838322": 76, "83854057": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "84": [62, 82, 92, 106, 107, 108, 122, 154], "840041": 88, "8403": 72, "840303": 91, "84030318": 91, "840718": [12, 108, 110], "840836": 91, "840995e": 87, "841": [60, 86], "841132": 87, "8415": 61, "841712": 88, "841847": 88, "841920": 77, "842205": 88, "842262": 108, "842405": 78, "842625": 86, "842746": 91, "8428": 87, "842853": 91, "843": 88, "843018": 106, "843296": 88, "843454": 76, "843730": 86, "844026": 106, "844308": 91, "8445": 84, "844549": [80, 81], "844663": 73, "844673": 138, "844707": 91, "844893": 86, "8450": 72, "845241": 92, "845525": 76, "846602": 88, "846707": 88, "84692394": 66, "847136": 78, "847595": 28, "847948": 78, "848": 57, "848454": 66, "848578": 88, "848734": 76, "848757e": 87, "848868": 78, "848997": 88, "849177": 69, "84930915e": 122, "849427": 106, "849747": 94, "849766": 88, "8497f641": 62, "8498": 88, "85": [34, 78, 82, 84, 88, 91, 96, 106, 107, 122], "8500000000000002": [78, 88, 91], "850038": 73, "850101": 77, "850321": 86, "850575": [76, 77], "850656": 83, "850960": 66, "851": 153, "851012": 85, "851100": 73, "851210": 69, "8513": 62, "851366": 86, "851656": 66, "852016": 91, "852592e": 85, "8526": 72, "852916e": 77, "853166": 68, "8534942": 66, "85397773": [108, 110], "85402594": 84, "854525": 72, "85488552": 66, "855": 88, "855236": 69, "855242": 76, "855780": 91, "856037": 77, "856404": 81, "856552": 73, "856758": 106, "8571": 59, "857161": 91, "857372": [108, 111], "857515": 106, "857544": 86, "858577": 90, "858635": 85, "858952": 73, "859": 88, "85911521e": 122, "85912862": 138, "859129": [38, 123, 138], "8597": 87, "85c5": 62, "85e": 61, "86": [106, 107, 122, 154], "860378": 88, "860663": 138, "860804": 91, "861019": 73, "861203": 67, "861210": [108, 113], "861227": 68, "861505": 72, "862043": [80, 81], "862274": 80, "862357": 69, "862359": 78, "862638": 66, "863337": 76, "86342793": 66, "863772": 87, "863982270": 123, "864": [92, 155], "86415573": 61, "86424193e": 122, "8644": 62, "864404": 88, "864664": 73, "865313": 88, "865394": 66, "865442": 85, "865562": [76, 77], "865860": [87, 88], "866102": [76, 77], "86610467": 91, "866105": 91, "866142": 77, "866179899731091": 97, "866179900": 97, "866579": 88, "8666": 67, "866798": 88, "86721053": 66, "867565": 91, "867775": 77, "8679": 88, "868": 62, "8681": 84, "86897905": [108, 113], "869": [62, 92], "869020": 78, "869301": 106, "869427": 37, "869586": 82, "869617": 106, "869658": 72, "869778": 90, "869912e": 77, "87": [61, 76, 82, 86, 106, 107, 122, 154], "870": 68, "8700": 61, "870099": [80, 81], "870187": 77, "870260": 91, "870288": 76, "870332": 91, "870444": 87, "870870": [108, 111], "870943": 77, "871": 62, "871208": 68, "871972": 83, "872": 92, "872354": 91, "872711e": 66, "872768": 91, "872852": 91, "87290240e": 122, "873039": 77, "87309461": [108, 113], "873198": 88, "873275": 72, "873677": [80, 81], "873848": 68, "87384812361": 59, "87384812362": 59, "87421647": 66, "87430335": 138, "874303353": 138, "874702": [80, 81], "874721": 76, "874808": 66, "8751": 88, "8759": 88, "876": 92, "876233": 68, "87623301": 59, "876431e": 78, "8767": 72, "87674597e": 122, "8768": 59, "8771": 88, "877153": 88, "87719285": 66, "877287": 16, "877446": 106, "877455": 90, "877833": [76, 77], "877903": 73, "878122": 85, "878281": 91, "878402": 76, "878746": 73, "878802": 88, "878895": 73, "878914": 76, "879103": 78, "87e": 61, "88": [61, 82, 92, 106, 122], "880106": 86, "880217": [108, 111], "880456": 69, "880579": 91, "880591": 90, "880886": 87, "8810": 87, "881201": 88, "88125046e": 122, "881465": 65, "881533": 68, "881567": 77, "88173062": 60, "88173585": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "881862": 68, "881937": 73, "882470": 16, "882641": 87, "882896": 91, "882928": 73, "882965": 72, "883224": 77, "883301": 78, "883622": 91, "883914": 78, "8843": 94, "8845": 59, "884821": 106, "884996": 78, "8850": 61, "885065": 91, "885513": 88, "885686e": 76, "885832": 92, "885978": [80, 81], "886": 155, "886086": [76, 77], "88629": 59, "886300": 76, "886611": 88, "88664": 62, "886764": 77, "887182": [108, 109], "887212": 88, "887270": 77, "887556": 78, "887731": 64, "887778": 88, "8879": 67, "888146": 86, "8881461": 60, "888217": 88, "888348": 77, "888352": 73, "888423": 88, "888757": 91, "888775": 81, "889236": 72, "889261": 76, "889293": 91, "889476": 66, "889566": 94, "889638": 73, "889733": 91, "88988263e": 122, "889913": [76, 77], "88ad": 62, "89": [61, 77, 106, 122, 153, 154], "890": [60, 86], "890204": [108, 113], "89027368": 138, "890273683": 138, "890342": 76, "89035917": 82, "890372": [70, 99, 104, 152], "8903720000100010000010": [62, 99, 104, 152], "8904": 57, "890454": 107, "890665": 83, "890855": 73, "8909": [60, 87, 155], "890933": 72, "891102": 77, "891167": 66, "891527": [108, 113], "891593": 76, "891606": 87, "891649": 25, "891678": 66, "891757": 77, "891993": 66, "892": 62, "892648": 91, "892796": [76, 77], "892828": 83, "893": 62, "8932105": 60, "893461": 73, "893521": 77, "893649": [76, 77], "893851": 91, "893884": 88, "894": [62, 89], "894329e": 88, "8946549": 63, "894701": 77, "89472978": [108, 113], "895106": [76, 77], "895275": 16, "895308": 88, "895333": 91, "895442": 87, "895690": [76, 77], "895768e": 78, "895853e": 76, "896023": 91, "896182e": 83, "896263": 83, "896333": 77, "896761": 67, "897115": 76, "89718365": 66, "897220": 91, "8974": 87, "8974226": [108, 111], "898183": 73, "89859055": 66, "898722": 91, "89876042": 66, "899021": 106, "899250": 73, "899296": [108, 111], "899460": 91, "899568": 76, "8996": 84, "899654": 73, "8bdee1a1d83d": 62, "8da924c": 62, "8e3aa840": 62, "9": [12, 14, 15, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 99, 104, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 139, 145, 151, 152, 154, 155], "90": [40, 61, 63, 64, 78, 88, 91, 96, 106, 122, 154], "9000000000000002": [78, 88, 91], "900021": 107, "900127": [108, 113], "900249": 67, "900829": 83, "90083655": 66, "901013": 77, "901148": 91, "90136": 87, "901360": 87, "90145324": [108, 113], "901526": 82, "901563": 16, "901998": 85, "902": 138, "902219": 76, "902573": 78, "902910": 66, "902920": 106, "903056e": 91, "903339": 78, "903366e": 78, "903418": 86, "903681": 91, "903731": 85, "90376019": 66, "903767": [76, 77], "903879": 77, "904315": 69, "9045": 88, "904900": 76, "904976": 76, "905494": 78, "905724": 72, "905858": 95, "905951": 94, "906051": 78, "906051023766621": 78, "906195": [108, 111], "9067": 88, "906716732639898": [80, 81], "906757": 74, "907115": 91, "907176": 91, "907491": 78, "907801": 86, "907879": 73, "90794478": 138, "907944783": 138, "908024": 95, "908117330": 122, "908912": 77, "909091e": 69, "909304": [76, 77], "909571": 73, "90963122e": 122, "909942e": 24, "909997": [87, 155], "91": [93, 106, 122, 154], "91053356": [108, 111], "9109": 62, "91102953": 91, "911030": 91, "911048": 69, "9112": 87, "911794e": 77, "911818": 77, "912033": 30, "912230": [76, 77], "9126": [61, 155], "9127": [61, 155], "912801": 77, "912837": 76, "913": 62, "91315015": 60, "913280": 95, "913302": 66, "913570": 69, "913585": 83, "913774": 78, "91419749": 66, "9142": 88, "91438767e": 122, "9145": 59, "914598": 73, "914651": 69, "915": [61, 62, 87, 88], "915000e": [87, 88], "915225": 72, "915229": 68, "915488": [80, 81], "915502": 77, "916171": 90, "916236": 59, "916355": 77, "916359": 73, "9166667": 62, "91683613": [108, 111], "916913": 91, "916914": 91, "916921": 88, "916984": 91, "917": [62, 89], "917497": 80, "91771387": 91, "917714": 91, "9179": 88, "918": 92, "918044": 76, "918199e": 76, "918227": 78, "918293": [108, 113], "918414": 69, "919196": 66, "919879": 88, "91e": 61, "92": [66, 93, 106, 107, 122, 154], "920283": 88, "920337": 81, "920695": 88, "9209": 59, "9210": 88, "921061": 95, "921198": 83, "921372": 78, "921389": 69, "921778": 83, "921803": 69, "921913": 86, "921956": [76, 77], "921e4f0d": 62, "922019": 68, "922232": 76, "922642": 66, "922656": 72, "922668": 83, "922713": 66, "922996": 86, "923033": 77, "923084e": 78, "923517": 96, "923607": 91, "92369755": 60, "923943": 155, "924002": 91, "924061": 76, "924078e": 69, "924232e": 85, "9243": 88, "924396": [80, 81], "924443": 73, "924540": 85, "924634": 65, "924724": 78, "924732": 88, "9248": 62, "924821": 78, "925248": [80, 81], "92527036": 66, "925410": 72, "92566": 108, "925736": 78, "925994": 87, "926260": 80, "926621": 78, "926685": 86, "927": 58, "927074": 91, "9271874": 66, "927232": 88, "9274": 88, "927501": 77, "927563": 77, "927631": 66, "928269": 88, "92827999": 108, "92881435e": 122, "928925": 66, "9289936": 66, "929031": 76, "92905": 60, "929454": 69, "929607": 88, "92964644": [108, 111], "929699": 77, "92972925e": 138, "929729e": [38, 123, 138], "929775": 68, "9299": 67, "929923": 67, "93": [61, 83, 93, 106, 107, 122, 154], "930266": 69, "9304028": 60, "93061562": 66, "93074943": [108, 113], "930750": 68, "931": 98, "93134081": [108, 111], "931479": 91, "931646": 76, "931978": 152, "932027": 78, "932327": 76, "932404e": 88, "9325": 59, "932628": 69, "9327": 59, "932906": 77, "932973": 91, "93300": 108, "93301564": 66, "933069": 68, "93314266": 66, "933156": 68, "933260": 69, "933603": 77, "93362373": 69, "933722e": 69, "933730": 69, "933749": 68, "933996": 78, "934122": 77, "93412525": 66, "934270": 86, "934433": [76, 77], "9345": 62, "934511": 138, "934548": 67, "93458": 94, "934634": 77, "934992": 78, "935": [57, 84], "935220": [108, 109], "935591": 91, "935730": 91, "935793": [108, 111], "935795": 66, "935989": 86, "9359891": 60, "935993": 77, "936094": 76, "936238": 77, "93648": 96, "936739": 91, "937038": 66, "937088": 66, "937586": 88, "937613": 66, "938": 138, "938263": [108, 113], "938460": 76, "9386744462704798": 85, "9388": 88, "938836": [108, 113], "939068": [80, 81], "939390": 76, "9395": 88, "93958082416": 155, "94": [84, 106, 122, 154, 155], "94010905": 66, "941047": 16, "941440": 76, "9416397": 66, "941709": 78, "9417090334740552": 78, "942139": 81, "942312": 91, "942460e": 91, "9425": 59, "942550": 88, "942629": 88, "942661": 86, "94288514": 66, "94309994e": 122, "943548": 83, "943693": [108, 109], "944021": 66, "944045e": 91, "944169": 66, "944253e": 91, "944266": [80, 81], "94427158": [108, 113], "944280": 88, "944317": 88, "94441007e": 122, "944630": 91, "94473": 63, "945058": 76, "9457957": 66, "946180": 83, "94629": 96, "946297": 78, "946406": 81, "946433": 91, "946968": 78, "947194": 76, "947290": 77, "947440": 90, "947466": 107, "947855": 73, "9480": 88, "948038": 69, "948112": 92, "948828": 76, "948975": 82, "94906344": 60, "949241": 138, "94940816": 66, "949456": 91, "949459e": 76, "9494928": 66, "949552e": 69, "949866": 77, "949904008": 122, "949912": 72, "95": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 48, 59, 61, 63, 64, 65, 66, 67, 68, 69, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 87, 88, 91, 92, 94, 95, 96, 106, 108, 122, 138, 139, 145, 154, 155], "950": 57, "9500": 88, "950280": 91, "950545": 74, "95062986e": 122, "951": 89, "951532": 86, "951604": 77, "951920": 90, "952": [61, 92, 155], "952146": [108, 113], "9523": 59, "952839": 91, "95305": 63, "95311164": [108, 113], "9532": 88, "953683": 86, "953696": 68, "953706": 76, "95372559e": 122, "953884": 108, "954": 138, "95401167e": 122, "954738": 77, "954764": 69, "9552": 59, "955519": 68, "955541": 29, "95559917": 107, "955689": 76, "956047": 60, "9561": 59, "956272": 86, "956282": 69, "956574": 88, "956588": 108, "956769": 68, "957229": 81, "957339": 86, "957375": 86, "957417": 88, "957479": 78, "9574793755564219": 78, "957627": 69, "957745": 78, "9579": 61, "957996": 78, "958": 138, "9580": 61, "958608": 72, "958612": 72, "958636": 82, "958789": [108, 111], "959441": 77, "959613": 106, "95e": 61, "96": [61, 64, 76, 77, 89, 106, 122, 154], "960049": 73, "960236": 108, "960808": 78, "960846": 88, "9609": 59, "960938": 69, "961018": 77, "961538": 88, "961962": 78, "962022": 77, "962041": 77, "962228": 77, "962364": 73, "962478": 66, "9625": 88, "963367": 72, "9636": 72, "964": 89, "964025e": 91, "964261e": 86, "964317": 88, "9647": 59, "964914": 16, "964940": 77, "965": 64, "965059": 67, "965376": 76, "965531": 108, "96582": 107, "966015": 91, "966031": 72, "966566": 77, "967206": 72, "967419": 69, "967467": 94, "967872": 67, "96810319": 69, "968114": 69, "968127": 73, "968134e": 91, "968288": 88, "968305e": 76, "969": 57, "969141": [106, 107, 108], "969702563412915": 78, "969703": 78, "9699": 87, "97": [12, 15, 16, 24, 25, 26, 27, 28, 29, 30, 37, 38, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 152, 154, 155], "970065": 91, "9701": 72, "970652": 77, "971": 122, "971058": [80, 81], "971059": 72, "971132": 88, "971967": [108, 111], "972051": 88, "972162": 66, "972405": 76, "972735": 72, "973140": 72, "97314470": 60, "973241": 91, "973279": 68, "973293": 77, "973874": 78, "974023": 76, "974104": 77, "974202": 78, "974213": 77, "97441062": [80, 81], "974414": 78, "974416": 77, "974523": 69, "97470872": 94, "9748910611": 60, "974925": 77, "97499195": 91, "974992": 91, "975": [76, 77, 80, 81, 83, 84, 89, 108, 111], "975289": 73, "9753": 62, "975475": 66, "975561": 66, "976072": 68, "976088": 91, "97619643": [108, 113], "976398": 77, "9764": 72, "976477": 69, "976562": 91, "976791e": 77, "977113": 77, "977280": [76, 77], "977295": 88, "9773": 88, "977489": 88, "978000e": 88, "978052": 77, "9783": 72, "978438e": 77, "978446": 76, "978618": 72, "9787": 88, "978977": 91, "979": 92, "979031": 72, "979055": 68, "979101": 66, "979135": 106, "979384": 83, "979409e": 77, "979966": 83, "98": [76, 77, 89, 106, 122, 154], "98023747": [108, 111], "9802393": 60, "980407": 77, "980574": 76, "980643e": 78, "980771": 76, "980838": 76, "981104": 90, "981339": 77, "981622": 76, "981672": 78, "981703": 76, "982024": 67, "982353e": 88, "982417": 78, "982528": 69, "982815": 88, "982948": 76, "983192": 91, "983389": 76, "983434": 69, "983482": 88, "983483": 73, "983759": 155, "98393441": 94, "984083": [80, 81], "984562": 91, "984866": 138, "984872": [76, 77], "984937": 78, "9850": 69, "98505871e": 122, "985207": [76, 77], "985279": 88, "985569": 88, "98578197": 69, "985971": 77, "986249": 87, "986672": 67, "986683": 76, "98673": 82, "9870004": 62, "9875": 59, "98750578": 64, "987770": 68, "9880384": 62, "988081": 76, "988421": [76, 77], "988463": 91, "989600": 89, "989608": 66, "99": [52, 61, 73, 76, 77, 88, 89, 106, 122, 154], "990": 89, "990207": 69, "990210": 88, "990438": 89, "990567e": 88, "990838": 76, "991": 62, "99112": 68, "9914": [87, 88, 94], "9915": [61, 87, 88, 94], "991512": 61, "991824": 76, "991963": [76, 77], "991977": 88, "992": [89, 92], "9920": 69, "9921": 69, "992288": 76, "99232145": 94, "992359": 16, "992519": 69, "992582": [76, 77], "993": 89, "993416": 83, "993442": 68, "993512": [108, 111], "993959": 69, "994": 92, "994168239": 60, "994208": 73, "994304": 88, "994332": 74, "994335": 66, "9944": [15, 108, 110], "994544": 66, "9948104": 63, "994864": 88, "994937": 81, "9951": 59, "995248": 91, "99549118e": 122, "99571372e": 122, "9961": 87, "9961392": 60, "996413": 88, "996729": 76, "996934": 86, "9970": 88, "997034": 96, "997038": 77, "997494": 96, "997571": 86, "997621": 78, "997934": [80, 81], "998": [57, 122], "998050": 90, "998063": 74, "998291": 66, "99864670889": 155, "998855": 88, "999": [52, 65, 72, 82, 94, 155], "9990": 69, "999055": 69, "999120": 90, "999207": 91, "9994": 69, "9995": [65, 76, 77], "9996": [65, 76, 77], "9996553": 61, "9997": [65, 76, 77], "9998": [65, 76, 77], "9999": [65, 76, 77], "99c8": 62, "A": [10, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 34, 35, 37, 38, 39, 43, 44, 46, 47, 48, 50, 51, 57, 58, 59, 61, 62, 66, 67, 68, 69, 72, 74, 75, 79, 84, 85, 89, 90, 92, 93, 94, 95, 98, 99, 104, 106, 107, 108, 109, 111, 113, 121, 138, 139, 140, 146, 147, 148, 149, 150, 152, 153, 155], "ATE": [26, 30, 31, 61, 70, 72, 73, 83, 87, 94, 95, 106, 108, 118, 121, 123, 131, 139, 147], "ATEs": [72, 73, 89], "And": [63, 89, 96, 139, 141, 142], "As": [58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 72, 73, 75, 76, 77, 78, 79, 82, 83, 84, 85, 86, 87, 88, 89, 91, 95, 96, 97, 107, 108, 111, 112, 113, 122, 123, 125, 127, 129, 138, 139, 140, 149, 155], "At": [18, 31, 39, 60, 64, 65, 67, 73, 82, 84, 86, 88, 91, 155], "Being": 155, "But": [83, 84], "By": [59, 60, 86, 92, 95, 107, 108, 111, 113, 139, 145], "For": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 25, 26, 38, 47, 51, 57, 59, 60, 62, 64, 66, 67, 68, 69, 72, 73, 74, 82, 83, 84, 85, 86, 88, 90, 92, 94, 95, 97, 98, 99, 100, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 147, 149, 150, 151, 152, 155], "ITE": [35, 72, 73], "ITEs": [72, 73], "If": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 58, 60, 64, 66, 67, 68, 69, 72, 75, 76, 77, 83, 84, 86, 88, 92, 98, 99, 104, 106, 107, 108, 110, 116, 123, 124, 125, 126, 127, 128, 131, 138, 139, 140, 141, 142, 143, 144, 145, 148, 149, 150, 155], "In": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 46, 50, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 122, 123, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155], "It": [13, 59, 60, 61, 72, 76, 77, 79, 80, 81, 86, 87, 88, 92, 93, 95, 100, 103, 104, 107, 108, 109, 122, 150, 154], "No": [33, 53, 57, 59, 61, 62, 63, 64, 66, 67, 68, 69, 70, 72, 73, 82, 87, 88, 92, 94, 96, 99, 101, 102, 103, 104, 107, 108, 110, 111, 113, 123, 138, 152, 153], "Not": [108, 114], "Of": [84, 138, 155], "On": [58, 75, 85, 89, 93, 98, 153], "One": [61, 66, 69, 87, 88, 95, 106, 138], "Or": 44, "Such": [95, 107], "That": [44, 155], "The": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 112, 114, 117, 119, 120, 121, 122, 123, 128, 131, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 149, 150, 151, 153, 154, 155], "Then": [18, 78, 91, 93, 108, 138, 139, 149, 150, 151], "There": [61, 87, 95, 108, 114, 151, 155], "These": [25, 61, 62, 67, 71, 85, 87, 90, 92, 94, 106, 108, 155], "To": [19, 21, 57, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 72, 73, 74, 75, 76, 77, 78, 80, 81, 83, 84, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 98, 99, 104, 106, 107, 108, 112, 122, 138, 139, 140, 145, 149, 150, 151, 152, 155], "Will": [14, 16, 22, 24, 25, 26, 27, 28, 29, 30], "With": [34, 76, 77, 107, 153], "_": [9, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 58, 60, 64, 65, 66, 68, 69, 75, 76, 77, 78, 79, 80, 81, 86, 87, 88, 90, 91, 92, 93, 97, 98, 103, 104, 106, 108, 111, 114, 122, 123, 125, 138, 139, 140, 145], "_0": [58, 60, 75, 79, 86, 97, 98, 122, 123, 136, 137, 138, 139, 149], "_1": [17, 18, 19, 31, 35, 39, 63, 89, 96, 123, 136, 137], "_2": [17, 18, 19, 31, 35, 39, 89], "_3": [17, 18, 19, 31, 35, 39], "_4": [17, 18, 19, 31, 35, 39], "_5": [31, 35], "__": [50, 51], "__init__": [85, 93], "__version__": 151, "_a": [123, 125, 127], "_all_coef": 122, "_all_s": 122, "_b": [123, 125, 127], "_check": 72, "_compute_scor": 21, "_compute_score_deriv": 21, "_coordinate_desc": 86, "_d": [72, 92, 108], "_est_causal_pars_and_s": 154, "_estimator_typ": 85, "_h": [92, 108], "_i": [58, 63, 75, 91, 96, 98, 108, 114], "_id": 122, "_j": [17, 18, 19, 31, 35, 39, 41, 60, 86, 138], "_l": 107, "_lower_quantil": [66, 69], "_m": [107, 122], "_mean": [66, 69], "_n": [123, 124, 125, 126, 127, 131, 138, 139, 145, 148], "_n_folds_per_clust": 86, "_offset": 107, "_pred": 107, "_rmse": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "_upper_quantil": [66, 69], "_x": 45, "_y": [92, 108], "a0": 85, "a09a": 62, "a09b": 62, "a1": 85, "a3d9": 62, "a4a147": 89, "a5e6": 62, "a5e7": 62, "a6ba": 62, "a79359d2da46": 62, "a840": 62, "a_": [63, 96], "a_0": 42, "a_1": 42, "a_j": [108, 115], "ab": [59, 93, 150], "ab71": 62, "abadi": [10, 64], "abb0fd28": 62, "abdt": [70, 99, 104, 152], "abl": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 58, 75, 84, 88, 89, 107, 139, 140, 149], "about": [14, 16, 61, 64, 65, 72, 84, 87, 93, 108, 150, 152, 155], "abov": [58, 61, 67, 73, 75, 76, 77, 80, 81, 84, 85, 87, 89, 90, 91, 92, 95, 98, 106, 107, 108, 112, 114, 151], "absolut": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 84, 107], "abstract": [21, 59, 60, 86, 123, 150, 154], "abus": [66, 69, 108, 111, 113, 114], "acc": [23, 59], "acceler": 72, "accept": [17, 19, 100, 104, 106, 107], "access": [46, 47, 59, 61, 66, 67, 68, 69, 72, 80, 81, 82, 84, 94, 102, 104, 107, 139, 145, 155], "accompani": 150, "accord": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 58, 61, 63, 64, 67, 73, 75, 78, 87, 91, 92, 95, 96, 107, 108, 109, 114, 138, 139, 141, 142, 143, 144, 146, 147, 155], "accordingli": [63, 64, 84, 85, 87, 92, 96], "account": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 60, 61, 66, 69, 86, 87, 88, 94, 95, 139, 145, 149, 155], "accumul": [61, 87, 88, 94], "accur": 72, "accuraci": [46, 50, 59, 108], "acemoglu": 153, "achiev": [60, 72, 83, 86, 90, 95, 108, 138], "acic_2024_post": 89, "acknowledg": [61, 62, 87], "acm": 153, "acov": 153, "across": [13, 17, 19, 61, 72, 87, 89, 155], "action": 154, "activ": [4, 5, 6, 7, 8, 9, 151, 154], "actual": [44, 66, 69, 82, 95], "acycl": [63, 96, 155], "ad": [5, 6, 7, 8, 9, 10, 11, 21, 46, 47, 50, 51, 82, 99, 104, 107, 108, 138, 139, 140, 154], "adapt": [25, 87, 154], "add": [59, 60, 63, 64, 65, 70, 72, 73, 80, 81, 82, 89, 91, 92, 94, 95, 96, 107, 108, 153, 154], "add_trac": 95, "addit": [12, 13, 14, 15, 16, 17, 18, 19, 22, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 41, 42, 43, 45, 48, 66, 67, 68, 69, 79, 95, 100, 101, 102, 103, 104, 107, 108, 109, 123, 132, 139, 146, 147, 149, 153, 154], "addition": [31, 39, 69, 73, 78, 88, 94, 107, 108, 122, 138, 139, 145, 152], "additional_inform": 13, "additional_paramet": 13, "address": 95, "adel": 153, "adj": [92, 95], "adj_coef_bench": 95, "adj_est": 95, "adj_vanderweelearah": 95, "adjust": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 49, 52, 60, 65, 83, 86, 88, 94, 95, 106, 108, 114, 138, 139, 145, 153, 154, 155], "adjust_p": 52, "adopt": [64, 108, 110, 114], "advanc": [85, 105, 122, 153], "advantag": [58, 59, 61, 73, 75, 87, 88, 98, 151], "advers": [139, 140], "adversari": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 94, 139, 145, 149], "ae": [58, 60, 61, 63], "ae56": 62, "ae89": 62, "aesthet": 58, "aeturrel": 43, "afd9e4": 89, "affect": [72, 73, 79, 108, 154, 155], "after": [52, 53, 59, 61, 62, 63, 64, 79, 87, 88, 95, 96, 106, 107, 108, 114, 139, 141, 145, 151, 155], "after_stat": 58, "ag": [61, 87, 88, 90, 94, 155], "again": [58, 59, 60, 61, 63, 64, 66, 69, 73, 75, 82, 85, 86, 87, 92, 93, 94, 95, 96, 98, 139, 141, 142], "against": [64, 72, 82, 84, 90, 107], "agebra": 106, "agegt54": [62, 70, 99, 104, 152], "agelt35": [62, 70, 99, 104, 152], "agg": [59, 66, 69, 93], "agg_df": [66, 69], "agg_df_anticip": [66, 69], "agg_dict": [66, 69], "agg_dictionari": [66, 69], "agg_did_obj": [108, 109], "aggrag": [108, 109], "aggreg": [13, 16, 59, 97, 109, 122, 154], "aggregate_over_split": 44, "aggregated_eventstudi": [66, 67, 68, 69], "aggregated_framework": [66, 67, 68, 69, 108, 109], "aggregated_group": [66, 69], "aggregated_tim": [66, 68, 69], "aggregation_0": 13, "aggregation_1": 13, "aggregation_color_idx": 13, "aggregation_method_nam": 13, "aggregation_nam": 13, "aggregation_weight": [13, 66, 67, 68, 69], "aggt": 59, "ai": [68, 93, 153], "aim": 92, "aipw": 89, "aipw_est_1": 89, "aipw_est_2": 89, "aipw_obj_1": 89, "aipw_obj_2": 89, "air": [60, 86], "al": [10, 11, 32, 34, 41, 42, 58, 60, 61, 62, 64, 75, 76, 77, 78, 79, 80, 81, 84, 86, 87, 88, 91, 94, 98, 108, 110, 114, 122, 123, 129, 131, 132, 133, 138, 139, 140, 149, 150, 152, 154], "alexandr": [79, 153], "algebra": 108, "algorithm": [57, 59, 60, 62, 63, 64, 66, 67, 68, 69, 73, 75, 78, 83, 84, 86, 88, 91, 94, 96, 105, 107, 108, 110, 111, 113, 122, 123, 138, 154, 155], "alia": [46, 47, 50, 51], "align": [58, 60, 63, 65, 66, 69, 75, 78, 84, 86, 87, 89, 90, 91, 96, 108, 111, 113, 114, 123, 125, 127, 154], "all": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 47, 50, 51, 54, 58, 59, 60, 61, 63, 64, 72, 73, 75, 82, 83, 84, 85, 86, 87, 88, 90, 92, 95, 96, 98, 99, 101, 104, 106, 107, 108, 109, 110, 112, 114, 122, 123, 125, 127, 138, 139, 149, 150, 151, 154], "all_coef": 122, "all_dml1_coef": 97, "all_s": 122, "all_smpl": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 83], "all_smpls_clust": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38], "all_z_col": [60, 86], "allow": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 46, 47, 50, 51, 61, 66, 69, 72, 73, 87, 88, 92, 104, 106, 107, 108, 122, 123, 138, 150, 154, 155], "almqvist": 153, "along": [72, 107], "alongsid": [100, 102, 104], "alpha": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 40, 42, 58, 60, 61, 63, 66, 69, 70, 72, 73, 75, 76, 77, 78, 79, 83, 84, 85, 86, 87, 88, 91, 97, 98, 106, 107, 108, 122, 123, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149], "alpha_": [41, 60, 86, 107], "alpha_0": [139, 149], "alpha_ml_l": 79, "alpha_ml_m": 79, "alpha_x": [25, 33, 108], "alreadi": [18, 63, 64, 66, 69, 93, 96, 107, 108, 110], "also": [12, 14, 15, 16, 22, 25, 26, 38, 57, 58, 59, 60, 61, 62, 64, 66, 67, 69, 72, 73, 74, 75, 76, 77, 80, 81, 82, 84, 85, 86, 87, 88, 90, 92, 94, 95, 98, 106, 107, 108, 122, 123, 138, 139, 140, 151, 152, 154, 155], "alter": [60, 86], "altern": [59, 61, 62, 67, 87, 90, 105, 107, 138, 150, 152], "although": 95, "alwai": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 47, 51, 59, 92, 154], "always_tak": [25, 61, 87], "alyssa": 153, "amamb": 86, "american": [40, 89], "amgrem": 86, "amhorn": 86, "amit": [95, 153], "amjavl": 86, "ammata": 86, "among": [61, 79, 87, 88, 94, 95], "amount": [16, 61, 85, 87, 88, 155], "amp": [57, 60, 62, 63, 64, 66, 67, 68, 69, 86, 88, 94, 96], "an": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 46, 47, 50, 51, 58, 59, 60, 61, 62, 63, 66, 67, 68, 69, 73, 75, 76, 77, 79, 82, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 106, 107, 108, 109, 112, 114, 115, 122, 123, 138, 139, 140, 145, 150, 152, 153, 154, 155], "analog": [20, 21, 60, 66, 67, 69, 86, 88, 94, 106, 108, 110, 123, 124, 125, 126, 127, 138, 139, 145], "analys": [99, 104, 155], "analysi": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 45, 58, 60, 61, 75, 86, 87, 88, 93, 98, 105, 106, 108, 113, 140, 145, 149, 150, 154], "analyst": 93, "analyt": [89, 91], "analyz": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 61, 87, 88, 94, 155], "ancillari": 95, "andrea": 153, "angl": 61, "angrist": 89, "ani": [57, 58, 59, 62, 63, 64, 72, 74, 75, 93, 95, 96, 98, 108, 151, 155], "anna": [12, 14, 15, 17, 18, 19, 31, 35, 39, 59, 64, 66, 67, 68, 69, 108, 109, 110, 112, 114, 153], "annal": [138, 153], "anneal": 107, "annot": 58, "annual": 153, "anoth": [58, 59, 60, 61, 75, 84, 85, 86, 93, 98, 107, 108, 116], "anticip": [14, 16, 17, 19, 59, 67, 68, 108, 109, 111, 112, 113, 114], "anticipation_period": [14, 16, 17, 19, 66, 69], "anymor": [60, 86], "aos1161": 138, "aos1230": 138, "aos1671": 138, "ap": [61, 87], "ape_e401_uncond": 61, "ape_p401_uncond": 61, "api": [72, 99, 104, 150, 154], "apo": [22, 23, 72, 115, 128, 146], "apo_result": 72, "apoorva": 154, "apoorva__l": 89, "apoorval": 89, "app": 154, "appdata": 72, "appeal": 95, "append": [72, 75, 84, 93, 98], "appendix": [34, 36, 63, 66, 69, 94, 96, 139, 140], "appli": [11, 12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 52, 53, 57, 58, 60, 61, 62, 63, 65, 66, 68, 69, 75, 83, 84, 86, 87, 88, 92, 93, 95, 96, 98, 108, 122, 123, 138, 150, 152, 153, 154, 155], "applic": [58, 64, 72, 75, 89, 95, 98, 100, 104, 106, 122, 153, 155], "applicatoin": 67, "apply_along_axi": 90, "apply_cross_fit": [58, 122], "apply_crossfit": 154, "appreci": 150, "approach": [14, 16, 22, 24, 25, 26, 27, 28, 29, 30, 59, 60, 72, 73, 86, 92, 94, 95, 105, 107, 108, 122, 138, 139, 140, 151, 153, 155], "appropri": [61, 79, 87, 108, 122, 155], "approx": [106, 108, 111, 113], "approxim": [58, 69, 75, 76, 77, 78, 84, 91, 95, 98, 106, 108, 138, 154, 155], "april": 66, "apt": 151, "ar": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 37, 38, 39, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 75, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 134, 135, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155], "arang": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 65, 75, 78, 88, 90, 91, 94, 95, 99, 104, 107], "arbitrarili": [47, 51], "architectur": [72, 123, 153], "arellano": 153, "arg": [85, 92, 106, 108], "argmax": 93, "argmin": 84, "argu": [58, 61, 75, 87, 88, 94, 98, 155], "argument": [12, 14, 15, 17, 18, 19, 22, 24, 25, 26, 27, 28, 30, 37, 38, 41, 42, 43, 44, 45, 48, 61, 64, 66, 67, 68, 69, 76, 77, 82, 84, 86, 87, 88, 93, 97, 99, 106, 107, 108, 109, 154, 155], "aris": [58, 59, 60, 67, 75, 86, 95, 98, 155], "aronow": 89, "around": [59, 61, 72, 87, 88, 92, 108, 123], "arr": 90, "arrai": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 46, 47, 48, 49, 50, 51, 52, 63, 64, 66, 67, 68, 69, 72, 73, 75, 76, 77, 78, 84, 86, 89, 90, 93, 94, 95, 96, 97, 98, 100, 102, 103, 106, 107, 122, 138, 139, 145, 152, 154, 155], "arrang": 60, "array_lik": 29, "articl": [43, 150], "arxiv": [41, 59, 60, 86, 93, 95, 150, 153, 154], "as_learn": [62, 107], "asarrai": [76, 77], "aspect": [61, 87, 88], "assert": 107, "assess": 59, "asset": [88, 94, 155], "assign": [4, 5, 6, 7, 8, 9, 17, 19, 52, 61, 66, 69, 72, 87, 92, 99, 104, 106, 107, 108, 109, 121, 139, 155], "assmput": [108, 121], "associ": [61, 79, 87, 108, 138, 153], "assum": [57, 60, 64, 74, 86, 89, 90, 93, 95, 108, 109, 110, 112, 114, 123, 124, 125, 126, 127, 138, 139, 149, 155], "assumpt": [59, 60, 61, 63, 64, 65, 66, 67, 69, 84, 86, 87, 89, 92, 96, 108, 109, 110, 112, 114, 121, 138, 155], "assur": 154, "astyp": [66, 68, 74, 92, 95], "asymptot": [20, 21, 58, 60, 72, 75, 86, 98, 122, 138, 153], "ate": [72, 73], "ate_estim": [63, 96], "ates": [72, 73], "athei": 153, "att": [16, 17, 19, 26, 31, 59, 65, 82, 83, 90, 95, 106, 108, 109, 110, 111, 112, 113, 114, 118, 123, 131, 139, 147, 154], "att_": [108, 112], "att_gt": [59, 67, 68], "attach": 59, "atte_estim": 64, "attempt": [46, 47], "attenu": [61, 87], "attr": 61, "attribut": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 48, 49, 50, 51, 52, 53, 84, 85, 97, 101, 104, 107, 122, 123, 138], "attributeerror": [46, 47], "attrict": 108, "attrit": [30, 63, 96, 108, 121], "au": [62, 107, 150, 152], "auc": 59, "author": [59, 95, 150], "auto_ml": 85, "autodoubleml": 85, "autom": 85, "automat": [13, 16, 58, 66, 68, 69, 75, 82, 98, 106, 139, 145], "automl": 154, "automl_l": 85, "automl_l_lesstim": 85, "automl_m": 85, "automl_m_lesstim": 85, "automobil": [60, 86], "autos": 79, "autosklearn": 85, "auxiliari": [58, 75, 98], "avail": [33, 59, 61, 62, 64, 66, 67, 69, 72, 73, 79, 84, 87, 88, 89, 90, 92, 95, 98, 99, 104, 106, 107, 108, 109, 110, 111, 113, 139, 149, 150, 151, 154, 155], "avaiv": 49, "aver": 73, "averag": [17, 18, 19, 22, 23, 25, 26, 31, 38, 39, 57, 59, 62, 63, 64, 65, 66, 68, 69, 74, 82, 88, 89, 90, 92, 93, 94, 95, 96, 105, 109, 110, 114, 115, 116, 117, 118, 121, 128, 131, 138, 146, 147, 153, 154, 155], "average_it": [72, 73], "avoid": [53, 58, 59, 75, 92, 108, 122, 151, 154], "awai": 94, "ax": [13, 16, 65, 66, 67, 68, 69, 72, 73, 75, 76, 77, 78, 80, 81, 84, 85, 86, 87, 88, 89, 91, 92], "ax1": [72, 73, 78, 83, 88, 91], "ax2": [72, 73, 78, 83, 88, 91], "axhlin": [65, 72, 85, 92], "axi": [13, 16, 60, 61, 72, 73, 79, 83, 84, 86, 87, 89, 90, 92], "axvlin": [72, 73, 75], "b": [12, 14, 15, 16, 17, 19, 43, 58, 60, 62, 75, 76, 77, 86, 89, 91, 92, 95, 98, 106, 107, 108, 114, 138, 139, 149, 150, 152, 153], "b208": 62, "b371": 62, "b5d34a6f42b": 62, "b5d7": 62, "b_": 108, "b_0": 42, "b_1": 42, "b_j": 43, "bach": [79, 84, 85, 95, 150, 153, 154], "back": 72, "backbon": 84, "backend": [5, 6, 7, 8, 9, 59, 88, 94, 95, 99, 101, 103, 105, 154], "backward": [4, 99, 104, 154], "bad": 89, "bag": 72, "balanc": [61, 66, 67, 69, 87, 88], "bam5698": 72, "band": [59, 105, 155], "bandwidth": [27, 28, 29, 44, 92, 108, 154], "bar": [82, 85, 87, 106, 108, 123, 128, 131, 139, 146], "base": [12, 14, 15, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 37, 38, 45, 49, 53, 58, 59, 60, 61, 63, 64, 65, 67, 68, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 102, 104, 106, 107, 108, 109, 112, 114, 121, 122, 123, 138, 139, 140, 145, 150, 152, 153, 154, 155], "base_estim": [50, 51, 92], "baseestim": 93, "baselin": [14, 35, 61, 85, 87], "basi": [22, 26, 38, 48, 69, 76, 77, 83, 106], "basic": [59, 60, 61, 64, 68, 86, 87, 88, 89, 92, 94, 95, 105, 107], "basis_df": 83, "basis_matrix": 83, "batch": 62, "battocchi": 153, "bay": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 138], "bb2913dc": 62, "bbotk": [62, 107, 154], "bbox_inch": 75, "bbox_to_anchor": 75, "bcallaway11": [59, 67], "bd929a9e": 62, "bde4": 62, "becam": [61, 87, 88], "becaus": [47, 51, 57, 58, 59, 60, 74, 75, 82, 86, 89, 93, 95, 98, 155], "becker": [62, 107], "becom": [60, 85, 86, 106, 122], "bee": 65, "been": [13, 16, 60, 61, 66, 68, 69, 85, 86, 87, 88, 94, 95, 106, 107, 108, 114, 150, 154], "befor": [17, 19, 52, 53, 59, 61, 65, 66, 68, 69, 73, 82, 87, 91, 95, 108, 110, 155], "begin": [33, 40, 41, 58, 60, 61, 62, 63, 65, 66, 69, 75, 78, 84, 86, 87, 89, 90, 91, 96, 97, 99, 104, 107, 108, 111, 113, 114, 122, 123, 125, 127, 138, 152, 155], "behav": [66, 68, 69, 93], "behavior": [61, 89, 107], "behind": [67, 108], "being": [17, 19, 20, 21, 35, 36, 45, 50, 51, 60, 69, 86, 92, 95, 108, 113, 114, 122, 123, 129, 138, 139, 145, 150], "belloni": [34, 79, 138, 153], "below": [52, 53, 57, 61, 67, 74, 87, 89, 93, 108, 151, 152], "bench_x1": 95, "bench_x2": 95, "benchmark": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 54, 72, 73, 82, 140, 154], "benchmark_dict": [54, 94], "benchmark_inc": 94, "benchmark_pira": 94, "benchmark_result": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38], "benchmark_twoearn": 94, "benchmarking_set": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 73, 82, 94, 95, 139, 140], "benchmarking_vari": 82, "benefit": [58, 61, 75, 87, 98], "bernoulli": 33, "berri": [60, 86], "besid": 152, "best": [22, 26, 38, 47, 48, 51, 66, 69, 72, 76, 77, 80, 81, 85, 151], "best_loss": 85, "beta": [30, 33, 34, 36, 40, 61, 63, 87, 90, 92, 96, 108], "beta_": [63, 96], "beta_0": [32, 63, 90, 96, 106], "beta_a": [31, 39, 95], "beta_j": [33, 34, 36, 40], "better": [59, 66, 69, 72, 73, 84, 95, 108, 114], "between": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 53, 57, 63, 65, 66, 69, 72, 73, 74, 78, 79, 85, 89, 91, 93, 94, 95, 96, 108, 116, 123, 124, 125, 126, 127, 131, 134, 135, 138, 139, 149, 152, 154], "betwen": [57, 74], "beyond": 153, "bia": [36, 57, 63, 74, 79, 92, 95, 96, 105, 108, 121, 122, 123, 136, 137, 139, 149, 153, 154], "bias": [57, 61, 66, 69, 74, 87, 88, 94, 155], "bias_bench": 95, "bibtex": 150, "big": [79, 97, 122, 123, 124, 125, 132, 138, 139, 141, 142, 143, 144, 147, 148, 149], "bigg": [60, 86, 123, 130, 131, 139, 142, 147], "bilia": 11, "bilinski": 153, "bin": [58, 72, 73, 75, 151], "binari": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 32, 38, 45, 57, 59, 61, 62, 64, 74, 82, 83, 84, 87, 89, 90, 95, 106, 107, 110, 114, 117, 118, 121, 139, 146, 147, 154, 155], "binary_outcom": 45, "binary_treat": [32, 76, 80, 82], "bind": 154, "binder": [62, 107, 150, 152, 154], "binomi": [74, 89, 90, 91, 93], "bischl": [62, 107, 150, 152], "black": [58, 62, 70, 99, 104, 152], "blob": 59, "blog": 43, "blondel": [150, 152], "blp": [48, 60, 86], "blp_data": [60, 86], "blp_model": [80, 81], "blue": [58, 60, 63, 86], "bodori": 153, "bond": [61, 87, 88], "bonferroni": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 138], "bonu": [11, 62, 99, 104, 152], "book": [62, 93, 95, 107, 153], "bool": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 44, 45, 46, 47, 48, 50, 51, 52, 53, 82, 92], "boolean": [36, 80, 81, 99, 104, 122], "boost": [57, 61, 64, 66, 69, 72, 74, 84, 87], "boost_class": [61, 87], "boost_summari": 87, "boostrap": [78, 154], "bootstrap": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 48, 66, 67, 68, 69, 73, 76, 77, 78, 80, 81, 88, 91, 105, 106, 108, 109, 122, 123, 150, 152, 154, 155], "both": [12, 13, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 59, 61, 62, 64, 65, 72, 83, 84, 85, 87, 88, 90, 92, 93, 94, 95, 99, 100, 104, 107, 108, 111, 112, 113, 138, 139, 140, 145, 148, 149, 154, 155], "bottom": [60, 61, 84, 86, 87, 88], "bound": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 35, 37, 38, 52, 53, 61, 66, 69, 72, 73, 82, 83, 87, 94, 95, 139, 140, 145, 149, 154, 155], "branch": 62, "brantli": [59, 153], "break": [58, 154], "breviti": 155, "brew": 151, "brewer": 60, "bridg": 95, "brief": 98, "briefli": 67, "bring": [57, 74], "brucher": [150, 152], "bsd": [150, 154], "bst": 87, "budget": [85, 107], "bug": [150, 154], "build": [60, 84, 86, 90], "build_design_matric": [76, 77], "build_sim_dataset": 59, "built": [49, 85, 107, 150], "bureau": [95, 122, 153], "busi": [36, 41, 60, 86, 95, 153], "b\u00fchlmann": 153, "c": [10, 11, 17, 18, 19, 34, 39, 40, 42, 57, 58, 59, 60, 61, 62, 65, 70, 72, 74, 75, 79, 80, 81, 86, 87, 89, 92, 93, 98, 99, 104, 107, 108, 114, 123, 125, 127, 139, 142, 144, 150, 151, 152, 153, 155], "c1": [10, 11, 42, 60, 79, 86, 93, 98, 150, 153], "c68": [10, 11, 42, 60, 79, 86, 93, 98, 150, 153], "c895": 62, "c_": [16, 108, 111, 113, 114, 123, 125, 127, 138], "c_d": [34, 139, 147, 148, 149], "c_i": [108, 114], "c_y": [34, 139, 149], "ca1af7be64b2": 62, "caac5a95": 62, "calcualt": 90, "calcul": [22, 26, 38, 59, 61, 66, 69, 72, 73, 76, 77, 78, 80, 81, 84, 85, 87, 91, 94, 139, 145, 149], "calendar": [66, 67, 68, 69], "calibr": [14, 16, 22, 24, 25, 26, 27, 28, 29, 30, 52, 53, 84, 85, 95], "calibration_method": [52, 53], "call": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 47, 50, 51, 57, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 72, 74, 76, 77, 78, 80, 81, 87, 88, 90, 91, 92, 94, 95, 96, 99, 104, 107, 122, 123, 138, 139, 145, 149, 152, 154, 155], "callabl": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 58, 75, 76, 77, 84, 105, 107, 150], "callawai": [17, 19, 59, 66, 67, 68, 69, 108, 109, 112, 114, 153], "camera": 79, "cameron": [60, 86], "can": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 44, 47, 49, 51, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 104, 106, 107, 108, 109, 110, 112, 114, 115, 121, 122, 123, 124, 125, 126, 127, 128, 131, 134, 135, 138, 139, 140, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155], "candid": 95, "cannot": [84, 92, 95, 108, 155], "capabl": [5, 6, 7, 8, 9, 57, 74], "capo": [22, 83], "capo0": 83, "capo1": 83, "capsiz": [72, 73, 85, 89, 92], "capthick": [72, 73, 92], "cardin": [60, 86], "care": [67, 93, 107], "carlo": [31, 32, 35, 39, 76, 77, 80, 81, 95, 153], "casalicchio": [62, 107, 150, 152], "case": [4, 5, 6, 7, 8, 9, 11, 14, 22, 25, 26, 32, 44, 57, 60, 61, 66, 67, 69, 74, 76, 77, 78, 79, 82, 83, 85, 86, 90, 91, 92, 93, 94, 95, 99, 104, 106, 107, 108, 110, 114, 121, 122, 123, 125, 127, 138, 139, 145, 152, 154, 155], "cat": [58, 154], "catboost": 84, "cate": [26, 38, 48, 83, 105, 154], "cate_obj": 106, "categori": 72, "cattaneo": [108, 153], "caus": [58, 75, 92, 98], "causal": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 44, 57, 58, 60, 61, 62, 63, 74, 75, 83, 84, 85, 86, 87, 89, 93, 94, 96, 97, 98, 99, 102, 104, 105, 108, 114, 122, 138, 139, 145, 153], "causal_contrast": [23, 72, 73, 83, 108], "causal_contrast_att": 83, "causal_contrast_c": 83, "causal_contrast_model": [72, 73, 108], "causal_contrast_result": 72, "causaldml": 153, "causalml": [93, 153], "causalweight": 153, "caution": 138, "caveat": 95, "cbind": 60, "cbook": [66, 67, 68, 69], "cc": 87, "ccp_alpha": [26, 49, 87], "cd": 151, "cd_fast": 86, "cda85647": 62, "cdf": 106, "cdid": [60, 86], "cdot": [17, 18, 19, 31, 35, 39, 45, 60, 65, 66, 69, 78, 82, 86, 89, 91, 92, 95, 106, 108, 109, 111, 113, 114, 122, 123, 125, 127, 128, 131, 132, 136, 137, 138, 139, 142, 144, 146], "cdot1": 82, "cell": 85, "center": [66, 67, 69, 72, 79], "central": [122, 154], "certain": [108, 123, 125, 127], "cexcol": 60, "cexrow": 60, "cf_d": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 54, 66, 69, 73, 82, 83, 94, 95, 139, 140, 145, 146, 147, 148, 149, 155], "cf_y": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 54, 66, 69, 73, 82, 83, 94, 95, 139, 140, 145, 146, 147, 148, 149, 155], "cff": 154, "chad": 95, "challeng": [60, 86, 139, 140], "chanc": 69, "chang": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 50, 51, 61, 63, 64, 68, 88, 94, 95, 96, 108, 113, 114, 123, 127, 131, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 151, 153, 154], "channel": 155, "chapter": [20, 21, 62, 93, 107, 139, 149], "charact": [61, 62, 107, 154], "characterist": [94, 155], "chart": 85, "check": [46, 47, 50, 51, 58, 61, 64, 65, 66, 69, 75, 84, 85, 87, 88, 93, 97, 98, 108, 109, 150, 151, 154], "check_data": 154, "check_scor": 154, "checkmat": 154, "chernozhukov": [10, 11, 34, 40, 42, 58, 60, 61, 75, 79, 84, 85, 86, 87, 88, 93, 94, 98, 122, 123, 131, 138, 139, 140, 149, 150, 153, 154], "chetverikov": [10, 11, 42, 60, 79, 86, 93, 98, 138, 150, 153], "chiang": [41, 60, 86, 153], "chieh": 153, "choic": [5, 6, 7, 8, 9, 14, 16, 22, 24, 25, 26, 27, 28, 29, 30, 61, 66, 67, 68, 69, 79, 87, 90, 106, 107, 108, 112, 123, 125, 127, 139, 140, 145, 149, 154], "cholecyst": 93, "choos": [57, 61, 67, 74, 75, 79, 84, 87, 88, 97, 108, 112, 122, 123, 124, 125, 126, 127, 131, 134, 135, 138, 152, 155], "chosen": [22, 35, 39, 84, 107, 108], "chou": 89, "chr": 61, "christian": [79, 153], "christoph": 153, "chunk": 107, "ci": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 64, 65, 66, 67, 68, 69, 72, 73, 76, 77, 78, 80, 81, 82, 83, 85, 87, 88, 91, 92, 94, 95, 106, 108, 139, 145, 154, 155], "ci_at": [72, 73], "ci_cvar": [78, 88], "ci_cvar_0": 78, "ci_cvar_1": 78, "ci_joint": [66, 67, 68, 69, 73], "ci_joint_cvar": 78, "ci_joint_lqt": 91, "ci_joint_qt": 91, "ci_length": 64, "ci_low": [72, 73], "ci_lpq_0": 91, "ci_lpq_1": 91, "ci_lqt": [88, 91], "ci_pointwis": [72, 73], "ci_pq_0": [88, 91], "ci_pq_1": [88, 91], "ci_qt": [88, 91], "ci_upp": [72, 73], "cinelli": [95, 139, 140, 153], "circumv": 155, "citat": 154, "cite": 150, "cl": 65, "claim": 62, "clarifi": [66, 67, 68, 69], "clash": 59, "class": [0, 4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 46, 47, 48, 49, 50, 51, 52, 53, 61, 62, 63, 64, 66, 67, 68, 69, 70, 72, 73, 82, 83, 85, 87, 88, 93, 94, 96, 97, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 122, 123, 138, 150, 152, 154], "class_estim": 92, "class_learn": 88, "class_learner_1": 84, "class_learner_2": 84, "classes_": [85, 93], "classic": [59, 60, 67, 86, 155], "classif": [26, 46, 50, 57, 59, 61, 62, 63, 64, 66, 67, 68, 69, 84, 85, 88, 90, 94, 96, 106, 107, 108, 110, 111, 113, 155], "classifavg": 62, "classifi": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 38, 44, 46, 50, 62, 72, 73, 85, 92, 93, 107, 154], "classifiermixin": 93, "classmethod": [4, 5, 6, 7, 8, 9, 52], "claudia": [153, 154], "claus": [150, 154], "clean": 154, "cleaner": 84, "cleanup": 154, "clear": [60, 72, 86], "clearli": [66, 69, 92], "clever": 84, "client": 72, "clip": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 52, 53], "clipping_threshold": [12, 15, 52, 53, 76], "clone": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 57, 58, 62, 75, 84, 86, 88, 97, 107, 108, 122, 123, 138, 139, 145, 151, 152], "close": [53, 59, 61, 72, 87, 93, 95, 139, 140], "cluster": [5, 6, 7, 8, 9, 12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 41, 68, 99, 100, 102, 103, 104, 153, 154], "cluster_col": [4, 5, 6, 8, 9, 60, 86, 99, 100, 102, 103, 104], "cluster_var": [4, 5, 6, 7, 8, 9, 41, 99, 100, 104], "cluster_var_i": [60, 86], "cluster_var_j": [60, 86], "cmap": 86, "cmd": 154, "co": [43, 65], "codaci": 154, "code": [22, 26, 38, 43, 57, 59, 60, 61, 62, 63, 74, 79, 87, 98, 106, 107, 108, 122, 123, 138, 151, 152, 154, 155], "codecov": 154, "coef": [12, 15, 16, 24, 25, 26, 27, 28, 29, 30, 37, 38, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 94, 95, 96, 97, 98, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 152, 155], "coef_": 95, "coef_df": 60, "coef_valu": 85, "coeffici": [16, 31, 32, 39, 47, 48, 51, 61, 63, 80, 81, 84, 87, 89, 90, 92, 95, 96, 106, 138, 139, 145, 155], "coefs_t": 90, "coefs_w": 90, "coffici": [139, 145], "cofid": 48, "coincid": [65, 83, 88], "col": [58, 60, 87], "col_nam": [66, 69], "collect": [62, 63, 64, 72, 86, 96], "colnam": [60, 84], "color": [13, 16, 61, 63, 65, 69, 72, 73, 75, 76, 77, 78, 85, 86, 87, 88, 89, 91, 92, 95], "color_palett": [13, 16, 66, 69, 72, 73, 75, 86, 87, 88], "colorbar": 86, "colorblind": [13, 16, 66, 69, 72, 73], "colorramppalett": 60, "colorscal": [76, 77], "colour": [58, 60], "column": [5, 6, 7, 8, 9, 16, 64, 65, 66, 67, 68, 69, 70, 72, 73, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 92, 94, 95, 96, 99, 100, 101, 102, 103, 104, 106, 107, 108, 111, 113, 122, 152, 154, 155], "column_stack": [8, 65, 72, 73, 80, 81, 92, 94, 95, 108], "colv": 60, "com": [43, 59, 61, 62, 67, 68, 79, 89, 95, 107, 150, 151], "comb": 79, "combin": [14, 16, 59, 60, 62, 64, 67, 68, 72, 73, 83, 84, 85, 86, 95, 107, 108, 112, 122, 139, 145, 154], "combind": 88, "combined_loss": 79, "come": [97, 107, 123, 139, 140, 150, 155], "command": [151, 154], "comment": [99, 104], "commit": 154, "common": [84, 94, 95, 106, 108, 153], "commonli": 72, "companion": 153, "compar": [58, 60, 65, 66, 69, 72, 75, 76, 77, 78, 80, 81, 83, 89, 91, 92, 93, 95, 98, 107, 108, 139, 140, 154], "comparevers": 61, "comparison": [66, 69, 73, 84, 89], "compat": [4, 57, 59, 74, 99, 104, 154], "complement": 95, "complet": [85, 98, 139, 145, 151], "complex": [26, 59, 85], "compli": [92, 108], "complianc": [91, 92, 108, 123, 132], "complic": [62, 155], "complier": [61, 87, 88, 91, 92, 106, 108], "compon": [17, 19, 50, 51, 59, 61, 67, 72, 79, 84, 85, 87, 90, 106, 107, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 134, 135, 154], "compont": 59, "composit": 153, "comprehens": 72, "compris": 138, "comput": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 48, 54, 58, 59, 61, 62, 66, 67, 68, 69, 72, 75, 87, 88, 93, 94, 95, 122, 123, 139, 140, 141, 142, 143, 144, 145, 146, 147, 150, 153, 154, 155], "computation": [139, 140], "concat": [72, 85, 86, 87, 90, 138], "concaten": [65, 87, 138], "concentr": 138, "concern": 95, "conclud": [92, 95, 155], "cond": [108, 110, 121], "conda": [153, 154], "condit": [20, 21, 22, 24, 26, 31, 32, 38, 39, 58, 60, 61, 63, 64, 65, 66, 69, 73, 75, 82, 83, 86, 87, 90, 92, 95, 96, 98, 105, 108, 111, 112, 113, 114, 138, 139, 146, 147, 149, 152, 153, 154, 155], "conduct": [106, 108, 110, 111, 113, 155], "conf": [59, 91], "confer": 153, "confid": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 48, 59, 60, 61, 63, 64, 66, 67, 68, 69, 72, 73, 76, 77, 78, 80, 81, 83, 86, 88, 91, 92, 94, 96, 105, 106, 108, 122, 123, 139, 145, 152, 153, 154, 155], "confidenceband": 78, "confidenti": 95, "config": [52, 53, 89], "configur": [14, 16, 22, 24, 25, 26, 27, 28, 29, 30, 53, 62, 85], "confint": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 48, 61, 63, 64, 65, 66, 67, 68, 69, 72, 73, 76, 77, 78, 80, 81, 83, 84, 88, 90, 91, 92, 93, 94, 96, 106, 122, 138, 150, 152, 155], "conflict": 151, "confound": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 37, 38, 39, 54, 57, 61, 66, 67, 69, 74, 82, 87, 91, 94, 95, 99, 104, 108, 119, 120, 138, 139, 140, 145, 148, 149, 152, 153, 154, 155], "congress": 153, "connect": [61, 87, 88], "consequ": [31, 39, 60, 82, 86, 94, 106, 108, 110, 139, 140, 146, 147, 149], "conserv": [94, 95, 139, 149], "consid": [24, 25, 26, 27, 28, 45, 52, 53, 58, 60, 61, 63, 64, 65, 66, 67, 68, 69, 75, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 106, 107, 108, 109, 114, 117, 118, 122, 123, 138, 139, 140, 150, 155], "consider": [95, 108], "consist": [37, 38, 47, 51, 61, 64, 85, 87, 88, 89, 95, 98, 99, 104, 108, 114, 119, 120, 152, 154], "consol": [58, 154], "constant": [17, 19, 34, 47, 51, 66, 69, 79, 90, 106, 108, 138], "constrained_layout": 75, "construct": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 37, 38, 48, 62, 65, 66, 69, 76, 77, 78, 83, 88, 93, 94, 97, 106, 123, 129, 137, 138, 154, 155], "construct_framework": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "construct_iv": 86, "constructiv": 60, "constructor": [62, 100, 104], "consum": [60, 86], "cont": 35, "cont_d": [72, 73], "contain": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 45, 46, 47, 50, 51, 58, 60, 61, 66, 67, 69, 73, 75, 76, 77, 80, 81, 84, 86, 87, 93, 98, 100, 101, 103, 104, 106, 107, 108, 109, 111, 113, 138, 139, 140, 145, 154], "context": [95, 108, 121, 155], "contin": [35, 85], "continu": [35, 57, 62, 72, 73, 74, 79, 89, 92, 108, 139, 149, 154, 155], "contour": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 79, 82, 94, 95, 139, 145], "contour_plot": 95, "contours_z": [76, 77], "contrast": [23, 64, 72, 78, 83, 108, 116], "contribut": [151, 154], "contributor": 154, "control": [14, 16, 17, 19, 40, 45, 52, 59, 67, 68, 79, 83, 88, 90, 92, 93, 95, 99, 100, 104, 108, 109, 111, 112, 113, 114, 123, 125, 127, 139, 142, 144, 155], "control_group": [14, 16, 66, 67, 68, 69, 108, 109, 111, 113], "conveni": [99, 102, 104], "convent": [61, 67, 87, 88, 92, 108, 114], "converg": [58, 75, 84, 86, 98], "convergencewarn": 86, "convers": 86, "convert": [72, 78, 86, 91], "convex": 89, "cooper": 154, "coor": [62, 107, 150, 152], "coordin": [72, 95], "copi": [69, 85, 87, 90, 95], "cor": [139, 149], "core": [17, 19, 66, 67, 68, 69, 70, 72, 73, 78, 82, 86, 87, 88, 91, 94, 99, 101, 104, 107, 152, 154], "cores_us": [78, 88, 91], "correct": [82, 83, 95, 106, 138, 154], "correctli": [46, 50, 64, 89, 94, 139, 149], "correl": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 60, 63, 79, 86, 93, 94, 96, 108, 139, 140, 149], "correpond": [66, 69, 108], "correspond": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 58, 60, 61, 62, 64, 65, 66, 67, 68, 69, 73, 75, 76, 77, 79, 83, 84, 86, 87, 88, 90, 91, 94, 95, 98, 106, 107, 108, 110, 112, 114, 115, 121, 122, 123, 125, 127, 138, 139, 140, 142, 144, 145, 147, 149, 154, 155], "correspondingli": [66, 69], "cosh": 43, "coul": 60, "could": [57, 62, 66, 67, 69, 74, 76, 77, 85, 95, 154, 155], "counfound": [31, 39, 91, 94, 106, 139, 149], "count": [72, 73, 87, 88], "counti": 67, "countour": [139, 145], "countyr": 67, "coupl": [61, 87, 88], "cournapeau": [150, 152], "cours": [61, 84, 87, 95, 138, 155], "cov": [30, 31, 45, 92], "cov_nam": [92, 108], "cov_typ": [22, 26, 38, 48, 154], "covari": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 19, 26, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 48, 49, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 70, 72, 73, 75, 76, 77, 79, 80, 81, 82, 85, 86, 87, 88, 89, 90, 93, 94, 95, 96, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 110, 111, 113, 119, 120, 121, 123, 124, 125, 126, 127, 138, 139, 140, 152, 153, 154], "cover": [59, 79, 94, 103, 104], "coverag": [66, 69, 84, 92, 93, 106, 154], "cp": [61, 62, 107], "cpu": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 72], "cpu_count": [78, 88, 91], "cran": [62, 153, 154], "creat": [13, 16, 17, 19, 32, 52, 57, 60, 62, 66, 69, 73, 74, 75, 76, 77, 78, 80, 81, 86, 88, 90, 91, 95, 99, 104, 107, 139, 140, 145, 149, 151, 154], "create_synthetic_group_data": 90, "crictial": 122, "critic": [95, 155], "cross": [12, 14, 15, 16, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 52, 53, 57, 58, 59, 61, 62, 63, 67, 72, 75, 84, 85, 87, 88, 92, 95, 98, 100, 104, 105, 107, 111, 112, 114, 126, 137, 138, 141, 142, 145, 154, 155], "cross_sectional_data": [15, 18, 64, 108, 110], "crossfit": [84, 108], "crosstab": 89, "crucial": [79, 108, 155], "csail": [150, 152], "csdid": 68, "csv": [68, 79], "cuda": 72, "cumul": 108, "current": [49, 53, 59, 64, 65, 66, 69, 83, 123, 139, 149, 150, 151, 155], "custom": [13, 16, 58, 59, 67, 75, 95, 107], "custom_measur": 59, "cut": 90, "cutoff": [44, 45, 92, 108], "cv": [52, 62, 87, 107, 122], "cv_calibr": [52, 53], "cv_glmnet": [60, 61, 62, 63, 107, 108, 138, 152], "cvar": [24, 29, 105, 129, 154], "cvar_0": 78, "cvar_1": 78, "d": [4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 48, 49, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 113, 115, 117, 118, 119, 120, 121, 122, 123, 124, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 143, 145, 146, 147, 148, 149, 150, 152, 153, 155], "d0": [78, 91, 138], "d0_true": 91, "d0cdb0ea4795": 62, "d1": [78, 89, 91, 138], "d10": 138, "d1_true": 91, "d2": [89, 138], "d21ee5775b5f": 62, "d2cml": 68, "d3": 138, "d4": 138, "d5": 138, "d5a0c70f1d98": 62, "d6": 138, "d7": 138, "d8": 138, "d9": 138, "d_": [35, 41, 60, 65, 73, 86, 108, 110, 114, 138], "d_0": [108, 115], "d_1": [89, 138], "d_2": 89, "d_col": [4, 5, 6, 7, 8, 9, 16, 57, 58, 60, 61, 62, 63, 66, 67, 68, 69, 74, 76, 77, 80, 81, 83, 86, 87, 88, 90, 92, 93, 94, 97, 98, 99, 100, 101, 103, 104, 107, 108, 109, 111, 113, 122, 123, 152, 154, 155], "d_i": [32, 33, 34, 36, 40, 42, 43, 58, 63, 64, 73, 75, 78, 89, 91, 92, 96, 98, 108, 110, 121], "d_j": [73, 108, 115, 116, 138], "d_k": [108, 116, 138], "d_l": [108, 115], "d_w": 90, "da1440": 89, "dag": [63, 95, 96, 155], "dai": 66, "dark": [58, 75], "darkblu": 60, "darkr": 60, "dash": [63, 66, 69, 72, 73], "dat": [99, 104], "data": [0, 10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 50, 51, 59, 65, 79, 84, 89, 93, 97, 99, 101, 102, 103, 105, 106, 107, 109, 111, 112, 113, 114, 122, 138, 142, 143, 144, 145, 153, 154], "data_apo": [72, 73], "data_cvar": 88, "data_dict": [44, 76, 77, 80, 81, 82, 92, 108], "data_dml": 94, "data_dml_bas": [61, 76, 77, 80, 81, 87, 88, 90], "data_dml_base_iv": [61, 87, 88], "data_dml_flex": [61, 87], "data_dml_flex_iv": 61, "data_dml_iv_flex": 87, "data_dml_new": 90, "data_fram": 155, "data_lqt": 88, "data_pq": 88, "data_qt": 88, "data_transf": [60, 86, 87], "dataclass": 53, "datafram": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 44, 48, 49, 60, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 100, 101, 102, 103, 106, 107, 108, 110, 123, 138, 139, 140, 145, 152, 155], "dataset": [0, 5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 58, 59, 63, 64, 66, 69, 72, 73, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 139, 145, 152, 155], "datatyp": [68, 154], "date": [16, 17, 19, 107], "date_format": 16, "datetim": [7, 16, 17, 19, 66, 67, 69, 101, 104], "datetime64": [66, 101, 104], "datetime_unit": [7, 16, 66, 69, 101, 104, 108, 109, 111, 113], "david": 154, "db": [61, 87, 88, 94, 155], "dbl": [59, 60, 61, 62, 99, 104, 138, 152, 155], "dc13a11076b3": 62, "ddc9": 62, "de": [57, 74, 153], "deal": [57, 74], "debias": [10, 11, 41, 42, 60, 79, 86, 93, 105, 107, 122, 150, 153, 154], "debt": [61, 87, 88], "decai": [63, 96], "decid": [61, 87, 93], "decis": [26, 57, 61, 74, 87, 88, 106, 108, 153, 155], "decision_effect": 57, "decision_impact": [57, 74], "decisiontreeclassifi": [26, 49, 87], "decisiontreeregressor": 87, "declar": 155, "decomposit": [108, 109], "decreas": 92, "dedic": [99, 103, 104], "deep": [46, 47, 50, 51, 85], "deeper": 26, "def": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 66, 69, 75, 78, 84, 85, 86, 89, 90, 91, 93, 95, 107, 123], "default": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 59, 60, 63, 64, 66, 67, 68, 69, 80, 81, 84, 86, 90, 92, 94, 95, 96, 97, 100, 104, 106, 107, 108, 122, 138, 139, 145, 146, 152, 155], "default_arg": [66, 69], "default_convert": 86, "default_jitt": 16, "defier": [92, 108], "defin": [17, 18, 19, 31, 32, 33, 34, 35, 36, 39, 40, 41, 42, 43, 44, 45, 47, 51, 58, 61, 62, 64, 67, 72, 73, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 87, 88, 90, 91, 92, 93, 94, 95, 106, 107, 108, 110, 111, 113, 114, 121, 123, 124, 126, 127, 139, 140, 145, 149, 154], "definit": [43, 66, 69, 80, 81, 83, 108, 112, 139, 146, 147], "defint": 139, "degre": [45, 61, 76, 77, 83, 86, 87, 92, 106, 139, 140], "dekel": 153, "delete_origin": 62, "deliber": 89, "delta": [16, 40, 59, 64, 66, 69, 95, 108, 110, 111, 112, 113, 114, 123, 125, 127], "delta_bench": 95, "delta_i": 59, "delta_j": 40, "delta_t": [17, 19, 66, 69], "delta_theta": [54, 73, 82, 94, 95, 139, 140], "delta_v": 95, "demand": [60, 86, 139, 140], "demir": [10, 11, 42, 60, 79, 86, 93, 98, 122, 150, 153], "demo": [67, 95], "demonstr": [58, 59, 60, 67, 72, 75, 86, 92, 95, 99, 104, 108, 138, 150, 152], "deni": 153, "denomin": [139, 140, 146, 147], "denot": [19, 37, 60, 61, 63, 64, 65, 69, 86, 87, 92, 95, 96, 106, 108, 110, 111, 113, 114, 115, 119, 123, 139, 140, 145, 147, 149], "dens_net_tfa": 61, "densiti": [27, 28, 29, 58, 63, 72, 73, 75], "dep": 70, "dep1": [62, 70, 99, 104, 152], "dep2": [62, 70, 99, 104, 152], "depend": [17, 19, 22, 24, 26, 27, 29, 32, 62, 64, 66, 69, 76, 77, 80, 81, 82, 84, 85, 90, 92, 97, 106, 107, 108, 111, 112, 113, 123, 132, 133, 139, 140, 146, 149, 152, 153], "deprec": [4, 14, 16, 22, 24, 25, 26, 27, 28, 29, 30, 64, 65, 66, 67, 68, 69, 76, 97, 99, 104, 108, 122, 123, 139], "deprecationwarn": [65, 76], "depreci": 154, "depth": [26, 49, 61, 62, 90, 97, 106, 107, 108, 122, 123, 138, 152, 155], "deriv": [12, 14, 15, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 108, 138], "describ": [13, 59, 60, 66, 69, 86, 87, 88, 95, 107, 122, 151, 154], "descript": [19, 61, 68, 70, 94, 107, 122, 123, 125, 127, 139, 140, 142, 144], "deserv": 108, "design": [8, 17, 19, 44, 45, 72, 73, 85, 102, 104, 105, 153, 154], "design_info": [76, 77], "design_matrix": [76, 77, 106], "desir": [39, 62, 90, 108, 151], "detail": [12, 14, 15, 20, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 58, 61, 62, 64, 65, 72, 73, 75, 79, 85, 88, 92, 94, 95, 98, 99, 104, 106, 107, 109, 110, 111, 112, 113, 114, 121, 123, 129, 131, 132, 133, 136, 137, 138, 139, 140, 142, 144, 149, 150, 151, 152, 154, 155], "determin": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 47, 51, 61, 69, 78, 87, 88, 91, 92, 94, 108, 138, 139, 149], "determinist": [90, 92, 106, 108], "deutsch": 150, "dev": [151, 154], "develop": [59, 60, 62, 86, 95, 108, 110, 114, 154], "deviat": [84, 108, 139, 149], "devic": 72, "dezeur": 153, "df": [5, 6, 7, 8, 9, 16, 57, 58, 60, 63, 65, 66, 69, 72, 73, 74, 76, 77, 78, 83, 86, 89, 91, 92, 94, 95, 96, 98, 100, 101, 103, 104, 106, 108, 109, 111, 113], "df_agg": 79, "df_all_apo": 72, "df_all_at": 72, "df_anticip": [66, 69], "df_apo": [72, 73], "df_apo_ci": 73, "df_apos_ci": 73, "df_ate": [72, 73], "df_bench": 95, "df_binari": 95, "df_bonu": [62, 99, 104, 152], "df_capo0": 83, "df_capo1": 83, "df_cate": [76, 77, 83], "df_causal_contrast_c": 83, "df_ci": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 48], "df_coef": 84, "df_cvar": 88, "df_fuzzi": 92, "df_lqte": 88, "df_ml_g0": 84, "df_ml_g1": 84, "df_ml_m": 84, "df_pa": [64, 96], "df_perform": 72, "df_plot": 60, "df_post_treat": [66, 69], "df_pq": 88, "df_qte": 88, "df_result": 79, "df_sharp": 92, "df_sort": [72, 73], "df_summari": 87, "df_treat": 69, "df_wide": 86, "dfg": 150, "dgp": [17, 18, 19, 60, 63, 65, 66, 69, 78, 79, 86, 89, 90, 91, 95, 96], "dgp1": [17, 18, 19], "dgp2": [17, 18, 19], "dgp3": [17, 18, 19], "dgp4": [17, 18, 19], "dgp5": [17, 18, 19], "dgp6": [17, 18, 19], "dgp_confounded_irm_data": 95, "dgp_dict": 95, "dgp_tpye": 64, "dgp_type": [17, 18, 19, 64, 66, 69], "diagnost": 67, "diagon": 95, "diagram": [57, 74, 108], "dichotom": [57, 74], "dict": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 48, 49, 50, 51, 54, 66, 69, 76, 77, 79, 85, 95, 107], "dict_kei": [139, 145], "dict_rdd": [102, 104], "dictionari": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 35, 37, 38, 39, 45, 54, 66, 69, 76, 77, 80, 81, 94, 106, 107, 108, 109, 139, 145], "dictonari": [61, 87], "did": [0, 5, 7, 58, 64, 65, 66, 67, 68, 69, 86, 100, 101, 104, 105, 109, 110, 111, 113, 114, 123, 125, 127, 154, 155], "did_aggreg": [66, 68, 69], "did_data": 65, "did_multi": [108, 109], "diff": 87, "differ": [5, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 37, 38, 39, 57, 58, 60, 61, 62, 63, 66, 68, 69, 72, 73, 74, 75, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 91, 92, 94, 95, 96, 97, 100, 104, 105, 106, 107, 109, 110, 111, 113, 114, 122, 124, 125, 126, 127, 151, 152, 153, 154, 155], "differenti": 108, "difficult": 95, "dillon": 153, "dim": [45, 61], "dim_x": [12, 14, 15, 17, 19, 22, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 45, 58, 60, 62, 75, 83, 84, 85, 86, 98, 106, 107, 108, 139, 145], "dim_z": [37, 40, 108], "dimens": [17, 19, 32, 41, 60, 86, 90, 122], "dimension": [13, 32, 34, 37, 38, 79, 93, 106, 108, 119, 120, 122, 138, 139, 145, 152, 153], "direct": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 58, 63, 65, 75, 83, 96, 98, 108, 155], "directli": [44, 58, 59, 61, 73, 75, 84, 94, 98, 99, 104, 139, 145, 152, 155], "discontinu": [8, 44, 45, 102, 104, 105, 153, 154], "discret": [23, 35, 72, 73, 86, 108, 115, 154], "discretis": 88, "discuss": [33, 60, 61, 86, 87, 108, 109, 114, 153, 154, 155], "disjoint": [60, 80, 81, 86], "displai": [13, 60, 66, 67, 68, 69, 72, 73, 86, 95, 106, 107, 139, 145], "displot": 87, "disproportion": [61, 87], "disregard": [47, 51], "dist": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "distinct": [99, 104], "distinguish": [16, 66, 69], "distr": 107, "distribut": [45, 58, 64, 72, 73, 75, 84, 95, 98, 108, 110, 114, 139, 147, 151, 153, 154], "diverg": [44, 58, 75, 98], "divid": [66, 69, 108], "dmatrix": [76, 77, 106], "dml": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 57, 58, 62, 63, 67, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 123, 138, 139, 145, 151], "dml1": [105, 152, 154, 155], "dml2": [57, 60, 62, 63, 70, 86, 105, 108, 123, 138, 152, 154, 155], "dml_apo": 83, "dml_apo_obj": 108, "dml_apos_att": 83, "dml_apos_obj": 108, "dml_combin": 138, "dml_cover": 93, "dml_cv_predict": 154, "dml_cvar": [78, 88], "dml_cvar_0": 78, "dml_cvar_1": 78, "dml_cvar_obj": [24, 106], "dml_data": [7, 16, 59, 60, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 82, 83, 84, 86, 89, 93, 94, 95, 96, 100, 101, 102, 103, 104, 106, 107, 108, 109, 111, 113, 138, 155], "dml_data_anticip": [66, 69], "dml_data_arrai": [103, 104], "dml_data_bench": 95, "dml_data_bonu": [62, 152], "dml_data_df": 155, "dml_data_fuzzi": 92, "dml_data_lasso": 70, "dml_data_sharp": 92, "dml_data_sim": [62, 152], "dml_df": [60, 86], "dml_did": [64, 65], "dml_did_obj": [12, 15, 16, 108, 109, 110, 111, 113], "dml_doc": 72, "dml_iivm": 93, "dml_iivm_boost": [61, 87], "dml_iivm_forest": [61, 87], "dml_iivm_lasso": [61, 87], "dml_iivm_obj": [25, 74, 108], "dml_iivm_tre": [61, 87], "dml_irm": [76, 80, 83, 84, 90], "dml_irm_at": 82, "dml_irm_att": 83, "dml_irm_boost": [61, 87], "dml_irm_forest": [61, 87], "dml_irm_gat": 82, "dml_irm_gatet": 82, "dml_irm_lasso": [61, 70, 87], "dml_irm_new": 90, "dml_irm_obj": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 94, 106, 107, 108], "dml_irm_obj_ext": 107, "dml_irm_rf": 70, "dml_irm_tre": [61, 87], "dml_irm_weighted_att": 83, "dml_kwarg": 83, "dml_length": 93, "dml_long": 54, "dml_lpq_0": 91, "dml_lpq_1": 91, "dml_lpq_obj": [27, 106], "dml_lqte": [88, 91], "dml_obj": [59, 66, 67, 68, 69, 72, 73, 94, 95], "dml_obj_al": [66, 69], "dml_obj_anticip": [66, 69], "dml_obj_bench": 95, "dml_obj_lasso": 67, "dml_obj_linear": [66, 69], "dml_obj_linear_logist": 67, "dml_obj_nyt": [66, 69], "dml_obj_univers": [66, 69], "dml_pliv": [60, 86], "dml_pliv_obj": [37, 60, 86, 108], "dml_plr": [77, 81, 138], "dml_plr_1": 138, "dml_plr_2": 138, "dml_plr_boost": [61, 87], "dml_plr_forest": [61, 87, 155], "dml_plr_lasso": [61, 70, 87], "dml_plr_no_split": 122, "dml_plr_obj": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 94, 97, 106, 107, 108, 122, 123, 138, 139, 140, 145], "dml_plr_obj_extern": 122, "dml_plr_obj_intern": 122, "dml_plr_obj_onfold": 85, "dml_plr_obj_untun": 85, "dml_plr_rf": 70, "dml_plr_tree": [61, 87, 155], "dml_pq_0": [88, 91], "dml_pq_1": [88, 91], "dml_pq_obj": [28, 106], "dml_procedur": [70, 97, 152, 154, 155], "dml_qte": [88, 91], "dml_qte_obj": [29, 106], "dml_robust_confset": 93, "dml_robust_length": 93, "dml_short": 54, "dml_ssm": [63, 96, 108], "dml_standard_ci": 93, "dml_tune": 154, "dmldummyclassifi": 107, "dmldummyregressor": 107, "dmlmt": 153, "dnorm": 58, "do": [59, 60, 61, 62, 66, 67, 69, 83, 84, 86, 87, 88, 89, 95, 106, 107, 122, 139, 149, 152, 155], "doabl": 123, "doc": [57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 150, 154], "docu": 154, "document": [65, 66, 67, 68, 69, 71, 72, 76, 77, 80, 81, 83, 85, 95, 108, 114, 123, 139, 150, 154], "doe": [16, 23, 29, 59, 60, 61, 65, 66, 69, 72, 73, 83, 86, 87, 89, 92, 94, 95, 108, 123, 127, 139, 149, 155], "doesn": [57, 74], "doi": [10, 11, 17, 18, 19, 31, 33, 36, 39, 41, 42, 59, 60, 62, 79, 86, 95, 98, 107, 122, 138, 150, 152, 154], "domain": 90, "don": [59, 85], "done": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 67, 85, 88, 107, 122, 139, 140], "dosag": [72, 73], "dot": [30, 65, 66, 69, 90, 99, 104, 106, 107, 108, 112, 114, 115, 138, 152], "doubl": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 41, 42, 43, 44, 61, 67, 79, 84, 85, 87, 89, 93, 105, 107, 122, 123, 138, 139, 140, 154], "double_ml": [69, 72, 93], "double_ml_bonus_data": 70, "double_ml_data_from_data_fram": [58, 98, 99, 104, 155], "double_ml_data_from_matrix": [59, 62, 99, 104, 107, 138, 152], "double_ml_framework": [108, 109], "double_ml_irm": 70, "double_ml_score_mixin": 0, "doubleiivm": 150, "doubleml": [0, 58, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 88, 89, 90, 91, 92, 93, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 122, 123, 139, 145, 152, 153, 154], "doubleml2022python": 150, "doubleml2024r": 150, "doubleml_did_eval_linear": 59, "doubleml_did_eval_rf": 59, "doubleml_did_linear": 59, "doubleml_did_rf": 59, "doubleml_framework": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "doublemlapo": [72, 73, 83, 108, 123, 128, 154], "doublemlblp": [22, 26, 38, 76, 77, 83, 106, 154], "doublemlclusterdata": [0, 99, 104], "doublemlcvar": [78, 106, 123, 129, 154], "doublemldata": [0, 4, 7, 10, 11, 12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 57, 60, 62, 63, 64, 65, 70, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 103, 105, 106, 107, 108, 110, 122, 123, 138, 139, 145, 154, 155], "doublemldid": [64, 65, 108, 110, 123, 126, 154], "doublemldidaggreg": [66, 67, 68, 69, 108, 109], "doublemldidbinari": 65, "doublemldidc": [64, 108, 110, 123, 124, 154], "doublemldiddata": [0, 12, 15, 18, 64, 65, 100, 108, 110], "doublemldidmulti": [66, 67, 68, 69, 108, 109, 111, 112, 113, 123, 125, 127, 154], "doublemlframework": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 66, 67, 68, 69, 108, 109, 122, 138, 154], "doublemlframwork": 23, "doublemlidid": [108, 110], "doublemlididc": [108, 110], "doublemliivm": [57, 61, 74, 87, 93, 107, 108, 122, 123, 130, 154], "doublemlirm": [12, 14, 15, 22, 24, 25, 27, 28, 30, 37, 38, 59, 61, 70, 73, 76, 80, 82, 83, 84, 87, 89, 90, 94, 95, 106, 107, 108, 122, 123, 131, 150, 154], "doublemllpq": [91, 106, 123, 132, 154], "doublemlpaneldata": [0, 14, 16, 65, 67, 68, 101, 108, 109, 111, 112, 113, 154], "doublemlpliv": [107, 108, 122, 123, 134, 150, 154], "doublemlplr": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 58, 61, 62, 70, 75, 77, 81, 85, 87, 89, 94, 97, 98, 106, 107, 108, 122, 123, 135, 138, 139, 145, 150, 152, 154, 155], "doublemlpolicytre": [26, 106], "doublemlpq": [88, 91, 106, 123, 133, 154], "doublemlqt": [78, 88, 91, 106, 138, 154], "doublemlrdddata": [0, 44, 92, 102, 108], "doublemlresampl": [83, 84], "doublemlsmm": 154, "doublemlssm": [63, 96, 108, 123, 136, 137], "doublemlssmdata": [0, 30, 36, 96, 103, 108], "doubli": [18, 31, 39, 59, 67, 93, 153], "down": 95, "download": [57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 99, 104, 151, 152], "download_fil": 67, "downward": 95, "dpg_dict": 94, "dpi": [58, 75, 89], "dr": [108, 112], "dramat": 59, "draw": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 95, 122, 154], "draw_sample_split": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 83, 84, 122], "drawn": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 45, 61, 66, 69, 87, 88, 90, 122], "drive": [58, 75, 98], "driven": [95, 155], "drop": [59, 85, 86, 89, 99, 104, 107, 123, 124, 125, 126, 127, 138], "dropna": [66, 69], "dt": [66, 123, 124, 139, 141], "dt_bonu": [99, 104], "dta": [59, 68], "dtrain": 87, "dtype": [66, 67, 68, 69, 70, 72, 73, 80, 81, 82, 86, 87, 88, 93, 94, 99, 101, 104, 106, 152], "dualiti": 86, "dubourg": [150, 152], "duchesnai": [150, 152], "due": [58, 59, 67, 75, 76, 77, 82, 94, 95, 98, 108, 121, 139, 140, 154, 155], "duflo": [10, 11, 42, 60, 79, 86, 93, 98, 122, 150, 153], "dummi": [22, 26, 38, 46, 47, 48, 67, 85, 95, 106, 107, 108, 110, 154], "dummyclassifi": [46, 67], "dummyregressor": [47, 67], "duplic": 154, "durabl": [62, 70, 99, 104, 152], "durat": 11, "dure": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 59, 60, 61, 62, 63, 85, 86, 87, 107, 122, 152, 154, 155], "dx": 33, "dynam": [59, 153], "e": [6, 7, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 35, 36, 37, 38, 39, 42, 44, 46, 47, 50, 51, 58, 59, 60, 61, 63, 64, 66, 67, 68, 69, 72, 73, 75, 76, 77, 79, 82, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 98, 101, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 155], "e20ea26": 62, "e401": [61, 87, 88, 94, 155], "e4016553": 155, "e45228": 89, "e57c": 62, "each": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 46, 47, 50, 51, 60, 62, 65, 66, 67, 68, 69, 72, 73, 80, 81, 84, 85, 86, 88, 89, 90, 93, 94, 95, 97, 99, 101, 104, 107, 108, 114, 122, 138, 139, 145, 155], "earlier": [66, 69, 155], "earn": [61, 87, 88], "earner": [61, 87, 94], "easi": [62, 93, 123], "easier": 85, "easili": [62, 84, 85, 88, 154], "ec973f": 89, "ecolor": [65, 72, 73, 87, 89], "econ": 153, "econml": 153, "econom": [36, 40, 41, 43, 60, 79, 86, 89, 95, 122, 153], "econometr": [10, 11, 17, 18, 19, 31, 39, 42, 43, 59, 60, 79, 86, 93, 98, 150, 153], "econometrica": [34, 60, 86, 89, 93, 98, 153], "ecosystem": [150, 155], "ectj": [10, 11, 42, 60, 79, 86, 98, 150], "ed": 153, "edge_color": 75, "edgecolor": 75, "edit": [151, 153], "edu": [150, 152], "educ": [61, 87, 88, 94, 155], "ee97bda7": 62, "effect": [12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 45, 46, 47, 50, 51, 57, 58, 59, 60, 62, 63, 64, 65, 73, 74, 75, 79, 82, 86, 90, 92, 93, 96, 98, 105, 107, 109, 110, 112, 114, 116, 117, 118, 121, 122, 123, 131, 138, 139, 140, 152, 153, 154, 155], "effici": [72, 108, 153], "effort": 123, "eight": [60, 86], "either": [12, 14, 15, 16, 17, 19, 22, 24, 25, 26, 27, 28, 30, 32, 37, 38, 62, 65, 66, 69, 79, 90, 92, 106, 107, 108, 111, 114, 155], "eleanor": 153, "element": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 60, 63, 64, 66, 67, 68, 69, 76, 77, 78, 84, 86, 88, 91, 94, 96, 108, 111, 112, 113, 123, 124, 125, 126, 127, 128, 139, 142, 144, 145, 148, 149, 154], "element_text": [60, 61], "elementari": 153, "elif": [80, 81, 90], "elig": [88, 94, 155], "eligibl": [61, 87, 94], "ell": [58, 60, 75, 79, 86, 98, 123, 134, 135, 152], "ell_0": [25, 37, 38, 58, 75, 79, 85, 98, 108, 117, 123, 135], "ell_1": 67, "ell_2": 84, "els": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 48, 59, 60, 61, 65, 72, 80, 81, 86, 90, 95], "em": 153, "emphas": [60, 86], "empir": [20, 21, 58, 60, 67, 75, 86, 89, 95, 98, 108, 111, 112, 113, 122, 123, 138], "emploi": [60, 79, 86, 95, 123, 130], "employ": [61, 67, 87, 88], "employe": 155, "empti": 86, "emul": [139, 140], "enabl": [13, 72, 73, 90, 94, 106, 139, 140, 154], "enable_metadata_rout": [46, 47, 50, 51], "encapsul": [46, 47, 50, 51, 107], "encod": 89, "encount": [69, 72], "end": [33, 40, 41, 58, 59, 60, 61, 63, 65, 66, 69, 75, 78, 79, 84, 86, 87, 89, 90, 91, 96, 97, 99, 104, 107, 108, 111, 113, 114, 122, 123, 125, 127, 138, 152, 155], "endogen": [61, 87, 88, 155], "enet_coordinate_descent_gram": 86, "enforc": 67, "engin": [62, 153], "enrol": [61, 87, 88], "ensembl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 70, 72, 73, 75, 76, 77, 80, 81, 82, 84, 87, 90, 94, 95, 97, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 139, 145, 152, 155], "ensemble_learner_pipelin": 107, "ensemble_pipe_classif": 62, "ensemble_pipe_regr": 62, "ensur": [50, 51, 60, 72, 83, 85, 86, 90, 93, 102, 104], "entir": [16, 58, 61, 75, 87, 98, 139, 140], "entri": [12, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 58, 60, 66, 67, 68, 69, 70, 72, 73, 75, 82, 86, 87, 88, 94, 98, 99, 101, 104, 107, 150, 152, 154], "enumer": [65, 72, 73, 78, 80, 81, 84, 86, 87, 88, 91, 97, 107, 122], "env": [72, 151], "environ": 151, "ep": [72, 89], "epanechnikov": 44, "epsilon": [61, 64, 65, 78, 87, 91, 106, 108, 110, 114], "epsilon_": [60, 65, 66, 69, 86], "epsilon_0": 45, "epsilon_1": 45, "epsilon_i": [32, 78, 89, 90, 91], "epsilon_sampl": 90, "epsilon_tru": [78, 91], "eqnarrai": 61, "equal": [13, 17, 19, 22, 26, 60, 63, 66, 69, 86, 89, 93, 96, 106, 107, 108, 139, 147], "equat": [45, 60, 61, 86, 87, 95, 97, 138, 155], "equilibrium": [60, 86], "equiv": [108, 114, 123, 125, 127], "equival": [79, 83, 122], "err": [12, 15, 16, 24, 25, 26, 27, 28, 29, 30, 37, 38, 64, 66, 67, 68, 69, 70, 72, 73, 74, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 91, 94, 95, 96, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 152, 155], "error": [5, 6, 7, 8, 9, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 50, 51, 57, 58, 59, 61, 62, 63, 65, 72, 75, 79, 80, 81, 84, 85, 87, 92, 95, 98, 107, 108, 119, 120, 122, 123, 138, 139, 145, 152, 154, 155], "errorbar": [65, 72, 73, 80, 81, 85, 87, 89, 92], "erstellt": [60, 61, 62], "es_linear_logist": 67, "es_rf": 67, "escap": 86, "esim": 92, "especi": [84, 85, 99, 104], "essenti": 95, "est": 92, "est_method": 59, "esther": [122, 153], "estim": [10, 12, 13, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 44, 46, 47, 48, 49, 50, 51, 58, 59, 60, 62, 65, 73, 75, 76, 77, 78, 80, 81, 83, 84, 86, 90, 92, 93, 97, 98, 105, 106, 107, 108, 109, 110, 111, 112, 113, 116, 118, 124, 125, 126, 129, 132, 133, 137, 139, 140, 145, 150, 153, 154], "estimand": 93, "estimator_list": 85, "et": [10, 11, 32, 34, 41, 42, 58, 60, 61, 62, 64, 75, 76, 77, 78, 79, 80, 81, 84, 86, 87, 88, 91, 94, 98, 108, 110, 114, 122, 123, 129, 131, 132, 133, 138, 139, 140, 149, 150, 152, 154], "eta": [20, 21, 58, 60, 61, 65, 85, 86, 87, 91, 92, 97, 106, 108, 111, 112, 113, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 149, 152, 155], "eta1": 89, "eta2": 89, "eta_": [138, 139, 149], "eta_0": [44, 97, 108, 111, 112, 113, 123, 138], "eta_d": [92, 108], "eta_i": [17, 19, 32, 65, 66, 69, 90, 91, 92, 108], "eta_sampl": 90, "eta_tru": 91, "etc": [12, 13, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 60, 84, 85, 86, 154], "ev": [58, 75, 98], "eval": [16, 62, 66, 67, 68, 69, 107, 108, 111, 112, 113, 123, 125, 127], "eval_metr": [61, 87, 155], "eval_pr": 59, "eval_predict": 59, "evalu": [12, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 34, 37, 38, 59, 62, 65, 66, 67, 68, 69, 76, 77, 78, 82, 83, 88, 91, 94, 97, 108, 112, 114, 153, 154], "evaluate_learn": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 84, 85, 107, 154], "evalut": 107, "even": [61, 62, 66, 69, 87, 89, 92, 107, 108, 155], "event": [108, 109], "eventstudi": [16, 66, 67, 68, 69, 108, 109], "eventu": [60, 86], "everi": [60, 67, 86], "everyth": 150, "evid": [82, 85], "exact": [83, 95], "exactli": [92, 95, 108], "examin": 72, "exampl": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 44, 52, 53, 57, 58, 61, 62, 63, 64, 65, 66, 68, 69, 70, 72, 73, 74, 75, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 106, 107, 108, 109, 111, 112, 113, 121, 122, 123, 138, 139, 145, 150, 152, 154, 155], "example_attgt": 59, "example_attgt_dml_eval_linear": 59, "example_attgt_dml_eval_rf": 59, "example_attgt_dml_linear": 59, "example_attgt_dml_rf": 59, "except": [47, 51, 79, 95, 154], "excess": 84, "exclud": 54, "exclus": [22, 26, 38, 80, 81, 106], "execut": [62, 155], "exemplarili": 152, "exemplatori": 90, "exhaust": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 107], "exhibit": [60, 86], "exist": [46, 47, 50, 51, 83, 108, 110, 114, 139, 149], "exogen": [61, 87, 88, 108, 155], "exp": [17, 18, 19, 31, 32, 34, 35, 39, 42, 58, 65, 75, 76, 77, 80, 81, 89, 90, 98], "expect": [31, 39, 47, 51, 59, 63, 64, 66, 67, 69, 73, 82, 84, 85, 92, 95, 96, 102, 103, 104, 106, 108, 122, 138, 139, 146, 152], "experi": [11, 33, 34, 58, 61, 75, 87, 93, 95, 98, 99, 104, 122, 152, 153], "experiment": [12, 14, 15, 16, 17, 18, 19, 123, 124, 125, 126, 127, 139, 141, 142, 143, 144], "expertis": 95, "explain": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 94, 139, 140, 148, 149], "explan": [60, 64, 86, 94, 139, 148, 150, 155], "explanatori": [95, 138], "explicit": 95, "explicitli": [66, 68, 69, 82, 155], "exploit": [58, 75, 98, 108, 155], "explor": 85, "exponenti": 138, "export": [85, 154], "expos": [99, 100, 102, 103, 104, 108, 114], "exposur": [7, 17, 19, 65, 66, 67, 68, 69], "express": [60, 79, 92, 139, 149], "ext": 16, "extend": [95, 100, 103, 104, 107, 150, 154], "extendend": [139, 149], "extens": [107, 123, 150, 153, 154], "extent": 79, "extern": [58, 75, 83, 85, 105, 139, 140, 154], "external_predict": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 75, 107], "externalptr": 61, "extra": 62, "extract": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 72, 85], "extralearn": 62, "extrem": [52, 53, 61, 87], "extreme_threshold": [52, 53], "ey": 79, "ezequiel": 154, "f": [61, 62, 64, 65, 66, 69, 72, 73, 75, 78, 79, 84, 86, 87, 88, 90, 91, 93, 94, 95, 96, 107, 139, 149, 150, 152], "f00584a57972": 62, "f1718fdeb9b0": 62, "f2e7": 62, "f3d24993": 62, "f6ebc": 89, "f_": [17, 18, 19, 31, 65, 106], "f_loc": [78, 91], "f_p": 65, "f_scale": [78, 91], "f_t": [66, 69], "f_x": 108, "face_color": 75, "facet_wrap": 61, "facilit": 85, "fact": [61, 87, 88], "factor": [45, 58, 59, 60, 61, 62, 75, 84, 98, 107, 139, 142, 144, 155], "faculti": 153, "fail": 154, "fair": 84, "fake": [57, 74, 93], "fall": 72, "fallback": 107, "fals": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 44, 45, 46, 47, 48, 50, 51, 52, 53, 58, 61, 62, 63, 64, 66, 69, 72, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 95, 96, 99, 104, 107, 108, 111, 122, 123, 124, 125, 126, 127, 138, 139, 141, 142, 143, 144, 155], "famili": [61, 87, 107], "familiar": 93, "fanci": 59, "far": [61, 87], "farbmach": 33, "fast": [84, 90, 107], "faster": 79, "fb5c25fa": 62, "fc9e": 62, "fd8a": 62, "featur": [10, 11, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 47, 49, 51, 59, 65, 66, 69, 70, 72, 82, 83, 84, 87, 90, 92, 95, 102, 104, 106, 107, 108], "featureless": [62, 107], "features_bas": [61, 87, 88, 94], "features_flex": 61, "featureunion": 62, "februari": [66, 95], "feder": 67, "femal": [62, 70, 99, 104, 152], "fern\u00e1ndez": [34, 122, 153], "fetch": [61, 86, 87, 88, 99, 104], "fetch_401k": [61, 87, 88, 94, 155], "fetch_bonu": [62, 70, 99, 104, 152], "few": [61, 87, 88], "ff7f0e": 65, "field": [60, 86, 107, 155], "fifteenth": 153, "fifth": [60, 66, 69], "fig": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 65, 66, 67, 68, 69, 72, 73, 76, 77, 78, 79, 83, 84, 85, 88, 89, 91, 92, 95], "fig_al": 75, "fig_dml": 75, "fig_non_orth": 75, "fig_orth_nosplit": 75, "fig_po_al": 75, "fig_po_dml": 75, "fig_po_nosplit": 75, "figsiz": [13, 16, 65, 66, 69, 70, 72, 73, 76, 77, 78, 80, 81, 83, 84, 85, 86, 87, 88, 89, 91, 92], "figur": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 42, 58, 60, 65, 66, 67, 69, 70, 72, 73, 75, 76, 77, 78, 79, 80, 81, 83, 85, 86, 87, 88, 91, 95, 98], "figure_format": 89, "file": [10, 11, 67, 79, 89, 153, 154], "filenam": 58, "fill": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 58, 60, 61, 63, 64, 84, 87, 96], "fill_between": [76, 77, 78, 83, 88, 91], "fill_valu": 84, "fillna": 66, "filter": 62, "filterwarn": [72, 75, 84], "final": [58, 62, 63, 65, 66, 68, 69, 73, 75, 76, 77, 78, 80, 81, 82, 88, 91, 92, 96, 98, 108, 121, 123, 125, 127, 155], "final_estim": 92, "financi": [10, 94, 155], "find": [61, 64, 65, 72, 87, 95, 106, 107, 155], "finish": 62, "finit": [58, 61], "firm": [60, 86, 94], "firmid": 86, "first": [7, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 41, 44, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 80, 81, 82, 83, 84, 86, 87, 88, 90, 91, 92, 95, 96, 98, 106, 108, 109, 112, 114, 122, 138, 139, 145, 151, 152, 154, 155], "fit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 46, 47, 48, 49, 50, 51, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 105, 106, 107, 108, 109, 110, 111, 113, 114, 123, 126, 137, 138, 139, 140, 145, 150, 154, 155], "fit_arg": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38], "fit_transform": [83, 86, 87], "five": 86, "fix": [65, 66, 69, 84, 154], "flag": [18, 99, 104, 122, 151], "flake8": 154, "flamlclassifierdoubleml": 85, "flamlregressordoubleml": 85, "flatten": [85, 89], "flexibl": [44, 57, 59, 61, 62, 64, 74, 87, 108, 150, 153, 154, 155], "flexibli": [61, 67, 87, 94], "float": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 44, 45, 46, 47, 48, 50, 51, 52, 53, 66, 67, 68, 69, 101, 104], "float32": [87, 88, 94], "float64": [66, 67, 68, 69, 70, 72, 73, 78, 80, 81, 82, 86, 87, 94, 99, 101, 104, 107, 152], "floor": 62, "floor_divid": 86, "flt": 62, "flush": 58, "fmt": [65, 72, 73, 80, 81, 85, 87, 89, 92], "fobj": 87, "focu": [60, 61, 67, 83, 86, 87, 88, 95, 106, 108, 110, 114, 121, 155], "focus": [88, 94, 95, 155], "fold": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 57, 60, 61, 62, 63, 64, 66, 67, 68, 69, 84, 86, 87, 88, 94, 96, 97, 105, 107, 108, 110, 111, 113, 123, 126, 138, 152, 155], "follow": [17, 18, 19, 31, 32, 35, 39, 58, 60, 61, 63, 64, 65, 66, 67, 68, 69, 75, 76, 77, 78, 80, 81, 85, 86, 87, 88, 91, 92, 93, 94, 95, 96, 98, 99, 104, 106, 107, 108, 109, 111, 112, 113, 114, 122, 123, 125, 127, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 155], "font_scal": [86, 87, 88], "fontsiz": [66, 69, 78, 88, 91], "force_all_d_finit": [5, 6, 7, 8, 9, 99, 100, 104], "force_all_x_finit": [4, 5, 6, 7, 8, 9, 99, 104], "forest": [33, 57, 58, 59, 61, 62, 64, 72, 74, 75, 82, 84, 87, 94, 98, 107, 152, 155], "forest_summari": 87, "forg": [151, 153, 154], "form": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 48, 50, 51, 61, 63, 64, 65, 76, 77, 78, 80, 81, 82, 84, 87, 91, 92, 94, 96, 106, 108, 109, 110, 114, 117, 118, 119, 120, 123, 125, 127, 128, 131, 139, 140, 145, 146, 147, 148, 149, 151, 152], "format": [7, 16, 67, 72, 75, 82, 139, 145, 154], "former": [67, 93], "formula": [60, 61, 86, 87, 92, 95, 154], "formula_flex": 61, "forschungsgemeinschaft": 150, "forthcom": [95, 153], "forum": 154, "forward": [26, 49], "found": [68, 76, 77, 79, 80, 81, 85, 98, 99, 104, 107, 108, 121, 152], "foundat": [72, 150, 153], "four": [61, 72, 84, 87, 108, 111, 154], "fourth": [60, 86], "frac": [17, 18, 19, 21, 25, 31, 33, 34, 36, 39, 40, 42, 43, 47, 51, 58, 60, 62, 65, 69, 75, 79, 82, 86, 89, 92, 97, 98, 106, 108, 111, 112, 113, 117, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143, 144, 146, 147, 148, 149], "fraction": [19, 62], "frame": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 48, 57, 58, 60, 61, 63, 66, 67, 68, 69, 70, 72, 73, 76, 77, 80, 81, 82, 86, 87, 88, 89, 90, 94, 98, 99, 101, 104, 152, 155], "framealpha": [66, 69], "frameon": [66, 69], "framework": [13, 16, 21, 58, 60, 62, 72, 75, 84, 85, 86, 89, 95, 98, 107, 138, 150, 152, 154, 155], "freez": 151, "fribourg": 153, "friendli": [66, 69, 72, 73], "from": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 51, 52, 53, 57, 58, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 103, 106, 107, 108, 109, 110, 111, 112, 113, 114, 122, 123, 138, 139, 145, 152, 154, 155], "from_arrai": [4, 5, 6, 7, 8, 9, 30, 44, 64, 65, 75, 78, 91, 98, 99, 100, 102, 103, 104, 107, 138, 152], "from_config": [52, 53], "from_product": 86, "front": 73, "fr\u00e9chet": [139, 149], "fs_kernel": [44, 108], "fs_specif": [44, 108], "fsize": [61, 87, 88, 94, 155], "full": [64, 65, 72, 73, 75, 78, 80, 81, 84, 87, 88, 91, 92, 93, 96, 98, 108], "fulli": [26, 61, 71, 85, 87, 93, 108, 118], "fun": 58, "func": 59, "function": [0, 4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 42, 43, 44, 57, 58, 61, 62, 63, 64, 66, 67, 68, 69, 72, 74, 75, 76, 77, 78, 83, 84, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 105, 107, 108, 109, 110, 111, 112, 113, 114, 117, 122, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 149, 150, 153, 154, 155], "fund": [61, 87, 88, 150], "further": [13, 16, 17, 18, 19, 31, 32, 35, 39, 41, 60, 62, 63, 64, 65, 66, 67, 68, 69, 73, 76, 77, 78, 82, 83, 84, 86, 88, 90, 91, 92, 94, 95, 96, 107, 108, 110, 113, 121, 123, 129, 132, 133, 136, 137, 138, 139, 140, 145, 148, 149, 150, 152, 154, 155], "furthermor": [75, 101, 104, 123, 128, 131], "futur": [4, 14, 16, 22, 24, 25, 26, 27, 28, 29, 30, 66, 67, 68, 69, 76, 108, 123, 139], "futurewarn": [65, 66, 67, 68, 69, 72], "fuzzi": [44, 45], "g": [6, 7, 12, 14, 15, 16, 17, 19, 22, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 44, 46, 47, 50, 51, 58, 59, 62, 64, 65, 66, 67, 68, 69, 70, 75, 76, 77, 79, 82, 84, 88, 89, 90, 93, 94, 96, 98, 101, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 123, 124, 125, 126, 127, 128, 130, 131, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 155], "g0": 72, "g1": 72, "g_": [45, 73, 108, 113, 123, 124, 126, 127, 129, 132, 133, 138], "g_0": [12, 14, 15, 16, 22, 25, 26, 28, 37, 38, 42, 43, 44, 45, 58, 60, 61, 72, 75, 84, 86, 87, 98, 106, 107, 108, 115, 116, 117, 118, 119, 120, 123, 125, 127, 128, 135, 136, 137, 139, 146, 147, 149, 152, 155], "g_1": [45, 84], "g_all": [58, 61], "g_all_po": 58, "g_ci": 61, "g_d": [123, 129, 133], "g_dml": 58, "g_dml_po": 58, "g_hat": [37, 38, 58, 75, 123], "g_hat0": [25, 26], "g_hat1": [25, 26], "g_i": [17, 19, 108, 111, 113, 114, 123, 125, 127], "g_k": 106, "g_nonorth": 58, "g_nosplit": 58, "g_nosplit_po": 58, "g_valu": 14, "g_x": 65, "gain": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 54, 84, 139, 140, 147, 154], "gain_statist": 154, "galleri": [98, 106, 107, 108, 109, 111, 113, 121, 150, 154], "gama": 85, "gamma": [36, 40, 43, 60, 86, 89, 90, 92, 95, 108, 123, 129, 132], "gamma_0": [32, 63, 90, 96, 123, 129, 132], "gamma_a": [31, 39, 95], "gamma_bench": 95, "gamma_v": 95, "gap": [86, 95], "gapo": 22, "gate": [22, 26, 38, 48, 89, 90, 105, 154], "gate_obj": 106, "gatet": 106, "gaussian": [27, 28, 29, 58, 75, 98, 106, 107, 138, 153], "ge": [18, 31, 32, 82, 90, 106, 108, 112, 114], "geer": 153, "gelbach": [60, 86], "gener": [0, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 47, 51, 52, 57, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 104, 105, 106, 107, 108, 111, 113, 114, 115, 122, 123, 125, 127, 128, 131, 138, 140, 141, 142, 143, 144, 146, 147, 149, 153, 154, 155], "generate_treat": 91, "generate_weakiv_data": 93, "geom_bar": 61, "geom_dens": [61, 63], "geom_errorbar": 61, "geom_funct": 58, "geom_histogram": 58, "geom_hlin": 61, "geom_point": 61, "geom_til": 60, "geom_vlin": [58, 63], "geq": [17, 19, 92, 108], "german": 150, "get": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 50, 51, 62, 66, 68, 69, 72, 73, 84, 89, 94, 95, 139, 140, 150, 151], "get_dummi": 89, "get_feature_names_out": [83, 86, 87], "get_legend_handles_label": [72, 73], "get_level_valu": 85, "get_logg": [58, 59, 60, 61, 62, 63, 97, 107, 108, 122, 123, 138, 152], "get_metadata_rout": [46, 47, 50, 51], "get_param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 50, 51, 85, 107], "get_ylim": 83, "ggdid": 59, "ggplot": [58, 60, 61, 63], "ggplot2": [58, 60, 61, 63], "ggsave": 58, "ggtitl": 61, "gh": 154, "git": 151, "github": [59, 61, 67, 79, 85, 89, 150, 153, 154], "githubusercont": [68, 79], "give": [61, 83, 87], "given": [12, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 34, 37, 38, 39, 42, 43, 44, 45, 46, 47, 50, 51, 58, 60, 63, 65, 67, 68, 73, 75, 80, 81, 86, 88, 89, 92, 95, 96, 98, 106, 108, 111, 112, 113, 123, 128, 138, 139, 145, 146, 147, 148, 149, 152, 154], "glmnet": [61, 62, 107, 154], "global": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 50, 51, 107, 108], "globalclassifi": 92, "globallearn": 92, "globalregressor": 92, "glrn": 62, "glrn_lasso": 62, "gmm": 93, "gname": 59, "go": [76, 77, 79, 83, 85, 92, 95], "goal": [73, 80, 81, 108], "goe": 108, "goldman": 153, "good": [79, 139, 140, 155], "gpu": 72, "gradient": [61, 72, 87], "gradientboostingclassifi": 84, "gradientboostingregressor": 84, "gradual": 95, "gramfort": [150, 152], "graph": [62, 63, 96, 155], "graph_ensemble_classif": 62, "graph_ensemble_regr": 62, "graph_obj": 92, "graph_object": [76, 77, 79, 95], "graphlearn": [62, 107], "grasp": [73, 139, 140], "great": [65, 155], "greater": 155, "green": [58, 76, 77, 78, 91], "greg": 153, "grei": [61, 72, 73], "grenand": 153, "grey50": 60, "grid": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 60, 62, 66, 69, 72, 73, 76, 77, 78, 79, 83, 88, 89, 91, 95, 107, 139, 145], "grid_arrai": [76, 77], "grid_basi": 83, "grid_bound": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 95], "grid_search": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 62, 107], "grid_siz": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 76, 77], "gridextra": 60, "gridsearchcv": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 107], "grisel": [150, 152], "grob": 60, "group": [7, 14, 16, 17, 19, 22, 26, 38, 57, 59, 67, 72, 73, 74, 82, 83, 88, 89, 90, 95, 105, 108, 109, 111, 112, 113, 114, 123, 125, 127, 139, 142, 144], "group_0": 106, "group_1": [80, 81, 106], "group_2": [80, 81, 106], "group_3": [80, 81], "group_effect": 90, "group_ind": 82, "group_treat": 82, "groupbi": [66, 69, 72, 79, 87, 93], "gruber": 33, "gt": [57, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 72, 73, 74, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 99, 104, 152], "gt_combin": [16, 66, 67, 69, 108, 109, 111, 112, 113], "gt_dict": [66, 69], "guarante": [60, 86], "guber": 33, "guess": [94, 139, 140], "guid": [20, 21, 46, 47, 50, 51, 58, 59, 60, 62, 65, 66, 67, 68, 69, 73, 75, 82, 83, 86, 92, 94, 107, 150, 152, 154], "guidelin": 154, "gunion": [62, 107], "gxidclusterperiodytreat": 59, "h": [17, 18, 19, 31, 33, 39, 41, 59, 60, 86, 92, 93, 101, 104, 108, 153], "h20": 85, "h_0": [66, 69, 73, 82, 83, 94, 95, 139, 145, 155], "h_f": [44, 108], "ha": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 48, 49, 50, 51, 58, 59, 60, 61, 67, 69, 75, 79, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 99, 104, 106, 107, 108, 110, 114, 139, 140, 145, 146, 147, 148, 149, 150, 155], "had": 67, "half": [58, 75, 89, 98, 122], "hand": [44, 84, 85, 89, 93, 155], "handbook": 89, "handl": [59, 67, 72, 73, 84, 100, 101, 104, 107, 154], "hansen": [10, 11, 34, 40, 42, 60, 79, 86, 93, 98, 150, 153], "happend": 84, "hard": [94, 139, 140], "harold": 153, "harsh": [46, 50], "hasn": [13, 16, 66, 68, 69], "hat": [58, 60, 75, 79, 82, 86, 89, 92, 97, 98, 106, 108, 122, 123, 138, 139, 140, 145, 148], "have": [16, 22, 23, 26, 29, 32, 35, 38, 48, 49, 50, 51, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 72, 73, 74, 76, 77, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 99, 104, 106, 107, 108, 112, 123, 125, 127, 138, 139, 140, 146, 149, 151, 152, 154, 155], "hazlett": [95, 139, 140], "hc": [59, 153], "hc0": [48, 154], "hdm": [60, 86], "he": [63, 96], "head": [59, 60, 62, 66, 67, 68, 69, 70, 76, 77, 80, 81, 83, 85, 86, 87, 89, 92, 95, 99, 100, 104, 106, 152], "heat": [60, 86], "heatmap": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 60, 86, 95], "heavili": 84, "hei": 153, "height": [13, 16, 58, 60, 79, 85], "help": [59, 61, 67, 78, 84, 88, 90, 95, 108, 109, 122, 155], "helper": [100, 104, 154], "henc": [59, 61, 62, 87, 95, 107, 123, 155], "here": [27, 28, 29, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 76, 77, 78, 80, 81, 82, 84, 86, 87, 88, 90, 91, 92, 95, 96, 99, 104, 107, 108, 111, 151], "heterogen": [17, 19, 26, 32, 61, 82, 87, 88, 90, 105, 108, 118, 122, 153, 154, 155], "heteroskedast": [80, 81], "heurist": [58, 75, 98], "high": [34, 37, 38, 61, 65, 79, 87, 88, 93, 97, 108, 119, 120, 138, 150, 152, 153], "higher": [59, 61, 79, 87, 88, 89, 92, 93, 154, 155], "highli": [61, 87, 150], "highlight": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 64, 83, 85, 95, 154], "highlightcolor": [76, 77], "hint": 85, "hispan": 70, "hist": [72, 73], "hist_e401": 61, "hist_p401": 61, "histogram": [72, 73], "histplot": 75, "hjust": 61, "hline": [99, 104, 138, 152, 155], "hold": [36, 53, 60, 61, 63, 85, 86, 87, 96, 106, 107, 108, 112], "holdout": [107, 122], "holm": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38], "home": [61, 65, 66, 68, 69, 87, 93, 95], "homogen": 108, "hook": 154, "hopefulli": 88, "horizont": [60, 65, 72, 86], "hostedtoolcach": [65, 66, 67, 68, 69, 86, 87, 92, 95], "hot": 89, "hotstart_backward": [62, 107], "hotstart_forward": [62, 107], "household": [61, 87, 88, 94], "how": [13, 17, 19, 46, 47, 50, 51, 57, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 98, 100, 104, 107, 108, 150, 151], "howev": [58, 61, 63, 75, 85, 87, 92, 95, 96, 98, 108, 155], "hown": [61, 87, 88, 94, 155], "hpwt": [60, 86], "hpwt0": 60, "hpwtairmpdspac": 60, "href": 150, "hspace": 84, "hstack": [30, 65], "html": [62, 150, 152, 154], "http": [33, 43, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 107, 150, 151, 152, 154], "huber": [36, 63, 96, 108, 121, 123, 136, 137, 153], "hue": [66, 69, 87], "huge": 84, "hugo": 153, "husd": [62, 70, 99, 104, 152], "hyperparamet": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 61, 62, 70, 72, 79, 84, 85, 87, 105, 152], "hypothes": [138, 153], "hypothesi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 61, 87, 94, 139, 145, 153], "hypothet": 95, "i": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 137, 138, 139, 140, 142, 144, 145, 146, 147, 149, 150, 151, 152, 154, 155], "i0": [64, 65, 108, 110], "i03": 150, "i1": [64, 108, 110], "i_": [40, 86, 90], "i_1": [60, 86], "i_2": [60, 86], "i_3": [60, 86], "i_4": 65, "i_est": 75, "i_fold": 60, "i_k": [60, 86, 97, 122, 138], "i_learn": 84, "i_level": 73, "i_rep": [58, 63, 64, 75, 84, 96, 98], "i_split": 86, "i_train": 75, "icp": 153, "id": [7, 16, 59, 60, 62, 66, 67, 68, 69, 86, 101, 104, 108, 109, 111, 113, 139, 144], "id_col": [7, 16, 66, 67, 68, 69, 101, 104, 108, 109, 111, 113], "id_var": 86, "idea": [61, 62, 87, 88, 95, 107, 108, 139, 140, 155], "ident": [17, 18, 19, 31, 32, 35, 39, 40, 49, 62, 67, 73, 83, 85, 92, 107, 108, 114, 123, 131, 139, 145], "identfi": 95, "identif": [66, 67, 68, 69, 92, 93, 108, 155], "identifi": [7, 60, 61, 64, 67, 72, 82, 86, 87, 88, 92, 95, 100, 101, 102, 103, 104, 106, 108, 110, 112, 114, 121, 139, 149, 154], "identifii": 106, "idnam": 59, "idx_gt_att": 16, "idx_learn": 72, "idx_tau": [78, 88, 91], "idx_treat": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 72, 73, 139, 145], "ieee": 153, "ifels": 59, "ignor": [46, 47, 50, 51, 67, 72, 75, 84, 92], "ignore_index": 72, "ii": [60, 86], "iid": [66, 69, 108, 110], "iivm": [20, 21, 25, 33, 88, 97, 106, 117, 130, 150, 154], "iivm_summari": 87, "iivmglmnet": 61, "iivmrang": 61, "iivmrpart": 61, "iivmxgboost11861": 61, "ij": [41, 60, 63, 73, 86, 96], "ilia": 153, "illustr": [58, 60, 61, 62, 63, 64, 65, 67, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 86, 87, 88, 90, 91, 94, 95, 96, 98, 107, 155], "iloc": [64, 65, 66, 67, 68, 69, 72, 73, 84, 86, 89, 93], "immedi": 151, "immun": [122, 153], "impact": [57, 74, 84, 89, 94], "implement": [12, 14, 15, 16, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 50, 51, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 75, 79, 83, 84, 86, 87, 89, 92, 94, 95, 96, 98, 105, 106, 107, 109, 110, 112, 114, 121, 122, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 152, 153, 154, 155], "impli": [31, 39, 60, 61, 66, 69, 86, 87, 88, 92, 106, 108, 114, 139, 141, 142, 143, 144, 146, 147], "implicitli": [108, 112], "implment": [65, 108, 109], "import": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 52, 53, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 139, 145, 151, 152, 154, 155], "importlib": 79, "impos": 95, "improv": [64, 66, 69, 84, 90, 108, 154], "in_sample_norm": [12, 14, 15, 16, 64, 123, 124, 125, 126, 127, 139, 141, 142, 143, 144], "inbuild": 84, "inbuilt": 84, "inc": [61, 87, 88, 94, 155], "includ": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 54, 59, 61, 65, 66, 67, 68, 69, 72, 73, 80, 81, 83, 87, 92, 94, 95, 106, 108, 138, 139, 145, 146, 147, 149, 151, 154, 155], "include_bia": [83, 86, 87], "include_never_tr": [17, 19, 69], "include_scenario": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 95], "incom": [61, 87, 88, 90, 94, 155], "incorpor": [62, 94, 139, 145], "increas": [19, 66, 69, 82, 84, 86, 95, 155], "increment": 154, "ind": 87, "independ": [12, 14, 15, 16, 18, 31, 32, 39, 45, 60, 62, 65, 82, 86, 90, 108, 114, 121, 123, 124, 125, 126, 127, 154], "index": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 58, 60, 65, 70, 72, 75, 79, 80, 81, 85, 86, 87, 89, 90, 98, 99, 104, 122, 123, 124, 125, 126, 127, 150, 152], "index_col": 79, "india": [122, 153], "indic": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 36, 37, 38, 44, 48, 60, 61, 63, 65, 66, 67, 68, 69, 82, 86, 87, 88, 92, 93, 95, 96, 97, 99, 100, 103, 104, 106, 108, 110, 114, 115, 121, 122], "individu": [17, 19, 22, 26, 50, 51, 59, 61, 65, 66, 69, 72, 73, 80, 81, 82, 85, 87, 88, 92, 94, 106, 108, 155], "individual_df": 65, "induc": [105, 122], "industri": [60, 86], "inf": [5, 6, 7, 8, 9, 59, 66, 67, 68, 69, 93], "inf_model": 123, "infer": [34, 40, 57, 58, 60, 67, 72, 74, 75, 79, 84, 85, 86, 93, 98, 105, 108, 122, 150, 152, 153, 154], "inferenti": 155, "infinit": [5, 6, 7, 8, 9, 93, 99, 100, 104, 154], "influenc": [25, 47, 51, 72, 108], "info": [57, 62, 66, 67, 68, 69, 70, 72, 73, 82, 85, 86, 87, 88, 94, 99, 101, 104, 152, 154, 155], "inform": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 46, 47, 48, 50, 51, 57, 62, 66, 67, 68, 69, 74, 76, 77, 84, 92, 93, 94, 95, 108, 109, 139, 140, 153], "infti": [58, 75, 98, 108, 114], "inher": 95, "inherit": [89, 100, 101, 102, 103, 104, 154], "initi": [4, 5, 6, 7, 8, 9, 12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 61, 62, 63, 64, 66, 67, 68, 69, 72, 78, 87, 88, 91, 92, 94, 95, 96, 99, 101, 104, 106, 107, 108, 122, 152, 154, 155], "inlin": [70, 89], "inlinebackend": 89, "inner": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 107], "innermost": 107, "input": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 47, 51, 62, 67, 94, 97, 107, 108, 112, 138, 139, 140, 145], "insensit": 108, "insid": [46, 47, 50, 51], "insight": [79, 95], "insignific": 94, "inspect": 152, "inspir": [31, 33, 34, 36, 66, 69, 95], "instabl": 53, "instal": [61, 72, 85, 92, 108, 154], "install_github": 151, "instanc": [50, 51, 61, 62, 72, 87, 107], "instanti": [60, 61, 86, 87, 107, 122], "instead": [4, 12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 46, 47, 50, 51, 57, 59, 61, 65, 66, 67, 68, 69, 72, 73, 74, 76, 82, 85, 87, 88, 99, 104, 106, 107, 108, 123, 127, 139, 141, 142, 143, 144, 147, 148, 154], "instruct": [151, 154], "instrument": [5, 6, 7, 8, 9, 10, 25, 33, 37, 40, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 72, 73, 82, 86, 87, 88, 91, 94, 96, 99, 101, 102, 103, 104, 107, 108, 110, 111, 113, 117, 119, 123, 132, 138, 152, 155], "instrument_effect": 57, "instrument_impact": 74, "instrument_strength": 93, "insuffienct": 85, "int": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 44, 45, 48, 49, 52, 59, 60, 61, 64, 67, 72, 74, 78, 90, 91, 93, 95, 96, 101, 104], "int32": 67, "int64": [66, 68, 69, 70, 86, 99, 101, 104, 152], "int8": [87, 88, 94], "integ": [18, 62, 107], "integr": [72, 85, 95, 139, 149, 154], "intend": [44, 62, 95, 155], "intent": [108, 155], "inter": 107, "interact": [22, 23, 25, 26, 31, 33, 34, 35, 44, 45, 73, 95, 105, 107, 117, 118, 146, 147, 150, 154, 155], "interchang": 138, "interest": [25, 26, 31, 37, 38, 39, 58, 61, 63, 64, 75, 79, 87, 88, 92, 93, 96, 98, 106, 108, 110, 112, 115, 116, 117, 118, 119, 120, 121, 123, 138, 152, 155], "interfac": [59, 61, 62, 72, 99, 104, 107, 122, 152], "intermedi": 95, "intern": [19, 59, 61, 62, 73, 85, 88, 107, 108, 109, 153], "internet": [61, 87, 88], "interpret": [66, 67, 68, 69, 80, 81, 93, 95, 106, 139, 140, 146, 147, 148, 149, 151, 155], "intersect": [95, 139, 145, 154], "interv": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 48, 59, 60, 61, 63, 64, 66, 67, 68, 69, 72, 73, 76, 77, 78, 80, 81, 83, 86, 88, 91, 92, 94, 96, 105, 106, 122, 123, 139, 145, 152, 153, 154, 155], "intial": 92, "introduc": [58, 75, 98, 99, 104, 138, 154, 155], "introduct": [58, 60, 62, 75, 86, 88, 94, 107, 108, 110, 114, 139, 140], "introductori": [59, 95], "intrument": [63, 96], "intspecifi": 44, "intuit": 95, "inuidur1": [62, 70, 99, 104, 152], "inuidur1femaleblackothracedep1dep2q2q3q4q5q6agelt35agegt54durablelusdhusdtg": [62, 99, 104, 152], "inuidur2": [70, 99, 104, 152], "inv_sigmoid": 89, "invalid": [58, 69, 75, 86, 93, 98], "invari": [108, 110, 114], "invers": [22, 24, 25, 26, 27, 28, 29, 30, 63, 96, 139, 146, 147], "invert": [25, 93], "invert_yaxi": 86, "investig": [79, 85, 95], "involv": [106, 107, 123, 155], "io": [89, 154], "ipw_norm": 154, "ipykernel_48354": 65, "ipykernel_50630": 76, "ipykernel_52285": 86, "ipynb": [57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96], "ira": [61, 87, 88], "irm": [0, 9, 12, 14, 15, 20, 21, 37, 38, 48, 49, 73, 77, 81, 82, 84, 95, 96, 97, 103, 104, 105, 107, 111, 113, 118, 131, 146, 147, 150, 154, 155], "irm_summari": 87, "irmglmnet": 61, "irmrang": 61, "irmrpart": 61, "irmxgboost8047": 61, "irrespect": 95, "irrevers": [108, 114], "is_classifi": [12, 14, 15, 22, 25, 26, 38], "is_gat": [22, 26, 38, 48], "isfinit": [66, 67, 68, 69], "isnan": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 84, 107], "isoton": [52, 53, 95], "isotonicregress": 95, "issn": 79, "issu": [13, 16, 95, 150, 153, 154], "ite": [66, 69, 72, 73, 80, 81, 82], "ite_lower_quantil": [66, 69], "ite_mean": [66, 69], "ite_upper_quantil": [66, 69], "item": [25, 72, 87, 97, 107, 122], "iter": [44, 57, 63, 64, 86, 87, 92, 96, 107, 138, 155], "itertool": 79, "its": [46, 47, 72, 95, 97, 106, 107, 108, 110, 111, 113, 122, 123, 138], "iv": [25, 33, 37, 38, 40, 41, 58, 60, 75, 86, 98, 99, 104, 117, 119, 134, 135, 139, 148, 150, 154, 155], "iv_2": 57, "iv_var": [60, 86], "iv\u00e1n": [122, 153], "j": [10, 11, 17, 18, 19, 31, 33, 34, 36, 39, 40, 41, 42, 43, 58, 59, 60, 62, 63, 73, 75, 79, 86, 89, 93, 96, 98, 107, 108, 115, 138, 150, 152], "j_": [60, 86], "j_0": 138, "j_1": [60, 86], "j_2": [60, 86], "j_3": [60, 86], "j_k": [60, 86], "jame": 153, "janari": [66, 69], "janni": [61, 87], "januari": 66, "jasenakova": 154, "javanmard": 153, "jbe": [60, 86], "jeconom": [17, 18, 19, 31, 39, 59], "jerzi": 153, "jia": 95, "jitter": [16, 72], "jitter_strength": 72, "jitter_valu": 16, "jk": [108, 116], "jmlr": [62, 150, 152, 154], "job": [61, 87, 88], "john": 153, "joint": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 48, 66, 67, 68, 69, 73, 76, 77, 78, 80, 81, 83, 88, 91, 93, 108, 110, 138, 154, 155], "jointli": [91, 106], "jonathan": 153, "joss": [62, 107, 150, 152], "journal": [10, 11, 17, 18, 19, 31, 36, 39, 41, 42, 59, 60, 62, 79, 86, 89, 93, 95, 98, 107, 150, 152, 153, 154], "jss": 150, "jump": [90, 92, 108], "jun": [59, 153], "jupyt": [57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96], "juraj": 153, "just": [59, 62, 64, 65, 66, 69, 73, 78, 80, 81, 82, 83, 90, 91, 108, 123, 124, 125, 126, 127, 139, 140], "justif": [122, 139, 140], "k": [10, 17, 18, 19, 33, 34, 36, 39, 40, 41, 42, 58, 60, 62, 75, 84, 85, 86, 92, 93, 97, 98, 105, 106, 108, 138, 155], "k_h": [92, 108], "kaggl": [61, 87], "kallu": [78, 88, 91, 93, 94, 123, 129, 132, 133, 153], "kappa": 108, "kato": [41, 60, 86, 138, 153], "kb": [67, 68, 72, 73, 82, 86, 87, 88, 94, 99, 101, 104, 152], "kde": [27, 28, 29, 87], "kdeplot": [64, 84, 96], "kdeunivari": [27, 28, 29], "kecsk\u00e9sov\u00e1": 154, "keel": 93, "keep": [47, 51, 59, 83, 95, 103, 104, 155], "kei": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 44, 49, 60, 61, 76, 77, 80, 81, 85, 86, 87, 88, 92, 95, 107, 108, 123, 139, 145, 154], "keith": 153, "kelz": 93, "kengo": 153, "kennedi": 93, "kept": [102, 104], "kernel": [27, 28, 29, 44, 47, 51, 92, 108], "kernel_regress": 92, "kernelreg": 92, "keyword": [17, 18, 19, 22, 26, 38, 41, 42, 43, 45, 48], "kf": 122, "kfold": [86, 122], "kind": [57, 74, 87], "kj": [17, 18, 19, 33, 34, 36, 39, 40, 41, 42, 58, 60, 75, 86, 98], "klaassen": [33, 79, 84, 85, 95, 150, 153], "klaa\u00dfen": 33, "knau": 153, "know": [64, 90, 93], "knowledg": [57, 74, 84, 89, 90], "known": [82, 84, 92, 93, 95, 107, 108, 114], "kohei": 153, "kotthof": 62, "kotthoff": [62, 107, 150, 152], "krueger": 89, "kueck": [61, 87], "kurz": [150, 153, 154], "kwarg": [17, 18, 19, 22, 26, 31, 35, 38, 39, 41, 42, 43, 44, 45, 46, 48, 85, 108], "l": [60, 62, 63, 70, 76, 77, 86, 93, 95, 96, 107, 139, 148, 150, 152], "l1": [87, 96, 108], "l_hat": [37, 38, 58, 75, 123], "lab": 63, "label": [13, 16, 46, 50, 65, 72, 73, 75, 76, 77, 78, 80, 81, 83, 85, 88, 89, 91, 92], "labor": 89, "laffer": 153, "laff\u00e9r": [36, 63, 96, 108, 121, 123, 136, 137], "lal": [89, 154], "lambda": [60, 61, 62, 63, 66, 69, 87, 89, 90, 107, 108, 123, 124, 125, 138, 152], "lambda_": 79, "lambda_0": [123, 124, 125], "lambda_t": [18, 19, 69], "land": 90, "lang": [62, 107, 150, 152], "langl": [32, 90], "lanni": 93, "lappli": 122, "larg": [58, 75, 82, 84, 85, 89, 95, 108], "larger": [26, 59, 92, 95, 139, 145], "largest": 84, "largli": 84, "lasso": [60, 61, 62, 63, 67, 87, 96, 107, 152, 153], "lasso_class": [61, 87], "lasso_pip": [62, 107], "lasso_summari": 87, "lassocv": [30, 67, 79, 86, 87, 96, 107, 108, 138, 152], "last": [18, 62, 151], "late": [25, 57, 61, 87, 93, 108, 117, 123, 130], "latent": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 94, 139, 148, 149], "later": [61, 62, 92, 95, 107, 155], "latest": 150, "latter": [50, 51, 93, 108], "layout": 79, "lbrace": [25, 26, 33, 34, 36, 60, 86, 97, 108, 115, 117, 118, 122, 123, 128, 138, 139, 146], "ldot": [37, 38, 60, 63, 86, 96, 97, 108, 119, 120, 122, 138, 152], "le": [18, 64, 90, 106, 108, 110, 123, 132, 133], "lead": [59, 95, 108], "leadsto": 138, "lear": [62, 107, 150, 152], "learn": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 41, 42, 43, 44, 57, 61, 62, 67, 70, 73, 74, 78, 79, 83, 84, 85, 87, 88, 89, 91, 92, 93, 95, 99, 104, 105, 107, 122, 123, 138, 139, 140, 154, 155], "learner": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 52, 57, 58, 59, 60, 61, 63, 64, 66, 67, 68, 69, 72, 75, 76, 77, 79, 86, 87, 88, 93, 94, 95, 96, 97, 98, 105, 108, 110, 111, 113, 122, 123, 138, 139, 145, 154, 155], "learner_class": [30, 154], "learner_cv": 62, "learner_dict": 72, "learner_forest_classif": 62, "learner_forest_regr": 62, "learner_l": 94, "learner_lasso": 62, "learner_list": 84, "learner_m": 94, "learner_nam": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 52, 72, 107], "learner_pair": 72, "learner_param_v": 62, "learner_rf": 138, "learnerclassif": 62, "learnerregr": 62, "learnerregrcvglmnet": 62, "learnerregrrang": [62, 107], "learning_r": [66, 69, 75, 78, 88, 91, 92, 95, 98], "least": [57, 61, 74, 87, 88, 94, 108, 112, 122], "leav": [63, 95, 96], "left": [17, 19, 33, 34, 36, 40, 41, 58, 60, 72, 73, 75, 84, 86, 87, 88, 89, 91, 92, 98, 108, 123, 124, 125, 126, 127, 138, 139, 141, 142, 143, 144, 146, 147], "legend": [61, 65, 66, 69, 72, 73, 75, 76, 77, 78, 80, 81, 83, 84, 88, 89, 91], "lemp": 67, "len": [72, 73, 78, 84, 85, 86, 88, 91, 93], "length": [12, 13, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 58, 62, 64, 66, 69, 93, 107], "leq": [60, 86], "less": [59, 61, 87, 88, 92, 95], "lester": 153, "let": [17, 18, 19, 31, 35, 39, 58, 59, 61, 62, 63, 64, 66, 69, 72, 73, 75, 78, 80, 81, 83, 84, 87, 88, 91, 95, 96, 97, 98, 107, 108, 110, 114, 121, 139, 140, 149, 155], "level": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 44, 48, 60, 61, 63, 64, 65, 66, 67, 68, 69, 72, 73, 76, 77, 78, 80, 81, 82, 83, 86, 87, 88, 91, 93, 94, 95, 96, 107, 115, 116, 123, 128, 139, 145, 146, 155], "level_0": [62, 86], "level_1": 86, "level_bound": [72, 73], "leverag": 72, "levi": 93, "levinsohn": [60, 86], "lewi": 153, "lgbm": 72, "lgbmclassifi": [64, 65, 66, 69, 72, 78, 84, 88, 91, 92, 95], "lgbmregressor": [64, 65, 66, 69, 72, 75, 78, 84, 88, 92, 95, 98], "lgr": [58, 59, 60, 61, 62, 63, 97, 107, 108, 122, 123, 138, 152], "lib": [65, 66, 67, 68, 69, 72, 86, 87, 92, 95], "liblinear": [87, 96, 108], "librari": [57, 58, 59, 60, 61, 62, 63, 72, 97, 98, 99, 104, 107, 108, 122, 123, 138, 151, 152, 155], "licens": [150, 154], "lie": 153, "lightgbm": [64, 65, 66, 69, 72, 75, 78, 84, 88, 91, 92, 95], "like": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 46, 47, 50, 51, 59, 61, 62, 66, 67, 68, 69, 79, 87, 88, 95, 107, 108, 109, 122, 152, 155], "lim": 89, "lim_": [92, 108], "limegreen": [76, 77], "limit": [89, 108, 114, 153], "limits_": 106, "lin": [92, 95, 108], "line": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 65, 67, 72, 73, 95], "line_width": 72, "linear": [17, 19, 20, 21, 22, 26, 31, 35, 37, 38, 39, 40, 41, 42, 43, 48, 57, 58, 59, 60, 62, 64, 65, 67, 72, 73, 74, 75, 76, 77, 79, 80, 83, 84, 85, 86, 93, 94, 95, 97, 98, 105, 106, 107, 111, 112, 113, 119, 120, 122, 124, 125, 126, 127, 128, 130, 131, 134, 135, 138, 145, 147, 148, 149, 150, 152, 153, 154, 155], "linear_learn": [66, 69], "linear_model": [22, 26, 30, 38, 48, 66, 67, 68, 69, 70, 72, 73, 74, 79, 83, 84, 86, 87, 92, 93, 95, 96, 107, 108, 138, 152], "linearli": [92, 108], "linearregress": [57, 66, 67, 68, 69, 72, 73, 74, 83, 84, 92, 93, 95], "linearscoremixin": [0, 123], "lineplot": [66, 69, 72, 73], "linestyl": [65, 66, 69, 72, 73, 85, 92], "linetyp": 63, "linewidth": [65, 66, 69, 72], "link": [95, 154], "linspac": [76, 77, 83, 95], "lint": 154, "linux": 151, "list": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 47, 51, 52, 58, 59, 60, 61, 62, 66, 67, 69, 75, 76, 77, 86, 88, 90, 98, 107, 122, 123, 151, 154], "list_confset": 25, "listedcolormap": 86, "literatur": [95, 108, 110, 114], "littl": [82, 93], "ll": [62, 138, 155], "lllllllllllllllll": [99, 104, 152], "lm": [57, 59, 95], "ln_alpha_ml_l": 79, "ln_alpha_ml_m": 79, "load": [57, 59, 61, 62, 79, 87, 88, 99, 104, 151, 152], "loader": 0, "loc": [65, 66, 67, 68, 69, 72, 73, 75, 78, 79, 80, 81, 86, 89, 91, 94, 95], "local": [25, 27, 72, 93, 106, 108, 117, 153, 154], "localconvert": 86, "locat": [78, 91, 108], "log": [60, 64, 66, 67, 68, 69, 72, 79, 84, 86, 88, 89, 94, 96, 107, 108, 110, 111, 113], "log_odd": 90, "log_p": [60, 86], "log_reg": [57, 59], "logarithm": [72, 79], "logic": [25, 62, 107], "logical_not": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 84, 107], "logist": [31, 45, 57, 59, 61, 63, 67, 72, 73, 74, 87, 93, 95, 96, 155], "logisticregress": [57, 66, 67, 68, 69, 70, 72, 73, 74, 83, 92, 93, 95], "logisticregressioncv": [30, 67, 84, 87, 96, 108], "logit": [84, 89], "loglik": 62, "logloss": [61, 72, 87, 155], "logloss_m": 72, "logo": 154, "logspac": 87, "long": [7, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 58, 67, 75, 84, 94, 95, 139, 140, 149, 153], "longer": [66, 69, 99, 104, 108, 112], "look": [59, 61, 62, 64, 65, 66, 67, 68, 69, 78, 84, 87, 88, 91, 92, 94], "loop": [72, 73, 93], "loss": [64, 66, 67, 68, 69, 72, 84, 85, 88, 92, 94, 96, 107, 108, 110, 111, 113], "loss_ml_g0": 84, "loss_ml_g1": 84, "loss_ml_m": 84, "low": [65, 82, 93, 106, 153], "lower": [25, 61, 62, 65, 66, 69, 72, 73, 78, 79, 82, 83, 88, 89, 91, 92, 93, 94, 95, 107, 139, 145, 149, 155], "lower_bound": [76, 77], "lpop": 67, "lpq": [27, 29, 88, 106, 132, 154], "lpq_0": 91, "lpq_1": 91, "lqte": 106, "lr": 92, "lrn": [57, 58, 59, 60, 61, 62, 63, 97, 107, 108, 122, 123, 138, 152, 155], "lrn_0": 62, "lt": [57, 59, 60, 61, 62, 63, 66, 67, 68, 69, 70, 72, 73, 82, 86, 87, 88, 90, 94, 95, 99, 104, 152], "lucien": 154, "luka": 153, "luk\u00e1\u0161": 36, "lusd": [62, 70, 99, 104, 152], "lvert": 79, "m": [7, 10, 11, 16, 30, 31, 40, 41, 42, 58, 60, 62, 66, 69, 70, 72, 75, 79, 82, 84, 85, 86, 89, 93, 98, 101, 104, 105, 106, 107, 108, 109, 111, 113, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 141, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154], "m_": [72, 73, 108, 111, 113, 115, 123, 125, 127, 128, 132, 138], "m_0": [12, 14, 15, 16, 22, 24, 25, 26, 28, 37, 38, 42, 43, 44, 58, 60, 61, 75, 79, 82, 85, 86, 87, 98, 106, 107, 108, 117, 118, 119, 120, 123, 124, 125, 126, 127, 129, 132, 133, 135, 136, 137, 152, 155], "m_hat": [25, 26, 37, 38, 58, 75, 83, 123], "m_i": [92, 108], "ma": [41, 60, 86, 93, 108, 109, 153], "mac": 151, "machin": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 41, 42, 43, 44, 57, 61, 62, 63, 64, 66, 67, 68, 69, 70, 73, 74, 78, 79, 83, 84, 85, 87, 88, 89, 91, 92, 93, 94, 95, 96, 105, 107, 108, 110, 111, 113, 122, 123, 138, 139, 140, 154, 155], "machineri": [79, 153], "mackei": 153, "maco": 151, "made": [108, 121, 155], "mae": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 84, 107], "maggi": 153, "magnitud": [139, 140], "mai": [47, 51, 63, 64, 96], "main": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 68, 72, 79, 88, 95, 108, 138, 139, 140, 153, 155], "mainli": 95, "maintain": [59, 150, 154], "mainten": 154, "major": [62, 95, 154], "make": [57, 72, 73, 74, 84, 85, 95, 106, 107, 154, 155], "make_confounded_irm_data": [95, 154], "make_confounded_plr_data": 94, "make_did_cs2021": [7, 16, 66, 101, 104, 108, 109, 113], "make_did_cs_cs2021": [69, 108, 111], "make_did_sz2020": [5, 12, 15, 64, 100, 104, 108, 110], "make_heterogeneous_data": [76, 77, 80, 81, 82], "make_iivm_data": [25, 27, 106, 108], "make_irm_data": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 83, 84, 106, 107, 108], "make_irm_data_discrete_treat": [72, 73], "make_pipelin": 87, "make_pliv_chs2015": [37, 108], "make_pliv_multiway_cluster_ckms2021": [60, 86], "make_plr_ccddhnr2018": [6, 7, 12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 58, 75, 85, 97, 98, 106, 107, 108, 122, 123, 138, 139, 145], "make_simple_rdd_data": [8, 44, 92, 102, 104, 108], "make_spd_matrix": 43, "make_ssm_data": [9, 63, 96, 103, 104, 108], "malt": [150, 153], "man": [57, 74], "manag": [107, 151], "mandatori": [102, 104], "mani": [20, 21, 40, 58, 59, 60, 62, 64, 75, 85, 86, 98, 123, 138, 155], "manili": 48, "manipul": [61, 62, 92, 108], "manual": [61, 83, 85, 94, 155], "mao": 153, "map": [25, 46, 47, 50, 51, 59, 60, 86, 106, 108, 117], "mapsto": [97, 106], "mar": [36, 108], "march": [79, 84, 85], "margin": [76, 77, 95], "marit": [61, 87], "marker": [66, 69, 73, 95], "markers": 89, "market": 89, "markettwo": 60, "markov": [43, 153], "marr": [61, 87, 88, 94, 155], "marshal": 107, "martin": [36, 95, 150, 153, 154], "masatoshi": 153, "masip": [93, 154], "mask": 16, "maskedarrai": [108, 109], "master": [59, 67], "mat": 60, "match": [107, 139, 148], "math": [30, 66, 67, 68, 69], "mathbb": [17, 18, 19, 20, 21, 25, 26, 31, 35, 37, 38, 39, 60, 63, 64, 65, 66, 69, 72, 73, 82, 84, 85, 86, 89, 92, 96, 106, 108, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 152, 155], "mathcal": [17, 18, 19, 31, 32, 33, 34, 35, 36, 39, 40, 41, 42, 43, 45, 58, 60, 63, 65, 75, 78, 86, 90, 91, 96, 98, 108, 109, 112, 114], "mathop": 106, "mathrm": [31, 39, 66, 67, 68, 69, 92, 108, 109, 111, 112, 113, 114, 123, 125, 127, 139, 142, 144], "matia": 153, "matplotlib": [13, 16, 64, 65, 66, 67, 68, 69, 70, 72, 73, 75, 76, 77, 78, 80, 81, 83, 84, 85, 86, 87, 88, 89, 91, 92, 95, 96], "matric": [90, 154], "matrix": [17, 18, 19, 31, 33, 34, 35, 36, 39, 40, 41, 42, 43, 47, 51, 58, 60, 61, 62, 63, 75, 86, 96, 98, 99, 104, 107, 138, 152, 154, 155], "matt": 153, "matter": [84, 89], "max": [17, 19, 61, 62, 72, 83, 87, 88, 93, 97, 106, 107, 108, 122, 123, 125, 127, 129, 138, 139, 142, 144, 152, 155], "max_depth": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 70, 87, 94, 97, 106, 107, 108, 110, 122, 123, 138, 139, 145, 152, 155], "max_featur": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 70, 87, 94, 97, 106, 107, 108, 122, 123, 138, 139, 145, 152, 155], "max_it": [72, 86, 87, 95], "maxim": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 90, 106, 108], "maxima": 138, "maximum": [52, 53, 106, 107], "mb": [66, 69, 70, 99, 104, 152], "mb706": 154, "mea": 33, "mean": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 46, 47, 50, 51, 57, 58, 60, 61, 64, 66, 69, 72, 73, 74, 75, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 91, 93, 94, 95, 98, 107, 108, 138, 155], "mean_absolute_error": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 84, 107], "meant": [67, 106, 154], "measir": 94, "measur": [59, 62, 67, 79, 85, 93, 94, 95, 107, 108, 109, 121, 139, 140, 146, 147, 148, 149], "measure_col": 79, "measure_func": 59, "measure_pr": 59, "measures_r": 59, "mechan": [46, 47, 50, 51, 95, 108, 114], "median": [93, 95, 122], "medium": 72, "melt": 60, "membership": 95, "memori": [66, 67, 68, 69, 70, 72, 73, 82, 86, 87, 88, 94, 99, 101, 104, 152], "mention": [82, 106], "merg": [61, 87], "mert": [122, 153], "meshgrid": [76, 77, 95], "messag": [58, 59, 60, 61, 62, 63, 72, 84, 152, 154], "meta": [46, 47, 50, 51, 107, 152], "metadata": [46, 47, 50, 51], "metadata_rout": [46, 47, 50, 51], "metadatarequest": [46, 47, 50, 51], "method": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 46, 47, 48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 80, 81, 83, 84, 86, 87, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 104, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 139, 140, 145, 150, 152, 154], "methodolog": 153, "methodologi": 95, "metric": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 46, 50, 72, 107], "michael": 153, "michaela": 154, "michel": [150, 152], "michela": [36, 153], "mid": [61, 87, 89, 92, 108, 123, 135], "mid_point": [72, 73], "might": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 60, 67, 78, 83, 84, 86, 90, 92, 94, 95, 107, 108], "mild": [58, 75, 98], "militari": 89, "miller": [60, 86], "mimic": 95, "min": [60, 61, 62, 63, 66, 67, 68, 69, 72, 78, 83, 86, 87, 88, 91, 92, 97, 107, 108, 112, 122, 123, 138, 152, 155], "min_": 106, "min_samples_leaf": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 49, 82, 87, 94, 97, 106, 107, 108, 110, 122, 123, 138, 139, 145, 155], "min_samples_split": [87, 108, 109, 111, 113], "miniconda3": 72, "minim": [26, 49, 61, 84, 87, 92, 108], "minimum": [52, 53, 67], "minor": [58, 67, 75, 98, 123, 154], "minsplit": 61, "minut": 85, "mirror": [99, 104], "miruna": 153, "mislead": 154, "miss": [5, 6, 7, 8, 9, 30, 62, 99, 100, 104, 107, 108, 123, 136, 154], "missing": [36, 63, 96], "misspecif": 64, "misspecifi": 64, "mit": [150, 152], "mixin": [0, 20, 21, 123], "ml": [43, 60, 61, 62, 67, 79, 85, 86, 87, 92, 93, 97, 105, 107, 108, 122, 150, 153, 154], "ml_g": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 57, 58, 59, 61, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 78, 80, 82, 83, 84, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 106, 107, 108, 109, 110, 111, 113, 154], "ml_g0": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 59, 61, 64, 66, 67, 68, 70, 84, 87, 94, 107, 108, 110, 113], "ml_g1": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 59, 61, 64, 66, 67, 68, 70, 84, 87, 94, 107, 108, 110, 113], "ml_g_d0": [96, 108], "ml_g_d0_t0": [64, 69, 108, 110, 111], "ml_g_d0_t1": [64, 69, 108, 110, 111], "ml_g_d1": [96, 108], "ml_g_d1_t0": [64, 69, 108, 110, 111], "ml_g_d1_t1": [64, 69, 108, 110, 111], "ml_g_d_lvl0": [72, 108], "ml_g_d_lvl1": [72, 108], "ml_g_sim": 30, "ml_l": [37, 38, 58, 60, 61, 62, 70, 75, 77, 81, 85, 86, 87, 89, 94, 97, 98, 107, 108, 122, 123, 138, 139, 145, 152, 154, 155], "ml_l_bonu": 152, "ml_l_forest": 62, "ml_l_forest_pip": 62, "ml_l_lasso": 62, "ml_l_lasso_pip": 62, "ml_l_rf": 155, "ml_l_sim": 152, "ml_l_tune": 107, "ml_l_xgb": 155, "ml_m": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 139, 145, 152, 154, 155], "ml_m_bench_control": 95, "ml_m_bench_treat": 95, "ml_m_bonu": 152, "ml_m_forest": 62, "ml_m_forest_pip": 62, "ml_m_lasso": 62, "ml_m_lasso_pip": 62, "ml_m_rf": 155, "ml_m_sim": [30, 152], "ml_m_tune": 107, "ml_m_xgb": 155, "ml_pi": [30, 63, 96, 108], "ml_pi_sim": 30, "ml_r": [25, 37, 57, 60, 61, 74, 86, 87, 93, 108, 154], "ml_r0": 108, "ml_r1": [61, 87, 108], "mlr": [62, 107], "mlr3": [57, 58, 59, 60, 61, 63, 97, 107, 108, 122, 123, 138, 150, 152, 154, 155], "mlr3book": [62, 107], "mlr3extralearn": [61, 107], "mlr3filter": 62, "mlr3learner": [57, 58, 59, 60, 61, 97, 107, 108, 122, 123, 138, 152, 155], "mlr3measur": 59, "mlr3pipelin": [107, 154], "mlr3tune": [62, 107, 154], "mlr3vers": 61, "mlrmeasur": 59, "mode": [95, 151], "model": [0, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 47, 48, 49, 51, 54, 57, 58, 59, 60, 62, 64, 65, 66, 67, 68, 69, 74, 75, 78, 79, 82, 84, 86, 88, 91, 94, 97, 98, 99, 101, 103, 104, 105, 107, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 124, 125, 126, 127, 128, 130, 131, 134, 135, 140, 145, 146, 147, 148, 149, 150, 153, 154], "model_data": [61, 87], "model_label": 85, "model_list": 72, "model_select": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 75, 86, 107, 122], "modellist": [72, 83], "modelmlestimatelowerupp": 61, "modern": [62, 107, 150, 152], "modul": [72, 92, 104, 108, 151], "moment": [20, 21, 60, 86, 108, 111, 112, 113, 123, 138, 139, 140, 149, 152], "monoton": 108, "mont": [31, 32, 35, 39, 76, 77, 80, 81], "montanari": 153, "month": [66, 69], "more": [26, 48, 57, 59, 61, 66, 67, 68, 69, 72, 73, 74, 76, 77, 79, 83, 84, 85, 87, 88, 92, 94, 95, 97, 106, 107, 108, 109, 110, 111, 112, 113, 114, 121, 123, 131, 138, 139, 140, 145, 149, 152, 155], "moreov": [61, 62, 67, 79, 107, 138, 155], "mortgag": [61, 87, 88], "most": [61, 78, 84, 87, 88, 91, 95, 106, 107, 108, 139, 145, 151], "motiv": [95, 98], "motivation_example_bch": 79, "mp": 59, "mpd": [60, 86], "mpdta": 67, "mpg": 86, "mse": [62, 79, 107], "mserd": 92, "msg": [69, 72], "msr": [62, 107], "mtry": [61, 62, 97, 107, 108, 122, 123, 138, 155], "mu": 65, "mu_": 65, "mu_0": 108, "mu_mean": 65, "much": [61, 62, 72, 87, 92, 93, 95, 155], "muld": [70, 99, 104, 152], "multi": [16, 46, 50, 59, 60, 76, 77, 86, 123, 125, 127, 154], "multiclass": [62, 85], "multiindex": 86, "multioutput": [47, 51], "multioutputregressor": [47, 51], "multipl": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 59, 60, 61, 63, 64, 67, 68, 72, 83, 86, 87, 94, 95, 96, 99, 104, 107, 112, 116, 119, 122, 138, 139, 140, 153, 154, 155], "multipletest": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38], "multipli": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 75, 105, 106, 123, 155], "multiprocess": [78, 88, 91], "multitest": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38], "multivariate_norm": 30, "multiwai": [41, 60, 86, 153], "music": 153, "must": [12, 13, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 53, 99, 100, 104, 107, 108], "mutat": 62, "mutual": [22, 26, 38, 61, 80, 81, 87, 88, 106], "my_sampl": 122, "my_task": 122, "n": [12, 14, 15, 17, 18, 19, 22, 24, 25, 26, 27, 28, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 57, 58, 60, 62, 63, 65, 66, 69, 72, 73, 74, 75, 78, 79, 82, 86, 89, 90, 91, 92, 93, 96, 97, 98, 106, 107, 108, 114, 122, 138, 150, 151], "n_": [35, 65, 69, 139, 142, 144], "n_aggreg": 13, "n_coef": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 139, 145], "n_color": [66, 69], "n_complier": 91, "n_core": [78, 88, 91], "n_estim": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 64, 65, 66, 69, 70, 72, 75, 76, 77, 78, 80, 81, 82, 87, 88, 90, 91, 92, 94, 95, 97, 98, 106, 107, 108, 110, 122, 123, 138, 139, 145, 152, 155], "n_eval": [62, 107], "n_featur": [46, 47, 50, 51], "n_fold": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 58, 59, 60, 61, 64, 66, 67, 69, 70, 75, 76, 77, 78, 80, 81, 83, 84, 86, 87, 88, 89, 90, 91, 92, 94, 95, 98, 107, 122, 152, 155], "n_folds_per_clust": [60, 86], "n_folds_tun": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "n_framework": 13, "n_iter": [44, 92, 108], "n_iter_randomized_search": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 107], "n_job": [78, 87, 88, 91], "n_jobs_cv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 84], "n_jobs_model": [16, 23, 29, 78, 88, 91], "n_learner": 72, "n_level": [35, 72, 73], "n_ob": [7, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 45, 48, 49, 58, 62, 63, 64, 65, 66, 69, 72, 73, 75, 76, 77, 80, 81, 82, 83, 84, 85, 92, 94, 95, 96, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 113, 122, 138, 139, 145, 152], "n_output": [46, 47, 50, 51], "n_period": [17, 19, 66, 69], "n_pre_treat_period": [17, 19, 66, 69], "n_rep": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 58, 59, 60, 63, 64, 66, 69, 70, 72, 73, 75, 82, 83, 84, 86, 92, 94, 95, 96, 98, 107, 122, 139, 145, 152, 155], "n_rep_boot": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 48, 66, 67, 68, 69, 73, 76, 77, 78, 80, 81, 83, 88, 91, 138], "n_sampl": [46, 47, 50, 51, 90, 93], "n_samples_fit": [47, 51], "n_split": 122, "n_t": 65, "n_target": [50, 51], "n_theta": 13, "n_time_period": 65, "n_true": [78, 91], "n_var": [58, 62, 75, 98, 99, 104, 107, 138, 152], "n_w": 90, "n_x": [32, 76, 77, 80, 81, 82], "na": [5, 6, 7, 8, 9, 58, 60, 63, 98, 154], "na_real_": [60, 154], "naiv": [58, 75, 98], "name": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 50, 51, 52, 58, 59, 60, 65, 66, 67, 69, 72, 80, 81, 82, 85, 86, 92, 94, 95, 107, 151, 154], "namespac": 59, "nan": [5, 6, 7, 8, 9, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 64, 65, 72, 73, 75, 78, 80, 81, 84, 85, 87, 88, 91, 96, 98, 107], "nanmean": 75, "narita": 153, "nat": [66, 69], "nathan": 153, "nation": [95, 122, 153], "nativ": 59, "natt": 90, "natur": 95, "nbest": 72, "ncol": [60, 61, 62, 92, 99, 104, 107, 138, 152], "ncoverag": 84, "ndarrai": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 52, 99, 104], "nearli": 84, "necess": [60, 86], "necessari": [59, 60, 72, 85, 86, 92, 108, 151], "need": [12, 14, 15, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 57, 58, 59, 61, 63, 72, 74, 75, 85, 88, 93, 96, 107, 122, 139, 149, 154, 155], "neg": [47, 51], "neighborhood": [92, 138], "neither": [5, 6, 7, 8, 9, 60, 86, 93, 99, 104], "neng": 153, "neq": [72, 92, 108], "nest": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 50, 51, 107, 123, 137, 139, 145], "net": [88, 94, 155], "net_tfa": [61, 87, 88, 94, 155], "network": 72, "nev": [108, 111, 113, 114], "never": [17, 19, 25, 59, 60, 66, 67, 68, 69, 86, 108, 114, 154], "never_tak": [25, 61, 87], "never_tr": [14, 16, 66, 67, 68, 69, 108, 109, 111, 113], "nevertheless": 83, "new": [57, 58, 59, 60, 61, 62, 63, 76, 77, 85, 87, 90, 97, 98, 99, 104, 106, 107, 108, 122, 123, 138, 150, 152, 153, 154, 155], "new_data": [76, 77, 90], "newei": [10, 11, 42, 60, 79, 86, 95, 98, 150, 153], "newest": 154, "next": [59, 61, 62, 76, 77, 78, 82, 84, 87, 88, 90, 91, 93, 95, 154], "neyman": [60, 86, 97, 105, 139, 149, 150, 153], "nfold": [60, 61, 63, 108], "nh": 108, "nice": [59, 72], "nifa": [87, 88, 94], "nil": 95, "nine": [60, 86], "nlogloss": 72, "nn": 92, "noack": [92, 108, 153, 154], "node": [61, 62, 97, 108, 122, 123, 138, 152, 155], "nois": [45, 89, 90], "nomin": 93, "non": [14, 17, 18, 19, 25, 41, 42, 43, 44, 57, 58, 61, 65, 66, 69, 72, 74, 75, 87, 88, 90, 92, 107, 122, 123, 125, 127, 138, 139, 142, 144], "non_orth_scor": [58, 75, 123], "nondur": 70, "none": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 44, 46, 47, 48, 50, 51, 52, 53, 60, 61, 64, 66, 67, 68, 69, 70, 72, 73, 74, 82, 87, 88, 93, 94, 95, 96, 99, 101, 102, 103, 104, 107, 108, 110, 111, 113, 123, 138, 151, 152], "nonignor": [30, 137], "nonlinear": [17, 19, 21, 61, 66, 69, 87, 92, 108, 123, 132, 133, 154], "nonlinearscoremixin": [0, 123], "nonparametr": [27, 28, 29, 92, 95, 123, 139, 140, 146, 147, 148, 149, 153], "nop": 62, "nor": [5, 6, 7, 8, 9, 60, 86, 93, 99, 104], "norm": 75, "normal": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 58, 63, 64, 65, 66, 68, 69, 74, 75, 78, 82, 88, 89, 90, 91, 92, 93, 96, 98, 99, 104, 107, 108, 123, 124, 125, 126, 127, 138, 152], "normalize_ipw": [22, 23, 24, 25, 26, 27, 28, 29, 30, 63, 83, 88, 96], "not_yet_tr": [14, 16, 66, 69], "notat": [60, 63, 64, 86, 96, 108, 110, 111, 112, 113, 114, 121, 123, 125, 127], "note": [5, 6, 7, 8, 9, 13, 16, 19, 20, 21, 25, 26, 30, 37, 38, 46, 47, 49, 50, 51, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 122, 123, 150, 152], "notebook": [57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 106, 107, 108, 155], "notic": [57, 74, 93], "now": [59, 60, 61, 63, 67, 68, 72, 76, 77, 84, 86, 87, 90, 93, 95, 96, 104, 152, 154], "np": [5, 6, 7, 8, 9, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 44, 52, 64, 65, 66, 67, 68, 69, 70, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 104, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 139, 145, 152, 155], "nrmse": 72, "nround": [58, 61, 155], "nrow": [59, 60, 62, 92, 99, 104, 107, 138, 152], "nu": [18, 25, 43, 63, 96, 108, 117, 139, 140, 142, 144, 145, 148, 149], "nu2": [69, 72, 84, 139, 145], "nu_0": [139, 149], "nu_i": [63, 96], "nuis_g0": 57, "nuis_g1": 57, "nuis_l": 155, "nuis_m": [57, 155], "nuis_r0": 57, "nuis_r1": 57, "nuis_rmse_ml_l": 79, "nuis_rmse_ml_m": 79, "nuisanc": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 42, 43, 44, 58, 59, 60, 61, 62, 63, 64, 67, 72, 75, 76, 77, 78, 79, 82, 84, 86, 87, 88, 91, 93, 94, 95, 96, 97, 98, 107, 108, 111, 112, 113, 122, 123, 124, 125, 126, 127, 128, 132, 138, 139, 142, 144, 149, 150, 154, 155], "nuisance_el": [139, 141, 142, 143, 144, 146, 147, 148], "nuisance_loss": [72, 84, 107, 154], "nuisance_target": 84, "null": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 59, 94, 107, 139, 145, 154], "null_hypothesi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 94, 139, 145], "num": [61, 62, 97, 107, 108, 122, 123, 138, 152], "num_leav": [65, 78, 88, 91], "number": [12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 51, 58, 60, 65, 66, 67, 69, 75, 76, 77, 78, 79, 80, 81, 84, 86, 88, 90, 91, 92, 95, 108, 112, 122, 138, 150, 152, 155], "numer": [21, 53, 57, 62, 83, 89, 107, 123, 139, 146, 147, 154], "numeric_onli": 79, "numpi": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 48, 49, 52, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 101, 104, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 139, 145, 152], "nuniqu": [66, 69], "ny": 153, "nyt": [108, 111, 113, 114], "o": [65, 72, 73, 79, 80, 81, 84, 85, 87, 89, 92, 150, 152], "ob": [59, 61, 65, 69, 92, 139, 142], "obei": 123, "obj": 87, "obj_dml_data": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 57, 58, 60, 67, 68, 74, 75, 78, 83, 85, 86, 91, 97, 98, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 139, 145, 154], "obj_dml_data_bonu": [99, 104], "obj_dml_data_bonus_df": [99, 104], "obj_dml_data_from_arrai": [5, 6, 7, 8, 9], "obj_dml_data_from_df": [5, 6, 8, 9], "obj_dml_data_sim": [99, 104], "obj_dml_data_sim_clust": [99, 104], "obj_dml_plr": [58, 75, 98], "obj_dml_plr_bonu": [62, 152], "obj_dml_plr_bonus_pip": 62, "obj_dml_plr_bonus_pipe2": 62, "obj_dml_plr_bonus_pipe3": 62, "obj_dml_plr_bonus_pipe_ensembl": 62, "obj_dml_plr_fullsampl": 85, "obj_dml_plr_lesstim": 85, "obj_dml_plr_nonorth": [58, 75], "obj_dml_plr_orth_nosplit": [58, 75], "obj_dml_plr_sim": [62, 152], "obj_dml_plr_sim_pip": 62, "obj_dml_plr_sim_pipe_ensembl": 62, "obj_dml_plr_sim_pipe_tun": 62, "obj_dml_sim": 30, "object": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 57, 61, 62, 63, 64, 66, 67, 68, 69, 70, 73, 76, 77, 78, 82, 83, 85, 87, 88, 91, 92, 96, 99, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 150, 152, 153, 154, 155], "obs_confound": [57, 74], "observ": [12, 14, 15, 16, 17, 18, 19, 22, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 48, 49, 54, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 82, 84, 85, 86, 87, 88, 91, 92, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 113, 114, 121, 122, 123, 124, 125, 126, 127, 138, 139, 140, 141, 142, 143, 144, 152, 153, 155], "obtain": [19, 25, 39, 57, 58, 59, 60, 63, 64, 66, 67, 68, 69, 74, 75, 76, 77, 78, 79, 84, 86, 91, 93, 95, 96, 97, 98, 106, 107, 122, 123, 138, 139, 140, 145, 151, 152], "obvious": [66, 69], "occur": [17, 19, 66, 69, 85, 154], "off": [90, 153], "offer": [17, 19, 59, 61, 87, 88, 95, 155], "offici": 151, "offset": 107, "often": 91, "oka": 153, "ol": [22, 26, 38, 48], "olma": [92, 108, 153, 154], "omega": [82, 106, 108, 109, 123, 128, 131, 139, 146, 147], "omega_": [41, 60, 86], "omega_1": [41, 60, 86], "omega_2": [41, 60, 86], "omega_epsilon": [60, 86], "omega_v": [41, 60, 86], "omega_x": [41, 60, 86], "omit": [66, 69, 94, 95, 108, 112, 123, 125, 127, 139, 140, 149, 153, 154, 155], "ommit": 95, "onc": [59, 85, 95, 108, 114, 155], "one": [13, 16, 37, 54, 57, 58, 59, 60, 61, 62, 66, 67, 68, 69, 72, 73, 74, 75, 76, 77, 84, 86, 88, 89, 92, 93, 94, 95, 98, 99, 104, 106, 107, 108, 109, 112, 114, 119, 122, 123, 124, 125, 126, 127, 131, 134, 135, 138, 139, 140, 145, 146, 147, 148, 152, 154], "ones": [62, 65, 78, 85, 91, 94, 106], "ones_lik": [72, 73, 91], "onli": [5, 6, 7, 8, 9, 12, 14, 15, 16, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 46, 47, 48, 50, 51, 52, 53, 59, 60, 61, 66, 69, 76, 77, 80, 81, 82, 84, 85, 86, 87, 88, 92, 97, 106, 107, 108, 111, 113, 121, 123, 125, 127, 129, 132, 133, 138, 139, 140, 142, 144, 146, 147, 149, 154], "onlin": 155, "onto": 84, "oo": 85, "oob_error": [62, 107], "oop": 154, "opac": [76, 77], "open": [62, 107, 150, 152], "oper": [62, 154], "opposit": [90, 92, 108], "oprescu": [32, 76, 77, 80, 81, 153], "opt": [65, 66, 67, 68, 69, 86, 87, 92, 95], "optim": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 44, 62, 76, 77, 85, 90, 106, 107, 153], "option": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 46, 47, 50, 51, 52, 53, 57, 58, 60, 61, 63, 66, 67, 69, 73, 76, 77, 80, 81, 82, 84, 86, 87, 88, 96, 99, 100, 102, 103, 104, 107, 108, 122, 123, 129, 132, 133, 138, 154], "oracl": [35, 45, 66, 69, 72, 73], "oracle_valu": [31, 35, 39, 45, 72, 73], "orang": 58, "orcal": [31, 39], "order": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 50, 59, 60, 61, 62, 67, 83, 86, 87, 92, 107, 108, 122, 123], "org": [33, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 107, 150, 151, 153, 154], "orient": [62, 107, 123, 150, 152, 153, 154], "origin": [46, 47, 49, 50, 51, 59, 62, 66, 67, 68, 69, 90, 94, 95, 106, 123, 131], "orign": [61, 87], "orth_sign": [48, 49, 83], "orthogon": [48, 49, 60, 61, 69, 72, 86, 87, 97, 105, 108, 138, 139, 149, 150, 153], "orthongon": [139, 149], "osx": 151, "other": [5, 6, 7, 8, 9, 37, 38, 46, 47, 50, 51, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 73, 75, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 98, 99, 102, 104, 106, 107, 108, 119, 120, 122, 123, 131, 138, 139, 149, 150, 151, 152, 153, 154, 155], "other_ind": 86, "otherwis": [12, 14, 15, 22, 25, 26, 38, 46, 47, 50, 51, 61, 87, 88, 90, 108, 110, 123, 125, 127], "othrac": [62, 70, 99, 104, 152], "our": [58, 59, 61, 62, 64, 66, 69, 72, 75, 76, 77, 78, 84, 85, 87, 88, 91, 92, 94, 95, 98, 108, 150, 152, 154, 155], "ourselv": 84, "out": [37, 38, 60, 62, 64, 65, 66, 67, 68, 69, 70, 79, 84, 85, 86, 88, 94, 95, 96, 97, 99, 104, 105, 106, 107, 108, 109, 110, 111, 113, 123, 134, 135, 138, 139, 140, 145, 148, 150, 152, 154, 155], "outcom": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 35, 37, 38, 39, 45, 57, 59, 60, 61, 62, 65, 66, 67, 68, 69, 70, 74, 79, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 99, 101, 102, 103, 104, 107, 110, 111, 113, 114, 115, 119, 120, 121, 125, 127, 128, 138, 140, 145, 146, 148, 149, 152, 154, 155], "outcome_0": 74, "outcome_1": 74, "outer": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 107], "outperform": 72, "output": [59, 66, 68, 69, 84, 93, 97, 107, 108, 111, 113, 138, 155], "output_list": 93, "outshr": 86, "outsid": 58, "over": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 37, 38, 58, 66, 67, 69, 73, 75, 79, 84, 98, 105, 107, 108, 109, 139, 145, 154], "overal": [13, 66, 67, 68, 69, 90, 95, 108, 109], "overall_aggregation_weight": [13, 66, 67, 68, 69], "overcom": [105, 123], "overfit": [85, 105, 122], "overlap": [64, 95, 108, 110, 114], "overrid": [107, 154], "overridden": 108, "overst": [61, 87, 88], "overview": [84, 138, 139, 145, 153], "overwrit": 154, "ownership": [61, 87], "p": [12, 14, 15, 16, 17, 18, 19, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 37, 38, 39, 45, 52, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 98, 106, 107, 108, 109, 110, 111, 113, 114, 122, 123, 124, 125, 126, 127, 128, 129, 132, 133, 136, 137, 138, 139, 146, 147, 150, 151, 152, 154], "p401": [61, 87, 88], "p_0": [123, 124, 125, 126], "p_1": 138, "p_adjust": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 122, 138, 150, 152], "p_dbl": [62, 107], "p_hat": 83, "p_int": 107, "p_n": 40, "p_val": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38], "p_x": [41, 60, 86], "p_x0": 89, "p_x1": 89, "packag": [57, 58, 60, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 85, 86, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 106, 107, 108, 110, 114, 121, 122, 123, 138, 139, 140, 150, 152, 153, 154, 155], "packagedata": 86, "packagevers": 61, "page": [95, 150, 153], "pair": [57, 74], "pake": [60, 86], "paket": [60, 61, 62], "pal": 60, "palett": [13, 16, 66, 69, 72, 73], "pand": [66, 69], "panda": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 48, 49, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 99, 101, 104, 106, 108, 139, 140, 152], "pandas2ri": 86, "panel": [6, 7, 12, 14, 16, 17, 18, 19, 67, 69, 99, 100, 101, 104, 111, 112, 114, 143, 144, 153, 154], "paper": [33, 40, 62, 85, 89, 92, 94, 95, 139, 149, 150, 152, 153, 154], "par": 70, "par_grid": [62, 107], "paradox": [62, 107, 154], "parallel": [59, 64, 65, 66, 69, 73, 78, 84, 91, 108, 110, 112, 114], "param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 50, 51, 85, 107], "param_grid": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 107], "param_nam": 59, "param_set": [62, 107], "param_v": 62, "paramet": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 58, 59, 60, 61, 63, 64, 66, 67, 69, 72, 73, 75, 76, 77, 78, 79, 82, 83, 84, 86, 87, 90, 91, 92, 93, 94, 95, 96, 97, 98, 104, 105, 106, 107, 110, 111, 112, 113, 114, 115, 116, 117, 118, 121, 122, 132, 133, 138, 139, 140, 145, 147, 149, 150, 152, 153, 154, 155], "parametr": [25, 59, 95, 98, 107, 155], "params_exact": 107, "params_nam": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 59], "parenttoc": 150, "part": [43, 58, 60, 61, 62, 68, 75, 84, 85, 86, 87, 98, 107, 122, 139, 149, 154, 155], "parti": 43, "partial": [21, 37, 38, 39, 40, 41, 42, 43, 60, 62, 70, 79, 85, 86, 94, 97, 105, 107, 119, 120, 122, 134, 135, 138, 145, 146, 147, 148, 149, 150, 152, 154, 155], "partial_": [123, 138], "partiallli": 94, "particip": [10, 88, 94, 155], "particular": [108, 150], "particularli": [72, 85], "partion": [60, 86], "partit": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 60, 86, 97, 105], "partli": 155, "pass": [22, 26, 38, 46, 47, 48, 50, 51, 59, 62, 67, 85, 107, 155], "passo": [150, 152], "past": 60, "paste0": [60, 63], "pastel": 75, "path": [107, 108], "path_to_r": 79, "patsi": [76, 77, 106], "pattern": 95, "paul": 153, "pd": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 44, 48, 64, 65, 66, 67, 68, 69, 72, 73, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 106, 108], "pdf": [75, 89], "pedregosa": [150, 152], "pedregosa11a": [150, 152], "pedro": [59, 153], "penal": [63, 67, 96], "penalti": [61, 62, 67, 74, 87, 93, 95, 96, 107, 108], "pennsylvania": [11, 99, 104, 152], "pension": [61, 87, 88, 155], "peopl": [61, 87, 88], "pep8": 154, "per": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 60, 66, 67, 68, 69, 86], "percent": 107, "percentag": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39], "perf_count": 84, "perfectli": [92, 108], "perform": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 49, 58, 60, 62, 64, 66, 67, 68, 69, 75, 79, 82, 84, 85, 86, 88, 94, 95, 96, 98, 107, 108, 110, 111, 113, 122, 123, 138, 150, 152, 153, 155], "performance_result": 72, "perfrom": 82, "perhap": 155, "period": [12, 14, 16, 17, 19, 59, 64, 65, 68, 101, 104, 109, 110, 111, 112, 113, 114, 125, 127, 153, 154], "perp": [108, 121], "perrot": [150, 152], "person": 155, "pessimist": 95, "peter": 153, "petra": 154, "petronelaj": 154, "pfister": [62, 107, 150, 152], "phi": [60, 86, 106, 138], "philipp": [95, 150, 153], "philippbach": [150, 154], "pi": [30, 34, 40, 43, 106, 108, 123, 136, 137], "pi_": [41, 60, 86], "pi_0": [123, 136, 137], "pi_i": [63, 96, 108], "pick": [92, 155], "pip": [92, 108], "pip3": 151, "pipe": 62, "pipe_forest_classif": 62, "pipe_forest_regr": 62, "pipe_lasso": 62, "pipelin": [46, 47, 50, 51, 62, 87, 154], "pipeop": 62, "pira": [61, 87, 88, 94, 155], "pivot": [72, 79, 86, 153], "pivot_logloss": 72, "pivot_rmse_g0": 72, "pivot_rmse_g1": 72, "plai": [85, 155], "plan": [10, 61, 87, 88, 155], "plausibl": [95, 139], "pleas": [46, 47, 50, 51, 59, 64, 65, 67, 73, 85, 95, 122, 150, 151], "plim": 89, "pliv": [20, 21, 37, 60, 86, 97, 106, 119, 134, 150, 154], "plm": [0, 6, 7, 12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 75, 85, 86, 94, 97, 98, 105, 106, 107, 122, 138, 145, 155], "plot": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 49, 58, 59, 61, 62, 63, 65, 66, 67, 68, 69, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 87, 88, 89, 91, 92, 94, 95, 96, 106, 139, 145], "plot_data": [66, 69], "plot_effect": [13, 16, 66, 67, 68, 69], "plot_tre": [49, 90, 106], "plotli": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 76, 77, 79, 92, 95], "plr": [12, 14, 15, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 62, 85, 89, 94, 97, 107, 120, 122, 135, 138, 145, 147, 148, 149, 150, 152, 154, 155], "plr_est": 89, "plr_est1": 89, "plr_est2": 89, "plr_obj": 89, "plr_obj_1": 89, "plr_obj_2": 89, "plr_summari": 87, "plrglmnet": 61, "plrranger": 61, "plrrpart": 61, "plrxgboost8700": 61, "plt": [64, 65, 66, 69, 70, 72, 73, 75, 76, 77, 78, 80, 81, 83, 84, 85, 86, 87, 88, 89, 91, 92, 95, 96], "plt_smpl": [60, 86], "plt_smpls_cluster": [60, 86], "plug": [82, 139, 141, 142, 143, 144, 145, 146, 147], "pm": [44, 60, 86, 138, 139, 145, 149], "pmatrix": [63, 96], "pmlr": [79, 84, 85], "po": [62, 107], "poe": 153, "point": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 59, 60, 72, 80, 81, 86, 95, 106, 108, 155], "pointwis": [48, 78, 80, 81, 91], "poli": [61, 83, 86, 87], "polici": [26, 37, 38, 49, 105, 108, 119, 120, 152, 153, 154], "policy_tre": [26, 90, 106], "policy_tree_2": 90, "policy_tree_obj": 106, "policytre": 90, "polit": 89, "poly_dict": 87, "polynomi": [10, 11, 45, 61, 70, 83, 87, 92], "polynomial_featur": [10, 11, 61, 70], "polynomialfeatur": [83, 86, 87], "poor": 93, "pop": [123, 125], "popul": [95, 108, 111, 113, 123, 127], "popular": [84, 108, 139, 140], "porport": 94, "posit": [43, 61, 66, 68, 69, 72, 84, 89, 95, 155], "posixct": [62, 107], "possibl": [5, 6, 7, 8, 9, 47, 50, 51, 59, 62, 66, 68, 69, 76, 77, 80, 81, 82, 83, 84, 85, 90, 92, 93, 94, 95, 107, 108, 112, 115, 116, 138, 139, 140, 154, 155], "possibli": [93, 139, 140], "post": [16, 40, 43, 108, 110, 112, 114, 138, 153], "postdoubl": 153, "poster": 89, "potenti": [17, 19, 22, 23, 24, 27, 28, 30, 31, 35, 45, 63, 64, 66, 69, 83, 89, 92, 96, 110, 114, 115, 121, 128, 129, 138, 146, 151, 154, 155], "potential_level": [72, 73], "power": [62, 85, 93, 95, 107, 153], "pp": [59, 79, 84, 85], "pq": [27, 28, 29, 88, 133, 154], "pq_0": [88, 91], "pq_1": [88, 91], "pr": [30, 57, 60, 61, 62, 63, 107, 108, 122, 123, 138, 152, 155], "practic": [84, 95, 153], "pre": [14, 16, 17, 19, 59, 63, 64, 66, 67, 68, 69, 96, 107, 108, 110, 111, 112, 113, 123, 125, 127, 154], "precis": [59, 108, 139, 147, 155], "precomput": [47, 51], "pred": [59, 85], "pred_df": 90, "pred_dict": 107, "pred_treat": 90, "predict": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 46, 47, 48, 49, 50, 51, 52, 58, 60, 61, 62, 67, 72, 75, 78, 79, 83, 84, 85, 86, 87, 90, 93, 95, 98, 106, 122, 139, 140, 145, 147, 154, 155], "predict_proba": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 38, 44, 46, 50, 85, 93, 107], "predictor": [22, 26, 38, 48, 49, 76, 77, 80, 81, 95, 97], "prefer": [61, 87, 88, 155], "preliminari": [24, 58, 75, 92, 123, 129, 132, 133, 137], "prepar": [59, 60, 86, 154], "preprint": [93, 153], "preprocess": [61, 67, 83, 86, 87, 88, 107], "presenc": [61, 87, 88], "present": [59, 67, 95, 107, 123, 131, 155], "prespecifi": 94, "pretreat": [12, 14, 15, 16, 59, 64], "prettenhof": [150, 152], "preval": 95, "prevent": [122, 154], "previou": [65, 82, 83, 89, 151, 155], "previous": [67, 93, 107, 155], "price": [60, 86], "priliminari": [27, 29], "primari": [72, 73], "principl": [139, 140], "print": [14, 16, 52, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 139, 145, 151, 152, 154, 155], "print_detail": 59, "print_period": [14, 16], "prior": [72, 84, 108, 121], "privat": 154, "prob": 62, "prob_dist": 93, "prob_dist_": 93, "probabilit": 82, "probabl": [17, 18, 19, 22, 24, 25, 26, 27, 28, 29, 30, 35, 50, 58, 59, 63, 64, 66, 69, 73, 75, 82, 89, 91, 92, 93, 95, 96, 98, 108, 123, 124, 125, 126, 127, 132, 153], "problem": [61, 67, 87, 88, 106, 107], "procedur": [58, 60, 61, 75, 84, 86, 87, 94, 95, 107, 138, 151, 154], "proceed": [40, 153], "process": [14, 16, 17, 18, 19, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 39, 40, 41, 42, 43, 45, 53, 59, 63, 64, 65, 68, 69, 76, 77, 78, 79, 80, 81, 84, 85, 90, 91, 95, 96, 105, 138, 139, 140, 153, 154], "processor": [52, 53], "produc": 89, "product": [76, 77, 79, 84, 95, 139, 149], "producton": 60, "program": [34, 61, 87, 88, 153, 155], "progress": 71, "project": [62, 76, 77, 106, 150, 154], "project_z": [76, 77], "prone": 123, "pronounc": 92, "propens": [14, 16, 17, 19, 22, 24, 25, 26, 27, 28, 29, 30, 31, 39, 52, 53, 61, 63, 64, 66, 67, 69, 72, 82, 83, 84, 87, 88, 95, 96, 106, 108, 111, 113, 115, 123, 125, 127, 139, 146], "propensity_scor": 52, "proper": 72, "properli": [72, 85, 155], "properti": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 61, 62, 66, 67, 68, 69, 84, 87, 88, 89, 93, 94, 99, 100, 102, 103, 104, 107, 108, 139, 145, 152, 154], "proport": [94, 139, 140, 148, 149], "propos": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 60, 62, 86, 92, 139, 140, 153, 154], "provid": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 46, 47, 50, 51, 52, 53, 59, 60, 61, 62, 67, 72, 76, 77, 80, 81, 83, 85, 86, 87, 92, 93, 95, 97, 98, 99, 101, 104, 105, 107, 138, 150, 152, 154, 155], "prune": [26, 49], "ps911c": 86, "ps944": 86, "ps_processor_config": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 76], "pscore1": 89, "pscore2": 89, "psi": [20, 21, 58, 59, 60, 86, 97, 108, 111, 112, 113, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 149, 152], "psi_": [138, 139, 142, 144, 145, 148, 149], "psi_a": [20, 25, 26, 37, 38, 58, 60, 75, 86, 108, 111, 112, 113, 122, 123, 124, 125, 126, 127, 128, 130, 131, 134, 135, 138], "psi_b": [20, 25, 26, 37, 38, 58, 75, 106, 108, 111, 112, 113, 122, 123, 124, 125, 126, 127, 128, 130, 131, 134, 135], "psi_el": [122, 123], "psi_j": 138, "psi_nu2": [139, 145], "psi_sigma2": [139, 145], "psprocessor": 53, "psprocessorconfig": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 52], "public": [57, 74, 154], "publish": [95, 150, 154], "pull": [61, 154], "purchas": 95, "pure": 95, "purp": [76, 77], "purpos": [58, 75, 82, 94, 95, 123, 125, 127, 139, 140, 152], "pval": 138, "px": [79, 92], "py": [65, 66, 67, 68, 69, 72, 76, 86, 87, 92, 93, 95, 150, 151, 154], "py3": 151, "py_al": 75, "py_did": 64, "py_did_pretest": 65, "py_dml": 75, "py_dml_nosplit": 75, "py_dml_po": 75, "py_dml_po_nosplit": 75, "py_double_ml_apo": 73, "py_double_ml_bas": 75, "py_double_ml_basic_iv": 74, "py_double_ml_c": 76, "py_double_ml_cate_plr": 77, "py_double_ml_cvar": 78, "py_double_ml_firststag": 79, "py_double_ml_g": 80, "py_double_ml_gate_plr": 81, "py_double_ml_gate_sensit": 82, "py_double_ml_irm_vs_apo": 83, "py_double_ml_learn": 84, "py_double_ml_meets_flaml": 85, "py_double_ml_multiway_clust": 86, "py_double_ml_pens": 87, "py_double_ml_pension_qt": 88, "py_double_ml_plm_irm_hetfx": 89, "py_double_ml_policy_tre": 90, "py_double_ml_pq": 91, "py_double_ml_rdflex": 92, "py_double_ml_robust_iv": 93, "py_double_ml_sensit": 94, "py_double_ml_sensitivity_book": 95, "py_double_ml_ssm": 96, "py_non_orthogon": 75, "py_panel": 66, "py_panel_data_exampl": 67, "py_panel_simpl": 68, "py_po_al": 75, "py_rep_c": 69, "py_tabpfn": 72, "pypi": [153, 154], "pyplot": [64, 65, 66, 69, 70, 72, 73, 75, 76, 77, 78, 80, 81, 83, 84, 85, 86, 87, 88, 89, 91, 92, 95, 96], "pyproject": 154, "pyreadr": 67, "python": [43, 59, 85, 92, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 108, 109, 110, 111, 113, 122, 123, 138, 139, 140, 145, 150, 152, 153, 154, 155], "python3": [65, 66, 67, 68, 69, 86, 87, 92, 95, 151], "pytorch": 72, "q": [62, 78, 91, 92, 107, 150, 152], "q2": [62, 70, 99, 104, 152], "q3": [62, 70, 99, 104, 152], "q4": [62, 70, 99, 104, 152], "q5": [62, 70, 99, 104, 152], "q6": [62, 70, 99, 104, 152], "q_i": [92, 108], "qquad": 34, "qte": [78, 88, 154], "quad": [17, 18, 19, 61, 63, 64, 87, 90, 92, 96, 106, 108, 110, 114, 121, 123, 132, 138, 139, 141, 142], "quadrat": [63, 96], "qualiti": [94, 97, 154], "quanitl": 88, "quant": 78, "quantifi": [95, 108, 114], "quantil": [16, 23, 24, 27, 28, 29, 35, 66, 69, 73, 78, 83, 94, 105, 107, 129, 132, 133, 153, 154], "quantiti": [57, 74, 95], "queri": 87, "question": [95, 155], "quick": 88, "quit": [72, 84, 90, 94, 139, 140], "r": [25, 33, 46, 47, 50, 51, 65, 66, 67, 68, 69, 75, 76, 77, 79, 86, 89, 92, 93, 95, 97, 98, 99, 104, 105, 108, 117, 122, 123, 130, 134, 138, 139, 140, 146, 147, 148, 149, 150, 152, 153, 154, 155], "r2_d": [34, 84], "r2_score": [47, 51], "r2_y": [34, 84], "r6": [62, 154], "r_0": [25, 37, 61, 87, 108, 117], "r_all": 58, "r_d": 34, "r_df": 86, "r_dml": 58, "r_dml_nosplit": 58, "r_dml_po": 58, "r_dml_po_nosplit": 58, "r_double_ml_bas": 58, "r_double_ml_basic_iv": 57, "r_double_ml_did": 59, "r_double_ml_multiway_clust": 60, "r_double_ml_pens": 61, "r_double_ml_pipelin": 62, "r_double_ml_ssm": 63, "r_hat": 37, "r_hat0": 25, "r_hat1": 25, "r_non_orthogon": 58, "r_po_al": 58, "r_y": 34, "rais": [5, 6, 7, 8, 9, 46, 47, 50, 51, 66, 67, 68, 69, 107], "randint": 89, "randn": 30, "random": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 36, 37, 38, 43, 44, 45, 57, 58, 59, 61, 62, 64, 65, 66, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 101, 104, 106, 107, 109, 110, 111, 113, 114, 122, 136, 138, 139, 145, 149, 152, 153, 155], "random_search": 107, "random_st": [35, 66, 69, 75, 82, 83, 90], "randomforest": [61, 72, 84, 87], "randomforest_class": [61, 76, 87, 90], "randomforest_reg": [76, 90], "randomforestclassifi": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 70, 72, 73, 76, 77, 80, 81, 82, 84, 87, 90, 92, 94, 95, 106, 107, 108, 109, 110, 111, 113, 155], "randomforestregressor": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 70, 72, 73, 75, 76, 77, 80, 81, 82, 84, 87, 90, 92, 94, 95, 97, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 139, 145, 152, 155], "randomized_search": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 107], "randomizedsearchcv": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 107], "randomli": [58, 60, 75, 86, 98, 122, 155], "rang": [8, 58, 64, 65, 72, 73, 75, 78, 80, 81, 84, 85, 86, 88, 90, 91, 92, 93, 95, 96, 98, 107, 108], "rangeindex": [66, 67, 68, 69, 70, 72, 73, 82, 86, 87, 88, 94, 99, 101, 104, 152], "ranger": [59, 61, 62, 97, 107, 108, 122, 123, 138, 152, 155], "rangl": [32, 90], "rank": 154, "rate": [79, 84, 108], "rather": [92, 95, 108], "ratio": [107, 122, 139, 140], "rational": 67, "ravel": [76, 77], "raw": [52, 61, 67, 68, 79, 87], "raw_res_manual_lasso_r_100_n_100_p_200_rho_0": 79, "rbind": 61, "rbindlist": 61, "rbinom": 57, "rbrace": [25, 26, 33, 34, 36, 60, 86, 97, 108, 115, 117, 118, 122, 123, 128, 138, 139, 146], "rcolorbrew": 60, "rcparam": [65, 70, 76, 77, 78, 80, 81, 83, 86, 87, 88, 91], "rd": [108, 154], "rda": 67, "rdbu": 60, "rdbu_r": 86, "rdbwselect": 108, "rdd": [0, 8, 102, 104, 105, 151], "rdflex": [92, 108, 154], "rdflex_fuzzi": 92, "rdflex_fuzzy_stack": 92, "rdflex_obj": [44, 108], "rdflex_sharp": 92, "rdflex_sharp_stack": 92, "rdrobust": [44, 92, 108, 151, 154], "rdrobust_fuzzi": 92, "rdrobust_fuzzy_noadj": 92, "rdrobust_sharp": 92, "rdrobust_sharp_noadj": 92, "rdt044": 79, "re": [69, 72, 86, 95, 151], "read": [67, 151], "read_csv": [68, 79], "read_r": 67, "readabl": [72, 154], "reader": [67, 93], "readili": 150, "real": [61, 87, 88, 94, 139, 140], "realat": 108, "realiz": [92, 108, 121], "reason": [5, 6, 7, 8, 9, 57, 72, 74, 79, 84, 85, 94, 95, 139, 140, 155], "recal": [70, 139, 149], "receiv": [17, 19, 66, 69, 73, 92, 108, 110, 112], "recent": [85, 108, 110, 114, 153], "recogn": [61, 87, 88], "recommend": [62, 66, 69, 72, 84, 92, 95, 97, 108, 122, 139, 151, 153, 154], "recov": [57, 59, 74, 89], "recreat": 72, "recsi": 153, "red": [60, 63, 72, 80, 81, 85, 86], "reduc": [61, 82, 85, 87, 92, 94, 95, 108, 154], "redund": 154, "reemploy": [11, 99, 104, 152], "ref": 67, "refactor": 154, "refer": [10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 61, 65, 66, 69, 72, 73, 82, 87, 88, 92, 94, 99, 104, 105, 106, 108, 109, 111, 113, 114, 123, 139, 140, 145, 153, 154], "reference_level": [23, 72, 73, 83, 108], "refin": 154, "refit": [139, 140], "reflect": [90, 95, 106], "reg": [17, 18, 19, 61, 87, 155], "reg_estim": 92, "reg_learn": 88, "reg_learner_1": 84, "reg_learner_2": 84, "regard": [95, 150], "regener": 154, "region": [60, 78, 86, 138, 153], "regr": [57, 58, 59, 60, 61, 62, 63, 97, 107, 108, 122, 123, 125, 138, 152, 155], "regravg": [62, 107], "regress": [8, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 48, 53, 57, 59, 60, 62, 63, 64, 66, 67, 68, 69, 72, 73, 74, 79, 85, 86, 89, 93, 94, 95, 96, 97, 98, 102, 104, 105, 106, 107, 110, 111, 113, 117, 118, 119, 120, 122, 127, 138, 140, 145, 146, 147, 148, 149, 150, 152, 153, 154, 155], "regressor": [47, 51, 58, 61, 72, 73, 75, 78, 84, 85, 87, 93, 98], "regular": [40, 105, 107, 123, 138, 153], "reich": [62, 107], "reinforc": 153, "reject": [61, 87], "rel": [61, 67, 72, 87, 93, 108, 109, 139, 140, 146, 147], "relat": [83, 95, 155], "relationship": [57, 72, 74, 79, 95, 138], "relev": [12, 14, 15, 16, 32, 46, 47, 48, 50, 51, 66, 69, 78, 90, 91, 99, 104, 108, 139, 155], "reli": [16, 64, 65, 66, 69, 76, 77, 82, 83, 106, 107, 108, 110, 123, 127, 139, 140, 155], "reload": 61, "remain": [59, 99, 104, 138, 155], "remark": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 58, 64, 65, 66, 67, 68, 69, 73, 75, 76, 77, 78, 80, 81, 82, 83, 84, 88, 94, 106, 107, 108, 109, 110, 111, 113, 122, 123, 124, 125, 126, 127, 132, 133, 138, 139, 142, 144, 147], "remot": 151, "remov": [4, 14, 16, 22, 24, 25, 26, 27, 28, 29, 30, 61, 65, 76, 83, 95, 99, 104, 105, 108, 122, 123, 139, 154], "renam": [66, 69, 87, 154], "render": [94, 95], "reorgan": 154, "rep": [58, 63, 98, 107, 138], "repeat": [12, 14, 15, 16, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 57, 58, 60, 61, 62, 63, 66, 67, 68, 75, 82, 86, 87, 88, 89, 92, 94, 96, 98, 100, 104, 105, 107, 111, 112, 113, 114, 138, 141, 142, 152, 154, 155], "repeatedkfold": 86, "repet": 94, "repetit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 48, 67, 76, 77, 79, 80, 81, 82, 84, 105, 107, 138, 152, 154, 155], "replac": [90, 95, 154], "replic": [10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 58, 61, 67, 68, 75, 79, 93, 95], "repo": 154, "report": [61, 85, 87, 150, 154], "repositori": [66, 69, 79, 92, 154], "repr": [58, 60], "repres": [17, 19, 72, 89, 95, 108], "represent": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 69, 72, 94, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 152, 154], "reproduc": 35, "request": [46, 47, 50, 51, 154], "requir": [37, 38, 46, 50, 53, 57, 61, 62, 66, 67, 68, 69, 72, 73, 82, 87, 88, 94, 99, 104, 108, 109, 111, 112, 113, 123, 125, 127, 138, 139, 140, 145, 151, 154, 155], "requirenamespac": 59, "rerun": 67, "res_df": 86, "res_dict": [31, 32, 35, 39, 45], "resampl": [57, 60, 62, 63, 64, 66, 67, 68, 69, 86, 88, 94, 96, 107, 108, 110, 111, 113, 122, 123, 138, 150, 152, 155], "resdat": 69, "research": [60, 62, 86, 89, 95, 122, 150, 152, 153, 155], "resembl": [63, 96], "reset": 59, "reset_index": [66, 69, 79, 86, 87], "reshap": [65, 75, 76, 77, 83, 99, 104], "reshape2": 60, "residu": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 47, 51, 94, 139, 140, 148, 149], "resolut": [62, 107], "resourc": 84, "resourcewis": 84, "respect": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 61, 66, 67, 68, 69, 73, 87, 88, 92, 106, 108, 112, 117, 121, 122, 139, 149, 155], "respons": [10, 62, 107], "rest": 108, "restart": 151, "restrict": 84, "restructur": 154, "restud": 79, "result": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 57, 58, 59, 62, 63, 64, 65, 66, 67, 68, 69, 72, 73, 75, 76, 77, 79, 82, 83, 84, 90, 92, 93, 94, 95, 96, 98, 107, 122, 123, 124, 125, 126, 127, 139, 140, 145, 152, 154], "result_iivm": 61, "result_irm": 61, "result_plr": 61, "results_df": 93, "retain": [46, 47, 50, 51], "retina": 89, "retir": [61, 87, 88, 94], "return": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 58, 59, 60, 62, 63, 65, 66, 67, 68, 69, 75, 78, 84, 85, 86, 89, 90, 91, 93, 94, 95, 96, 97, 107, 123, 139, 140, 154], "return_count": [72, 73, 84], "return_tune_r": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "return_typ": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 18, 22, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 40, 41, 42, 43, 58, 61, 62, 63, 64, 75, 83, 84, 85, 87, 88, 94, 96, 97, 98, 99, 100, 102, 103, 104, 106, 107, 108, 110, 122, 123, 138, 139, 145, 152, 155], "rev": 60, "reveal": 82, "review": [40, 79, 153], "revist": [60, 86], "reweight": [108, 109], "rf": 92, "rho": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 54, 66, 69, 73, 82, 83, 92, 94, 95, 139, 140, 145, 149, 155], "rho_val": 95, "richter": [62, 107, 150, 152], "riesz": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 69, 72, 94, 139, 140, 141, 142, 143, 144, 145, 148, 149], "riesz_rep": [139, 145], "right": [17, 19, 33, 34, 36, 40, 41, 58, 60, 75, 84, 86, 87, 88, 89, 91, 92, 95, 98, 108, 123, 124, 125, 126, 127, 138, 139, 141, 142, 143, 144, 146, 147], "rightarrow_": [58, 75, 98], "risk": [24, 105, 154], "ritov": 153, "rival": 86, "rival_ind": 86, "rmd": 59, "rmse": [59, 64, 66, 67, 68, 69, 72, 84, 85, 94, 96, 107, 108, 110, 111, 113, 123, 138, 152, 154], "rmse_dml_ml_l_fullsampl": 85, "rmse_dml_ml_l_lesstim": 85, "rmse_dml_ml_l_onfold": 85, "rmse_dml_ml_l_untun": 85, "rmse_dml_ml_m_fullsampl": 85, "rmse_dml_ml_m_lesstim": 85, "rmse_dml_ml_m_onfold": 85, "rmse_dml_ml_m_untun": 85, "rmse_g0": 72, "rmse_g1": 72, "rmse_oos_ml_l": 85, "rmse_oos_ml_m": 85, "rmse_oos_onfolds_ml_l": 85, "rmse_oos_onfolds_ml_m": 85, "rnorm": [57, 62, 99, 104, 107, 138, 152], "robin": [10, 11, 42, 60, 79, 86, 98, 150, 153], "robinson": [58, 75, 98], "robject": 86, "robu": [80, 81], "robust": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 41, 59, 66, 67, 69, 73, 82, 83, 92, 94, 95, 99, 104, 108, 139, 145, 153, 154, 155], "robust_confset": [25, 93, 154], "robust_cov": 93, "robust_length": 93, "roc\u00edo": 153, "role": [4, 5, 6, 7, 8, 9, 58, 75, 85, 98, 102, 104, 155], "romano": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 138], "root": [12, 14, 15, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 72, 79, 98, 107, 123, 153], "rotat": [85, 92], "roth": [92, 108, 110, 114, 153], "rough": [95, 155], "roughli": [66, 69, 95], "round": [52, 61, 67, 72, 73, 83, 84, 89, 95], "rout": [46, 47, 50, 51], "row": [13, 16, 58, 61, 65, 67, 70, 76, 77, 85, 86, 90, 99, 104, 108, 111, 113, 122, 152, 155], "rownam": 60, "rowv": 60, "roxygen2": 154, "royal": [95, 153], "rpart": [61, 62, 107], "rpart_cv": 62, "rprocess": 84, "rpy2": 86, "rpy2pi": 86, "rskf": 83, "rsmp": [62, 107, 122], "rsmp_tune": [62, 107], "rssb": 95, "rtype": 23, "ruben": 153, "ruiz": [57, 74], "rule": [59, 106], "run": [8, 59, 67, 72, 92, 102, 104, 108, 151, 154], "runif": 57, "runner": [65, 66, 68, 69, 93, 95], "runtime_learn": 62, "runtimewarn": 69, "rv": [66, 69, 73, 82, 83, 94, 95, 139, 145, 155], "rva": [66, 69, 73, 82, 83, 94, 95, 139, 145, 155], "rvert": 79, "rvert_": 79, "s1": 85, "s2": 85, "s_": [41, 60, 86, 108, 121], "s_1": 42, "s_2": 42, "s_col": [4, 9, 63, 92, 96, 103, 104, 108], "s_i": [36, 63, 92, 96, 108], "s_x": [41, 60, 86], "safeguard": [64, 107], "sake": [61, 87, 95, 155], "same": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 35, 37, 38, 48, 58, 60, 63, 66, 67, 69, 75, 76, 77, 82, 83, 84, 86, 88, 90, 92, 93, 94, 95, 96, 107, 108, 111, 113, 123, 126, 127, 138, 139, 147, 154], "samii": 89, "sampl": [9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 36, 37, 38, 39, 41, 44, 46, 47, 50, 51, 57, 59, 60, 62, 64, 66, 67, 68, 69, 74, 80, 81, 83, 84, 86, 88, 90, 93, 94, 103, 104, 105, 107, 110, 111, 113, 114, 121, 124, 138, 139, 142, 144, 152, 153, 154], "sample_weight": [44, 46, 47, 50, 51, 92], "sant": [12, 14, 15, 17, 18, 19, 31, 35, 39, 59, 64, 66, 67, 68, 69, 108, 109, 110, 112, 114, 153], "sara": 153, "sasaki": [41, 60, 86, 153], "satisfi": [63, 67, 96, 107, 123, 138], "save": [58, 61, 67, 75, 80, 81, 84, 85, 87, 88, 107, 139, 145, 155], "savefig": 75, "saveguard": 84, "saver": [61, 87, 88], "sc": [66, 69], "scalar": 108, "scale": [17, 19, 58, 60, 65, 78, 83, 89, 91, 95, 138, 139, 142, 144, 149], "scale_color_manu": 58, "scale_fill_manu": [58, 60], "scaled_psi": 83, "scatter": [65, 72, 73, 80, 81, 89, 92, 95], "scatterplot": [72, 73], "scenario": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 66, 69, 73, 82, 83, 94, 95, 108, 139, 145, 155], "scene": [76, 77, 79], "scene_camera": 79, "schacht": [79, 84, 85], "schaefer": 89, "schedul": [99, 104, 154], "scheme": [60, 86, 107, 108, 109, 122, 150], "schneider": 62, "schratz": [62, 107, 150, 152], "scienc": [43, 57, 74, 89, 153], "scikit": [67, 84, 87, 107, 150, 152, 154, 155], "scipi": 75, "score": [0, 8, 12, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 44, 45, 46, 47, 50, 51, 52, 53, 57, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 72, 76, 77, 78, 79, 82, 83, 84, 85, 86, 87, 88, 91, 92, 93, 94, 95, 96, 97, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 122, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 147, 148, 149, 150, 154, 155], "score_col": [8, 92, 102, 104, 108], "scoring_method": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "script": 151, "sd": 57, "se": [58, 60, 75, 94, 98, 107, 122, 138, 139, 145, 153, 155], "se_aggr": 107, "se_df": 60, "se_dml": [58, 75, 98], "se_dml_po": [58, 75, 98], "se_nonorth": [58, 75], "se_orth_nosplit": [58, 75], "se_orth_po_nosplit": [58, 75], "seaborn": [13, 16, 64, 66, 69, 70, 72, 73, 75, 84, 86, 87, 88, 95, 96], "seamlessli": 72, "search": [12, 14, 15, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 107, 123], "search_mod": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 107], "searchabl": 61, "second": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 41, 58, 60, 62, 75, 84, 85, 86, 97, 98, 122, 138, 139, 140, 149, 152], "secondari": [72, 73], "section": [15, 16, 18, 19, 59, 60, 61, 62, 65, 82, 85, 86, 88, 95, 100, 104, 109, 111, 112, 113, 114, 131, 141, 142, 154], "secur": 89, "see": [10, 11, 12, 14, 15, 19, 20, 21, 22, 24, 25, 26, 27, 28, 30, 34, 36, 37, 38, 46, 47, 48, 50, 51, 57, 59, 60, 61, 62, 64, 66, 67, 68, 69, 72, 73, 74, 76, 77, 83, 85, 86, 88, 89, 90, 92, 93, 94, 95, 107, 108, 110, 112, 114, 122, 123, 129, 131, 132, 133, 136, 137, 139, 140, 142, 144, 145, 149, 151, 152, 154], "seed": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 44, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 70, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 104, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 139, 145, 152, 155], "seek": 89, "seem": [59, 61, 82, 87, 88, 139, 155], "seen": [80, 81, 83], "sel_cols_chiang": 86, "select": [9, 17, 19, 30, 35, 36, 40, 57, 72, 79, 84, 92, 95, 97, 99, 103, 104, 105, 107, 121, 138, 152, 153, 154, 155], "selected_coef": 84, "selected_featur": [62, 107], "selected_learn": 84, "self": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 46, 47, 48, 49, 50, 51, 84, 85, 93, 155], "selfref": 61, "semenova": [76, 77, 153], "semi": 98, "semiparametr": 10, "sens": [94, 95], "sensemakr": [139, 140], "sensit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 54, 105, 106, 108, 113, 140, 145, 149, 154], "sensitivity_analysi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 66, 69, 73, 82, 83, 94, 95, 139, 145, 155], "sensitivity_benchmark": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 73, 82, 94, 95, 139, 140], "sensitivity_el": [139, 145], "sensitivity_param": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 94, 95, 139, 140, 145], "sensitivity_plot": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 73, 82, 94, 95, 139, 145], "sensitivity_summari": [66, 69, 73, 82, 83, 94, 95, 139, 145, 155], "sensitv": 83, "sensitvity_benchmark": 73, "sensiv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38], "senstiv": [139, 148], "sep": 58, "separ": [13, 72, 89, 94, 102, 103, 104, 107, 108, 122, 154], "seper": [85, 92, 94, 138, 139, 140], "seq_len": [58, 63, 98], "sequenc": 86, "sequenti": [11, 107], "ser": [66, 67, 68, 69], "seri": [66, 67, 68, 69, 95, 153], "serv": [17, 19, 99, 101, 104, 152, 154], "serverless": [153, 154], "servic": 89, "set": [4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 41, 42, 43, 46, 49, 50, 51, 53, 57, 58, 59, 60, 61, 63, 64, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 104, 106, 108, 109, 111, 112, 114, 122, 123, 124, 125, 126, 127, 128, 131, 138, 139, 140, 146, 147, 148, 151, 152, 154, 155], "set_as_param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "set_config": [46, 47, 50, 51], "set_fit_request": [50, 51], "set_fold_specif": 107, "set_index": 87, "set_ml_nuisance_param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 61, 70, 87, 107, 154], "set_param": [46, 47, 50, 51, 85, 107], "set_sample_split": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 83, 84, 122, 154], "set_score_request": [46, 47, 50, 51], "set_styl": [87, 88], "set_text": 84, "set_threshold": [58, 59, 60, 61, 62, 63, 97, 107, 108, 122, 123, 138, 152], "set_tick": 86, "set_ticklabel": 86, "set_titl": [72, 73, 83, 85, 86, 92], "set_x_d": [4, 5, 6, 7, 8, 9], "set_xlabel": [72, 73, 75, 83, 85, 86, 92], "set_xlim": 75, "set_xtick": 89, "set_xticklabel": 89, "set_ylabel": [72, 73, 83, 85, 86, 89, 92], "set_ylim": [78, 83, 85, 86, 91], "setdiff": 154, "setdiff1d": 86, "setminu": [60, 86, 138], "settings_l": 85, "settings_m": 85, "setup": [151, 154], "seven": [60, 86], "sever": [54, 61, 62, 66, 67, 68, 69, 72, 84, 85, 87, 88, 94, 95, 98, 107, 155], "shape": [8, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 48, 49, 50, 51, 65, 66, 69, 72, 73, 76, 77, 80, 81, 84, 86, 87, 90, 92, 94, 95, 107, 108], "share": [60, 61, 86, 87], "sharma": [95, 153], "sharp": 44, "shift": [66, 69], "shock": [60, 86], "short": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 94, 95, 108, 112, 139, 140, 153, 154, 155], "shortcut": 61, "shortli": [60, 62, 86, 107], "shota": 153, "should": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 46, 47, 50, 51, 61, 63, 66, 67, 69, 73, 80, 81, 84, 87, 92, 93, 94, 96, 99, 104, 106, 107, 108, 109, 116, 138, 139, 140, 150], "show": [57, 58, 60, 63, 64, 66, 69, 70, 72, 73, 74, 75, 76, 77, 79, 82, 83, 84, 85, 86, 89, 92, 93, 95, 96, 98, 139, 148, 151], "showcas": 90, "showlabel": 95, "showlegend": 95, "shown": [57, 74, 89, 152], "showscal": [76, 77, 79], "shrink": 92, "shuffl": 122, "side": [92, 108, 139, 145], "sigma": [17, 18, 19, 30, 31, 33, 34, 35, 36, 39, 40, 41, 42, 43, 58, 60, 63, 75, 86, 96, 98, 106, 122, 138, 139, 140, 142, 144, 145, 148, 149], "sigma2": [107, 139, 145], "sigma_": [17, 18, 19, 33, 34, 36, 39, 40, 41, 42, 58, 60, 75, 86, 98], "sigma_0": [139, 149], "sigma_j": 138, "sigmoid": 89, "sign": [93, 95], "signal": [48, 49], "signatur": [25, 26, 27, 28, 29, 37, 38, 123], "signif": [57, 59, 60, 61, 62, 63, 107, 108, 122, 123, 138, 152, 155], "signific": [57, 60, 61, 62, 63, 66, 69, 73, 82, 83, 87, 90, 92, 94, 95, 107, 108, 122, 123, 138, 139, 145, 152, 155], "significantli": 72, "silverman": [27, 28, 29], "sim": [17, 18, 19, 31, 32, 33, 34, 35, 36, 39, 40, 41, 42, 43, 45, 58, 59, 60, 63, 65, 75, 78, 86, 90, 91, 96, 98, 108], "sim_data": 68, "similar": [35, 39, 59, 62, 67, 76, 77, 82, 85, 88, 92, 93, 94, 95, 108, 123, 124, 125], "similarli": [67, 85, 93], "simpl": [32, 50, 51, 59, 62, 76, 77, 80, 81, 82, 83, 90, 95, 105, 108, 139, 140], "simplest": 106, "simpli": [62, 64, 155], "simplic": [61, 84, 87, 90, 95], "simplif": [139, 141], "simplifi": [66, 69, 83, 89, 95, 106, 139, 148], "simul": [17, 18, 19, 31, 32, 33, 34, 35, 36, 39, 40, 42, 43, 58, 62, 63, 66, 68, 69, 75, 76, 77, 78, 79, 80, 81, 84, 85, 91, 92, 95, 96, 98, 107, 138, 152], "simul_data": 30, "simulaten": [108, 116], "simulation_run": 79, "simult": 59, "simultan": [67, 105, 155], "sin": [32, 35, 43, 65, 76, 77, 80, 81], "sinc": [31, 39, 46, 50, 61, 63, 64, 65, 73, 80, 81, 82, 84, 85, 87, 89, 96, 107, 108, 110, 139, 145, 147, 151, 154], "singl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 64, 66, 67, 68, 69, 80, 81, 88, 89, 107, 138], "single_learner_pipelin": 107, "singleton": 122, "sinh": 43, "sipp": [61, 87, 88], "site": [65, 66, 67, 68, 69, 72, 86, 87, 92, 95], "situat": [60, 86], "six": [17, 19, 60], "sixth": 86, "size": [13, 16, 30, 58, 60, 61, 62, 65, 66, 67, 68, 69, 72, 75, 78, 79, 82, 84, 85, 87, 89, 90, 91, 93, 95, 97, 99, 104, 107, 108, 109, 122, 123, 138, 139, 142, 144, 152, 155], "sizeabl": 95, "skill": 153, "sklearn": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 43, 44, 46, 47, 49, 50, 51, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 92, 93, 94, 95, 96, 97, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 139, 145, 152, 155], "skotara": 95, "slide": 89, "slight": [108, 111, 113], "slightli": [12, 14, 15, 65, 66, 69, 80, 81, 82, 84, 106, 108, 112, 114, 123, 124, 125, 126, 127, 139, 140], "slow": [58, 75, 98], "slower": [58, 75, 98], "small": [32, 63, 64, 65, 72, 83, 90, 96, 108, 139, 140, 147], "smaller": [61, 64, 80, 81, 82, 85, 87, 92, 95, 108, 155], "smallest": [14, 72, 84], "smpl": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 58, 60, 75, 84, 86, 122, 123], "smpls_cluster": [60, 86], "smucler": [93, 154], "sn": [64, 66, 69, 70, 72, 73, 75, 84, 86, 87, 88, 95, 96], "so": [50, 51, 57, 61, 62, 63, 64, 66, 67, 69, 74, 85, 87, 89, 95, 96, 107, 138, 155], "social": [89, 153], "societi": [60, 86, 95, 153], "softwar": [62, 107, 150, 152, 153, 154], "solari": 154, "sole": [67, 95], "solut": [97, 106, 123], "solv": [20, 60, 86, 106, 107, 108, 111, 112, 113, 138], "solver": [87, 96, 108], "some": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 50, 51, 61, 62, 63, 64, 65, 67, 70, 84, 85, 87, 88, 92, 93, 94, 96, 106, 107, 108, 117, 151, 154], "sometim": [84, 108, 114], "sonabend": [62, 107], "sophist": 107, "sort": [13, 72, 87, 93, 108], "sort_bi": 13, "sort_valu": [72, 73], "sourc": [62, 107, 152, 154], "sourcefileload": 79, "sp": 59, "space": [60, 72, 86, 107], "spars": [79, 107, 138, 152, 153], "sparsiti": 153, "spec": 153, "special": [60, 86, 105, 108], "specialis": [102, 104], "specif": [12, 14, 15, 17, 19, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 44, 60, 61, 66, 67, 68, 69, 72, 73, 83, 84, 86, 87, 95, 99, 104, 105, 106, 107, 108, 111, 113, 122, 123, 131, 138, 145, 149, 150, 152], "specifi": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 52, 53, 57, 60, 61, 62, 63, 64, 66, 67, 68, 69, 72, 73, 74, 76, 77, 78, 80, 81, 83, 85, 86, 87, 88, 90, 91, 92, 94, 95, 96, 97, 99, 101, 104, 105, 106, 108, 128, 131, 151, 152, 154, 155], "specifii": 88, "speed": [16, 23, 29, 84], "speedup": 84, "spefici": 25, "spindler": [40, 79, 84, 85, 93, 95, 150, 153, 154], "spine": [87, 88], "spline": [76, 77, 106], "spline_basi": [76, 77, 106], "spline_grid": [76, 77], "split": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 57, 60, 62, 63, 64, 66, 67, 68, 69, 83, 84, 86, 88, 90, 94, 96, 105, 106, 107, 108, 110, 111, 113, 123, 138, 152, 154], "split_sampl": [83, 84], "sponsor": [61, 87, 88], "sprintf": 58, "sq_error": 79, "sqrt": [17, 18, 19, 31, 34, 35, 39, 58, 60, 62, 70, 75, 78, 86, 91, 98, 122, 138, 139, 140, 152], "squar": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 47, 51, 61, 72, 79, 87, 107, 108, 139, 149, 153], "squarederror": [61, 87, 155], "squeez": [64, 78, 91, 96], "src": 87, "ssm": [9, 36, 105, 121], "ssrn": 33, "stabil": 82, "stabl": [57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 108, 109, 150], "stack": [62, 107], "stackingclassifi": 92, "stackingregressor": 92, "stacklrn": 62, "stackrel": 108, "stage": [44, 67, 76, 77, 80, 81, 90, 92, 107, 108, 154, 155], "stagger": [108, 114], "stai": [108, 114], "standard": [16, 18, 59, 62, 66, 67, 68, 69, 78, 80, 81, 92, 93, 100, 102, 104, 108, 109, 111, 112, 113, 122, 123, 138, 139, 145, 149, 154, 155], "standard_norm": [99, 104, 107, 138, 152], "standardscal": 87, "star": 108, "start": [17, 19, 59, 61, 62, 66, 67, 68, 69, 72, 76, 77, 79, 82, 84, 85, 86, 87, 91, 95, 108, 110, 150, 155], "start_dat": [17, 19], "stat": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 75, 92, 99, 104, 107, 108, 138, 150, 153], "stat_bin": 58, "stat_dens": 61, "state": 155, "stationar": 64, "stationari": [108, 110], "statist": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 36, 37, 38, 41, 54, 60, 86, 93, 94, 95, 138, 139, 145, 150, 152, 153, 154, 155], "statsmodel": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 48, 92], "statu": [59, 61, 63, 64, 87, 89, 92, 96, 108, 114], "std": [12, 15, 16, 24, 25, 26, 27, 28, 29, 30, 37, 38, 57, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 72, 73, 74, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 91, 94, 95, 96, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 152, 155], "stefan": 153, "step": [19, 58, 61, 62, 75, 80, 81, 82, 87, 90, 98, 107, 108, 138, 150, 155], "stepdown": 138, "stick": [61, 87], "still": [63, 64, 76, 77, 80, 81, 82, 88, 92, 94, 96, 107, 108, 112], "stochast": [37, 38, 108, 119, 120, 152], "stock": [61, 87, 88, 93], "store": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 72, 93, 97, 107, 122, 123, 138, 139, 145, 154], "store_model": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 85], "store_predict": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 59, 87, 90], "stori": [95, 153], "str": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 44, 46, 47, 48, 50, 51, 52, 53, 61, 66, 69, 72, 73, 80, 81, 91, 92, 93, 106, 108, 154], "straightforward": [66, 69, 80, 81, 84, 106], "strategi": [52, 89, 95, 108, 155], "stratifi": [83, 84], "stratum": 89, "strength": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 93, 94, 95, 139, 140, 145, 148], "strftime": 66, "strictli": 108, "string": [12, 13, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 37, 38, 106, 138, 139, 145, 152, 154], "string_label": 89, "strong": [63, 93, 96, 139, 140], "stronger": [93, 108, 112, 138, 155], "structur": [10, 11, 16, 17, 19, 42, 60, 61, 63, 67, 79, 86, 87, 93, 96, 98, 107, 150, 153, 155], "student": 153, "studi": [36, 60, 61, 79, 84, 85, 86, 87, 88, 93, 94, 108, 109, 152, 155], "style": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 66, 69, 85, 154], "styler": 154, "styliz": 95, "sub": [46, 47, 50, 51, 60, 86], "subclass": [101, 104, 154], "subfold": 107, "subgroup": [25, 61, 87, 154], "subject": [60, 86], "submiss": 154, "submit": [66, 69], "submodul": 154, "subobject": [46, 47, 50, 51], "subplot": [60, 65, 72, 73, 75, 76, 77, 78, 80, 81, 83, 84, 85, 86, 87, 88, 89, 91, 92], "subplots_adjust": 84, "subpopul": [108, 121], "subsampl": [62, 84, 123, 125], "subscript": [108, 112, 123, 125, 127, 139, 140], "subsequ": [60, 72, 86], "subset": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 46, 50, 60, 84, 86, 90, 97, 106, 107, 139, 140, 142, 144], "subseteq": 106, "substanti": [61, 87, 89], "substract": 138, "subtract": 138, "sudo": 151, "suffic": 95, "suffici": [84, 85, 95], "suggest": [60, 61, 86, 87, 95, 154], "suitabl": [63, 76, 77, 96, 108, 112], "sum": [47, 51, 60, 61, 86, 87, 88, 91, 92, 106, 138], "sum_": [17, 19, 45, 58, 60, 75, 86, 92, 97, 98, 106, 108, 109, 114, 138], "sum_i": 89, "sum_oth": 86, "sum_riv": 86, "summar": [13, 59, 66, 67, 68, 69, 72, 73, 89, 95, 97, 139, 145], "summari": [12, 15, 16, 24, 25, 26, 27, 28, 29, 30, 37, 38, 57, 59, 60, 62, 63, 64, 66, 67, 68, 69, 70, 73, 74, 76, 77, 78, 80, 81, 82, 83, 86, 88, 91, 93, 94, 95, 96, 98, 99, 101, 104, 106, 107, 108, 109, 110, 111, 113, 122, 123, 138, 139, 152, 154, 155], "summary_df": 93, "summary_result": 61, "summary_stat": 72, "superior": 72, "suppli": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 76, 77, 80, 81, 82, 90, 99, 104, 106, 139, 140, 145, 146], "support": [25, 32, 44, 53, 59, 60, 66, 67, 68, 69, 84, 86, 90, 92, 103, 104, 107, 108, 117, 155], "support_s": [32, 76, 77, 80, 81, 90], "support_t": 90, "support_w": 90, "suppos": 95, "suppress": [59, 61, 62, 63], "suppresswarn": 58, "suprema": 138, "suptitl": [78, 84, 85, 88, 91], "supxlabel": [78, 88, 91], "supylabel": [78, 88, 91], "sure": [72, 73, 107, 154], "surfac": [76, 77, 79], "surgic": 93, "surpress": [60, 152], "survei": [61, 87, 88, 155], "susan": 153, "sven": [95, 150, 153], "svenklaassen": [150, 154], "svg": [58, 75], "switch": [58, 75, 95, 98], "symbol": 95, "symmetr": 43, "syntax": [92, 108], "syntaxwarn": 86, "synthesi": 153, "synthet": [17, 19, 32, 45, 57, 72, 74, 76, 77, 78, 80, 81, 85, 90, 91, 93], "syrgkani": [93, 95, 153], "system": 153, "szita": 153, "t": [4, 5, 7, 12, 13, 15, 16, 17, 18, 19, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 46, 47, 50, 51, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 100, 101, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 122, 123, 124, 125, 138, 139, 141, 142, 152, 155], "t_": [16, 66, 67, 68, 69, 108, 111, 112, 113, 123, 125, 127], "t_1_start": 84, "t_1_stop": 84, "t_2_start": 84, "t_2_stop": 84, "t_3_start": 84, "t_3_stop": 84, "t_col": [4, 5, 7, 15, 16, 66, 67, 68, 69, 100, 101, 104, 108, 109, 110, 111, 113], "t_df": 90, "t_diff": 65, "t_dml": 58, "t_g": [17, 19], "t_i": [64, 90, 92, 108, 110, 111, 114, 123, 125], "t_idx": 65, "t_nonorth": 58, "t_orth_nosplit": 58, "t_sigmoid": 90, "t_stat": 138, "t_value_ev": 14, "t_value_pr": 14, "tabl": [58, 60, 61, 62, 63, 72, 73, 93, 97, 99, 104, 107, 108, 122, 123, 138, 152, 155], "tabpfnclassifi": 72, "tabpfnregressor": 72, "tabular": [72, 84, 99, 104, 138, 152, 155], "taddi": 153, "tailor": [100, 104], "takatsu": 93, "take": [25, 26, 31, 32, 37, 38, 39, 63, 64, 65, 66, 67, 68, 69, 76, 77, 78, 79, 80, 81, 84, 88, 91, 92, 93, 94, 96, 97, 106, 107, 108, 109, 114, 117, 118, 119, 120, 123, 128, 131, 139, 146, 147, 148, 152], "taken": [61, 87, 88, 155], "taker": [25, 154], "talk": 155, "target": [12, 14, 15, 20, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 50, 51, 57, 60, 61, 62, 63, 76, 77, 84, 86, 106, 107, 108, 110, 111, 112, 113, 114, 115, 116, 117, 118, 121, 122, 123, 132, 133, 138, 139, 147, 149, 150, 152, 154, 155], "task": [57, 72, 85, 99, 104, 122, 155], "task_typ": 154, "tau": [45, 65, 78, 88, 89, 91, 92, 106, 108, 123, 129, 132, 133], "tau_": [89, 92, 108], "tau_0": [92, 108], "tau_1": 89, "tau_2": 89, "tau_vec": [78, 88, 91], "tax": [61, 87, 88], "te": [59, 76, 77, 90], "techniqu": [58, 75, 98, 122, 155], "teen": 67, "templat": 154, "ten": 85, "tend": [61, 87, 88, 108], "tensor": [76, 77], "tenth": 153, "term": [7, 14, 58, 60, 61, 62, 65, 75, 79, 86, 87, 89, 95, 98, 108, 114, 150, 155], "termin": [62, 107], "terminatorev": 62, "test": [11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 37, 38, 46, 47, 50, 51, 57, 58, 59, 60, 61, 62, 63, 66, 67, 68, 69, 75, 82, 86, 93, 95, 98, 107, 108, 122, 123, 138, 152, 153, 154, 155], "test_id": [60, 122], "test_ind": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38], "test_set": 122, "test_siz": 75, "text": [16, 17, 18, 19, 31, 33, 35, 39, 44, 45, 60, 61, 66, 67, 68, 69, 72, 78, 79, 89, 90, 91, 92, 95, 106, 108, 111, 112, 113, 114, 122, 123, 125, 127, 139, 142, 144], "textbf": [97, 107, 155], "textposit": 95, "textrm": [139, 140, 146, 147, 148, 149], "tg": [62, 70, 99, 104, 152], "th": [60, 86], "than": [26, 58, 59, 61, 75, 79, 83, 84, 87, 88, 89, 92, 93, 94, 95, 98, 108, 112, 139, 145, 155], "thank": [59, 61, 62, 87, 154], "thatw": 65, "thei": [59, 61, 65, 80, 81, 87, 89, 93, 99, 104, 108, 139, 149], "them": [13, 61, 62, 76, 77, 78, 82, 85, 87, 91, 108], "theme": [60, 61], "theme_minim": [58, 61, 63], "theorem": [108, 114, 139, 149], "theoret": [84, 95, 122, 153], "theori": [106, 153], "therebi": [60, 62, 86, 155], "therefor": [66, 68, 69, 73, 89, 92, 94, 122, 123, 139, 148], "theta": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 41, 43, 58, 60, 62, 63, 64, 65, 66, 69, 72, 73, 75, 79, 82, 83, 84, 86, 92, 94, 95, 96, 97, 98, 99, 104, 106, 107, 108, 109, 111, 112, 113, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 145, 148, 149, 152, 155], "theta_": [17, 19, 66, 69, 73, 92, 95, 106, 108, 115, 116, 138, 139, 149], "theta_0": [25, 26, 32, 37, 38, 58, 60, 61, 63, 73, 75, 76, 77, 79, 80, 81, 86, 87, 95, 96, 98, 106, 108, 110, 117, 118, 119, 120, 121, 123, 132, 133, 135, 138, 139, 146, 147, 149, 152], "theta_d": 72, "theta_dml": [58, 75, 98], "theta_dml_po": [58, 75, 98], "theta_initi": 75, "theta_nonorth": [58, 75], "theta_orth_nosplit": [58, 75], "theta_orth_po_nosplit": [58, 75], "theta_resc": 58, "theta_t": 65, "thi": [4, 12, 13, 14, 15, 16, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 46, 47, 49, 50, 51, 53, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 106, 107, 108, 112, 114, 115, 116, 117, 118, 122, 123, 124, 125, 126, 127, 129, 131, 132, 133, 138, 139, 140, 145, 146, 147, 150, 151, 152, 153, 154, 155], "think": 62, "third": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 58, 67, 68, 75, 86, 98, 122], "thirion": [150, 152], "this_df": [79, 87], "this_split_ind": 86, "those": [59, 61, 67, 87, 88, 93], "though": [57, 74, 89], "thread": [89, 107], "three": [60, 62, 80, 81, 151, 154], "threshold": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 52, 53, 92, 95, 107, 108], "through": [59, 67, 78, 80, 81, 91, 92, 99, 104, 107, 108], "throughout": [82, 93], "thu": [85, 92, 106, 108], "tibbl": 59, "tick": 16, "tick_param": 92, "tight": 75, "tight_layout": [66, 69, 85, 86, 92], "tighter": 92, "tild": [17, 18, 19, 31, 35, 39, 60, 86, 89, 97, 106, 122, 123, 125, 127, 132, 133, 136, 137, 138, 139, 148, 149], "tile": 93, "time": [5, 7, 12, 14, 16, 17, 19, 40, 41, 58, 59, 60, 61, 63, 64, 65, 67, 75, 79, 80, 81, 86, 87, 88, 92, 94, 95, 96, 100, 101, 104, 108, 109, 110, 111, 112, 113, 114, 123, 139, 153, 154, 155], "time_budget": 85, "time_df": 65, "time_period": 65, "time_typ": [17, 19, 66, 69], "titiunik": [108, 153], "titl": [13, 16, 60, 61, 63, 66, 67, 69, 72, 73, 76, 77, 78, 79, 80, 81, 84, 85, 86, 87, 88, 89, 91, 92, 95, 150], "title_fonts": [66, 69], "tmp": [65, 76, 86], "tname": 59, "tnr": [62, 107], "to_datetim": 66, "to_fram": 90, "to_numpi": [78, 82, 88, 91], "to_str": 72, "todo": [60, 70], "toeplitz": 79, "togeth": [80, 81, 103, 104, 138], "toler": 86, "tomasz": [153, 154], "toml": 154, "too": [53, 84], "tool": [59, 62, 94, 155], "top": [60, 84, 86, 87, 88, 92, 95, 108, 150], "total": [47, 51, 61, 85, 87, 108, 109], "total_width": 72, "tpot": 85, "track": [100, 102, 104], "tracker": 150, "tradit": [72, 138], "train": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 50, 51, 58, 60, 62, 72, 75, 76, 77, 78, 80, 81, 83, 84, 86, 87, 90, 91, 97, 98, 122], "train_id": [60, 122], "train_ind": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38], "train_set": 122, "train_test_split": 75, "transact": 153, "transform": [31, 39, 45, 72, 83, 89, 95, 155], "translat": [72, 79], "transpos": 65, "treament": 90, "treat": [16, 17, 18, 19, 26, 52, 59, 64, 65, 66, 67, 68, 69, 73, 82, 90, 92, 95, 106, 108, 110, 111, 113, 114, 118, 123, 125, 127, 138, 155], "treat1_param": 89, "treat2_param": 89, "treat_var": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 107], "treatment": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 42, 44, 52, 57, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 79, 82, 84, 85, 86, 90, 92, 93, 94, 95, 96, 98, 99, 100, 101, 102, 103, 104, 105, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 121, 122, 124, 125, 126, 127, 128, 129, 131, 132, 133, 138, 142, 144, 145, 146, 148, 150, 152, 153, 154, 155], "treatment_df": 65, "treatment_effect": [32, 76, 77], "treatment_level": [22, 23, 72, 73, 83, 108], "treatment_var": [4, 5, 6, 7, 8, 9], "tree": [26, 49, 61, 62, 64, 65, 66, 69, 84, 87, 97, 105, 107, 108, 122, 123, 138, 152, 154], "tree_param": [26, 49], "tree_summari": 87, "trees_class": [61, 87], "trend": [59, 64, 65, 66, 69, 86, 108, 110, 112, 114, 153], "tri": [79, 139, 140], "triangular": [44, 92, 108], "trim": [14, 16, 22, 24, 25, 26, 27, 28, 29, 30, 61, 87, 88, 95], "trimming_rul": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 88], "trimming_threshold": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 61, 76, 83, 87, 88, 90, 91, 95], "trm": [62, 107], "true": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 36, 37, 38, 44, 45, 46, 47, 50, 51, 53, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 104, 107, 108, 110, 122, 123, 128, 129, 132, 133, 136, 137, 138, 139, 141, 142, 143, 144, 149, 152, 155], "true_effect": [65, 76, 77, 80, 81, 93], "true_gatet_effect": 82, "true_group_effect": 82, "true_tau": 92, "truemfunct": 93, "truncat": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 88], "try": [84, 94], "tune": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 72, 79, 84, 92, 105, 108, 150, 152, 154], "tune_on_fold": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 107], "tune_r": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38], "tune_set": [62, 107], "tuned_model": 85, "tuner": 107, "tunergridsearch": 62, "tupl": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 66, 69, 108, 111, 112, 113], "turn": 95, "turrel": 43, "tutori": 61, "tw": [87, 88], "twice": 108, "twinx": [72, 73], "two": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 57, 58, 61, 62, 64, 66, 67, 68, 69, 74, 75, 78, 83, 84, 85, 87, 88, 89, 90, 91, 93, 94, 95, 97, 98, 106, 107, 110, 113, 122, 132, 138, 155], "twoclass": 62, "twoearn": [61, 87, 88, 94, 155], "type": [7, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 58, 59, 60, 61, 62, 66, 69, 75, 84, 85, 86, 92, 95, 98, 105, 107, 108, 123, 134, 135, 138, 139, 148, 154, 155], "typeerror": [66, 67, 68, 69], "typic": [72, 108, 114, 150], "u": [18, 25, 26, 27, 28, 29, 31, 32, 34, 36, 39, 47, 51, 58, 59, 60, 61, 64, 65, 66, 67, 69, 72, 73, 75, 78, 80, 81, 83, 84, 86, 87, 88, 90, 91, 93, 94, 95, 98, 108, 115, 117, 118, 139, 140, 151, 155], "u_hat": [58, 75, 123], "u_i": [33, 36, 40, 43], "u_t": 18, "uehara": 153, "uhash": 62, "ulf": 153, "unambigu": 95, "uncertainti": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 80, 81, 83, 92, 94, 139, 145, 155], "unchang": [46, 47, 50, 51], "uncondit": [61, 66, 69, 87, 155], "unconfounded": [95, 153], "under": [25, 30, 58, 61, 64, 67, 75, 87, 90, 92, 95, 98, 108, 112, 114, 121, 138, 153], "underbrac": [58, 65, 75, 98, 106], "underfit": 85, "underli": [31, 35, 61, 62, 66, 69, 72, 73, 80, 81, 89, 90, 108, 114, 139, 140, 155], "underlin": [60, 86], "underset": [92, 108], "understand": [66, 67, 69, 72, 95], "undesir": 107, "unevenli": 122, "unifi": 104, "uniform": [18, 44, 45, 65, 74, 76, 77, 78, 90, 91, 138], "uniform_averag": [47, 51], "uniformli": [25, 66, 69, 78, 88, 138], "union": 25, "uniqu": [7, 57, 66, 67, 68, 69, 72, 73, 74, 84, 92, 100, 101, 104, 108, 111, 113, 123, 139, 149], "unique_label": 85, "unit": [7, 17, 19, 58, 59, 63, 64, 65, 66, 67, 68, 69, 82, 92, 96, 101, 104, 108, 110, 111, 112, 113, 114, 123, 124, 125, 126, 127, 139, 142, 144, 154], "univari": [32, 76, 77], "univers": [16, 153], "unknown": 108, "unlik": [61, 87, 88, 95], "unobserv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 57, 61, 66, 69, 74, 87, 88, 94, 95, 108, 139, 140, 149, 155], "unpen": 59, "unstabl": [139, 140], "unter": [60, 61, 62], "untest": 95, "until": [108, 110, 154], "untreat": [95, 108, 110], "untun": 72, "up": [16, 23, 29, 61, 79, 84, 85, 87, 88, 94, 95, 107, 108, 110, 122, 139, 140, 151, 154, 155], "upcom": 154, "updat": [46, 47, 50, 51, 60, 86, 87, 153, 154], "update_layout": [76, 77, 79, 92, 95], "update_trac": [76, 77], "upload": 154, "upon": [123, 154], "upper": [25, 61, 62, 65, 66, 69, 72, 73, 75, 78, 82, 83, 88, 91, 92, 94, 95, 107, 139, 145, 149, 155], "upper_bound": [76, 77], "upsilon": [63, 96], "upsilon_i": [63, 96], "upward": [61, 87, 88, 95], "upweight": 89, "url": [67, 79, 150, 153], "us": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 50, 51, 52, 53, 58, 60, 61, 63, 64, 65, 66, 67, 68, 69, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 104, 106, 108, 109, 111, 113, 122, 123, 124, 125, 126, 127, 138, 139, 140, 145, 147, 148, 149, 150, 151, 152, 154, 155], "usa": 153, "usabl": 84, "usag": [59, 66, 67, 68, 69, 70, 72, 73, 82, 86, 87, 88, 94, 99, 152, 154], "use_label_encod": [87, 155], "use_other_treat_as_covari": [4, 5, 6, 7, 8, 9, 99, 104], "use_pred_offset": 107, "use_weight": 107, "usecolormap": [76, 77], "user": [20, 21, 46, 47, 50, 51, 58, 59, 60, 61, 62, 67, 72, 73, 75, 82, 83, 84, 86, 87, 92, 94, 106, 107, 108, 123, 138, 150, 151, 152, 154, 155], "userwarn": [65, 66, 68, 69, 72, 87, 92, 93, 95], "usual": [60, 64, 66, 67, 68, 69, 72, 76, 77, 84, 86, 92, 94, 95, 106, 107, 122, 139, 149], "util": [0, 21, 65, 66, 69, 72, 83, 84, 85, 89, 92, 95, 107, 108, 154], "v": [10, 11, 25, 26, 34, 36, 37, 38, 40, 41, 42, 47, 51, 58, 60, 61, 66, 69, 72, 73, 75, 82, 83, 84, 85, 86, 87, 89, 92, 93, 97, 98, 106, 108, 115, 117, 118, 119, 120, 138, 150, 152, 153, 154, 155], "v108": 150, "v12": [150, 152], "v22": 62, "v23": 150, "v_": [41, 60, 86, 108], "v_i": [33, 34, 36, 42, 43, 58, 75, 98, 108], "v_j": 138, "val": [34, 66, 67, 68, 69, 122, 153], "val_list": 79, "valid": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 33, 37, 38, 52, 53, 58, 59, 60, 61, 64, 65, 66, 67, 69, 72, 75, 78, 84, 85, 86, 87, 88, 91, 92, 95, 98, 105, 106, 107, 122, 123, 129, 132, 133, 139, 140, 153, 155], "valu": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 50, 51, 53, 54, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 72, 73, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 92, 93, 94, 95, 97, 99, 100, 101, 104, 105, 107, 108, 109, 114, 115, 121, 122, 129, 132, 133, 136, 137, 138, 139, 140, 145, 149, 152, 154, 155], "value_count": 87, "van": 153, "vanderpla": [150, 152], "vanish": [58, 75, 98], "var": [17, 18, 19, 31, 35, 39, 60, 86, 89, 92, 139, 140, 146, 147, 148, 149], "var_ep": 95, "varepsilon": [25, 31, 39, 41, 60, 63, 86, 96, 106, 108, 117], "varepsilon_": [17, 19, 41, 60, 66, 69, 86], "varepsilon_0": 18, "varepsilon_1": 18, "varepsilon_d": [35, 39], "varepsilon_i": [35, 40, 63, 78, 91, 96], "vari": [17, 19, 61, 65, 66, 69, 84, 87, 89, 95], "variabl": [4, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 44, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 72, 73, 79, 82, 85, 86, 87, 88, 92, 94, 95, 96, 99, 100, 101, 102, 103, 104, 106, 107, 108, 110, 111, 113, 114, 115, 117, 118, 119, 120, 122, 123, 138, 139, 140, 145, 149, 152, 153, 154, 155], "varianc": [20, 21, 60, 62, 86, 92, 94, 95, 105, 108, 122, 139, 140, 145, 147, 148, 149, 152, 154], "variant": [59, 67, 83], "variat": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 83, 94, 139, 140, 149], "variou": [59, 85, 95, 107, 155], "varoquaux": [150, 152], "vasili": [95, 153], "vast": 72, "vector": [12, 14, 15, 17, 19, 22, 24, 25, 26, 27, 28, 30, 32, 33, 34, 36, 37, 38, 40, 41, 43, 57, 60, 61, 63, 64, 74, 80, 81, 82, 86, 87, 90, 93, 96, 108, 110, 119, 120, 121, 138, 152, 154], "vee": [123, 125, 127], "venv": 151, "verbos": [61, 64, 65, 66, 69, 72, 75, 78, 84, 85, 88, 91, 92, 95], "veri": [59, 60, 62, 67, 82, 84, 86, 93, 95, 123, 150], "verifi": 89, "versa": [84, 89, 139, 145], "version": [4, 14, 16, 22, 24, 25, 26, 27, 28, 29, 30, 31, 46, 47, 50, 51, 60, 61, 62, 64, 65, 76, 95, 97, 99, 104, 106, 108, 111, 112, 113, 123, 138, 139, 141, 142, 143, 144, 146, 147, 150, 154], "versoin": 95, "vertic": [60, 72, 73, 86], "via": [12, 14, 15, 18, 21, 22, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 52, 59, 63, 64, 65, 66, 67, 68, 69, 78, 79, 80, 81, 82, 83, 84, 86, 92, 94, 96, 97, 99, 104, 105, 106, 107, 108, 109, 110, 111, 113, 122, 129, 137, 138, 139, 140, 145, 149, 150, 151, 152, 153, 154, 155], "viabl": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38], "vice": [84, 89, 139, 145], "victor": [79, 95, 122, 150, 153], "vignett": [59, 154], "villa": [57, 74], "violat": [66, 69], "violet": [78, 88, 91], "vira": 153, "virtual": [67, 151], "virtualenv": 151, "visibl": [88, 92, 95], "visit": [150, 155], "visual": [13, 60, 66, 67, 68, 69, 82, 83, 85, 86, 92], "vmax": 69, "vmin": 69, "vol": 59, "volum": [95, 150], "voluntari": 89, "vv740": 86, "vv760g": 86, "w": [10, 11, 17, 18, 19, 20, 21, 31, 39, 42, 46, 47, 50, 51, 60, 79, 86, 89, 90, 93, 97, 98, 108, 111, 112, 113, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 152], "w24678": 122, "w30302": 153, "w_": [17, 18, 19, 60, 86, 90, 108], "w_1": [17, 18, 19, 90], "w_2": [17, 18, 19, 90], "w_3": [17, 18, 19], "w_4": [17, 18, 19], "w_df": 90, "w_i": [36, 64, 90, 92, 97, 106, 108, 122, 123, 125, 127, 138], "wa": [60, 65, 66, 69, 85, 86, 92, 95, 154], "wage": 67, "wager": 153, "wai": [61, 84, 85, 87, 93, 95, 107, 123, 151], "wander": 43, "wang": 153, "want": [57, 60, 61, 62, 64, 74, 78, 84, 86, 91, 92, 107, 108, 150, 151, 153], "warn": [13, 16, 52, 57, 58, 59, 60, 61, 62, 63, 65, 66, 68, 69, 72, 75, 84, 87, 93, 95, 97, 107, 108, 122, 123, 138, 152, 154], "warn_msg_prefix": 93, "wayon": 60, "we": [26, 49, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 104, 106, 107, 108, 109, 111, 112, 113, 114, 118, 122, 123, 125, 127, 130, 138, 139, 140, 149, 151, 152, 154, 155], "weak": [25, 139, 140, 153, 154], "weakest": [66, 69], "wealth": [10, 94], "websit": [61, 62, 67, 107, 150], "wedg": [60, 86], "week": 154, "wei": 138, "weight": [13, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 46, 47, 50, 51, 60, 61, 62, 63, 66, 67, 68, 69, 72, 73, 82, 83, 86, 87, 92, 96, 105, 107, 108, 109, 123, 128, 131, 138, 139, 146, 147, 154], "weights_bar": [22, 26, 83], "weights_dict": 83, "weiss": [150, 152], "well": [5, 6, 7, 8, 9, 50, 51, 58, 60, 75, 79, 84, 85, 86, 93, 97, 98, 122, 151, 152], "were": [61, 63, 87, 88, 96, 155], "what": [59, 79, 84, 153], "when": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 46, 47, 50, 51, 53, 61, 64, 67, 72, 83, 87, 89, 93, 108, 118, 121, 123, 138, 150, 151, 152, 154], "whenev": [61, 87], "whera": [139, 147], "where": [12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 47, 48, 49, 51, 57, 58, 60, 61, 63, 64, 65, 66, 67, 68, 69, 73, 74, 75, 78, 82, 86, 87, 89, 90, 91, 92, 93, 95, 96, 97, 98, 106, 107, 108, 109, 110, 112, 113, 114, 115, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 145, 146, 147, 149, 151, 152, 154, 155], "wherea": [32, 63, 64, 66, 69, 73, 93, 95, 96, 108, 111, 113, 123, 131, 139, 146, 155], "whether": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 36, 37, 38, 44, 48, 52, 53, 61, 65, 84, 87, 88, 92, 93, 95, 99, 104, 107, 108, 139, 140, 154], "which": [5, 6, 7, 8, 9, 12, 14, 15, 16, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 46, 50, 52, 53, 57, 58, 59, 61, 62, 63, 64, 66, 67, 68, 69, 71, 72, 73, 74, 75, 79, 82, 84, 85, 87, 88, 90, 92, 94, 95, 96, 98, 99, 104, 106, 107, 108, 112, 123, 138, 139, 140, 145, 146, 147, 149, 151, 154, 155], "while": [57, 74, 108], "white": [60, 80, 81, 86, 95], "whitegrid": [87, 88], "whitnei": [95, 153], "who": [59, 61, 87, 95], "whole": [58, 64, 75, 92, 98, 107, 123, 124, 139, 140], "whom": 108, "why": [67, 72], "widehat": [66, 67, 68, 69, 108], "width": [13, 16, 58, 60, 72, 76, 77, 79], "wiki": 154, "wiksel": 153, "wild": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 138], "window": 151, "wise": [80, 81], "wish": 151, "within": [44, 60, 66, 67, 68, 69, 72, 80, 81, 86, 90, 92], "without": [25, 35, 44, 57, 58, 66, 67, 68, 69, 72, 74, 75, 84, 85, 95, 98, 105, 107, 108, 139, 140, 151, 154], "wolf": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 138], "won": 95, "word": [44, 92, 108, 154, 155], "work": [46, 47, 50, 51, 65, 66, 67, 68, 69, 71, 72, 73, 82, 84, 89, 93, 94, 95, 107, 108, 138, 151, 153], "workflow": [150, 154], "workspac": 87, "world": 153, "worri": 95, "wors": [47, 51], "would": [47, 51, 59, 61, 62, 66, 67, 68, 69, 76, 77, 79, 84, 87, 88, 92, 94, 95, 106, 107, 139, 149, 155], "wrapper": [4, 59, 92, 99, 104, 107], "wright": 93, "write": [58, 59, 63, 64, 75, 96, 98, 139, 149], "written": [108, 123, 139, 146, 147], "wrong": [84, 89], "wspace": 84, "wurd": [60, 61, 62], "www": [150, 151], "x": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 24, 25, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 51, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 102, 103, 104, 106, 107, 108, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 146, 147, 148, 149, 152, 155], "x0": [72, 73, 89, 92], "x1": [60, 62, 63, 64, 72, 73, 83, 85, 86, 89, 92, 94, 95, 96, 99, 102, 103, 104, 106, 107, 108, 123, 138, 139, 140, 152], "x10": [60, 62, 63, 83, 85, 86, 96, 99, 103, 104, 107, 108, 123, 138, 152], "x100": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x11": [60, 62, 63, 86, 96, 99, 103, 104, 107, 108, 123, 138, 152], "x12": [60, 62, 63, 86, 96, 99, 103, 104, 107, 108, 123, 138, 152], "x13": [60, 62, 63, 86, 96, 99, 103, 104, 107, 108, 123, 138, 152], "x14": [60, 62, 63, 86, 96, 99, 103, 104, 107, 108, 123, 138, 152], "x15": [60, 62, 63, 86, 96, 99, 103, 104, 107, 108, 123, 138, 152], "x16": [60, 62, 63, 86, 96, 99, 103, 104, 107, 108, 123, 138, 152], "x17": [60, 62, 63, 86, 96, 99, 103, 104, 107, 108, 123, 138, 152], "x18": [60, 62, 63, 86, 96, 99, 103, 104, 107, 108, 123, 138, 152], "x19": [60, 62, 63, 86, 96, 99, 103, 104, 107, 108, 123, 138, 152], "x1x2x3x4x5x6x7x8x9x10": 60, "x2": [60, 62, 63, 64, 72, 73, 83, 85, 86, 92, 94, 95, 96, 99, 102, 103, 104, 106, 107, 108, 123, 138, 152], "x20": [60, 62, 63, 86, 96, 99, 103, 104, 107, 108, 123, 138, 152], "x21": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x22": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x23": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x24": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x25": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x26": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x27": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x28": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x29": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x2_dummi": 95, "x2_preds_control": 95, "x2_preds_treat": 95, "x3": [60, 62, 63, 64, 72, 73, 83, 85, 86, 94, 95, 96, 99, 102, 103, 104, 106, 107, 108, 123, 138, 152], "x30": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x31": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x32": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x33": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x34": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x35": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x36": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x37": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x38": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x39": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x4": [60, 62, 63, 64, 72, 73, 83, 85, 86, 94, 95, 96, 99, 103, 104, 107, 108, 123, 138, 152], "x40": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x41": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x42": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x43": [60, 62, 63, 85, 86, 96, 99, 103, 104, 108, 152], "x44": [60, 62, 63, 85, 86, 96, 99, 103, 104, 108, 152], "x45": [60, 62, 63, 85, 86, 96, 99, 103, 104, 108, 152], "x46": [60, 62, 63, 85, 86, 96, 99, 103, 104, 108, 152], "x47": [60, 62, 63, 85, 86, 96, 99, 103, 104, 108, 152], "x48": [60, 62, 63, 85, 86, 96, 99, 103, 104, 108, 152], "x49": [60, 62, 63, 85, 86, 96, 99, 103, 104, 108, 152], "x5": [60, 62, 63, 83, 85, 86, 95, 96, 99, 103, 104, 107, 108, 123, 138, 152], "x50": [60, 62, 63, 85, 86, 96, 99, 103, 104, 108, 152], "x51": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x52": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x53": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x54": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x55": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x56": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x57": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x58": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x59": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x6": [60, 62, 63, 83, 85, 86, 96, 99, 103, 104, 107, 108, 123, 138, 152], "x60": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x61": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x62": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x63": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x64": [60, 62, 63, 65, 66, 67, 68, 69, 86, 87, 92, 95, 96, 99, 103, 104, 108, 152], "x65": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x66": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x67": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x68": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x69": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x7": [60, 62, 63, 83, 85, 86, 96, 99, 103, 104, 107, 108, 123, 138, 152], "x70": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x71": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x72": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x73": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x74": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x75": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x76": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x77": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x78": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x79": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x8": [60, 62, 63, 83, 85, 86, 96, 99, 103, 104, 107, 108, 123, 138, 152], "x80": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x81": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x82": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x83": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x84": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x85": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x86": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x87": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x88": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x89": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x9": [60, 62, 63, 83, 85, 86, 96, 99, 103, 104, 107, 108, 123, 138, 152], "x90": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x91": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x92": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x93": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x94": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x95": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x96": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x96x97x98x99x100ydcluster_var_icluster_var_jz": 60, "x97": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x98": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x99": [60, 62, 63, 86, 96, 99, 103, 104, 108, 152], "x_": [17, 19, 41, 42, 58, 60, 65, 75, 86, 95, 98], "x_0": [65, 76, 77, 80, 81, 82], "x_1": [17, 18, 19, 31, 35, 37, 38, 39, 65, 76, 77, 78, 80, 81, 82, 91, 95, 108, 119, 120, 139, 140, 152], "x_1x_3": [78, 91], "x_2": [17, 18, 19, 31, 35, 39, 65, 76, 77, 78, 80, 81, 82, 91, 95, 139, 140], "x_3": [17, 18, 19, 31, 35, 39, 65, 76, 77, 80, 81, 82, 139, 140], "x_4": [17, 18, 19, 31, 35, 39, 76, 77, 78, 80, 81, 82, 91], "x_5": [31, 35, 39, 76, 77, 80, 81], "x_6": [76, 77, 80, 81], "x_7": [76, 77, 80, 81], "x_8": [76, 77, 80, 81], "x_9": [76, 77, 80, 81], "x_binary_control": 95, "x_binary_tr": 95, "x_center": 72, "x_col": [4, 5, 6, 7, 8, 9, 16, 57, 60, 61, 62, 66, 67, 68, 69, 74, 79, 86, 87, 88, 90, 92, 93, 94, 95, 99, 100, 101, 104, 107, 108, 109, 111, 113, 152, 154, 155], "x_cols_bench": 95, "x_cols_binari": 95, "x_cols_poli": 86, "x_conf": 91, "x_conf_tru": 91, "x_df": 65, "x_domain": 62, "x_end": 72, "x_i": [32, 33, 34, 36, 40, 42, 43, 45, 58, 63, 64, 75, 78, 80, 81, 89, 91, 92, 96, 98, 106, 108, 110, 111, 113, 114, 121, 123, 125, 127], "x_jitter": 72, "x_p": [37, 38, 108, 119, 120, 152], "x_rang": 72, "x_start": 72, "x_train": 85, "x_true": [78, 91], "x_var": 62, "xaxis_titl": [76, 77, 79, 92, 95], "xformla": 59, "xgb": 85, "xgb_untuned_l": 85, "xgb_untuned_m": 85, "xgbclassifi": [84, 87, 89, 155], "xgboost": [58, 61, 84, 87, 89, 155], "xgbregressor": [84, 85, 87, 89, 155], "xi": [17, 18, 19, 35, 108], "xi_": 138, "xi_0": [41, 60, 86], "xi_i": [63, 96], "xiaoji": 153, "xintercept": [58, 63], "xlab": [58, 60, 61], "xlabel": [65, 66, 67, 69, 72, 73, 76, 77, 78, 80, 81, 85, 87, 88, 91], "xlim": [58, 61, 72], "xmax": 72, "xmax_rel": 72, "xmin": 72, "xmin_rel": 72, "xtick": [72, 73, 85], "xval": [62, 107], "xx": 75, "y": [4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 18, 22, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 51, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 113, 115, 117, 118, 119, 120, 122, 123, 124, 125, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 145, 146, 147, 148, 149, 152, 155], "y0": [59, 66, 69, 72, 73, 78, 91], "y0_cvar": 78, "y0_quant": [78, 91], "y1": [59, 66, 69, 78, 91], "y1_cvar": 78, "y1_quant": [78, 91], "y_": [16, 17, 19, 41, 60, 63, 64, 65, 66, 69, 86, 96, 108, 110, 111, 113, 114, 121, 123, 125, 127], "y_0": [12, 14, 18, 45, 123, 126], "y_1": [12, 14, 18, 45, 123, 126], "y_col": [4, 5, 6, 7, 8, 9, 16, 57, 58, 60, 61, 62, 63, 66, 67, 68, 69, 74, 76, 77, 79, 80, 81, 83, 86, 87, 88, 90, 92, 93, 94, 97, 98, 99, 100, 101, 103, 104, 107, 108, 109, 111, 113, 122, 123, 152, 154, 155], "y_df": [65, 90], "y_diff": 65, "y_i": [32, 33, 34, 36, 40, 42, 43, 58, 63, 64, 75, 78, 89, 90, 91, 92, 96, 98, 108, 110, 121], "y_label": [13, 16], "y_lower_quantil": [66, 69], "y_mean": [66, 69], "y_pred": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 47, 51, 84, 107], "y_train": 85, "y_true": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 47, 51, 84, 107], "y_upper_quantil": [66, 69], "ya": 153, "yasui": 153, "yata": 153, "yaxis_titl": [76, 77, 79, 92, 95], "year": [67, 150], "yerr": [65, 72, 73, 80, 81, 85, 87, 89, 92], "yet": [60, 66, 68, 69, 71, 108, 111, 113, 114], "yggvpl": 86, "yield": 108, "yintercept": 61, "ylab": [58, 60, 61], "ylabel": [65, 66, 67, 69, 72, 73, 76, 77, 78, 80, 81, 85, 87, 88, 91], "ylim": 87, "ymax": 61, "ymin": 61, "yname": 59, "york": 153, "you": [46, 47, 50, 51, 57, 58, 65, 66, 67, 68, 69, 72, 74, 86, 94, 108, 150, 151, 155], "your": [84, 151], "ython": 150, "yukun": 153, "yusuk": 153, "yuya": 153, "yy": 75, "z": [4, 5, 6, 7, 8, 9, 17, 18, 19, 25, 27, 30, 31, 33, 35, 36, 37, 39, 40, 41, 57, 60, 61, 63, 66, 69, 74, 76, 77, 79, 86, 87, 91, 93, 95, 96, 106, 108, 117, 119, 123, 130, 132, 134, 137, 138, 154], "z1": [7, 16, 37, 66, 69, 100, 101, 104, 108, 109, 110, 111, 113], "z2": [7, 16, 66, 69, 100, 101, 104, 108, 109, 110, 111, 113], "z3": [7, 16, 66, 69, 100, 101, 104, 108, 109, 110, 111, 113], "z4": [7, 16, 66, 69, 100, 101, 104, 108, 109, 110, 111, 113], "z_": [41, 60, 86], "z_1": [31, 35, 39, 66, 69], "z_2": [31, 35, 39], "z_3": [31, 35, 39], "z_4": [31, 35, 39, 66, 69], "z_5": 31, "z_col": [4, 5, 6, 7, 8, 9, 25, 27, 37, 57, 60, 61, 63, 74, 86, 87, 88, 93, 96, 99, 100, 104, 106, 108, 154], "z_i": [36, 40, 63, 91, 96, 108], "z_j": [17, 18, 19, 31, 35, 39], "z_true": 91, "zadik": 153, "zaxis_titl": [76, 77, 79], "zero": [14, 18, 45, 64, 65, 66, 69, 72, 78, 83, 84, 90, 91, 94, 95, 108, 123, 125, 127, 138, 139, 142, 144], "zeros_lik": 91, "zeta": [25, 37, 38, 61, 87, 106, 108, 117, 119, 120, 152], "zeta_": [41, 60, 86], "zeta_0": [41, 60, 86], "zeta_i": [34, 40, 42, 58, 75, 98], "zeta_j": 138, "zhang": 153, "zhao": [12, 14, 15, 18, 31, 35, 39, 59, 64, 66, 69, 108, 110, 114, 153], "zimmert": [64, 108, 114, 153], "zip": [76, 77], "zorder": [72, 73], "\u03c4_x0": 89, "\u03c4_x1": 89, "\u2139": 58}, "titles": ["API Reference", "<span class=\"section-number\">1. </span>DoubleML Data Class", "<span class=\"section-number\">3. </span>Datasets", "<span class=\"section-number\">2. </span>DoubleML Models", "<span class=\"section-number\">1.2. </span>doubleml.data.DoubleMLClusterData", "<span class=\"section-number\">1.6. </span>doubleml.data.DoubleMLDIDData", "<span class=\"section-number\">1.1. </span>doubleml.data.DoubleMLData", "<span class=\"section-number\">1.3. </span>doubleml.data.DoubleMLPanelData", "<span class=\"section-number\">1.5. </span>doubleml.data.DoubleMLRDDData", "<span class=\"section-number\">1.4. </span>doubleml.data.DoubleMLSSMData", "<span class=\"section-number\">3.1.1. </span>doubleml.datasets.fetch_401K", "<span class=\"section-number\">3.1.2. </span>doubleml.datasets.fetch_bonus", "<span class=\"section-number\">2.3.4. </span>doubleml.did.DoubleMLDID", "<span class=\"section-number\">2.3.2. </span>doubleml.did.DoubleMLDIDAggregation", "<span class=\"section-number\">2.3.3. </span>doubleml.did.DoubleMLDIDBinary", "<span class=\"section-number\">2.3.5. </span>doubleml.did.DoubleMLDIDCS", "<span class=\"section-number\">2.3.1. </span>doubleml.did.DoubleMLDIDMulti", "<span class=\"section-number\">3.2.13. </span>doubleml.did.datasets.make_did_CS2021", "<span class=\"section-number\">3.2.12. </span>doubleml.did.datasets.make_did_SZ2020", "<span class=\"section-number\">3.2.14. </span>doubleml.did.datasets.make_did_cs_CS2021", "<span class=\"section-number\">5.1. </span>doubleml.double_ml_score_mixins.LinearScoreMixin", "<span class=\"section-number\">5.2. </span>doubleml.double_ml_score_mixins.NonLinearScoreMixin", "<span class=\"section-number\">2.2.2. </span>doubleml.irm.DoubleMLAPO", "<span class=\"section-number\">2.2.3. </span>doubleml.irm.DoubleMLAPOS", "<span class=\"section-number\">2.2.7. </span>doubleml.irm.DoubleMLCVAR", "<span class=\"section-number\">2.2.4. </span>doubleml.irm.DoubleMLIIVM", "<span class=\"section-number\">2.2.1. </span>doubleml.irm.DoubleMLIRM", "<span class=\"section-number\">2.2.6. </span>doubleml.irm.DoubleMLLPQ", "<span class=\"section-number\">2.2.5. </span>doubleml.irm.DoubleMLPQ", "<span class=\"section-number\">2.2.8. </span>doubleml.irm.DoubleMLQTE", "<span class=\"section-number\">2.2.9. </span>doubleml.irm.DoubleMLSSM", "<span class=\"section-number\">3.2.5. </span>doubleml.irm.datasets.make_confounded_irm_data", "<span class=\"section-number\">3.2.3. </span>doubleml.irm.datasets.make_heterogeneous_data", "<span class=\"section-number\">3.2.2. </span>doubleml.irm.datasets.make_iivm_data", "<span class=\"section-number\">3.2.1. </span>doubleml.irm.datasets.make_irm_data", "<span class=\"section-number\">3.2.4. </span>doubleml.irm.datasets.make_irm_data_discrete_treatments", "<span class=\"section-number\">3.2.6. </span>doubleml.irm.datasets.make_ssm_data", "<span class=\"section-number\">2.1.2. </span>doubleml.plm.DoubleMLPLIV", "<span class=\"section-number\">2.1.1. </span>doubleml.plm.DoubleMLPLR", "<span class=\"section-number\">3.2.11. </span>doubleml.plm.datasets.make_confounded_plr_data", "<span class=\"section-number\">3.2.9. </span>doubleml.plm.datasets.make_pliv_CHS2015", "<span class=\"section-number\">3.2.10. </span>doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021", "<span class=\"section-number\">3.2.7. </span>doubleml.plm.datasets.make_plr_CCDDHNR2018", "<span class=\"section-number\">3.2.8. </span>doubleml.plm.datasets.make_plr_turrell2018", "<span class=\"section-number\">2.4.1. </span>doubleml.rdd.RDFlex", "<span class=\"section-number\">3.2.15. </span>doubleml.rdd.datasets.make_simple_rdd_data", "<span class=\"section-number\">4.1.2. </span>doubleml.utils.DMLDummyClassifier", "<span class=\"section-number\">4.1.1. </span>doubleml.utils.DMLDummyRegressor", "<span class=\"section-number\">4.1.3. </span>doubleml.utils.DoubleMLBLP", "<span class=\"section-number\">4.1.4. </span>doubleml.utils.DoubleMLPolicyTree", "<span class=\"section-number\">4.1.6. </span>doubleml.utils.GlobalClassifier", "<span class=\"section-number\">4.1.5. </span>doubleml.utils.GlobalRegressor", "<span class=\"section-number\">4.1.8. </span>doubleml.utils.PSProcessor", "<span class=\"section-number\">4.1.7. </span>doubleml.utils.PSProcessorConfig", "<span class=\"section-number\">4.2.1. </span>doubleml.utils.gain_statistics", "<span class=\"section-number\">5. </span>Score Mixin Classes for DoubleML Models", "<span class=\"section-number\">4. </span>Utility Classes and Functions", "R: Basic Instrumental Variables Calculation", "R: Basics of Double Machine Learning", "R: DoubleML for Difference-in-Differences", "R: Cluster Robust Double Machine Learning", "R: Impact of 401(k) on Financial Wealth", "R: Ensemble Learners and More with <code class=\"docutils literal notranslate\"><span class=\"pre\">mlr3pipelines</span></code>", "R: Sample Selection Models", "Python: Difference-in-Differences", "Python: Difference-in-Differences Pre-Testing", "Python: Panel Data with Multiple Time Periods", "Python: Real-Data Example for Multi-Period Difference-in-Differences", "Python: Panel Data Introduction", "Python: Repeated Cross-Sectional Data with Multiple Time Periods", "DML: Bonus Data", "Examples", "Python: Causal Machine Learning with TabPFN", "Python: Average Potential Outcome (APO) Models", "Python: Basic Instrumental Variables calculation", "Python: Basics of Double Machine Learning", "Python: Conditional Average Treatment Effects (CATEs) for IRM models", "Python: Conditional Average Treatment Effects (CATEs) for PLR models", "Python: Conditional Value at Risk of potential outcomes", "Python: First Stage and Causal Estimation", "Python: Group Average Treatment Effects (GATEs) for IRM models", "Python: Group Average Treatment Effects (GATEs) for PLR models", "Python: GATE Sensitivity Analysis", "Python: IRM and APO Model Comparison", "Python: Choice of learners", "DoubleML meets FLAML - How to tune learners automatically within <code class=\"docutils literal notranslate\"><span class=\"pre\">DoubleML</span></code>", "Python: Cluster Robust Double Machine Learning", "Python: Impact of 401(k) on Financial Wealth", "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)", "Python: PLM and IRM for Multiple Treatments", "Python: Policy Learning with Trees", "Python: Potential Quantiles and Quantile Treatment Effects", "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)", "Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments", "Python: Sensitivity Analysis", "Python: Sensitivity Analysis for Causal ML", "Python: Sample Selection Models", "<span class=\"section-number\">6. </span>Double machine learning algorithms", "<span class=\"section-number\">1. </span>The basics of double/debiased machine learning", "DoubleMLData from dataframes", "Key arguments", "Key arguments", "Key arguments", "Key arguments", "<span class=\"section-number\">2. </span>Data Backend", "User Guide", "<span class=\"section-number\">4. </span>Heterogeneous treatment effects", "<span class=\"section-number\">7. </span>Learners, hyperparameters and hyperparameter tuning", "<span class=\"section-number\">3. </span>Models", "&lt;no title&gt;", "Panel Data", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">9. </span>Sample-splitting, cross-fitting and repeated cross-fitting", "<span class=\"section-number\">5. </span>Score functions", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">8. </span>Variance estimation and confidence intervals", "<span class=\"section-number\">10. </span>Sensitivity analysis", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "DoubleML", "Installing DoubleML", "Getting Started", "Double Machine Learning Literature", "Release Notes", "DoubleML Workflow"], "titleterms": {"": 85, "0": 155, "1": [85, 95, 155], "2": [85, 95, 155], "2011": 95, "2023": 95, "3": [85, 95, 155], "4": [95, 155], "401": [61, 87, 88, 94], "5": [95, 155], "6": 155, "7": 155, "95": 85, "A": [60, 86], "ATE": [63, 82, 89, 96], "No": [60, 86], "One": [60, 76, 77, 86], "That": 93, "The": [61, 87, 89, 98, 152], "acknowledg": [59, 150], "acycl": [57, 74], "addit": 89, "adjust": [66, 69, 92], "advanc": [92, 107, 138], "aggreg": [66, 67, 68, 69, 108], "al": 95, "algorithm": [97, 139, 150, 152], "all": [66, 69], "altern": 123, "analysi": [66, 69, 73, 82, 83, 94, 95, 139, 155], "anticip": [66, 69], "api": [0, 85], "apo": [73, 83, 108, 123, 139], "applic": [60, 86, 94], "approach": [58, 75, 84, 98], "ar": 93, "arah": 95, "arbitrari": 89, "archiv": 71, "argument": [100, 101, 102, 103, 104], "arrai": [99, 104], "asset": [61, 87], "assumpt": 95, "att": [64, 66, 67, 68, 69], "augment": 89, "automat": 85, "automl": 85, "averag": [61, 72, 73, 76, 77, 80, 81, 83, 87, 106, 108, 123, 139], "backend": [60, 61, 86, 87, 104, 152, 155], "band": 138, "base": [62, 66, 69], "basic": [57, 58, 66, 69, 74, 75, 98], "benchmark": [94, 95, 139], "bia": [58, 75, 98], "binari": [108, 123], "bonu": 70, "bootstrap": 138, "build": 151, "calcul": [57, 74], "call": 85, "callabl": 123, "case": 71, "cate": [76, 77, 89, 106], "causal": [67, 70, 72, 73, 79, 95, 123, 152, 155], "chernozhukov": 95, "choic": 84, "citat": 150, "class": [1, 55, 56, 60, 86], "cluster": [60, 86], "code": 150, "coeffici": 85, "combin": [66, 69, 79], "compar": [84, 85], "comparison": [59, 72, 83, 85], "comput": [84, 85], "conclus": [85, 95], "conda": 151, "condit": [67, 76, 77, 78, 88, 106, 123], "confid": [85, 93, 138], "construct": 107, "contrast": 73, "control": [66, 69], "covari": [66, 69, 92], "coverag": [64, 79], "cran": 151, "creat": [72, 85], "cross": [60, 64, 69, 86, 108, 110, 122, 123, 139, 152], "custom": [84, 85], "cvar": [78, 88, 106, 123], "dag": [57, 74], "data": [1, 4, 5, 6, 7, 8, 9, 57, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 85, 86, 87, 88, 90, 91, 92, 94, 95, 96, 98, 104, 108, 110, 123, 139, 152, 155], "datafram": [99, 104], "dataset": [2, 10, 11, 17, 18, 19, 31, 32, 33, 34, 35, 36, 39, 40, 41, 42, 43, 45, 70], "debias": [58, 75, 98, 152], "default": 85, "defin": [60, 86], "demo": 59, "depend": 151, "descript": [66, 69], "design": [92, 108], "detail": [59, 66, 67, 68, 69, 108], "develop": 151, "dgp": [58, 72, 73, 75], "did": [3, 12, 13, 14, 15, 16, 17, 18, 19, 59, 108], "differ": [59, 64, 65, 67, 71, 84, 108, 123, 138, 139], "dimension": [76, 77], "direct": [57, 74], "disclaim": 95, "discontinu": [92, 108], "distribut": [63, 96], "dml": [60, 70, 86, 122, 152, 155], "dml1": 97, "dml2": 97, "dmldummyclassifi": 46, "dmldummyregressor": 47, "doubl": [58, 60, 75, 86, 97, 98, 150, 152, 153], "double_ml_score_mixin": [20, 21], "doubleml": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 59, 61, 62, 72, 74, 85, 87, 94, 95, 138, 150, 151, 155], "doublemlapo": [22, 23], "doublemlblp": 48, "doublemlclusterdata": [4, 60], "doublemlcvar": 24, "doublemldata": [6, 61, 72, 86, 87, 99, 104, 152], "doublemldid": 12, "doublemldidaggreg": 13, "doublemldidbinari": 14, "doublemldidc": 15, "doublemldiddata": [5, 104], "doublemldidmulti": 16, "doublemliivm": 25, "doublemlirm": 26, "doublemllpq": 27, "doublemlpaneldata": [7, 66, 69, 104], "doublemlpliv": [37, 60, 86], "doublemlplr": 38, "doublemlpolicytre": 49, "doublemlpq": 28, "doublemlqt": 29, "doublemlrdddata": [8, 104], "doublemlssm": 30, "doublemlssmdata": [9, 104], "effect": [61, 66, 67, 68, 69, 71, 72, 76, 77, 78, 80, 81, 83, 87, 88, 89, 91, 94, 95, 106, 108], "elig": [61, 87], "empir": 79, "ensembl": [62, 92], "error": [60, 86], "estim": [57, 61, 63, 64, 66, 67, 68, 69, 70, 72, 74, 79, 82, 85, 87, 88, 89, 91, 94, 95, 96, 122, 123, 138, 152, 155], "et": 95, "evalu": [72, 84, 85, 107], "event": [66, 67, 68, 69], "exampl": [59, 60, 67, 71, 76, 77, 86, 94, 100, 101, 102, 103, 104], "exploit": [59, 62], "extern": [107, 122], "featur": [62, 150], "fetch_401k": 10, "fetch_bonu": 11, "figur": 89, "file": 151, "final": 59, "financi": [61, 87, 88], "first": 79, "fit": [60, 85, 86, 122, 152], "flaml": 85, "flexibl": 92, "fold": [85, 122], "forest": 70, "formul": [95, 155], "from": [59, 62, 99, 104, 151], "full": 85, "function": [56, 59, 60, 86, 123, 152], "fuzzi": [92, 108], "gain_statist": 54, "gate": [80, 81, 82, 106], "gatet": 82, "gener": [2, 58, 71, 72, 73, 75, 85, 92, 98, 139], "get": 152, "github": 151, "global": 92, "globalclassifi": 50, "globalregressor": 51, "graph": [57, 74], "group": [66, 68, 69, 80, 81, 106], "guid": 105, "helper": [60, 86], "heterogen": [71, 83, 89, 106], "how": [62, 85], "hyperparamet": [83, 107], "identif": 95, "iivm": [61, 87, 108, 123], "impact": [61, 87, 88], "implement": [97, 108, 123, 139], "import": 72, "induc": [58, 75, 98], "infer": [138, 155], "initi": [60, 85, 86], "insight": 72, "instal": 151, "instrument": [57, 74, 93], "integr": 59, "interact": [61, 80, 87, 90, 108, 123, 139], "interv": [85, 93, 138], "introduct": 68, "invers": 89, "irm": [3, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 61, 70, 76, 80, 83, 87, 89, 90, 94, 106, 108, 123, 139], "iv": [57, 61, 74, 87, 108, 123], "k": [61, 87, 88, 94, 122], "kei": [72, 100, 101, 102, 103, 104], "lambda": 79, "lasso": [70, 79], "latest": 151, "lear": [60, 86], "learn": [58, 60, 72, 75, 86, 90, 97, 98, 106, 150, 152, 153], "learner": [62, 70, 83, 84, 85, 92, 107, 152], "less": 85, "level": 108, "linear": [61, 66, 69, 81, 87, 89, 92, 108, 123, 139], "linearscoremixin": 20, "literatur": 153, "load": [60, 70, 86, 95], "loader": 2, "local": [61, 87, 88, 91, 92, 123], "loss": 79, "lpq": [91, 123], "lqte": [88, 91], "m": 122, "machin": [58, 60, 72, 75, 86, 97, 98, 150, 152, 153], "main": 150, "mainten": 150, "make_confounded_irm_data": 31, "make_confounded_plr_data": 39, "make_did_cs2021": 17, "make_did_cs_cs2021": 19, "make_did_sz2020": 18, "make_heterogeneous_data": 32, "make_iivm_data": 33, "make_irm_data": 34, "make_irm_data_discrete_treat": 35, "make_pliv_chs2015": 40, "make_pliv_multiway_cluster_ckms2021": 41, "make_plr_ccddhnr2018": 42, "make_plr_turrell2018": 43, "make_simple_rdd_data": 45, "make_ssm_data": 36, "mar": [63, 96], "market": [60, 86], "matric": [99, 104], "meet": 85, "method": [72, 85, 155], "metric": [84, 85], "minimum": 107, "miss": [63, 96], "missing": [108, 123], "mixin": 55, "ml": [58, 59, 75, 95, 98, 155], "mlr3": 62, "mlr3extralearn": 62, "mlr3learner": 62, "mlr3pipelin": 62, "model": [3, 55, 61, 63, 70, 72, 73, 76, 77, 80, 81, 83, 85, 87, 89, 90, 93, 95, 96, 106, 108, 122, 123, 138, 139, 152, 155], "modul": 70, "more": 62, "motiv": [60, 86], "multi": 67, "multipl": [66, 69, 73, 89, 108], "multipli": 138, "naiv": [57, 74], "net": [61, 87], "neyman": [123, 152], "nonignor": [63, 96, 108, 123], "nonlinearscoremixin": 21, "nonrespons": [63, 96, 108, 123], "note": 154, "nuisanc": [85, 152], "object": [60, 72, 86, 94], "option": 151, "orthogon": [58, 75, 98, 123, 152], "out": [58, 75, 98], "outcom": [63, 64, 72, 73, 78, 96, 106, 108, 123, 139], "over": 138, "overcom": [58, 75, 98], "overfit": [58, 75, 98], "overlap": 89, "packag": [59, 61, 87, 151], "panel": [64, 66, 68, 108, 110, 123, 139], "parallel": 67, "paramet": [62, 70, 85, 108, 123], "partial": [58, 61, 75, 81, 87, 89, 98, 108, 123, 139], "particip": [61, 87], "partit": 122, "penalti": 79, "perform": [59, 72, 89], "period": [66, 67, 69, 108, 123, 139], "pip": 151, "pipelin": 107, "pliv": [108, 123], "plm": [3, 37, 38, 39, 40, 41, 42, 43, 89, 108, 123, 139], "plot": [60, 85, 86], "plr": [61, 70, 77, 81, 87, 106, 108, 123, 139], "polici": [90, 106], "potenti": [72, 73, 78, 88, 91, 106, 108, 123, 139], "pq": [91, 106, 123], "pre": 65, "predict": [59, 107], "preprocess": 62, "problem": 155, "process": [58, 60, 72, 73, 75, 86, 98], "product": [60, 86], "propens": 89, "provid": 122, "psprocessor": 52, "psprocessorconfig": 53, "python": [64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 107, 151], "qte": [91, 106], "qualiti": 79, "quantil": [88, 91, 106, 123], "question": 67, "r": [57, 58, 59, 60, 61, 62, 63, 71, 107, 151], "random": [63, 70, 96, 108, 123], "rank": 89, "rdd": [3, 44, 45, 92, 108], "rdflex": 44, "real": [60, 67, 86], "refer": [0, 57, 59, 60, 62, 74, 79, 84, 85, 86, 89, 93, 95, 98, 107, 122, 138, 150, 152], "regress": [61, 80, 81, 87, 90, 92, 108, 123, 139], "regular": [58, 75, 98], "releas": [151, 154], "remark": 59, "remov": [58, 75, 98], "repeat": [64, 69, 108, 110, 122, 123, 139], "repetit": 122, "requir": 107, "research": 67, "respect": [60, 86], "result": [60, 61, 86, 87, 89], "risk": [78, 88, 106, 123], "robust": [60, 86, 93], "run": 93, "sampl": [58, 63, 75, 85, 96, 98, 108, 122, 123], "sandbox": 71, "score": [55, 58, 75, 89, 98, 123, 152], "section": [64, 69, 108, 110, 123, 139], "select": [63, 66, 69, 96, 108, 123], "sensit": [66, 69, 73, 82, 83, 94, 95, 139, 155], "set": [62, 107], "setup": 72, "sharp": [92, 108], "simpl": [58, 75, 98], "simul": [57, 60, 64, 74, 86, 93, 94], "simultan": 138, "singl": 73, "small": 93, "sourc": [150, 151], "special": 104, "specif": [139, 155], "specifi": [70, 107, 123], "split": [58, 75, 98, 122], "ssm": 108, "stack": 92, "stage": 79, "standard": [60, 84, 86], "start": 152, "step": 85, "structur": 72, "studi": [66, 67, 68, 69, 71], "summari": [61, 72, 85, 87, 89], "tabpfn": 72, "takeawai": 72, "test": 65, "theori": 139, "time": [66, 68, 69, 84, 85], "train": 85, "treat": 83, "treatment": [61, 72, 76, 77, 78, 80, 81, 83, 87, 88, 89, 91, 106, 108, 123, 139], "tree": [90, 106], "trend": 67, "tune": [62, 85, 107], "two": [60, 76, 77, 86, 108, 123, 139], "type": 104, "uncondit": 67, "under": [63, 89, 96], "univers": [66, 69], "untun": 85, "up": 62, "us": [57, 59, 62, 70, 74, 85, 107], "usag": [100, 101, 102, 103, 104], "user": 105, "util": [46, 47, 48, 49, 50, 51, 52, 53, 54, 56], "v": 79, "valid": 138, "valu": [78, 88, 106, 123], "vanderweel": 95, "variabl": [57, 74, 93], "varianc": 138, "version": 151, "via": 123, "visual": 72, "wai": [60, 86], "weak": 93, "wealth": [61, 87, 88], "weight": [89, 106], "when": 85, "whl": 151, "within": 85, "without": [92, 122], "workflow": 155, "xgboost": 85, "zero": [60, 86]}})