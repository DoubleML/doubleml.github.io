Search.setIndex({"alltitles": {"(One-Way) Cluster Robust Double Machine Learing": [[54, "(One-Way)-Cluster-Robust-Double-Machine-Learing"], [79, "(One-Way)-Cluster-Robust-Double-Machine-Learing"]], "0. Problem Formulation": [[145, "problem-formulation"]], "1. Data Backend": [[145, "data-backend"]], "1. Formulation of Causal Model & Identification Assumptions": [[88, "1.-Formulation-of-Causal-Model-&-Identification-Assumptions"]], "2. Causal Model": [[145, "causal-model"]], "2. Estimation of Causal Effect": [[88, "2.-Estimation-of-Causal-Effect"]], "3. ML Methods": [[145, "ml-methods"]], "3. Sensitivity Analysis": [[88, "3.-Sensitivity-Analysis"]], "4. Benchmarking Analysis": [[88, "4.-Benchmarking-Analysis"]], "4. DML Specifications": [[145, "dml-specifications"]], "5. Conclusion": [[88, "5.-Conclusion"]], "5. Estimation": [[145, "estimation"]], "6. Inference": [[145, "inference"]], "7. Sensitivity Analysis": [[145, "sensitivity-analysis"]], "A Motivating Example: Two-Way Cluster Robust DML": [[54, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"], [79, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"]], "API Reference": [[0, null]], "ATE Estimation and Sensitivity": [[75, "ATE-Estimation-and-Sensitivity"]], "ATE estimates distribution": [[57, "ATE-estimates-distribution"], [57, "id3"], [89, "ATE-estimates-distribution"], [89, "id3"]], "ATT Estimation": [[60, "ATT-Estimation"], [60, "id1"], [62, "ATT-Estimation"], [63, "ATT-Estimation"], [63, "id1"]], "ATT Estimation: Conditional Parallel Trends": [[61, "ATT-Estimation:-Conditional-Parallel-Trends"]], "ATT Estimation: Unconditional Parallel Trends": [[61, "ATT-Estimation:-Unconditional-Parallel-Trends"]], "ATTE Estimation": [[58, "ATTE-Estimation"], [58, "id2"]], "Acknowledgements": [[140, "acknowledgements"]], "Acknowledgements and Final Remarks": [[53, "Acknowledgements-and-Final-Remarks"]], "Additional Results: CATE estimates": [[82, "Additional-Results:-CATE-estimates"]], "Advanced: External Predictions": [[97, "advanced-external-predictions"]], "Advanced: Global and Local Learners, Stacked Ensembles": [[85, "Advanced:-Global-and-Local-Learners,-Stacked-Ensembles"]], "Aggregated Effects": [[60, "Aggregated-Effects"], [63, "Aggregated-Effects"]], "Aggregation Details": [[60, "Aggregation-Details"], [61, "Aggregation-Details"], [62, "Aggregation-Details"], [63, "Aggregation-Details"]], "Algorithm DML1": [[90, "algorithm-dml1"]], "Algorithm DML2": [[90, "algorithm-dml2"]], "All Combinations": [[60, "All-Combinations"]], "All combinations": [[63, "All-combinations"]], "Anticipation": [[60, "Anticipation"], [63, "Anticipation"]], "Application Results": [[54, "Application-Results"], [79, "Application-Results"]], "Application: 401(k)": [[87, "Application:-401(k)"]], "AutoML with less Computation time": [[78, "AutoML-with-less-Computation-time"]], "Average Potential Outcome (APOs)": [[66, "Average-Potential-Outcome-(APOs)"]], "Average Potential Outcomes (APOs)": [[98, "average-potential-outcomes-apos"], [113, "average-potential-outcomes-apos"], [129, "average-potential-outcomes-apos"]], "Average Potential Outcomes (APOs) for Multiple Treatment Levels": [[98, "average-potential-outcomes-apos-for-multiple-treatment-levels"]], "Average Treatment Effect": [[76, "Average-Treatment-Effect"]], "Average Treatment Effect on the Treated": [[76, "Average-Treatment-Effect-on-the-Treated"]], "Basics": [[60, "Basics"], [63, "Basics"]], "Benchmarking": [[129, "benchmarking"]], "Benchmarking Analysis": [[87, "Benchmarking-Analysis"]], "Binary Interactive Regression Model (IRM)": [[98, "binary-interactive-regression-model-irm"], [113, "binary-interactive-regression-model-irm"]], "CATEs for IRM models": [[96, "cates-for-irm-models"]], "CATEs for PLR models": [[96, "cates-for-plr-models"]], "CVaR Treatment Effects": [[71, "CVaR-Treatment-Effects"]], "CVaR of potential outcomes": [[96, "cvar-of-potential-outcomes"]], "CVaR treatment effects": [[96, "cvar-treatment-effects"]], "Causal Analysis with DoubleML": [[88, "Causal-Analysis-with-DoubleML"]], "Causal Contrasts": [[66, "Causal-Contrasts"]], "Causal Research Question": [[61, "Causal-Research-Question"]], "Causal estimation vs. lasso penalty \\lambda": [[72, "Causal-estimation-vs.-lasso-penalty-\\lambda"]], "Chernozhukov et al. (2023): Benchmarking": [[88, "Chernozhukov-et-al.-(2023):-Benchmarking"]], "Citation": [[140, "citation"]], "Cluster Robust Cross Fitting": [[54, "Cluster-Robust-Cross-Fitting"], [79, "Cluster-Robust-Cross-Fitting"]], "Cluster Robust Standard Errors": [[54, "Cluster-Robust-Standard-Errors"], [79, "Cluster-Robust-Standard-Errors"]], "Clustering and double machine learning": [[54, "Clustering-and-double-machine-learning"], [79, "Clustering-and-double-machine-learning"]], "Combined loss vs. lasso penalty \\lambda": [[72, "Combined-loss-vs.-lasso-penalty-\\lambda"]], "Compare Metrics for Nuisance Estimation": [[78, "Compare-Metrics-for-Nuisance-Estimation"]], "Comparing different learners": [[77, "Comparing-different-learners"]], "Comparison and summary": [[78, "Comparison-and-summary"]], "Comparison to AutoML with less Computation time and Untuned XGBoost Learners": [[78, "Comparison-to-AutoML-with-less-Computation-time-and-Untuned-XGBoost-Learners"]], "Comparison to did package": [[53, "Comparison-to-did-package"]], "Computation time": [[77, "Computation-time"]], "Conclusion": [[78, "Conclusion"]], "Conditional Value at Risk (CVaR)": [[71, "Conditional-Value-at-Risk-(CVaR)"]], "Conditional average treatment effects (CATEs)": [[96, "conditional-average-treatment-effects-cates"]], "Conditional value at risk (CVaR)": [[96, "conditional-value-at-risk-cvar"], [113, "conditional-value-at-risk-cvar"]], "Confidence bands and multiplier bootstrap for valid simultaneous inference": [[128, "confidence-bands-and-multiplier-bootstrap-for-valid-simultaneous-inference"]], "Control Groups": [[60, "Control-Groups"], [63, "Control-Groups"]], "Coverage Simulation": [[58, "Coverage-Simulation"], [58, "id3"]], "Cross-fitting with K folds": [[112, "cross-fitting-with-k-folds"]], "Cross-fitting, DML algorithms and Neyman-orthogonal score functions": [[142, "cross-fitting-dml-algorithms-and-neyman-orthogonal-score-functions"]], "Custom evaluation metrics": [[77, "Custom-evaluation-metrics"]], "DML: Bonus Data": [[64, null]], "Data": [[55, "Data"], [57, "Data"], [57, "id1"], [58, "Data"], [58, "id1"], [60, "Data"], [61, "Data"], [62, "Data"], [63, "Data"], [69, "Data"], [70, "Data"], [71, "Data"], [73, "Data"], [74, "Data"], [75, "Data"], [76, "Data"], [80, "Data"], [81, "Data"], [83, "Data"], [84, "Data"], [84, "id1"], [87, "Data"], [89, "Data"], [89, "id1"], [142, "data"]], "Data Backend": [[94, null]], "Data Description": [[60, "Data-Description"], [63, "Data-Description"]], "Data Details": [[60, "Data-Details"], [63, "Data-Details"]], "Data Generating Process (DGP)": [[52, "Data-Generating-Process-(DGP)"], [66, "Data-Generating-Process-(DGP)"], [68, "Data-Generating-Process-(DGP)"]], "Data Generation": [[78, "Data-Generation"]], "Data Simulation": [[51, "Data-Simulation"], [67, "Data-Simulation"]], "Data and Effect Estimation": [[87, "Data-and-Effect-Estimation"]], "Data generating process": [[91, "data-generating-process"]], "Data preprocessing": [[56, "Data-preprocessing"]], "Data with Anticipation": [[60, "Data-with-Anticipation"], [63, "Data-with-Anticipation"]], "Data-Backend for Cluster Data": [[54, "Data-Backend-for-Cluster-Data"], [79, "Data-Backend-for-Cluster-Data"]], "Dataset Generators": [[2, "dataset-generators"]], "Dataset Loaders": [[2, "dataset-loaders"]], "Datasets": [[2, null]], "Define Helper Functions for Plotting": [[54, "Define-Helper-Functions-for-Plotting"], [79, "Define-Helper-Functions-for-Plotting"]], "Demo Example from did": [[53, "Demo-Example-from-did"]], "Details on Predictive Performance": [[53, "Details-on-Predictive-Performance"]], "Difference-in-Differences": [[65, "difference-in-differences"]], "Difference-in-Differences Models": [[113, "difference-in-differences-models"], [129, "difference-in-differences-models"]], "Difference-in-Differences Models (DID)": [[98, "difference-in-differences-models-did"]], "Difference-in-Differences for Panel Data": [[129, "difference-in-differences-for-panel-data"]], "Difference-in-Differences for repeated cross-sections": [[129, "difference-in-differences-for-repeated-cross-sections"]], "Disclaimer": [[88, "Disclaimer"]], "Double Machine Learning Algorithm": [[140, "double-machine-learning-algorithm"]], "Double Machine Learning Literature": [[143, null]], "Double machine learning algorithms": [[90, null]], "Double/debiased machine learning": [[52, "Double/debiased-machine-learning"], [68, "Double/debiased-machine-learning"], [91, "double-debiased-machine-learning"]], "DoubleML": [[140, null]], "DoubleML Data Class": [[1, null]], "DoubleML Models": [[3, null]], "DoubleML Object": [[87, "DoubleML-Object"]], "DoubleML Workflow": [[145, null]], "DoubleML meets FLAML - How to tune learners automatically within DoubleML": [[78, null]], "DoubleMLData": [[94, "doublemldata"]], "DoubleMLData from arrays and matrices": [[92, "doublemldata-from-arrays-and-matrices"], [94, "doublemldata-from-arrays-and-matrices"]], "DoubleMLData from dataframes": [[92, null], [94, "doublemldata-from-dataframes"]], "DoubleMLPanelData": [[60, "DoubleMLPanelData"], [63, "DoubleMLPanelData"], [94, "doublemlpaneldata"]], "Effect Aggregation": [[61, "Effect-Aggregation"], [62, "Effect-Aggregation"], [98, "effect-aggregation"]], "Effect Heterogeneity": [[65, "effect-heterogeneity"], [76, "Effect-Heterogeneity"]], "Empirical coverage vs. lasso penalty \\lambda": [[72, "Empirical-coverage-vs.-lasso-penalty-\\lambda"]], "Estimate DML models without sample-splitting": [[112, "estimate-dml-models-without-sample-splitting"]], "Estimate double/debiased machine learning models": [[142, "estimate-double-debiased-machine-learning-models"]], "Estimating Potential Quantiles and Quantile Treatment Effects": [[81, "Estimating-Potential-Quantiles-and-Quantile-Treatment-Effects"]], "Estimating local quantile treatment effects (LQTEs)": [[81, "Estimating-local-quantile-treatment-effects-(LQTEs)"]], "Estimating the Average Treatment Effect of 401(k) Eligibility on Net Financial Assets": [[55, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"], [80, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"]], "Estimating the treatment effect on the Conditional Value a Risk (CVaR)": [[81, "Estimating-the-treatment-effect-on-the-Conditional-Value-a-Risk-(CVaR)"]], "Estimation": [[57, "Estimation"], [57, "id2"], [89, "Estimation"], [89, "id2"]], "Estimation quality vs. \\lambda": [[72, "Estimation-quality-vs.-\\lambda"]], "Evaluate learners": [[97, "evaluate-learners"]], "Event Study Aggregation": [[60, "Event-Study-Aggregation"], [61, "Event-Study-Aggregation"], [62, "Event-Study-Aggregation"], [63, "Event-Study-Aggregation"]], "Example: Sensitivity Analysis for Causal ML": [[88, null]], "Examples": [[65, null]], "Exploiting the Functionalities of did": [[53, "Exploiting-the-Functionalities-of-did"]], "Externally provide a sample splitting / partition": [[112, "externally-provide-a-sample-splitting-partition"]], "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)": [[85, null]], "Fuzzy RDD": [[85, "Fuzzy-RDD"]], "Fuzzy RDD Without Adjustment": [[85, "Fuzzy-RDD-Without-Adjustment"]], "Fuzzy RDD with Flexible Adjustment": [[85, "Fuzzy-RDD-with-Flexible-Adjustment"]], "Fuzzy RDD with Linear Adjustment": [[85, "Fuzzy-RDD-with-Linear-Adjustment"]], "Fuzzy Regression Discontinuity Design": [[98, "fuzzy-regression-discontinuity-design"]], "GATE Estimation and Sensitivity": [[75, "GATE-Estimation-and-Sensitivity"]], "GATET Estimation and Sensitivity": [[75, "GATET-Estimation-and-Sensitivity"]], "GATEs for IRM models": [[96, "gates-for-irm-models"]], "GATEs for PLR models": [[96, "gates-for-plr-models"]], "General Examples": [[65, "general-examples"]], "General algorithm": [[129, "general-algorithm"]], "Generate Fuzzy Data": [[85, "Generate-Fuzzy-Data"]], "Generate Sharp Data": [[85, "Generate-Sharp-Data"]], "Getting Started": [[142, null]], "Group Aggregation": [[60, "Group-Aggregation"], [62, "Group-Aggregation"], [63, "Group-Aggregation"]], "Group Average Treatment Effects (GATEs)": [[73, "Group-Average-Treatment-Effects-(GATEs)"], [74, "Group-Average-Treatment-Effects-(GATEs)"]], "Group average treatment effects (GATEs)": [[96, "group-average-treatment-effects-gates"]], "Group-Time Combinations": [[60, "Group-Time-Combinations"], [63, "Group-Time-Combinations"]], "Heterogeneous treatment effects": [[96, null]], "How to exploit more features of mlr3pipelines in DoubleML": [[56, "How-to-exploit-more-features-of-mlr3pipelines-in-DoubleML"]], "Hyperparameter tuning": [[97, "hyperparameter-tuning"], [97, "id16"]], "Hyperparameter tuning with pipelines": [[97, "hyperparameter-tuning-with-pipelines"]], "Implementation": [[129, "implementation"]], "Implementation Details": [[98, "implementation-details"]], "Implementation of the double machine learning algorithms": [[90, "implementation-of-the-double-machine-learning-algorithms"]], "Implementation of the score function and the estimate of the causal parameter": [[113, "implementation-of-the-score-function-and-the-estimate-of-the-causal-parameter"]], "Implemented Neyman orthogonal score functions": [[113, "implemented-neyman-orthogonal-score-functions"]], "Initialize DoubleMLClusterData object": [[54, "Initialize-DoubleMLClusterData-object"], [79, "Initialize-DoubleMLClusterData-object"]], "Initialize the objects of class DoubleMLPLIV": [[54, "Initialize-the-objects-of-class-DoubleMLPLIV"], [79, "Initialize-the-objects-of-class-DoubleMLPLIV"]], "Installing DoubleML": [[141, null]], "Instrumental Variables Directed Acyclic Graph (IV - DAG)": [[51, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"], [67, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"]], "Interactive IV Model (IIVM)": [[55, "Interactive-IV-Model-(IIVM)"], [80, "Interactive-IV-Model-(IIVM)"]], "Interactive IV model (IIVM)": [[98, "interactive-iv-model-iivm"], [113, "interactive-iv-model-iivm"]], "Interactive Regression Model (IRM)": [[55, "Interactive-Regression-Model-(IRM)"], [73, "Interactive-Regression-Model-(IRM)"], [80, "Interactive-Regression-Model-(IRM)"], [83, "Interactive-Regression-Model-(IRM)"]], "Interactive regression model (IRM)": [[129, "interactive-regression-model-irm"]], "Interactive regression models (IRM)": [[98, "interactive-regression-models-irm"], [113, "interactive-regression-models-irm"], [129, "interactive-regression-models-irm"]], "Learners and Hyperparameters": [[76, "Learners-and-Hyperparameters"]], "Learners to estimate the nuisance models": [[142, "learners-to-estimate-the-nuisance-models"]], "Learners, hyperparameters and hyperparameter tuning": [[97, null]], "Linear Covariate Adjustment": [[60, "Linear-Covariate-Adjustment"], [63, "Linear-Covariate-Adjustment"]], "Load Data": [[88, "Load-Data"]], "Load and Process Data": [[54, "Load-and-Process-Data"], [79, "Load-and-Process-Data"]], "Load bonus data using the dml datasets module": [[64, "Load-bonus-data-using-the-dml-datasets-module"]], "Local Average Treatment Effects of 401(k) Participation on Net Financial Assets": [[55, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"], [80, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"]], "Local Potential Quantile Estimation": [[84, "Local-Potential-Quantile-Estimation"]], "Local Potential Quantiles (LPQs)": [[84, "Local-Potential-Quantiles-(LPQs)"]], "Local Quantile Treatment Effects (LQTEs)": [[84, "Local-Quantile-Treatment-Effects-(LQTEs)"]], "Local potential quantiles (LPQs)": [[113, "local-potential-quantiles-lpqs"]], "Main Features": [[140, "main-features"]], "Minimum requirements for learners": [[97, "minimum-requirements-for-learners"], [97, "id2"]], "Missingness at Random": [[98, "missingness-at-random"], [113, "missingness-at-random"]], "Model-specific implementations": [[129, "model-specific-implementations"]], "Models": [[98, null]], "Motivation": [[54, "Motivation"], [79, "Motivation"]], "Multiple Average Potential Outcome Models (APOS)": [[66, "Multiple-Average-Potential-Outcome-Models-(APOS)"]], "Naive estimation": [[51, "Naive-estimation"], [67, "Naive-estimation"]], "No Clustering / Zero-Way Clustering": [[54, "No-Clustering-/-Zero-Way-Clustering"], [79, "No-Clustering-/-Zero-Way-Clustering"]], "Nonignorable Nonresponse": [[98, "nonignorable-nonresponse"], [113, "nonignorable-nonresponse"]], "One-Way Clustering with Respect to the Market": [[54, "One-Way-Clustering-with-Respect-to-the-Market"], [79, "One-Way-Clustering-with-Respect-to-the-Market"]], "One-Way Clustering with Respect to the Product": [[54, "One-Way-Clustering-with-Respect-to-the-Product"], [79, "One-Way-Clustering-with-Respect-to-the-Product"]], "One-dimensional Example": [[69, "One-dimensional-Example"], [70, "One-dimensional-Example"]], "Outcome missing at random (MAR)": [[57, "Outcome-missing-at-random-(MAR)"], [89, "Outcome-missing-at-random-(MAR)"]], "Outcome missing under nonignorable nonresponse": [[57, "Outcome-missing-under-nonignorable-nonresponse"], [89, "Outcome-missing-under-nonignorable-nonresponse"]], "Overcoming regularization bias by orthogonalization": [[52, "Overcoming-regularization-bias-by-orthogonalization"], [68, "Overcoming-regularization-bias-by-orthogonalization"], [91, "overcoming-regularization-bias-by-orthogonalization"]], "Panel Data": [[98, "id9"], [100, null], [113, "panel-data"], [113, "id3"], [129, "panel-data"]], "Panel Data (Repeated Outcomes)": [[58, "Panel-Data-(Repeated-Outcomes)"]], "Panel data": [[98, "panel-data"]], "Parameter tuning": [[56, "Parameter-tuning"]], "Parameters & Implementation": [[98, "parameters-implementation"]], "Partialling out score": [[52, "Partialling-out-score"], [68, "Partialling-out-score"], [91, "partialling-out-score"]], "Partially Linear Regression Model (PLR)": [[55, "Partially-Linear-Regression-Model-(PLR)"], [74, "Partially-Linear-Regression-Model-(PLR)"], [80, "Partially-Linear-Regression-Model-(PLR)"]], "Partially linear IV regression model (PLIV)": [[98, "partially-linear-iv-regression-model-pliv"], [113, "partially-linear-iv-regression-model-pliv"]], "Partially linear models (PLM)": [[98, "partially-linear-models-plm"], [113, "partially-linear-models-plm"], [129, "partially-linear-models-plm"]], "Partially linear regression model (PLR)": [[98, "partially-linear-regression-model-plr"], [113, "partially-linear-regression-model-plr"], [129, "partially-linear-regression-model-plr"]], "Plot Coefficients and 95% Confidence Intervals": [[78, "Plot-Coefficients-and-95%-Confidence-Intervals"]], "Policy Learning with Trees": [[83, "Policy-Learning-with-Trees"], [96, "policy-learning-with-trees"]], "Potential Quantile Estimation": [[84, "Potential-Quantile-Estimation"]], "Potential Quantiles (PQs)": [[84, "Potential-Quantiles-(PQs)"]], "Potential quantiles (PQs)": [[96, "potential-quantiles-pqs"], [113, "potential-quantiles-pqs"]], "Python: Average Potential Outcome (APO) Models": [[66, null]], "Python: Basic Instrumental Variables calculation": [[67, null]], "Python: Basics of Double Machine Learning": [[68, null]], "Python: Building the package from source": [[141, "python-building-the-package-from-source"]], "Python: Case studies": [[65, "python-case-studies"]], "Python: Choice of learners": [[77, null]], "Python: Cluster Robust Double Machine Learning": [[79, null]], "Python: Conditional Average Treatment Effects (CATEs) for IRM models": [[69, null]], "Python: Conditional Average Treatment Effects (CATEs) for PLR models": [[70, null]], "Python: Conditional Value at Risk of potential outcomes": [[71, null]], "Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments": [[86, null]], "Python: Difference-in-Differences": [[58, null]], "Python: Difference-in-Differences Pre-Testing": [[59, null]], "Python: First Stage and Causal Estimation": [[72, null]], "Python: GATE Sensitivity Analysis": [[75, null]], "Python: Group Average Treatment Effects (GATEs) for IRM models": [[73, null]], "Python: Group Average Treatment Effects (GATEs) for PLR models": [[74, null]], "Python: IRM and APO Model Comparison": [[76, null]], "Python: Impact of 401(k) on Financial Wealth": [[80, null]], "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)": [[81, null]], "Python: Installing DoubleML": [[141, "python-installing-doubleml"]], "Python: Installing a released version from a .whl file": [[141, "python-installing-a-released-version-from-a-whl-file"]], "Python: Installing the latest release from pip or conda": [[141, "python-installing-the-latest-release-from-pip-or-conda"]], "Python: Learners and hyperparameters": [[97, "python-learners-and-hyperparameters"]], "Python: Optional Dependencies": [[141, "python-optional-dependencies"]], "Python: PLM and IRM for Multiple Treatments": [[82, null]], "Python: Panel Data Introduction": [[62, null]], "Python: Panel Data with Multiple Time Periods": [[60, null]], "Python: Pepeated Cross-Sectional Data with Multiple Time Periods": [[63, null]], "Python: Policy Learning with Trees": [[83, null]], "Python: Potential Quantiles and Quantile Treatment Effects": [[84, null]], "Python: Real-Data Example for Multi-Period Difference-in-Differences": [[61, null]], "Python: Sample Selection Models": [[89, null]], "Python: Sensitivity Analysis": [[87, null]], "Quantile Treatment Effects (QTEs)": [[84, "Quantile-Treatment-Effects-(QTEs)"]], "Quantile treatment effects (QTEs)": [[96, "quantile-treatment-effects-qtes"]], "Quantiles": [[96, "quantiles"]], "R: Basic Instrumental Variables Calculation": [[51, null]], "R: Basics of Double Machine Learning": [[52, null]], "R: Case studies": [[65, "r-case-studies"]], "R: Cluster Robust Double Machine Learning": [[54, null]], "R: DoubleML for Difference-in-Differences": [[53, null]], "R: Ensemble Learners and More with mlr3pipelines": [[56, null]], "R: Impact of 401(k) on Financial Wealth": [[55, null]], "R: Installing DoubleML": [[141, "r-installing-doubleml"]], "R: Installing the development version from GitHub": [[141, "r-installing-the-development-version-from-github"]], "R: Installing the latest release from CRAN": [[141, "r-installing-the-latest-release-from-cran"]], "R: Learners and hyperparameters": [[97, "r-learners-and-hyperparameters"]], "R: Sample Selection Models": [[57, null]], "Ranking Treatment Effects under Treatment Propensity and Treatment Effect Heterogeneity": [[82, "Ranking-Treatment-Effects-under-Treatment-Propensity-and-Treatment-Effect-Heterogeneity"]], "Real-Data Application": [[54, "Real-Data-Application"], [79, "Real-Data-Application"]], "References": [[51, "References"], [53, "References"], [54, "References"], [56, "References"], [67, "References"], [72, "References"], [77, "References"], [78, "References"], [79, "References"], [82, "References"], [86, "References"], [88, "References"], [91, "references"], [97, "references"], [112, "references"], [128, "references"], [140, "references"], [142, "references"]], "Regression Discontinuity Designs (RDD)": [[98, "regression-discontinuity-designs-rdd"]], "Regularization Bias in Simple ML-Approaches": [[52, "Regularization-Bias-in-Simple-ML-Approaches"], [68, "Regularization-Bias-in-Simple-ML-Approaches"]], "Regularization bias in simple ML-approaches": [[91, "regularization-bias-in-simple-ml-approaches"]], "Release Notes": [[144, null]], "Repeated Cross-Sectional Data": [[58, "Repeated-Cross-Sectional-Data"], [113, "repeated-cross-sectional-data"], [113, "id4"], [129, "repeated-cross-sectional-data"]], "Repeated cross-fitting with K folds and M repetitions": [[112, "repeated-cross-fitting-with-k-folds-and-m-repetitions"]], "Repeated cross-sections": [[98, "repeated-cross-sections"], [98, "id10"], [100, "repeated-cross-sections"]], "Running a small simulation": [[86, "Running-a-small-simulation"]], "Sample Selection Models": [[113, "sample-selection-models"]], "Sample Selection Models (SSM)": [[98, "sample-selection-models-ssm"]], "Sample splitting to remove bias induced by overfitting": [[52, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [68, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [91, "sample-splitting-to-remove-bias-induced-by-overfitting"]], "Sample-splitting without cross-fitting": [[112, "sample-splitting-without-cross-fitting"]], "Sample-splitting, cross-fitting and repeated cross-fitting": [[112, null]], "Sandbox/Archive": [[65, "sandbox-archive"]], "Score Mixin Classes for DoubleML Models": [[49, null]], "Score functions": [[113, null]], "Selected Combinations": [[60, "Selected-Combinations"], [63, "Selected-Combinations"]], "Sensitivity Analysis": [[60, "Sensitivity-Analysis"], [63, "Sensitivity-Analysis"], [66, "Sensitivity-Analysis"], [76, "Sensitivity-Analysis"], [87, "Sensitivity-Analysis"], [87, "id1"]], "Sensitivity Analysis with IRM": [[87, "Sensitivity-Analysis-with-IRM"]], "Sensitivity analysis": [[129, null]], "Set up learners based on mlr3pipelines": [[56, "Set-up-learners-based-on-mlr3pipelines"]], "Sharp RDD": [[85, "Sharp-RDD"]], "Sharp RDD Without Adjustment": [[85, "Sharp-RDD-Without-Adjustment"]], "Sharp RDD with Flexible Adjustment": [[85, "Sharp-RDD-with-Flexible-Adjustment"]], "Sharp RDD with Linear Adjustment": [[85, "Sharp-RDD-with-Linear-Adjustment"]], "Sharp Regression Discontinuity Design": [[98, "sharp-regression-discontinuity-design"]], "Simulate two-way cluster data": [[54, "Simulate-two-way-cluster-data"], [79, "Simulate-two-way-cluster-data"]], "Simulation Example": [[87, "Simulation-Example"]], "Simultaneous inference over different DoubleML models (advanced)": [[128, "simultaneous-inference-over-different-doubleml-models-advanced"]], "Single Average Potential Outcome Models (APO)": [[66, "Single-Average-Potential-Outcome-Models-(APO)"]], "Source code and maintenance": [[140, "source-code-and-maintenance"]], "Special Data Types": [[94, "special-data-types"]], "Specify learner and estimate causal parameter: IRM model with Lasso as learner": [[64, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: IRM model with random forest as learner": [[64, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-random-forest-as-learner"]], "Specify learner and estimate causal parameter: PLR model with Lasso as learner": [[64, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: PLR model with random forest as learner": [[64, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-random-forest-as-learner"]], "Specifying alternative score functions via callables": [[113, "specifying-alternative-score-functions-via-callables"]], "Specifying learners and set hyperparameters": [[97, "specifying-learners-and-set-hyperparameters"], [97, "id9"]], "Standard approach": [[77, "Standard-approach"]], "Step 1: Custom API for FLAML Models within DoubleML": [[78, "Step-1:-Custom-API-for-FLAML-Models-within-DoubleML"]], "Step 1: Initialize and Train the AutoML Models:": [[78, "Step-1:-Initialize-and-Train-the-AutoML-Models:"]], "Step 2: Evaluate the Tuned Models": [[78, "Step-2:-Evaluate-the-Tuned-Models"]], "Step 2: Using the API when calling DoubleML\u2019s .fit() Method": [[78, "Step-2:-Using-the-API-when-calling-DoubleML's-.fit()-Method"]], "Step 3: Create and Fit DoubleML Model": [[78, "Step-3:-Create-and-Fit-DoubleML-Model"]], "Summary Figure": [[82, "Summary-Figure"]], "Summary of Results": [[55, "Summary-of-Results"], [80, "Summary-of-Results"]], "The Augmented Inverse Propensity Weighting Model estimates the ATE under arbitrary effect and propensity score heterogeneity": [[82, "The-Augmented-Inverse-Propensity-Weighting-Model-estimates-the-ATE-under-arbitrary-effect-and-propensity-score-heterogeneity"]], "The Data Backend: DoubleMLData": [[55, "The-Data-Backend:-DoubleMLData"], [80, "The-Data-Backend:-DoubleMLData"]], "The DoubleML package": [[55, "The-DoubleML-package"], [80, "The-DoubleML-package"]], "The Partially Linear Model performs overlap weighting": [[82, "The-Partially-Linear-Model-performs-overlap-weighting"]], "The basics of double/debiased machine learning": [[91, null]], "The causal model": [[142, "the-causal-model"]], "The data-backend DoubleMLData": [[142, "the-data-backend-doublemldata"]], "Theory": [[129, "theory"]], "Time Aggregation": [[60, "Time-Aggregation"], [62, "Time-Aggregation"], [63, "Time-Aggregation"]], "Tuning on the Folds": [[78, "Tuning-on-the-Folds"]], "Tuning on the full Sample": [[78, "Tuning-on-the-full-Sample"]], "Two treatment periods": [[98, "two-treatment-periods"], [113, "two-treatment-periods"], [129, "two-treatment-periods"]], "Two-Dimensional Example": [[69, "Two-Dimensional-Example"], [70, "Two-Dimensional-Example"]], "Two-Way Clustering with Respect to Product and Market": [[54, "Two-Way-Clustering-with-Respect-to-Product-and-Market"], [79, "Two-Way-Clustering-with-Respect-to-Product-and-Market"]], "Universal Base Period": [[60, "Universal-Base-Period"], [63, "Universal-Base-Period"]], "Untuned (default parameter) XGBoost": [[78, "Untuned-(default-parameter)-XGBoost"]], "Use ensemble learners based on mlr3pipelines": [[56, "Use-ensemble-learners-based-on-mlr3pipelines"]], "User Guide": [[95, null]], "Using DoubleML": [[51, "Using-DoubleML"], [67, "Using-DoubleML"]], "Using ML for DiD: Integrating DoubleML in did": [[53, "Using-ML-for-DiD:-Integrating-DoubleML-in-did"]], "Using learners from mlr3, mlr3learners and mlr3extralearners": [[56, "Using-learners-from-mlr3,-mlr3learners-and-mlr3extralearners"]], "Using pipelines to construct learners": [[97, "using-pipelines-to-construct-learners"]], "Utility Classes": [[50, "utility-classes"]], "Utility Classes and Functions": [[50, null]], "Utility Functions": [[50, "utility-functions"]], "VanderWeele and Arah (2011): Benchmarking": [[88, "VanderWeele-and-Arah-(2011):-Benchmarking"]], "Variance estimation": [[128, "variance-estimation"]], "Variance estimation and confidence intervals": [[128, null]], "Weighted Average Treatment Effects": [[96, "weighted-average-treatment-effects"]], "doubleml.data.DoubleMLClusterData": [[4, null]], "doubleml.data.DoubleMLData": [[5, null]], "doubleml.data.DoubleMLPanelData": [[6, null]], "doubleml.datasets.fetch_401K": [[7, null]], "doubleml.datasets.fetch_bonus": [[8, null]], "doubleml.datasets.make_confounded_irm_data": [[9, null]], "doubleml.datasets.make_confounded_plr_data": [[10, null]], "doubleml.datasets.make_heterogeneous_data": [[11, null]], "doubleml.datasets.make_iivm_data": [[12, null]], "doubleml.datasets.make_irm_data": [[13, null]], "doubleml.datasets.make_irm_data_discrete_treatments": [[14, null]], "doubleml.datasets.make_pliv_CHS2015": [[15, null]], "doubleml.datasets.make_pliv_multiway_cluster_CKMS2021": [[16, null]], "doubleml.datasets.make_plr_CCDDHNR2018": [[17, null]], "doubleml.datasets.make_plr_turrell2018": [[18, null]], "doubleml.datasets.make_ssm_data": [[19, null]], "doubleml.did": [[3, "doubleml-did"]], "doubleml.did.DoubleMLDID": [[20, null]], "doubleml.did.DoubleMLDIDAggregation": [[21, null]], "doubleml.did.DoubleMLDIDBinary": [[22, null]], "doubleml.did.DoubleMLDIDCS": [[23, null]], "doubleml.did.DoubleMLDIDMulti": [[24, null]], "doubleml.did.datasets.make_did_CS2021": [[25, null]], "doubleml.did.datasets.make_did_SZ2020": [[26, null]], "doubleml.double_ml_score_mixins.LinearScoreMixin": [[27, null]], "doubleml.double_ml_score_mixins.NonLinearScoreMixin": [[28, null]], "doubleml.irm": [[3, "doubleml-irm"]], "doubleml.irm.DoubleMLAPO": [[29, null]], "doubleml.irm.DoubleMLAPOS": [[30, null]], "doubleml.irm.DoubleMLCVAR": [[31, null]], "doubleml.irm.DoubleMLIIVM": [[32, null]], "doubleml.irm.DoubleMLIRM": [[33, null]], "doubleml.irm.DoubleMLLPQ": [[34, null]], "doubleml.irm.DoubleMLPQ": [[35, null]], "doubleml.irm.DoubleMLQTE": [[36, null]], "doubleml.irm.DoubleMLSSM": [[37, null]], "doubleml.plm": [[3, "doubleml-plm"]], "doubleml.plm.DoubleMLPLIV": [[38, null]], "doubleml.plm.DoubleMLPLR": [[39, null]], "doubleml.rdd": [[3, "doubleml-rdd"]], "doubleml.rdd.RDFlex": [[40, null]], "doubleml.rdd.datasets.make_simple_rdd_data": [[41, null]], "doubleml.utils.DMLDummyClassifier": [[42, null]], "doubleml.utils.DMLDummyRegressor": [[43, null]], "doubleml.utils.DoubleMLBLP": [[44, null]], "doubleml.utils.DoubleMLPolicyTree": [[45, null]], "doubleml.utils.GlobalClassifier": [[46, null]], "doubleml.utils.GlobalRegressor": [[47, null]], "doubleml.utils.gain_statistics": [[48, null]]}, "docnames": ["api/api", "api/data_class", "api/datasets", "api/dml_models", "api/generated/doubleml.data.DoubleMLClusterData", "api/generated/doubleml.data.DoubleMLData", "api/generated/doubleml.data.DoubleMLPanelData", "api/generated/doubleml.datasets.fetch_401K", "api/generated/doubleml.datasets.fetch_bonus", "api/generated/doubleml.datasets.make_confounded_irm_data", "api/generated/doubleml.datasets.make_confounded_plr_data", "api/generated/doubleml.datasets.make_heterogeneous_data", "api/generated/doubleml.datasets.make_iivm_data", "api/generated/doubleml.datasets.make_irm_data", "api/generated/doubleml.datasets.make_irm_data_discrete_treatments", "api/generated/doubleml.datasets.make_pliv_CHS2015", "api/generated/doubleml.datasets.make_pliv_multiway_cluster_CKMS2021", "api/generated/doubleml.datasets.make_plr_CCDDHNR2018", "api/generated/doubleml.datasets.make_plr_turrell2018", "api/generated/doubleml.datasets.make_ssm_data", "api/generated/doubleml.did.DoubleMLDID", "api/generated/doubleml.did.DoubleMLDIDAggregation", "api/generated/doubleml.did.DoubleMLDIDBinary", "api/generated/doubleml.did.DoubleMLDIDCS", "api/generated/doubleml.did.DoubleMLDIDMulti", "api/generated/doubleml.did.datasets.make_did_CS2021", "api/generated/doubleml.did.datasets.make_did_SZ2020", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin", "api/generated/doubleml.irm.DoubleMLAPO", "api/generated/doubleml.irm.DoubleMLAPOS", "api/generated/doubleml.irm.DoubleMLCVAR", "api/generated/doubleml.irm.DoubleMLIIVM", "api/generated/doubleml.irm.DoubleMLIRM", "api/generated/doubleml.irm.DoubleMLLPQ", "api/generated/doubleml.irm.DoubleMLPQ", "api/generated/doubleml.irm.DoubleMLQTE", "api/generated/doubleml.irm.DoubleMLSSM", "api/generated/doubleml.plm.DoubleMLPLIV", "api/generated/doubleml.plm.DoubleMLPLR", "api/generated/doubleml.rdd.RDFlex", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data", "api/generated/doubleml.utils.DMLDummyClassifier", "api/generated/doubleml.utils.DMLDummyRegressor", "api/generated/doubleml.utils.DoubleMLBLP", "api/generated/doubleml.utils.DoubleMLPolicyTree", "api/generated/doubleml.utils.GlobalClassifier", "api/generated/doubleml.utils.GlobalRegressor", "api/generated/doubleml.utils.gain_statistics", "api/mixins", "api/utility", "examples/R_double_ml_basic_iv", "examples/R_double_ml_basics", "examples/R_double_ml_did", "examples/R_double_ml_multiway_cluster", "examples/R_double_ml_pension", "examples/R_double_ml_pipeline", "examples/R_double_ml_ssm", "examples/did/py_did", "examples/did/py_did_pretest", "examples/did/py_panel", "examples/did/py_panel_data_example", "examples/did/py_panel_simple", "examples/did/py_rep_cs", "examples/double_ml_bonus_data", "examples/index", "examples/py_double_ml_apo", "examples/py_double_ml_basic_iv", "examples/py_double_ml_basics", "examples/py_double_ml_cate", "examples/py_double_ml_cate_plr", "examples/py_double_ml_cvar", "examples/py_double_ml_firststage", "examples/py_double_ml_gate", "examples/py_double_ml_gate_plr", "examples/py_double_ml_gate_sensitivity", "examples/py_double_ml_irm_vs_apo", "examples/py_double_ml_learner", "examples/py_double_ml_meets_flaml", "examples/py_double_ml_multiway_cluster", "examples/py_double_ml_pension", "examples/py_double_ml_pension_qte", "examples/py_double_ml_plm_irm_hetfx", "examples/py_double_ml_policy_tree", "examples/py_double_ml_pq", "examples/py_double_ml_rdflex", "examples/py_double_ml_robust_iv", "examples/py_double_ml_sensitivity", "examples/py_double_ml_sensitivity_booking", "examples/py_double_ml_ssm", "guide/algorithms", "guide/basics", "guide/data/base_data", "guide/data/panel_data", "guide/data_backend", "guide/guide", "guide/heterogeneity", "guide/learners", "guide/models", "guide/models/did/did_aggregation", "guide/models/did/did_binary", "guide/models/did/did_cs", "guide/models/did/did_implementation", "guide/models/did/did_pa", "guide/models/did/did_setup", "guide/models/irm/apo", "guide/models/irm/apos", "guide/models/irm/iivm", "guide/models/irm/irm", "guide/models/plm/pliv", "guide/models/plm/plr", "guide/models/ssm/ssm", "guide/resampling", "guide/scores", "guide/scores/did/did_cs_binary_score", "guide/scores/did/did_cs_score", "guide/scores/did/did_pa_binary_score", "guide/scores/did/did_pa_score", "guide/scores/irm/apo_score", "guide/scores/irm/cvar_score", "guide/scores/irm/iivm_score", "guide/scores/irm/irm_score", "guide/scores/irm/lpq_score", "guide/scores/irm/pq_score", "guide/scores/plm/pliv_score", "guide/scores/plm/plr_score", "guide/scores/ssm/mar_score", "guide/scores/ssm/nr_score", "guide/se_confint", "guide/sensitivity", "guide/sensitivity/benchmarking", "guide/sensitivity/did/did_cs_binary_sensitivity", "guide/sensitivity/did/did_cs_sensitivity", "guide/sensitivity/did/did_pa_binary_sensitivity", "guide/sensitivity/did/did_pa_sensitivity", "guide/sensitivity/implementation", "guide/sensitivity/irm/apo_sensitivity", "guide/sensitivity/irm/irm_sensitivity", "guide/sensitivity/plm/plr_sensitivity", "guide/sensitivity/theory", "index", "intro/install", "intro/intro", "literature/literature", "release/release", "workflow/workflow"], "envversion": {"nbsphinx": 4, "sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1}, "filenames": ["api/api.rst", "api/data_class.rst", "api/datasets.rst", "api/dml_models.rst", "api/generated/doubleml.data.DoubleMLClusterData.rst", "api/generated/doubleml.data.DoubleMLData.rst", "api/generated/doubleml.data.DoubleMLPanelData.rst", "api/generated/doubleml.datasets.fetch_401K.rst", "api/generated/doubleml.datasets.fetch_bonus.rst", "api/generated/doubleml.datasets.make_confounded_irm_data.rst", "api/generated/doubleml.datasets.make_confounded_plr_data.rst", "api/generated/doubleml.datasets.make_heterogeneous_data.rst", "api/generated/doubleml.datasets.make_iivm_data.rst", "api/generated/doubleml.datasets.make_irm_data.rst", "api/generated/doubleml.datasets.make_irm_data_discrete_treatments.rst", "api/generated/doubleml.datasets.make_pliv_CHS2015.rst", "api/generated/doubleml.datasets.make_pliv_multiway_cluster_CKMS2021.rst", "api/generated/doubleml.datasets.make_plr_CCDDHNR2018.rst", "api/generated/doubleml.datasets.make_plr_turrell2018.rst", "api/generated/doubleml.datasets.make_ssm_data.rst", "api/generated/doubleml.did.DoubleMLDID.rst", "api/generated/doubleml.did.DoubleMLDIDAggregation.rst", "api/generated/doubleml.did.DoubleMLDIDBinary.rst", "api/generated/doubleml.did.DoubleMLDIDCS.rst", "api/generated/doubleml.did.DoubleMLDIDMulti.rst", "api/generated/doubleml.did.datasets.make_did_CS2021.rst", "api/generated/doubleml.did.datasets.make_did_SZ2020.rst", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin.rst", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin.rst", "api/generated/doubleml.irm.DoubleMLAPO.rst", "api/generated/doubleml.irm.DoubleMLAPOS.rst", "api/generated/doubleml.irm.DoubleMLCVAR.rst", "api/generated/doubleml.irm.DoubleMLIIVM.rst", "api/generated/doubleml.irm.DoubleMLIRM.rst", "api/generated/doubleml.irm.DoubleMLLPQ.rst", "api/generated/doubleml.irm.DoubleMLPQ.rst", "api/generated/doubleml.irm.DoubleMLQTE.rst", "api/generated/doubleml.irm.DoubleMLSSM.rst", "api/generated/doubleml.plm.DoubleMLPLIV.rst", "api/generated/doubleml.plm.DoubleMLPLR.rst", "api/generated/doubleml.rdd.RDFlex.rst", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data.rst", "api/generated/doubleml.utils.DMLDummyClassifier.rst", "api/generated/doubleml.utils.DMLDummyRegressor.rst", "api/generated/doubleml.utils.DoubleMLBLP.rst", "api/generated/doubleml.utils.DoubleMLPolicyTree.rst", "api/generated/doubleml.utils.GlobalClassifier.rst", "api/generated/doubleml.utils.GlobalRegressor.rst", "api/generated/doubleml.utils.gain_statistics.rst", "api/mixins.rst", "api/utility.rst", "examples/R_double_ml_basic_iv.ipynb", "examples/R_double_ml_basics.ipynb", "examples/R_double_ml_did.ipynb", "examples/R_double_ml_multiway_cluster.ipynb", "examples/R_double_ml_pension.ipynb", "examples/R_double_ml_pipeline.ipynb", "examples/R_double_ml_ssm.ipynb", "examples/did/py_did.ipynb", "examples/did/py_did_pretest.ipynb", "examples/did/py_panel.ipynb", "examples/did/py_panel_data_example.ipynb", "examples/did/py_panel_simple.ipynb", "examples/did/py_rep_cs.ipynb", "examples/double_ml_bonus_data.ipynb", "examples/index.rst", "examples/py_double_ml_apo.ipynb", "examples/py_double_ml_basic_iv.ipynb", "examples/py_double_ml_basics.ipynb", "examples/py_double_ml_cate.ipynb", "examples/py_double_ml_cate_plr.ipynb", "examples/py_double_ml_cvar.ipynb", "examples/py_double_ml_firststage.ipynb", "examples/py_double_ml_gate.ipynb", "examples/py_double_ml_gate_plr.ipynb", "examples/py_double_ml_gate_sensitivity.ipynb", "examples/py_double_ml_irm_vs_apo.ipynb", "examples/py_double_ml_learner.ipynb", "examples/py_double_ml_meets_flaml.ipynb", "examples/py_double_ml_multiway_cluster.ipynb", "examples/py_double_ml_pension.ipynb", "examples/py_double_ml_pension_qte.ipynb", "examples/py_double_ml_plm_irm_hetfx.ipynb", "examples/py_double_ml_policy_tree.ipynb", "examples/py_double_ml_pq.ipynb", "examples/py_double_ml_rdflex.ipynb", "examples/py_double_ml_robust_iv.ipynb", "examples/py_double_ml_sensitivity.ipynb", "examples/py_double_ml_sensitivity_booking.ipynb", "examples/py_double_ml_ssm.ipynb", "guide/algorithms.rst", "guide/basics.rst", "guide/data/base_data.rst", "guide/data/panel_data.rst", "guide/data_backend.rst", "guide/guide.rst", "guide/heterogeneity.rst", "guide/learners.rst", "guide/models.rst", "guide/models/did/did_aggregation.rst", "guide/models/did/did_binary.rst", "guide/models/did/did_cs.rst", "guide/models/did/did_implementation.rst", "guide/models/did/did_pa.rst", "guide/models/did/did_setup.rst", "guide/models/irm/apo.rst", "guide/models/irm/apos.rst", "guide/models/irm/iivm.rst", "guide/models/irm/irm.rst", "guide/models/plm/pliv.rst", "guide/models/plm/plr.rst", "guide/models/ssm/ssm.rst", "guide/resampling.rst", "guide/scores.rst", "guide/scores/did/did_cs_binary_score.rst", "guide/scores/did/did_cs_score.rst", "guide/scores/did/did_pa_binary_score.rst", "guide/scores/did/did_pa_score.rst", "guide/scores/irm/apo_score.rst", "guide/scores/irm/cvar_score.rst", "guide/scores/irm/iivm_score.rst", "guide/scores/irm/irm_score.rst", "guide/scores/irm/lpq_score.rst", "guide/scores/irm/pq_score.rst", "guide/scores/plm/pliv_score.rst", "guide/scores/plm/plr_score.rst", "guide/scores/ssm/mar_score.rst", "guide/scores/ssm/nr_score.rst", "guide/se_confint.rst", "guide/sensitivity.rst", "guide/sensitivity/benchmarking.rst", "guide/sensitivity/did/did_cs_binary_sensitivity.rst", "guide/sensitivity/did/did_cs_sensitivity.rst", "guide/sensitivity/did/did_pa_binary_sensitivity.rst", "guide/sensitivity/did/did_pa_sensitivity.rst", "guide/sensitivity/implementation.rst", "guide/sensitivity/irm/apo_sensitivity.rst", "guide/sensitivity/irm/irm_sensitivity.rst", "guide/sensitivity/plm/plr_sensitivity.rst", "guide/sensitivity/theory.rst", "index.rst", "intro/install.rst", "intro/intro.rst", "literature/literature.rst", "release/release.rst", "workflow/workflow.rst"], "indexentries": {"aggregate() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.aggregate", false]], "aggregate_over_splits() (doubleml.rdd.rdflex method)": [[40, "doubleml.rdd.RDFlex.aggregate_over_splits", false]], "bootstrap() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.bootstrap", false]], "bootstrap() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.bootstrap", false]], "capo() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.capo", false]], "cate() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.cate", false]], "cate() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.cate", false]], "causal_contrast() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.causal_contrast", false]], "confint() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.confint", false]], "confint() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.confint", false]], "confint() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.confint", false]], "confint() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.confint", false]], "confint() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.confint", false]], "confint() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.confint", false]], "confint() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.confint", false]], "confint() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.confint", false]], "confint() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.confint", false]], "confint() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.confint", false]], "confint() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.confint", false]], "confint() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.confint", false]], "confint() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.confint", false]], "confint() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.confint", false]], "confint() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.confint", false]], "confint() (doubleml.rdd.rdflex method)": [[40, "doubleml.rdd.RDFlex.confint", false]], "confint() (doubleml.utils.doublemlblp method)": [[44, "doubleml.utils.DoubleMLBLP.confint", false]], "construct_framework() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.construct_framework", false]], "construct_framework() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.construct_framework", false]], "dmldummyclassifier (class in doubleml.utils)": [[42, "doubleml.utils.DMLDummyClassifier", false]], "dmldummyregressor (class in doubleml.utils)": [[43, "doubleml.utils.DMLDummyRegressor", false]], "doublemlapo (class in doubleml.irm)": [[29, "doubleml.irm.DoubleMLAPO", false]], "doublemlapos (class in doubleml.irm)": [[30, "doubleml.irm.DoubleMLAPOS", false]], "doublemlblp (class in doubleml.utils)": [[44, "doubleml.utils.DoubleMLBLP", false]], "doublemlclusterdata (class in doubleml.data)": [[4, "doubleml.data.DoubleMLClusterData", false]], "doublemlcvar (class in doubleml.irm)": [[31, "doubleml.irm.DoubleMLCVAR", false]], "doublemldata (class in doubleml.data)": [[5, "doubleml.data.DoubleMLData", false]], "doublemldid (class in doubleml.did)": [[20, "doubleml.did.DoubleMLDID", false]], "doublemldidaggregation (class in doubleml.did)": [[21, "doubleml.did.DoubleMLDIDAggregation", false]], "doublemldidbinary (class in doubleml.did)": [[22, "doubleml.did.DoubleMLDIDBinary", false]], "doublemldidcs (class in doubleml.did)": [[23, "doubleml.did.DoubleMLDIDCS", false]], "doublemldidmulti (class in doubleml.did)": [[24, "doubleml.did.DoubleMLDIDMulti", false]], "doublemliivm (class in doubleml.irm)": [[32, "doubleml.irm.DoubleMLIIVM", false]], "doublemlirm (class in doubleml.irm)": [[33, "doubleml.irm.DoubleMLIRM", false]], "doublemllpq (class in doubleml.irm)": [[34, "doubleml.irm.DoubleMLLPQ", false]], "doublemlpaneldata (class in doubleml.data)": [[6, "doubleml.data.DoubleMLPanelData", false]], "doublemlpliv (class in doubleml.plm)": [[38, "doubleml.plm.DoubleMLPLIV", false]], "doublemlplr (class in doubleml.plm)": [[39, "doubleml.plm.DoubleMLPLR", false]], "doublemlpolicytree (class in doubleml.utils)": [[45, "doubleml.utils.DoubleMLPolicyTree", false]], "doublemlpq (class in doubleml.irm)": [[35, "doubleml.irm.DoubleMLPQ", false]], "doublemlqte (class in doubleml.irm)": [[36, "doubleml.irm.DoubleMLQTE", false]], "doublemlssm (class in doubleml.irm)": [[37, "doubleml.irm.DoubleMLSSM", false]], "draw_sample_splitting() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.draw_sample_splitting", false]], "evaluate_learners() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.evaluate_learners", false]], "fetch_401k() (in module doubleml.datasets)": [[7, "doubleml.datasets.fetch_401K", false]], "fetch_bonus() (in module doubleml.datasets)": [[8, "doubleml.datasets.fetch_bonus", false]], "fit() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.fit", false]], "fit() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.fit", false]], "fit() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.fit", false]], "fit() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.fit", false]], "fit() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.fit", false]], "fit() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.fit", false]], "fit() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.fit", false]], "fit() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.fit", false]], "fit() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.fit", false]], "fit() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.fit", false]], "fit() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.fit", false]], "fit() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.fit", false]], "fit() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.fit", false]], "fit() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.fit", false]], "fit() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.fit", false]], "fit() (doubleml.rdd.rdflex method)": [[40, "doubleml.rdd.RDFlex.fit", false]], "fit() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.fit", false]], "fit() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.fit", false]], "fit() (doubleml.utils.doublemlblp method)": [[44, "doubleml.utils.DoubleMLBLP.fit", false]], "fit() (doubleml.utils.doublemlpolicytree method)": [[45, "doubleml.utils.DoubleMLPolicyTree.fit", false]], "fit() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.fit", false]], "fit() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.fit", false]], "from_arrays() (doubleml.data.doublemlclusterdata class method)": [[4, "doubleml.data.DoubleMLClusterData.from_arrays", false]], "from_arrays() (doubleml.data.doublemldata class method)": [[5, "doubleml.data.DoubleMLData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlpaneldata class method)": [[6, "doubleml.data.DoubleMLPanelData.from_arrays", false]], "gain_statistics() (in module doubleml.utils)": [[48, "doubleml.utils.gain_statistics", false]], "gapo() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.gapo", false]], "gate() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.gate", false]], "gate() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.gate", false]], "get_metadata_routing() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.get_metadata_routing", false]], "get_params() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.get_params", false]], "get_params() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.get_params", false]], "get_params() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.get_params", false]], "get_params() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.get_params", false]], "get_params() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.get_params", false]], "get_params() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.get_params", false]], "get_params() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.get_params", false]], "get_params() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.get_params", false]], "get_params() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.get_params", false]], "get_params() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.get_params", false]], "get_params() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.get_params", false]], "get_params() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.get_params", false]], "get_params() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.get_params", false]], "get_params() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.get_params", false]], "globalclassifier (class in doubleml.utils)": [[46, "doubleml.utils.GlobalClassifier", false]], "globalregressor (class in doubleml.utils)": [[47, "doubleml.utils.GlobalRegressor", false]], "linearscoremixin (class in doubleml.double_ml_score_mixins)": [[27, "doubleml.double_ml_score_mixins.LinearScoreMixin", false]], "make_confounded_irm_data() (in module doubleml.datasets)": [[9, "doubleml.datasets.make_confounded_irm_data", false]], "make_confounded_plr_data() (in module doubleml.datasets)": [[10, "doubleml.datasets.make_confounded_plr_data", false]], "make_did_cs2021() (in module doubleml.did.datasets)": [[25, "doubleml.did.datasets.make_did_CS2021", false]], "make_did_sz2020() (in module doubleml.did.datasets)": [[26, "doubleml.did.datasets.make_did_SZ2020", false]], "make_heterogeneous_data() (in module doubleml.datasets)": [[11, "doubleml.datasets.make_heterogeneous_data", false]], "make_iivm_data() (in module doubleml.datasets)": [[12, "doubleml.datasets.make_iivm_data", false]], "make_irm_data() (in module doubleml.datasets)": [[13, "doubleml.datasets.make_irm_data", false]], "make_irm_data_discrete_treatments() (in module doubleml.datasets)": [[14, "doubleml.datasets.make_irm_data_discrete_treatments", false]], "make_pliv_chs2015() (in module doubleml.datasets)": [[15, "doubleml.datasets.make_pliv_CHS2015", false]], "make_pliv_multiway_cluster_ckms2021() (in module doubleml.datasets)": [[16, "doubleml.datasets.make_pliv_multiway_cluster_CKMS2021", false]], "make_plr_ccddhnr2018() (in module doubleml.datasets)": [[17, "doubleml.datasets.make_plr_CCDDHNR2018", false]], "make_plr_turrell2018() (in module doubleml.datasets)": [[18, "doubleml.datasets.make_plr_turrell2018", false]], "make_simple_rdd_data() (in module doubleml.rdd.datasets)": [[41, "doubleml.rdd.datasets.make_simple_rdd_data", false]], "make_ssm_data() (in module doubleml.datasets)": [[19, "doubleml.datasets.make_ssm_data", false]], "nonlinearscoremixin (class in doubleml.double_ml_score_mixins)": [[28, "doubleml.double_ml_score_mixins.NonLinearScoreMixin", false]], "p_adjust() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.p_adjust", false]], "p_adjust() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.p_adjust", false]], "plot_effects() (doubleml.did.doublemldidaggregation method)": [[21, "doubleml.did.DoubleMLDIDAggregation.plot_effects", false]], "plot_effects() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.plot_effects", false]], "plot_tree() (doubleml.utils.doublemlpolicytree method)": [[45, "doubleml.utils.DoubleMLPolicyTree.plot_tree", false]], "policy_tree() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.policy_tree", false]], "predict() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.predict", false]], "predict() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.predict", false]], "predict() (doubleml.utils.doublemlpolicytree method)": [[45, "doubleml.utils.DoubleMLPolicyTree.predict", false]], "predict() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.predict", false]], "predict() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.predict", false]], "predict_proba() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.predict_proba", false]], "predict_proba() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.predict_proba", false]], "rdflex (class in doubleml.rdd)": [[40, "doubleml.rdd.RDFlex", false]], "robust_confset() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.robust_confset", false]], "score() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.score", false]], "score() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.score", false]], "score() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.score", false]], "score() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.score", false]], "sensitivity_analysis() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_analysis", false]], "sensitivity_benchmark() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_benchmark", false]], "sensitivity_plot() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_plot", false]], "set_fit_request() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.set_fit_request", false]], "set_fit_request() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.set_fit_request", false]], "set_ml_nuisance_params() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.set_ml_nuisance_params", false]], "set_params() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.set_params", false]], "set_params() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.set_params", false]], "set_params() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.set_params", false]], "set_params() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.set_params", false]], "set_sample_splitting() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.set_sample_splitting", false]], "set_score_request() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.set_score_request", false]], "set_score_request() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.set_score_request", false]], "set_x_d() (doubleml.data.doublemlclusterdata method)": [[4, "doubleml.data.DoubleMLClusterData.set_x_d", false]], "set_x_d() (doubleml.data.doublemldata method)": [[5, "doubleml.data.DoubleMLData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlpaneldata method)": [[6, "doubleml.data.DoubleMLPanelData.set_x_d", false]], "tune() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.tune", false]], "tune() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.tune", false]], "tune() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.tune", false]], "tune() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.tune", false]], "tune() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.tune", false]], "tune() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.tune", false]], "tune() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.tune", false]], "tune() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.tune", false]], "tune() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.tune", false]], "tune() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.tune", false]], "tune() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.tune", false]], "tune() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.tune", false]]}, "objects": {"doubleml.data": [[4, 0, 1, "", "DoubleMLClusterData"], [5, 0, 1, "", "DoubleMLData"], [6, 0, 1, "", "DoubleMLPanelData"]], "doubleml.data.DoubleMLClusterData": [[4, 1, 1, "", "from_arrays"], [4, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLData": [[5, 1, 1, "", "from_arrays"], [5, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLPanelData": [[6, 1, 1, "", "from_arrays"], [6, 1, 1, "", "set_x_d"]], "doubleml.datasets": [[7, 2, 1, "", "fetch_401K"], [8, 2, 1, "", "fetch_bonus"], [9, 2, 1, "", "make_confounded_irm_data"], [10, 2, 1, "", "make_confounded_plr_data"], [11, 2, 1, "", "make_heterogeneous_data"], [12, 2, 1, "", "make_iivm_data"], [13, 2, 1, "", "make_irm_data"], [14, 2, 1, "", "make_irm_data_discrete_treatments"], [15, 2, 1, "", "make_pliv_CHS2015"], [16, 2, 1, "", "make_pliv_multiway_cluster_CKMS2021"], [17, 2, 1, "", "make_plr_CCDDHNR2018"], [18, 2, 1, "", "make_plr_turrell2018"], [19, 2, 1, "", "make_ssm_data"]], "doubleml.did": [[20, 0, 1, "", "DoubleMLDID"], [21, 0, 1, "", "DoubleMLDIDAggregation"], [22, 0, 1, "", "DoubleMLDIDBinary"], [23, 0, 1, "", "DoubleMLDIDCS"], [24, 0, 1, "", "DoubleMLDIDMulti"]], "doubleml.did.DoubleMLDID": [[20, 1, 1, "", "bootstrap"], [20, 1, 1, "", "confint"], [20, 1, 1, "", "construct_framework"], [20, 1, 1, "", "draw_sample_splitting"], [20, 1, 1, "", "evaluate_learners"], [20, 1, 1, "", "fit"], [20, 1, 1, "", "get_params"], [20, 1, 1, "", "p_adjust"], [20, 1, 1, "", "sensitivity_analysis"], [20, 1, 1, "", "sensitivity_benchmark"], [20, 1, 1, "", "sensitivity_plot"], [20, 1, 1, "", "set_ml_nuisance_params"], [20, 1, 1, "", "set_sample_splitting"], [20, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDAggregation": [[21, 1, 1, "", "plot_effects"]], "doubleml.did.DoubleMLDIDBinary": [[22, 1, 1, "", "bootstrap"], [22, 1, 1, "", "confint"], [22, 1, 1, "", "construct_framework"], [22, 1, 1, "", "draw_sample_splitting"], [22, 1, 1, "", "evaluate_learners"], [22, 1, 1, "", "fit"], [22, 1, 1, "", "get_params"], [22, 1, 1, "", "p_adjust"], [22, 1, 1, "", "sensitivity_analysis"], [22, 1, 1, "", "sensitivity_benchmark"], [22, 1, 1, "", "sensitivity_plot"], [22, 1, 1, "", "set_ml_nuisance_params"], [22, 1, 1, "", "set_sample_splitting"], [22, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDCS": [[23, 1, 1, "", "bootstrap"], [23, 1, 1, "", "confint"], [23, 1, 1, "", "construct_framework"], [23, 1, 1, "", "draw_sample_splitting"], [23, 1, 1, "", "evaluate_learners"], [23, 1, 1, "", "fit"], [23, 1, 1, "", "get_params"], [23, 1, 1, "", "p_adjust"], [23, 1, 1, "", "sensitivity_analysis"], [23, 1, 1, "", "sensitivity_benchmark"], [23, 1, 1, "", "sensitivity_plot"], [23, 1, 1, "", "set_ml_nuisance_params"], [23, 1, 1, "", "set_sample_splitting"], [23, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDMulti": [[24, 1, 1, "", "aggregate"], [24, 1, 1, "", "bootstrap"], [24, 1, 1, "", "confint"], [24, 1, 1, "", "fit"], [24, 1, 1, "", "p_adjust"], [24, 1, 1, "", "plot_effects"], [24, 1, 1, "", "sensitivity_analysis"], [24, 1, 1, "", "sensitivity_benchmark"], [24, 1, 1, "", "sensitivity_plot"]], "doubleml.did.datasets": [[25, 2, 1, "", "make_did_CS2021"], [26, 2, 1, "", "make_did_SZ2020"]], "doubleml.double_ml_score_mixins": [[27, 0, 1, "", "LinearScoreMixin"], [28, 0, 1, "", "NonLinearScoreMixin"]], "doubleml.irm": [[29, 0, 1, "", "DoubleMLAPO"], [30, 0, 1, "", "DoubleMLAPOS"], [31, 0, 1, "", "DoubleMLCVAR"], [32, 0, 1, "", "DoubleMLIIVM"], [33, 0, 1, "", "DoubleMLIRM"], [34, 0, 1, "", "DoubleMLLPQ"], [35, 0, 1, "", "DoubleMLPQ"], [36, 0, 1, "", "DoubleMLQTE"], [37, 0, 1, "", "DoubleMLSSM"]], "doubleml.irm.DoubleMLAPO": [[29, 1, 1, "", "bootstrap"], [29, 1, 1, "", "capo"], [29, 1, 1, "", "confint"], [29, 1, 1, "", "construct_framework"], [29, 1, 1, "", "draw_sample_splitting"], [29, 1, 1, "", "evaluate_learners"], [29, 1, 1, "", "fit"], [29, 1, 1, "", "gapo"], [29, 1, 1, "", "get_params"], [29, 1, 1, "", "p_adjust"], [29, 1, 1, "", "sensitivity_analysis"], [29, 1, 1, "", "sensitivity_benchmark"], [29, 1, 1, "", "sensitivity_plot"], [29, 1, 1, "", "set_ml_nuisance_params"], [29, 1, 1, "", "set_sample_splitting"], [29, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLAPOS": [[30, 1, 1, "", "bootstrap"], [30, 1, 1, "", "causal_contrast"], [30, 1, 1, "", "confint"], [30, 1, 1, "", "draw_sample_splitting"], [30, 1, 1, "", "fit"], [30, 1, 1, "", "sensitivity_analysis"], [30, 1, 1, "", "sensitivity_benchmark"], [30, 1, 1, "", "sensitivity_plot"], [30, 1, 1, "", "set_sample_splitting"]], "doubleml.irm.DoubleMLCVAR": [[31, 1, 1, "", "bootstrap"], [31, 1, 1, "", "confint"], [31, 1, 1, "", "construct_framework"], [31, 1, 1, "", "draw_sample_splitting"], [31, 1, 1, "", "evaluate_learners"], [31, 1, 1, "", "fit"], [31, 1, 1, "", "get_params"], [31, 1, 1, "", "p_adjust"], [31, 1, 1, "", "sensitivity_analysis"], [31, 1, 1, "", "sensitivity_benchmark"], [31, 1, 1, "", "sensitivity_plot"], [31, 1, 1, "", "set_ml_nuisance_params"], [31, 1, 1, "", "set_sample_splitting"], [31, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLIIVM": [[32, 1, 1, "", "bootstrap"], [32, 1, 1, "", "confint"], [32, 1, 1, "", "construct_framework"], [32, 1, 1, "", "draw_sample_splitting"], [32, 1, 1, "", "evaluate_learners"], [32, 1, 1, "", "fit"], [32, 1, 1, "", "get_params"], [32, 1, 1, "", "p_adjust"], [32, 1, 1, "", "robust_confset"], [32, 1, 1, "", "sensitivity_analysis"], [32, 1, 1, "", "sensitivity_benchmark"], [32, 1, 1, "", "sensitivity_plot"], [32, 1, 1, "", "set_ml_nuisance_params"], [32, 1, 1, "", "set_sample_splitting"], [32, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLIRM": [[33, 1, 1, "", "bootstrap"], [33, 1, 1, "", "cate"], [33, 1, 1, "", "confint"], [33, 1, 1, "", "construct_framework"], [33, 1, 1, "", "draw_sample_splitting"], [33, 1, 1, "", "evaluate_learners"], [33, 1, 1, "", "fit"], [33, 1, 1, "", "gate"], [33, 1, 1, "", "get_params"], [33, 1, 1, "", "p_adjust"], [33, 1, 1, "", "policy_tree"], [33, 1, 1, "", "sensitivity_analysis"], [33, 1, 1, "", "sensitivity_benchmark"], [33, 1, 1, "", "sensitivity_plot"], [33, 1, 1, "", "set_ml_nuisance_params"], [33, 1, 1, "", "set_sample_splitting"], [33, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLLPQ": [[34, 1, 1, "", "bootstrap"], [34, 1, 1, "", "confint"], [34, 1, 1, "", "construct_framework"], [34, 1, 1, "", "draw_sample_splitting"], [34, 1, 1, "", "evaluate_learners"], [34, 1, 1, "", "fit"], [34, 1, 1, "", "get_params"], [34, 1, 1, "", "p_adjust"], [34, 1, 1, "", "sensitivity_analysis"], [34, 1, 1, "", "sensitivity_benchmark"], [34, 1, 1, "", "sensitivity_plot"], [34, 1, 1, "", "set_ml_nuisance_params"], [34, 1, 1, "", "set_sample_splitting"], [34, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLPQ": [[35, 1, 1, "", "bootstrap"], [35, 1, 1, "", "confint"], [35, 1, 1, "", "construct_framework"], [35, 1, 1, "", "draw_sample_splitting"], [35, 1, 1, "", "evaluate_learners"], [35, 1, 1, "", "fit"], [35, 1, 1, "", "get_params"], [35, 1, 1, "", "p_adjust"], [35, 1, 1, "", "sensitivity_analysis"], [35, 1, 1, "", "sensitivity_benchmark"], [35, 1, 1, "", "sensitivity_plot"], [35, 1, 1, "", "set_ml_nuisance_params"], [35, 1, 1, "", "set_sample_splitting"], [35, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLQTE": [[36, 1, 1, "", "bootstrap"], [36, 1, 1, "", "confint"], [36, 1, 1, "", "draw_sample_splitting"], [36, 1, 1, "", "fit"], [36, 1, 1, "", "p_adjust"], [36, 1, 1, "", "set_sample_splitting"]], "doubleml.irm.DoubleMLSSM": [[37, 1, 1, "", "bootstrap"], [37, 1, 1, "", "confint"], [37, 1, 1, "", "construct_framework"], [37, 1, 1, "", "draw_sample_splitting"], [37, 1, 1, "", "evaluate_learners"], [37, 1, 1, "", "fit"], [37, 1, 1, "", "get_params"], [37, 1, 1, "", "p_adjust"], [37, 1, 1, "", "sensitivity_analysis"], [37, 1, 1, "", "sensitivity_benchmark"], [37, 1, 1, "", "sensitivity_plot"], [37, 1, 1, "", "set_ml_nuisance_params"], [37, 1, 1, "", "set_sample_splitting"], [37, 1, 1, "", "tune"]], "doubleml.plm": [[38, 0, 1, "", "DoubleMLPLIV"], [39, 0, 1, "", "DoubleMLPLR"]], "doubleml.plm.DoubleMLPLIV": [[38, 1, 1, "", "bootstrap"], [38, 1, 1, "", "confint"], [38, 1, 1, "", "construct_framework"], [38, 1, 1, "", "draw_sample_splitting"], [38, 1, 1, "", "evaluate_learners"], [38, 1, 1, "", "fit"], [38, 1, 1, "", "get_params"], [38, 1, 1, "", "p_adjust"], [38, 1, 1, "", "sensitivity_analysis"], [38, 1, 1, "", "sensitivity_benchmark"], [38, 1, 1, "", "sensitivity_plot"], [38, 1, 1, "", "set_ml_nuisance_params"], [38, 1, 1, "", "set_sample_splitting"], [38, 1, 1, "", "tune"]], "doubleml.plm.DoubleMLPLR": [[39, 1, 1, "", "bootstrap"], [39, 1, 1, "", "cate"], [39, 1, 1, "", "confint"], [39, 1, 1, "", "construct_framework"], [39, 1, 1, "", "draw_sample_splitting"], [39, 1, 1, "", "evaluate_learners"], [39, 1, 1, "", "fit"], [39, 1, 1, "", "gate"], [39, 1, 1, "", "get_params"], [39, 1, 1, "", "p_adjust"], [39, 1, 1, "", "sensitivity_analysis"], [39, 1, 1, "", "sensitivity_benchmark"], [39, 1, 1, "", "sensitivity_plot"], [39, 1, 1, "", "set_ml_nuisance_params"], [39, 1, 1, "", "set_sample_splitting"], [39, 1, 1, "", "tune"]], "doubleml.rdd": [[40, 0, 1, "", "RDFlex"]], "doubleml.rdd.RDFlex": [[40, 1, 1, "", "aggregate_over_splits"], [40, 1, 1, "", "confint"], [40, 1, 1, "", "fit"]], "doubleml.rdd.datasets": [[41, 2, 1, "", "make_simple_rdd_data"]], "doubleml.utils": [[42, 0, 1, "", "DMLDummyClassifier"], [43, 0, 1, "", "DMLDummyRegressor"], [44, 0, 1, "", "DoubleMLBLP"], [45, 0, 1, "", "DoubleMLPolicyTree"], [46, 0, 1, "", "GlobalClassifier"], [47, 0, 1, "", "GlobalRegressor"], [48, 2, 1, "", "gain_statistics"]], "doubleml.utils.DMLDummyClassifier": [[42, 1, 1, "", "fit"], [42, 1, 1, "", "get_metadata_routing"], [42, 1, 1, "", "get_params"], [42, 1, 1, "", "predict"], [42, 1, 1, "", "predict_proba"], [42, 1, 1, "", "score"], [42, 1, 1, "", "set_params"], [42, 1, 1, "", "set_score_request"]], "doubleml.utils.DMLDummyRegressor": [[43, 1, 1, "", "fit"], [43, 1, 1, "", "get_metadata_routing"], [43, 1, 1, "", "get_params"], [43, 1, 1, "", "predict"], [43, 1, 1, "", "score"], [43, 1, 1, "", "set_params"], [43, 1, 1, "", "set_score_request"]], "doubleml.utils.DoubleMLBLP": [[44, 1, 1, "", "confint"], [44, 1, 1, "", "fit"]], "doubleml.utils.DoubleMLPolicyTree": [[45, 1, 1, "", "fit"], [45, 1, 1, "", "plot_tree"], [45, 1, 1, "", "predict"]], "doubleml.utils.GlobalClassifier": [[46, 1, 1, "", "fit"], [46, 1, 1, "", "get_metadata_routing"], [46, 1, 1, "", "get_params"], [46, 1, 1, "", "predict"], [46, 1, 1, "", "predict_proba"], [46, 1, 1, "", "score"], [46, 1, 1, "", "set_fit_request"], [46, 1, 1, "", "set_params"], [46, 1, 1, "", "set_score_request"]], "doubleml.utils.GlobalRegressor": [[47, 1, 1, "", "fit"], [47, 1, 1, "", "get_metadata_routing"], [47, 1, 1, "", "get_params"], [47, 1, 1, "", "predict"], [47, 1, 1, "", "score"], [47, 1, 1, "", "set_fit_request"], [47, 1, 1, "", "set_params"], [47, 1, 1, "", "set_score_request"]]}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function"}, "terms": {"": [4, 5, 6, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 46, 47, 51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 72, 73, 74, 75, 77, 79, 80, 81, 85, 87, 88, 89, 90, 92, 93, 94, 97, 98, 100, 101, 103, 104, 111, 113, 126, 127, 128, 129, 130, 140, 142, 143, 144, 145], "0": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 141, 142, 144], "00": [60, 74, 76, 80, 81, 112], "000": [82, 86, 128, 145], "00000": 76, "000000": [60, 62, 64, 66, 76, 80, 81, 92, 94, 96, 142], "0000000": 128, "0000000000000010000100": [56, 92, 94, 142], "000000e": [60, 74, 76, 80, 81], "000004": 63, "00000591": 84, "000006": [66, 84], "000017": 84, "000025": 79, "000034": 80, "000039": 79, "000063": 61, "000064": 67, "000067": 79, "000076": 98, "000091": 79, "0001": [61, 64, 80], "000104": 61, "000112": 61, "000137": 61, "000169": 63, "000175": 63, "000178": 112, "000219": [35, 96], "000242": [36, 96], "000341": 79, "000360": 63, "000375": 63, "000433": 60, "000441": 63, "000442": 79, "000449": 61, "00047580260495": 51, "000484": 60, "000488": 79, "000494": 75, "0005": 64, "000513": 61, "000522": 79, "000527419": 112, "000537": 60, "000548": 63, "000557": 63, "000566": 63, "000576": 60, "000585": 60, "0005a80b528f": 56, "000602": 60, "000604": 60, "000612": 60, "000615": 63, "000620": 63, "000629": 63, "000640": 60, "000670": 79, "000743": 87, "0007870222": 112, "000793": 61, "0008": 61, "000915799": 128, "0009157990": 128, "000943": [69, 70], "001": [25, 51, 53, 54, 55, 56, 57, 68, 97, 98, 112, 113, 128, 142, 145], "001049": [98, 103], "001051": 79, "001234": 81, "001237": 60, "00133": 56, "00138944": [90, 113], "001403": 85, "001416": 61, "001435": 61, "001447": 61, "001471": 76, "001494": [96, 97, 98], "0015": 61, "001515": 61, "001579": 63, "0016": [55, 80], "001603": [98, 103], "001643": 61, "001698": 76, "001714": 96, "0018": [55, 80], "0019": 64, "001907": 76, "001918469": 112, "001946": 61, "001964": 61, "001968": 61, "002095": 61, "002169338": 128, "0021693380": 128, "0021693381": 128, "002277": 69, "002290": 59, "0023": 53, "002388": 78, "002392": 61, "002436": 75, "002442445": 112, "0026": 64, "002601": [98, 99], "0027": 61, "002710": 60, "002719": 61, "002779": 87, "0028": [53, 55, 80], "002821": 88, "00282133350419121": 88, "00290774": [98, 101], "002983": 79, "003": [9, 10, 26, 86], "003045": 76, "003070": 61, "003074": 98, "003081": 61, "003134": 84, "003161": 60, "003187": 69, "003191": 63, "003220": 66, "003231": 60, "003244": 61, "003306": 63, "003328": 84, "0034": 72, "003404": 66, "003415": 66, "003427": 79, "00346915": 60, "0035": 61, "003533266": 112, "003544": 61, "003607": 70, "003779": 75, "0038": 61, "003836": 84, "003924": 75, "003944": 69, "003958": 63, "003975": 69, "004": 82, "004090": 61, "00409412": [90, 113], "004114082": 112, "004134": 60, "0042": [55, 80], "004253": 66, "004373": 61, "004392": 75, "00444435": 112, "004526": 66, "004542": 76, "004594": 61, "0046": 61, "004688": 32, "0047": [55, 80], "004818101": 112, "004846": 88, "005": 82, "005069": 61, "005124": 61, "00518448": [98, 103], "005281": 60, "005339": [69, 70], "005402": 62, "005552": 61, "005857": [61, 79], "005963": 60, "005e": 98, "006055": 66, "006243843": 112, "006267": 70, "0064": 61, "006425": 81, "0065": 61, "006622": 63, "0068101213851626": 78, "006922": 64, "006958": [69, 70], "006998": 63, "007056555": 112, "007210e": 81, "007230": 61, "00728": 142, "0073": 64, "007332": 71, "007332393760465": 71, "007421": 96, "007428": 60, "00778625": 112, "0078540263583833": 78, "008": 88, "008023": 81, "008086": 60, "008103": 61, "008223": [69, 70], "008266e": 81, "008318": 60, "008487": 64, "0084871742256079": 78, "008642": 96, "008743": 61, "008883698": 113, "00888458890362062": 90, "008884589": 90, "008dbd": 82, "008e80": 82, "009": [82, 88], "009122": 84, "009150": 62, "009171": [98, 101], "009255": 69, "009329847": 113, "009428": 71, "00944171905420782": 88, "009446": 63, "00950122695463054": 90, "009501226954630540": 90, "009501227": 90, "009530": 60, "009645422": 54, "009648": 82, "009656": 84, "00972": 64, "009727": [98, 99], "009790": 81, "009944622": 112, "009986": 84, "01": [20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 45, 51, 54, 55, 56, 57, 60, 63, 69, 70, 76, 80, 81, 82, 83, 84, 85, 97, 98, 101, 103, 112, 113, 128, 142, 145], "010": 51, "0100": 61, "010213": 87, "010233": 82, "010269": 79, "010422": 60, "010450": 54, "0105": 61, "010658": 63, "010744": 61, "01091843": [98, 101], "010940": 79, "011": 82, "011131": 84, "011169": 63, "0112": 53, "011204": 76, "01128": 64, "011323": [98, 99], "011598": 84, "011764": 82, "011773": 60, "011801": 61, "0118095": 54, "011823": 87, "011988e": 84, "012075": 60, "012189": 60, "01219": 56, "012194": 61, "012375": 63, "012470": 61, "0125": 61, "012538": 60, "012733": 61, "01274": 88, "012780": 81, "012831": 88, "012842": 63, "013": 82, "013034": 88, "013128": 69, "013236": 82, "013311": 61, "013313": 78, "013450": 76, "01351638": 54, "013593": 87, "013617": 81, "013677": 85, "013712": 69, "013938": 63, "01398951": 54, "013990": 128, "014": 85, "01403089": 54, "014080": [69, 70], "014083": 61, "014203": 61, "014343": 63, "014432": 59, "014637": 79, "014681": 87, "014873e": 69, "015": 56, "015038": 71, "0151": 61, "015104": 61, "015143": 61, "015552": 69, "015565": 84, "0156853566737638": 78, "015694": 61, "015698": 84, "01574297": 84, "015743": 84, "015831": 69, "016011": 70, "016154": 79, "016200": [69, 70], "016302": 61, "016315": 73, "016323": 61, "016359": 82, "0164": 61, "01643": 143, "0166": 61, "016624": 61, "016781": 61, "016842": 60, "016900": 61, "017": 56, "017140": 69, "017393e": 128, "017581": 61, "017660": 76, "01772": 130, "017777e": 70, "017789": 61, "017800092": 128, "0178000920": 128, "0179": 61, "017949": 61, "018": [56, 82], "0180": 61, "018023": 83, "018068": 61, "018092": 96, "018148": 84, "018418": 61, "0184989": [98, 101], "018508": 69, "018604": 61, "01903": [56, 97, 140, 142], "01916030e": 112, "01925597": 54, "019261": 61, "019439633": 128, "0194396330": 128, "0194396331": 128, "0195": 61, "019595": 61, "019596": 71, "019660": [36, 96], "019769": 61, "019805": 61, "01990373": 89, "019911": 61, "019941": 61, "019974": 81, "02": [60, 63, 69, 70, 80, 81, 84, 96, 98, 101, 103, 112], "020048": 61, "02016117": 142, "020166": 84, "020271": 79, "020272": 76, "0203": 61, "020309": 61, "020360838": 128, "0203608380": 128, "0203608381": 128, "02045101": 112, "02052929": [90, 113], "02069043": 112, "02079162e": 112, "020819": 96, "02092": 142, "021269": [73, 74], "021296": 61, "02156224": 112, "02163217": 54, "021690": 74, "021823": 78, "021866": 83, "021926": 71, "022125": 60, "022155": 61, "022181": 69, "02222097": 112, "022258": 76, "022295e": 69, "0223": 61, "022372": 61, "02247976": 54, "02260304": [98, 103], "022768": 64, "022783": 87, "022915": 79, "022947": 61, "022969": 81, "023020e": [80, 81], "023052": 70, "0231": 61, "023206": 61, "023256": 84, "0234": 61, "023537": 78, "023563": 128, "0237957": 63, "023955": 81, "024112": 62, "024266": 76, "024346": 69, "024355": 59, "024364": 129, "024401": [73, 74], "024592": 61, "024604": 79, "02467": 86, "024782": 84, "024846": 63, "024926": 59, "025": [69, 70, 73, 74, 76, 82], "025077": [70, 128], "02528067": 77, "0253": 56, "025300e": 70, "025443": 64, "025496": 69, "025576": 61, "025691": 61, "0257": 53, "025813114": 128, "0258131140": 128, "02584": 56, "025841": 76, "025964": 76, "026167": 61, "0262": 61, "026603": 61, "026669": 81, "026723": 71, "026782": 63, "026822": 78, "026966": 76, "02703487": 112, "027276": 60, "027356": 61, "027583": 60, "027902": 61, "02791": 64, "027999": 61, "0281": 56, "028219": 61, "028233": 61, "028294": 60, "028520": [69, 70], "028630": [98, 99], "028685": 63, "028731": 96, "028835": 60, "02897287": 58, "028993": 61, "02900983": 84, "029010": 84, "029022": 70, "029209": 145, "029234": 61, "029238": 60, "029364": [129, 135], "0295": 61, "029699": 61, "029831": 84, "029910e": [80, 81], "02e": 55, "03": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 57, 60, 63, 66, 69, 70, 71, 75, 76, 80, 81, 84, 85, 87, 88, 98, 99, 101, 103, 112, 129, 135, 145], "030006": 61, "030059": 96, "0301": 56, "030123": [98, 101], "03018": 34, "030272": 61, "030346": 142, "030397": 61, "0304": 61, "03045": 57, "0306": 61, "0307": 56, "030791": 62, "0308": 61, "030806": 61, "030934": 84, "030962": 84, "031007": 78, "03113": 89, "031134": 97, "031156": 70, "031229": 61, "031269": 64, "0313": 61, "03144316": 112, "031501": 63, "031639": 84, "031718": 60, "031820": 70, "031895": 61, "03191": 143, "03220": 144, "0323": 53, "032344": 82, "03244552": 97, "0325": 142, "03258": 78, "032580": 78, "032587": 63, "032675": [98, 101], "032738": 78, "032941": 69, "032953": 87, "033": 82, "033061": 61, "0331": 61, "03318953": 63, "033265": 78, "033396": 61, "033567": 63, "03360444": 112, "033756": 71, "033883": 61, "033946": [73, 74], "03402455": 112, "034065": 70, "03411": 142, "034226": 81, "034337": 62, "03438": 57, "034489": 61, "034578": 61, "0346": 61, "034690": 71, "0347": 61, "03470953": 63, "034738": 61, "0348": 61, "034812763": 128, "0348127630": 128, "0348127631": 128, "034846": 80, "034873": 63, "03489": [16, 54, 79], "035014": 61, "035072": 62, "035119185": 128, "0351191850": 128, "0351191851": 128, "035264": 70, "03534806": [98, 101], "03536": 142, "03538": 56, "03539": 56, "035391": 64, "0354": 56, "035411": 142, "035441": 70, "03545": 56, "035545": 64, "035572": 64, "0356": 61, "035689": [98, 103], "035730": 84, "03574": 64, "035762": 84, "035785": 70, "035821": 61, "035830": 61, "0359": 56, "035989": 61, "0360": 61, "036083": 61, "036099": 61, "036129015": 128, "0361290150": 128, "0361290151": 128, "036143": 84, "036147": 84, "036240": 66, "036487": 63, "03666389": 112, "036707": 61, "036729": 79, "03677839": 112, "0368": 53, "036891": 61, "036945": 81, "03698487": 84, "036985": 84, "037008": [73, 74], "037114": 76, "0374": 56, "037504": 70, "037509": 89, "037529": 76, "037577": [98, 99], "037747": [69, 70], "03800628": 112, "038103": 76, "038249": 61, "038481": 61, "038572": 61, "038700": 60, "038764": 62, "038789": 60, "038845": 69, "038940e": 63, "039036": 69, "039141": 66, "03917696": [113, 128], "03920960e": 112, "039310e": 71, "0394": 61, "039460": 61, "039552": 60, "039633": 60, "039661": 76, "039682": 61, "0398": 61, "039895": 76, "039922": 61, "039926": 60, "039991e": 69, "04": [10, 40, 55, 60, 63, 66, 69, 70, 80, 81, 84, 85, 87, 98, 99, 101, 103, 112, 145], "040010": 76, "040079": 70, "040112": 128, "040139": [69, 70], "040396727": 112, "040463": 62, "040533": [113, 128], "04053339": 128, "040562": 69, "040629": 39, "040688": 69, "040778": 61, "040784": 66, "0408": 69, "040912": 70, "040919": 70, "040934": 82, "0410": 61, "041064": 63, "041147": 71, "0412": 61, "041221": 61, "041284": 71, "041370": 63, "041387": 71, "041459": 81, "041491e": 71, "041647": 63, "04165": 98, "041691": 63, "0418": 53, "041831": 71, "0419": 61, "041925": 69, "042": 82, "042034": 88, "042249": 70, "042265": 71, "042387": 61, "042415": 60, "0425": 97, "0428": 89, "042804": 76, "042822e": 81, "042844e": 84, "043082": [98, 103], "043108": 81, "043177": 61, "043238": 62, "0433": 53, "0434e374": 56, "043514": 61, "043609": 61, "043644": 60, "043694": 61, "04387": 97, "043998": 70, "044066": 60, "044113": 71, "04415": 56, "044170": 61, "044176": 76, "0442": 61, "044226": 61, "044239": 76, "04424": 56, "04444978": 128, "044449780": 128, "0445": 97, "04465": 54, "044704": 70, "04486": 142, "04487585": [129, 135], "04491": 98, "044929": 76, "04497975": [129, 135], "045": 82, "04501612": 128, "04502": [97, 113, 128], "045144": 79, "045172": 76, "045313": 69, "045379": 142, "04552": 79, "045553": 71, "045624": 59, "04563": 97, "045638": 69, "045649": 62, "045754": 84, "045840": 62, "04586": 97, "045890": 63, "045932": 84, "045984": 76, "045993": 97, "046238": 60, "04625": 97, "046451": 76, "046507": [98, 103], "046525": 96, "046527": 71, "04653976": 84, "046540": 84, "046587": 70, "0466028": 54, "046714": 61, "046728": 87, "046766": 62, "04682310e": 112, "046852": 60, "046918": 61, "046918901": 112, "046922": 97, "047034": 82, "047194": 32, "047239": 76, "047288": 96, "047451": 61, "047652e": 70, "047724": 69, "047873": 76, "047954": 79, "048220": 78, "048308": 74, "048476": 76, "048699": 89, "048723": 97, "048765": 60, "048853": 70, "049264": 66, "049541": 61, "049573": [98, 103], "049669": 63, "04973": 70, "049980": 62, "05": [40, 51, 53, 54, 55, 56, 57, 60, 63, 69, 70, 71, 72, 77, 79, 80, 81, 82, 84, 85, 88, 97, 98, 99, 101, 103, 112, 113, 128, 142, 145], "050038987": 112, "05039": 87, "05046679": 112, "050494e": 70, "050538": 70, "050629": 60, "050891": 63, "050958": 61, "051": 56, "051215": 60, "051300": 62, "051651": 85, "051867e": 71, "052": 85, "052000e": 81, "052023": 76, "052106": 63, "052244": 62, "052298": 84, "052380": 69, "0524": 61, "052429": 61, "052488": 74, "052502": 84, "052745": 71, "053": [56, 98], "053049": 70, "053051": 61, "053217": 60, "0533": 53, "053331": 71, "053342": 81, "053389": 128, "053436": 33, "053541": 84, "053558": 71, "05373617": 112, "053770": 61, "053849e": 69, "054": [51, 56, 85], "054013": 60, "054068": 79, "054162": 79, "054348": 128, "054370": 71, "054529": 128, "054771e": 84, "055165": 87, "055171": 70, "055383": 61, "055439": [78, 81], "055493": 88, "055617": 61, "055680": 128, "0558": 61, "055890": 61, "056": 85, "056227": 61, "056294": 62, "056499": 74, "056745": 69, "056764": 69, "056915": 76, "057095": 84, "057207": 63, "057471": 62, "0576": [55, 80], "057762": 84, "057792": 69, "057845": 60, "057912": 61, "057962": 71, "058042": 128, "058276": 81, "058375": 66, "058441": 62, "058463": 84, "058499": 61, "058508": 89, "058595": 69, "058700": 63, "0588": 61, "058837": 62, "05891": 98, "0590": 53, "059128": 69, "059352": 63, "059384": 84, "059627": 81, "059630": 59, "059662": 63, "059685": 84, "06": [9, 10, 26, 60, 63, 66, 69, 70, 71, 80, 81, 84, 96, 97, 145], "0600": 61, "060000": 61, "060016": 66, "06008533": 98, "060201": 84, "060212": [80, 81], "060306": 61, "060417": 69, "060581": 77, "060845": 128, "060933": 69, "061": 82, "061036": 60, "0611": 53, "06111111": 56, "061123": 61, "061479": 63, "0615": 53, "061958": 61, "062": [85, 98], "062265": 61, "062414": 81, "062478": 63, "062507": 84, "06269": 86, "0628": 53, "062805": 62, "062964": 128, "062988": 69, "063017": 66, "063163": 61, "0632": 53, "063234e": 70, "06329768": [98, 101], "063326": 60, "063327": 76, "063354": 62, "063454": 61, "0635": 53, "063593": 70, "0636": 53, "063643": 60, "063700": 69, "0638": 53, "063881": 98, "063912": 60, "063986": 62, "0640": 53, "064161": 81, "064213": 70, "06428": 80, "064280": 80, "064283450": 112, "064440": 60, "0645": 53, "064598": 62, "0646222": 55, "0647": 53, "0649": 53, "064918": 60, "064960e": 60, "065": 88, "0653": 53, "065356": [73, 74], "065368": 78, "0654": 53, "065413": 62, "065451": 81, "0655": 53, "065547": 62, "065593": 62, "065681": 61, "065725": 71, "065834": 62, "0659": 53, "065969": 98, "065976": 76, "065986": 61, "066067": 60, "06614": 62, "06615948": 63, "0662": 53, "066295": 76, "0663": 61, "066464": 87, "066483": 60, "066525": 63, "0666732734": 112, "066686": 62, "0667": 61, "066718": 61, "066889": 84, "0669": 53, "06692492": 112, "067045": 60, "067046e": 69, "0671": 53, "06713714": 112, "067212": 76, "067240": 84, "06724028": 84, "067243": 60, "0673": 53, "0675": 53, "067528": 88, "067721": 128, "067803": 62, "068073": 70, "06811": 98, "068147": 61, "06827": 87, "06834315": 58, "068352": 61, "068377": 81, "068453": 62, "068514": 69, "068598": 60, "068700": 96, "068934": 66, "06895837": 54, "069443": 66, "0695854": 54, "069589": 76, "069600": 81, "069882e": 69, "07": [63, 69, 70, 81, 84, 85, 88, 112], "070020": 84, "070157": 62, "07016": 60, "070196": 71, "0701961897676835": 71, "0702127": 54, "070328": 63, "070370": 60, "0704": [53, 61], "070433": 76, "070497": 88, "070534": 37, "070552": 69, "070574e": 81, "0707": 53, "070751": 69, "07085301": 98, "070884": 84, "0711": 53, "071285": 128, "07136": [54, 79], "071362": 69, "071446": 61, "071488e": 71, "0716": 53, "07168291": 54, "071777": 97, "071782": [36, 96], "071864": 62, "0719": 53, "07202564": [73, 74], "072146": 60, "07222222": 56, "072293": 83, "07229774": [98, 103], "072516": 76, "072605": 66, "0727": 98, "073": 85, "073013": 84, "073207": 79, "073275": 69, "073325": 61, "073384": 76, "073447": [98, 103], "07347676": 54, "07350015": [16, 19, 54, 79], "073520": 71, "073527": 61, "0736": 53, "07366": [56, 97], "073694": 70, "073743": 61, "073860": 63, "073878": 60, "0739130271918385": 78, "073929": 78, "0743": 53, "074304": 128, "07436521": 112, "074426": 84, "07456127": 54, "074617": 70, "07479278": 87, "074905593": 112, "074916": 61, "074927": 66, "075261": 59, "075384": 84, "07538443": 84, "07544271e": 112, "07561": 142, "07564554e": 112, "075712": 60, "0758": 88, "075809": 66, "075869": 97, "075942": 78, "076019": 80, "076156": 128, "076179312": 128, "0761793120": 128, "076322": 84, "076347": 71, "076422": 61, "0765": 56, "076596": 69, "076649589": 112, "076653": [98, 103], "076684": 142, "076767": 61, "07685043": 112, "07689": 56, "07691847": 112, "076953": [73, 74], "076971": 64, "077144e": 70, "077161": 81, "077274": 61, "07727773e": 112, "077319": 84, "077338": 61, "077502": [129, 135], "077555": 69, "077592": 76, "077702": 66, "077762": 63, "0777777777777778": 97, "07777778": [56, 97], "07781396": 112, "077840": 81, "077883": 84, "07788588": [98, 103], "077923e": 69, "07796": 98, "078": 82, "078017": 69, "078096": 128, "07812014": 112, "078207": 64, "078256": 61, "07828372": 128, "078328e": 63, "078426": 96, "078474": 128, "078709": 70, "078810": 84, "078913": 62, "0790189": 112, "079085": 64, "07915": 56, "07919896": 112, "07942v3": 143, "079458e": 80, "079500e": 69, "07961": 87, "07978296": 112, "08": [63, 71, 81, 84, 88, 98], "080": 82, "080011713": 112, "0802": 61, "08031571": 112, "080514": 60, "080854": 81, "0809": 61, "08091581": 112, "080918": 62, "080947": 64, "081": 56, "081026": 61, "081100": 84, "08111892": 112, "081230": [69, 70], "081396": 74, "081488": 79, "08154161": 112, "081586": 60, "081622": 62, "08181827e": 112, "0820": 53, "082090": 60, "082197": 76, "082263": 38, "082297": 112, "082309": 61, "082400e": 69, "082574": 33, "082804": 59, "082858": 70, "082905": [98, 103], "082934": 81, "08293718": 112, "082973": 79, "083": 82, "083258": 128, "083318": 128, "08333333": 56, "08333617": 112, "083355": 61, "0835771416": 54, "0836": 76, "08364": 76, "083706": 88, "0837381": 112, "083750": 81, "083847": 61, "083949": 88, "084": 54, "084007": 75, "084156": 70, "084184": 71, "0841842065698133": 71, "084269": 81, "084323": 69, "084337": 98, "084633": 73, "084817": 61, "085336": 60, "0853505": 54, "085395": 69, "08556": 112, "085566": 71, "085592": 76, "085671": 69, "085794": 62, "085965": 81, "086004": 76, "08602774e": 112, "086104": 61, "08615": 112, "0862": 140, "086264": 71, "08664208": 112, "086679": 97, "086772": 63, "08682821": 112, "086854": 63, "086889": 73, "08696538": 61, "087146475": 112, "0872": 53, "087222": 70, "087236": 63, "08726531": 61, "087273": 82, "08755926": 112, "087561": 69, "087634": 69, "087745": 70, "087947": 84, "08795033": 112, "088048": 84, "088282": 74, "088357": 84, "088457": 62, "08848": 97, "088482": [36, 96], "088504e": 38, "08858972": 112, "08870909": 112, "08888889": 56, "088935": [98, 101], "089346": 61, "0894": 53, "08968939": 54, "089964": 78, "089996": 60, "08e": 55, "09": [69, 70, 71, 80, 81, 84, 96], "09000000000000001": 97, "090025": 81, "09015": 53, "090255": 84, "090436": 70, "090758": 60, "091158": 60, "091179e": 69, "091263": 78, "091391": 128, "091406": 129, "091535": 69, "0916": 53, "091824": 70, "091992": 83, "092": 82, "092229": 88, "092247": 84, "092263": 98, "092365": 128, "092637": 63, "092685": 60, "092771": 62, "092919": 130, "092930": 60, "092935": 69, "093043": 84, "09310496": 128, "093153": 84, "093474": 84, "09347419": 84, "0935": 98, "09351167": 98, "09372905": 112, "093746": 128, "093950": 79, "094004": 60, "094026": 79, "094118": 84, "094378e": 69, "094381": 79, "09444444": 56, "094549": 63, "094581e": 70, "094624": 112, "094829": 98, "094999": 84, "095104": 66, "095302": 63, "095649": 61, "095654": 69, "095781": 31, "095785": 66, "095813": 60, "09603": 140, "096163": 60, "096337": 79, "096418": 66, "096550": 73, "096613": 60, "096616": 96, "096688": 70, "096741": 58, "09682314": 98, "096884": 60, "096915": 88, "097": 85, "097009": 76, "097125": 60, "097157": 88, "097432": 60, "097468": 71, "09756": 86, "097571": 61, "09779675": 128, "097796750": 128, "098": 55, "098159": 60, "098256": 84, "09830758": 87, "098308": 87, "098317": 81, "098319": 84, "0986": 53, "098712": 84, "09879814e": 112, "09888240": 112, "098901": 76, "099": 85, "099001": 70, "099058": 61, "099307": 70, "099485": [98, 99], "099589": 60, "099602": 60, "099647": 83, "099670": 81, "099731": [69, 70], "09980311": 128, "09988": 143, "0_": 15, "0ff823b17d45": 56, "0x1747bdd4520": 64, "0x1747bdd6b90": 64, "0x2920d7b7150": 83, "0x7f34159be3f0": 145, "0x7f3415afe600": 98, "0x7f3415afeb10": 135, "0x7f3415d4cb60": 99, "0x7f3415d80e60": 97, "0x7f3415e235c0": 98, "0x7f3415ef9100": 97, "0x7f3415ef9eb0": 97, "0x7f341622c890": 98, "0x7f3416702cf0": 98, "0x7f3416861970": 98, "0x7f3417146a50": 98, "0x7f341788d880": 128, "0x7f341788db80": 128, "0x7f34179716a0": 128, "0x7f3417cd4530": 129, "0x7f341ce64ef0": 128, "0x7f34258c6120": 99, "0x7f72a1b426f0": 88, "1": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144], "10": [7, 8, 9, 10, 12, 14, 16, 17, 19, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 129, 135, 140, 142, 143, 144, 145], "100": [16, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 56, 57, 58, 69, 70, 72, 75, 76, 77, 79, 82, 86, 88, 89, 90, 92, 94, 96, 97, 98, 100, 112, 113, 128, 129, 135, 142, 144], "1000": [25, 32, 34, 52, 58, 59, 67, 68, 73, 74, 75, 77, 78, 80, 81, 85, 87, 88, 91, 98], "10000": [51, 59, 69, 70, 80, 81, 84], "100000e": 81, "100044": 74, "100092": 60, "100154": 78, "100208": 96, "100301": 63, "100356": 71, "10038": 87, "100385": 78, "10039862": [89, 98], "100517": 128, "100630": 63, "100715": 69, "10079785": 98, "1008": 61, "100807": [69, 70], "100833": 61, "100858": 87, "10089588": 84, "10089589": 61, "100896": 84, "10092": 81, "100923": 84, "100_000": 82, "101": [9, 10, 26, 53, 85, 96, 143, 144], "10126": 81, "10127930": 128, "101279300": 128, "1015": [55, 80], "1016": [9, 10, 25, 26, 53], "1016010": 55, "1018": [81, 88], "101998": 66, "102": [92, 94, 96, 142, 144], "10235": 81, "102382": 62, "10248868": 61, "10258": 81, "102616": 71, "102775": 71, "10299": 80, "103": [69, 79, 85, 89, 96, 144], "10303466": 112, "10307": 128, "1031": 81, "103189": 81, "10348": 80, "103497": 84, "103748": 60, "1038": 81, "103806": 71, "103951906910721": 71, "103952": 71, "10396": 80, "104": [55, 80, 89, 96, 144], "10406": 81, "104087": 69, "1041": 53, "10414": 81, "10439": 60, "104459": 60, "10449": 86, "104492": 76, "1045303": 54, "104787": 79, "104849": 69, "104890": 61, "105": [15, 54, 76, 79, 96, 144], "105009": 60, "105135": 63, "105137": 61, "1052437": 61, "105303": 60, "105318": 84, "1054": 56, "105461": 96, "1055": [53, 86], "105778": 60, "106": [56, 96, 144], "10607": [64, 92, 94, 142], "10616094": 61, "10618": 81, "10637173e": 112, "106391": 128, "1065": [72, 77, 78], "106595": [98, 100], "106715": 75, "106746": 84, "107": [56, 61, 88, 96, 144], "107073": 71, "107156": 76, "107295": 128, "1073": 81, "107413": 69, "10747": [64, 92, 94, 142], "10799": 81, "108": [96, 140, 143, 144], "1080": [16, 19, 53, 54, 79], "10824": [64, 92, 94, 142], "108257e": 81, "108259": 66, "10831": [64, 92, 94, 142], "108358": 63, "10878571": 84, "108786": 84, "108870": [98, 103], "109": [69, 96], "109005": 84, "10903": 80, "109069": 128, "109079e": 84, "109273": 79, "109277": 76, "10928": 81, "1093": 72, "109454": 81, "109470": 70, "1096": [53, 86], "10967": 80, "10975807": 112, "109861": 142, "1099472942084532": 67, "10e": [71, 84], "11": [39, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 84, 85, 87, 88, 90, 92, 94, 96, 97, 98, 99, 100, 103, 112, 113, 128, 129, 135, 142, 145], "110": [96, 144], "110081": 76, "1101": 81, "110155142": 112, "11019365749799062": 88, "110194": 88, "110359": 79, "110365": 88, "110427": 82, "110681": 87, "1107": 81, "11071087": [89, 98], "110717": 128, "1109": 81, "110902": 71, "110902411746278": 71, "110936": 60, "111": [70, 96, 98, 144], "1111": [7, 8, 17, 52, 54, 68, 72, 79, 88, 91, 98, 129, 135, 140], "111164": 83, "11120": 81, "1112204425": 112, "111586": 60, "1117": [72, 77, 78], "111704": 63, "1118": 55, "111907": 60, "111985": 63, "11199615e": 112, "112": [56, 96, 144], "1120": 80, "11201086": 61, "112078": 96, "11208236": [90, 113], "1122": 81, "112216": 71, "11281836": 61, "1129": 81, "113": [7, 96, 144, 145], "113022": 76, "11311": 80, "113149": 76, "113207": 84, "113270": 71, "113415": 81, "113582": 62, "11375": 81, "113780": 79, "113952": 76, "11399": 80, "114": [96, 144], "114127": 60, "1144500": 54, "11447": 87, "114530": 73, "1145370": 54, "114570": 70, "11458": 81, "114647": 71, "1147": 53, "1148": 81, "114834": 81, "11488": 81, "11495": 81, "114989": 66, "115": [96, 144], "11500": [80, 145], "115060e": 84, "1151": 61, "1151610541568202": 78, "115296e": 81, "115297e": 80, "115407": 60, "1155142425200442": 78, "11552911": 87, "115552": 60, "11559": 81, "115636": 70, "115664": 60, "11570": 80, "115792e": 81, "115901": 66, "115972": 69, "116": [96, 144], "116027": 71, "116090": [98, 101], "11615399": 63, "11617": 81, "116274": 71, "116299": 61, "116569": 81, "1166": [80, 143], "1167": 80, "11673": 81, "11675": 81, "117": [69, 96], "11700": 145, "117072": 73, "117087": 63, "117112": 70, "117242": 84, "11724226": 84, "117366": 84, "11743": 145, "11750": 81, "117565": 62, "1176": 53, "1177": 53, "117710": 71, "117909": 63, "11792": 55, "117956": 63, "11796": 81, "118": 96, "11802": 81, "1182": 55, "118229": 60, "11823404": 88, "118255": 84, "1186": 55, "118601": 79, "11861": 55, "1187": 87, "1187339840850312": 79, "11879": 81, "118799": 81, "118938e": 98, "118952": 79, "119": [88, 96, 144], "119130": 60, "11932": 81, "11935": 87, "11942111": [98, 103], "119766": 84, "1198": [54, 79], "119820": [98, 101], "12": [21, 24, 25, 37, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 68, 69, 70, 71, 73, 74, 75, 76, 78, 79, 80, 81, 82, 84, 85, 87, 88, 90, 92, 94, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 129, 135, 140, 142, 143, 144, 145], "120": [57, 58, 78, 89, 96, 144], "12002": 80, "1200x600": [60, 61, 63], "1200x800": [60, 61, 63], "1202": 143, "120468": 84, "12046836": 84, "120567": [73, 74], "120613": 60, "120721": 79, "120775": 62, "12080467": [98, 101], "12097": [7, 8, 17, 54, 72, 79, 91, 140], "121": [81, 96, 144], "1210": 81, "12101": 81, "12105472": 128, "121054720": 128, "1211": 81, "121256": 60, "1213405": 54, "121399": 81, "1214": 128, "121435": 60, "121584e": 84, "121627": 63, "121711": 81, "121774": 75, "121824": 70, "12196389e": 112, "122": [9, 10, 26, 53, 60, 85, 92, 94, 96, 143, 144], "12214": 55, "12223182e": 112, "1222478": 112, "122408": 71, "122421": 76, "122721277": 112, "122777": 128, "123": [40, 55, 56, 60, 63, 80, 88, 96, 144, 145], "1230": 81, "123192": 88, "12323": 81, "1234": [51, 52, 53, 64, 67, 68, 85, 86, 91, 97, 112, 128], "12348": 88, "1238": 81, "123917": 81, "123950": [98, 99, 103], "124": 96, "12410": 81, "124306": 78, "124480": 78, "124805": 80, "124825": 70, "125": [96, 144], "12500": 80, "125065": 128, "12539340": 128, "1255": 81, "12579": 81, "1258": 54, "126": [96, 144], "12606": 81, "12612": 81, "126132": 60, "126215": 60, "126266": 60, "126395": 63, "126745": 60, "126777": 128, "12680172": 61, "126802": 81, "12689": 81, "127": [9, 96, 144], "127006": 81, "12705095": [113, 128], "12707800": 54, "127161": 63, "1272404618426184": 78, "127337": 76, "127385": 62, "127420": [98, 101], "12744403": 63, "12752825": 128, "127563": 87, "1277": 82, "127778": 81, "127886": 63, "127889": 96, "128": [55, 96, 144], "12802": 55, "12814": 81, "128229": 76, "128300e": 70, "128312": 84, "12833833": 63, "128408": 79, "1285": 53, "12861": 81, "128651": 70, "128763": 62, "129": [79, 96, 144], "12945": 143, "12947879": [98, 101], "1295": [53, 81], "129514": 81, "12955": 80, "129606": 69, "12967588": 112, "129798": 69, "1298": 81, "12980459": 61, "12980769e": 112, "12983057": 98, "13": [10, 12, 14, 25, 26, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 68, 69, 70, 71, 73, 74, 75, 76, 78, 79, 80, 81, 84, 85, 86, 87, 88, 90, 92, 94, 96, 97, 98, 100, 101, 112, 113, 128, 129, 135, 142, 145], "130": [56, 73, 79, 96, 144], "13003274": [98, 101], "130122": 87, "13034980e": 112, "130370": 71, "1306": 87, "130829": 84, "13084": 98, "1309": 61, "13091": 81, "1309844442144665": 78, "131": [96, 144], "13102231": 98, "131024": 78, "13119": 87, "1312": 145, "131211": 81, "1313": [55, 145], "131353": 62, "13137893e": 112, "131483": 76, "131640": 63, "1318": 53, "131842": 76, "131856": 63, "13191418": 61, "13196903": 112, "132": [56, 69, 79, 96, 144], "13208": 145, "1321": [80, 145], "132221": 63, "132248": 96, "1324": [55, 80], "132454": 59, "132481": 96, "1325": 55, "13257": 80, "132671": 71, "132679": 63, "1328": 87, "132903": 81, "132982": 69, "133": [56, 92, 94, 96, 143, 144], "13300": 81, "133202": 81, "133421": 81, "13356": 81, "133564": 61, "133593": 61, "133596": 84, "133839": 76, "13398": 88, "133f5a": 82, "134": [79, 89, 96, 144], "134037": 62, "1340371": 53, "134080": 62, "1341": 55, "134146": 81, "1341897": 61, "1342": 81, "134211": 84, "1343": 80, "134542": 69, "134567": 81, "1346035": 55, "134687": 81, "13474": 81, "134765": 81, "134784": 98, "134784e": 69, "1348": 80, "13490": 81, "135": [56, 96, 98, 144], "13505272": 54, "135142": 70, "135211": 63, "135344": 66, "135352": 20, "135379": 128, "135455": 63, "135665": 70, "135707": 97, "135856": 84, "13585644": 84, "135871": 79, "136": [51, 64, 79, 88, 96, 144], "1360": 55, "13602": 88, "136089": 79, "1361": 81, "136102": 69, "1362430723104844": 78, "13642": 81, "136442": 79, "1366": 82, "136836": 79, "137": [9, 56, 64, 96, 144], "1371": 81, "137162": 60, "137165": 98, "137213": 70, "137245": 61, "137256": 61, "1373": 61, "137396": 84, "137472": 61, "137627e": 63, "13771721": 63, "1378": 81, "137999": 98, "138": [96, 144], "1380": 80, "138068": 73, "13809": 81, "138264": 88, "138355": 61, "138378": 71, "1384": 80, "1386": 53, "13868238": 128, "138682380": 128, "138698": 128, "1387": 53, "138851": 73, "13893": 81, "139": [88, 96, 142], "139117e": 69, "13913169": 61, "139488": 60, "139491": 128, "13949701": 61, "1395": 61, "139508": 76, "13956": 87, "139567": 62, "139681": 60, "1398": 81, "139830": [98, 103], "1399": 53, "14": [51, 52, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 68, 69, 70, 71, 73, 74, 75, 76, 78, 79, 80, 81, 82, 84, 85, 87, 88, 90, 92, 94, 96, 97, 98, 100, 101, 112, 113, 128, 129, 135, 142, 143, 145], "140": [57, 58, 76, 81, 89, 96, 144], "1400": 81, "14000073": 112, "140073": [61, 69], "140081": 78, "1401": 53, "14018": 98, "14025551": 61, "140432": 62, "140770": [69, 70], "140833": 71, "140861": 54, "140891": 60, "140926": 84, "141": [81, 96, 144], "141002": 70, "141098e": 81, "14114": 87, "14141": 81, "141460": 66, "141546": 128, "141820": 71, "142": [96, 144], "14200098": 128, "142119": 69, "142270": 59, "142312350": 112, "142382": 69, "1424": 97, "14268": 98, "142702": 63, "142768": 60, "14281403493938022": 97, "14289": 81, "143": [92, 94, 96, 144], "143210": 60, "143342": 70, "143495": 96, "1435": 81, "143534": 69, "143646": 60, "14368145": 128, "144": [96, 144], "14400": 80, "14405": 81, "14406": 81, "144084": 71, "1441": 53, "144137": 58, "144241": 73, "1443": 81, "144500e": 81, "144669": 84, "1447": [63, 81], "144800": 71, "144908": 83, "144971": 80, "145": [96, 144], "145027": 66, "14521606": 61, "145245": 84, "14532650": 128, "145456": 60, "145625": 84, "14571482": 61, "145748": 128, "14587": 81, "14587412": 63, "145895": 82, "146": [51, 96, 144], "146037": 84, "146087": 142, "146142808990006": 71, "146143": 71, "14625": 81, "146435": 70, "1465": 55, "146641": 128, "14667": 81, "1468115": 54, "146973": 71, "1469734445741286": 71, "147": [96, 144], "147015e": 81, "14702": 64, "14706180": 112, "147121": 84, "147404": 60, "14744": 81, "14772": 81, "1479": [60, 81], "14790924": 128, "147909240": 128, "147917": 82, "147927": 64, "14798": 81, "148": [96, 144], "148005": 66, "14803": 81, "148134": [69, 70], "148161": 84, "148213": 61, "14845": 64, "1485": 81, "148750e": [80, 81], "148790": 81, "148802": 81, "149": [96, 144], "1492": 51, "149215e": 70, "149228": 88, "149233": 62, "149285": 84, "149472": 88, "149714": 79, "14984": 81, "149858": [35, 96], "149898": 84, "15": [8, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 54, 55, 56, 58, 60, 63, 66, 68, 69, 70, 71, 73, 75, 76, 77, 78, 79, 80, 81, 84, 85, 86, 87, 88, 92, 94, 96, 97, 98, 100, 101, 112, 113, 128, 129, 135, 142, 145], "150": [15, 56, 88, 96, 144], "15000": [55, 80], "150000": 55, "15000000000000002": [71, 81, 84, 97], "150000e": 81, "1502": 54, "150200": 79, "150334": 81, "150408": 54, "150614": 64, "150719e": 80, "151": [96, 144], "151047e": 73, "151063": 69, "15113": 81, "15154227": 61, "151636": 71, "15172334": 61, "151819": 84, "1518981": 61, "15194": 80, "152": [96, 144], "152034": 81, "152148": [69, 70], "152353": 70, "152706": 98, "15275409": 61, "15285": 81, "152896": 78, "152926": 59, "153": [82, 88, 96, 144], "1530959776797396": 71, "153096": 71, "153119": 71, "153314": 70, "15347": 81, "15354": 87, "153587": 79, "153633": 64, "153639": 112, "153935": 70, "153987": 63, "154": 96, "154106": 63, "15430": 145, "154421": 128, "1545": 81, "154557": 84, "154620": 63, "154758": 128, "154821": [98, 101], "154828": 71, "154890": 78, "155": [96, 144], "155000": 80, "1550148": 61, "155025": 84, "155120": 84, "155160": 66, "155172": 60, "155174": 66, "155423": 66, "155516": 83, "15556": 81, "1557093": 54, "156": [96, 144], "1560": 81, "156016": 60, "156021": 84, "156169": 70, "156202": [69, 70], "156317": [69, 70], "1564": 128, "156545": 128, "156684": 70, "1569": 81, "156969": 71, "157": [70, 96, 144], "157091": 128, "157154": 69, "15755985": 112, "1576": 81, "157733": 78, "1577657": 54, "157834e": 60, "157e": 98, "158": [96, 144], "158007": 84, "15815035": 55, "158178": 71, "1582": 81, "158375": 63, "1584": 112, "158461": 61, "1586": 81, "158697": 128, "158726": 96, "1588905": 61, "1589": 81, "15891559": 84, "158916": 84, "159": 144, "15915": 62, "15916": [53, 62], "159265": 63, "159386": 87, "1596": 56, "15962525": 61, "159633e": 70, "15980169": 61, "159841": 69, "159959": 81, "16": [31, 51, 52, 54, 55, 56, 57, 60, 62, 63, 66, 69, 70, 71, 74, 75, 76, 79, 80, 81, 84, 85, 87, 88, 92, 94, 96, 97, 98, 100, 101, 112, 113, 128, 129, 135, 142, 145], "160": [57, 58, 89, 144], "160178": 60, "1604": 55, "1606785": 61, "16078746": 61, "160836": 76, "160932": 71, "161": [56, 143, 144], "161049": 70, "161141": 79, "161198": 83, "161236": 84, "161243": 84, "161269": 69, "161288": [70, 78], "161543": 81, "161594": 60, "1619": [55, 61], "162": 144, "16201": 81, "16211": 80, "16211005": 63, "162153": 84, "1622": 81, "16241": 81, "162436": 88, "16247691": 61, "162593": 78, "162601": 60, "1626685": 54, "162683": 88, "162710": 71, "162755": 63, "1628": 80, "162930": 81, "163": [81, 144], "163194": 84, "163494": 60, "163519": [98, 101], "163566": 81, "16363297": 61, "163895": 71, "164": [66, 85, 144], "164034": 128, "164467": 80, "16454169": 61, "164608": 84, "164698": 75, "164727": 60, "1648": 53, "164801": 84, "164805": 71, "164864": 79, "164946": 62, "165": 144, "16500": 80, "165130": 63, "165178": 84, "1652782681": 112, "16536299": 128, "165362990": 128, "16539906e": 112, "1654": 81, "165419": 84, "16553": 80, "165549": 142, "165707": 66, "165797": 63, "16590": 81, "16597": 81, "166": [82, 144], "166044": 60, "166076": 63, "1661": 80, "166238": 78, "166375": 96, "166517": 76, "166870": 60, "167": [55, 80, 144], "16725": 81, "167490": 60, "167547": 84, "167581e": 69, "1676": 81, "167765": 81, "167825": 63, "167993": 128, "168": 144, "16803512": 128, "168089": 78, "168092": 128, "1681": [53, 80], "168195": 87, "1686": 61, "168605": 61, "168614": 84, "168679": 60, "16877633": 112, "168931": 84, "169": [56, 144], "1691": [53, 81], "16910": 81, "169117": 88, "169196": 84, "169230e": 71, "16951": 81, "16984": 81, "17": [51, 52, 54, 55, 56, 60, 63, 66, 69, 70, 75, 76, 79, 80, 81, 84, 85, 87, 88, 92, 94, 96, 97, 98, 100, 112, 113, 128, 129, 135, 142, 145], "170": 144, "1704": 81, "170457": 60, "170496": 60, "170591": 61, "170705": 96, "170709e": 70, "17083": 81, "170933": [98, 103], "171": 144, "1712": 143, "171300": 61, "1714": 55, "171575": 84, "171696": 96, "171709": 60, "171815": 97, "171833": 70, "171848e": 69, "1719": [60, 61, 62, 63], "171942": 81, "17196599": 61, "172": [85, 144], "172022": 128, "172083": 70, "1721524": 61, "172269": 62, "17247912": 61, "172495": 63, "172628": 78, "172793": 84, "17281952": 61, "173": 144, "173090": 60, "173339": 63, "173504": 74, "17354773": 61, "17354827": 61, "17372": 81, "1738": 81, "17385178": 97, "173969": 128, "173e": 85, "174": 144, "174185": 84, "174499": 128, "174516e": 84, "17453": 81, "1746": 81, "174835": 70, "174968": 80, "17499": 81, "175": 144, "1751": 80, "175176": 84, "17522": 81, "175254": 80, "175284": 71, "175369": 70, "175635027": 54, "17576": 81, "175780": 62, "175894": 88, "175931": [96, 97, 98], "176495": 84, "17655394": 84, "176554": 84, "1767": 112, "176929": 128, "177": [143, 144], "177007": 84, "17700723": 84, "177043": [69, 70], "177076": 60, "1773": 81, "1774": 53, "177463": 83, "177496": 84, "177544": 60, "177611": 84, "177740": 69, "177751": 84, "17778": 81, "177785e": 63, "177830": 70, "17799": 81, "177995": 84, "178": 144, "178169": 73, "178218": 70, "17823": 56, "178704": 128, "178763": 84, "1788123": 63, "178934": 128, "178949": 60, "178971": 62, "179": [73, 144], "179026": 70, "179101": 96, "179364": 63, "1795850": 54, "179588e": 84, "179777": 70, "1798913180930109556": 82, "18": [52, 54, 55, 56, 60, 63, 64, 69, 70, 75, 76, 77, 79, 80, 81, 84, 85, 87, 88, 92, 94, 96, 97, 98, 100, 112, 128, 142, 145], "180": [57, 58, 89, 144], "180143": 66, "18015": 81, "180176e": 81, "180262": 70, "180271": 76, "1803": 53, "18030": 81, "180405": 60, "180464": 60, "180575": [73, 74], "180635": 60, "1807": [53, 81], "1809": 143, "180951": 84, "180955": 63, "181": [51, 63, 144], "1812": 81, "1814": 53, "18141": 81, "181446": 128, "18163923": 61, "182": 144, "1820": 53, "18226727": 61, "182427": 70, "182633": 84, "18273096": [98, 101], "182849": 84, "182919": 63, "183": [56, 60, 80, 98, 144], "183052": 60, "183339": 69, "183373": 98, "183526": 71, "183553": 85, "18356413": 98, "18368": 81, "183855": 97, "183888": 79, "184": [56, 143, 144], "184224": 66, "184247": 69, "184303": [98, 103], "184347": 70, "184369": 60, "185": [55, 56], "18500": 81, "185130": 85, "18516129": [98, 103], "18529341": 112, "1855": 81, "185940": 62, "185984": 69, "186": [63, 81, 144], "18604": 81, "186140": 60, "1862": 53, "186237": 70, "18631": 81, "18637": 140, "186589": 66, "18666": 81, "186689": 98, "186735": 84, "186775": [98, 99, 103], "18678094e": 112, "186836": 84, "186961": 63, "187": 144, "187153": 128, "187402": 63, "187664": 69, "187690": 84, "18789": 81, "188": [63, 144], "188087e": 63, "188175": 84, "1881752": 84, "188223": 84, "18824114": 112, "188400": 70, "188541": [98, 99], "1887": [96, 97, 98], "188760": 76, "18888149e": 112, "188882": 70, "188991": 128, "189": [56, 60, 82, 85, 144], "189195": 81, "189248": 69, "189293": 81, "189539": 62, "1895815": [16, 54, 79], "189737": 84, "189739": 66, "189774": 62, "189927": 81, "189998": 84, "19": [52, 54, 55, 56, 60, 63, 69, 70, 76, 78, 79, 80, 81, 84, 85, 87, 88, 96, 97, 98, 100, 101, 112, 128, 142, 145], "190": [56, 144], "19000": 81, "190096": 128, "19031969": 84, "190320": 84, "19033538": 54, "19053": 63, "190534": [98, 101], "190648": 32, "19073905e": 112, "190809": 84, "190892": 88, "1909": [16, 54, 79], "190915": 71, "190921": 74, "190976": 85, "190982": 84, "191": [56, 60, 143, 144], "191019212527283342445860686972949598100": 112, "191019212527283342445860686972949598100371516202223434751626364747781848591964818353839414853545556575973768288899956121417262930323437465052667980838790": 112, "191192": 69, "1912": 143, "191223": 70, "1912705": 91, "191294": 70, "191373": 60, "191534": 80, "191716": 81, "1918": 53, "192": [63, 82, 144], "1922": 81, "192240": 128, "192441": 60, "192505": 83, "192526": 87, "19252647": 87, "192539": [36, 96], "192587": 84, "192952": 66, "193": [63, 144], "193060": 84, "193253": 69, "193285": 69, "193308": [36, 96], "193341": 70, "193375": 76, "19356": 60, "19374710e": 112, "19382": 81, "193849": 85, "19385": 81, "193f0d909729": 56, "194": [60, 63, 77, 81, 144], "194092": 69, "1941": 55, "19413": [80, 81], "194303": 70, "194601": 58, "194610": 60, "194786": [98, 99], "195": [60, 63, 93, 94, 144], "19508": 88, "19508031003642456": 88, "19509680e": 112, "195322": 62, "195377": 84, "195396": 84, "195547": 81, "195564": 79, "19559": [55, 80], "195761": 84, "195781": 69, "1959": 143, "195963": 76, "196": [63, 144], "196037e": 63, "196189": 84, "196437": 81, "196478e": 70, "196655": 76, "19680840": 128, "196e": 40, "197": 144, "1970": 81, "197000e": 81, "19705": 81, "197225": [64, 92, 94, 142], "1972250000001000100001": [56, 92, 94, 142], "1974": 81, "197424": 97, "197484": 128, "19756": 81, "19758": 81, "197600": 59, "197711": 81, "197920": 69, "19793": 81, "19794": 81, "197980": 60, "198": [60, 63, 144], "198082198": 112, "198218": 79, "19824": 81, "198351": 84, "198493": 76, "198500": 63, "198503": 85, "198549": 64, "198687": 55, "1988": [52, 68, 91, 98], "199": 144, "1990": [55, 80, 81], "1991": [55, 80, 81, 145], "199281e": 84, "199282e": 81, "199412": 70, "199458": 128, "1995": [54, 79], "199570": 61, "1998": 82, "19983954": 89, "199893": 73, "1999": [82, 89], "1_": [71, 84], "1d": 21, "1e": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 76, 81], "1f77b4": 59, "1m": 97, "1x_4x_3": 59, "2": [7, 9, 10, 11, 12, 13, 14, 15, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 45, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 107, 112, 113, 128, 129, 130, 132, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144], "20": [9, 10, 12, 13, 14, 16, 17, 18, 20, 22, 23, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 55, 56, 57, 58, 60, 63, 69, 70, 71, 73, 75, 76, 77, 79, 80, 81, 84, 87, 88, 89, 90, 91, 96, 97, 98, 100, 112, 113, 128, 129, 135, 142, 145], "200": [11, 14, 15, 25, 53, 57, 58, 60, 63, 71, 72, 77, 83, 84, 89, 91, 97, 143, 144], "2000": [8, 37, 55, 57, 66, 69, 70, 71, 76, 80, 81, 84, 86, 89, 96, 98], "20000": [55, 80], "20000000000000004": [71, 81, 84], "200000e": 81, "200023": 63, "200049": 69, "2001": 61, "20010": 81, "200110": 81, "2003": [7, 61, 143], "200303": 142, "2004": [61, 98, 102], "2005": [58, 61], "20055": 81, "2006": [61, 81, 98, 102], "20065403": 61, "2007": [61, 98, 102], "200707": 63, "20073763": 77, "20074": 81, "200790751": 112, "200827": 60, "20085038": 61, "200965": 63, "201": [56, 60, 63, 81, 144], "2010": [54, 79], "2011": [54, 79, 140, 142], "201103": 60, "2013": [72, 128, 143], "2014": [128, 143], "201413": 63, "2015": [15, 143], "201514": [98, 101], "20152486": 61, "201528": [69, 70], "201549": 63, "20158": 81, "2016": 82, "2017": [13, 143], "201768": 79, "201788e": 81, "2018": [7, 8, 17, 18, 52, 54, 55, 58, 68, 72, 77, 79, 80, 81, 86, 87, 91, 98, 104, 112, 113, 121, 128, 140, 143, 144], "2019": [11, 56, 69, 70, 71, 73, 74, 81, 84, 87, 97, 113, 119, 122, 123, 140, 142, 143], "201e": 85, "202": 144, "2020": [9, 10, 12, 14, 20, 22, 23, 25, 26, 53, 56, 58, 60, 63, 88, 97, 98, 100, 104, 129, 130, 143], "2020435": 54, "2021": [16, 25, 53, 54, 56, 60, 61, 62, 63, 69, 70, 79, 98, 99, 102, 104, 143, 144], "20219609": 54, "2022": [87, 88, 98, 100, 104, 129, 130, 139, 140, 143], "2023": [19, 57, 86, 89, 98, 111, 113, 126, 127, 143], "2024": [51, 67, 72, 77, 78, 82, 85, 86, 88, 98, 140, 143], "2025": [25, 60, 63, 86, 98, 99, 101, 103], "2025073": 61, "202603": 70, "202650e": 71, "20269": 81, "20274": 81, "202846": 70, "203": [55, 69, 80, 144], "203010": 63, "203284": 71, "20329": 81, "203577": 60, "2036": 81, "203828": 81, "204": [60, 63, 144], "204007": 84, "20400735": 84, "204148": 63, "204283": 63, "204362": 88, "204455": 70, "204482": 84, "204794": 84, "204893": 66, "205": [85, 87, 144], "205187": 71, "205224": 87, "205333e": 80, "20539721": 63, "20575459": 61, "205938": 79, "205969": 60, "206": 144, "2061": 81, "206186": 63, "206232": 63, "206253": [80, 81], "206256": 76, "2064": 81, "206614": 84, "207": [63, 85, 98, 144], "20704478": 61, "2075": 53, "207794": 61, "2078": 61, "207834": 70, "20783816": 54, "207840": 74, "207885": 80, "207912": 128, "208": [60, 63, 66, 144], "208034e": 81, "208044": 63, "2080787": 54, "20823898": 54, "208562": 61, "2086": 81, "208700": 61, "209": 66, "209014": 84, "209219e": 87, "209257": 20, "209546e": 81, "209894": 84, "21": [7, 8, 17, 52, 54, 55, 56, 60, 63, 69, 70, 72, 79, 80, 81, 84, 86, 87, 88, 91, 96, 97, 98, 100, 112, 128, 140, 142, 143, 145], "210": [10, 14, 25, 26, 51, 60, 63, 66, 82], "2103": [81, 140], "2103034": 54, "210319": [69, 70], "210323": 84, "2104": 144, "2107": 143, "21078": 81, "211": [60, 63, 66, 82, 85, 144], "211002": [98, 103], "211024": 60, "21105": [56, 97, 140, 142], "211132431364045496165677071757886929397": 112, "2111324313640454961656770717578869293971910192125272833424458606869729495981003715162022234347516263647477818485919648183538394148535455565759737682888999": 112, "2111324313640454961656770717578869293971910192125272833424458606869729495981003715162022234347516263647477818485919656121417262930323437465052667980838790": 112, "2111324313640454961656770717578869293971910192125272833424458606869729495981004818353839414853545556575973768288899956121417262930323437465052667980838790": 112, "211132431364045496165677071757886929397371516202223434751626364747781848591964818353839414853545556575973768288899956121417262930323437465052667980838790": 112, "2112": 88, "21142": 81, "211534": 71, "21155656": 84, "211557": 84, "211570": 60, "212": [60, 63, 66, 82, 144], "2122": 81, "21257396e": 112, "212811": 66, "212844": 79, "212863": 69, "213": [63, 66, 85, 143, 144], "213026": 81, "213053": 60, "213070": 70, "213135": 70, "213171306": 112, "213199": 98, "21361": 81, "213743e": 70, "2139": 12, "214": [60, 63, 144], "214458": 78, "214764": 87, "214769": 76, "215": [63, 66, 144], "215069": 84, "215342": 84, "2155": 81, "21550": 81, "21562": 81, "21573": 81, "215967": 128, "216": [63, 66, 144], "216207": 97, "21624417": 54, "216280": 63, "2163": 81, "216344": 84, "21669513e": 112, "216761": 83, "216943": 96, "217": [60, 63, 66, 85, 143, 144], "21716": 81, "2171802": [54, 79], "217244": 34, "217684": 76, "218": [63, 66, 144], "21804": [55, 80], "218383": 66, "218767": 81, "2189": 81, "218938": 81, "219": [9, 10, 26, 53, 60, 63, 66, 143, 144], "2191274": 54, "219585": 66, "2197237644227434": 78, "22": [52, 54, 55, 56, 60, 63, 69, 70, 78, 79, 80, 84, 85, 87, 88, 96, 97, 98, 100, 112, 128, 142, 145], "220": [40, 60, 63, 66, 144], "220088": 81, "220171": [98, 101], "220398": 69, "220407": 78, "220556": 62, "220568": [98, 101], "220770": 60, "220772": 84, "220972": 60, "221": [60, 66, 144], "221191": 63, "2213": 79, "2214": 79, "221419": 81, "2215": 79, "2216": 79, "2217": [54, 79], "2218": 143, "221829": 63, "222": [60, 63, 82, 144], "2222": [52, 54, 68, 98], "22222": 81, "222261": 96, "222569": 60, "22272803e": 112, "222843": 84, "222882": [70, 78], "223": [60, 85, 144], "223035": 60, "223158": 78, "22336235": 54, "223402": 63, "223456": 60, "223485956098176": [73, 74], "223617": 78, "22375856": 54, "22390": 80, "223928": 78, "224": [60, 63, 85, 144], "2244": 143, "224897": [69, 70], "225": [25, 53, 60, 63, 89, 143, 144], "225034": 58, "22505965": 54, "22507006e": 112, "22515044": [98, 101], "225175": 84, "225222": 84, "22522221": 84, "22528": 81, "225350": 70, "225427": 66, "225459760731946": 71, "225460": 71, "225574": 79, "2256": 81, "22562": 81, "225670": 78, "225776": 88, "225899": [98, 103], "226": [63, 144], "226137": 62, "226174": 60, "2264": 53, "226479": 76, "226524": 84, "226598": 79, "226840": 63, "226938": 74, "226969": 66, "227": [60, 81, 144], "22708010": 112, "2271071": 19, "227166": 63, "2276": 53, "2279": 81, "227932e": 80, "228": [60, 63, 144], "228035": 81, "2281": 81, "228234": 63, "228404": 78, "228465": 60, "228597e": 70, "228630": 70, "228648": 55, "229": [55, 60, 63, 144], "22913386": 61, "22913401": 61, "22913599": 61, "22913925": 61, "22925": 81, "22937": 81, "229443": 84, "229452": [96, 97, 98], "229472": 80, "2295": [63, 81], "229759": 97, "2298": 53, "229961": [69, 70], "229994": [69, 70], "22m": 97, "23": [23, 43, 47, 54, 55, 56, 58, 60, 63, 69, 70, 77, 79, 80, 81, 84, 87, 88, 92, 94, 96, 97, 98, 112, 128, 140, 142, 143, 145], "230": [25, 53, 143, 144], "230009": [73, 74], "230055": [98, 101], "230121": 60, "2302": 86, "23046358": 61, "230551": 63, "230590": 62, "230669": 60, "2307": [54, 79, 86, 91], "2308": 87, "230842": 69, "230956": 59, "231": [7, 60, 63, 144], "23113": 98, "231153": 70, "231310": 84, "231430": 128, "231467": 98, "23168468": 61, "231734": 98, "231798": 98, "231986": 84, "231e": 85, "232": [63, 144], "232134": [69, 70], "232157": 70, "23238782": 61, "23250009": 61, "232748": 60, "2328": 81, "232868e": 70, "232959": [73, 74], "232e": 98, "233": [13, 60, 144], "233029": 69, "233154": 145, "2335": 53, "233705": 70, "23391813": 61, "234": [143, 144], "234017": 62, "23404391": [98, 101], "234062e": 63, "234137": 88, "234153": 88, "234205": 81, "234431": 78, "234534": 71, "234605": 64, "23467184": 63, "234798": 81, "234910": 79, "235": [60, 143, 144], "235138": 63, "235291": 69, "23549489": [98, 101], "2359": 145, "23590": 81, "236": [63, 144], "236008": 71, "236015e": 69, "236309": 81, "236884": 78, "23690345e": 112, "236985": 60, "237": [56, 63, 144], "237115": 70, "237200e": 69, "237252": 81, "237341": 69, "237461": 87, "23748": 81, "23751359e": 112, "237834": 60, "237896": 84, "23789633": 84, "238": [54, 79, 144], "238101": 84, "238225": 128, "238251": 71, "238336": 63, "238420": 60, "238450": 62, "238529": 33, "23856": 81, "238619": 66, "238794": 84, "239": 144, "239019": 76, "239243": 69, "239267": 78, "239313": 70, "239560": 63, "23965": 81, "23e": 55, "24": [54, 55, 56, 60, 63, 69, 70, 77, 78, 79, 80, 81, 84, 87, 88, 89, 96, 97, 98, 112, 128, 142, 143, 144, 145], "240": 60, "240023": 61, "240127": [69, 70], "240146": 70, "240287": 63, "240295": 87, "2403": 86, "240532": [69, 70], "240601": [98, 101], "2407": 53, "24080030a4d": 56, "240813": 75, "240998": 63, "241": 144, "241049": 84, "241064": 70, "241206": 63, "241503": 96, "2416": 53, "241609": 81, "241645": 70, "241678": 69, "241827": 70, "241962": 88, "24199": 81, "241e": 85, "242": [60, 143, 144], "242000": 81, "242124": [80, 81], "242139": 128, "242158": [80, 81], "242231": 60, "2424": 76, "242422": 60, "242427": 76, "2424596822": 74, "242815": 128, "242902": 84, "243": [60, 144], "243056": 62, "2430561": 53, "243246": 84, "2438": 81, "2439": 81, "243e": 85, "244": [40, 51, 81, 112, 144], "244090": 81, "244455": 84, "244622": 128, "24469564": 142, "245": [143, 144], "245011": 63, "245062": 84, "2451": 53, "24510393": 55, "245245": 60, "245370": 79, "245512": 84, "245531": 69, "245720": 59, "246": 144, "246624": 96, "2467506": 54, "246753": 84, "246879": 84, "247": [85, 144], "247020": 71, "247057e": 84, "2471": 81, "2472": 81, "2474": 63, "247617": 96, "247717": 81, "24774": [80, 81], "247826": 79, "247977": 69, "248171": 84, "248441": 96, "248638": 71, "249": [54, 79, 82, 144], "2491": 81, "24917": 81, "249357": 63, "249509": 63, "249601": [98, 99, 103], "2499": [61, 93, 94], "249986": 78, "25": [9, 10, 14, 15, 16, 17, 25, 26, 36, 37, 54, 55, 56, 59, 60, 63, 69, 70, 71, 72, 77, 78, 79, 80, 81, 84, 88, 89, 96, 97, 98, 112, 128, 142, 145], "250": [82, 144], "2500": [61, 81, 93, 94, 98, 103], "25000000000000006": [71, 81, 84], "250073": 81, "250210": 71, "2503": 81, "250341": [98, 101], "250354": 84, "250425": 71, "250592": 61, "2506": 86, "250941": 60, "251": [60, 80, 81, 87], "251339": 60, "251412": 70, "251480": 70, "251953": 81, "252": 60, "252133": 81, "252253": 87, "25240463": 98, "252524": 84, "252601": 128, "252888": 63, "252967": 63, "253026": [69, 70], "2532": 81, "253381": 63, "253437": 83, "253457": 63, "253675": [63, 80], "253724": 84, "25374": 81, "254": [60, 81, 144], "25401679": 54, "254035": 78, "254038": 74, "254083": 70, "2543": 81, "254324": 71, "254388": 63, "254400": 128, "25445067": 63, "255": [81, 144], "255034e": 70, "2554": 61, "255658533": 112, "255995": 69, "256": [81, 97], "256082": 96, "256416": 84, "256567": 79, "25672": 81, "256944": 84, "256992": 81, "257019": 70, "257207": 54, "257377": 59, "2575": [98, 101], "257523": 69, "2580158": 61, "258083": 70, "258158": [69, 70], "258225": 61, "2583": 81, "25849222": 61, "258522": 69, "258541e": 37, "258558e": 82, "25861476": 61, "25879544": 112, "258862": 60, "258951": 84, "259164": 70, "259367": 96, "259395": 75, "2594": [55, 80], "259828": [69, 70], "25984146": 61, "259875": 70, "25x_3": 59, "26": [54, 55, 56, 58, 60, 63, 64, 69, 70, 77, 79, 80, 81, 89, 92, 94, 96, 97, 98, 112, 128, 142], "260": 60, "26009643": 112, "260097": 63, "26009865": 63, "26016": 81, "260161": [35, 96], "260211": [69, 70], "260356": 80, "260360": 84, "260526": 63, "260738": [98, 103], "260762": 66, "261": [60, 85], "2610": 81, "2613": 81, "261624": [80, 81], "261685": 81, "26175": 81, "261777": 81, "261903": 79, "2619317": 54, "262423e": 81, "262425": 60, "262621": 79, "262829": 112, "263": [7, 81, 144], "2633": 81, "263893": 60, "263942e": 70, "26395": 63, "263974e": 84, "264": [60, 143, 144], "264086": 59, "264088": 60, "264274e": 81, "264390": 60, "264884": 81, "265": 144, "2651": 98, "265119": 83, "2652": [56, 80, 81], "265547": 81, "265744": 78, "2658": 74, "265929": 85, "266": 144, "266147": 85, "266664": 63, "266686": 69, "266922": 128, "267": 82, "2670691": 54, "267500": 79, "267581": 81, "267845": 60, "2678752": 112, "267950": 84, "267989": 60, "268": 144, "268045945": 112, "268055": 81, "268343": 78, "268628e": 70, "268942": 84, "268943": 69, "268998": 55, "269043": 84, "269112": 98, "269175": 60, "269262": 63, "269641": 63, "269917": 60, "269920": 60, "269977": 81, "26bd56a6": 56, "26e": 55, "27": [10, 14, 25, 26, 52, 54, 55, 56, 57, 58, 60, 63, 64, 69, 70, 77, 79, 80, 81, 89, 92, 94, 96, 97, 98, 112, 128, 142, 143], "270": 144, "2700": 56, "270248": [98, 103], "270644": [69, 70], "271": 144, "271004": [80, 81], "271083": 81, "271183": 80, "271556": [98, 99], "271681": 63, "272": 51, "272296": 81, "272332e": 69, "272408": 70, "272610": [98, 101], "272662": 81, "272802": 63, "273": 56, "273299": 70, "273356": 71, "27371": [55, 80], "27372": [55, 80], "274": [56, 81], "2740991": 53, "274251e": 80, "274267": 79, "27429763": [98, 100], "27461519": [98, 103], "274793": 84, "274825": [36, 96], "27487": 81, "2754": 53, "275596": 128, "276": [56, 144], "276031": 63, "276148": 84, "276189e": 79, "2764": 81, "2766091": 55, "276947": 60, "27713": 81, "277299": 64, "277429": 63, "27751": 81, "277512": 70, "277561e": 79, "27794072": [98, 101], "277968": 84, "278": [87, 144], "2780": 54, "278000": 79, "278035": 66, "2782": 61, "278391": 81, "278434": 73, "278454": 76, "278568": 62, "2786": 128, "278804": 70, "279": [60, 144], "279053": 63, "279066": 63, "279415": 63, "27951256e": 112, "279595": 66, "279683": 60, "27986": 81, "279933e": 70, "27e": 112, "28": [54, 55, 56, 60, 62, 63, 69, 70, 72, 77, 79, 80, 89, 96, 97, 98, 112, 128, 142, 144], "280": 60, "280067": 62, "280196": 74, "280432": 60, "280454dd": 56, "280514": 128, "280888": 63, "280963": 83, "281": [85, 144], "281024": 84, "28111364": 55, "281214": 63, "28121947": 112, "2815": 81, "2818": 53, "2819": 128, "282": [85, 143, 144], "28208771": 63, "282200": 74, "282335": 60, "282459": 60, "2825": [140, 142], "28251": 81, "282870": 81, "2830": [140, 142], "283041": 69, "283207": 69, "28326": 81, "283386": 69, "2836": 53, "2836059": 54, "28382": 81, "2838546": [98, 101], "283974": 84, "283992": 69, "283994": 84, "283e": 85, "284": 144, "28425026": 87, "284271": 75, "284397": 145, "28452": [55, 80], "2849": 81, "284987": 81, "285": [85, 98, 144], "285001": 66, "2851": 61, "285483": 76, "285838": 63, "285873": 60, "285e": 85, "286": 144, "286203": 69, "286371": 69, "2865": [53, 81], "286507": 71, "286563e": 81, "286593": 81, "287": 144, "287041": 84, "287123": 96, "287168": 63, "287196": 69, "287794": 60, "287815": 87, "287926": 84, "288": [82, 144], "28897491": 112, "288976": 81, "289": [143, 144], "289062": 80, "289317": 60, "289357": 70, "289440": [69, 70], "289555": 76, "29": [54, 55, 56, 60, 63, 69, 70, 77, 79, 80, 87, 89, 96, 97, 98, 112, 128, 142], "290": 98, "290565": 70, "290736e": 70, "290901": 66, "290987": 80, "291": [81, 85, 144], "2910": 81, "291008": 69, "291011": [98, 100], "291071": 84, "29107127": 84, "291405": 84, "291406": 84, "291434": 70, "291500e": [80, 81], "291517": [69, 70], "29168951": [98, 103], "291963": 84, "292": [60, 83, 144], "292028": 71, "292047": 128, "292105": 84, "292302995303554": 71, "292303": 71, "2925": 56, "2927": 81, "292997": 84, "29299726": 84, "293120": 63, "293218": 84, "293617e": 81, "294": 144, "294067": [69, 70], "294123": 96, "294449": 69, "295": [143, 144], "295307": 69, "295481": 84, "29548121": 84, "295837": [64, 92, 94, 142], "2958370000000100000100": [56, 92, 94, 142], "2958370001000010011100": [56, 92, 94, 142], "2958371000000010010100": [56, 92, 94, 142], "296": 112, "296099": 66, "296228": 81, "296729": 79, "296779": 63, "29678199": [90, 113], "296901": 69, "296910": 63, "297": 144, "297276": [98, 103], "297287": [69, 70], "2973": 81, "297349": [73, 74], "297682": 84, "297687": 81, "297749": 81, "297790": 63, "297816": 63, "29784405": 87, "298": [13, 56], "298076": 69, "298120": 71, "298228e": 81, "298606": 63, "298619": 60, "299": [56, 85], "299110": 63, "29930": 63, "29931": 63, "299537": 74, "299676": 60, "29970629": 63, "299712": 73, "2999": 66, "29999": 60, "2_": [19, 57, 89, 129, 130, 139], "2_x": [19, 57, 89], "2d": [21, 113, 122], "2dx_5": [71, 84], "2e": [51, 53, 54, 55, 56, 57, 97, 98, 113, 128, 142], "2f": 75, "2m": [129, 135, 139], "2n_t": 59, "2x": 84, "2x_0": [11, 69, 70, 73, 74], "2x_4": 59, "3": [9, 10, 11, 12, 13, 14, 17, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 104, 112, 113, 128, 129, 135, 140, 141, 142, 143, 144], "30": [11, 51, 52, 54, 56, 57, 58, 60, 62, 63, 66, 67, 68, 69, 70, 71, 77, 79, 80, 81, 84, 85, 89, 96, 97, 98, 112, 128, 142], "300": [52, 68, 71, 81, 84, 91, 143], "3000": 66, "30000": 60, "30000000000000004": [71, 81, 84], "300031": 69, "300284": 60, "30031116e": 112, "300682": 63, "300892e": 70, "30093956": 87, "301": 56, "301366": 128, "301371": 84, "3016": 80, "301737": 69, "30189": 81, "302149": 69, "302269": 60, "30227357": [98, 101], "30235672": 60, "302357": 84, "302382": 78, "302487": 63, "302648": 79, "302750839": 112, "302925": 60, "303007": 69, "303324": 79, "303489": 84, "303613": 84, "30361321": 84, "30383": 81, "303835": 79, "303f00f0bd62": 56, "304077": 61, "304130": 84, "304159": 84, "304201": 59, "304253": 63, "30527": 81, "305341": 84, "305612": 79, "305775": 84, "305b": 56, "306297": 69, "30645": 81, "30672815": 54, "306824": 63, "306915": 79, "306963": 84, "307045": 63, "307407": 84, "307699": 60, "308": 81, "308568": 70, "308774": 69, "30917769": [73, 74], "309539": 78, "309606": 63, "309772": 79, "309823e": 81, "30982972": 84, "309830": 84, "31": [54, 55, 56, 57, 60, 62, 63, 69, 70, 77, 79, 80, 81, 89, 96, 97, 98, 112, 128, 142, 145], "310000e": 81, "310145": 78, "310761": 83, "311": 85, "311253": 81, "311321": 70, "311667": 70, "311712": 73, "311869": 96, "3120": 81, "312652": 85, "312663": 60, "313": [51, 98], "313056": 128, "313209": 71, "313324": 81, "31337878": 81, "313535": 84, "31378": 56, "313870": 76, "314": 112, "3141": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 56, 57, 64, 79, 90, 92, 94, 96, 97, 98, 113, 128, 142], "314247": 88, "314341": 69, "3146": 37, "314625": 70, "314651": 73, "31476": [80, 81], "315": 144, "315031": 88, "315036": 69, "3151": 81, "315155": 70, "315290": [73, 74], "315310": 69, "315769e": 69, "316": [56, 144], "316193": 84, "31632": 81, "316407": 98, "316540": 79, "316717": [69, 70], "316826": 69, "316863": 70, "317394": 59, "317487": 84, "317607": 84, "31781485": 63, "318": [56, 144], "318000e": 81, "318438": 81, "318512": 60, "318552": 81, "318584": 128, "31869544": 63, "318753": [73, 74], "319": [56, 144], "319100": [73, 74], "31910229": [98, 103], "319420": 76, "319759": 84, "319850": 84, "319975": 60, "32": [39, 54, 55, 56, 60, 62, 63, 69, 70, 77, 78, 79, 80, 81, 89, 96, 97, 98, 100, 112, 113, 128, 142], "320": 81, "320108": 63, "320314": 80, "32037392": 112, "320633": 71, "321": [82, 144], "321029": 60, "321234": 63, "32132043": 63, "321686": 128, "321723": 82, "322186e": 69, "322349": 63, "32236455588136": 58, "322404": 87, "322477": 60, "322697": 63, "322751": 70, "322995": 61, "322996": 63, "3234": 81, "323636": 80, "32366503": [98, 101], "323679": 79, "324": [55, 81, 144], "324518": 83, "32458367": 54, "3245837": 84, "324714": 60, "325056": 84, "325090": 81, "325486": 69, "325599": 69, "32565613": 60, "326": 85, "326148": 70, "326332": 60, "32665052": 63, "326721": 70, "326740": 84, "32674263": 62, "326871": 88, "3268714482135149": 88, "326951": 63, "327": 144, "327257": 69, "327803": 96, "327958": 66, "32796851": 63, "328220": 63, "328471": 69, "32875335": 62, "329339": 58, "329340": 63, "32950022e": 112, "329611": 63, "33": [54, 55, 56, 60, 63, 69, 70, 73, 77, 78, 79, 80, 81, 89, 96, 97, 98, 112, 128, 142, 143], "330": 144, "3300": [55, 80], "330068": 69, "330100": 69, "330143": 84, "33014346": 84, "330163": 70, "330285": [69, 70], "3304269": 54, "330615": 84, "330731": [36, 96], "331365": 73, "331448": 63, "331521": 84, "331602": 81, "33175566": 84, "331756": 84, "332502": 70, "332782": [36, 96], "3329": 81, "332996": 79, "3333": [52, 54, 68, 96, 97, 98], "3333333": 56, "33333333": [60, 62, 63], "33335939e": 112, "3335": 81, "333581": 80, "333655": 69, "333704": 70, "333955": 70, "334": 55, "334750": 71, "335": 82, "33500": 81, "335176": 81, "335446": 66, "335609e": 84, "335846": 84, "335853": 81, "336": 144, "33613": [98, 99], "336382": 70, "336461": 81, "336612": 59, "336870": 61, "337380": 84, "3376": 53, "337619": 58, "337747": 60, "337811": 63, "338": 87, "33804757": 112, "338147": 63, "33849": 81, "3386": 112, "338603": 69, "338775": 71, "338908": 71, "339158": 63, "339269": 87, "33928": 81, "339443": 70, "339570": 84, "339875": [73, 74], "339922": 60, "34": [52, 53, 54, 55, 56, 57, 60, 62, 63, 69, 70, 74, 77, 79, 80, 81, 87, 89, 96, 97, 98, 112, 128, 145], "340": [55, 81], "340029": 70, "340141": 62, "340142": 98, "340217": 61, "340219": 63, "340235": 76, "340274": 87, "340884": 62, "340906": 60, "341336": 34, "341472": 78, "341755e": 69, "3420": 81, "342117": 76, "342332": 63, "342362": 66, "342632": 78, "342675": 54, "34287815": 87, "342989": 81, "342992": 79, "343": [81, 85, 144], "343268": 60, "343371": 63, "343639": 96, "343685": 69, "34375": 80, "343828": 69, "344212": 145, "344440": 96, "34450402": 62, "344505": [80, 81], "344551": 63, "344579": [98, 101], "34459778": 63, "344640": 84, "344787": [69, 70], "344834": 59, "345": [82, 144], "345065e": 81, "345381": 71, "3453813031813522": 71, "3454": 81, "345612": 63, "345852": 70, "345903": 84, "345989": 69, "346107": 80, "346206": 84, "346238": 87, "346269": 70, "346678": 83, "346942": 60, "346964": 69, "347310": [36, 96], "347412": 63, "347682": 63, "347696": 71, "34769649731686": 71, "347929": 81, "348": 85, "348080": 63, "348319": 70, "34858240261807": 58, "348617": 84, "348622": 85, "3486659": 61, "348669e": 63, "348691": 63, "348700": 70, "34870827": 61, "348980e": 70, "349213": 62, "3492131": 53, "349383": 79, "349384": 60, "34940175": 61, "34943627": 77, "349638": 70, "34967621": 54, "34975177": 61, "349772": 74, "34m": 97, "34mglmnet": 97, "34mmlr3": 97, "34mmlr3learner": 97, "34mmlr3pipelin": 97, "34mranger": 97, "34mrpart": 97, "35": [55, 56, 60, 63, 69, 70, 71, 79, 80, 81, 84, 96, 97, 98, 112, 128, 129, 135, 145], "3500000000000001": [71, 81, 84], "350165": 97, "350208": 69, "350518": 84, "350712": [73, 74], "35077502": [129, 135], "351220": 70, "351482": 60, "351629": 81, "351766": 83, "352": [55, 79], "352250e": 80, "352259e": 81, "3522697": 54, "352661": 60, "352719": 63, "35292": 81, "352990": 81, "352998": 81, "353105": 37, "353412": 84, "35341202": 84, "35365143": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "353748e": 84, "3538": 53, "353922": 60, "354": 81, "354188": 59, "354371": 84, "354688": 38, "355065": 66, "355209": 84, "35553463": 63, "355627": 96, "355651": 70, "355805": 60, "35595486": 61, "35595969": 61, "35596091": 61, "35596142": 61, "356136e": 81, "356167": 74, "356183": 81, "35620768e": 112, "3564": 81, "3565": 81, "3568": 98, "356894": 63, "357": 81, "35709502": 112, "357170": 69, "35731523": [98, 100], "357402": 82, "358063": 60, "358158": [80, 145], "358289": 79, "358395": 87, "358799": 128, "359": [85, 145], "359100": 81, "359161e": 69, "359229": 66, "3593": 87, "359307": 70, "35th": 143, "36": [55, 56, 60, 62, 63, 69, 70, 79, 80, 96, 97, 98, 112, 128], "360004": 84, "360065": 128, "360098": 63, "360249": 75, "360475": [69, 70], "360572": 70, "360655": 81, "360683": 71, "360801": 71, "361": 85, "361374": [98, 101], "361518": 71, "361518457569366": 71, "361521": 38, "361623": 76, "361897e": 63, "3619201": 12, "362157": 69, "362206": 60, "36231307e": 112, "362627": 60, "362857": 60, "363276": 54, "363563": 63, "364221": 69, "3643": 128, "364340": [98, 101], "364595": 54, "3647": 56, "364800": 84, "364889": 60, "365": 51, "36501": 81, "365551": 70, "36557195e": 112, "36566025e": 112, "366": 81, "366157": 63, "36616": 81, "366310": 69, "366479": 63, "366529": 83, "366718627": 54, "366950": 69, "36696349": [98, 103], "367": [40, 85], "367181": 69, "367323": 84, "367366": 76, "367398": 78, "367571": 71, "367625": 84, "3678802": 112, "368": [60, 62, 63], "368152": 79, "3682": [55, 80, 81], "368324": 79, "368499": 71, "3684990272106954": 71, "368577": 96, "36895073": 63, "369088": 60, "369556": 71, "3696": 87, "369696": 63, "369796": 84, "369869": 80, "369981": 79, "36m": 97, "37": [55, 62, 69, 70, 76, 79, 80, 81, 96, 97, 98, 112, 128], "370254e": 80, "3702770": 54, "370419": 60, "370736": 79, "3707775": 54, "370908": 69, "3710": 81, "371357": [80, 81], "371429": 71, "37151620222343475162636474778184859196": 112, "371570447": 112, "371683": 63, "371850e": 70, "37191227": [98, 101], "37198715": 60, "372": 143, "37200": [80, 81], "372097": 71, "3722": 81, "372311": 63, "37231324": 89, "3724": 81, "372427": 70, "3727679": 54, "37317871": 63, "373218e": 78, "373451": 96, "3738573": 54, "374364": 84, "37436439": 84, "3745": 81, "374821e": 81, "374862": 69, "374917e": 69, "375081": 81, "375269": 60, "375274": 69, "375405": 63, "375465": 84, "375621": 78, "375844": 78, "376357": 60, "376399": [98, 101], "3766": 76, "376617": 76, "376760": 70, "376806": 70, "377060": 81, "377147": 96, "377311": 84, "377374": 63, "377609": 62, "377669": 70, "378068": 60, "378161": 61, "378351": 32, "378588": 69, "378596": 79, "378688": 84, "378727": 69, "378834": 84, "3788859": 54, "379": 143, "379038": 84, "37939": 81, "379614": 84, "379626": 69, "379742": 63, "379875": 60, "379981e": 70, "38": [56, 69, 70, 80, 96, 97, 98, 112, 128], "3800694": 54, "380837": [80, 81], "380881": 63, "38088999": 63, "381": 85, "381072": 84, "381129": 82, "381603": 69, "381684e": 80, "381685e": 81, "381689": 84, "3817": 81, "382286": 81, "382582e": 31, "382824": 63, "382872": 71, "382880961": 112, "383297": 84, "383531": 78, "384": 81, "384385": 63, "384443": 70, "384677": 66, "38470495": [98, 103], "384777": 81, "384865": 70, "384892": 62, "384928": 69, "3851": 81, "385160": 70, "385240": 128, "385266": 60, "385917": 79, "386": [56, 81], "386084": 63, "386102": 71, "386283": 60, "38646076": 62, "386465": 63, "386502": 81, "386831": 66, "386988": 58, "387": 56, "3871": 53, "387271": 60, "387411": 60, "387426": 84, "387522": 63, "387780": 84, "388026": 69, "388071": 84, "388185": 66, "38818693": 112, "388216e": 97, "388615": 61, "388668": 84, "38866808": 84, "388804": 63, "388871": 81, "389": 56, "389126": 98, "389164": 78, "389366": 62, "389382": 60, "389489": [98, 99], "389566": 83, "38973512e": 112, "389755": 69, "38990574": 112, "38e": 112, "39": [51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 69, 70, 74, 75, 77, 79, 80, 81, 87, 88, 89, 96, 97, 98, 112, 128], "39010121e": 112, "390304": [98, 101], "390379": 84, "391144": 60, "391377": 88, "391925": 60, "392128": 70, "392242": 75, "39236801": 77, "392400": 81, "39255938": [98, 101], "392623": 70, "392752": 58, "392833": 87, "392864e": [80, 81], "392917": 69, "39318978": 112, "393604": 71, "393654": 66, "394226": 70, "39425708": 54, "3946722045": 112, "395076e": 81, "395136": 79, "395268": 96, "395569": 69, "395603": 69, "3958": 98, "39582475": 62, "395889": 81, "396": [40, 98], "39611477": 55, "396173": 73, "39621961e": 112, "396300": 73, "3964": 81, "396440": 63, "396531": 81, "396767": 60, "396985": 79, "396992": [69, 70], "397140": 71, "397155": 70, "39727": 81, "397313": 53, "397536": 78, "397578": 75, "397811": 87, "3979": 62, "398": [92, 94, 142], "398166": 66, "39834657": 112, "398447": 60, "3985": 81, "398556": 63, "398770": 84, "398999": 96, "399": 55, "399056": 84, "399223": 59, "39923621": 62, "399343e": 69, "399355": 59, "399679": 98, "399692": 84, "399858": 88, "399969": 60, "39m": 97, "3cd0": 56, "3dx_1": [71, 84], "3e1c": 56, "3ec2": 56, "3f5d93": 82, "3x_": 84, "3x_4": [71, 84], "4": [10, 14, 19, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 129, 135, 142, 144], "40": [54, 57, 58, 69, 70, 71, 78, 80, 81, 82, 84, 89, 92, 94, 96, 97, 98, 112, 128, 129, 135], "400": 79, "4000": [60, 63], "4000000000000001": 97, "40000000000000013": [71, 81, 84], "400113": 76, "40029364": [129, 135], "400316": 60, "400823": 84, "400855956463958": 71, "400856": 71, "400865615": 112, "400905": 66, "401": [7, 145], "401247": [113, 128], "40127723e": 112, "401653": 63, "401690e": 70, "401931": [73, 74], "402": 82, "402077": 81, "40210118": 112, "402113": 128, "402301e": 97, "402619": 39, "402672": 63, "40287053": 62, "402902": 81, "402919": 60, "403": 85, "403425": 84, "403626490670169": 90, "4036264906701690": 90, "403626491": 90, "403715": 31, "403771948": 113, "4039": 53, "404267": 69, "404300": 66, "404318": 53, "404411": 69, "40452": 81, "404550": 83, "405050": 70, "405203": 59, "405374": 81, "40540559": 62, "40583": 53, "405890": [36, 96], "406284": 60, "406285": 84, "406446": 71, "4065173": 112, "40658248": 62, "40676": 53, "407": 85, "40732": 76, "407558": 69, "40755812": 62, "407565": 69, "408130": 60, "408476": [129, 135], "40847623": [129, 135], "408479": 79, "408509": 70, "408539": 84, "408565": 84, "409": 51, "409154": 53, "4093": 87, "409328": 81, "40937846": 112, "409395": 84, "409746": 71, "409848": [69, 70], "41": [69, 70, 80, 81, 96, 97, 98, 112, 128], "410100": 69, "410393": 71, "410667": 96, "410681": 59, "410682": 69, "410795": 79, "41093655": 112, "41113": 62, "411146e": 70, "411190": [69, 70], "411218": 60, "411219e": 63, "411291": 83, "411295": 84, "411304": [69, 70], "41142856": 62, "411447": 81, "411582": 84, "411768": 70, "412004": 73, "412127": 84, "412295": 60, "412304": 88, "412477": 59, "412631": 60, "412653": 79, "412714": 71, "412726": 70, "412941e": 70, "413095": 63, "413247e": 69, "41336": 97, "413376": 98, "41341040": 54, "413608": 84, "414": 85, "41405088": 62, "414073": 33, "41428195": [98, 101], "414533": 70, "41525168e": 112, "415375": 69, "415556": 96, "41566": 98, "415812": 145, "415988": 81, "416052": 66, "416132": 70, "41651666": 62, "4166": 81, "4166667": 56, "416757": 84, "416899": 69, "416919": 70, "416e": 85, "417": 82, "41700205": 63, "417539": 63, "417640": 69, "417727": 80, "417736": 78, "417767": [73, 74], "417834": 66, "41798768e": 112, "418": 40, "418056": 84, "41805621": 84, "418180": 63, "418400": 76, "418741": 66, "418806e": 71, "418827": 62, "418969": 96, "41918406e": 112, "419371": 84, "419871": 66, "41989983e": 112, "4199952": 54, "41e5": 56, "42": [14, 20, 23, 24, 57, 58, 59, 62, 66, 68, 69, 70, 71, 73, 74, 75, 76, 77, 81, 83, 84, 87, 88, 89, 93, 94, 96, 97, 98, 99, 100, 101, 103, 112, 128, 143], "4200": 81, "420316e": 81, "420446": 63, "42056232": 62, "42073312": 54, "42094064": [98, 103], "420967": 71, "421083": 53, "421105": 60, "4211349413": 54, "421163": 69, "421200": 88, "421297e": 70, "421357": [73, 74], "421576e": 81, "421793": 87, "421919": 81, "422007": 87, "422266": 81, "422293e": 96, "422325": 71, "422591": 70, "42320096": 62, "42338": 81, "423392": 63, "4235839": [73, 74], "42361681": 62, "42388291": 62, "42388745": 89, "423921e": 96, "423951": 53, "424079": 63, "424108": 71, "424127": 112, "42412729": 54, "424292": 69, "424328": 84, "424651": 98, "424717": 71, "424748": 88, "425": 79, "42500448": 62, "42501113": 62, "425103": 53, "425208": 81, "425325": 76, "425493": 53, "42550": 81, "425636": 76, "426055": 53, "426418": 63, "426540": 79, "426540301": 54, "426736": 81, "427": 81, "427486": [69, 70], "42755087": 87, "427551": 87, "427573": 79, "427654": 76, "427725": 84, "428": [128, 145], "428046": 83, "428082": 63, "42811700": 145, "428255": 84, "428411": [80, 81], "428467": 84, "4284675": 84, "428771": [36, 96], "42880491": 112, "4290": 53, "429057": 70, "429156": 60, "429230": 69, "429705": 69, "42ba": 56, "43": [55, 62, 66, 69, 70, 80, 96, 97, 98, 112, 128], "430298e": [80, 81], "430465": [98, 103], "430595": 70, "430608": 69, "431061e": 69, "4311947070055128": 97, "431253": 78, "431306": 84, "431437": 87, "431701914": 140, "431998": 66, "432130e": 80, "432300e": 84, "43231359e": 112, "432707": 85, "43294": 56, "432f": 56, "433": [56, 85], "433221": 71, "4336": 81, "433630": [98, 101], "43374433": 89, "433750": 69, "433753": 78, "4339": 53, "434054e": 76, "434121": 78, "434340e": 63, "434535": 84, "43453524": 84, "434540": 63, "434643125": 112, "435": 56, "43503345": 98, "43511": 81, "435401": 79, "4357": 81, "435927": 81, "435967": 79, "43597565": 84, "435976": 84, "436": [56, 81], "436016": 78, "436240": 60, "43627032": 58, "436327": 81, "436394": 78, "43660075": 62, "436764": 85, "436806": 81, "436817": 78, "437430": 63, "437667": 80, "437924": 81, "438": 79, "438038": 63, "438219": 84, "438289": 81, "438569": 81, "438578e": 81, "438740": 60, "43883": 74, "438834": 70, "4389": 81, "438960": 79, "439401e": 70, "439541": [80, 81], "439675": 85, "439699": 66, "439835": 63, "43989": 96, "439958": 78, "43f0": 56, "44": [58, 62, 66, 69, 70, 96, 97, 98, 100, 112, 128], "440320": 81, "440605": 97, "440673": 62, "440747": 69, "440a": 56, "441153": 84, "441209": 84, "441219": 73, "44124313": 98, "441282": 69, "4416552": 54, "441764": 60, "441849": 69, "441968": 63, "442847": [98, 99], "443016": 71, "443032": 80, "44312177": 55, "443672": [98, 103], "443686": 84, "4437": 81, "443e": 85, "444046": 81, "4444": [52, 54, 68, 98], "444500": [80, 81], "444850": 81, "4449272": 81, "445": 85, "445476": 69, "44563945e": 112, "445763": 60, "4461928741399595": 71, "446193": 71, "4462": 56, "44647451": 87, "44713577e": 112, "447485": 62, "447492": 81, "447624": [69, 70], "447706": 71, "447849": 58, "448": 81, "448252": 70, "448399558": 112, "448456e": 70, "448546": 63, "448569": 69, "448587": 71, "448745": 84, "448842": 70, "4489": 81, "44890536": 112, "448923": 75, "449107": 23, "449150": [36, 96], "44950": 81, "449625": 63, "449677": 76, "44fa97767be8": 56, "45": [69, 70, 71, 73, 75, 78, 81, 84, 96, 97, 98, 112, 128], "4500": 80, "45000000000000007": [71, 81, 84, 97], "450031": 78, "450152": 79, "4505648": [98, 101], "450812e": 70, "450870601": 54, "451312e": 69, "451934": 60, "452": [56, 145], "452091": 81, "452114": 96, "452488701": 54, "452489": 79, "452623": 70, "453": [51, 56], "453279": 69, "4535": 81, "453537": 63, "4539": 56, "454026": 63, "454081": 81, "454131": 63, "454184": 82, "454397": 84, "454406": 66, "45467447": 112, "455": 56, "45500": 81, "455078": 71, "455091": 70, "455107": 71, "455120": 84, "4552": 56, "455293": 71, "4552b8af": 56, "455448": 87, "455672": 81, "4558": 61, "455858": 63, "45594267": [98, 101], "455981": 129, "456370": 79, "456458e": 69, "4566031": 128, "45660310": 128, "4567": 87, "456892": 71, "457088": 84, "457090": 63, "457667": 81, "457871": 63, "458114": 81, "458196": [98, 101], "458307": 130, "458420": 81, "4584447": 54, "458784": 69, "458814": 76, "458855": 55, "458973": 60, "459178": 63, "4592": 54, "459200": 79, "45924394": 62, "459383": 71, "459418": 70, "459436": 78, "45957837": 112, "459760": 81, "459812": 71, "459871": 63, "46": [62, 69, 70, 75, 77, 80, 96, 97, 98, 112, 128], "460": 81, "4601": 81, "460207": [69, 70], "460218": 71, "460289": 84, "460744": 80, "461": 145, "4610": 145, "461227e": 69, "461412": 96, "461469": 61, "461629": 88, "462347": 63, "462451": 71, "462533e": 60, "462567": 70, "462979": 69, "462993": 61, "463179": 60, "46328686": [98, 101], "463325": 84, "4634": 81, "463418": 88, "463668": 81, "463763": 63, "463766": 74, "463816": [98, 103], "463857": 81, "463903": 70, "463b": 56, "464": 98, "464076": 71, "464284": 79, "46448227": 112, "464668": 34, "465": 66, "46507214": 87, "465424": 78, "465649": 88, "465730": 88, "4659651": 90, "465965114589023": 90, "4659651145890230": 90, "466047": 84, "46618738": 112, "466440": 71, "466475": 63, "466756": 84, "467": 81, "46709481": 112, "46722576e": 112, "467613": 79, "467613401": 54, "467681": [69, 70], "467770": 71, "468072": 70, "468075": 84, "46807543": 84, "46811985": 84, "468120": 84, "468406": 81, "468449": [98, 103], "468596": 63, "468907": 66, "468919": 81, "468d": 56, "469": 56, "469400": 60, "469474": 88, "469592": 62, "469676": 66, "469825": 71, "469895": 70, "469905": 70, "47": [55, 58, 66, 69, 70, 80, 87, 96, 97, 98, 112, 128, 144], "470055": 70, "470904": 69, "471": 66, "471204": 63, "471435": 85, "471451": 60, "471622": 69, "471927": 60, "472": 81, "47222159": 89, "472255": 81, "472293": 63, "472699": 70, "472891": 84, "47292025": 112, "472e": 56, "473099": 71, "47319": 98, "47419634": 142, "474214": [73, 74], "474731": 96, "475": 145, "475193": 63, "475304": 81, "475517": 78, "475569": 69, "475895": 63, "475e": 85, "476121": [98, 101], "476304": [98, 101], "476856": 71, "477130": [69, 70], "477150": 84, "477247": 70, "477357": 85, "477474": 79, "477518": 63, "47759584": 112, "47761563": 58, "478032": 81, "478058": 60, "478059": 70, "478064": 70, "4781": 81, "47857478": 112, "479533e": 63, "479655": 78, "47966100e": 112, "479722": 70, "479860": 81, "479869": 62, "479876": [73, 74], "479882": 70, "479928": 84, "479959": [98, 103], "47be": 56, "48": [56, 66, 69, 70, 74, 80, 81, 82, 96, 97, 98, 112, 128], "480": 66, "480133e": 84, "480199": 76, "48029755": 87, "480332e": 63, "480579": 70, "48069071": [90, 113, 128], "480691": [113, 128], "480800e": 84, "480965": 60, "481172": 84, "481218": 81, "481399": [80, 81], "481705": 98, "481713": 76, "481761e": 81, "481790": 60, "48183538394148535455565759737682888999": 112, "482": [56, 66], "482012": 73, "482038": 71, "48208358": 84, "482084": 84, "482179": 69, "482251": 39, "482461": [129, 135], "48246134": [129, 135], "482483": 84, "482616": 78, "482790": 59, "482898e": 70, "48296": 87, "483": [85, 98], "48315": 87, "483186": 59, "483192": [80, 81], "48331": 87, "4835": 81, "483711": 84, "483717": 71, "48390784": 98, "484022": 63, "48404": 54, "484303": 70, "4845": 81, "484640": 84, "484822": 60, "48489102": [98, 101], "4849": 56, "485": [56, 81], "485197": 69, "485270": 61, "48550": 88, "485617": [80, 81], "485812e": 81, "48583": [80, 81], "485871": 74, "486": [15, 81], "486127": 62, "486178e": 69, "486202": 71, "486532": 84, "48654880": 112, "48661": 81, "486649": 63, "487": [66, 81], "487352": 61, "487467": 81, "487524": 76, "487530": 60, "487641e": 84, "487793": 70, "487872": 67, "488394": 69, "488460": 81, "488485": 81, "48873663": 58, "488811": 84, "488909": [80, 81], "488982e": 71, "489488": [98, 103], "4895498": 84, "489550": 84, "489567": 86, "489699": 71, "489951": 70, "49": [56, 66, 69, 70, 96, 97, 112, 128], "490000e": 81, "490070931": 54, "490488e": 80, "490504e": 81, "490700": 84, "490896": 66, "490941": 81, "49098": 87, "491034": 69, "491245": 79, "49135": 37, "4915707": [98, 100], "491740": 60, "492": 81, "492174": 63, "492305": 60, "4923156": 90, "49231564722955": 90, "492315647229550": 90, "492417e": 98, "492637": 75, "492656": 70, "49270769e": 112, "493": [85, 98, 143], "493102e": 78, "493144": 88, "493195": 76, "493219": 84, "493313": 81, "493325": 20, "493426": 85, "493789": 60, "494": 85, "494089": 70, "494129": 84, "494324": 79, "494324401": 54, "495": 83, "495108": 78, "49530782": 54, "495657": 71, "495752": 84, "49596416e": 112, "496": 83, "496391": 63, "49650883": 87, "496551": 84, "496591": [98, 103], "4967": 63, "496714": 88, "496777": 145, "49693": 97, "496931": 63, "497": 83, "497168": 78, "4973": 63, "497422": 70, "497543": 82, "497655": 23, "497674": 58, "497964": 96, "498": 83, "498122": 76, "498260": 63, "498286": 78, "4986": 63, "4988": 63, "498921": 84, "498979": [81, 82], "498992": 69, "498f": 56, "499": [81, 83, 92, 94, 142], "499000e": [80, 81], "499776": 81, "49d4": 56, "4a53": 56, "4b8f": 56, "4dba": 56, "4dd2": 56, "4e": [54, 55], "4ecd": 56, "4fee": 56, "4x": 84, "4x_0": [11, 69, 70, 73, 74], "4x_1": [11, 69, 70], "5": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 112, 113, 121, 128, 129, 135, 141, 142, 144], "50": [36, 54, 56, 57, 59, 66, 71, 74, 77, 78, 80, 81, 82, 84, 96, 97, 112, 128], "500": [6, 9, 10, 12, 13, 17, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 52, 56, 60, 61, 62, 63, 64, 68, 69, 70, 73, 74, 77, 80, 83, 85, 87, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 113, 128, 129, 135, 142, 145], "5000": [41, 60, 61, 62, 63, 69, 70, 71, 84, 86], "50000": 79, "500000": [80, 81], "5000000000000001": [71, 81, 84], "500084": 84, "500267": 75, "5003517412": 54, "5005": 63, "500517": 84, "500604": 63, "500758": 60, "50093148e": 112, "501021": [81, 82], "501047e": 69, "5012": 63, "501983": 84, "502005": 96, "502016": 78, "502084": 98, "502205": 69, "502457": 82, "502494": 71, "5025850": 54, "502595": 70, "502612": 84, "502745": 60, "502901": 70, "502995": 84, "503": 40, "503374": 78, "503504": 97, "503511": 81, "503700": 66, "50398782e": 112, "504286": 79, "5042861": 54, "504548e": 70, "505": 82, "5050973": 54, "505258": 63, "505260": 63, "505264": 69, "505353": 70, "505795": [98, 101], "506050": 69, "506644": 69, "506659": 81, "506687": 81, "50672034": 54, "506900e": 84, "506903": 71, "506974": 60, "507": 85, "507285": 76, "50768b": 82, "508153": 83, "508433": 69, "508447": 63, "508459": 79, "5085": 81, "508947": [98, 100], "509059": 81, "509196": 84, "509409": 63, "509439": 60, "509461": 84, "50967": 88, "5097": 88, "5098": [64, 92, 94, 142], "509853": 84, "5099": [56, 64, 92, 94, 142], "509951": 71, "509958": 79, "51": [53, 55, 56, 62, 66, 73, 78, 96, 97, 112, 128, 144], "510000e": [80, 81], "510121": 69, "510385": 79, "510555": 66, "51079110": 54, "511257": 78, "511326": 62, "511465": 60, "511515": 81, "511540": 81, "5115547": 90, "5115547181877": 90, "51155471818770": 90, "511665": 69, "511668": 88, "5116683753999616": 88, "511862": 84, "512": 79, "512108": 84, "512149": 84, "51214922": 84, "512431": 60, "51243406e": 112, "512519": 79, "512572": 84, "512672": [98, 129, 135], "5131": 80, "513222": 76, "513624": 96, "513763": 63, "513992": 84, "514": 56, "514173": 70, "514545": 81, "51494845": [98, 103], "515031": 69, "515280": 60, "515338e": 69, "515358": 71, "5154": 81, "5154789948092002": 79, "5155": 56, "515621": 63, "515672": 70, "516": 56, "516125": 71, "516222": 84, "516242": 70, "516255": 84, "516256": 84, "516528": 84, "51655584": 112, "516797": 70, "517": [56, 79], "517279": 70, "5175": 81, "517753": 69, "517798": 66, "518175": 79, "518375": 70, "518446": 81, "518478": 66, "518610": 98, "518782": 81, "518846": 79, "518854": 66, "519645": 82, "51966955": 54, "519710": 84, "52": [53, 56, 75, 78, 85, 96, 97, 112, 128], "520": 81, "520415": 69, "520641": 87, "520930": 71, "521002": 71, "521233": 66, "521611": 70, "521632": 69, "521788": 69, "522510": 63, "522514": 63, "522753": 38, "522835": 59, "523030": 88, "523163": 71, "5232": 77, "52343523e": 112, "523442": 63, "523794e": 84, "523807": 83, "523977545": 54, "52424539": 54, "524657": 84, "524934": [69, 70], "5250": 81, "525064": 66, "52510803": 55, "5251546891842586": 88, "52517015": 112, "5255": 56, "525722": 69, "52590": [55, 80], "526": 79, "526532": 81, "526582": 76, "526769": [69, 70], "526984": 70, "527226": 69, "52732": 97, "527452": 70, "527540": 69, "528000e": 87, "528381e": 89, "528411": 60, "528580": 84, "528763": 70, "528937": [73, 74], "528996901": 54, "528997": 79, "529": 79, "529405": 53, "529468": 96, "529782": 53, "53": [53, 56, 66, 75, 92, 94, 96, 97, 98, 100, 112, 128, 140, 143], "530659": 75, "530793": 69, "530940": 84, "53094017": 84, "531": 56, "531223": 71, "531594": 81, "53209683": 98, "532266": 71, "53236472": 112, "53257": 97, "532738": 84, "53273833": 84, "532751": 73, "5329": 81, "533": 85, "533148e": 60, "533283": 98, "533489": 59, "533583": 63, "533900": 84, "534139": 66, "534211875": 112, "534588": 60, "5346": 56, "535179": 84, "535318": 84, "535609": 81, "535718e": 81, "53606675": 84, "536067": 84, "536143": 81, "536746": 84, "536778e": 70, "536798e": [80, 81], "537240": 84, "53724023": 84, "5373616": 112, "537438": [98, 99], "53789": 112, "53791422": 98, "538": 56, "538013": 81, "538105": 70, "5382": 87, "538826": 60, "538937": [80, 81], "539455": 84, "539475": 84, "53947541": 84, "539491": [73, 74], "539767": 71, "54": [53, 55, 56, 58, 85, 91, 96, 97, 112, 128, 144], "540": 82, "54014493": [98, 101], "540240": 81, "540375": 76, "5405": 76, "540549": 76, "5408": 53, "541": 85, "541060": 76, "541159": 84, "541406": 63, "54163": 87, "5416844": 90, "541684435562712": 90, "541821": 81, "541990": 81, "542": 85, "542136": 69, "542159": 70, "542170": 70, "542268": [98, 99, 103], "542319": 60, "542333": 81, "542446": 74, "542451": 84, "542560": 88, "542584": 71, "5425843074324594": 71, "542647": 84, "542648": 96, "542671": 79, "542883": [129, 135], "5428834": [129, 135], "542919": 96, "542989": 84, "543": [79, 81], "543052": 66, "543075": 71, "543136": 71, "543380": 79, "5434231": 90, "543423145188043": 90, "5436005": 54, "543691": 70, "54373574": [98, 101], "543764": 74, "54378": 87, "543832": 84, "544097": 84, "544383": 88, "544555": 79, "544669": 73, "54483": [113, 128], "5448331": [113, 128], "5451453": 112, "54517706e": 112, "545492": 66, "54550506": 85, "545605e": 84, "545919": 81, "545930": 96, "546294": 81, "5467606094959261": 71, "546761": 71, "546872": 63, "546928": [98, 101], "546953": 70, "547039": 69, "54716": 87, "547324": 70, "547431": 83, "5476": 81, "5478138": 112, "5479": 81, "547909": 81, "547967935": 112, "548032": 62, "548609": 62, "549109e": 81, "549892": 60, "55": [55, 56, 71, 80, 81, 84, 96, 97, 112, 128], "5500000000000002": [71, 81, 84], "550242": 78, "550394": 61, "551317": 70, "551586928482123": 71, "551587": 71, "551686": 71, "551728": 112, "55176": 97, "5518": 81, "552": 81, "552508": 81, "5526016": 112, "552694e": 69, "552727": 79, "552776": 84, "553004": 66, "55307": 97, "553427": 62, "553522": 70, "553754": 96, "553878": [35, 96], "553916": 81, "554076": 71, "554793e": 112, "554874854": 112, "554937": [98, 101], "554984": 63, "555": 79, "555137": 70, "555150": 81, "555445": 83, "555498": 84, "5555": [52, 68], "555536": 69, "5558373": 112, "555949e": 81, "555954": 81, "556191": [69, 70], "556334": 60, "55642562": [98, 101], "556792": 84, "5574dcd4": 56, "557595": 79, "557731": 83, "5577409": 112, "557999": 79, "558134": [69, 70], "55820564": [98, 101], "5584": 79, "5585": 79, "55863386": 98, "558655": 71, "5589": 79, "559": 145, "5590": 79, "5590729": 112, "559144": 71, "559186": 71, "5592": 79, "559394": 84, "559522": 84, "559592e": 69, "559680": 81, "55dc37e31fb1": 56, "55e": 55, "56": [56, 91, 96, 97, 112, 128, 140, 143], "560135": [113, 128], "56018481": 84, "560185": 84, "5602727": 58, "560530": 70, "560689": 53, "560723": 75, "56121417262930323437465052667980838790": 112, "561348": 70, "561410": 63, "5614534": [98, 101], "5616": 80, "561711": 81, "561785": [98, 100], "561883": 39, "562013": 84, "56223": 87, "562288": 88, "562390": 96, "562452": 78, "562518": 81, "562556": 62, "5625561": 53, "562712": [69, 70], "563067": 85, "563374e": 71, "563503": 84, "563528": 81, "563563": 76, "563673": 81, "56387280e": 112, "56390147e": 112, "56403823": [98, 101], "564045": 84, "564073": 81, "5641": 81, "564142": 71, "564232": [69, 70], "56425415": [98, 101], "564284": 60, "564451": 70, "564483": 63, "564577": 81, "565": 98, "565066": 71, "565373": 69, "566": 88, "566024": 84, "566091": 81, "56611407": [98, 101], "566162": 60, "566236": 60, "566388": 69, "567004": 87, "567215": 78, "567343": 81, "567364": 70, "567529": 84, "567695": 69, "567945": [73, 74], "568287": 76, "56876517": [98, 101], "568932": [98, 103], "569138": 63, "569315e": 70, "569449": 96, "569540": 70, "569590": 78, "56965663": 84, "569657": 84, "569911": 54, "5699994715": 54, "57": [56, 85, 96, 97, 112, 128, 145], "570038": 71, "5700384030890744": 71, "570111": 83, "57015458": [98, 101], "5702": 81, "570486": 53, "570562": 53, "570722": 142, "570936": 69, "571778": 53, "5718": 81, "5722": 80, "572408e": 70, "57245066": 84, "572451": 84, "572717": 86, "572991": 70, "573700": 59, "574": 56, "574160": 76, "5748": 97, "57496671": 54, "575": [8, 51], "575317": 63, "575381": 80, "57572422": 87, "5758": 61, "575810": 69, "57585824": 87, "57592948e": 112, "57599221": 87, "575e": 85, "576": 56, "576238": 63, "5763996": 54, "57643609": 87, "576704": 63, "577": 56, "5770": 80, "57715074": 54, "577271": 79, "577273": 69, "5776971": 87, "57775704": 87, "577807": [69, 70], "577813": 69, "577e": 85, "578081": 81, "578307": 84, "57843": 112, "578432": 60, "57843836": [98, 101], "578493": 63, "578523": 79, "578557": 70, "578846e": 71, "579125": 80, "57914935": 55, "579197": 76, "579213": 88, "579238": 71, "579322e": 80, "579381": 60, "579875e": 69, "579927": 62, "57e": 55, "58": [9, 55, 80, 88, 96, 97, 112, 128, 144], "5800": 81, "58000": 80, "5804": 56, "580414": 88, "580751": 80, "580853": 69, "580922": 73, "581655": 81, "581827": 78, "581849": 70, "582031": 80, "582049": 61, "582146": 70, "58241568": 112, "5825085": [98, 101], "582754": 86, "582761": 71, "582991": 80, "583034": 73, "583195": [69, 70], "583201": 70, "5833333": 56, "583534": 84, "583692": 78, "584012": 81, "584057e": 69, "584742": 74, "584849": 71, "584928": 69, "584942e": 79, "5852": 81, "585394": [98, 99], "585426": 112, "585479": 76, "585793": 71, "586362": 84, "5864": 53, "5866": 81, "586719": 71, "586719493648897": 71, "586794": 69, "5868472": 54, "586919": 60, "586921": 78, "58695426": 112, "587135": 70, "587292": 81, "588": 81, "58812": 97, "5882": 80, "588233": 69, "588364": 96, "588854": 69, "589147e": 78, "589402": 60, "589440": 71, "589914": 60, "589958": 70, "59": [60, 70, 96, 97, 98, 112, 128, 145], "5902201": 112, "590320": 59, "5905": 80, "590530": [98, 101], "590736": 84, "590813": 84, "590904": 70, "590911": 71, "590991": 71, "591080": 59, "591411": 73, "591441": 31, "591652": 80, "591678": 80, "591782": 84, "59199423e": 112, "592186": 70, "59247436": 63, "592681e": 71, "59300411": [98, 101], "593040": 60, "59307502e": 112, "5931003": [98, 101], "593648": 97, "593981": 96, "594": 8, "594316e": 84, "5948429": 112, "595353": 71, "59563003": [98, 103], "59566647": 63, "596": 81, "596069e": 81, "596270": [73, 74], "5964": 77, "596460": 70, "59647543": 63, "596758": 69, "597": [51, 55], "597098": 81, "59716542": 63, "597214": 60, "597923": 81, "597992": 63, "598043": 63, "59808200": 112, "598178": 81, "59827652": [98, 101], "598425": 60, "59854797": 98, "5985730": 55, "59861": 81, "598761e": 70, "599239": 63, "599297": [96, 97, 98], "599334": [98, 103], "599555": 60, "599874": 82, "5cb31a99b9cc": 56, "5d": [71, 84], "5x_2": 59, "5x_3": 59, "5z_i": 84, "6": [8, 9, 10, 14, 15, 20, 21, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 92, 94, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 129, 135, 140, 142, 143, 144], "60": [54, 57, 58, 71, 81, 82, 84, 89, 96, 97, 112, 128, 143], "600": 79, "6000": 81, "6000000000000002": [71, 81, 84], "600000e": 81, "600195": 69, "600254": 83, "600694": 98, "600776": 66, "601": 55, "601061": 71, "60113395": [98, 101], "601598": 79, "601783e": 70, "601823": 63, "601984": 70, "602079": 74, "602091": 60, "602168": 71, "602322": 96, "602386e": 69, "602470": 60, "602492": 69, "602587": 84, "602628": 71, "6029": 81, "604016": 81, "604022": 63, "604111": 81, "60458433": [98, 101], "604603": 23, "60481177": [98, 101], "604825": 81, "604841": [80, 81], "60488374": [98, 101], "605": 81, "60505262": 61, "605195": 74, "60534014": 61, "605818": 63, "60587047": 61, "606034": 84, "606129": 84, "60630253": 61, "606342": 71, "606759": 81, "6068": 54, "606800": 79, "606954": 71, "607264": 69, "6075": 145, "60753412": [98, 101], "607600": 81, "60775387": 63, "607900e": 70, "60795263": [98, 101], "608": [72, 85], "608392": 84, "60857": 53, "608814188": 112, "608818": 87, "60883483": 61, "60883536": 61, "60885818": 61, "60886695": 61, "608909": 60, "609": 85, "609230": 63, "609522": 78, "609575": [98, 100], "61": [60, 66, 96, 97, 112, 128, 144], "610318": 66, "610843": 63, "611": 145, "6110": 81, "611104": 63, "611269": 79, "61170069": 85, "611859": 74, "612": 85, "612081": 63, "612246": 76, "612792": 81, "612916": 60, "613244": 70, "6133": 55, "613314": 71, "613408": 84, "613498": 81, "613574": 75, "613622": 70, "613691": [98, 100], "614": 85, "61404894": 98, "614145": 60, "614162": 63, "614188": 79, "614540": 63, "614678": 81, "615": 66, "615498": 31, "615863": [73, 74], "616009": 63, "616372": 80, "616617": 70, "61669761": [129, 135], "616698": [129, 135], "616828": 81, "617": 79, "61728": 96, "617283": 81, "6173": 56, "61771229": 85, "6178444": 112, "617877": 84, "618": 51, "618069": 80, "61810738": 55, "618574": 70, "618776": 58, "618805e": 63, "618881": 70, "619": 85, "619128": 70, "619237": 63, "619294": 66, "619302": 60, "619351": [69, 70], "619390": [69, 70], "619454": 59, "619613": 80, "619903": 70, "61e": [55, 145], "62": [31, 66, 74, 75, 96, 97, 112, 128], "620156": 84, "620874e": 98, "620995": 89, "621002": 63, "62108072": [98, 101], "621094": [80, 81], "621318": 84, "62131806": 84, "621359": 96, "621490": 84, "6215": 80, "622": [81, 85], "622147": 63, "622153": 81, "622272": 66, "6224": 54, "623024": 71, "623173": 69, "623341": 60, "623780": 63, "624": 79, "6240": 87, "62403053": 58, "6243811": 54, "6244152": 112, "624535": 97, "624764": 70, "624798": 80, "624818": 70, "624919": 81, "624988": 81, "625": [54, 79], "625159": 75, "62540248": 63, "625477": 84, "625766": 73, "625767": 69, "625891": [73, 74], "62632225": 63, "626433": 84, "6266": 81, "626633": 70, "627505": [73, 74], "627560": 84, "627564": 71, "627588e": 81, "628": 85, "628069": 79, "629346": 81, "629549": 70, "629595": 37, "629740": 69, "629e": 85, "63": [54, 66, 79, 96, 97, 112, 128, 143, 144], "630150e": 84, "630880": 85, "630914": 75, "631083": 70, "63117637": 98, "631333": 84, "6318": [80, 145], "632058": 79, "63212174": 63, "63215159": 63, "63245862e": 112, "632747e": 84, "632958": 83, "6330631": 128, "633433": 79, "634": 85, "63407762": 145, "634078": [80, 145], "634577": 128, "63499": 81, "635": 40, "635000e": [80, 81], "635016": 63, "635199": [80, 81], "635768": 69, "63593298": 98, "636048": 98, "636453": 34, "636575": 71, "636982": 60, "637326": 84, "637389": 63, "6379": 80, "638264": 84, "638461": 78, "638488": 75, "639": 80, "639135": 79, "639138": 63, "63916605": 55, "639345": 81, "639580": 70, "639603": 70, "64": [66, 74, 80, 81, 85, 96, 97, 112, 128, 142], "640": 81, "640516e": 63, "640781": 60, "640900": 81, "641332": 63, "641378": 63, "641528": 84, "641547": 84, "64154727": 84, "641759": 60, "641887": 63, "64197957": 84, "641980": 84, "642": 85, "6420": 81, "642016": 84, "642329": 66, "642648": [98, 99], "64269": 87, "643133": 81, "643369": 61, "64340": 87, "643512": 71, "643679": [98, 103], "643752": 84, "643939": 66, "644113": 96, "644371": 70, "644665": 71, "64476745e": 112, "644799": 59, "644918": 60, "644985": 69, "645": 81, "645583": 66, "645752": 63, "64579": 53, "6458": 54, "645800": 79, "646": 51, "646117": 70, "646571": 63, "646937": 59, "647002": 81, "647004": 98, "647010": 81, "647196": 59, "64723": 87, "647254e": 69, "647689": 88, "647737": 61, "647864": 96, "647873": 84, "647884": 63, "64797": 87, "648": 80, "648355": 69, "6485": 61, "648579": 62, "648580900": 112, "648690": 70, "648769": 70, "649": 143, "649105": 60, "649158": 84, "649514": 69, "649738": 69, "65": [66, 71, 75, 81, 84, 85, 96, 97, 112, 128], "650": [72, 98], "6500000000000001": [71, 81, 84], "650000e": 81, "650234": 66, "65080959": 63, "650810": 81, "650867": 71, "651127": 70, "65126076": 63, "652071": 81, "6522": 143, "652312": 73, "652324": 66, "652349": 84, "652350": 79, "65244287": 63, "652450e": [80, 81], "6527": 72, "652778": 79, "6528": 81, "652990": 63, "6530": 81, "653008e": 80, "653829": 76, "653846": 71, "653901": [69, 70], "654070e": 98, "654622": 82, "654755": 59, "655284": 84, "6553": 145, "6554": 143, "655422": 81, "655547": 69, "65557405e": 112, "655959": 76, "65646049": 63, "656745": 61, "656751": 63, "656775": 62, "657": 56, "657283": 87, "65778974": 63, "658": 79, "658021": [98, 103], "658038": 63, "658267": 84, "658592": 70, "6586": 53, "658702": 70, "659": 56, "659245": [69, 70], "659339": 70, "659387": 62, "6593871": 53, "659423": [69, 70], "659473": 88, "659636": 71, "659735": 69, "659755": 85, "6598": 77, "659835": 70, "66": [66, 76, 77, 82, 96, 97, 112, 128, 142, 144], "660": [56, 98], "660030": 60, "660073": 70, "66008073": 60, "660320": 74, "660479": [98, 100], "6607402": 85, "660776": 84, "660788": [98, 101], "66133": 98, "661369": 83, "661388": 69, "66141783": 60, "66151911": 63, "66182525": 60, "662032": 63, "6623797": 62, "6625": 81, "6626": 112, "66269281": 62, "662816": 60, "662975": 78, "663081975281988": 71, "663082": 71, "663177": 66, "663182": 71, "6634357241067574": 88, "66348653": 60, "663529": 84, "663533": 81, "66357743": 62, "663672": 76, "663765": 70, "664103e": 81, "664147": 81, "664409": 70, "664797": 69, "664824": 81, "664850": 79, "665209": 63, "665264": 84, "665653": [98, 103], "665909": 63, "666": 51, "66601815": [98, 100], "666104": 84, "666307": 59, "66634662": 60, "6666667": 56, "666733": 63, "666742": 96, "666959e": 76, "667": 79, "667492e": 81, "667536": 84, "667614": 71, "667614205604159": 71, "667867": 63, "667921": 60, "667981": 69, "667985": 75, "668337": 81, "668452": 75, "668584": 59, "668981": 73, "669": 82, "669579": 70, "66989604": 58, "67": [51, 56, 80, 88, 96, 97, 112, 128, 142], "670541": 62, "670867": [36, 96], "67095958": [98, 101], "671099": 63, "671224": 70, "671271": [69, 70], "67136": 81, "671502": 60, "6716717587835648": 71, "671672": 71, "671690": 69, "6722": 56, "672234": [69, 70], "672368": 71, "6723684718264447": 71, "672384": [69, 70], "67245350": 54, "672511": 69, "673005": 63, "673092": [69, 70], "673302": 79, "673330": 70, "673539": 63, "67410934": 54, "6745349414": 54, "674552": 81, "67456": 88, "674609": 71, "674747": 78, "675233": 70, "675293": 83, "675527": 63, "675625": 96, "675653": [98, 99], "675775": 78, "676405": 71, "6765": [55, 80], "676534": 128, "676641": 69, "676756": 84, "676787": 63, "676807": 80, "677123": 69, "6774": 60, "677614": 84, "677744": 60, "677850": 60, "677980": 71, "678": 85, "678117": 81, "678389": 60, "678826": 71, "67929982": 62, "6793023": 62, "67936506": [98, 100], "679539": 79, "67973229": 62, "679781": 63, "679789e": 69, "67ad635a": 56, "68": [56, 66, 82, 86, 87, 96, 97, 112, 128], "680": 81, "680104": 82, "680677": [98, 101], "6807": 112, "6810775": 87, "681176": 79, "681246": 70, "681448": 81, "681521": 69, "681562": 81, "681817dcfcda": 56, "682": 98, "682122": 78, "682269": 81, "6826": 80, "682875": 71, "683178": 63, "683487": 70, "683581": 98, "683637e": 76, "683687": 70, "683903": 63, "683942": 84, "683984": 38, "684": 145, "68410364": 55, "68411700": [55, 145], "684128": 70, "684142": 69, "684502": 84, "685104": 20, "685107": 84, "68554404e": 112, "68562150e": 112, "685807": 84, "685989": 98, "686270": 70, "686627": 69, "687041": 63, "687345": 84, "687612": 70, "687647": 84, "687854": 59, "687871": 79, "6878711": 54, "688": 143, "688119692": 112, "688540": 96, "688641": 78, "688747": 81, "688886": 96, "688918": 81, "688956": 69, "689072": [98, 101], "689088": [69, 70], "689188": 59, "689373": 60, "689392": 84, "689600": 78, "689932": 69, "69": [75, 96, 97, 112, 128, 144], "690261": 62, "690334": 71, "6903344145051182": 71, "69045508": 62, "69093191": 62, "691097": 69, "69110151": 62, "691157": 58, "69120154": 63, "69140475e": 112, "691423": 69, "691511": 80, "691848e": 70, "691911": 96, "692297": 70, "692460": 78, "69247661": 63, "692579": 70, "692725": 84, "692907": 81, "692959": 69, "693316": 81, "693497e": 81, "693690": 81, "693796": 79, "694129": 60, "694154": 71, "694561": 85, "694845e": 81, "694919": 79, "69493612": 112, "6950": 81, "695045": 69, "69508862": 98, "695581": 75, "69562150e": 112, "695711": 78, "69572427": [98, 103], "695928": 69, "696011": [35, 96], "696224": 96, "69622541": 60, "696289": [73, 74], "69684828": 98, "696966": 70, "697": 79, "697000": 71, "697258": 63, "697315": 60, "697420": [73, 74], "697545": 84, "697561": 61, "697616": 70, "697693": 69, "69794892": 60, "698223": 59, "698244": 59, "69840389e": 112, "698509": 69, "698642": [98, 103], "698694": 79, "698751": 78, "698859": 61, "69902856": 60, "699035": 84, "699082": 71, "69921": 56, "699259e": 84, "699333": 71, "699616": 78, "699697": 70, "6_design_1a": 72, "6_r2d_0": 72, "6_r2y_0": 72, "6b": 128, "6cea": 56, "7": [10, 14, 17, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 129, 135, 142, 143, 144], "70": [55, 57, 71, 73, 80, 81, 84, 96, 97, 112, 128, 144], "700": [69, 70, 72, 79], "7000000000000002": [71, 81, 84], "700015": 84, "700102": 84, "700314": 66, "700458": 69, "700643": [98, 101], "701078": 84, "701088": 80, "701106": 75, "701265": 73, "701413": 81, "70147416": 63, "701672e": 71, "701841e": 74, "701866": 84, "7018663": 84, "701966": 81, "702489": 81, "703049": 69, "70305686": [98, 103], "703108e": 39, "70329553": 60, "703325": 76, "70344386": [98, 103], "703478": 63, "703772": 81, "7040": 81, "70414611": 63, "704482": 76, "7045": 76, "704503": 60, "704558": 76, "704814": 69, "704896": 76, "704908": 60, "705090": 70, "705354": 69, "705380": 60, "70557077": [98, 103], "705581": 81, "7055958": 90, "705595810371231": 90, "7055958103712310": 90, "705794": 70, "70583": 87, "705991": 60, "706056": 81, "706077": 70, "706122": 70, "70624654": 60, "706430": 69, "706645": 71, "706657": 71, "706664": 60, "706862": 23, "706993": 63, "707125": 70, "707441": 70, "707738": 70, "70774361": [98, 103], "707868": 84, "707963e": 80, "708190": 79, "708235": 69, "708459": 84, "708472": 70, "7086": 63, "708727": 63, "708821": 66, "708837": 66, "709026": 59, "709596": 69, "709606": [36, 96], "71": [96, 97, 112, 128, 144], "710059": 66, "710319": 70, "710515": 69, "710586e": 79, "711024": 81, "711328": 81, "711383e": 69, "711518": 81, "711638": 98, "712064": 69, "712082": 81, "712095": 66, "712157": 83, "712268": 69, "712372e": 70, "712503": 87, "712592": 80, "712774": 73, "712846": 96, "712960": 71, "712984394": 112, "713": 81, "713407": 81, "713457": 69, "713604": 60, "713986": 81, "713993": 70, "714240": 79, "714250": 70, "714321": 80, "714534e": 70, "714651": 84, "71465114": 84, "715013": 81, "715180e": 81, "7154": 81, "715407": 71, "7155": 81, "715529": 63, "715531": 63, "715718": 63, "7158581": 54, "716013e": 69, "716098": 66, "7161": 81, "716316": 63, "716318": 61, "716387": 69, "716427e": 70, "716456": 84, "716525": 60, "716595e": 81, "716615": 66, "716762": 71, "716793": 71, "716799": 79, "7167991": 54, "716801": 78, "717": 81, "717130": 81, "717185": 84, "718686": 88, "71947092": 60, "719552": 70, "72": [57, 76, 82, 96, 97, 112, 128, 144], "7204309": [98, 103], "720559": 69, "720571": 84, "720573": 69, "720589": 85, "720664": 79, "721018": 69, "721071": 84, "721245": 70, "7215093d9089": 56, "72155839e": 112, "721609": 81, "721925": 63, "721973": 60, "72228103": [98, 101], "722316": 84, "722634": 84, "72269685": [98, 103], "72273802": 60, "722848": 71, "722881": 84, "7229": 81, "723": 56, "723314": 84, "723342": 96, "723345e": 84, "723657": 69, "723846": 66, "7239": 81, "72396776": 60, "72409348": 60, "7241399": 54, "724338": 84, "724661": 63, "724767": [73, 74], "724918": 88, "725": 56, "725010": 66, "725061": 69, "725087": 81, "725166": 84, "725565": 69, "725802": 23, "725820": 78, "725919": 69, "726": [56, 85], "726658": 78, "7268131": 54, "727157": 63, "727159e": 70, "727543": 59, "727693": 81, "727704": 81, "727780": 63, "727976": 71, "7282094": [98, 100], "728294": 83, "7283": 63, "72852639": 63, "728710": 84, "72875815e": 112, "728852": 81, "728e": 85, "7296": 60, "729668": 96, "729867": 69, "72987186": [98, 103], "73": [55, 66, 96, 97, 112, 128], "730023": 81, "7308": 53, "730809": 69, "731174": 69, "731317": 71, "731542": 61, "731906": 63, "732": 85, "732067": 69, "732137": 69, "732150": 70, "732470": 63, "7326": 81, "732638": 84, "73285": 34, "732918": 73, "7329225": 60, "733": 81, "733047": 70, "733644": 69, "734635": 69, "734770": 70, "734948": 84, "735369e": 96, "7357": 81, "735848": 96, "735941": 33, "735964": 59, "736082": [69, 70], "736084": 84, "73608412": 84, "736643": 60, "736823": 70, "737052": 81, "73750654": [98, 101], "7375615": 55, "73764317e": 112, "737951": [69, 70], "738": 80, "738065": 70, "738223": 81, "738315": 81, "738659e": 81, "738876": 70, "739": 81, "739063": 69, "739089": [98, 103], "73920967": 63, "7395359436844482": 71, "739536": 71, "739595": 85, "739720": 81, "739817": 75, "74": [9, 55, 70, 80, 96, 97, 112, 128, 144], "740": 79, "740180e": 84, "740367": 69, "740417": 80, "740505": 66, "740785": 69, "740869": 71, "741104": 71, "741380": 85, "741523": 66, "741702": 84, "7418": 53, "74189": 56, "742128": 84, "742375": 69, "742407": 83, "742411": 69, "742907": 84, "743073": 63, "7432": 53, "743247": 81, "743341": 70, "743609": 69, "7437": 81, "743983": 63, "74402577": 84, "744026": 84, "744236": 87, "744263": 60, "74461783e": 112, "74475816": [98, 103], "745": 81, "745022": 66, "745248": 63, "745444": 69, "745638": 80, "745881": 69, "746": 51, "746361": 84, "746699": 61, "746834": 60, "746843": 74, "7470": 81, "747646": 81, "74777354": 63, "74782803": 63, "747945": 54, "747961": 81, "748084": 70, "748089": 62, "748377": 80, "748513": 81, "748880": 81, "749": 112, "749199": 63, "74938952": [98, 100], "7494": 61, "749443": 81, "749854893": 113, "74e": 112, "75": [9, 26, 36, 56, 59, 66, 71, 76, 80, 81, 84, 96, 97, 112, 128, 144], "75000": 88, "7500000000000002": [71, 81, 84], "750000e": 81, "750597": 70, "750701": 66, "75093732": 63, "751013": 81, "751261": 81, "751633": 81, "75171": 80, "751710": [71, 80], "751712655588833": 90, "7517126555888330": 90, "751712656": 90, "752015": 32, "752270": 63, "752283": 81, "752696": 66, "752871": [98, 101], "752909": 76, "75329595": [98, 101], "7533": 80, "753323": 69, "753393": 69, "753523": 84, "753866": 70, "754469": 69, "754499": 70, "754678": 78, "754692": 96, "754786": 82, "7548": 88, "754870": 79, "755688": 69, "755701e": 69, "755885": 96, "755910": 81, "7559417564883749": 71, "755942": 71, "7560824": 54, "756200": 66, "756655": 63, "756805": 79, "756867e": 81, "756905": 23, "756969": 71, "757": [85, 143], "757151": [69, 70], "757183": 71, "757411": 84, "757559": 76, "757819": 79, "757917e": 84, "758196": 63, "758391": 81, "758695": [98, 101], "758831": 70, "758841": 60, "75887": 56, "759006": 58, "759054": 70, "75908345": 63, "7596": 60, "759833": 70, "76": [96, 97, 112, 128, 143, 144], "760104": 84, "7603": 53, "760386": [98, 100], "760778": 79, "760915": 59, "761": [54, 79], "761069": 61, "761224": 66, "761429": 70, "761435": 63, "761714": 71, "762284": 84, "76228406": 84, "762299": [98, 103], "762478": 60, "762748": 81, "7628744": [98, 101], "7635": 63, "763691": 81, "764093": [69, 70], "76419024e": 112, "764315": 84, "76444177e": 112, "764478": 83, "764569750": 112, "7646": 81, "764798": 84, "764953": 80, "765202": 81, "76533827": 63, "76535102": [98, 103], "765363": [69, 70], "765500e": [80, 81], "765710e": 89, "765792": 84, "76591188": 54, "765960": 69, "7660": 53, "766059": 63, "76608187": 61, "766107": 62, "7663": 81, "766499": 84, "766750e": 60, "766940": 66, "767": 51, "76702611e": 112, "767188": [73, 74], "767435": 88, "767549": 70, "768071": 84, "768273": [73, 74], "768763": 70, "768798": 66, "769361": 84, "769805": 84, "77": [85, 96, 97, 112, 128], "770556": 81, "770944": [73, 74], "7710": 87, "771157": 128, "771390e": 81, "7714": 82, "77140435": 63, "7716982": 55, "771741": 81, "771965": 81, "772104": 69, "772157": 76, "77227783e": 112, "772309": 63, "772396": 70, "772444": 96, "77245739": 63, "772791": 81, "77289874e": 112, "773": 56, "77301119": 63, "773126": 61, "773177": 71, "77329414": [98, 101], "773339": 76, "773488": 84, "77348822": 84, "773769": 78, "77401500e": 112, "774271e": 81, "775": [56, 81], "775191": [69, 70], "775285": 69, "775878": 63, "775969": 87, "776254e": 69, "7763": 80, "776544": 60, "776728e": 79, "776887": 80, "77746575": [98, 103], "7776071": 54, "777718": 78, "777728": 96, "777867": 76, "777e": 85, "778042": 63, "778400": 69, "778470": 60, "7786": 53, "77888695": 112, "779": 85, "779068": 76, "779108": 69, "779167": 31, "779517": [69, 70], "779682": 71, "7799": 77, "779912": 81, "78": [85, 96, 97, 112, 128, 144], "780": 56, "780041": 63, "780054": 63, "780068": 78, "780338": 69, "780458": 84, "780728": 82, "780856": 80, "7809": 61, "781": 81, "781172": 60, "781233": 81, "781446": 63, "781530": 84, "781681": 84, "782": 56, "782050": 84, "782524": 63, "782555": 81, "783": 56, "783276": 98, "7833": 53, "78331585": 63, "7838": 53, "78386025": [98, 103], "784": 128, "784238": 79, "784405": 87, "784483": 79, "784624": 71, "784792": 78, "784872": 66, "785": 56, "785038": 70, "785153": 70, "785815": 66, "785911": 84, "785e": 40, "786": 56, "786090": 78, "786108": 62, "786191": 75, "786237": 69, "786428": 82, "786563": 78, "786744": 71, "786815": 60, "786986": 66, "78711285e": 112, "787695": [98, 101], "78777": 87, "788": 143, "78818": 56, "788868": 70, "789032": 69, "789039": 70, "78930202": 63, "789330": 70, "789355e": 87, "789671": 71, "789671060840732": 71, "79": [66, 96, 97, 112, 144], "790039e": 69, "790115": 81, "790261": 96, "790723": [73, 74], "79105217": 112, "791097": 80, "791241": 84, "791297": [36, 96], "79164735": [98, 101], "7919965": [98, 101], "792": 51, "792396": 66, "7927": 63, "792939": 71, "792972": 96, "79330022": [98, 103], "793315": 96, "79338596e": 112, "793570": 84, "793598": 70, "793735": 84, "793818": [69, 70], "794": 98, "79408099": 112, "794366": 81, "79458848e": 112, "794805": 73, "795": 85, "795020": 62, "795647": 84, "7957": 81, "795932": 97, "796014": 70, "796233": 61, "796384": 70, "796444": 81, "796596e": 69, "796e": 85, "797086": 69, "797189": [98, 103], "797280": 84, "797454": 98, "797737": 128, "797868": 70, "79792890e": 112, "797965": 128, "798071": 20, "798308": 80, "7984": 61, "798769e": 60, "798783": [73, 74], "799403": 84, "79953099": [98, 103], "7999": 89, "7b428990": 56, "7x": 84, "8": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 68, 69, 70, 71, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 92, 94, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 129, 135, 142, 144, 145], "80": [57, 58, 71, 81, 84, 89, 96, 97, 112, 144], "800": 79, "8000": [19, 57, 89], "8000000000000002": [71, 81, 84], "8001": 61, "800143": 69, "800326e": 69, "800351": 69, "800763": 63, "80091075": 63, "801623": 81, "802": 85, "802130": 61, "802289": 81, "802738": 96, "803112": 78, "803300": 69, "803492e": 84, "803563": 81, "803902e": 81, "804": 81, "804219": 84, "804284": 87, "804316": 84, "804484": 84, "804651": 63, "80473269": 63, "8048": 55, "804828": 84, "804889": 81, "805007": 79, "805153e": [80, 81], "805293": 70, "805321": 60, "8055563": 54, "805774": 69, "8059": 80, "806071": 60, "806218e": 81, "806531": 81, "806554": 69, "806671": 82, "806732": 75, "80696592e": 112, "80714504e": 112, "807879": 84, "808": [55, 128], "808246": 80, "808284": 81, "808368697": 112, "808640": 81, "809125": 69, "809484": 82, "8095": 82, "809913": [69, 70], "80a8": 56, "81": [54, 69, 72, 75, 77, 96, 97, 112, 144], "810044": 80, "810134": 84, "8102": [53, 80], "810306": 66, "810322": 69, "810350": 60, "810363": 81, "810382": [80, 81], "810419": 70, "810707": 81, "810895": 70, "811011": 70, "811155": 75, "811458": 80, "811513": 70, "8116912": 128, "811696": 69, "811825": 79, "811901": 84, "81190107": 84, "812": [51, 85], "812028": [98, 99], "812133": 60, "8132463": 54, "813293": 84, "813342": 128, "813682": 81, "8138": 61, "813804": 61, "814136": 71, "814246e": 70, "814351": 71, "814913": 79, "8152": 81, "815213e": 70, "815224": 128, "815226": [98, 100], "81568484": 84, "815685": 84, "815838": 62, "815993": 84, "816176": 88, "816318": 79, "816373": 69, "816752": 81, "816968": 62, "816982": 69, "817119": 69, "817291": 81, "8173602": 77, "817967": 98, "817991": 63, "81827267": 84, "818273": 84, "818289": 84, "81828926": 84, "818380": [69, 70], "818548": 60, "81856": 56, "819223": 96, "819507": 78, "82": [88, 96, 97, 112, 144], "8202": 55, "820292": 82, "820366": 79, "8209": 55, "820928": 62, "820963": 66, "821": 143, "8210": 55, "821021": 71, "821095": 62, "821457": 81, "821566": 84, "821755": 63, "821855": 96, "821970": 78, "821995": 70, "8221": 53, "822289": [80, 145], "82228913": 145, "822367": 63, "822481": 60, "822482": 71, "822681": 61, "8227": 81, "822772060": 112, "822822": 71, "823247": 84, "823273": [69, 70], "823772": [98, 101], "824350": [69, 70], "824657": 78, "824701": 71, "824750": 71, "824889": 71, "824961e": 81, "8250": 53, "825587": 70, "825617": 79, "825862": 84, "825980": 71, "8259803249536914": 71, "8260": 80, "826065": [69, 70], "826426": [98, 100], "826467e": 69, "826492": 84, "826519": [36, 96], "82666866e": 112, "82684324": 87, "827332": 63, "827375": 58, "827381": 84, "827735": 84, "827938162750831": [73, 74], "828058": 81, "828157": 66, "828618": 76, "828778e": 69, "828912": 69, "828915": [73, 74], "829536": 60, "829543": 71, "829730e": 70, "829764": 96, "83": [96, 97, 112, 144], "830263": 78, "830273": 66, "830301": 83, "830442": 69, "830467": 69, "83073558": 63, "831": 85, "831019": 71, "831190": 70, "83123857": 112, "831278": 69, "831741": 69, "831833": [98, 103], "832086": 84, "8324": 98, "832540": 82, "832576": 62, "8326928": 87, "832693": 87, "832875": 84, "83287529": 84, "833": 51, "833024": 79, "833065": 66, "833227e": 97, "8334": 60, "833464": 81, "833907": 79, "834133": 76, "83436056": [98, 101], "834380": 63, "834916": [98, 101], "835": 85, "8350": 81, "835035": 78, "835043956": 112, "835239": [98, 103], "835344": 66, "835596": 81, "835750": 76, "835935": 70, "836234": 98, "838114": 84, "838235": 82, "838457": 81, "83903849": 63, "83905": 20, "83913714": 63, "839247": 63, "839259": 63, "84": [56, 75, 85, 96, 97, 98, 112, 144], "840041": 81, "840303": 84, "84030318": 84, "840593": 61, "840673": 69, "840718": [98, 100], "840836": 84, "840995e": 80, "841": [54, 79], "841132": 80, "841372e": 60, "8415": 55, "841847": 81, "842132": 98, "842405": 71, "842625": 79, "842746": 84, "84277912": 60, "8428": 80, "842853": 84, "843018": 96, "843358": 63, "843730": 79, "843796": 69, "8440": 81, "844107e": 70, "844308": 84, "844549": [73, 74], "844663": 66, "844667": 128, "844707": 84, "844889": 79, "845241": 85, "845534": 78, "846388": 71, "847029": 70, "847131": 60, "847555": 69, "847595": [35, 96], "847948": 71, "847962": 69, "847966": 81, "848090": 60, "848688e": 78, "8487233": 112, "848757e": 80, "848868": 71, "84930915e": 112, "849427": 96, "849747": 87, "8497f641": 56, "8499": 81, "85": [13, 71, 75, 81, 84, 89, 96, 97, 112], "8500000000000002": [71, 81, 84], "850038": 66, "850169": 60, "850216": 60, "850321": 79, "850439": 70, "850575": [69, 70], "850656": 76, "850794": 84, "851": 143, "851198": 81, "8513": 56, "851366": 79, "851648": 63, "852": 81, "852605": 62, "85265193": 77, "85280376": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "85342094": 63, "853648": 62, "85397773": [98, 100], "855035": 69, "855780": 84, "855862": 70, "856404": 74, "85646683": 63, "856758": 96, "8571": 53, "857161": 84, "85731520": 112, "857372": [98, 101], "857515": 96, "857544": 79, "857765": 81, "858046": 61, "858212e": 70, "85889683": 112, "858952": 66, "859": 81, "85911521e": 112, "85912862": 128, "859129": [113, 128], "8597": 80, "85974356": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "85c5": 56, "85e": 55, "86": [96, 97, 112, 144], "860663": 128, "860804": 84, "860992": 81, "861019": 66, "861210": [98, 103], "861519": 69, "862043": [73, 74], "862359": 71, "862414": 60, "862418": 61, "862828": 63, "862927": 63, "863227": 60, "863772": 80, "863982270": 113, "864": 85, "86415573": 55, "86424193e": 112, "864348": 62, "8644": 56, "864401": 61, "864664": 66, "864741e": 81, "865313": 81, "865562": [69, 70], "8658025": 60, "865854": 81, "865860": [80, 81], "865914": 70, "866102": [69, 70], "866179899731091": 90, "866179900": 90, "866579": 81, "866798": 81, "867201": 78, "867565": 84, "867776": 63, "8679": 81, "868": 56, "868002": 63, "8685788": 84, "868579": 84, "86897905": [98, 103], "869": [56, 85], "869020": 71, "869195": 70, "869398": 70, "869477": 69, "869585": 39, "869586": 75, "86981702": 63, "87": [55, 69, 75, 79, 96, 97, 112, 144], "870": 62, "8700": 55, "870099": [73, 74], "870260": 84, "870332": 84, "870444": 80, "870857": 84, "870870": [98, 101], "871": 56, "8714": 61, "871545e": 69, "871923": 70, "871972": 76, "872": 85, "872132": 70, "872222": 81, "872727": 69, "872768": 84, "872852": 84, "87290240e": 112, "872994": 81, "87309461": [98, 103], "873198": 81, "873677": [73, 74], "873848": 62, "87384812361": 53, "87384812362": 53, "87430335": 128, "874303353": 128, "874548": 63, "874702": [73, 74], "8750": 81, "8759": 81, "876": 85, "876083": 81, "876115690": 112, "876233": 62, "87623301": 53, "876431e": 71, "876450": 60, "876549": 81, "87674597e": 112, "8768": 53, "87708292": 60, "8771": 81, "877153": 81, "877455": 83, "877833": [69, 70], "877903": 66, "878281": 84, "878289": 81, "878402": 69, "878628": 60, "878746": 66, "878847e": 81, "878895": 66, "878968e": 69, "879": 98, "879049": 81, "879058": 78, "879103": 71, "879486": 62, "879509": 69, "879870": 63, "87e": 55, "88": [55, 75, 85, 96, 112], "880106": 79, "880217": [98, 101], "880579": 84, "880591": 83, "880808e": 81, "880880e": 81, "880886": 80, "880925": 61, "880926": 62, "8810": 80, "881201": 81, "88125046e": 112, "881462": 63, "881465": 59, "881581": 33, "88173062": 54, "881937": 66, "88196266": 112, "882154": 63, "882311": 62, "882475": 71, "882641": 80, "882928": 66, "883485": 70, "883622": 84, "883914": 71, "883953": 78, "884": 145, "884132": 84, "8843": 87, "8845": 53, "884774": 60, "884821": 96, "884996": 71, "8850": 55, "885065": 84, "885832": 85, "885956": 69, "885978": [73, 74], "886041": 70, "886086": [69, 70], "886266": 81, "88629": 53, "886314": 70, "88664": 56, "887182": [98, 99], "887345": 81, "887531": 60, "887556": 71, "887648": 70, "887680": 69, "888146": 79, "8881461": 54, "888352": 66, "888445": 70, "888775": 74, "888804": 81, "88882034": 60, "8891": 61, "889293": 84, "889326": 70, "889566": 87, "889638": 66, "889703": 60, "889733": 84, "889792": 70, "88988263e": 112, "889913": [69, 70], "889963": 84, "88ad": 56, "89": [55, 70, 96, 112, 143, 144], "890": [54, 79], "890204": [98, 103], "89027368": 128, "890273683": 128, "890318": 69, "89035917": 75, "890372": [64, 92, 94, 142], "8903720000100010000010": [56, 92, 94, 142], "8904": 51, "890454": 97, "890507": 82, "890665": 76, "890785": 63, "890837": 61, "890855": 66, "8909": [54, 80, 145], "891527": [98, 103], "891606": 80, "891752": 70, "891997": 69, "892": 56, "892648": 84, "892796": [69, 70], "892828": 76, "893": 56, "8932105": 54, "893461": 66, "893649": [69, 70], "893851": 84, "893899": 63, "894": 56, "894307e": 81, "894318": 60, "894448": 70, "894541": 60, "8946549": 57, "89472978": [98, 103], "895106": [69, 70], "895308": 81, "895333": 84, "895442": 80, "895690": [69, 70], "895768e": 71, "896023": 84, "896182e": 76, "896263": 76, "8966733": 60, "896761": 61, "897220": 84, "897240": 81, "8974": 80, "8974226": [98, 101], "897451": 69, "897495e": 70, "898183": 66, "89864665": 60, "898722": 84, "899021": 96, "899250": 66, "899296": [98, 101], "899460": 84, "899654": 66, "899662e": 69, "899716": 70, "8bdee1a1d83d": 56, "8da924c": 56, "8e3aa840": 56, "9": [20, 22, 23, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 90, 92, 94, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 129, 135, 141, 142, 144, 145], "90": [15, 55, 57, 58, 71, 81, 84, 89, 96, 112, 144], "9000000000000002": [71, 81, 84], "900000e": 81, "900021": 97, "900127": [98, 103], "900497": 63, "90069466": 60, "90079483": 60, "900829": 76, "90088042": 60, "901013": 70, "901148": 84, "90136": 80, "901360": 80, "90145324": [98, 103], "901526": 75, "901683": 81, "901705": 69, "901725": 63, "902": 128, "902297": 60, "902573": 71, "90289125": 63, "902920": 96, "903056e": 84, "903275": 60, "903285": 63, "903339": 71, "903351e": 71, "903418": 79, "903488": 60, "903674": 69, "903681": 84, "903767": [69, 70], "904156": 71, "9041560442482157": 71, "904315": 69, "904347": 60, "904396": 69, "905042": 70, "905494": 71, "905858": 88, "905951": 87, "906072": 96, "9061": 81, "906195": [98, 101], "906716732639898": [73, 74], "906757": 67, "90705726": 60, "907115": 84, "907176": 84, "9073": 81, "907491": 71, "907650": 63, "907801": 79, "90781325": 60, "907879": 66, "90794478": 128, "907944783": 128, "907961": 81, "908024": 88, "908663": 70, "908767": 78, "909304": [69, 70], "909571": 66, "909589": 63, "90963122e": 112, "909942e": 96, "909975": 81, "909997": [80, 145], "91": [86, 96, 112, 144], "910000e": 81, "910208": 63, "910371": 60, "91053356": [98, 101], "910895": 70, "9109": 56, "91102953": 84, "911030": 84, "9112": 80, "911277": 70, "911662": 73, "912230": [69, 70], "9126": [55, 145], "9127": [55, 145], "912903": 69, "913": 56, "91315015": 54, "913280": 88, "913371": 70, "913415e": 69, "913485": 81, "913585": 76, "913774": 71, "914176": 60, "914187": 62, "9142": 81, "91436576": 63, "91438767e": 112, "9145": 53, "914598": 66, "915": [55, 56, 80, 81], "915000e": [80, 81], "915007": 60, "915260e": 69, "915488": [73, 74], "9158080176561963": 78, "916": 51, "916090": 61, "9161": 61, "916236": 53, "916359": 66, "916528": 73, "9166667": 56, "91683613": [98, 101], "916914": 84, "916930": 69, "917": 56, "917000": 70, "917066": 81, "917248": 84, "91724807": 84, "917436": 84, "918": 85, "918227": 71, "918293": [98, 103], "919432": 84, "9197": 81, "919969": 69, "91e": 55, "92": [60, 86, 96, 97, 112, 144], "920052": 70, "920105": 63, "920335": 81, "920337": 74, "920430": 62, "920645": 81, "9209": 53, "9210": 81, "9210139": 60, "921061": 88, "921198": 76, "921256e": 70, "921372": 71, "921778": 76, "921913": 79, "921956": [69, 70], "921e4f0d": 56, "922160": 81, "922251": 69, "9223": 81, "922668": 76, "922996": 79, "923074e": 71, "923517": 89, "923607": 84, "92369755": 54, "923804": 71, "923943": 145, "923977": 81, "924002": 84, "9243": 81, "924396": [73, 74], "924443": 66, "924634": 59, "9248": 56, "924821": 71, "924843": 79, "924921": 96, "925": 58, "925187": 60, "925248": [73, 74], "92562619": 60, "92566": 98, "925660": 69, "925736": 71, "925957": 73, "925994": 80, "925995": 70, "926196": 62, "926227": 70, "926621": 71, "926901": 78, "927": [52, 145], "927074": 84, "927232": 81, "9274": 81, "92749181": 60, "927950": 81, "92827999": 98, "92830753": 60, "92881435e": 112, "928947": 79, "929002": 62, "92905": 54, "929282": 62, "929345e": 60, "929643": 69, "92964644": [98, 101], "92972925e": 128, "929729e": [113, 128], "92983553": 60, "93": [55, 76, 86, 96, 97, 112, 144], "9304028": 54, "93074943": [98, 103], "931": 91, "93134081": [98, 101], "931479": 84, "931978": 142, "932027": 71, "932404e": 81, "932407": 61, "9325": 53, "9327": 53, "932973": 84, "93300": 98, "933322": 70, "933671": 70, "933857": 70, "933996": 71, "934058": 69, "934243": 70, "934433": [69, 70], "9345": 56, "934500": 70, "934511": 128, "934549": 81, "93458": 87, "934642": 62, "934963": 70, "934992": 71, "935": [40, 77, 98], "935220": [98, 99], "935384": 60, "935591": 84, "935730": 84, "935764": 70, "935793": [98, 101], "935989": 79, "9359891": 54, "93648": 89, "936494": 69, "936739": 84, "937102": 62, "937116": 79, "937586": 81, "937893": 60, "938": 128, "938263": [98, 103], "938355": 61, "938836": [98, 103], "939068": [73, 74], "9392": 81, "939250": 69, "939458": 69, "9395": 81, "93958082416": 145, "939814": 63, "94": [58, 77, 96, 112, 144, 145], "94030041": 60, "940354721701296": 71, "940355": 71, "940373": 81, "94076349": 60, "941051": 60, "941113": 60, "941440": 69, "941724": 81, "941788": 73, "942139": 74, "942312": 84, "94243755": 60, "942460e": 84, "942489": 81, "9425": 53, "942550": 81, "942661": 79, "942823": 81, "942864e": 60, "94309994e": 112, "943548": 76, "943693": [98, 99], "943938": 84, "943949e": 84, "944253e": 84, "944266": [73, 74], "94427158": [98, 103], "944280": 81, "94441007e": 112, "94473": 57, "945881": 69, "946180": 76, "94629": 89, "946297": 71, "946406": 74, "946433": 84, "946533": 69, "94659176": 60, "946658": 81, "94676689": 63, "946968": 71, "946983": 63, "947350": 63, "947440": 83, "947466": 97, "947613": 70, "947855": 66, "9480": 81, "948112": 85, "948154e": 73, "948730": 62, "948785e": 69, "948868": 81, "948975": 75, "94906344": 54, "949241": 128, "949456": 84, "949866": 70, "95": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 53, 55, 57, 58, 59, 60, 61, 62, 63, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 80, 81, 84, 85, 87, 88, 89, 96, 98, 112, 128, 129, 135, 144, 145], "950": 51, "9500": 81, "950158": 69, "950424": 63, "950545": 67, "95062986e": 112, "95099547": 60, "951502": 84, "951532": 79, "95161334": 63, "951920": 83, "952": [55, 85, 145], "952146": [98, 103], "9523": 53, "95281352": 60, "952839": 84, "95305": 57, "95311164": [98, 103], "9534": 81, "953683": 79, "953704": 69, "95372559e": 112, "953884": 98, "954": 128, "95401167e": 112, "955005e": 81, "9551": 81, "9552": 53, "955541": [36, 96], "95559917": 97, "955701": 69, "955e": 98, "956044": 62, "956047": 54, "956074": 60, "9561": 53, "956574": 81, "956588": 98, "956724": 71, "9567242535070148": 71, "956747": 62, "956877": 70, "956892": 81, "957052494": 112, "957229": 74, "957375": 79, "95739593": 60, "957745": 71, "9579": 55, "957996": 71, "958": 128, "9580": 55, "95802522": 60, "958105": 96, "958247": 60, "958356e": 63, "958541": 81, "958636": 75, "958789": [98, 101], "959132": 70, "959384": 70, "959613": 96, "95e": 55, "96": [55, 69, 70, 82, 96, 112, 144], "960236": 98, "9605": 81, "960808": 71, "960834": 70, "9609": 53, "961539": 81, "961962": 71, "962364": 66, "962373": 70, "962700": 63, "962954": 70, "963055": 81, "963144": 60, "963427e": 70, "96355697": 63, "964025e": 84, "964065e": 70, "964261e": 79, "964318": 81, "964321": 60, "9647": 53, "965341": 70, "965531": 98, "965696": 69, "965774": 81, "96582": 97, "965831": 63, "966015": 84, "966097": 37, "966659": 71, "9666592590622916": 71, "96697013": 63, "967092": 70, "96742036": 112, "967467": 87, "968127": 66, "968134e": 84, "968258e": 69, "968577": 58, "968772": 63, "969": 82, "969141": [96, 97, 98], "969232": 61, "9699": 80, "969925e": 70, "97": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 58, 59, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 142, 144, 145], "970065": 84, "970150": 70, "970365": 63, "971058": [73, 74], "971145": 62, "97135467": 63, "971901": 63, "971967": [98, 101], "972": 82, "972138e": 60, "972509": 70, "972745": 69, "972748": 71, "97276281": 84, "972763": 84, "97314470": 54, "973156": 96, "973229": 70, "973241": 84, "97327108": 60, "973331": 81, "97362675": 60, "973741": 70, "973890": 70, "974004": 63, "974175": 62, "974202": 71, "974213": 70, "97441062": [73, 74], "974414": 71, "974487": 69, "97470872": 87, "9748910611": 54, "97498141": 63, "975": [69, 70, 73, 74, 76, 77, 82, 98, 101], "975289": 66, "9753": 56, "975349": 60, "975447": 58, "975450": 70, "975461": 79, "975592": 66, "97590725": 63, "976088": 84, "97619643": [98, 103], "976548e": 70, "976562": 84, "977166": 63, "977202": 70, "977280": [69, 70], "977295": 81, "977507": 70, "977820": 69, "978": 82, "978303": 78, "9787": 81, "978977": 84, "979": [51, 85], "979384": 76, "979475": 69, "979583": 60, "979702": 69, "979857": 69, "979966": 76, "979971e": 69, "98": [69, 70, 81, 96, 112, 144], "980026": 81, "98023747": [98, 101], "9802393": 54, "980440": 70, "980643e": 71, "981104": 83, "981403": 70, "981438": 69, "981672": 71, "981715": 69, "982": 82, "982019e": 70, "982353e": 81, "982417": 71, "982720": 69, "982797": 83, "983192": 84, "983253": 69, "983531": 60, "983759": 145, "98393441": 87, "9840": 63, "984024": 83, "984083": [73, 74], "984551": 32, "984562": 84, "984866": 128, "984872": [69, 70], "984937": 71, "98503007": 60, "98505871e": 112, "985207": [69, 70], "985654": 70, "986249": 80, "986383": 81, "986417": 69, "986658974": 112, "98673": 75, "987": 82, "9870004": 56, "987220": 81, "987307": 78, "9875": 53, "987726": 70, "9880384": 56, "988421": [69, 70], "988463": 84, "988541": 69, "988709": 81, "988712": 63, "988780": 81, "989631": 62, "989767": 82, "99": [55, 66, 69, 70, 82, 96, 112, 144], "990": 82, "990163": 62, "990210": 81, "990352": 82, "99060365": 60, "990903": 69, "990957": 62, "991": 56, "9914": [80, 81, 87], "991444e": 73, "9915": [55, 80, 81, 87], "991512": 55, "991963": [69, 70], "991977": 81, "991988": 69, "992": 85, "992060": 60, "99232145": 87, "992432": 62, "992582": [69, 70], "992681": 60, "993201": 70, "993416": 76, "993512": [98, 101], "993575": 81, "994": 85, "994168239": 54, "994208": 66, "994214": 81, "994332": 67, "994367": 63, "994377": 69, "9944": [77, 98, 100], "9948104": 57, "994851": 81, "994937": 74, "995015": 81, "9951": 53, "995248": 84, "99549118e": 112, "99571372e": 112, "996": 82, "996026": 60, "9961": 80, "9961392": 54, "996313": 69, "996892": 78, "996934": 79, "9970": 81, "997034": 89, "997353": 60, "997494": 89, "997571": 79, "997621": 71, "997774": 63, "997934": [73, 74], "998063": 67, "998233": 60, "99864670889": 145, "998766": 81, "999": [58, 59, 75, 87, 145], "999207": 84, "9995": [59, 69, 70], "9996": [59, 69, 70], "9996553": 55, "9997": [59, 69, 70], "9998": [59, 69, 70], "9999": [59, 69, 70], "99c8": 56, "A": [7, 9, 10, 13, 14, 18, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 51, 52, 53, 55, 56, 60, 61, 62, 63, 67, 68, 72, 74, 77, 78, 82, 83, 85, 86, 87, 88, 91, 92, 94, 96, 97, 98, 99, 101, 103, 111, 128, 129, 130, 136, 137, 138, 139, 140, 142, 143, 145], "ATE": [9, 33, 37, 55, 64, 66, 76, 80, 87, 88, 96, 98, 108, 111, 113, 121, 129, 137], "ATEs": [66, 82], "And": [57, 82, 89, 129, 131, 132], "As": [52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 66, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 84, 88, 89, 90, 97, 98, 101, 102, 103, 112, 113, 115, 117, 119, 128, 129, 130, 139, 145], "At": [9, 10, 26, 54, 58, 59, 61, 66, 75, 77, 79, 81, 84, 145], "Being": 145, "But": [76, 77], "By": [53, 54, 79, 85, 88, 97, 98, 101, 103, 129, 135], "For": [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 33, 39, 43, 47, 51, 53, 54, 56, 58, 60, 61, 62, 63, 66, 67, 75, 76, 77, 78, 79, 81, 83, 85, 87, 88, 90, 91, 92, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 137, 139, 140, 141, 142, 145], "ITE": [14, 66], "ITEs": 66, "If": [4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 52, 54, 58, 60, 61, 62, 63, 68, 69, 70, 76, 77, 79, 81, 85, 91, 92, 94, 96, 97, 98, 100, 106, 113, 114, 115, 116, 117, 118, 121, 128, 129, 130, 131, 132, 133, 134, 135, 138, 139, 140, 145], "In": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 46, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 112, 113, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145], "It": [21, 53, 54, 55, 69, 70, 72, 73, 74, 79, 80, 81, 85, 86, 88, 97, 98, 99, 112, 140, 144], "No": [12, 51, 53, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 75, 80, 81, 85, 87, 89, 92, 93, 94, 97, 98, 100, 101, 103, 113, 128, 142, 143], "Not": [98, 104], "Of": [77, 128, 145], "On": [52, 68, 78, 82, 86, 91, 143], "One": [55, 60, 63, 80, 81, 88, 96, 128], "Or": 40, "Such": [88, 97], "That": [40, 145], "The": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 104, 107, 109, 110, 111, 112, 113, 118, 121, 125, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 143, 144, 145], "Then": [26, 71, 84, 86, 98, 128, 129, 139, 140, 141], "There": [55, 80, 88, 98, 104, 141, 145], "These": [32, 55, 56, 61, 65, 78, 80, 83, 85, 87, 96, 98, 145], "To": [28, 51, 52, 54, 55, 56, 57, 58, 60, 61, 62, 63, 66, 67, 68, 69, 70, 71, 73, 74, 76, 77, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 91, 92, 94, 96, 97, 98, 102, 112, 128, 129, 130, 135, 139, 140, 141, 142, 145], "With": [13, 69, 70, 97, 143], "_": [52, 54, 59, 60, 62, 63, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 83, 84, 85, 86, 90, 91, 96, 98, 101, 104, 112, 113, 115, 128, 129, 130, 135], "_0": [52, 54, 68, 72, 79, 90, 91, 112, 113, 126, 127, 128, 129, 139], "_1": [9, 10, 14, 25, 26, 57, 82, 89, 113, 126, 127], "_2": [9, 10, 14, 25, 26, 82], "_3": [9, 10, 14, 25, 26], "_4": [9, 10, 14, 25, 26], "_5": [9, 14], "__": [46, 47], "__init__": [78, 86], "__version__": 141, "_a": [113, 115, 117], "_all_coef": 112, "_all_s": 112, "_b": [113, 115, 117], "_compute_scor": 28, "_compute_score_deriv": 28, "_coordinate_desc": 79, "_d": [85, 98], "_est_causal_pars_and_s": 144, "_estimator_typ": 78, "_h": [85, 98], "_i": [52, 57, 68, 84, 89, 91, 98, 104], "_id": 112, "_j": [9, 10, 14, 16, 25, 26, 54, 79, 128], "_l": 97, "_lower_quantil": [60, 63], "_m": [97, 112], "_mean": [60, 63], "_n": [113, 114, 115, 116, 117, 121, 128, 129, 135, 138], "_n_folds_per_clust": 79, "_offset": 97, "_pred": 97, "_rmse": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "_upper_quantil": [60, 63], "_x": 41, "_y": [85, 98], "a0": 78, "a09a": 56, "a09b": 56, "a1": 78, "a3d9": 56, "a4a147": 82, "a5e6": 56, "a5e7": 56, "a6ba": 56, "a79359d2da46": 56, "a840": 56, "a_": [57, 89], "a_0": 17, "a_1": 17, "a_j": [98, 105], "ab": [53, 86, 140], "ab71": 56, "abadi": [7, 58], "abb0fd28": 56, "abdt": [64, 92, 94, 142], "abl": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 52, 68, 77, 81, 82, 97, 129, 130, 139], "about": [22, 24, 55, 58, 59, 77, 80, 86, 98, 140, 142, 145], "abov": [52, 55, 61, 66, 68, 69, 70, 73, 74, 77, 78, 80, 82, 83, 84, 85, 88, 91, 96, 97, 98, 102, 104, 141], "absolut": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 77, 97], "abstract": [28, 53, 54, 79, 113, 140, 144], "abus": [60, 63, 98, 101, 103, 104], "acc": [30, 53], "accept": [25, 96, 97], "access": [42, 43, 53, 55, 60, 61, 62, 63, 73, 74, 75, 77, 87, 97, 129, 135, 145], "accompani": 140, "accord": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 55, 57, 58, 61, 66, 68, 71, 80, 84, 85, 88, 89, 97, 98, 99, 104, 128, 129, 131, 132, 133, 134, 136, 137, 145], "accordingli": [57, 58, 77, 78, 80, 85, 89], "account": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 54, 55, 60, 63, 79, 80, 81, 87, 88, 129, 135, 139, 145], "accumul": [55, 80, 81, 87], "accuraci": [42, 46, 53, 98], "acemoglu": 143, "achiev": [54, 76, 79, 83, 88, 98, 128], "acic_2024_post": 82, "acknowledg": [55, 56, 80], "acm": 143, "acov": 143, "across": [21, 25, 55, 80, 82, 145], "action": 144, "activ": [4, 5, 6, 141, 144], "actual": [40, 60, 63, 75, 88], "acycl": [57, 89, 145], "ad": [4, 5, 6, 7, 8, 28, 42, 43, 46, 47, 75, 92, 94, 97, 98, 128, 129, 130, 144], "adapt": [32, 80, 144], "add": [53, 54, 57, 58, 59, 64, 66, 73, 74, 75, 82, 84, 85, 87, 88, 89, 97, 98, 143, 144], "add_trac": 88, "addit": [14, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 44, 60, 61, 62, 63, 72, 88, 93, 94, 97, 98, 99, 113, 122, 129, 136, 137, 139, 143, 144], "addition": [9, 10, 63, 66, 71, 81, 87, 97, 98, 112, 128, 129, 135, 142], "additional_inform": 21, "additional_paramet": 21, "address": 88, "adel": 143, "adj": [85, 88], "adj_coef_bench": 88, "adj_est": 88, "adj_vanderweelearah": 88, "adjust": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 45, 54, 59, 76, 79, 81, 87, 88, 96, 98, 104, 128, 129, 135, 143, 144, 145], "adopt": [58, 98, 100, 104], "advanc": [78, 95, 112, 143], "advantag": [52, 53, 55, 66, 68, 80, 81, 91, 141], "advers": [129, 130], "adversari": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 87, 129, 135, 139], "ae": [52, 54, 55, 57], "ae56": 56, "ae89": 56, "aesthet": 52, "aeturrel": 18, "afd9e4": 82, "affect": [66, 72, 98, 144, 145], "after": [53, 55, 56, 57, 58, 72, 80, 81, 88, 89, 96, 97, 98, 104, 129, 131, 135, 141, 145], "after_stat": 52, "ag": [55, 80, 81, 83, 87, 145], "again": [52, 53, 54, 55, 57, 58, 60, 63, 66, 68, 75, 78, 79, 80, 85, 86, 87, 88, 89, 91, 129, 131, 132], "against": [58, 75, 77, 83, 97], "agebra": 96, "agegt54": [56, 64, 92, 94, 142], "agelt35": [56, 64, 92, 94, 142], "agg": [53, 60, 63, 86], "agg_df": [60, 63], "agg_df_anticip": [60, 63], "agg_dict": [60, 63], "agg_dictionari": [60, 63], "agg_did_obj": [98, 99], "aggrag": [98, 99], "aggreg": [21, 24, 53, 90, 99, 112, 144], "aggregate_over_split": 40, "aggregated_eventstudi": [60, 61, 62, 63], "aggregated_framework": [60, 61, 62, 63, 98, 99], "aggregated_group": [60, 63], "aggregated_tim": [60, 62, 63], "aggregation_0": 21, "aggregation_1": 21, "aggregation_color_idx": 21, "aggregation_method_nam": 21, "aggregation_nam": 21, "aggregation_weight": [21, 60, 61, 62, 63], "aggt": 53, "ai": [62, 86, 143], "aim": 85, "aipw": 82, "aipw_est_1": 82, "aipw_est_2": 82, "aipw_obj_1": 82, "aipw_obj_2": 82, "air": [54, 79], "al": [7, 8, 11, 13, 16, 17, 52, 54, 55, 56, 58, 68, 69, 70, 71, 72, 73, 74, 77, 79, 80, 81, 84, 87, 91, 98, 100, 104, 112, 113, 119, 121, 122, 123, 128, 129, 130, 139, 140, 142, 144], "alexandr": [72, 143], "algebra": 98, "algorithm": [51, 53, 54, 56, 57, 58, 60, 61, 62, 63, 66, 68, 71, 76, 77, 79, 81, 84, 87, 89, 95, 97, 98, 100, 101, 103, 112, 113, 128, 144, 145], "alia": [42, 43, 46, 47], "align": [52, 54, 57, 59, 60, 63, 68, 71, 77, 79, 80, 82, 83, 84, 89, 98, 101, 103, 104, 113, 115, 117, 144], "all": [4, 5, 6, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 46, 47, 48, 52, 53, 54, 55, 57, 58, 66, 68, 75, 76, 77, 78, 79, 80, 81, 83, 85, 88, 89, 91, 92, 93, 94, 96, 97, 98, 99, 100, 102, 104, 112, 113, 115, 117, 128, 129, 139, 140, 141, 144], "all_coef": 112, "all_dml1_coef": 90, "all_s": 112, "all_smpl": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 76], "all_smpls_clust": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "all_z_col": [54, 79], "allow": [4, 5, 6, 20, 21, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 55, 60, 63, 66, 80, 81, 85, 94, 96, 97, 98, 112, 113, 128, 140, 144, 145], "almqvist": 143, "along": 97, "alpha": [15, 17, 20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 55, 57, 60, 63, 64, 66, 68, 69, 70, 71, 72, 76, 77, 78, 79, 80, 81, 84, 90, 91, 96, 97, 98, 112, 113, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139], "alpha_": [16, 54, 79, 97], "alpha_0": [129, 139], "alpha_ml_l": 72, "alpha_ml_m": 72, "alpha_x": [12, 32, 98], "alreadi": [26, 57, 58, 60, 63, 86, 89, 97, 98, 100], "also": [20, 22, 23, 24, 29, 32, 33, 39, 51, 52, 53, 54, 55, 56, 58, 60, 61, 63, 66, 67, 68, 69, 70, 73, 74, 75, 77, 78, 79, 80, 81, 83, 85, 87, 88, 91, 96, 97, 98, 112, 113, 128, 129, 130, 141, 142, 144, 145], "alter": [54, 79], "altern": [53, 55, 56, 61, 80, 83, 95, 97, 128, 140, 142], "although": 88, "alwai": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 47, 53, 85, 144], "always_tak": [32, 55, 80], "alyssa": 143, "amamb": 79, "american": [15, 82], "amgrem": 79, "amhorn": 79, "amit": [88, 143], "amjavl": 79, "ammata": 79, "among": [55, 72, 80, 81, 87, 88], "amount": [24, 55, 78, 80, 81, 145], "amp": [51, 54, 56, 57, 58, 60, 61, 62, 63, 79, 81, 87, 89], "an": [4, 5, 6, 9, 10, 14, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 52, 53, 54, 55, 56, 57, 60, 61, 62, 63, 66, 68, 69, 70, 72, 75, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 96, 97, 98, 99, 102, 104, 105, 112, 113, 128, 129, 130, 135, 140, 142, 143, 144, 145], "analog": [27, 28, 54, 60, 61, 63, 79, 81, 87, 96, 98, 100, 113, 114, 115, 116, 117, 128, 129, 135], "analys": 145, "analysi": [20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 52, 54, 55, 68, 79, 80, 81, 86, 91, 95, 96, 98, 103, 130, 135, 139, 140, 144], "analyst": 86, "analyt": [82, 84], "analyz": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 55, 80, 81, 87, 145], "ancillari": 88, "andrea": 143, "angl": 55, "angrist": 82, "ani": [51, 52, 53, 56, 57, 58, 67, 68, 86, 88, 89, 91, 98, 141, 145], "anna": [9, 10, 14, 20, 22, 23, 25, 26, 53, 58, 60, 61, 62, 63, 98, 99, 100, 102, 104, 143], "annal": [128, 143], "anneal": 97, "annot": 52, "annual": 143, "anoth": [52, 53, 54, 55, 68, 77, 78, 79, 86, 91, 97, 98, 106], "anticip": [22, 24, 25, 53, 61, 62, 98, 99, 101, 102, 103, 104], "anticipation_period": [22, 24, 25, 60, 63], "anymor": [54, 79], "aos1161": 128, "aos1230": 128, "aos1671": 128, "ap": [55, 80], "ape_e401_uncond": 55, "ape_p401_uncond": 55, "api": [92, 94, 140, 144], "apo": [29, 30, 105, 118, 136], "apoorva": 144, "apoorva__l": 82, "apoorval": 82, "app": 144, "appeal": 88, "append": [68, 77, 86, 91], "appendix": [13, 19, 57, 60, 63, 87, 89, 129, 130], "appli": [8, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 54, 55, 56, 57, 58, 59, 60, 62, 63, 68, 76, 77, 79, 80, 81, 85, 86, 88, 89, 91, 98, 112, 113, 128, 140, 142, 143, 144, 145], "applic": [52, 58, 68, 82, 88, 91, 96, 112, 143, 145], "applicatoin": 61, "apply_along_axi": 83, "apply_cross_fit": [52, 112], "apply_crossfit": 144, "appreci": 140, "approach": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 53, 54, 66, 79, 85, 87, 88, 95, 97, 98, 112, 128, 129, 130, 141, 143, 145], "appropri": [55, 72, 80, 98, 112, 145], "approx": [96, 98, 101, 103], "approxim": [52, 68, 69, 70, 71, 77, 84, 88, 91, 96, 98, 128, 144, 145], "april": [60, 63], "apt": 141, "ar": [4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 68, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 124, 125, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145], "arang": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 59, 68, 71, 81, 83, 84, 87, 88, 97], "arbitrarili": [43, 47], "architectur": [113, 143], "arellano": 143, "arg": [78, 85, 96, 98], "argmax": 86, "argmin": 77, "argu": [52, 55, 68, 80, 81, 87, 91, 145], "argument": [16, 17, 18, 20, 22, 23, 25, 26, 29, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 44, 55, 58, 60, 61, 62, 63, 69, 70, 75, 77, 80, 81, 86, 90, 96, 97, 98, 99, 144, 145], "aris": [52, 53, 54, 61, 68, 79, 88, 91, 145], "aronow": 82, "around": [53, 55, 80, 81, 85, 98, 113], "arr": 83, "arrai": [4, 5, 6, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 45, 46, 47, 57, 58, 60, 61, 62, 63, 66, 68, 69, 70, 71, 77, 79, 82, 83, 86, 87, 88, 89, 90, 91, 96, 97, 112, 128, 129, 135, 142, 144, 145], "arrang": 54, "array_lik": 36, "articl": [18, 140], "arxiv": [16, 53, 54, 79, 86, 88, 140, 143, 144], "as_learn": [56, 97], "asarrai": [69, 70], "aspect": [55, 80, 81], "assert": 97, "assess": 53, "asset": [81, 87, 145], "assign": [4, 5, 6, 25, 55, 60, 63, 74, 80, 85, 96, 97, 98, 99, 111, 129, 145], "assmput": [98, 111], "associ": [55, 72, 80, 98, 128, 143], "assum": [51, 54, 58, 67, 79, 82, 83, 86, 88, 98, 99, 100, 102, 104, 113, 114, 115, 116, 117, 128, 129, 139, 145], "assumpt": [53, 54, 55, 57, 58, 59, 60, 61, 63, 77, 79, 80, 82, 85, 89, 98, 99, 100, 102, 104, 111, 128, 145], "assur": 144, "astyp": [60, 62, 63, 67, 85, 88], "asymptot": [27, 28, 52, 54, 68, 79, 91, 112, 128, 143], "ate": 66, "ate_estim": [57, 89], "ates": 66, "athei": 143, "att": [9, 24, 25, 33, 53, 59, 75, 76, 83, 88, 96, 98, 99, 100, 101, 102, 103, 104, 108, 113, 121, 129, 137, 144], "att_": [98, 102], "att_gt": [53, 61, 62], "attach": 53, "atte_estim": 58, "attempt": [42, 43], "attenu": [55, 80], "attr": 55, "attribut": [4, 5, 6, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 45, 46, 47, 77, 78, 90, 93, 94, 97, 112, 113, 128], "attributeerror": [42, 43], "attrict": 98, "attrit": [37, 57, 89, 98, 111], "au": [56, 97, 140, 142], "auc": 53, "author": [53, 88, 140], "auto_ml": 78, "autodoubleml": 78, "autom": 78, "automat": [21, 24, 52, 60, 62, 63, 68, 75, 91, 96, 129, 135], "automl": 144, "automl_l": 78, "automl_l_lesstim": 78, "automl_m": 78, "automl_m_lesstim": 78, "automobil": [54, 79], "autos": 72, "autosklearn": 78, "auxiliari": [52, 68, 91], "avail": [12, 53, 55, 56, 58, 60, 61, 63, 66, 72, 77, 80, 81, 82, 83, 85, 88, 91, 96, 97, 98, 99, 100, 101, 103, 129, 139, 140, 141, 144, 145], "avaiv": 45, "aver": 66, "averag": [9, 10, 25, 26, 29, 30, 32, 33, 39, 51, 53, 56, 57, 58, 59, 60, 62, 63, 67, 75, 81, 82, 83, 85, 86, 87, 88, 89, 95, 99, 100, 104, 105, 106, 107, 108, 111, 118, 121, 128, 136, 137, 143, 144, 145], "average_it": 66, "avoid": [52, 53, 68, 85, 98, 112, 141, 144], "awai": 87, "ax": [21, 24, 59, 60, 61, 62, 63, 66, 68, 69, 70, 71, 73, 74, 77, 78, 79, 80, 81, 82, 84, 85], "ax1": [66, 71, 76, 81, 84], "ax2": [66, 71, 76, 81, 84], "axhlin": [59, 78, 85], "axi": [21, 24, 54, 55, 66, 72, 76, 77, 79, 80, 82, 83, 85], "axvlin": [66, 68], "b": [18, 20, 22, 23, 24, 25, 52, 54, 56, 68, 69, 70, 79, 82, 84, 85, 88, 91, 96, 97, 98, 104, 128, 129, 139, 140, 142, 143], "b208": 56, "b371": 56, "b5d34a6f42b": 56, "b5d7": 56, "b_": 98, "b_0": 17, "b_1": 17, "b_j": 18, "bach": [72, 77, 78, 88, 140, 143, 144], "backbon": 77, "backend": [4, 5, 6, 53, 81, 87, 88, 92, 93, 95, 144], "backward": 144, "bad": 82, "balanc": [55, 60, 61, 63, 80, 81], "band": [53, 95, 145], "bandwidth": [34, 35, 36, 40, 85, 98, 144], "bar": [75, 78, 80, 96, 98, 113, 118, 121, 129, 136], "base": [11, 14, 20, 22, 23, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 45, 52, 53, 54, 55, 57, 58, 59, 61, 62, 66, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 96, 97, 98, 99, 102, 104, 111, 112, 113, 128, 129, 130, 135, 140, 142, 143, 144, 145], "base_estim": [46, 47, 85], "baseestim": 86, "baselin": [14, 22, 55, 78, 80], "basi": [29, 33, 39, 44, 63, 69, 70, 76, 96], "basic": [53, 54, 55, 58, 62, 79, 80, 81, 82, 85, 87, 88, 95, 97], "basis_df": 76, "basis_matrix": 76, "batch": 56, "battocchi": 143, "bay": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 128], "bb2913dc": 56, "bbotk": [56, 97, 144], "bbox_inch": 68, "bbox_to_anchor": 68, "bcallaway11": [53, 61], "bd929a9e": 56, "bde4": 56, "becam": [55, 80, 81], "becaus": [43, 47, 51, 52, 53, 54, 67, 68, 74, 75, 79, 82, 86, 88, 91, 145], "becker": [56, 97], "becom": [54, 74, 78, 79, 96, 112], "bee": 59, "been": [21, 24, 54, 55, 60, 62, 63, 78, 79, 80, 81, 87, 88, 96, 97, 98, 104, 140, 144], "befor": [25, 53, 55, 59, 60, 62, 63, 66, 75, 80, 84, 88, 98, 100, 145], "begin": [12, 15, 16, 52, 54, 55, 56, 57, 59, 60, 63, 68, 71, 77, 79, 80, 82, 83, 84, 89, 90, 92, 94, 97, 98, 101, 103, 104, 112, 113, 115, 117, 128, 142, 145], "behav": [60, 62, 63, 74, 86], "behavior": [55, 82, 97], "behaviour": 74, "behind": [61, 98], "being": [14, 19, 25, 27, 28, 41, 46, 47, 54, 79, 85, 88, 98, 103, 104, 112, 113, 119, 128, 129, 135, 140], "belloni": [13, 72, 128, 143], "below": [51, 55, 61, 67, 80, 82, 86, 98, 141, 142], "bench_x1": 88, "bench_x2": 88, "benchmark": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 66, 75, 130, 144], "benchmark_dict": [48, 87], "benchmark_inc": 87, "benchmark_pira": 87, "benchmark_result": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39], "benchmark_twoearn": 87, "benchmarking_set": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 66, 75, 87, 88, 129, 130], "benchmarking_vari": 75, "benefit": [52, 55, 68, 80, 91], "bernoulli": 12, "berri": [54, 79], "besid": 142, "best": [29, 33, 39, 43, 44, 47, 60, 63, 69, 70, 73, 74, 78, 141], "best_loss": 78, "beta": [12, 13, 15, 19, 37, 55, 57, 80, 83, 85, 89, 98], "beta_": [57, 89], "beta_0": [11, 57, 83, 89, 96], "beta_a": [9, 10, 88], "beta_j": [12, 13, 15, 19], "better": [53, 60, 63, 66, 77, 88, 98, 104], "between": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 57, 59, 60, 63, 66, 67, 71, 72, 78, 82, 84, 86, 87, 88, 89, 98, 106, 113, 114, 115, 116, 117, 121, 124, 125, 128, 129, 139, 142, 144], "betwen": [51, 67], "beyond": 143, "bia": [19, 51, 57, 67, 72, 85, 88, 89, 95, 98, 111, 112, 113, 126, 127, 129, 139, 143, 144], "bias": [51, 55, 60, 63, 67, 80, 81, 87, 145], "bias_bench": 88, "bibtex": 140, "big": [72, 90, 112, 113, 114, 115, 122, 128, 129, 131, 132, 133, 134, 137, 138, 139], "bigg": [54, 79, 113, 120, 121, 129, 132, 137], "bilia": 8, "bilinski": 143, "bin": [52, 66, 68, 141], "binari": [11, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 37, 39, 41, 51, 53, 55, 56, 58, 67, 75, 76, 77, 80, 82, 83, 88, 96, 97, 100, 104, 107, 108, 111, 129, 136, 137, 144, 145], "binary_outcom": 41, "binary_treat": [11, 69, 73, 75], "bind": 144, "binder": [56, 97, 140, 142, 144], "binomi": [67, 82, 83, 84, 86], "bischl": [56, 97, 140, 142], "black": [52, 56, 64, 92, 94, 142], "blob": 53, "blog": 18, "blondel": [140, 142], "blp": [44, 54, 79], "blp_data": [54, 79], "blp_model": [73, 74], "blue": [52, 54, 57, 79], "bodori": 143, "bond": [55, 80, 81], "bonferroni": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 128], "bonu": [8, 56, 92, 94, 142], "book": [56, 86, 88, 97, 143], "bool": [4, 5, 6, 9, 11, 14, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 75, 85], "boolean": [19, 73, 74, 92, 94, 112], "boost": [51, 55, 58, 60, 63, 67, 77, 80], "boost_class": [55, 80], "boost_summari": 80, "boostrap": [71, 144], "bootstrap": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 60, 61, 62, 63, 66, 69, 70, 71, 73, 74, 81, 84, 95, 96, 98, 99, 112, 113, 140, 142, 144, 145], "both": [10, 11, 20, 21, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 55, 56, 58, 59, 76, 77, 78, 80, 81, 83, 85, 86, 87, 88, 92, 94, 97, 98, 101, 102, 103, 128, 129, 130, 135, 138, 139, 144, 145], "bottom": [54, 55, 77, 79, 80, 81], "bound": [14, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 60, 63, 66, 75, 76, 80, 87, 88, 129, 130, 135, 139, 144, 145], "branch": 56, "brantli": [53, 143], "break": [52, 144], "breviti": 145, "brew": 141, "brewer": 54, "bridg": 88, "brief": 91, "briefli": 61, "bring": [51, 67], "brucher": [140, 142], "bsd": [140, 144], "bst": 80, "budget": [78, 97], "bug": [140, 144], "build": [54, 77, 79, 83], "build_design_matric": [69, 70], "build_sim_dataset": 53, "built": [45, 78, 97, 140], "bureau": [88, 112, 143], "busi": [16, 19, 54, 79, 88, 143], "b\u00fchlmann": 143, "c": [7, 8, 10, 13, 15, 17, 25, 26, 51, 52, 53, 54, 55, 56, 59, 64, 67, 68, 72, 73, 74, 79, 80, 82, 85, 86, 91, 92, 94, 97, 98, 104, 113, 115, 117, 129, 132, 134, 140, 141, 142, 143, 145], "c1": [7, 8, 17, 54, 72, 79, 86, 91, 140, 143], "c68": [7, 8, 17, 54, 72, 79, 86, 91, 140, 143], "c895": 56, "c_": [24, 98, 101, 103, 104, 113, 115, 117, 128], "c_d": [13, 129, 137, 138, 139], "c_i": [98, 104], "c_y": [13, 129, 139], "ca1af7be64b2": 56, "caac5a95": 56, "calcualt": 83, "calcul": [29, 33, 39, 53, 55, 60, 63, 66, 69, 70, 71, 73, 74, 77, 78, 80, 84, 87, 129, 135, 139], "calendar": [60, 61, 62, 63], "calibr": [77, 78, 88], "call": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 43, 46, 47, 51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 67, 69, 70, 71, 73, 74, 79, 80, 81, 83, 84, 85, 87, 88, 89, 92, 94, 97, 112, 113, 128, 129, 135, 139, 142, 144, 145], "callabl": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 68, 69, 70, 77, 95, 97, 140], "callawai": [25, 53, 60, 61, 62, 63, 98, 99, 102, 104, 143], "camera": 72, "cameron": [54, 79], "can": [4, 5, 6, 9, 11, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 45, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 102, 104, 105, 111, 112, 113, 114, 115, 116, 117, 118, 121, 124, 125, 128, 129, 130, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145], "candid": 88, "cannot": [77, 85, 88, 98, 145], "capabl": [4, 5, 6, 51, 67], "capo": [29, 76], "capo0": 76, "capo1": 76, "capsiz": [66, 78, 82, 85], "capthick": [66, 85], "cardin": [54, 79], "care": [61, 86, 97], "carlo": [9, 10, 11, 14, 69, 70, 73, 74, 88, 143], "casalicchio": [56, 97, 140, 142], "case": [4, 5, 6, 8, 11, 22, 29, 32, 33, 40, 51, 54, 55, 60, 61, 63, 67, 69, 70, 71, 72, 74, 75, 76, 78, 79, 83, 84, 85, 86, 87, 88, 92, 94, 96, 97, 98, 100, 104, 111, 112, 113, 115, 117, 128, 129, 135, 142, 144, 145], "cast": 63, "cat": [52, 144], "catboost": 77, "cate": [33, 39, 44, 76, 95, 144], "cate_obj": 96, "cattaneo": [98, 143], "caus": [52, 68, 85, 91], "causal": [12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 54, 55, 56, 57, 67, 68, 76, 77, 78, 79, 80, 82, 86, 87, 89, 90, 91, 92, 94, 95, 98, 104, 112, 128, 129, 135, 143], "causal_contrast": [30, 66, 76, 98], "causal_contrast_att": 76, "causal_contrast_c": 76, "causal_contrast_model": [66, 98], "causaldml": 143, "causalml": [86, 143], "causalweight": 143, "caution": 128, "caveat": [74, 88], "cbind": 54, "cbook": [60, 61, 62, 63], "cc": 80, "ccp_alpha": [33, 45, 80], "cd": 141, "cd_fast": 79, "cda85647": 56, "cdf": 96, "cdid": [54, 79], "cdot": [9, 10, 14, 25, 26, 41, 54, 59, 60, 63, 71, 75, 79, 82, 84, 85, 88, 96, 98, 99, 101, 103, 104, 112, 113, 115, 117, 118, 121, 122, 126, 127, 128, 129, 132, 134, 136], "cdot1": 75, "cell": 78, "center": [60, 61, 63, 72], "central": [112, 144], "certain": [74, 98, 113, 115, 117], "cexcol": 54, "cexrow": 54, "cf_d": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 60, 63, 66, 75, 76, 87, 88, 129, 130, 135, 136, 137, 138, 139, 145], "cf_y": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 60, 63, 66, 75, 76, 87, 88, 129, 130, 135, 136, 137, 138, 139, 145], "cff": 144, "chad": 88, "chain": 74, "chainedassignmenterror": 74, "challeng": [54, 79, 129, 130], "chang": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 55, 57, 58, 62, 74, 81, 87, 88, 89, 98, 103, 104, 113, 117, 121, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 141, 143, 144], "channel": 145, "chapter": [27, 28, 56, 86, 97, 129, 139], "charact": [55, 56, 97, 144], "characterist": [87, 145], "chart": 78, "check": [42, 43, 46, 47, 52, 55, 58, 59, 60, 63, 68, 77, 78, 80, 81, 86, 90, 91, 98, 99, 140, 141, 144], "check_data": 144, "check_scor": 144, "checkmat": 144, "chernozhukov": [7, 8, 13, 15, 17, 52, 54, 55, 68, 72, 77, 78, 79, 80, 81, 86, 87, 91, 112, 113, 121, 128, 129, 130, 139, 140, 143, 144], "chetverikov": [7, 8, 17, 54, 72, 79, 86, 91, 128, 140, 143], "chiang": [16, 54, 79, 143], "chieh": 143, "choic": [4, 5, 6, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 55, 60, 61, 62, 63, 72, 80, 83, 96, 97, 98, 102, 113, 115, 117, 129, 130, 135, 139, 144], "cholecyst": 86, "choos": [51, 55, 61, 67, 68, 72, 77, 80, 81, 90, 98, 102, 112, 113, 114, 115, 116, 117, 121, 124, 125, 128, 142, 145], "chosen": [10, 14, 29, 77, 97, 98], "chou": 82, "chr": 55, "christian": [72, 143], "christoph": 143, "chunk": 97, "ci": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 58, 59, 60, 61, 62, 63, 66, 69, 70, 71, 73, 74, 75, 76, 78, 80, 81, 84, 85, 87, 88, 96, 98, 129, 135, 144, 145], "ci_at": 66, "ci_cvar": [71, 81], "ci_cvar_0": 71, "ci_cvar_1": 71, "ci_joint": [60, 61, 62, 63, 66], "ci_joint_cvar": 71, "ci_joint_lqt": 84, "ci_joint_qt": 84, "ci_length": 58, "ci_low": 66, "ci_lpq_0": 84, "ci_lpq_1": 84, "ci_lqt": [81, 84], "ci_pointwis": 66, "ci_pq_0": [81, 84], "ci_pq_1": [81, 84], "ci_qt": [81, 84], "ci_upp": 66, "cinelli": [88, 129, 130, 143], "circumv": 145, "citat": 144, "cite": 140, "claim": 56, "clarifi": [60, 61, 62, 63], "clash": 53, "class": [0, 4, 5, 6, 20, 21, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 75, 76, 78, 80, 81, 86, 87, 89, 90, 92, 93, 94, 96, 97, 98, 99, 112, 113, 128, 140, 142, 144], "class_estim": 85, "class_learn": 81, "class_learner_1": 77, "class_learner_2": 77, "classes_": [78, 86], "classic": [53, 54, 61, 79, 145], "classif": [33, 42, 46, 51, 53, 55, 56, 57, 60, 61, 62, 63, 77, 78, 83, 87, 96, 97, 98, 100, 101, 103, 145], "classifavg": 56, "classifi": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 39, 40, 42, 46, 56, 66, 78, 85, 97, 144], "classifiermixin": 86, "classmethod": [4, 5, 6], "claudia": [143, 144], "claus": [140, 144], "clean": 144, "cleaner": 77, "cleanup": 144, "clear": [54, 79], "clearli": [60, 63, 85], "clever": 77, "clone": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 56, 68, 77, 79, 81, 90, 97, 98, 112, 113, 128, 129, 135, 141, 142], "close": [53, 55, 80, 86, 88, 129, 130], "cluster": [4, 16, 20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 62, 143, 144], "cluster_col": [4, 54, 79], "cluster_var": [4, 16], "cluster_var_i": [4, 54, 79], "cluster_var_j": [4, 54, 79], "cmap": 79, "cmd": 144, "co": [18, 59], "codaci": 144, "code": [18, 29, 33, 39, 51, 53, 54, 55, 56, 57, 67, 72, 80, 91, 96, 97, 98, 112, 113, 128, 141, 142, 144, 145], "codecov": 144, "coef": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 87, 88, 89, 90, 91, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 142, 145], "coef_": 88, "coef_df": 54, "coef_valu": 78, "coeffici": [9, 10, 11, 24, 43, 44, 47, 55, 57, 73, 74, 77, 80, 82, 83, 85, 88, 89, 96, 128, 129, 135, 145], "coefs_t": 83, "coefs_w": 83, "coffici": [129, 135], "cofid": 44, "coincid": [59, 76, 81], "col": [52, 54, 74, 80], "col_nam": [60, 63], "collect": [56, 57, 58, 79, 89], "colnam": [54, 77], "color": [21, 24, 55, 57, 59, 63, 66, 68, 69, 70, 71, 78, 79, 80, 81, 82, 84, 85, 88], "color_palett": [21, 24, 60, 63, 66, 68, 79, 80, 81], "colorbar": 79, "colorblind": [21, 24, 60, 63, 66], "colorramppalett": 54, "colorscal": [69, 70], "colour": [52, 54], "column": [4, 5, 6, 58, 59, 60, 61, 62, 63, 64, 66, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 85, 87, 88, 89, 92, 93, 94, 96, 97, 98, 101, 103, 112, 142, 144, 145], "column_stack": [59, 66, 73, 74, 85, 87, 88, 98], "colv": 54, "com": [18, 53, 55, 56, 61, 62, 72, 82, 88, 97, 140, 141], "comb": 72, "combin": [22, 24, 53, 54, 56, 58, 61, 62, 66, 76, 77, 78, 79, 88, 97, 98, 102, 112, 129, 135, 144], "combind": 81, "combined_loss": 72, "come": [90, 97, 113, 129, 130, 140, 145], "command": [141, 144], "comment": [92, 94], "commit": 144, "common": [77, 87, 88, 96, 98, 143], "companion": 143, "compar": [52, 54, 59, 60, 63, 68, 69, 70, 71, 73, 74, 76, 79, 82, 84, 85, 86, 88, 91, 97, 98, 129, 130, 144], "comparevers": 55, "comparison": [60, 63, 66, 77, 82], "compat": [51, 53, 67, 144], "complement": 88, "complet": [78, 91, 129, 135, 141], "complex": [33, 53, 78], "compli": [85, 98], "complianc": [84, 85, 98, 113, 122], "complic": [56, 145], "complier": [55, 80, 81, 84, 85, 96, 98], "compon": [25, 46, 47, 53, 55, 61, 72, 77, 78, 80, 83, 96, 97, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 124, 125, 144], "compont": 53, "composit": 143, "compris": 128, "comput": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 48, 52, 53, 55, 56, 60, 61, 62, 63, 68, 80, 81, 86, 87, 88, 112, 113, 129, 130, 131, 132, 133, 134, 135, 136, 137, 140, 143, 144, 145], "computation": [129, 130], "concat": [78, 79, 80, 83, 128], "concaten": [59, 80, 128], "concentr": 128, "concern": 88, "conclud": [85, 88, 145], "cond": [98, 100, 111], "conda": [79, 143, 144], "condit": [9, 10, 11, 27, 28, 29, 31, 33, 39, 52, 54, 55, 57, 58, 59, 60, 63, 66, 68, 75, 76, 79, 80, 83, 85, 88, 89, 91, 95, 98, 101, 102, 103, 104, 128, 129, 136, 137, 139, 142, 143, 144, 145], "conduct": [96, 98, 100, 101, 103, 145], "conf": [53, 84], "confer": 143, "confid": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 53, 54, 55, 57, 58, 60, 61, 62, 63, 66, 69, 70, 71, 73, 74, 76, 79, 81, 84, 85, 87, 89, 95, 96, 98, 112, 113, 129, 135, 142, 143, 144, 145], "confidenceband": 71, "confidenti": 88, "config": 82, "configur": [56, 78], "confint": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 55, 57, 58, 59, 60, 61, 62, 63, 66, 69, 70, 71, 73, 74, 76, 77, 81, 83, 84, 85, 86, 87, 89, 96, 112, 128, 140, 142, 145], "conflict": 141, "confound": [9, 10, 11, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 51, 55, 60, 61, 63, 67, 75, 80, 84, 87, 88, 92, 94, 98, 109, 110, 128, 129, 130, 135, 138, 139, 142, 143, 144, 145], "congress": 143, "connect": [55, 80, 81], "consequ": [9, 10, 54, 75, 79, 87, 96, 98, 100, 129, 130, 136, 137, 139], "conserv": [87, 88, 129, 139], "consid": [31, 32, 33, 34, 35, 41, 52, 54, 55, 57, 58, 59, 60, 61, 62, 63, 68, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 96, 97, 98, 99, 104, 107, 108, 112, 113, 128, 129, 130, 140, 145], "consider": [88, 98], "consist": [38, 39, 43, 47, 55, 58, 78, 80, 81, 82, 88, 91, 92, 94, 98, 104, 109, 110, 142, 144], "consol": [52, 144], "constant": [13, 25, 43, 47, 60, 63, 72, 83, 96, 98, 128], "constrained_layout": 68, "construct": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 37, 38, 39, 44, 56, 59, 60, 63, 69, 70, 71, 76, 81, 86, 87, 90, 96, 113, 119, 127, 128, 144, 145], "construct_framework": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "construct_iv": 79, "constructiv": 54, "constructor": 56, "consum": [54, 79], "cont": 14, "cont_d": 66, "contain": [14, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 46, 47, 52, 54, 55, 60, 61, 63, 66, 68, 69, 70, 73, 74, 77, 79, 80, 86, 91, 93, 94, 96, 97, 98, 99, 101, 103, 128, 129, 130, 135, 144], "context": [88, 98, 111, 145], "contin": [14, 78], "continu": [14, 51, 56, 66, 67, 72, 82, 85, 98, 129, 139, 144, 145], "contour": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 72, 75, 87, 88, 129, 135], "contour_plot": 88, "contours_z": [69, 70], "contrast": [30, 58, 71, 76, 98, 106], "contribut": [141, 144], "contributor": 144, "control": [15, 22, 24, 25, 41, 53, 61, 62, 72, 76, 81, 83, 85, 86, 88, 98, 99, 101, 102, 103, 104, 113, 115, 117, 129, 132, 134, 145], "control_group": [22, 24, 60, 61, 62, 63, 98, 99, 101, 103], "convent": [40, 55, 61, 80, 81, 85, 98, 104], "converg": [52, 68, 77, 79, 91], "convergencewarn": 79, "convers": 79, "convert": [71, 79, 84], "convex": 82, "cooper": 144, "coor": [56, 97, 140, 142], "coordin": 88, "copi": [74, 78, 80, 83, 88], "cor": [129, 139], "core": [25, 58, 60, 61, 62, 63, 64, 66, 71, 75, 79, 80, 81, 84, 87, 89, 92, 93, 94, 97, 142, 144], "cores_us": [71, 81, 84], "correct": [75, 76, 88, 96, 128, 144], "correctli": [42, 46, 58, 82, 87, 129, 139], "correl": [20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 54, 57, 72, 79, 86, 87, 89, 98, 129, 130, 139], "correpond": [60, 63, 98], "correspond": [9, 10, 14, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 55, 56, 58, 59, 60, 61, 62, 63, 66, 68, 69, 70, 72, 76, 77, 79, 80, 81, 83, 84, 87, 88, 91, 96, 97, 98, 100, 102, 104, 105, 111, 112, 113, 115, 117, 128, 129, 130, 132, 134, 135, 137, 139, 144, 145], "correspondingli": [60, 63], "cosh": 18, "coul": 54, "could": [51, 56, 60, 61, 63, 67, 69, 70, 78, 88, 144, 145], "counfound": [9, 10, 84, 87, 96, 129, 139], "count": [66, 80, 81], "counti": 61, "countour": [129, 135], "countyr": 61, "coupl": [55, 80, 81], "cournapeau": [140, 142], "cours": [55, 77, 80, 88, 128, 145], "cov": [9, 37, 41, 85], "cov_nam": [85, 98], "cov_typ": [29, 33, 39, 44, 144], "covari": [4, 5, 6, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 33, 38, 39, 40, 41, 44, 45, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 64, 66, 68, 69, 70, 72, 73, 74, 75, 78, 79, 80, 81, 82, 83, 86, 87, 88, 89, 91, 92, 93, 94, 96, 97, 98, 100, 101, 103, 109, 110, 111, 113, 114, 115, 116, 117, 128, 129, 130, 142, 143, 144], "cover": [53, 72, 87], "coverag": [60, 63, 77, 85, 86, 96, 144], "cp": [55, 56, 97], "cpu": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "cpu_count": [71, 81, 84], "cran": [56, 143, 144], "creat": [11, 21, 24, 25, 51, 54, 56, 60, 63, 66, 67, 68, 69, 70, 71, 73, 74, 79, 81, 83, 84, 88, 97, 129, 130, 135, 139, 141, 144], "create_synthetic_group_data": 83, "crictial": 112, "critic": [88, 145], "cross": [20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 53, 55, 56, 57, 61, 68, 77, 78, 80, 81, 85, 88, 91, 95, 97, 101, 102, 104, 116, 127, 128, 131, 132, 135, 144, 145], "cross_sectional_data": [23, 26, 58, 98, 100], "crossfit": [77, 98], "crosstab": 82, "crucial": [72, 98, 145], "csail": [140, 142], "csdid": 62, "csv": [62, 72], "cumul": 98, "current": [45, 53, 58, 59, 60, 63, 74, 76, 113, 129, 139, 140, 141, 145], "custom": [21, 24, 52, 53, 61, 68, 88, 97], "custom_measur": 53, "cut": 83, "cutoff": [40, 41, 85, 98], "cv": [56, 80, 97, 112], "cv_glmnet": [54, 55, 56, 57, 97, 98, 128, 142], "cvar": [31, 36, 95, 119, 144], "cvar_0": 71, "cvar_1": 71, "d": [4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 105, 107, 108, 109, 110, 111, 112, 113, 114, 116, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 133, 135, 136, 137, 138, 139, 140, 142, 143, 145], "d0": [71, 84, 128], "d0_true": 84, "d0cdb0ea4795": 56, "d1": [71, 82, 84, 128], "d10": 128, "d1_true": 84, "d2": [82, 128], "d21ee5775b5f": 56, "d2cml": 62, "d3": 128, "d4": 128, "d5": 128, "d5a0c70f1d98": 56, "d6": 128, "d7": 128, "d8": 128, "d9": 128, "d_": [14, 16, 54, 59, 66, 79, 98, 100, 104, 128], "d_0": [98, 105], "d_1": [82, 128], "d_2": 82, "d_col": [4, 5, 6, 24, 51, 52, 54, 55, 56, 57, 60, 61, 62, 63, 67, 69, 70, 73, 74, 76, 79, 80, 81, 83, 85, 86, 87, 90, 91, 92, 93, 94, 97, 98, 99, 101, 103, 112, 113, 142, 144, 145], "d_i": [11, 12, 13, 15, 17, 18, 19, 52, 57, 58, 66, 68, 71, 82, 84, 85, 89, 91, 98, 100, 111], "d_j": [66, 98, 105, 106, 128], "d_k": [98, 106, 128], "d_l": [98, 105], "d_w": 83, "da1440": 82, "dag": [57, 88, 89, 145], "dai": 60, "dark": [52, 68], "darkblu": 54, "darkr": 54, "dash": [57, 60, 63, 66], "dat": [92, 94], "data": [0, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 46, 47, 53, 59, 72, 77, 82, 86, 90, 92, 93, 95, 96, 97, 99, 101, 102, 103, 104, 112, 128, 132, 133, 134, 135, 143, 144], "data_apo": 66, "data_cvar": 81, "data_dict": [40, 69, 70, 73, 74, 75, 85, 98], "data_dml": 87, "data_dml_bas": [55, 69, 70, 73, 74, 80, 81, 83], "data_dml_base_iv": [55, 80, 81], "data_dml_flex": [55, 80], "data_dml_flex_iv": 55, "data_dml_iv_flex": 80, "data_dml_new": 83, "data_fram": 145, "data_lqt": 81, "data_pq": 81, "data_qt": 81, "data_transf": [54, 79, 80], "datafram": [4, 5, 6, 7, 8, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 45, 54, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 93, 96, 97, 98, 100, 113, 128, 129, 130, 135, 142, 145], "dataset": [0, 4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 57, 58, 60, 63, 66, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 129, 135, 142, 145], "datatyp": [62, 144], "date": [24, 25, 97], "date_format": 24, "datetim": [6, 24, 25, 60, 61, 63, 93, 94], "datetime64": [60, 63, 93, 94], "datetime_unit": [6, 24, 60, 63, 93, 94, 98, 99, 101, 103], "david": 144, "db": [55, 80, 81, 87, 145], "dbl": [53, 54, 55, 56, 92, 94, 128, 142, 145], "dc13a11076b3": 56, "ddc9": 56, "de": [51, 67, 143], "deal": [51, 67], "debias": [7, 8, 16, 17, 54, 72, 79, 86, 95, 97, 112, 140, 143, 144], "debt": [55, 80, 81], "decai": [57, 89], "decid": [55, 80, 86], "decis": [33, 51, 55, 67, 80, 81, 96, 98, 143, 145], "decision_effect": 51, "decision_impact": [51, 67], "decisiontreeclassifi": [33, 45, 80], "decisiontreeregressor": 80, "declar": 145, "decomposit": [98, 99], "decreas": 85, "deep": [42, 43, 46, 47, 78], "deeper": 33, "def": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 60, 63, 68, 71, 77, 78, 79, 82, 83, 84, 86, 88, 97, 113], "default": [4, 5, 6, 9, 10, 11, 14, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 53, 54, 57, 58, 60, 61, 62, 63, 73, 74, 77, 79, 83, 85, 87, 88, 89, 90, 96, 97, 98, 112, 128, 129, 135, 136, 142, 145], "default_arg": [60, 63], "default_convert": 79, "default_jitt": 24, "defier": [85, 98], "defin": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 40, 41, 43, 47, 52, 55, 56, 58, 61, 66, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 80, 81, 83, 84, 85, 86, 87, 88, 96, 97, 98, 100, 101, 103, 104, 111, 113, 114, 116, 117, 129, 130, 135, 139, 144], "definit": [18, 60, 63, 73, 74, 76, 98, 102, 129, 136, 137], "defint": 129, "degre": [41, 55, 69, 70, 76, 79, 80, 85, 96, 129, 130], "dekel": 143, "delete_origin": 56, "deliber": 82, "delta": [15, 24, 53, 58, 60, 63, 88, 98, 100, 101, 102, 103, 104, 113, 115, 117], "delta_bench": 88, "delta_i": 53, "delta_j": 15, "delta_t": [25, 60, 63], "delta_theta": [48, 66, 75, 87, 88, 129, 130], "delta_v": 88, "demand": [54, 79, 129, 130], "demir": [7, 8, 17, 54, 72, 79, 86, 91, 112, 140, 143], "demo": [61, 88], "demonstr": [52, 53, 54, 61, 68, 79, 85, 88, 92, 94, 98, 128, 140, 142], "deni": 143, "denomin": [129, 130, 136, 137], "denot": [38, 54, 55, 57, 58, 59, 63, 79, 80, 85, 88, 89, 96, 98, 100, 101, 103, 104, 105, 109, 113, 129, 130, 135, 137, 139], "dens_net_tfa": 55, "densiti": [34, 35, 36, 52, 57, 66, 68], "dep": 64, "dep1": [56, 64, 92, 94, 142], "dep2": [56, 64, 92, 94, 142], "depend": [11, 25, 29, 31, 33, 34, 36, 56, 58, 60, 63, 69, 70, 73, 74, 75, 77, 78, 83, 85, 90, 96, 97, 98, 101, 102, 103, 113, 122, 123, 129, 130, 136, 139, 142, 143], "deprec": [58, 59, 60, 61, 62, 63, 90, 98, 112, 113, 129], "depreci": 144, "depth": [33, 45, 55, 56, 83, 90, 96, 97, 98, 112, 113, 128, 142, 145], "deriv": [20, 22, 23, 25, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 98, 128], "describ": [21, 53, 54, 60, 63, 79, 80, 81, 88, 97, 112, 141, 144], "descript": [55, 62, 64, 87, 97, 112, 113, 115, 117, 129, 130, 132, 134], "deserv": 98, "design": [25, 40, 41, 66, 78, 95, 143, 144], "design_info": [69, 70], "design_matrix": [69, 70, 96], "desir": [10, 56, 83, 98, 141], "detail": [20, 22, 23, 27, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 52, 55, 56, 58, 59, 66, 68, 72, 78, 81, 85, 87, 88, 91, 92, 94, 96, 97, 99, 100, 101, 102, 103, 104, 111, 113, 119, 121, 122, 123, 126, 127, 128, 129, 130, 132, 134, 139, 140, 141, 142, 144, 145], "determin": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 47, 55, 71, 80, 81, 84, 85, 87, 98, 128, 129, 139], "determinist": [83, 85, 96, 98], "deutsch": 140, "dev": [141, 144], "develop": [53, 54, 56, 79, 88, 98, 100, 104, 144], "deviat": [77, 98, 129, 139], "dezeur": 143, "df": [4, 5, 6, 24, 51, 52, 54, 57, 59, 60, 63, 66, 67, 69, 70, 71, 74, 76, 79, 82, 84, 85, 87, 88, 89, 91, 93, 94, 96, 98, 99, 101, 103], "df_agg": 72, "df_anticip": [60, 63], "df_apo": 66, "df_apo_ci": 66, "df_apos_ci": 66, "df_ate": 66, "df_bench": 88, "df_binari": 88, "df_bonu": [56, 92, 94, 142], "df_capo0": 76, "df_capo1": 76, "df_cate": [69, 70, 76], "df_causal_contrast_c": 76, "df_ci": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44], "df_coef": 77, "df_cvar": 81, "df_fuzzi": 85, "df_lqte": 81, "df_ml_g0": 77, "df_ml_g1": 77, "df_ml_m": 77, "df_pa": [58, 89], "df_plot": 54, "df_post_treat": [60, 63], "df_pq": 81, "df_qte": 81, "df_result": 72, "df_sharp": 85, "df_sort": 66, "df_summari": 80, "df_wide": 79, "dfg": 140, "dgp": [25, 26, 54, 57, 59, 60, 63, 71, 72, 79, 82, 83, 84, 88, 89], "dgp1": [25, 26], "dgp2": [25, 26], "dgp3": [25, 26], "dgp4": [25, 26], "dgp5": [25, 26], "dgp6": [25, 26], "dgp_dict": 88, "dgp_tpye": 58, "dgp_type": [25, 26, 58, 60, 63], "diagnost": 61, "diagon": 88, "diagram": [51, 67, 98], "dichotom": [51, 67], "dict": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 45, 46, 47, 48, 60, 63, 69, 70, 72, 78, 88, 97], "dict_kei": [129, 135], "dictionari": [9, 10, 11, 14, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 48, 60, 63, 69, 70, 73, 74, 87, 96, 97, 98, 99, 129, 135], "dictonari": [55, 80], "did": [0, 4, 5, 6, 52, 58, 59, 60, 61, 62, 63, 79, 93, 94, 95, 99, 100, 101, 103, 104, 113, 115, 117, 144, 145], "did_aggreg": [60, 62, 63], "did_multi": [98, 99], "diff": 80, "differ": [9, 10, 11, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 51, 52, 54, 55, 56, 57, 60, 62, 63, 66, 67, 68, 71, 73, 74, 75, 76, 78, 79, 80, 81, 82, 84, 85, 87, 88, 89, 90, 95, 96, 97, 99, 100, 101, 103, 104, 112, 114, 115, 116, 117, 141, 142, 143, 144, 145], "differenti": 98, "difficult": 88, "dillon": 143, "dim": [41, 55], "dim_x": [12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 25, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 52, 54, 56, 68, 76, 77, 78, 79, 91, 96, 97, 98, 129, 135], "dim_z": [15, 38, 98], "dimens": [11, 16, 25, 54, 79, 83, 112], "dimension": [11, 13, 21, 38, 39, 72, 86, 96, 98, 109, 110, 112, 128, 129, 135, 142, 143], "direct": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 57, 59, 68, 76, 89, 91, 98, 145], "directli": [40, 52, 53, 55, 66, 68, 77, 87, 91, 129, 135, 142, 145], "discontinu": [40, 41, 95, 143, 144], "discret": [14, 30, 66, 79, 98, 105, 144], "discretis": 81, "discuss": [12, 54, 55, 79, 80, 98, 99, 104, 143, 144, 145], "disjoint": [54, 73, 74, 79], "displai": [21, 54, 60, 61, 62, 63, 66, 79, 88, 96, 97, 129, 135], "displot": 80, "disproportion": [55, 80], "disregard": [43, 47], "dist": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "distinguish": [24, 60, 63], "distr": 97, "distribut": [41, 52, 58, 66, 68, 77, 88, 91, 98, 100, 104, 129, 137, 141, 143, 144], "diverg": [40, 52, 68, 91], "divid": [60, 63, 98], "dmatrix": [69, 70, 96], "dml": [20, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 56, 57, 58, 61, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 113, 128, 129, 135, 141], "dml1": [95, 142, 144, 145], "dml2": [51, 54, 56, 57, 58, 64, 79, 81, 95, 98, 113, 128, 142, 144, 145], "dml_apo": 76, "dml_apo_obj": 98, "dml_apos_att": 76, "dml_apos_obj": 98, "dml_base": 79, "dml_combin": 128, "dml_cover": 86, "dml_cv_predict": 144, "dml_cvar": [71, 81], "dml_cvar_0": 71, "dml_cvar_1": 71, "dml_cvar_obj": [31, 96], "dml_data": [6, 24, 53, 54, 57, 58, 59, 60, 61, 62, 63, 64, 66, 75, 76, 77, 79, 82, 86, 87, 88, 89, 93, 94, 96, 97, 98, 99, 101, 103, 128, 145], "dml_data_anticip": [60, 63], "dml_data_bench": 88, "dml_data_bonu": [56, 142], "dml_data_df": 145, "dml_data_fuzzi": 85, "dml_data_lasso": 64, "dml_data_sharp": 85, "dml_data_sim": [56, 142], "dml_df": [54, 79], "dml_did": [58, 59], "dml_did_obj": [20, 23, 24, 98, 99, 100, 101, 103], "dml_iivm": 86, "dml_iivm_boost": [55, 80], "dml_iivm_forest": [55, 80], "dml_iivm_lasso": [55, 80], "dml_iivm_obj": [32, 67, 98], "dml_iivm_tre": [55, 80], "dml_irm": [69, 73, 76, 77, 83], "dml_irm_at": 75, "dml_irm_att": 76, "dml_irm_boost": [55, 80], "dml_irm_forest": [55, 80], "dml_irm_gat": 75, "dml_irm_gatet": 75, "dml_irm_lasso": [55, 64, 80], "dml_irm_new": 83, "dml_irm_obj": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 87, 96, 97, 98], "dml_irm_obj_ext": 97, "dml_irm_rf": 64, "dml_irm_tre": [55, 80], "dml_irm_weighted_att": 76, "dml_kwarg": 76, "dml_length": 86, "dml_long": 48, "dml_lpq_0": 84, "dml_lpq_1": 84, "dml_lpq_obj": [34, 96], "dml_lqte": [81, 84], "dml_obj": [53, 60, 61, 62, 63, 66, 87, 88], "dml_obj_al": [60, 63], "dml_obj_anticip": [60, 63], "dml_obj_bench": 88, "dml_obj_lasso": 61, "dml_obj_linear": [60, 63], "dml_obj_linear_logist": 61, "dml_obj_nyt": [60, 63], "dml_obj_univers": [60, 63], "dml_pliv": [54, 79], "dml_pliv_obj": [38, 54, 79, 98], "dml_plr": [70, 74, 128], "dml_plr_1": 128, "dml_plr_2": 128, "dml_plr_boost": [55, 80], "dml_plr_forest": [55, 80, 145], "dml_plr_lasso": [55, 64, 80], "dml_plr_no_split": 112, "dml_plr_obj": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 87, 90, 96, 97, 98, 112, 113, 128, 129, 130, 135], "dml_plr_obj_extern": 112, "dml_plr_obj_intern": 112, "dml_plr_obj_onfold": 78, "dml_plr_obj_untun": 78, "dml_plr_rf": 64, "dml_plr_tree": [55, 80, 145], "dml_pq_0": [81, 84], "dml_pq_1": [81, 84], "dml_pq_obj": [35, 96], "dml_procedur": [64, 90, 142, 144, 145], "dml_qte": [81, 84], "dml_qte_obj": [36, 96], "dml_robust_confset": 86, "dml_robust_length": 86, "dml_short": 48, "dml_ssm": [57, 89, 98], "dml_standard_ci": 86, "dml_tune": 144, "dmldummyclassifi": 97, "dmldummyregressor": 97, "dmlmt": 143, "dnorm": 52, "do": [53, 54, 55, 56, 60, 61, 63, 76, 77, 79, 80, 81, 82, 88, 96, 97, 112, 129, 139, 142, 145], "doabl": 113, "doc": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 140, 144], "docu": 144, "document": [59, 60, 61, 62, 63, 65, 69, 70, 73, 74, 76, 78, 88, 98, 104, 113, 129, 140, 144], "doe": [24, 30, 36, 53, 54, 55, 66, 76, 79, 80, 82, 87, 88, 98, 113, 117, 129, 139, 145], "doesn": [51, 67], "doi": [7, 8, 9, 10, 12, 16, 17, 19, 25, 26, 53, 54, 56, 72, 79, 88, 91, 97, 112, 128, 140, 142, 144], "domain": 83, "don": [53, 78], "done": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 61, 78, 81, 97, 112, 129, 130], "dosag": 66, "dot": [37, 59, 60, 63, 83, 92, 94, 96, 97, 98, 102, 104, 105, 128, 142], "doubl": [4, 5, 6, 7, 8, 16, 17, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 55, 61, 72, 77, 78, 80, 82, 86, 95, 97, 112, 113, 128, 129, 130, 144], "double_ml": 60, "double_ml_bonus_data": 64, "double_ml_data_from_data_fram": [52, 91, 92, 94, 145], "double_ml_data_from_matrix": [53, 56, 92, 94, 97, 128, 142], "double_ml_framework": [98, 99], "double_ml_irm": [64, 83], "double_ml_score_mixin": 0, "doubleiivm": 140, "doubleml": [0, 52, 54, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 112, 113, 129, 135, 142, 143, 144], "doubleml2022python": 140, "doubleml2024r": 140, "doubleml_did_eval_linear": 53, "doubleml_did_eval_rf": 53, "doubleml_did_linear": 53, "doubleml_did_rf": 53, "doubleml_framework": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "doublemlapo": [66, 76, 98, 113, 118, 144], "doublemlblp": [29, 33, 39, 69, 70, 76, 96, 144], "doublemlclusterdata": [0, 16], "doublemlcvar": [71, 96, 113, 119, 144], "doublemldata": [0, 6, 7, 8, 12, 13, 15, 17, 18, 19, 20, 22, 23, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 54, 56, 57, 58, 59, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 95, 96, 97, 98, 100, 112, 113, 128, 129, 135, 144, 145], "doublemldid": [58, 59, 98, 100, 113, 116, 144], "doublemldidaggreg": [60, 61, 62, 63, 98, 99], "doublemldidc": [58, 98, 100, 113, 114, 144], "doublemldidmulti": [60, 61, 62, 63, 98, 99, 101, 102, 103, 113, 115, 117, 144], "doublemlframework": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 61, 62, 63, 98, 99, 112, 128, 144], "doublemlframwork": 30, "doublemlidid": [98, 100], "doublemlididc": [98, 100], "doublemliivm": [51, 55, 67, 80, 86, 97, 98, 112, 113, 120, 144], "doublemlirm": [20, 22, 23, 29, 31, 32, 34, 35, 37, 38, 39, 53, 55, 64, 66, 69, 73, 75, 76, 77, 80, 82, 83, 87, 88, 96, 97, 98, 112, 113, 121, 140, 144], "doublemllpq": [84, 96, 113, 122, 144], "doublemlpaneldata": [0, 22, 24, 61, 62, 93, 98, 99, 101, 102, 103, 144], "doublemlpliv": [97, 98, 112, 113, 124, 140, 144], "doublemlplr": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 52, 55, 56, 64, 68, 70, 74, 78, 80, 82, 87, 90, 91, 96, 97, 98, 112, 113, 125, 128, 129, 135, 140, 142, 144, 145], "doublemlpolicytre": [33, 96], "doublemlpq": [81, 84, 96, 113, 123, 144], "doublemlqt": [71, 81, 84, 96, 128, 144], "doublemlresampl": [76, 77], "doublemlsmm": 144, "doublemlssm": [57, 89, 98, 113, 126, 127], "doubli": [9, 10, 26, 53, 61, 86, 143], "down": 88, "download": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 92, 94, 141, 142], "download_fil": 61, "downward": 88, "dpg_dict": 87, "dpi": [52, 68, 82], "dr": [98, 102], "dramat": 53, "draw": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 88, 112, 144], "draw_sample_split": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 76, 77, 112], "drawn": [11, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 55, 60, 63, 80, 81, 83, 112], "drive": [52, 68, 91], "driven": [88, 145], "drop": [53, 78, 79, 82, 92, 94, 97, 113, 114, 115, 116, 117, 128], "dropna": [60, 63], "dt": [60, 113, 114, 129, 131], "dt64ns_dtype": 63, "dt_bonu": [92, 94], "dta": [53, 62], "dtrain": 80, "dtype": [58, 60, 61, 62, 63, 64, 66, 73, 74, 75, 77, 79, 80, 81, 86, 87, 89, 92, 93, 94, 96, 142], "dualiti": 79, "dubourg": [140, 142], "duchesnai": [140, 142], "due": [52, 53, 61, 68, 69, 70, 75, 87, 88, 91, 98, 111, 129, 130, 144, 145], "duflo": [7, 8, 17, 54, 72, 79, 86, 91, 112, 140, 143], "dummi": [29, 33, 39, 42, 43, 44, 61, 78, 88, 96, 97, 98, 100, 144], "dummyclassifi": [42, 61], "dummyregressor": [43, 61], "duplic": 144, "durabl": [56, 64, 92, 94, 142], "durat": 8, "dure": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 54, 55, 56, 57, 78, 79, 80, 97, 112, 142, 144, 145], "dx": 12, "dynam": [53, 143], "e": [5, 6, 7, 8, 9, 10, 14, 17, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 52, 53, 54, 55, 57, 58, 60, 61, 62, 63, 66, 68, 69, 70, 72, 75, 77, 78, 79, 80, 81, 82, 85, 86, 87, 88, 89, 91, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145], "e20ea26": 56, "e401": [55, 80, 81, 87, 145], "e4016553": 145, "e45228": 82, "e57c": 56, "each": [14, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 54, 56, 59, 60, 61, 62, 63, 66, 73, 74, 77, 78, 79, 81, 82, 83, 86, 87, 88, 90, 92, 93, 94, 97, 98, 104, 112, 128, 129, 135, 145], "earlier": [60, 63, 145], "earn": [55, 80, 81], "earner": [55, 80, 87], "easi": [56, 86, 113], "easier": 78, "easili": [56, 77, 78, 81, 144], "ec973f": 82, "ecolor": [59, 66, 80, 82], "econ": 143, "econml": 143, "econom": [15, 16, 18, 19, 54, 72, 79, 82, 88, 112, 143], "econometr": [7, 8, 9, 10, 17, 18, 25, 26, 53, 54, 72, 79, 86, 91, 140, 143], "econometrica": [13, 54, 79, 82, 86, 91, 143], "ecosystem": [140, 145], "ectj": [7, 8, 17, 54, 72, 79, 91, 140], "ed": 143, "edge_color": 68, "edgecolor": 68, "edit": [141, 143], "edu": [140, 142], "educ": [55, 80, 81, 87, 145], "ee97bda7": 56, "effect": [9, 10, 11, 14, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 46, 47, 51, 52, 53, 54, 56, 57, 58, 59, 66, 67, 68, 72, 75, 79, 83, 85, 86, 89, 91, 95, 97, 99, 100, 102, 104, 106, 107, 108, 111, 112, 113, 121, 128, 129, 130, 142, 143, 144, 145], "effici": [98, 143], "effort": 113, "eight": [54, 79], "either": [11, 20, 22, 23, 24, 25, 29, 31, 32, 33, 34, 35, 37, 38, 39, 56, 59, 60, 63, 72, 83, 85, 96, 97, 98, 101, 104, 145], "eleanor": 143, "element": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 57, 58, 60, 61, 62, 63, 69, 70, 71, 77, 79, 81, 84, 87, 89, 98, 101, 102, 103, 113, 114, 115, 116, 117, 118, 129, 132, 134, 135, 138, 139, 144], "element_text": [54, 55], "elementari": 143, "elif": [73, 74, 83], "elig": [81, 87, 145], "eligibl": [55, 80, 87], "ell": [52, 54, 68, 72, 79, 91, 113, 124, 125, 142], "ell_0": [32, 38, 39, 52, 68, 72, 78, 91, 98, 107, 113, 125], "ell_1": 61, "ell_2": 77, "els": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 44, 53, 54, 55, 59, 73, 74, 79, 83, 88], "em": 143, "emphas": [54, 79], "empir": [27, 28, 52, 54, 61, 68, 79, 82, 88, 91, 98, 101, 102, 103, 112, 113, 128], "emploi": [54, 72, 79, 88, 113, 120], "employ": [55, 61, 80, 81], "employe": 145, "empti": 79, "emul": [129, 130], "enabl": [21, 66, 83, 87, 96, 129, 130, 144], "enable_metadata_rout": [42, 43, 46, 47], "encapsul": [42, 43, 46, 47, 97], "encod": 82, "encount": 63, "end": [12, 15, 16, 52, 53, 54, 55, 57, 59, 60, 63, 68, 71, 72, 77, 79, 80, 82, 83, 84, 89, 90, 92, 94, 97, 98, 101, 103, 104, 112, 113, 115, 117, 128, 142, 145], "endogen": [55, 80, 81, 145], "enet_coordinate_descent_gram": 79, "enforc": 61, "engin": [56, 143], "enrol": [55, 80, 81], "ensembl": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 64, 66, 68, 69, 70, 73, 74, 75, 77, 80, 83, 87, 88, 90, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 129, 135, 142, 145], "ensemble_learner_pipelin": 97, "ensemble_pipe_classif": 56, "ensemble_pipe_regr": 56, "ensur": [46, 47, 54, 74, 76, 78, 79, 83, 86], "entir": [24, 52, 55, 68, 80, 91, 129, 130], "entri": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 52, 54, 58, 60, 61, 62, 63, 64, 66, 68, 75, 79, 80, 81, 87, 89, 91, 92, 93, 94, 97, 140, 142, 144], "enumer": [59, 66, 71, 73, 74, 77, 79, 80, 81, 84, 90, 97, 112], "env": [79, 141], "environ": 141, "ep": 82, "epanechnikov": 40, "epsilon": [55, 58, 59, 71, 80, 84, 96, 98, 100, 104], "epsilon_": [54, 59, 60, 63, 79], "epsilon_0": 41, "epsilon_1": 41, "epsilon_i": [11, 71, 82, 83, 84], "epsilon_sampl": 83, "epsilon_tru": [71, 84], "eqnarrai": 55, "equal": [21, 25, 29, 33, 54, 57, 60, 63, 79, 82, 86, 89, 96, 97, 98, 129, 137], "equat": [41, 54, 55, 79, 80, 88, 90, 128, 145], "equilibrium": [54, 79], "equiv": [98, 104, 113, 115, 117], "equival": [72, 76, 112], "err": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 58, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 73, 74, 75, 76, 78, 79, 80, 81, 82, 84, 87, 88, 89, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 142, 145], "error": [4, 5, 6, 20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 51, 52, 53, 55, 56, 57, 59, 68, 72, 73, 74, 77, 78, 80, 85, 88, 91, 97, 98, 109, 110, 112, 113, 128, 129, 135, 142, 144, 145], "errorbar": [59, 66, 73, 74, 78, 80, 82, 85], "erstellt": [54, 55, 56], "es_linear_logist": 61, "es_rf": 61, "esim": 85, "especi": [77, 78], "essenti": 88, "est": 85, "est_method": 53, "esther": [112, 143], "estim": [4, 5, 7, 9, 10, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 52, 53, 54, 56, 59, 66, 68, 69, 70, 71, 73, 74, 76, 77, 79, 83, 85, 86, 90, 91, 95, 96, 97, 98, 99, 100, 101, 102, 103, 106, 108, 114, 115, 116, 119, 122, 123, 127, 129, 130, 135, 140, 143, 144], "estimand": 86, "estimatior": [4, 5], "estimator_list": 78, "et": [7, 8, 11, 13, 16, 17, 52, 54, 55, 56, 58, 68, 69, 70, 71, 72, 73, 74, 77, 79, 80, 81, 84, 87, 91, 98, 100, 104, 112, 113, 119, 121, 122, 123, 128, 129, 130, 139, 140, 142, 144], "eta": [27, 28, 52, 54, 55, 59, 78, 79, 80, 84, 85, 90, 96, 98, 101, 102, 103, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 139, 142, 145], "eta1": 82, "eta2": 82, "eta_": [128, 129, 139], "eta_0": [40, 90, 98, 101, 102, 103, 113, 128], "eta_d": [85, 98], "eta_i": [11, 25, 59, 60, 63, 83, 84, 85, 98], "eta_sampl": 83, "eta_tru": 84, "etc": [20, 21, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 77, 78, 79, 144], "ev": [52, 68, 91], "eval": [24, 56, 60, 61, 62, 63, 97, 98, 101, 102, 103, 113, 115, 117], "eval_metr": [55, 80, 145], "eval_pr": 53, "eval_predict": 53, "evalu": [13, 20, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 56, 59, 60, 61, 62, 63, 69, 70, 71, 75, 76, 81, 84, 87, 90, 98, 102, 104, 143, 144], "evaluate_learn": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 77, 78, 97, 144], "evalut": 97, "even": [55, 56, 60, 63, 80, 82, 85, 97, 98, 145], "event": [98, 99], "eventstudi": [24, 60, 61, 62, 63, 98, 99], "eventu": [54, 79], "everi": [54, 61, 79], "everyth": 140, "evid": [75, 78], "exact": [76, 88], "exactli": [85, 88, 98], "exampl": [4, 5, 6, 11, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 55, 56, 57, 58, 59, 60, 62, 63, 64, 66, 67, 68, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 96, 97, 98, 99, 101, 102, 103, 111, 112, 113, 128, 129, 135, 140, 142, 144, 145], "example_attgt": 53, "example_attgt_dml_eval_linear": 53, "example_attgt_dml_eval_rf": 53, "example_attgt_dml_linear": 53, "example_attgt_dml_rf": 53, "except": [43, 47, 72, 88, 144], "excess": 77, "exclud": 48, "exclus": [29, 33, 39, 73, 74, 96], "execut": [56, 145], "exemplarili": 142, "exemplatori": 83, "exhaust": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 97], "exhibit": [54, 79], "exist": [42, 43, 46, 47, 76, 98, 100, 104, 129, 139], "exogen": [55, 80, 81, 98, 145], "exp": [9, 10, 11, 13, 14, 17, 25, 26, 52, 59, 68, 69, 70, 73, 74, 82, 83, 91], "expect": [9, 10, 43, 47, 53, 57, 58, 60, 61, 63, 66, 75, 77, 78, 85, 88, 89, 96, 98, 112, 128, 129, 136, 142], "experi": [8, 12, 13, 52, 55, 68, 80, 86, 88, 91, 92, 94, 112, 142, 143], "experiment": [20, 22, 23, 24, 25, 26, 113, 114, 115, 116, 117, 129, 131, 132, 133, 134], "expertis": 88, "explain": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 87, 129, 130, 138, 139], "explan": [54, 58, 79, 87, 129, 138, 140, 145], "explanatori": [88, 128], "explicit": 88, "explicitli": [60, 62, 63, 75, 145], "exploit": [52, 68, 91, 98, 145], "explor": 78, "exponenti": 128, "export": [78, 144], "expos": [98, 104], "exposur": [6, 25, 59, 60, 61, 62, 63], "express": [54, 72, 85, 129, 139], "ext": 24, "extend": [88, 94, 97, 140, 144], "extendend": [129, 139], "extens": [97, 113, 140, 143, 144], "extent": 72, "extern": [52, 68, 76, 78, 95, 129, 130, 144], "external_predict": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 68, 97], "externalptr": 55, "extra": 56, "extract": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 78], "extralearn": 56, "extrem": [55, 80], "ey": 72, "ezequiel": 144, "f": [55, 56, 58, 59, 60, 63, 66, 68, 71, 72, 77, 79, 80, 81, 83, 84, 87, 88, 89, 97, 129, 139, 140, 142], "f00584a57972": 56, "f1718fdeb9b0": 56, "f2e7": 56, "f3d24993": 56, "f6ebc": 82, "f_": [9, 25, 26, 59, 96], "f_loc": [71, 84], "f_p": 59, "f_scale": [71, 84], "f_t": [60, 63], "f_x": 98, "face_color": 68, "facet_wrap": 55, "facilit": 78, "fact": [55, 80, 81], "factor": [41, 52, 53, 54, 55, 56, 68, 77, 91, 97, 129, 132, 134, 145], "faculti": 143, "fail": 144, "fair": 77, "fake": [51, 67, 86], "fallback": 97, "fals": [4, 5, 6, 7, 8, 9, 11, 14, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 52, 55, 56, 57, 58, 60, 63, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 88, 89, 92, 94, 97, 98, 101, 112, 113, 114, 115, 116, 117, 128, 129, 131, 132, 133, 134, 145], "famili": [55, 80, 97], "familiar": 86, "fanci": 53, "far": [55, 80], "farbmach": 12, "fast": [77, 83, 97], "faster": 72, "fb5c25fa": 56, "fc9e": 56, "fd8a": 56, "featur": [7, 8, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 43, 45, 47, 53, 60, 63, 64, 75, 76, 77, 80, 83, 96, 97, 98], "featureless": [56, 97], "features_bas": [55, 80, 81, 87], "features_flex": 55, "featureunion": 56, "februari": [60, 63, 88], "feder": 61, "femal": [56, 64, 92, 94, 142], "fern\u00e1ndez": [13, 112, 143], "fetch": [55, 79, 80, 81, 92, 94], "fetch_401k": [55, 80, 81, 87, 145], "fetch_bonu": [56, 64, 92, 94, 142], "few": [55, 80, 81], "ff7f0e": 59, "field": [54, 79, 97, 145], "fifteenth": 143, "fifth": [54, 60, 63], "fig": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 59, 60, 61, 62, 63, 66, 69, 70, 71, 72, 76, 77, 78, 81, 82, 84, 85, 88], "fig_al": 68, "fig_dml": 68, "fig_non_orth": 68, "fig_orth_nosplit": 68, "fig_po_al": 68, "fig_po_dml": 68, "fig_po_nosplit": 68, "figsiz": [21, 24, 59, 60, 63, 64, 66, 69, 70, 71, 73, 74, 76, 77, 78, 79, 80, 81, 82, 84, 85], "figur": [17, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 54, 59, 60, 61, 63, 64, 66, 68, 69, 70, 71, 72, 73, 74, 76, 78, 79, 80, 81, 84, 88, 91], "figure_format": 82, "file": [7, 8, 61, 72, 82, 143, 144], "filenam": 52, "fill": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 54, 55, 57, 58, 77, 80, 89], "fill_between": [69, 70, 71, 76, 81, 84], "fill_valu": 77, "fillna": 60, "filter": 56, "filterwarn": 68, "final": [52, 56, 57, 59, 60, 62, 63, 66, 68, 69, 70, 71, 73, 74, 75, 81, 84, 85, 89, 91, 98, 111, 113, 115, 117, 145], "final_estim": 85, "financi": [7, 87, 145], "find": [55, 58, 59, 80, 88, 96, 97, 145], "finish": 56, "finit": [52, 55], "firm": [54, 79, 87], "firmid": 79, "first": [6, 9, 10, 16, 20, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 79, 80, 81, 83, 84, 85, 88, 89, 91, 96, 98, 99, 102, 104, 112, 128, 129, 135, 141, 142, 144, 145], "fit": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 95, 96, 97, 98, 99, 100, 101, 103, 104, 113, 116, 127, 128, 129, 130, 135, 140, 144, 145], "fit_arg": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39], "fit_transform": [76, 79, 80], "five": 79, "fix": [59, 60, 63, 77, 144], "flag": [26, 112, 141], "flake8": 144, "flamlclassifierdoubleml": 78, "flamlregressordoubleml": 78, "flatten": [78, 82], "flexibl": [40, 51, 53, 55, 56, 58, 67, 80, 98, 140, 143, 144, 145], "flexibli": [55, 61, 80, 87], "float": [9, 10, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 60, 61, 62, 63, 93, 94], "float32": [80, 81, 87], "float64": [58, 60, 61, 62, 63, 64, 66, 74, 75, 79, 80, 87, 89, 92, 93, 94, 97, 142], "floor": 56, "floor_divid": 79, "flt": 56, "flush": 52, "fmt": [59, 66, 73, 74, 78, 80, 82, 85], "fobj": 80, "focu": [54, 55, 61, 76, 79, 80, 81, 88, 96, 98, 100, 104, 111, 145], "focus": [81, 87, 88, 145], "fold": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 54, 55, 56, 57, 58, 60, 61, 62, 63, 77, 79, 80, 81, 87, 89, 90, 95, 97, 98, 100, 101, 103, 113, 116, 128, 142, 145], "follow": [9, 10, 11, 14, 25, 26, 52, 54, 55, 57, 58, 59, 60, 61, 62, 63, 68, 69, 70, 71, 73, 74, 78, 79, 80, 81, 84, 85, 86, 87, 88, 89, 91, 92, 94, 96, 97, 98, 99, 101, 102, 103, 104, 112, 113, 115, 117, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145], "font_scal": [79, 80, 81], "fontsiz": [60, 63, 71, 81, 84], "force_all_d_finit": [5, 6], "force_all_x_finit": [4, 5, 6], "forest": [12, 51, 52, 53, 55, 56, 58, 67, 68, 75, 77, 80, 87, 91, 97, 142, 145], "forest_summari": 80, "forg": [141, 143, 144], "form": [9, 10, 11, 20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 46, 47, 55, 57, 58, 59, 69, 70, 71, 73, 74, 75, 77, 80, 84, 85, 87, 89, 96, 98, 99, 100, 104, 107, 108, 109, 110, 113, 115, 117, 118, 121, 129, 130, 135, 136, 137, 138, 139, 141, 142], "format": [6, 24, 61, 68, 75, 129, 135, 144], "former": [61, 86], "formula": [54, 55, 79, 80, 85, 88, 144], "formula_flex": 55, "forschungsgemeinschaft": 140, "forthcom": [88, 143], "forum": 144, "forward": [33, 45], "found": [62, 69, 70, 72, 73, 74, 78, 91, 92, 94, 97, 98, 111, 142], "foundat": [140, 143], "four": [55, 77, 80, 98, 101, 144], "fourth": [54, 79], "frac": [9, 10, 12, 13, 15, 17, 18, 19, 25, 26, 28, 32, 43, 47, 52, 54, 56, 59, 63, 68, 72, 75, 79, 82, 85, 90, 91, 96, 98, 101, 102, 103, 107, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 126, 127, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 139], "fraction": 56, "frame": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 51, 52, 54, 55, 57, 58, 60, 61, 62, 63, 64, 66, 69, 70, 73, 74, 75, 79, 80, 81, 82, 83, 87, 89, 91, 92, 93, 94, 142, 145], "framealpha": [60, 63], "frameon": [60, 63], "framework": [21, 24, 28, 52, 54, 56, 68, 77, 78, 79, 82, 88, 91, 97, 128, 140, 142, 144, 145], "freez": 141, "fribourg": 143, "friendli": [60, 63, 66], "from": [4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 47, 51, 52, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 96, 97, 98, 99, 100, 101, 102, 103, 104, 112, 113, 128, 129, 135, 142, 144, 145], "from_arrai": [4, 5, 6, 37, 40, 58, 59, 68, 71, 84, 91, 92, 94, 97, 128, 142], "from_product": 79, "front": 66, "fr\u00e9chet": [129, 139], "fs_kernel": [40, 98], "fs_specif": [40, 98], "fsize": [55, 80, 81, 87, 145], "full": [58, 59, 66, 68, 71, 73, 74, 77, 80, 81, 84, 85, 86, 89, 91, 98], "fulli": [33, 55, 65, 78, 80, 86, 98, 108], "fun": 52, "func": 53, "function": [0, 4, 5, 6, 17, 18, 20, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 55, 56, 57, 58, 60, 61, 62, 63, 67, 68, 69, 70, 71, 76, 77, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 95, 97, 98, 99, 100, 101, 102, 103, 104, 107, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 134, 139, 140, 143, 144, 145], "fund": [55, 80, 81, 140], "further": [9, 10, 11, 14, 16, 21, 24, 25, 26, 54, 56, 57, 58, 59, 60, 61, 62, 63, 66, 69, 70, 71, 75, 76, 77, 79, 81, 83, 84, 85, 87, 88, 89, 97, 98, 100, 103, 111, 113, 119, 122, 123, 126, 127, 128, 129, 130, 135, 138, 139, 140, 142, 144, 145], "furthermor": [68, 93, 94, 113, 118, 121], "futur": [60, 61, 62, 63, 98, 113, 129], "futurewarn": [60, 61, 62, 63, 74], "fuzzi": [40, 41], "g": [5, 6, 9, 10, 20, 22, 23, 24, 25, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 52, 53, 56, 58, 59, 60, 61, 62, 63, 64, 68, 69, 70, 72, 75, 77, 81, 82, 83, 86, 87, 89, 91, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 113, 114, 115, 116, 117, 118, 120, 121, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145], "g_": [41, 66, 98, 103, 113, 114, 116, 117, 119, 122, 123, 128], "g_0": [17, 18, 20, 22, 23, 24, 29, 32, 33, 35, 38, 39, 40, 41, 52, 54, 55, 68, 77, 79, 80, 91, 96, 97, 98, 105, 106, 107, 108, 109, 110, 113, 115, 117, 118, 125, 126, 127, 129, 136, 137, 139, 142, 145], "g_1": [41, 77], "g_all": [52, 55], "g_all_po": 52, "g_ci": 55, "g_d": [113, 119, 123], "g_dml": 52, "g_dml_po": 52, "g_hat": [38, 39, 52, 68, 113], "g_hat0": [32, 33], "g_hat1": [32, 33], "g_i": [25, 98, 101, 103, 104, 113, 115, 117], "g_k": 96, "g_nonorth": 52, "g_nosplit": 52, "g_nosplit_po": 52, "g_valu": 22, "g_x": 59, "gain": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 77, 129, 130, 137, 144], "gain_statist": 144, "galleri": [91, 96, 97, 98, 99, 101, 103, 111, 140, 144], "gama": 78, "gamma": [15, 18, 19, 54, 79, 82, 83, 85, 88, 98, 113, 119, 122], "gamma_0": [11, 57, 83, 89, 113, 119, 122], "gamma_a": [9, 10, 88], "gamma_bench": 88, "gamma_v": 88, "gap": [79, 88], "gapo": 29, "gate": [29, 33, 39, 44, 82, 83, 95, 144], "gate_obj": 96, "gatet": 96, "gaussian": [34, 35, 36, 52, 68, 91, 96, 97, 128, 143], "ge": [9, 11, 26, 75, 83, 96, 98, 102, 104], "geer": 143, "gelbach": [54, 79], "gener": [0, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 43, 47, 51, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 92, 94, 95, 96, 97, 98, 101, 103, 104, 105, 112, 113, 115, 117, 118, 121, 128, 130, 131, 132, 133, 134, 136, 137, 139, 143, 144, 145], "generate_treat": 84, "generate_weakiv_data": 86, "geom_bar": 55, "geom_dens": [55, 57], "geom_errorbar": 55, "geom_funct": 52, "geom_histogram": 52, "geom_hlin": 55, "geom_point": 55, "geom_til": 54, "geom_vlin": [52, 57], "geq": [25, 85, 98], "german": 140, "get": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 56, 60, 62, 63, 66, 77, 82, 87, 88, 129, 130, 140, 141], "get_dummi": 82, "get_feature_names_out": [76, 79, 80], "get_legend_handles_label": 66, "get_level_valu": 78, "get_logg": [52, 53, 54, 55, 56, 57, 90, 97, 98, 112, 113, 128, 142], "get_metadata_rout": [42, 43, 46, 47], "get_param": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 78, 97], "get_ylim": 76, "ggdid": 53, "ggplot": [52, 54, 55, 57], "ggplot2": [52, 54, 55, 57], "ggsave": 52, "ggtitl": 55, "gh": 144, "git": 141, "github": [53, 55, 61, 72, 78, 82, 140, 143, 144], "githubusercont": [62, 72], "give": [55, 76, 80], "given": [9, 10, 13, 17, 18, 20, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 46, 47, 52, 54, 57, 59, 61, 62, 66, 68, 73, 74, 79, 81, 82, 85, 88, 89, 91, 96, 98, 101, 102, 103, 113, 118, 128, 129, 135, 136, 137, 138, 139, 142, 144], "glmnet": [55, 56, 97, 144], "global": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 46, 47, 97, 98], "globalclassifi": 85, "globallearn": 85, "globalregressor": 85, "glrn": 56, "glrn_lasso": 56, "gmm": 86, "gname": 53, "go": [69, 70, 72, 76, 78, 85, 88], "goal": [66, 73, 74, 98], "goe": 98, "goldman": 143, "good": [72, 129, 130, 145], "gradient": [55, 80], "gradientboostingclassifi": 77, "gradientboostingregressor": 77, "gradual": 88, "gramfort": [140, 142], "graph": [56, 57, 89, 145], "graph_ensemble_classif": 56, "graph_ensemble_regr": 56, "graph_obj": 85, "graph_object": [69, 70, 72, 88], "graphlearn": [56, 97], "grasp": [66, 129, 130], "great": [59, 145], "greater": 145, "green": [52, 69, 70, 71, 84], "greg": 143, "grei": [55, 66], "grenand": 143, "grey50": 54, "grid": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 54, 56, 60, 63, 66, 69, 70, 71, 72, 76, 81, 82, 84, 88, 97, 129, 135], "grid_arrai": [69, 70], "grid_basi": 76, "grid_bound": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 88], "grid_search": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 56, 97], "grid_siz": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 69, 70], "gridextra": 54, "gridsearchcv": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 97], "grisel": [140, 142], "grob": 54, "group": [6, 22, 24, 25, 29, 33, 39, 51, 53, 61, 66, 67, 75, 76, 81, 82, 83, 88, 95, 98, 99, 101, 102, 103, 104, 113, 115, 117, 129, 132, 134], "group_0": 96, "group_1": [73, 74, 96], "group_2": [73, 74, 96], "group_3": [73, 74], "group_effect": 83, "group_ind": 75, "group_treat": 75, "groupbi": [60, 63, 72, 80, 86], "gruber": 12, "gt": [51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 92, 94, 142], "gt_combin": [24, 60, 61, 63, 98, 99, 101, 102, 103], "gt_dict": [60, 63], "guarante": [54, 79], "guber": 12, "guess": [87, 129, 130], "guid": [27, 28, 42, 43, 46, 47, 52, 53, 54, 56, 59, 60, 61, 62, 63, 66, 68, 75, 76, 79, 85, 87, 97, 140, 142, 144], "guidelin": 144, "gunion": [56, 97], "gxidclusterperiodytreat": 53, "h": [9, 10, 12, 16, 25, 26, 53, 54, 79, 85, 86, 93, 94, 98, 143], "h20": 78, "h_0": [60, 63, 66, 75, 76, 87, 88, 129, 135, 145], "h_f": [40, 98], "ha": [20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 45, 46, 47, 52, 53, 54, 55, 61, 68, 72, 77, 78, 79, 80, 81, 82, 85, 86, 87, 88, 96, 97, 98, 100, 104, 129, 130, 135, 136, 137, 138, 139, 140, 145], "had": 61, "half": [52, 68, 82, 91, 112], "hand": [40, 77, 78, 82, 86, 145], "handbook": 82, "handl": [53, 61, 66, 77, 93, 94, 97, 144], "hansen": [7, 8, 13, 15, 17, 54, 72, 79, 86, 91, 140, 143], "happend": 77, "hard": [87, 129, 130], "harold": 143, "harsh": [42, 46], "hasn": [21, 24, 60, 62, 63], "hat": [52, 54, 68, 72, 75, 79, 82, 85, 90, 91, 96, 98, 112, 113, 128, 129, 130, 135, 138], "have": [11, 14, 24, 29, 30, 33, 36, 39, 44, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 66, 67, 69, 70, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 92, 94, 96, 97, 98, 102, 113, 115, 117, 128, 129, 130, 136, 139, 141, 142, 144, 145], "hazlett": [88, 129, 130], "hc": [53, 143], "hc0": [44, 144], "hdm": [54, 79], "he": [57, 89], "head": [53, 54, 56, 60, 61, 62, 63, 64, 69, 70, 73, 74, 76, 78, 79, 80, 82, 85, 88, 92, 94, 96, 142], "heat": [54, 79], "heatmap": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 54, 79, 88], "heavili": 77, "hei": 143, "height": [21, 24, 52, 54, 72, 78], "help": [53, 55, 61, 71, 77, 81, 83, 88, 98, 99, 112, 145], "helper": 144, "henc": [53, 55, 56, 80, 88, 97, 113, 145], "here": [34, 35, 36, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 69, 70, 71, 73, 74, 75, 77, 79, 80, 81, 83, 84, 85, 88, 89, 92, 94, 97, 98, 101, 141], "heterogen": [11, 25, 33, 55, 75, 80, 81, 83, 95, 98, 108, 112, 143, 144, 145], "heteroskedast": [73, 74], "heurist": [52, 68, 91], "high": [13, 38, 39, 55, 59, 72, 80, 81, 86, 90, 98, 109, 110, 128, 140, 142, 143], "higher": [53, 55, 72, 80, 81, 82, 85, 86, 144, 145], "highli": [55, 80, 140], "highlight": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 58, 76, 78, 88, 144], "highlightcolor": [69, 70], "hint": 78, "hispan": 64, "hist": 66, "hist_e401": 55, "hist_p401": 55, "histogram": 66, "histplot": 68, "hjust": 55, "hline": [92, 94, 128, 142, 145], "hold": [19, 54, 55, 57, 78, 79, 80, 89, 96, 97, 98, 102], "holdout": [97, 112], "holm": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39], "home": [55, 60, 62, 63, 80, 88], "homogen": 98, "hook": 144, "hopefulli": 81, "horizont": [54, 59, 79], "hostedtoolcach": [60, 61, 62, 63, 80], "hot": 82, "hotstart_backward": [56, 97], "hotstart_forward": [56, 97], "household": [55, 80, 81, 87], "how": [21, 25, 42, 43, 46, 47, 51, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 91, 97, 98, 140, 141], "howev": [52, 55, 57, 68, 78, 80, 85, 88, 89, 91, 98, 145], "hown": [55, 80, 81, 87, 145], "hpwt": [54, 79], "hpwt0": 54, "hpwtairmpdspac": 54, "href": 140, "hspace": 77, "hstack": [37, 59], "html": [56, 74, 140, 142, 144], "http": [12, 18, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 97, 140, 141, 142, 144], "huber": [19, 57, 89, 98, 111, 113, 126, 127, 143], "hue": [60, 63, 80], "huge": 77, "hugo": 143, "husd": [56, 64, 92, 94, 142], "hyperparamet": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 55, 56, 64, 72, 77, 78, 80, 95, 142], "hypothes": [128, 143], "hypothesi": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 80, 87, 129, 135, 143], "hypothet": 88, "i": [4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 127, 128, 129, 130, 132, 134, 135, 136, 137, 139, 140, 141, 142, 144, 145], "i0": [58, 59, 98, 100], "i03": 140, "i1": [58, 98, 100], "i8": 63, "i_": [15, 79, 83], "i_1": [54, 79], "i_2": [54, 79], "i_3": [54, 79], "i_4": 59, "i_est": 68, "i_fold": 54, "i_k": [54, 79, 90, 112, 128], "i_learn": 77, "i_level": 66, "i_rep": [52, 57, 58, 68, 77, 89, 91], "i_split": 79, "i_train": 68, "icp": 143, "id": [6, 24, 53, 54, 56, 60, 61, 62, 63, 79, 93, 94, 98, 99, 101, 103, 129, 134], "id_col": [6, 24, 60, 61, 62, 63, 93, 94, 98, 99, 101, 103], "id_var": 79, "idea": [55, 56, 80, 81, 88, 97, 98, 129, 130, 145], "ident": [9, 10, 11, 14, 15, 25, 26, 45, 56, 61, 66, 76, 78, 85, 97, 98, 104, 113, 121, 129, 135], "identfi": 88, "identif": [60, 61, 62, 63, 85, 86, 98, 145], "identifi": [6, 54, 55, 58, 61, 75, 79, 80, 81, 85, 88, 93, 94, 96, 98, 100, 102, 104, 111, 129, 139, 144], "identifii": 96, "idnam": 53, "idx_gt_att": 24, "idx_tau": [71, 81, 84], "idx_treat": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 66, 129, 135], "ieee": 143, "ifels": 53, "ignor": [42, 43, 46, 47, 61, 68, 85], "ii": [54, 79], "iid": [60, 63, 98, 100], "iivm": [12, 27, 28, 32, 81, 90, 96, 107, 120, 140, 144], "iivm_summari": 80, "iivmglmnet": 55, "iivmrang": 55, "iivmrpart": 55, "iivmxgboost11861": 55, "ij": [16, 54, 57, 66, 79, 89], "ilia": 143, "illustr": [52, 54, 55, 56, 57, 58, 59, 61, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 83, 84, 87, 88, 89, 91, 97, 145], "iloc": [58, 59, 60, 61, 62, 63, 66, 77, 79, 82, 86], "immedi": 141, "immun": [112, 143], "impact": [51, 67, 77, 82, 87], "implement": [20, 22, 23, 24, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 68, 72, 76, 77, 79, 80, 82, 85, 87, 88, 89, 91, 95, 96, 97, 99, 100, 102, 104, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 142, 143, 144, 145], "impli": [9, 10, 54, 55, 60, 63, 79, 80, 81, 85, 96, 98, 104, 129, 131, 132, 133, 134, 136, 137], "implicitli": [98, 102], "implment": [59, 98, 99], "import": [4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 129, 135, 141, 142, 144, 145], "importlib": 72, "impos": 88, "improv": [58, 60, 63, 77, 83, 98, 144], "in_sample_norm": [20, 22, 23, 24, 58, 113, 114, 115, 116, 117, 129, 131, 132, 133, 134], "inbuild": 77, "inbuilt": 77, "inc": [55, 80, 81, 87, 145], "includ": [20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 53, 55, 59, 60, 61, 62, 63, 66, 73, 74, 76, 80, 85, 87, 88, 96, 98, 128, 129, 135, 136, 137, 139, 141, 144, 145], "include_bia": [76, 79, 80], "include_never_tr": [25, 63], "include_scenario": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 88], "incom": [55, 80, 81, 83, 87, 145], "incorpor": [56, 87, 129, 135], "increas": [60, 63, 75, 77, 79, 88, 145], "increment": 144, "ind": 80, "independ": [9, 10, 11, 20, 22, 23, 24, 26, 41, 54, 56, 59, 75, 79, 83, 98, 104, 111, 113, 114, 115, 116, 117, 144], "index": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 54, 59, 64, 68, 72, 73, 74, 78, 79, 80, 82, 83, 91, 92, 94, 112, 113, 114, 115, 116, 117, 140, 142], "index_col": 72, "india": [112, 143], "indic": [4, 5, 6, 11, 14, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 54, 55, 57, 59, 60, 61, 62, 63, 75, 79, 80, 81, 85, 86, 88, 89, 90, 92, 94, 96, 98, 100, 104, 105, 111, 112], "individu": [25, 29, 33, 46, 47, 53, 55, 59, 60, 63, 66, 73, 74, 75, 78, 80, 81, 85, 87, 96, 98, 145], "individual_df": 59, "induc": [95, 112], "industri": [54, 79], "inf": [4, 5, 6, 53, 60, 61, 62, 63, 86], "inf_model": 113, "infer": [13, 15, 51, 52, 54, 61, 67, 68, 72, 77, 78, 79, 86, 91, 95, 98, 112, 140, 142, 143, 144], "inferenti": 145, "infinit": [4, 5, 6, 86, 144], "influenc": [32, 43, 47, 98], "info": [51, 56, 58, 60, 61, 62, 63, 64, 66, 75, 78, 79, 80, 81, 87, 89, 92, 93, 94, 142, 144, 145], "inform": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 46, 47, 51, 56, 60, 61, 62, 63, 67, 69, 70, 77, 85, 86, 87, 88, 98, 99, 129, 130, 143], "infti": [52, 68, 91, 98, 104], "inher": 88, "inherit": [82, 93, 94, 144], "initi": [4, 5, 6, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 55, 56, 57, 58, 60, 61, 62, 63, 71, 80, 81, 84, 85, 87, 88, 89, 92, 93, 94, 96, 97, 98, 112, 142, 144, 145], "inlin": [64, 82], "inlinebackend": 82, "inner": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 97], "innermost": 97, "input": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 56, 61, 87, 90, 98, 102, 128, 129, 130, 135], "insensit": 98, "insid": [42, 43, 46, 47], "insight": [72, 88], "insignific": 87, "inspect": 142, "inspir": [9, 12, 13, 19, 60, 63, 88], "instal": [55, 78, 85, 98, 144], "install_github": 141, "instanc": [46, 47, 55, 56, 80, 97], "instanti": [54, 55, 79, 80, 97, 112], "instead": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 51, 53, 55, 60, 61, 62, 63, 66, 67, 74, 75, 78, 80, 81, 96, 97, 98, 113, 117, 129, 131, 132, 133, 134, 137, 138, 144], "instruct": [141, 144], "instrument": [4, 5, 6, 7, 12, 15, 32, 38, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 75, 79, 80, 81, 84, 87, 89, 92, 93, 94, 97, 98, 100, 101, 103, 107, 109, 113, 122, 128, 142, 145], "instrument_effect": 51, "instrument_impact": 67, "instrument_strength": 86, "insuffienct": 78, "int": [9, 10, 11, 14, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 53, 54, 55, 58, 61, 67, 71, 83, 84, 86, 88, 89, 93, 94], "int32": 61, "int64": [60, 62, 63, 64, 77, 79, 92, 93, 94, 142], "int8": [80, 81, 87], "integ": [26, 56, 97], "integr": [78, 88, 129, 139, 144], "intend": [40, 56, 88, 145], "intent": [98, 145], "inter": 97, "interact": [9, 12, 13, 14, 29, 30, 32, 33, 40, 41, 66, 88, 95, 97, 107, 108, 136, 137, 140, 144, 145], "interchang": 128, "interest": [9, 10, 32, 33, 38, 39, 52, 55, 57, 58, 68, 72, 80, 81, 85, 86, 89, 91, 96, 98, 100, 102, 105, 106, 107, 108, 109, 110, 111, 113, 128, 142, 145], "interfac": [53, 55, 56, 92, 94, 97, 112, 142], "intermedi": [74, 88], "intern": [53, 55, 56, 66, 78, 81, 97, 98, 99, 143], "internet": [55, 80, 81], "interpret": [60, 61, 62, 63, 73, 74, 86, 88, 96, 129, 130, 136, 137, 138, 139, 141, 145], "intersect": [88, 129, 135, 144], "interv": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 53, 54, 55, 57, 58, 60, 61, 62, 63, 66, 69, 70, 71, 73, 74, 76, 79, 81, 84, 85, 87, 89, 95, 96, 112, 113, 129, 135, 142, 143, 144, 145], "intial": 85, "introduc": [52, 68, 91, 92, 94, 128, 144, 145], "introduct": [52, 54, 56, 68, 79, 81, 87, 97, 98, 100, 104, 129, 130], "introductori": [53, 88], "intrument": [57, 89], "intspecifi": 40, "intuit": 88, "inuidur1": [56, 64, 92, 94, 142], "inuidur1femaleblackothracedep1dep2q2q3q4q5q6agelt35agegt54durablelusdhusdtg": [56, 92, 94, 142], "inuidur2": [64, 92, 94, 142], "inv_sigmoid": 82, "invalid": [52, 63, 68, 91], "invari": [98, 100, 104], "invers": [29, 31, 32, 33, 34, 35, 36, 37, 57, 89, 129, 136, 137], "invert": [32, 86], "invert_yaxi": 79, "investig": [72, 78, 88], "involv": [96, 97, 113, 145], "io": [82, 144], "ipw_norm": 144, "ipykernel_46831": 74, "ipynb": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89], "ira": [55, 80, 81], "irm": [0, 13, 14, 20, 22, 23, 27, 28, 38, 39, 44, 45, 77, 88, 90, 95, 97, 101, 103, 108, 121, 136, 137, 140, 144, 145], "irm_summari": 80, "irmglmnet": 55, "irmrang": 55, "irmrpart": 55, "irmxgboost8047": 55, "irrespect": 88, "irrevers": [98, 104], "is_classifi": [20, 22, 23, 29, 32, 33, 39], "is_gat": [29, 33, 39, 44], "isfinit": [60, 61, 62, 63], "isnan": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 77, 97], "isoton": 88, "isotonicregress": 88, "issn": 72, "issu": [21, 24, 88, 140, 143, 144], "ite": [60, 63, 66, 73, 74, 75], "ite_lower_quantil": [60, 63], "ite_mean": [60, 63], "ite_upper_quantil": [60, 63], "item": [32, 80, 90, 97, 112], "iter": [40, 51, 57, 58, 79, 80, 85, 89, 97, 128, 145], "itertool": 72, "its": [42, 43, 88, 90, 96, 97, 98, 100, 101, 103, 112, 113, 128], "iv": [12, 15, 16, 32, 38, 39, 52, 54, 68, 79, 91, 92, 94, 107, 109, 124, 125, 129, 138, 140, 144, 145], "iv_2": 51, "iv_var": [54, 79], "iv\u00e1n": [112, 143], "j": [7, 8, 9, 10, 12, 13, 15, 16, 17, 18, 19, 25, 26, 52, 53, 54, 56, 57, 66, 68, 72, 79, 82, 86, 89, 91, 97, 98, 105, 128, 140, 142], "j_": [54, 79], "j_0": 128, "j_1": [54, 79], "j_2": [54, 79], "j_3": [54, 79], "j_k": [54, 79], "jame": 143, "janari": [60, 63], "janni": [55, 80], "januari": [60, 63], "jasenakova": 144, "javanmard": 143, "jbe": [54, 79], "jeconom": [9, 10, 25, 26, 53], "jerzi": 143, "jia": 88, "jitter": 24, "jitter_valu": 24, "jk": [98, 106], "jmlr": [56, 140, 142, 144], "job": [55, 80, 81], "john": 143, "joint": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 60, 61, 62, 63, 66, 69, 70, 71, 73, 74, 76, 81, 84, 86, 98, 100, 128, 144, 145], "jointli": [84, 96], "jonathan": 143, "joss": [56, 97, 140, 142], "journal": [7, 8, 9, 10, 16, 17, 19, 25, 26, 53, 54, 56, 72, 79, 82, 86, 88, 91, 97, 140, 142, 143, 144], "jss": 140, "jump": [83, 85, 98], "jun": [53, 143], "jupyt": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89], "juraj": 143, "just": [53, 56, 58, 59, 60, 63, 66, 71, 73, 74, 75, 76, 83, 84, 98, 113, 114, 115, 116, 117, 129, 130], "justif": [112, 129, 130], "k": [7, 10, 12, 13, 15, 16, 17, 19, 25, 26, 52, 54, 56, 68, 77, 78, 79, 85, 86, 90, 91, 95, 96, 98, 128, 145], "k_h": [85, 98], "kaggl": [55, 80], "kallu": [71, 81, 84, 86, 87, 113, 119, 122, 123, 143], "kappa": 98, "kato": [16, 54, 79, 128, 143], "kb": [58, 61, 62, 66, 75, 79, 80, 81, 87, 92, 93, 94, 142], "kde": [34, 35, 36, 80], "kdeplot": [58, 77, 89], "kdeunivari": [34, 35, 36], "kecsk\u00e9sov\u00e1": 144, "keel": 86, "keep": [43, 47, 53, 74, 76, 88, 145], "kei": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 45, 54, 55, 69, 70, 73, 74, 78, 79, 80, 81, 85, 88, 97, 98, 113, 129, 135, 144], "keith": 143, "kelz": 86, "kengo": 143, "kennedi": 86, "kernel": [34, 35, 36, 40, 43, 47, 85, 98], "kernel_regress": 85, "kernelreg": 85, "keyword": [16, 17, 18, 25, 26, 29, 33, 39, 41, 44], "kf": 112, "kfold": [79, 112], "kind": [51, 67, 80], "kj": [10, 12, 13, 15, 16, 17, 19, 25, 26, 52, 54, 68, 79, 91], "klaassen": [12, 72, 77, 78, 88, 140, 143], "klaa\u00dfen": 12, "knau": 143, "know": [58, 83, 86], "knowledg": [51, 67, 77, 82, 83], "known": [75, 77, 85, 86, 88, 97, 98, 104], "kohei": 143, "kotthof": 56, "kotthoff": [56, 97, 140, 142], "krueger": 82, "kueck": [55, 80], "kurz": [140, 143, 144], "kwarg": [9, 10, 14, 16, 17, 18, 25, 26, 29, 33, 39, 40, 41, 42, 44, 78, 98], "l": [54, 56, 57, 64, 69, 70, 79, 86, 88, 89, 97, 129, 138, 140, 142], "l1": [80, 89, 98], "l_hat": [38, 39, 52, 68, 113], "lab": 57, "label": [21, 24, 42, 46, 59, 66, 68, 69, 70, 71, 73, 74, 76, 78, 81, 82, 84, 85], "labor": 82, "laffer": 143, "laff\u00e9r": [19, 57, 89, 98, 111, 113, 126, 127], "lal": [82, 144], "lambda": [54, 55, 56, 57, 60, 63, 80, 82, 83, 97, 98, 113, 114, 115, 128, 142], "lambda_": 72, "lambda_0": [113, 114, 115], "lambda_t": [26, 63], "land": 83, "lang": [56, 97, 140, 142], "langl": [11, 83], "lanni": 86, "lappli": 112, "larg": [52, 68, 75, 77, 78, 82, 88, 98], "larger": [33, 53, 85, 88, 129, 135], "largest": 77, "largli": 77, "lasso": [54, 55, 56, 57, 61, 80, 89, 97, 142, 143], "lasso_class": [55, 80], "lasso_pip": [56, 97], "lasso_summari": 80, "lassocv": [37, 61, 72, 79, 80, 89, 97, 98, 128, 142], "last": [26, 56, 141], "late": [32, 51, 55, 80, 86, 98, 107, 113, 120], "latent": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 87, 129, 138, 139], "later": [55, 56, 85, 88, 97, 145], "latest": 140, "latter": [46, 47, 86, 98], "layout": 72, "lbrace": [12, 13, 19, 32, 33, 54, 79, 90, 98, 105, 107, 108, 112, 113, 118, 128, 129, 136], "ldot": [38, 39, 54, 57, 79, 89, 90, 98, 109, 110, 112, 128, 142], "le": [26, 58, 83, 96, 98, 100, 113, 122, 123], "lead": [53, 88, 98], "leadsto": 128, "lear": [56, 97, 140, 142], "learn": [4, 5, 6, 7, 8, 16, 17, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 55, 56, 61, 64, 66, 67, 71, 72, 76, 77, 78, 80, 81, 82, 84, 85, 86, 88, 92, 94, 95, 97, 112, 113, 128, 129, 130, 144, 145], "learner": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55, 57, 58, 60, 61, 62, 63, 68, 69, 70, 72, 79, 80, 81, 87, 88, 89, 90, 91, 95, 98, 100, 101, 103, 112, 113, 128, 129, 135, 144, 145], "learner_class": [37, 144], "learner_cv": 56, "learner_forest_classif": 56, "learner_forest_regr": 56, "learner_l": 87, "learner_lasso": 56, "learner_list": 77, "learner_m": 87, "learner_nam": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 97], "learner_param_v": 56, "learner_rf": 128, "learnerclassif": 56, "learnerregr": 56, "learnerregrcvglmnet": 56, "learnerregrrang": [56, 97], "learning_r": [60, 63, 68, 71, 81, 84, 85, 88, 91], "least": [51, 55, 67, 80, 81, 87, 98, 102, 112], "leav": [57, 88, 89], "left": [12, 13, 15, 16, 19, 25, 52, 54, 66, 68, 77, 79, 80, 81, 82, 84, 85, 91, 98, 113, 114, 115, 116, 117, 128, 129, 131, 132, 133, 134, 136, 137], "legend": [55, 59, 60, 63, 66, 68, 69, 70, 71, 73, 74, 76, 77, 81, 82, 84], "lemp": 61, "len": [66, 71, 77, 78, 79, 81, 84, 86], "length": [20, 21, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 56, 58, 60, 63, 86, 97], "leq": [54, 79], "less": [53, 55, 80, 81, 85, 88], "lester": 143, "let": [9, 10, 14, 25, 26, 52, 53, 55, 56, 57, 58, 60, 63, 66, 68, 71, 73, 74, 76, 77, 80, 81, 84, 88, 89, 90, 91, 97, 98, 100, 104, 111, 129, 130, 139, 145], "level": [14, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 54, 55, 57, 58, 59, 60, 61, 62, 63, 66, 69, 70, 71, 73, 74, 75, 76, 79, 80, 81, 84, 86, 87, 88, 89, 97, 105, 106, 113, 118, 129, 135, 136, 145], "level_0": [56, 79], "level_1": 79, "level_bound": 66, "levi": 86, "levinsohn": [54, 79], "lewi": 143, "lgbmclassifi": [58, 59, 60, 63, 71, 77, 81, 84, 85, 88], "lgbmregressor": [58, 59, 60, 63, 68, 71, 77, 81, 85, 88, 91], "lgr": [52, 53, 54, 55, 56, 57, 90, 97, 98, 112, 113, 128, 142], "lib": [60, 61, 62, 63, 79, 80], "liblinear": [80, 89, 98], "librari": [51, 52, 53, 54, 55, 56, 57, 90, 91, 92, 94, 97, 98, 112, 113, 128, 141, 142, 145], "licens": [140, 144], "lie": 143, "lightgbm": [58, 59, 60, 63, 68, 71, 77, 81, 84, 85, 88], "like": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 53, 55, 56, 60, 61, 62, 63, 72, 74, 80, 81, 88, 97, 98, 99, 112, 142, 145], "lim": 82, "lim_": [85, 98], "limegreen": [69, 70], "limit": [82, 98, 104, 143], "limits_": 96, "lin": [85, 88, 98], "line": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 59, 61, 66, 88], "linear": [9, 10, 14, 15, 16, 17, 18, 25, 27, 28, 29, 33, 38, 39, 44, 51, 52, 53, 54, 56, 58, 59, 61, 66, 67, 68, 69, 70, 72, 73, 76, 77, 78, 79, 86, 87, 88, 90, 91, 95, 96, 97, 101, 102, 103, 109, 110, 112, 114, 115, 116, 117, 118, 120, 121, 124, 125, 128, 135, 137, 138, 139, 140, 142, 143, 144, 145], "linear_learn": [60, 63], "linear_model": [29, 33, 37, 39, 44, 60, 61, 62, 63, 64, 66, 67, 72, 76, 77, 79, 80, 85, 86, 88, 89, 97, 98, 128, 142], "linearli": [85, 98], "linearregress": [51, 60, 61, 62, 63, 66, 67, 76, 77, 85, 86, 88], "linearscoremixin": [0, 113], "lineplot": [60, 63, 66], "linestyl": [59, 60, 63, 66, 78, 85], "linetyp": 57, "linewidth": [59, 60, 63], "link": [88, 144], "linspac": [69, 70, 76, 88], "lint": 144, "linux": 141, "list": [4, 5, 6, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 47, 52, 53, 54, 55, 56, 60, 61, 63, 68, 69, 70, 79, 81, 83, 91, 97, 112, 113, 141, 144], "list_confset": 32, "listedcolormap": 79, "literatur": [88, 98, 100, 104], "littl": [75, 86], "ll": [56, 128, 145], "lllllllllllllllll": [92, 94, 142], "lm": [51, 53, 88], "ln_alpha_ml_l": 72, "ln_alpha_ml_m": 72, "load": [51, 53, 55, 56, 72, 80, 81, 92, 94, 141, 142], "loader": 0, "loc": [59, 60, 61, 62, 63, 66, 68, 71, 72, 74, 79, 82, 84, 87, 88], "local": [32, 34, 86, 96, 98, 107, 143, 144], "localconvert": 79, "locat": [71, 84, 98], "log": [54, 60, 61, 62, 63, 72, 77, 79, 82, 87, 97, 98, 100, 101, 103], "log_odd": 83, "log_p": [54, 79], "log_reg": [51, 53], "logarithm": 72, "logic": [32, 56, 97], "logical_not": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 77, 97], "logist": [9, 41, 51, 53, 55, 57, 61, 66, 67, 80, 86, 88, 89, 145], "logisticregress": [51, 60, 61, 62, 63, 64, 66, 67, 76, 85, 86, 88], "logisticregressioncv": [37, 61, 77, 80, 89, 98], "logit": [77, 82], "loglik": 56, "logloss": [55, 80, 145], "logo": 144, "logspac": 80, "long": [6, 9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 61, 68, 77, 87, 88, 129, 130, 139, 143], "longer": [60, 63, 98, 102], "look": [53, 55, 56, 58, 59, 60, 61, 62, 63, 71, 77, 80, 81, 84, 85, 87], "loop": [66, 86], "loss": [60, 61, 62, 63, 77, 78, 85, 87, 97, 98, 100, 101, 103], "loss_ml_g0": 77, "loss_ml_g1": 77, "loss_ml_m": 77, "low": [59, 75, 86, 96, 143], "lower": [32, 55, 56, 59, 60, 63, 66, 71, 72, 75, 76, 81, 82, 84, 85, 86, 87, 88, 97, 129, 135, 139, 145], "lower_bound": [69, 70], "lpop": 61, "lpq": [34, 36, 81, 96, 122, 144], "lpq_0": 84, "lpq_1": 84, "lqte": 96, "lr": 85, "lrn": [51, 52, 53, 54, 55, 56, 57, 90, 97, 98, 112, 113, 128, 142, 145], "lrn_0": 56, "lt": [51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 75, 79, 80, 81, 83, 87, 88, 89, 92, 94, 142], "lucien": 144, "luka": 143, "luk\u00e1\u0161": 19, "lusd": [56, 64, 92, 94, 142], "lvert": 72, "m": [6, 7, 8, 9, 15, 16, 17, 24, 37, 52, 54, 56, 60, 63, 64, 68, 72, 75, 77, 78, 79, 82, 86, 91, 93, 94, 95, 96, 97, 98, 99, 101, 103, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144], "m_": [66, 98, 101, 103, 105, 113, 115, 117, 118, 122, 128], "m_0": [17, 18, 20, 22, 23, 24, 29, 31, 32, 33, 35, 38, 39, 40, 52, 54, 55, 68, 72, 75, 78, 79, 80, 91, 96, 97, 98, 107, 108, 109, 110, 113, 114, 115, 116, 117, 119, 122, 123, 125, 126, 127, 142, 145], "m_hat": [32, 33, 38, 39, 52, 68, 76, 113], "m_i": [85, 98], "ma": [16, 54, 79, 86, 98, 99, 143], "mac": 141, "machin": [4, 5, 6, 7, 8, 16, 17, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 71, 72, 76, 77, 78, 80, 81, 82, 84, 85, 86, 87, 88, 89, 95, 97, 98, 100, 101, 103, 112, 113, 128, 129, 130, 144, 145], "machineri": [72, 143], "mackei": 143, "maco": 141, "made": [98, 111, 145], "mae": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 77, 97], "maggi": 143, "magnitud": [129, 130], "mai": [43, 47, 57, 58, 89], "main": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 62, 72, 81, 88, 98, 128, 129, 130, 143, 145], "mainli": 88, "maintain": [53, 140, 144], "mainten": 144, "major": [56, 88, 144], "make": [51, 66, 67, 77, 78, 88, 96, 97, 144, 145], "make_confounded_irm_data": [88, 144], "make_confounded_plr_data": 87, "make_did_cs2021": [6, 24, 60, 63, 93, 94, 98, 99, 103], "make_did_cs_cs2021": [63, 98, 101], "make_did_sz2020": [20, 23, 58, 98, 100], "make_heterogeneous_data": [69, 70, 73, 74, 75], "make_iivm_data": [32, 34, 96, 98], "make_irm_data": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 76, 77, 96, 97, 98], "make_irm_data_discrete_treat": 66, "make_pipelin": 80, "make_pliv_chs2015": [38, 98], "make_pliv_multiway_cluster_ckms2021": [4, 54, 79], "make_plr_ccddhnr2018": [5, 6, 20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 68, 78, 90, 91, 96, 97, 98, 112, 113, 128, 129, 135], "make_simple_rdd_data": [40, 85, 98], "make_spd_matrix": 18, "make_ssm_data": [57, 89, 98], "malt": [140, 143], "man": [51, 67], "manag": [97, 141], "mani": [15, 27, 28, 52, 53, 54, 56, 58, 68, 78, 79, 91, 113, 128, 145], "manili": 44, "manipul": [55, 56, 85, 98], "manual": [55, 76, 78, 87, 145], "mao": 143, "map": [32, 42, 43, 46, 47, 53, 54, 79, 96, 98, 107], "mapsto": [90, 96], "mar": [19, 98], "march": [72, 77, 78], "margin": [69, 70, 88], "marit": [55, 80], "marker": [60, 63, 66, 88], "markers": 82, "market": 82, "markettwo": 54, "markov": [18, 143], "marr": [55, 80, 81, 87, 145], "marshal": 97, "martin": [19, 88, 140, 143, 144], "masatoshi": 143, "masip": [86, 144], "mask": 24, "maskedarrai": [98, 99], "master": [53, 61], "mat": 54, "match": [97, 129, 138], "math": [37, 60, 61, 62, 63], "mathbb": [9, 10, 14, 25, 26, 27, 28, 32, 33, 38, 39, 54, 57, 58, 59, 60, 63, 66, 75, 77, 78, 79, 82, 85, 89, 96, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 125, 126, 127, 128, 129, 131, 132, 133, 134, 135, 136, 137, 138, 139, 142, 145], "mathcal": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 41, 52, 54, 57, 59, 68, 71, 79, 83, 84, 89, 91, 98, 99, 102, 104], "mathop": 96, "mathrm": [9, 10, 60, 61, 62, 63, 85, 98, 99, 101, 102, 103, 104, 113, 115, 117, 129, 132, 134], "matia": 143, "matplotlib": [21, 24, 58, 59, 60, 61, 62, 63, 64, 66, 68, 69, 70, 71, 73, 74, 76, 77, 78, 79, 80, 81, 82, 84, 85, 88, 89], "matric": [83, 144], "matrix": [9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 43, 47, 52, 54, 55, 56, 57, 68, 79, 89, 91, 92, 94, 97, 128, 142, 144, 145], "matt": 143, "matter": [77, 82], "max": [25, 55, 56, 76, 80, 81, 86, 90, 96, 97, 98, 112, 113, 115, 117, 119, 128, 129, 132, 134, 142, 145], "max_depth": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 64, 80, 87, 90, 96, 97, 98, 100, 112, 113, 128, 129, 135, 142, 145], "max_featur": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 64, 80, 87, 90, 96, 97, 98, 112, 113, 128, 129, 135, 142, 145], "max_it": [79, 80, 88], "maxim": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 83, 96, 98], "maxima": 128, "maximum": [96, 97], "mb": [60, 63, 64, 89, 92, 94, 142], "mb706": 144, "mea": 12, "mean": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 51, 52, 54, 55, 58, 60, 63, 66, 67, 68, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 84, 86, 87, 88, 91, 97, 98, 128, 145], "mean_absolute_error": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 77, 97], "meant": [61, 96, 144], "measir": 87, "measur": [53, 56, 61, 72, 78, 86, 87, 88, 97, 98, 99, 111, 129, 130, 136, 137, 138, 139], "measure_col": 72, "measure_func": 53, "measure_pr": 53, "measures_r": 53, "mechan": [42, 43, 46, 47, 88, 98, 104], "median": [86, 88, 112], "melt": 54, "membership": 88, "memori": [58, 60, 61, 62, 63, 64, 66, 75, 79, 80, 81, 87, 89, 92, 93, 94, 142], "mention": [75, 96], "merg": [55, 80], "mert": [112, 143], "meshgrid": [69, 70, 88], "messag": [52, 53, 54, 55, 56, 57, 142, 144], "meta": [42, 43, 46, 47, 97, 142], "metadata": [42, 43, 46, 47], "metadata_rout": [42, 43, 46, 47], "metadatarequest": [42, 43, 46, 47], "method": [4, 5, 6, 20, 21, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 73, 74, 76, 77, 79, 80, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 129, 130, 135, 140, 142, 144], "methodolog": 143, "methodologi": 88, "metric": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 46, 97], "michael": 143, "michaela": 144, "michel": [140, 142], "michela": [19, 143], "mid": [55, 80, 82, 85, 98, 113, 125], "mid_point": 66, "might": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 54, 61, 71, 76, 77, 79, 83, 85, 87, 88, 97, 98], "mild": [52, 68, 91], "militari": 82, "miller": [54, 79], "mimic": 88, "min": [54, 55, 56, 57, 60, 61, 62, 63, 71, 76, 79, 80, 81, 84, 85, 90, 97, 98, 102, 112, 113, 128, 142, 145], "min_": 96, "min_samples_leaf": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 75, 80, 87, 90, 96, 97, 98, 100, 112, 113, 128, 129, 135, 145], "min_samples_split": [80, 98, 99, 101, 103], "minim": [33, 45, 55, 77, 80, 85, 98], "minimum": 61, "minor": [52, 61, 68, 91, 113, 144], "minsplit": 55, "minut": 78, "miruna": 143, "mislead": 144, "miss": [4, 5, 6, 37, 56, 97, 98, 113, 126, 144], "missing": [19, 57, 89], "misspecif": 58, "misspecifi": 58, "mit": [140, 142], "mixin": [0, 27, 28, 113], "ml": [18, 54, 55, 56, 61, 72, 78, 79, 80, 85, 86, 90, 95, 97, 98, 112, 140, 143, 144], "ml_g": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 55, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 71, 73, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 96, 97, 98, 99, 100, 101, 103, 144], "ml_g0": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 53, 55, 58, 60, 61, 62, 64, 77, 80, 87, 97, 98, 100, 103], "ml_g1": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 53, 55, 58, 60, 61, 62, 64, 77, 80, 87, 97, 98, 100, 103], "ml_g_d0": [89, 98], "ml_g_d0_t0": [58, 63, 98, 100, 101], "ml_g_d0_t1": [58, 63, 98, 100, 101], "ml_g_d1": [89, 98], "ml_g_d1_t0": [58, 63, 98, 100, 101], "ml_g_d1_t1": [58, 63, 98, 100, 101], "ml_g_d_lvl0": 98, "ml_g_d_lvl1": 98, "ml_g_sim": 37, "ml_l": [38, 39, 52, 54, 55, 56, 64, 68, 70, 74, 78, 79, 80, 82, 87, 90, 91, 97, 98, 112, 113, 128, 129, 135, 142, 144, 145], "ml_l_bonu": 142, "ml_l_forest": 56, "ml_l_forest_pip": 56, "ml_l_lasso": 56, "ml_l_lasso_pip": 56, "ml_l_rf": 145, "ml_l_sim": 142, "ml_l_tune": 97, "ml_l_xgb": 145, "ml_m": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 129, 135, 142, 144, 145], "ml_m_bench_control": 88, "ml_m_bench_treat": 88, "ml_m_bonu": 142, "ml_m_forest": 56, "ml_m_forest_pip": 56, "ml_m_lasso": 56, "ml_m_lasso_pip": 56, "ml_m_rf": 145, "ml_m_sim": [37, 142], "ml_m_tune": 97, "ml_m_xgb": 145, "ml_pi": [37, 57, 89, 98], "ml_pi_sim": 37, "ml_r": [32, 38, 51, 54, 55, 67, 79, 80, 86, 98, 144], "ml_r0": 98, "ml_r1": [55, 80, 98], "mlr": [56, 97], "mlr3": [51, 52, 53, 54, 55, 57, 90, 97, 98, 112, 113, 128, 140, 142, 144, 145], "mlr3book": [56, 97], "mlr3extralearn": [55, 97], "mlr3filter": 56, "mlr3learner": [51, 52, 53, 54, 55, 90, 97, 98, 112, 113, 128, 142, 145], "mlr3measur": 53, "mlr3pipelin": [97, 144], "mlr3tune": [56, 97, 144], "mlr3vers": 55, "mlrmeasur": 53, "mode": [88, 141], "model": [0, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 47, 48, 51, 52, 53, 54, 56, 58, 59, 60, 61, 62, 63, 67, 68, 71, 72, 75, 77, 79, 81, 84, 87, 90, 91, 92, 93, 94, 95, 97, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111, 114, 115, 116, 117, 118, 120, 121, 124, 125, 130, 135, 136, 137, 138, 139, 140, 143, 144], "model_data": [55, 80], "model_label": 78, "model_select": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 68, 79, 97, 112], "modellist": 76, "modelmlestimatelowerupp": 55, "modern": [56, 97, 140, 142], "modul": [85, 98, 141], "moment": [27, 28, 54, 79, 98, 101, 102, 103, 113, 128, 129, 130, 139, 142], "monoton": 98, "mont": [9, 10, 11, 14, 69, 70, 73, 74], "montanari": 143, "month": [60, 63], "more": [33, 44, 51, 53, 55, 60, 61, 62, 63, 66, 67, 69, 70, 72, 76, 77, 78, 80, 81, 85, 87, 88, 90, 96, 97, 98, 99, 100, 101, 102, 103, 104, 111, 113, 121, 128, 129, 130, 135, 139, 142, 145], "moreov": [55, 56, 61, 72, 97, 128, 145], "mortgag": [55, 80, 81], "most": [55, 71, 77, 80, 81, 84, 88, 96, 97, 98, 129, 135, 141], "motiv": [88, 91], "motivation_example_bch": 72, "mp": 53, "mpd": [54, 79], "mpdta": 61, "mpg": 79, "mse": [56, 72, 97], "mserd": 85, "msg": 60, "msr": [56, 97], "mtry": [55, 56, 90, 97, 98, 112, 113, 128, 145], "mu": 59, "mu_": 59, "mu_0": 98, "mu_mean": 59, "much": [55, 56, 80, 85, 86, 88, 145], "muld": [64, 92, 94, 142], "multi": [24, 42, 46, 53, 54, 69, 70, 79, 113, 115, 117, 144], "multiclass": [56, 78], "multiindex": 79, "multioutput": [43, 47], "multioutputregressor": [43, 47], "multipl": [4, 5, 6, 14, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 54, 55, 57, 58, 61, 62, 76, 79, 80, 87, 88, 89, 92, 94, 97, 102, 106, 109, 112, 128, 129, 130, 143, 144, 145], "multipletest": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39], "multipli": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 68, 95, 96, 113, 145], "multiprocess": [71, 81, 84], "multitest": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39], "multivariate_norm": 37, "multiwai": [16, 54, 79, 143], "music": 143, "must": [20, 21, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 97, 98], "mutat": 56, "mutual": [29, 33, 39, 55, 73, 74, 80, 81, 96], "my_sampl": 112, "my_task": 112, "n": [9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 25, 26, 29, 31, 32, 33, 34, 35, 37, 38, 39, 41, 51, 52, 54, 56, 57, 59, 60, 63, 66, 67, 68, 71, 72, 75, 79, 82, 83, 84, 85, 86, 89, 90, 91, 96, 97, 98, 104, 112, 128, 140, 141], "n_": [14, 59, 63, 129, 132, 134], "n_aggreg": 21, "n_coef": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 129, 135], "n_color": [60, 63], "n_complier": 84, "n_core": [71, 81, 84], "n_estim": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 58, 59, 60, 63, 64, 68, 69, 70, 71, 73, 74, 75, 80, 81, 83, 84, 85, 87, 88, 90, 91, 96, 97, 98, 100, 112, 113, 128, 129, 135, 142, 145], "n_eval": [56, 97], "n_featur": [42, 43, 46, 47], "n_fold": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 54, 55, 58, 60, 61, 63, 64, 68, 69, 70, 71, 73, 74, 76, 77, 79, 80, 81, 82, 83, 84, 85, 87, 88, 91, 97, 112, 142, 145], "n_folds_per_clust": [54, 79], "n_folds_tun": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "n_framework": 21, "n_iter": [40, 85, 98], "n_iter_randomized_search": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 97], "n_job": 80, "n_jobs_cv": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 77], "n_jobs_model": [24, 30, 36, 71, 81, 84], "n_level": [14, 66], "n_ob": [6, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 44, 45, 52, 56, 57, 58, 59, 60, 63, 66, 68, 69, 70, 73, 74, 75, 76, 77, 78, 85, 87, 88, 89, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 112, 128, 129, 135, 142], "n_output": [42, 43, 46, 47], "n_period": [25, 60, 63], "n_pre_treat_period": [25, 60, 63], "n_rep": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 54, 57, 58, 60, 63, 64, 66, 68, 75, 76, 77, 79, 85, 87, 88, 89, 91, 97, 112, 129, 135, 142, 145], "n_rep_boot": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 60, 61, 62, 63, 66, 69, 70, 71, 73, 74, 76, 81, 84, 128], "n_sampl": [42, 43, 46, 47, 83, 86], "n_samples_fit": [43, 47], "n_split": 112, "n_t": 59, "n_target": [46, 47], "n_theta": 21, "n_time_period": 59, "n_true": [71, 84], "n_var": [52, 56, 68, 91, 92, 94, 97, 128, 142], "n_w": 83, "n_x": [11, 69, 70, 73, 74, 75], "na": [4, 5, 6, 52, 54, 57, 91, 144], "na_real_": [54, 144], "naiv": [52, 68, 91], "name": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 52, 53, 54, 60, 61, 63, 73, 74, 75, 78, 79, 85, 87, 88, 97, 141, 144], "namespac": 53, "nan": [4, 5, 6, 20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 58, 59, 66, 68, 71, 73, 74, 77, 78, 80, 81, 84, 89, 91, 97], "nanmean": 68, "narita": 143, "nat": [60, 63], "nathan": 143, "nation": [88, 112, 143], "nativ": 53, "natt": 83, "natur": 88, "ncol": [54, 55, 56, 85, 92, 94, 97, 128, 142], "ncoverag": 77, "ndarrai": [4, 5, 6, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 92, 94], "nearli": 77, "necess": [54, 79], "necessari": [53, 54, 78, 79, 85, 98, 141], "need": [20, 22, 23, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 53, 55, 57, 67, 68, 78, 81, 86, 89, 97, 112, 129, 139, 144, 145], "neg": [43, 47], "neighborhood": [85, 128], "neither": [4, 5, 6, 54, 79, 92, 94], "neng": 143, "neq": [85, 98], "nest": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 46, 47, 97, 113, 127, 129, 135], "net": [81, 87, 145], "net_tfa": [55, 80, 81, 87, 145], "nev": [98, 101, 103, 104], "never": [25, 32, 53, 54, 60, 61, 62, 63, 74, 79, 98, 104, 144], "never_tak": [32, 55, 80], "never_tr": [22, 24, 60, 61, 62, 63, 98, 99, 101, 103], "nevertheless": 76, "new": [51, 52, 53, 54, 55, 56, 57, 69, 70, 78, 80, 83, 90, 91, 92, 94, 96, 97, 98, 112, 113, 128, 140, 142, 143, 144, 145], "new_data": [69, 70, 83], "newei": [7, 8, 17, 54, 72, 79, 88, 91, 140, 143], "newest": 144, "next": [53, 55, 56, 69, 70, 71, 75, 77, 80, 81, 83, 84, 86, 88, 144], "neyman": [54, 79, 90, 95, 129, 139, 140, 143], "nfold": [54, 55, 57, 98], "nh": 98, "nice": 53, "nifa": [80, 81, 87], "nil": 88, "nine": [54, 79], "nn": 85, "noack": [85, 98, 143, 144], "node": [55, 56, 90, 98, 112, 113, 128, 142, 145], "nois": [41, 82, 83], "nomin": 86, "non": [16, 17, 18, 22, 25, 26, 32, 40, 51, 52, 55, 59, 60, 63, 67, 68, 80, 81, 83, 85, 97, 112, 113, 115, 117, 128, 129, 132, 134], "non_orth_scor": [52, 68, 113], "nondur": 64, "none": [4, 5, 6, 14, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 54, 55, 58, 60, 61, 62, 63, 64, 66, 67, 75, 80, 81, 86, 87, 88, 89, 92, 93, 94, 97, 98, 100, 101, 103, 113, 128, 141, 142], "nonignor": [37, 127], "nonlinear": [25, 28, 55, 60, 63, 80, 85, 98, 113, 122, 123, 144], "nonlinearscoremixin": [0, 113], "nonparametr": [34, 35, 36, 85, 88, 113, 129, 130, 136, 137, 138, 139, 143], "nop": 56, "nor": [4, 5, 6, 54, 79, 92, 94], "norm": 68, "normal": [20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 57, 58, 59, 60, 62, 63, 67, 68, 71, 75, 81, 82, 83, 84, 85, 86, 89, 91, 92, 94, 97, 98, 113, 114, 115, 116, 117, 128, 142], "normalize_ipw": [29, 30, 31, 32, 33, 34, 35, 36, 37, 57, 76, 81, 89], "not_yet_tr": [22, 24, 60, 63], "notat": [54, 57, 58, 79, 89, 98, 100, 101, 102, 103, 104, 111, 113, 115, 117], "note": [4, 5, 6, 21, 24, 27, 28, 32, 33, 37, 38, 39, 42, 43, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 112, 113, 140, 142], "notebook": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 96, 97, 98, 145], "notic": [51, 67, 86], "now": [53, 54, 55, 57, 61, 62, 69, 70, 77, 79, 80, 83, 86, 88, 89, 142, 144], "np": [4, 5, 6, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 58, 59, 60, 61, 62, 63, 64, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 129, 135, 142, 145], "nround": [52, 55, 145], "nrow": [53, 54, 56, 85, 92, 94, 97, 128, 142], "nu": [18, 26, 32, 57, 89, 98, 107, 129, 130, 132, 134, 135, 138, 139], "nu2": [60, 129, 135], "nu_0": [129, 139], "nu_i": [57, 89], "nuis_g0": 51, "nuis_g1": 51, "nuis_l": 145, "nuis_m": [51, 145], "nuis_r0": 51, "nuis_r1": 51, "nuis_rmse_ml_l": 72, "nuis_rmse_ml_m": 72, "nuisanc": [4, 5, 6, 17, 18, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 54, 55, 56, 57, 58, 61, 68, 69, 70, 71, 72, 75, 77, 79, 80, 81, 84, 86, 87, 88, 89, 90, 91, 97, 98, 101, 102, 103, 112, 113, 114, 115, 116, 117, 118, 122, 128, 129, 132, 134, 139, 140, 144, 145], "nuisance_el": [129, 131, 132, 133, 134, 136, 137, 138], "nuisance_loss": [77, 97, 144], "nuisance_target": 77, "null": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 53, 87, 97, 129, 135, 144], "null_hypothesi": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 87, 129, 135], "num": [55, 56, 90, 97, 98, 112, 113, 128, 142], "num_leav": [59, 71, 81, 84], "number": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 47, 52, 54, 59, 60, 61, 63, 68, 69, 70, 71, 72, 73, 74, 77, 79, 81, 83, 84, 85, 88, 98, 102, 112, 128, 140, 142, 145], "numer": [28, 51, 56, 76, 82, 97, 113, 129, 136, 137, 144], "numeric_onli": 72, "numpi": [4, 5, 6, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 45, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 94, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 129, 135, 142], "nuniqu": [60, 63], "ny": 143, "nyt": [98, 101, 103, 104], "o": [59, 66, 72, 73, 74, 77, 78, 80, 82, 85, 128, 140, 142], "ob": [53, 55, 59, 63, 85, 129, 132], "obei": 113, "obj": 80, "obj_dml_data": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 54, 61, 62, 67, 68, 71, 76, 78, 79, 84, 90, 91, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 129, 135, 144], "obj_dml_data_bonu": [92, 94], "obj_dml_data_bonus_df": [92, 94], "obj_dml_data_from_arrai": [4, 5, 6], "obj_dml_data_from_df": [4, 5], "obj_dml_data_sim": [92, 94], "obj_dml_plr": [52, 68, 91], "obj_dml_plr_bonu": [56, 142], "obj_dml_plr_bonus_pip": 56, "obj_dml_plr_bonus_pipe2": 56, "obj_dml_plr_bonus_pipe3": 56, "obj_dml_plr_bonus_pipe_ensembl": 56, "obj_dml_plr_fullsampl": 78, "obj_dml_plr_lesstim": 78, "obj_dml_plr_nonorth": [52, 68], "obj_dml_plr_orth_nosplit": [52, 68], "obj_dml_plr_sim": [56, 142], "obj_dml_plr_sim_pip": 56, "obj_dml_plr_sim_pipe_ensembl": 56, "obj_dml_plr_sim_pipe_tun": 56, "obj_dml_sim": 37, "object": [4, 5, 6, 7, 8, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 51, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 69, 70, 71, 74, 75, 76, 78, 80, 81, 84, 85, 89, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 140, 142, 143, 144, 145], "obs_confound": [51, 67], "observ": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 37, 38, 39, 41, 44, 45, 48, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 75, 77, 78, 79, 80, 81, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 104, 111, 112, 113, 114, 115, 116, 117, 128, 129, 130, 131, 132, 133, 134, 142, 143, 145], "obtain": [10, 32, 51, 52, 53, 54, 57, 58, 60, 61, 62, 63, 67, 68, 69, 70, 71, 72, 77, 79, 84, 86, 88, 89, 90, 91, 96, 97, 112, 113, 128, 129, 130, 135, 141, 142], "obvious": [60, 63], "occur": [25, 60, 63, 78, 144], "off": [83, 143], "offer": [25, 53, 55, 80, 81, 88, 145], "offici": 141, "offset": 97, "often": 84, "oka": 143, "ol": [29, 33, 39, 44], "olma": [85, 98, 143, 144], "omega": [75, 96, 98, 99, 113, 118, 121, 129, 136, 137], "omega_": [16, 54, 79], "omega_1": [16, 54, 79], "omega_2": [16, 54, 79], "omega_epsilon": [54, 79], "omega_v": [16, 54, 79], "omega_x": [16, 54, 79], "omit": [60, 63, 87, 88, 98, 102, 113, 115, 117, 129, 130, 139, 143, 144, 145], "ommit": 88, "onc": [53, 78, 88, 98, 104, 145], "one": [21, 24, 38, 48, 51, 52, 53, 54, 55, 56, 60, 61, 62, 63, 66, 67, 68, 69, 70, 77, 79, 81, 82, 85, 86, 87, 88, 91, 96, 97, 98, 99, 102, 104, 109, 112, 113, 114, 115, 116, 117, 121, 124, 125, 128, 129, 130, 135, 136, 137, 138, 142, 144], "ones": [56, 59, 71, 78, 84, 87, 96], "ones_lik": [66, 84], "onli": [4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 46, 47, 53, 54, 55, 60, 63, 69, 70, 73, 74, 75, 77, 78, 79, 80, 81, 85, 90, 96, 97, 98, 101, 103, 111, 113, 115, 117, 119, 122, 123, 128, 129, 130, 132, 134, 136, 137, 139, 144], "onlin": 145, "onto": 77, "oo": 78, "oob_error": [56, 97], "oop": 144, "opac": [69, 70], "open": [56, 97, 140, 142], "oper": [56, 144], "opposit": [83, 85, 98], "oprescu": [11, 69, 70, 73, 74, 143], "opt": [60, 61, 62, 63, 80], "optim": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 40, 56, 69, 70, 78, 83, 96, 97, 143], "option": [20, 21, 22, 23, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 51, 52, 54, 55, 57, 60, 61, 63, 66, 69, 70, 73, 74, 75, 77, 79, 80, 81, 89, 97, 98, 112, 113, 119, 122, 123, 128, 144], "oracl": [14, 41, 60, 63, 66], "oracle_valu": [9, 10, 14, 41, 66], "orang": 52, "orcal": [9, 10], "order": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 46, 53, 54, 55, 56, 61, 76, 79, 80, 85, 97, 98, 112, 113], "org": [12, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 97, 140, 141, 143, 144], "orient": [56, 97, 113, 140, 142, 143, 144], "origin": [42, 43, 45, 46, 47, 53, 56, 60, 61, 62, 63, 74, 83, 87, 88, 96, 113, 121], "orign": [55, 80], "orth_sign": [44, 45, 76], "orthogon": [44, 45, 54, 55, 60, 79, 80, 90, 95, 98, 128, 129, 139, 140, 143], "orthongon": [129, 139], "osx": 141, "other": [4, 5, 6, 38, 39, 42, 43, 46, 47, 52, 54, 55, 56, 57, 58, 60, 61, 62, 63, 66, 68, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 91, 92, 94, 96, 97, 98, 109, 110, 112, 113, 121, 128, 129, 139, 140, 141, 142, 143, 144, 145], "other_ind": 79, "otherwis": [20, 22, 23, 29, 32, 33, 39, 42, 43, 46, 47, 55, 80, 81, 83, 98, 100, 113, 115, 117], "othrac": [56, 64, 92, 94, 142], "our": [52, 53, 55, 56, 58, 60, 63, 68, 69, 70, 71, 77, 78, 80, 81, 84, 85, 87, 88, 91, 98, 140, 142, 144, 145], "ourselv": 77, "out": [38, 39, 54, 56, 58, 59, 60, 61, 62, 63, 64, 72, 77, 78, 79, 81, 87, 88, 89, 90, 92, 94, 95, 96, 97, 98, 99, 100, 101, 103, 113, 124, 125, 128, 129, 130, 135, 138, 140, 142, 144, 145], "outcom": [4, 5, 6, 9, 10, 14, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 51, 53, 54, 55, 56, 59, 60, 61, 62, 63, 64, 67, 72, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 92, 93, 94, 97, 100, 101, 103, 104, 105, 109, 110, 111, 115, 117, 118, 128, 130, 135, 136, 138, 139, 142, 144, 145], "outcome_0": 67, "outcome_1": 67, "outer": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 97], "output": [53, 60, 62, 63, 77, 86, 90, 98, 101, 103, 128, 145], "output_list": 86, "outshr": 79, "outsid": 52, "over": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 37, 38, 39, 52, 60, 61, 63, 66, 68, 72, 77, 91, 95, 97, 98, 99, 129, 135, 144], "overal": [21, 60, 61, 62, 63, 83, 88, 98, 99], "overall_aggregation_weight": [21, 60, 61, 62, 63], "overcom": [95, 113], "overfit": [78, 95, 112], "overlap": [58, 88, 98, 100, 104], "overrid": [97, 144], "overridden": 98, "overst": [55, 80, 81], "overview": [77, 128, 129, 135, 143], "overwrit": 144, "ownership": [55, 80], "p": [9, 10, 11, 13, 20, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 91, 96, 97, 98, 99, 100, 101, 103, 104, 112, 113, 114, 115, 116, 117, 118, 119, 122, 123, 126, 127, 128, 129, 136, 137, 140, 141, 142, 144], "p401": [55, 80, 81], "p_0": [113, 114, 115, 116], "p_1": 128, "p_adjust": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 112, 128, 140, 142], "p_dbl": [56, 97], "p_hat": 76, "p_int": 97, "p_n": 15, "p_val": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39], "p_x": [16, 54, 79], "p_x0": 82, "p_x1": 82, "packag": [51, 52, 54, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 78, 79, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 96, 97, 98, 100, 104, 111, 112, 113, 128, 129, 130, 140, 142, 143, 144, 145], "packagedata": 79, "packagevers": 55, "page": [88, 140, 143], "pair": [51, 67], "pake": [54, 79], "paket": [54, 55, 56], "pal": 54, "palett": [21, 24, 60, 63, 66], "pand": [60, 63], "panda": [4, 5, 6, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 44, 45, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 96, 98, 129, 130, 142], "pandas2ri": 79, "panel": [5, 6, 20, 22, 24, 25, 26, 61, 63, 93, 94, 101, 102, 104, 133, 134, 143, 144], "paper": [12, 15, 56, 78, 82, 85, 87, 88, 129, 139, 140, 142, 143, 144], "par": 64, "par_grid": [56, 97], "paradox": [56, 97, 144], "parallel": [53, 58, 59, 60, 63, 66, 71, 77, 84, 98, 100, 102, 104], "param": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 78, 97], "param_grid": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 97], "param_nam": 53, "param_set": [56, 97], "param_v": 56, "paramet": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 55, 57, 58, 60, 61, 63, 66, 68, 69, 70, 71, 72, 75, 76, 77, 79, 80, 83, 84, 85, 86, 87, 88, 89, 90, 91, 94, 95, 96, 97, 100, 101, 102, 103, 104, 105, 106, 107, 108, 111, 112, 122, 123, 128, 129, 130, 135, 137, 139, 140, 142, 143, 144, 145], "parametr": [32, 53, 88, 91, 97, 145], "params_exact": 97, "params_nam": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 53], "parenttoc": 140, "part": [18, 52, 54, 55, 56, 62, 68, 77, 78, 79, 80, 91, 97, 112, 129, 139, 144, 145], "parti": 18, "partial": [10, 15, 16, 17, 18, 28, 38, 39, 54, 56, 64, 72, 78, 79, 87, 90, 95, 97, 109, 110, 112, 124, 125, 128, 135, 136, 137, 138, 139, 140, 142, 144, 145], "partial_": [113, 128], "partiallli": 87, "particip": [7, 81, 87, 145], "particular": [98, 140], "particularli": 78, "partion": [54, 79], "partit": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 79, 90, 95], "partli": 145, "pass": [29, 33, 39, 42, 43, 44, 46, 47, 53, 56, 61, 78, 97, 145], "passo": [140, 142], "past": 54, "paste0": [54, 57], "pastel": 68, "path": [97, 98], "path_to_r": 72, "patsi": [69, 70, 96], "pattern": 88, "paul": 143, "pd": [4, 5, 6, 7, 8, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 58, 59, 60, 61, 62, 63, 66, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 96, 98], "pdf": [68, 82], "pedregosa": [140, 142], "pedregosa11a": [140, 142], "pedro": [53, 143], "penal": [57, 61, 89], "penalti": [55, 56, 61, 67, 80, 86, 88, 89, 97, 98], "pennsylvania": [8, 92, 94, 142], "pension": [55, 80, 81, 145], "peopl": [55, 80, 81], "pep8": 144, "per": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 60, 61, 62, 63, 79], "percent": 97, "percentag": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39], "perf_count": 77, "perfectli": [85, 98], "perform": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 45, 52, 54, 56, 58, 60, 61, 62, 63, 68, 72, 74, 75, 77, 78, 79, 81, 87, 88, 89, 91, 97, 98, 100, 101, 103, 112, 113, 128, 140, 142, 143, 145], "perfrom": 75, "perhap": 145, "period": [20, 22, 24, 25, 53, 58, 59, 62, 93, 94, 99, 100, 101, 102, 103, 104, 115, 117, 143, 144], "perp": [98, 111], "perrot": [140, 142], "person": 145, "pessimist": 88, "peter": 143, "petra": 144, "petronelaj": 144, "pfister": [56, 97, 140, 142], "phi": [54, 79, 96, 128], "philipp": [88, 140, 143], "philippbach": [140, 144], "pi": [13, 15, 18, 37, 96, 98, 113, 126, 127], "pi_": [16, 54, 79], "pi_0": [113, 126, 127], "pi_i": [57, 89, 98], "pick": [85, 145], "pip": [85, 98], "pip3": 141, "pipe": 56, "pipe_forest_classif": 56, "pipe_forest_regr": 56, "pipe_lasso": 56, "pipelin": [42, 43, 46, 47, 56, 80, 144], "pipeop": 56, "pira": [55, 80, 81, 87, 145], "pivot": [72, 79, 143], "plai": [78, 145], "plan": [7, 55, 80, 81, 145], "plausibl": [88, 129], "pleas": [42, 43, 46, 47, 53, 58, 59, 61, 66, 78, 88, 112, 140, 141], "plim": 82, "pliv": [27, 28, 38, 54, 79, 90, 96, 109, 124, 140, 144], "plm": [0, 95, 97, 128, 135, 145], "plot": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 45, 52, 53, 55, 56, 57, 59, 60, 61, 62, 63, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 80, 81, 82, 84, 85, 87, 88, 89, 96, 129, 135], "plot_data": [60, 63], "plot_effect": [21, 24, 60, 61, 62, 63], "plot_tre": [45, 83, 96], "plotli": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 69, 70, 72, 85, 88], "plr": [27, 28, 39, 56, 78, 82, 87, 90, 97, 110, 112, 125, 128, 135, 137, 138, 139, 140, 142, 144, 145], "plr_est": 82, "plr_est1": 82, "plr_est2": 82, "plr_obj": 82, "plr_obj_1": 82, "plr_obj_2": 82, "plr_summari": 80, "plrglmnet": 55, "plrranger": 55, "plrrpart": 55, "plrxgboost8700": 55, "plt": [58, 59, 60, 63, 64, 66, 68, 69, 70, 71, 73, 74, 76, 77, 78, 79, 80, 81, 82, 84, 85, 88, 89], "plt_smpl": [54, 79], "plt_smpls_cluster": [54, 79], "plug": [75, 129, 131, 132, 133, 134, 135, 136, 137], "pm": [40, 54, 79, 128, 129, 135, 139], "pmatrix": [57, 89], "pmlr": [72, 77, 78], "po": [56, 97], "poe": 143, "point": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 53, 54, 73, 74, 79, 88, 96, 98, 145], "pointwis": [44, 71, 73, 74, 84], "poli": [55, 76, 79, 80], "polici": [33, 38, 39, 45, 95, 98, 109, 110, 142, 143, 144], "policy_tre": [33, 83, 96], "policy_tree_2": 83, "policy_tree_obj": 96, "policytre": 83, "polit": 82, "poly_dict": 80, "polynomi": [7, 8, 41, 55, 64, 76, 80, 85], "polynomial_featur": [7, 8, 55, 64], "polynomialfeatur": [76, 79, 80], "poor": 86, "pop": [113, 115], "popul": [88, 98, 101, 103, 113, 117], "popular": [77, 98, 129, 130], "porport": 87, "posit": [18, 55, 60, 62, 63, 82, 88, 145], "posixct": [56, 97], "possibl": [4, 5, 6, 43, 46, 47, 53, 56, 60, 62, 63, 69, 70, 73, 74, 75, 76, 77, 78, 83, 85, 86, 87, 88, 97, 98, 102, 105, 106, 128, 129, 130, 144, 145], "possibli": [86, 129, 130], "post": [15, 18, 24, 98, 100, 102, 104, 128, 143], "postdoubl": 143, "poster": 82, "potenti": [9, 14, 25, 29, 30, 31, 34, 35, 37, 41, 57, 58, 60, 63, 76, 82, 85, 89, 100, 104, 105, 111, 118, 119, 128, 136, 141, 144, 145], "potential_level": 66, "power": [56, 78, 86, 88, 97, 143], "pp": [53, 72, 77, 78], "pq": [34, 35, 36, 81, 123, 144], "pq_0": [81, 84], "pq_1": [81, 84], "pr": [37, 51, 54, 55, 56, 57, 97, 98, 112, 113, 128, 142, 145], "practic": [77, 88, 143], "pre": [22, 24, 25, 53, 57, 58, 60, 61, 62, 63, 89, 97, 98, 100, 101, 102, 103, 113, 115, 117, 144], "precis": [53, 98, 129, 137, 145], "precomput": [43, 47], "pred": [53, 78], "pred_df": 83, "pred_dict": 97, "pred_treat": 83, "predict": [4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 52, 54, 55, 56, 61, 68, 71, 72, 76, 77, 78, 79, 80, 83, 86, 88, 91, 96, 112, 129, 130, 135, 137, 144, 145], "predict_proba": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 39, 40, 42, 46, 78, 86, 97], "predictor": [29, 33, 39, 44, 45, 69, 70, 73, 74, 88, 90], "prefer": [55, 80, 81, 145], "preliminari": [31, 52, 68, 85, 113, 119, 122, 123, 127], "prepar": [53, 54, 79, 144], "preprint": [86, 143], "preprocess": [55, 61, 76, 79, 80, 81, 97], "presenc": [55, 80, 81], "present": [53, 61, 88, 97, 113, 121, 145], "prespecifi": 87, "pretreat": [20, 22, 23, 24, 53, 58], "prettenhof": [140, 142], "preval": 88, "prevent": [112, 144], "previou": [59, 75, 76, 82, 141, 145], "previous": [61, 86, 97, 145], "price": [54, 79], "priliminari": [34, 36], "primari": 66, "principl": [129, 130], "print": [22, 24, 40, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 129, 135, 141, 142, 144, 145], "print_detail": 53, "print_period": [22, 24], "prior": [77, 98, 111], "privat": 144, "prob": 56, "prob_dist": 86, "prob_dist_": 86, "probabilit": 75, "probabl": [14, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 46, 52, 53, 57, 58, 60, 63, 66, 68, 75, 82, 84, 85, 88, 89, 91, 98, 113, 114, 115, 116, 117, 122, 143], "problem": [55, 61, 80, 81, 96, 97], "procedur": [52, 54, 55, 68, 77, 79, 80, 87, 88, 97, 128, 141, 144], "proceed": [15, 143], "process": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 41, 53, 57, 58, 59, 62, 63, 69, 70, 71, 72, 73, 74, 77, 78, 83, 84, 88, 89, 95, 128, 129, 130, 143, 144], "produc": 82, "product": [69, 70, 72, 77, 88, 129, 139], "producton": 54, "program": [13, 55, 80, 81, 143, 145], "progress": 65, "project": [56, 69, 70, 96, 140, 144], "project_z": [69, 70], "prone": 113, "pronounc": 85, "propens": [9, 10, 25, 34, 36, 55, 57, 58, 60, 61, 63, 75, 76, 77, 80, 81, 88, 89, 96, 98, 101, 103, 105, 113, 115, 117, 129, 136], "properli": [78, 145], "properti": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 56, 60, 61, 62, 63, 77, 80, 81, 82, 86, 87, 97, 98, 129, 135, 142, 144], "proport": [87, 129, 130, 138, 139], "propos": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 54, 56, 79, 85, 129, 130, 143, 144], "provid": [4, 5, 6, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 53, 54, 55, 56, 61, 69, 70, 73, 74, 76, 78, 79, 80, 85, 88, 90, 91, 92, 93, 94, 95, 97, 128, 140, 142, 144, 145], "prune": [33, 45], "ps911c": 79, "ps944": 79, "pscore1": 82, "pscore2": 82, "psi": [27, 28, 52, 53, 54, 79, 90, 98, 101, 102, 103, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 139, 142], "psi_": [128, 129, 132, 134, 135, 138, 139], "psi_a": [27, 32, 33, 38, 39, 52, 54, 68, 79, 98, 101, 102, 103, 112, 113, 114, 115, 116, 117, 118, 120, 121, 124, 125, 128], "psi_b": [27, 32, 33, 38, 39, 52, 68, 96, 98, 101, 102, 103, 112, 113, 114, 115, 116, 117, 118, 120, 121, 124, 125], "psi_el": [112, 113], "psi_j": 128, "psi_nu2": [129, 135], "psi_sigma2": [129, 135], "public": [51, 67, 144], "publish": [88, 140, 144], "pull": [55, 144], "purchas": 88, "pure": 88, "purp": [69, 70], "purpos": [52, 68, 75, 87, 88, 113, 115, 117, 129, 130, 142], "pval": 128, "px": [72, 85], "py": [60, 61, 62, 63, 74, 79, 80, 88, 140, 141, 144], "py3": 141, "py_al": 68, "py_did": 58, "py_did_pretest": 59, "py_dml": 68, "py_dml_nosplit": 68, "py_dml_po": 68, "py_dml_po_nosplit": 68, "py_double_ml_apo": 66, "py_double_ml_bas": 68, "py_double_ml_basic_iv": 67, "py_double_ml_c": 69, "py_double_ml_cate_plr": 70, "py_double_ml_cvar": 71, "py_double_ml_firststag": 72, "py_double_ml_g": 73, "py_double_ml_gate_plr": 74, "py_double_ml_gate_sensit": 75, "py_double_ml_irm_vs_apo": 76, "py_double_ml_learn": 77, "py_double_ml_meets_flaml": 78, "py_double_ml_multiway_clust": 79, "py_double_ml_pens": 80, "py_double_ml_pension_qt": 81, "py_double_ml_plm_irm_hetfx": 82, "py_double_ml_policy_tre": 83, "py_double_ml_pq": 84, "py_double_ml_rdflex": 85, "py_double_ml_robust_iv": 86, "py_double_ml_sensit": 87, "py_double_ml_sensitivity_book": 88, "py_double_ml_ssm": 89, "py_non_orthogon": 68, "py_panel": 60, "py_panel_data_exampl": 61, "py_panel_simpl": 62, "py_po_al": 68, "py_rep_c": 63, "pydata": 74, "pypi": [143, 144], "pyplot": [58, 59, 60, 63, 64, 66, 68, 69, 70, 71, 73, 74, 76, 77, 78, 79, 80, 81, 82, 84, 85, 88, 89], "pyproject": 144, "pyreadr": 61, "python": [18, 53, 78, 88, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 103, 112, 113, 128, 129, 130, 135, 140, 142, 143, 144, 145], "python3": [60, 61, 62, 63, 80, 141], "q": [56, 71, 84, 85, 97, 140, 142], "q2": [56, 64, 92, 94, 142], "q3": [56, 64, 92, 94, 142], "q4": [56, 64, 92, 94, 142], "q5": [56, 64, 92, 94, 142], "q6": [56, 64, 92, 94, 142], "q_i": [85, 98], "qquad": 13, "qte": [71, 81, 144], "quad": [25, 26, 55, 57, 58, 80, 83, 85, 89, 96, 98, 100, 104, 111, 113, 122, 128, 129, 131, 132], "quadrat": [57, 89], "qualiti": [87, 90, 144], "quanitl": 81, "quant": 71, "quantifi": [88, 98, 104], "quantil": [14, 24, 30, 31, 34, 35, 36, 60, 63, 66, 71, 76, 87, 95, 97, 119, 122, 123, 143, 144], "quantiti": [51, 67, 88], "queri": 80, "question": [88, 145], "quick": 81, "quit": [77, 83, 87, 129, 130], "r": [12, 32, 42, 43, 46, 47, 59, 60, 61, 62, 63, 68, 69, 70, 72, 79, 82, 85, 86, 88, 90, 91, 92, 94, 95, 98, 107, 112, 113, 120, 124, 128, 129, 130, 136, 137, 138, 139, 140, 142, 143, 144, 145], "r2_d": [13, 77], "r2_score": [43, 47], "r2_y": [13, 77], "r6": [56, 144], "r_0": [32, 38, 55, 80, 98, 107], "r_all": 52, "r_d": 13, "r_df": 79, "r_dml": 52, "r_dml_nosplit": 52, "r_dml_po": 52, "r_dml_po_nosplit": 52, "r_double_ml_bas": 52, "r_double_ml_basic_iv": 51, "r_double_ml_did": 53, "r_double_ml_multiway_clust": 54, "r_double_ml_pens": 55, "r_double_ml_pipelin": 56, "r_double_ml_ssm": 57, "r_hat": 38, "r_hat0": 32, "r_hat1": 32, "r_non_orthogon": 52, "r_po_al": 52, "r_y": 13, "rais": [4, 5, 6, 42, 43, 46, 47, 60, 61, 62, 63, 97], "randint": 82, "randn": 37, "random": [11, 14, 18, 19, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 51, 52, 53, 55, 56, 58, 59, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 96, 97, 99, 100, 101, 103, 104, 112, 126, 128, 129, 135, 139, 142, 143, 145], "random_search": 97, "random_st": [14, 60, 63, 68, 75, 76, 83], "randomforest": [55, 77, 80], "randomforest_class": [55, 69, 80, 83], "randomforest_reg": [69, 83], "randomforestclassifi": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 64, 66, 69, 70, 73, 74, 75, 77, 80, 83, 85, 87, 88, 96, 97, 98, 99, 100, 101, 103, 145], "randomforestregressor": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 64, 66, 68, 69, 70, 73, 74, 75, 77, 80, 83, 85, 87, 88, 90, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 129, 135, 142, 145], "randomized_search": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 97], "randomizedsearchcv": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 97], "randomli": [52, 54, 68, 79, 91, 112, 145], "rang": [52, 58, 59, 66, 68, 71, 73, 74, 77, 78, 79, 81, 83, 84, 85, 86, 88, 89, 91, 97, 98], "rangeindex": [58, 60, 61, 62, 63, 64, 66, 75, 79, 80, 81, 87, 89, 92, 93, 94, 142], "ranger": [53, 55, 56, 90, 97, 98, 112, 113, 128, 142, 145], "rangl": [11, 83], "rank": 144, "rate": [72, 77, 98], "rather": [85, 88, 98], "ratio": [97, 112, 129, 130], "rational": 61, "ravel": [69, 70], "raw": [55, 61, 62, 72, 80], "raw_res_manual_lasso_r_100_n_100_p_200_rho_0": 72, "rbind": 55, "rbindlist": 55, "rbinom": 51, "rbrace": [12, 13, 19, 32, 33, 54, 79, 90, 98, 105, 107, 108, 112, 113, 118, 128, 129, 136], "rcolorbrew": 54, "rcparam": [59, 64, 69, 70, 71, 73, 74, 76, 79, 80, 81, 84], "rd": [98, 144], "rda": 61, "rdbu": 54, "rdbu_r": 79, "rdbwselect": 98, "rdd": [0, 4, 5, 6, 95, 141], "rdflex": [85, 98, 144], "rdflex_fuzzi": 85, "rdflex_fuzzy_stack": 85, "rdflex_obj": [40, 98], "rdflex_sharp": 85, "rdflex_sharp_stack": 85, "rdrobust": [40, 85, 98, 141, 144], "rdrobust_fuzzi": 85, "rdrobust_fuzzy_noadj": 85, "rdrobust_sharp": 85, "rdrobust_sharp_noadj": 85, "rdt044": 72, "re": [60, 79, 88, 141], "read": [61, 141], "read_csv": [62, 72], "read_r": 61, "readabl": 144, "reader": [61, 86], "readili": 140, "real": [55, 80, 81, 87, 129, 130], "realat": 98, "realiz": [85, 98, 111], "reason": [4, 5, 6, 51, 67, 72, 77, 78, 87, 88, 129, 130, 145], "recal": [64, 129, 139], "receiv": [25, 60, 63, 66, 85, 98, 100, 102], "recent": [78, 98, 100, 104, 143], "recogn": [55, 80, 81], "recommend": [56, 60, 63, 77, 85, 88, 90, 98, 112, 129, 141, 143, 144], "recov": [51, 53, 67, 82], "recsi": 143, "red": [54, 57, 73, 74, 78, 79], "reduc": [55, 75, 78, 80, 85, 87, 88, 98, 144], "redund": 144, "reemploy": [8, 92, 94, 142], "ref": 61, "refactor": 144, "refer": [7, 8, 9, 10, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 59, 60, 63, 66, 75, 80, 81, 85, 87, 92, 94, 95, 96, 98, 99, 101, 103, 104, 113, 129, 130, 135, 143, 144], "reference_level": [30, 66, 76, 98], "refin": 144, "refit": [129, 130], "reflect": [83, 88, 96], "reg": [25, 26, 55, 80, 145], "reg_estim": 85, "reg_learn": 81, "reg_learner_1": 77, "reg_learner_2": 77, "regard": [88, 140], "regener": 144, "region": [54, 71, 79, 128, 143], "regr": [51, 52, 53, 54, 55, 56, 57, 90, 97, 98, 112, 113, 115, 128, 142, 145], "regravg": [56, 97], "regress": [9, 10, 12, 13, 14, 15, 16, 17, 18, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 44, 51, 53, 54, 56, 57, 60, 61, 62, 63, 66, 67, 72, 78, 79, 82, 86, 87, 88, 89, 90, 91, 95, 96, 97, 100, 101, 103, 107, 108, 109, 110, 112, 117, 128, 130, 135, 136, 137, 138, 139, 140, 142, 143, 144, 145], "regressor": [43, 47, 52, 55, 66, 68, 71, 77, 78, 80, 91], "regular": [15, 95, 97, 113, 128, 143], "reich": [56, 97], "reinforc": 143, "reject": [55, 80], "rel": [55, 61, 80, 86, 98, 99, 129, 130, 136, 137], "relat": [76, 88, 145], "relationship": [51, 67, 72, 88, 128], "relev": [4, 5, 6, 11, 20, 22, 23, 24, 42, 43, 44, 46, 47, 60, 63, 71, 83, 84, 98, 129, 145], "reli": [24, 58, 59, 60, 63, 69, 70, 75, 76, 96, 97, 98, 100, 113, 117, 129, 130, 145], "reload": 55, "remain": [53, 128, 145], "remark": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 52, 58, 59, 60, 61, 62, 63, 66, 68, 69, 70, 71, 73, 74, 75, 76, 77, 81, 87, 96, 97, 98, 99, 100, 101, 103, 112, 113, 114, 115, 116, 117, 122, 123, 128, 129, 132, 134, 137], "remot": 141, "remov": [55, 76, 88, 95, 98, 112, 113, 129, 144], "renam": [60, 63, 80, 144], "render": [87, 88], "reorgan": 144, "rep": [52, 57, 91, 97, 128], "repeat": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 54, 55, 56, 57, 60, 61, 62, 63, 68, 75, 79, 80, 81, 82, 85, 87, 89, 91, 95, 97, 101, 102, 103, 104, 128, 131, 132, 142, 144, 145], "repeatedkfold": 79, "repet": 87, "repetit": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 61, 69, 70, 72, 73, 74, 75, 77, 95, 97, 128, 142, 144, 145], "replac": [83, 88, 144], "replic": [7, 8, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 55, 61, 62, 68, 72, 86, 88], "repo": 144, "report": [55, 78, 80, 140, 144], "repositori": [60, 63, 72, 85, 144], "repr": [52, 54], "repres": [25, 82, 88, 98], "represent": [10, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 87, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 142, 144], "reproduc": 14, "request": [42, 43, 46, 47, 144], "requir": [38, 39, 42, 46, 51, 55, 56, 60, 61, 62, 63, 66, 75, 80, 81, 87, 98, 99, 101, 102, 103, 113, 115, 117, 128, 129, 130, 135, 141, 144, 145], "requirenamespac": 53, "rerun": 61, "res_df": 79, "res_dict": [9, 10, 11, 14, 41], "resampl": [51, 54, 56, 57, 58, 60, 61, 62, 63, 79, 81, 87, 89, 97, 98, 100, 101, 103, 112, 113, 128, 140, 142, 145], "resdat": 63, "research": [54, 56, 79, 82, 88, 112, 140, 142, 143, 145], "resembl": [57, 89], "reset": 53, "reset_index": [60, 63, 72, 79, 80], "reshap": [59, 68, 69, 70, 76], "reshape2": 54, "residu": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 87, 129, 130, 138, 139], "resolut": [56, 97], "resourc": 77, "resourcewis": 77, "respect": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 60, 61, 62, 63, 66, 80, 81, 85, 96, 98, 102, 107, 111, 112, 129, 139, 145], "respons": [7, 56, 97], "rest": 98, "restart": 141, "restrict": 77, "restructur": 144, "restud": 72, "result": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 51, 52, 53, 56, 57, 58, 59, 60, 61, 62, 63, 66, 68, 69, 70, 72, 75, 76, 77, 83, 85, 86, 87, 88, 89, 91, 97, 112, 113, 114, 115, 116, 117, 129, 130, 135, 142, 144], "result_iivm": 55, "result_irm": 55, "result_plr": 55, "results_df": 86, "retain": [42, 43, 46, 47], "retina": 82, "retir": [55, 80, 81, 87], "return": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 56, 57, 60, 61, 62, 63, 68, 71, 74, 77, 78, 79, 82, 83, 84, 86, 87, 88, 89, 90, 97, 113, 129, 130, 144], "return_count": [66, 77], "return_tune_r": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "return_typ": [4, 5, 6, 7, 8, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 55, 56, 57, 58, 68, 76, 77, 78, 80, 81, 87, 89, 90, 91, 92, 94, 96, 97, 98, 100, 112, 113, 128, 129, 135, 142, 145], "rev": 54, "reveal": 75, "review": [15, 72, 143], "revist": [54, 79], "reweight": [98, 99], "rf": 85, "rho": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 60, 63, 66, 75, 76, 85, 87, 88, 129, 130, 135, 139, 145], "rho_val": 88, "richter": [56, 97, 140, 142], "riesz": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 87, 129, 130, 131, 132, 133, 134, 135, 138, 139], "riesz_rep": [129, 135], "right": [12, 13, 15, 16, 19, 25, 52, 54, 68, 77, 79, 80, 81, 82, 84, 85, 88, 91, 98, 113, 114, 115, 116, 117, 128, 129, 131, 132, 133, 134, 136, 137], "rightarrow_": [52, 68, 91], "risk": [31, 95, 144], "ritov": 143, "rival": 79, "rival_ind": 79, "rmd": 53, "rmse": [53, 58, 60, 61, 62, 63, 77, 78, 81, 87, 89, 97, 98, 100, 101, 103, 113, 128, 142, 144], "rmse_dml_ml_l_fullsampl": 78, "rmse_dml_ml_l_lesstim": 78, "rmse_dml_ml_l_onfold": 78, "rmse_dml_ml_l_untun": 78, "rmse_dml_ml_m_fullsampl": 78, "rmse_dml_ml_m_lesstim": 78, "rmse_dml_ml_m_onfold": 78, "rmse_dml_ml_m_untun": 78, "rmse_oos_ml_l": 78, "rmse_oos_ml_m": 78, "rmse_oos_onfolds_ml_l": 78, "rmse_oos_onfolds_ml_m": 78, "rnorm": [51, 56, 92, 94, 97, 128, 142], "robin": [7, 8, 17, 54, 72, 79, 91, 140, 143], "robinson": [52, 68, 91], "robject": 79, "robu": [73, 74], "robust": [9, 10, 16, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 53, 60, 61, 63, 66, 75, 76, 85, 87, 88, 98, 129, 135, 143, 144, 145], "robust_confset": [32, 86, 144], "robust_cov": 86, "robust_length": 86, "roc\u00edo": 143, "role": [4, 5, 6, 52, 68, 78, 91, 145], "romano": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 128], "root": [20, 22, 23, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 72, 91, 97, 113, 143], "rotat": [78, 85], "roth": [85, 98, 100, 104, 143], "rough": [88, 145], "roughli": [60, 63, 88], "round": [55, 61, 66, 76, 77, 82, 88], "rout": [42, 43, 46, 47], "row": [21, 52, 55, 59, 61, 64, 69, 70, 78, 79, 83, 92, 94, 98, 101, 103, 112, 142, 145], "row_index": 74, "rownam": 54, "rowv": 54, "roxygen2": 144, "royal": [88, 143], "rpart": [55, 56, 97], "rpart_cv": 56, "rprocess": 77, "rpy2": 79, "rpy2pi": 79, "rskf": 76, "rsmp": [56, 97, 112], "rsmp_tune": [56, 97], "rssb": 88, "rtype": 30, "ruben": 143, "ruiz": [51, 67], "rule": [53, 96], "run": [53, 61, 85, 98, 141, 144], "runif": 51, "runner": [60, 62, 63, 88], "runtime_learn": 56, "runtimewarn": 63, "rv": [60, 63, 66, 75, 76, 87, 88, 129, 135, 145], "rva": [60, 63, 66, 75, 76, 87, 88, 129, 135, 145], "rvert": 72, "rvert_": 72, "s1": 78, "s2": 78, "s_": [16, 54, 79, 98, 111], "s_1": 17, "s_2": 17, "s_col": [4, 5, 57, 85, 89, 98], "s_i": [19, 57, 85, 89, 98], "s_x": [16, 54, 79], "safeguard": [58, 97], "sake": [55, 80, 88, 145], "same": [14, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 44, 52, 54, 57, 60, 61, 63, 68, 69, 70, 75, 76, 77, 79, 81, 83, 85, 86, 87, 88, 89, 97, 98, 101, 103, 113, 116, 117, 128, 129, 137, 144], "samii": 82, "sampl": [9, 10, 16, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 51, 53, 54, 56, 58, 60, 61, 62, 63, 67, 73, 74, 76, 77, 79, 81, 83, 86, 87, 95, 97, 100, 101, 103, 104, 111, 114, 128, 129, 132, 134, 142, 143, 144], "sample_weight": [40, 42, 43, 46, 47, 85], "sant": [9, 10, 14, 20, 22, 23, 25, 26, 53, 58, 60, 61, 62, 63, 98, 99, 100, 102, 104, 143], "sara": 143, "sasaki": [16, 54, 79, 143], "satisfi": [57, 61, 89, 97, 113, 128], "save": [52, 55, 61, 68, 73, 74, 77, 78, 80, 81, 97, 129, 135, 145], "savefig": 68, "saveguard": 77, "saver": [55, 80, 81], "sc": [60, 63], "scalar": 98, "scale": [25, 52, 54, 59, 71, 76, 82, 84, 88, 128, 129, 132, 134, 139], "scale_color_manu": 52, "scale_fill_manu": [52, 54], "scaled_psi": 76, "scatter": [59, 66, 73, 74, 82, 85, 88], "scatterplot": 66, "scenario": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 63, 66, 75, 76, 87, 88, 98, 129, 135, 145], "scene": [69, 70, 72], "scene_camera": 72, "schacht": [72, 77, 78], "schaefer": 82, "schedul": 144, "scheme": [54, 79, 97, 98, 99, 112, 140], "schneider": 56, "schratz": [56, 97, 140, 142], "scienc": [18, 51, 67, 82, 143], "scikit": [61, 77, 80, 97, 140, 142, 144, 145], "scipi": 68, "score": [0, 4, 5, 6, 9, 10, 20, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 84, 85, 86, 87, 88, 89, 90, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 134, 135, 137, 138, 139, 140, 144, 145], "scoring_method": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "script": 141, "sd": 51, "se": [52, 54, 68, 87, 91, 97, 112, 128, 129, 135, 143, 145], "se_df": 54, "se_dml": [52, 68, 91], "se_dml_po": [52, 68, 91], "se_nonorth": [52, 68], "se_orth_nosplit": [52, 68], "se_orth_po_nosplit": [52, 68], "seaborn": [21, 24, 58, 60, 63, 64, 66, 68, 77, 79, 80, 81, 88, 89], "search": [20, 22, 23, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 97, 113], "search_mod": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 97], "searchabl": 55, "second": [16, 20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 56, 68, 77, 78, 79, 90, 91, 112, 128, 129, 130, 139, 142], "secondari": 66, "section": [23, 24, 26, 53, 54, 55, 56, 59, 75, 78, 79, 81, 88, 99, 101, 102, 103, 104, 121, 131, 132, 144], "secur": 82, "see": [7, 8, 13, 19, 20, 22, 23, 27, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 46, 47, 51, 53, 54, 55, 56, 58, 60, 61, 62, 63, 66, 67, 69, 70, 74, 76, 78, 79, 81, 82, 83, 85, 86, 87, 88, 97, 98, 100, 102, 104, 112, 113, 119, 121, 122, 123, 126, 127, 129, 130, 132, 134, 135, 139, 141, 142, 144], "seed": [14, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55, 56, 57, 58, 59, 64, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 129, 135, 142, 145], "seek": 82, "seem": [53, 55, 75, 80, 81, 129, 145], "seen": [73, 74, 76], "sel_cols_chiang": 79, "select": [4, 5, 6, 14, 15, 19, 25, 37, 51, 72, 77, 85, 88, 90, 92, 94, 95, 97, 111, 128, 142, 143, 144, 145], "selected_coef": 77, "selected_featur": [56, 97], "selected_learn": 77, "self": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 77, 78, 86, 145], "selfref": 55, "semenova": [69, 70, 143], "semi": 91, "semiparametr": 7, "sens": [87, 88], "sensemakr": [129, 130], "sensit": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 95, 96, 98, 103, 130, 135, 139, 144], "sensitivity_analysi": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 63, 66, 75, 76, 87, 88, 129, 135, 145], "sensitivity_benchmark": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 66, 75, 87, 88, 129, 130], "sensitivity_el": [129, 135], "sensitivity_param": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 87, 88, 129, 130, 135], "sensitivity_plot": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 66, 75, 87, 88, 129, 135], "sensitivity_summari": [60, 63, 66, 75, 76, 87, 88, 129, 135, 145], "sensitv": 76, "sensitvity_benchmark": 66, "sensiv": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39], "senstiv": [129, 138], "sep": 52, "separ": [21, 82, 87, 97, 98, 112, 144], "seper": [78, 85, 87, 128, 129, 130], "seq_len": [52, 57, 91], "sequenti": 8, "ser": [60, 61, 62, 63], "seri": [60, 61, 62, 63, 74, 88, 143], "serv": [25, 92, 93, 94, 142, 144], "serverless": [143, 144], "servic": 82, "set": [4, 5, 6, 7, 8, 9, 11, 16, 17, 18, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 45, 46, 47, 51, 52, 53, 54, 55, 57, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 85, 86, 87, 88, 89, 90, 91, 92, 94, 96, 98, 99, 101, 102, 104, 112, 113, 114, 115, 116, 117, 118, 121, 128, 129, 130, 136, 137, 138, 141, 142, 144, 145], "set_as_param": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "set_config": [42, 43, 46, 47], "set_fit_request": [46, 47], "set_fold_specif": 97, "set_index": 80, "set_ml_nuisance_param": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 55, 64, 80, 97, 144], "set_param": [42, 43, 46, 47, 78, 97], "set_sample_split": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 76, 77, 112, 144], "set_score_request": [42, 43, 46, 47], "set_styl": [80, 81], "set_text": 77, "set_threshold": [52, 53, 54, 55, 56, 57, 90, 97, 98, 112, 113, 128, 142], "set_tick": 79, "set_ticklabel": 79, "set_titl": [66, 76, 78, 79, 85], "set_x_d": [4, 5, 6], "set_xlabel": [66, 68, 76, 78, 79, 85], "set_xlim": 68, "set_xtick": 82, "set_xticklabel": 82, "set_ylabel": [66, 76, 78, 79, 82, 85], "set_ylim": [71, 76, 78, 79, 84], "setdiff": 144, "setdiff1d": 79, "setminu": [54, 79, 128], "settings_l": 78, "settings_m": 78, "setup": [141, 144], "seven": [54, 79], "sever": [48, 55, 56, 60, 61, 62, 63, 77, 78, 80, 81, 87, 88, 91, 97, 145], "shape": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 45, 46, 47, 59, 60, 63, 66, 69, 70, 73, 74, 77, 79, 80, 83, 85, 87, 88, 97, 98], "share": [54, 55, 79, 80], "sharma": [88, 143], "sharp": 40, "shift": [60, 63], "shock": [54, 79], "short": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 87, 88, 98, 102, 129, 130, 143, 144, 145], "shortcut": 55, "shortli": [54, 56, 79, 97], "shota": 143, "should": [4, 5, 6, 20, 21, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 55, 57, 60, 61, 63, 66, 73, 74, 77, 80, 85, 86, 87, 89, 92, 94, 96, 97, 98, 99, 106, 128, 129, 130, 140], "show": [51, 52, 54, 57, 58, 60, 63, 64, 66, 67, 68, 69, 70, 72, 75, 76, 77, 78, 79, 82, 85, 86, 88, 89, 91, 129, 138, 141], "showcas": 83, "showlabel": 88, "showlegend": 88, "shown": [51, 67, 82, 142], "showscal": [69, 70, 72], "shrink": 85, "shuffl": 112, "side": [85, 98, 129, 135], "sigma": [9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 37, 52, 54, 57, 68, 79, 89, 91, 96, 112, 128, 129, 130, 132, 134, 135, 138, 139], "sigma2": [129, 135], "sigma_": [10, 12, 13, 15, 16, 17, 19, 25, 26, 52, 54, 68, 79, 91], "sigma_0": [129, 139], "sigma_j": 128, "sigmoid": 82, "sign": [86, 88], "signal": [44, 45], "signatur": [32, 33, 34, 35, 36, 38, 39, 113], "signif": [51, 53, 54, 55, 56, 57, 97, 98, 112, 113, 128, 142, 145], "signific": [51, 54, 55, 56, 57, 60, 63, 66, 75, 76, 80, 83, 85, 87, 88, 97, 98, 112, 113, 128, 129, 135, 142, 145], "silverman": [34, 35, 36], "sim": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 41, 52, 53, 54, 57, 59, 68, 71, 79, 83, 84, 89, 91, 98], "sim_data": 62, "similar": [10, 14, 53, 56, 61, 69, 70, 75, 78, 81, 85, 86, 87, 88, 98, 113, 114, 115], "similarli": [61, 78, 86], "simpl": [11, 20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 46, 47, 53, 56, 69, 70, 73, 74, 75, 76, 83, 88, 95, 98, 129, 130], "simplest": 96, "simpli": [56, 58, 145], "simplic": [55, 77, 80, 83, 88], "simplif": [129, 131], "simplifi": [60, 63, 76, 82, 88, 96, 129, 138], "simul": [9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 25, 26, 52, 56, 57, 60, 62, 63, 68, 69, 70, 71, 72, 73, 74, 77, 78, 84, 85, 88, 89, 91, 97, 128, 142], "simul_data": 37, "simulaten": [98, 106], "simulation_run": 72, "simult": 53, "simultan": [61, 95, 145], "sin": [11, 14, 18, 59, 69, 70, 73, 74], "sinc": [9, 10, 42, 46, 55, 57, 58, 59, 66, 73, 74, 75, 77, 78, 80, 82, 89, 97, 98, 100, 129, 135, 137, 141, 144], "singl": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 58, 60, 61, 62, 63, 73, 74, 81, 82, 97, 128], "single_learner_pipelin": 97, "singleton": 112, "sinh": 18, "sipp": [55, 80, 81], "site": [60, 61, 62, 63, 79, 80], "situat": [54, 79], "six": [25, 54], "sixth": 79, "size": [21, 24, 37, 52, 54, 55, 56, 59, 60, 61, 62, 63, 68, 71, 72, 75, 77, 78, 80, 82, 83, 84, 86, 88, 90, 92, 94, 97, 98, 99, 112, 113, 128, 129, 132, 134, 142, 145], "sizeabl": 88, "skill": 143, "sklearn": [18, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 45, 46, 47, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 85, 86, 87, 88, 89, 90, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 129, 135, 142, 145], "skotara": 88, "slide": 82, "slight": [98, 101, 103], "slightli": [20, 22, 23, 59, 60, 63, 73, 74, 75, 77, 96, 98, 102, 104, 113, 114, 115, 116, 117, 129, 130], "slow": [52, 68, 91], "slower": [52, 68, 91], "small": [11, 57, 58, 59, 76, 83, 89, 98, 129, 130, 137], "smaller": [55, 58, 73, 74, 75, 78, 80, 85, 88, 98, 145], "smallest": [22, 77], "smpl": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 68, 77, 79, 112, 113], "smpls_cluster": [54, 79], "smucler": [86, 144], "sn": [58, 60, 63, 64, 66, 68, 77, 79, 80, 81, 88, 89], "so": [46, 47, 51, 55, 56, 57, 58, 60, 61, 63, 67, 78, 80, 82, 88, 89, 97, 128, 145], "social": [82, 143], "societi": [54, 79, 88, 143], "softwar": [56, 97, 140, 142, 143, 144], "solari": 144, "sole": [61, 88], "solut": [90, 96, 113], "solv": [27, 54, 79, 96, 97, 98, 101, 102, 103, 128], "solver": [80, 89, 98], "some": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 55, 56, 57, 58, 59, 61, 64, 77, 78, 80, 81, 85, 86, 87, 89, 96, 97, 98, 107, 141, 144], "sometim": [77, 98, 104], "sonabend": [56, 97], "sophist": 97, "sort": [21, 80, 86, 98], "sort_bi": 21, "sort_valu": 66, "sourc": [56, 97, 142, 144], "sourcefileload": 72, "sp": 53, "space": [54, 79, 97], "spars": [72, 97, 128, 142, 143], "sparsiti": 143, "spec": 143, "special": [54, 79, 95, 98], "specif": [20, 22, 23, 25, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 40, 54, 55, 60, 61, 62, 63, 66, 76, 77, 79, 80, 88, 92, 94, 95, 96, 97, 98, 101, 103, 112, 113, 121, 128, 135, 139, 140, 142], "specifi": [4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 54, 55, 56, 57, 58, 60, 61, 62, 63, 66, 67, 69, 70, 71, 73, 74, 76, 78, 79, 80, 81, 83, 84, 85, 87, 88, 89, 90, 92, 93, 94, 95, 96, 98, 118, 121, 141, 142, 144, 145], "specifii": 81, "speed": [24, 30, 36, 77], "speedup": 77, "spefici": 32, "spindler": [15, 72, 77, 78, 86, 88, 140, 143, 144], "spine": [80, 81], "spline": [69, 70, 96], "spline_basi": [69, 70, 96], "spline_grid": [69, 70], "split": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 54, 56, 57, 58, 60, 61, 62, 63, 76, 77, 79, 81, 83, 87, 89, 95, 96, 97, 98, 100, 101, 103, 113, 128, 142, 144], "split_sampl": [76, 77], "sponsor": [55, 80, 81], "sprintf": 52, "sq_error": 72, "sqrt": [9, 10, 13, 14, 25, 26, 52, 54, 56, 64, 68, 71, 79, 84, 91, 112, 128, 129, 130, 142], "squar": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 55, 72, 80, 97, 98, 129, 139, 143], "squarederror": [55, 80, 145], "squeez": [58, 71, 84, 89], "src": 80, "ssm": [4, 5, 6, 19, 95, 111], "ssrn": 12, "stabil": 75, "stabl": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 98, 99, 140], "stack": [56, 97], "stackingclassifi": 85, "stackingregressor": 85, "stacklrn": 56, "stackrel": 98, "stage": [40, 61, 69, 70, 73, 74, 83, 85, 97, 98, 144, 145], "stagger": [98, 104], "stai": [98, 104], "standard": [24, 26, 53, 56, 60, 61, 62, 63, 71, 73, 74, 85, 86, 98, 99, 101, 102, 103, 112, 113, 128, 129, 135, 139, 144, 145], "standard_norm": [92, 94, 97, 128, 142], "standardscal": 80, "star": 98, "start": [25, 53, 55, 56, 60, 61, 62, 63, 69, 70, 72, 75, 77, 78, 79, 80, 84, 88, 98, 100, 140, 145], "start_dat": 25, "stat": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 68, 85, 92, 94, 97, 98, 128, 140, 143], "stat_bin": 52, "stat_dens": 55, "state": 145, "stationar": 58, "stationari": [98, 100], "statist": [16, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 54, 79, 86, 87, 88, 128, 129, 135, 140, 142, 143, 144, 145], "statsmodel": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 85], "statu": [53, 55, 57, 58, 80, 82, 85, 89, 98, 104], "std": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 73, 74, 75, 76, 78, 79, 80, 81, 82, 84, 87, 88, 89, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 142, 145], "stefan": 143, "step": [52, 55, 56, 68, 73, 74, 75, 80, 83, 91, 97, 98, 128, 140, 145], "stepdown": 128, "stick": [55, 80], "still": [57, 58, 69, 70, 73, 74, 75, 81, 85, 87, 89, 97, 98, 102], "stochast": [38, 39, 98, 109, 110, 142], "stock": [55, 80, 81, 86], "store": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 86, 90, 97, 112, 113, 128, 129, 135, 144], "store_model": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 78], "store_predict": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 80, 83], "stori": [88, 143], "str": [4, 5, 6, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 55, 60, 63, 66, 73, 74, 84, 85, 96, 98, 144], "straightforward": [60, 63, 73, 74, 77, 96], "strategi": [82, 88, 98, 145], "stratifi": [76, 77], "stratum": 82, "strength": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 86, 87, 88, 129, 130, 135, 138], "strftime": 60, "strictli": 98, "string": [20, 21, 22, 23, 24, 29, 31, 32, 33, 34, 35, 37, 38, 39, 96, 128, 129, 135, 142, 144], "string_label": 82, "strong": [57, 86, 89, 129, 130], "stronger": [86, 98, 102, 128, 145], "structur": [7, 8, 17, 24, 25, 54, 55, 57, 61, 72, 79, 80, 86, 89, 91, 97, 140, 143, 145], "student": 143, "studi": [19, 54, 55, 72, 77, 78, 79, 80, 81, 86, 87, 98, 99, 142, 145], "style": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 63, 78, 144], "styler": 144, "styliz": 88, "sub": [42, 43, 46, 47, 54, 79], "subclass": [93, 94, 144], "subfold": 97, "subgroup": [32, 55, 80, 144], "subject": [54, 79], "submiss": 144, "submit": [60, 63], "submodul": 144, "subobject": [42, 43, 46, 47], "subplot": [54, 59, 66, 68, 69, 70, 71, 73, 74, 76, 77, 78, 79, 80, 81, 82, 84, 85], "subplots_adjust": 77, "subpopul": [98, 111], "subsampl": [56, 77, 113, 115], "subscript": [98, 102, 113, 115, 117, 129, 130], "subsequ": [54, 79], "subset": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 46, 54, 77, 79, 83, 90, 96, 97, 129, 130, 132, 134], "subseteq": 96, "substanti": [55, 80, 82], "substract": 128, "subtract": 128, "sudo": 141, "suffic": 88, "suffici": [77, 78, 88], "suggest": [54, 55, 79, 80, 88, 144], "suitabl": [57, 69, 70, 89, 98, 102], "sum": [43, 47, 54, 55, 79, 80, 81, 84, 85, 96, 128], "sum_": [25, 41, 52, 54, 68, 79, 85, 90, 91, 96, 98, 99, 104, 128], "sum_i": 82, "sum_oth": 79, "sum_riv": 79, "summar": [21, 53, 60, 61, 62, 63, 66, 82, 88, 90, 129, 135], "summari": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 53, 54, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 73, 74, 75, 76, 79, 81, 84, 86, 87, 88, 89, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 112, 113, 128, 129, 142, 144, 145], "summary_df": 86, "summary_result": 55, "suppli": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 69, 70, 73, 74, 75, 83, 96, 129, 130, 135, 136], "support": [11, 32, 40, 53, 54, 60, 61, 62, 63, 77, 79, 83, 85, 94, 97, 98, 107, 145], "support_s": [11, 69, 70, 73, 74, 83], "support_t": 83, "support_w": 83, "suppos": 88, "suppress": [53, 55, 56, 57], "suppresswarn": 52, "suprema": 128, "suptitl": [71, 77, 78, 81, 84], "supxlabel": [71, 81, 84], "supylabel": [71, 81, 84], "sure": [66, 97, 144], "surfac": [69, 70, 72], "surgic": 86, "surpress": [54, 142], "survei": [55, 80, 81, 145], "susan": 143, "sven": [88, 140, 143], "svenk": 79, "svenklaassen": [140, 144], "svg": [52, 68], "switch": [52, 68, 88, 91], "symbol": 88, "symmetr": 18, "syntax": [85, 98], "synthesi": 143, "synthet": [11, 25, 41, 51, 67, 69, 70, 71, 73, 74, 78, 83, 84, 86], "syrgkani": [86, 88, 143], "system": 143, "szita": 143, "t": [4, 5, 6, 9, 10, 14, 20, 21, 23, 24, 25, 26, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 112, 113, 114, 115, 128, 129, 131, 132, 142, 145], "t_": [24, 60, 61, 62, 63, 98, 101, 102, 103, 113, 115, 117], "t_1_start": 77, "t_1_stop": 77, "t_2_start": 77, "t_2_stop": 77, "t_3_start": 77, "t_3_stop": 77, "t_col": [4, 5, 6, 23, 24, 60, 61, 62, 63, 93, 94, 98, 99, 100, 101, 103], "t_df": 83, "t_diff": 59, "t_dml": 52, "t_g": 25, "t_i": [58, 83, 85, 98, 100, 101, 104, 113, 115], "t_idx": 59, "t_nonorth": 52, "t_orth_nosplit": 52, "t_sigmoid": 83, "t_stat": 128, "t_value_ev": 22, "t_value_pr": 22, "tabl": [52, 54, 55, 56, 57, 66, 86, 90, 92, 94, 97, 98, 112, 113, 128, 142, 145], "tabular": [77, 92, 94, 128, 142, 145], "taddi": 143, "takatsu": 86, "take": [9, 10, 11, 32, 33, 38, 39, 57, 58, 59, 60, 61, 62, 63, 69, 70, 71, 72, 73, 74, 77, 81, 84, 85, 86, 87, 89, 90, 96, 97, 98, 99, 104, 107, 108, 109, 110, 113, 118, 121, 129, 136, 137, 138, 142], "taken": [55, 80, 81, 145], "taker": [32, 144], "talk": 145, "target": [20, 22, 23, 27, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 46, 47, 51, 54, 55, 56, 57, 69, 70, 77, 79, 96, 97, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 111, 112, 113, 122, 123, 128, 129, 137, 139, 140, 142, 144, 145], "task": [51, 78, 92, 94, 112, 145], "task_typ": 144, "tau": [41, 59, 71, 81, 82, 84, 85, 96, 98, 113, 119, 122, 123], "tau_": [82, 85, 98], "tau_0": [85, 98], "tau_1": 82, "tau_2": 82, "tau_vec": [71, 81, 84], "tax": [55, 80, 81], "te": [53, 69, 70, 83], "techniqu": [52, 68, 91, 112, 145], "teen": 61, "templat": 144, "ten": 78, "tend": [55, 80, 81, 98], "tensor": [69, 70], "tenth": 143, "term": [6, 22, 52, 54, 55, 56, 59, 68, 72, 79, 80, 82, 88, 91, 98, 104, 140, 145], "termin": [56, 97], "terminatorev": 56, "test": [8, 12, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 60, 61, 62, 63, 68, 75, 79, 86, 88, 91, 97, 98, 112, 113, 128, 142, 143, 144, 145], "test_id": [54, 112], "test_ind": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "test_set": 112, "test_siz": 68, "text": [9, 10, 12, 14, 24, 25, 26, 40, 41, 54, 55, 60, 61, 62, 63, 71, 72, 82, 83, 84, 85, 88, 96, 98, 101, 102, 103, 104, 112, 113, 115, 117, 129, 132, 134], "textbf": [90, 97, 145], "textposit": 88, "textrm": [129, 130, 136, 137, 138, 139], "tg": [56, 64, 92, 94, 142], "th": [54, 79], "than": [33, 52, 53, 55, 68, 72, 76, 77, 80, 81, 82, 85, 86, 87, 88, 91, 98, 102, 129, 135, 145], "thank": [53, 55, 56, 80, 144], "thatw": 59, "thei": [53, 55, 59, 73, 74, 80, 82, 86, 98, 129, 139], "them": [21, 55, 56, 69, 70, 71, 75, 78, 80, 84, 98], "theme": [54, 55], "theme_minim": [52, 55, 57], "theorem": [98, 104, 129, 139], "theoret": [77, 88, 112, 143], "theori": [96, 143], "therebi": [54, 56, 79, 145], "therefor": [60, 62, 63, 66, 82, 85, 87, 112, 113, 129, 138], "theta": [9, 10, 12, 13, 14, 16, 18, 19, 20, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 56, 57, 58, 59, 60, 63, 66, 68, 72, 75, 76, 77, 79, 85, 87, 88, 89, 90, 91, 92, 94, 96, 97, 98, 99, 101, 102, 103, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 135, 138, 139, 142, 145], "theta_": [25, 60, 63, 66, 85, 88, 96, 98, 105, 106, 128, 129, 139], "theta_0": [11, 32, 33, 38, 39, 52, 54, 55, 57, 66, 68, 69, 70, 72, 73, 74, 79, 80, 88, 89, 91, 96, 98, 100, 107, 108, 109, 110, 111, 113, 122, 123, 125, 128, 129, 136, 137, 139, 142], "theta_dml": [52, 68, 91], "theta_dml_po": [52, 68, 91], "theta_initi": 68, "theta_nonorth": [52, 68], "theta_orth_nosplit": [52, 68], "theta_orth_po_nosplit": [52, 68], "theta_resc": 52, "theta_t": 59, "thi": [9, 10, 20, 21, 22, 23, 24, 25, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 96, 97, 98, 102, 104, 105, 106, 107, 108, 112, 113, 114, 115, 116, 117, 119, 121, 122, 123, 128, 129, 130, 135, 136, 137, 140, 141, 142, 143, 144, 145], "think": 56, "third": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 61, 62, 68, 79, 91, 112], "thirion": [140, 142], "this_df": [72, 80], "this_split_ind": 79, "those": [53, 55, 61, 80, 81, 86], "though": [51, 67, 82], "thread": [82, 97], "three": [54, 56, 73, 74, 141, 144], "threshold": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 85, 88, 98], "through": [53, 61, 71, 73, 74, 84, 85, 97, 98], "throughout": [75, 86], "thu": [78, 85, 96, 98], "tibbl": 53, "tick": 24, "tick_param": 85, "tight": 68, "tight_layout": [60, 63, 78, 79, 85], "tighter": 85, "tild": [9, 10, 14, 25, 26, 54, 79, 82, 90, 96, 112, 113, 115, 117, 122, 123, 126, 127, 128, 129, 138, 139], "tile": 86, "time": [4, 5, 6, 15, 16, 20, 22, 24, 25, 52, 53, 54, 55, 57, 58, 59, 61, 68, 72, 73, 74, 79, 80, 81, 85, 87, 88, 89, 93, 94, 98, 99, 100, 101, 102, 103, 104, 113, 129, 143, 144, 145], "time_budget": 78, "time_df": 59, "time_period": 59, "time_typ": [25, 60, 63], "titiunik": [98, 143], "titl": [21, 24, 54, 55, 57, 60, 61, 63, 66, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 84, 85, 88, 140], "title_fonts": [60, 63], "tmp": 74, "tname": 53, "tnr": [56, 97], "to_datetim": [60, 63], "to_fram": 83, "to_numpi": [71, 75, 81, 84], "todo": [54, 64], "toeplitz": 72, "togeth": [73, 74, 128], "toler": 79, "tomasz": [143, 144], "toml": 144, "too": 77, "tool": [53, 56, 87, 145], "top": [54, 77, 79, 80, 81, 85, 88, 98, 140], "total": [43, 47, 55, 78, 80, 98, 99], "tpot": 78, "tracker": 140, "tradit": 128, "train": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 46, 47, 52, 54, 56, 68, 69, 70, 71, 73, 74, 76, 77, 79, 80, 83, 84, 90, 91, 112], "train_id": [54, 112], "train_ind": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "train_set": 112, "train_test_split": 68, "transact": 143, "transform": [9, 10, 41, 76, 82, 88, 145], "translat": 72, "transpos": 59, "treament": 83, "treat": [24, 25, 26, 33, 53, 58, 59, 60, 61, 62, 63, 66, 75, 83, 85, 88, 96, 98, 100, 101, 103, 104, 108, 113, 115, 117, 128, 145], "treat1_param": 82, "treat2_param": 82, "treat_var": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 97], "treatment": [4, 5, 6, 7, 8, 9, 10, 11, 14, 17, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 72, 75, 77, 78, 79, 83, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 111, 112, 114, 115, 116, 117, 118, 119, 121, 122, 123, 128, 132, 134, 135, 136, 138, 140, 142, 143, 144, 145], "treatment_df": 59, "treatment_effect": [11, 69, 70], "treatment_level": [29, 30, 66, 76, 98], "treatment_var": [4, 5, 6], "tree": [33, 45, 55, 56, 58, 59, 60, 63, 77, 80, 90, 95, 97, 98, 112, 113, 128, 142, 144], "tree_param": [33, 45], "tree_summari": 80, "trees_class": [55, 80], "trend": [53, 58, 59, 60, 63, 79, 98, 100, 102, 104, 143], "tri": [72, 129, 130], "triangular": [40, 85, 98], "trim": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 55, 80, 81, 88], "trimming_rul": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 81], "trimming_threshold": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 55, 69, 76, 80, 81, 83, 84, 88], "trm": [56, 97], "true": [4, 5, 6, 7, 8, 9, 14, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 94, 97, 98, 100, 112, 113, 118, 119, 122, 123, 126, 127, 128, 129, 131, 132, 133, 134, 139, 142, 145], "true_effect": [59, 69, 70, 73, 74, 86], "true_gatet_effect": 75, "true_group_effect": 75, "true_tau": 85, "truemfunct": 86, "truncat": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 81], "try": [77, 87], "tune": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 72, 77, 85, 95, 98, 140, 142, 144], "tune_on_fold": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 97], "tune_r": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "tune_set": [56, 97], "tuned_model": 78, "tuner": 97, "tunergridsearch": 56, "tupl": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 60, 63, 98, 101, 102, 103], "turn": 88, "turrel": 18, "tutori": 55, "tw": [80, 81], "twice": 98, "twinx": 66, "two": [11, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 55, 56, 58, 60, 61, 62, 63, 67, 68, 71, 76, 77, 78, 80, 81, 82, 83, 84, 86, 87, 88, 90, 91, 96, 97, 100, 103, 112, 122, 128, 145], "twoclass": 56, "twoearn": [55, 80, 81, 87, 145], "type": [6, 9, 10, 11, 14, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 55, 56, 60, 63, 68, 77, 78, 79, 85, 88, 91, 95, 97, 98, 113, 124, 125, 128, 129, 138, 144, 145], "typeerror": [60, 61, 62, 63], "typic": [74, 98, 104, 140], "u": [9, 10, 11, 13, 19, 26, 32, 33, 34, 35, 36, 43, 47, 52, 53, 54, 55, 58, 59, 60, 61, 63, 66, 68, 71, 73, 74, 76, 77, 79, 80, 81, 83, 84, 86, 87, 88, 91, 98, 105, 107, 108, 129, 130, 141, 145], "u_hat": [52, 68, 113], "u_i": [12, 15, 18, 19], "u_t": 26, "uehara": 143, "uhash": 56, "ulf": 143, "unambigu": 88, "uncertainti": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 73, 74, 76, 85, 87, 129, 135, 145], "unchang": [42, 43, 46, 47], "uncondit": [55, 60, 63, 80, 145], "unconfounded": [88, 143], "under": [32, 37, 52, 55, 58, 61, 68, 80, 83, 85, 88, 91, 98, 102, 104, 111, 128, 143], "underbrac": [52, 59, 68, 91, 96], "underfit": 78, "underli": [9, 14, 55, 56, 60, 63, 66, 73, 74, 82, 83, 98, 104, 129, 130, 145], "underlin": [54, 79], "underset": [85, 98], "understand": [60, 61, 63, 88], "undesir": 97, "unevenli": 112, "uniform": [26, 40, 41, 59, 67, 69, 70, 71, 83, 84, 128], "uniform_averag": [43, 47], "uniformli": [32, 60, 63, 71, 81, 128], "union": 32, "uniqu": [6, 51, 60, 61, 62, 63, 66, 67, 77, 85, 93, 94, 98, 101, 103, 113, 129, 139], "unique_label": 78, "unit": [6, 25, 52, 53, 57, 58, 59, 60, 61, 62, 63, 75, 85, 89, 93, 94, 98, 100, 101, 102, 103, 104, 113, 114, 115, 116, 117, 129, 132, 134, 144], "univari": [11, 69, 70], "univers": [24, 143], "unknown": 98, "unlik": [55, 80, 81, 88], "unobserv": [9, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 51, 55, 60, 63, 67, 80, 81, 87, 88, 98, 129, 130, 139, 145], "unpen": 53, "unstabl": [129, 130], "unter": [54, 55, 56], "untest": 88, "until": [98, 100, 144], "untreat": [88, 98, 100], "up": [24, 30, 36, 55, 72, 77, 78, 80, 81, 87, 88, 97, 98, 100, 112, 129, 130, 141, 144, 145], "upcom": 144, "updat": [42, 43, 46, 47, 54, 74, 79, 80, 143, 144], "update_layout": [69, 70, 72, 85, 88], "update_trac": [69, 70], "upload": 144, "upon": [113, 144], "upper": [32, 55, 56, 59, 60, 63, 66, 68, 71, 75, 76, 81, 84, 85, 87, 88, 97, 129, 135, 139, 145], "upper_bound": [69, 70], "upsilon": [57, 89], "upsilon_i": [57, 89], "upward": [55, 80, 81, 88], "upweight": 82, "url": [61, 72, 140, 143], "us": [4, 5, 6, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 52, 54, 55, 57, 58, 59, 60, 61, 62, 63, 66, 68, 69, 70, 71, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 94, 96, 98, 99, 101, 103, 112, 113, 114, 115, 116, 117, 128, 129, 130, 135, 137, 138, 139, 140, 141, 142, 144, 145], "usa": 143, "usabl": 77, "usag": [53, 58, 60, 61, 62, 63, 64, 66, 75, 79, 80, 81, 87, 89, 92, 93, 94, 142, 144], "use_label_encod": [80, 145], "use_other_treat_as_covari": [4, 5, 6, 92, 94], "use_pred_offset": 97, "use_weight": 97, "usecolormap": [69, 70], "user": [27, 28, 42, 43, 46, 47, 52, 53, 54, 55, 56, 61, 66, 68, 75, 76, 77, 79, 80, 85, 87, 96, 97, 98, 113, 128, 140, 141, 142, 144, 145], "user_guid": 74, "userwarn": [60, 62, 63, 80, 88], "usual": [54, 58, 60, 61, 62, 63, 69, 70, 77, 79, 85, 87, 88, 96, 97, 112, 129, 139], "util": [0, 28, 76, 77, 78, 82, 85, 97, 98, 144], "v": [7, 8, 13, 15, 16, 17, 19, 32, 33, 38, 39, 43, 47, 52, 54, 55, 60, 63, 66, 68, 75, 76, 77, 78, 79, 80, 82, 85, 86, 90, 91, 96, 98, 105, 107, 108, 109, 110, 128, 140, 142, 143, 144, 145], "v108": 140, "v12": [140, 142], "v22": 56, "v23": 140, "v_": [16, 54, 79, 98], "v_i": [12, 13, 17, 18, 19, 52, 68, 91, 98], "v_j": 128, "val": [13, 60, 61, 62, 63, 112, 143], "val_list": 72, "valid": [4, 5, 6, 12, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 53, 54, 55, 58, 61, 68, 71, 77, 78, 79, 80, 81, 84, 91, 95, 96, 97, 112, 113, 119, 122, 123, 129, 130, 143, 145], "valu": [4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 46, 47, 48, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 66, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 90, 93, 94, 95, 97, 98, 99, 104, 105, 111, 112, 119, 122, 123, 126, 127, 128, 129, 130, 135, 139, 142, 144, 145], "value_count": 80, "van": 143, "vanderpla": [140, 142], "vanish": [52, 68, 91], "var": [9, 10, 14, 25, 26, 54, 79, 82, 85, 129, 130, 136, 137, 138, 139], "var_ep": 88, "varepsilon": [9, 10, 16, 32, 54, 57, 79, 89, 96, 98, 107], "varepsilon_": [16, 25, 54, 60, 63, 79], "varepsilon_0": 26, "varepsilon_1": 26, "varepsilon_d": [10, 14], "varepsilon_i": [14, 15, 57, 71, 84, 89], "vari": [25, 55, 59, 60, 63, 77, 80, 82, 88], "variabl": [4, 5, 6, 7, 10, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 72, 75, 78, 79, 80, 81, 85, 87, 88, 89, 92, 93, 94, 96, 97, 98, 100, 101, 103, 104, 105, 107, 108, 109, 110, 112, 113, 128, 129, 130, 135, 139, 142, 143, 144, 145], "varianc": [27, 28, 54, 56, 79, 85, 87, 88, 95, 98, 112, 129, 130, 135, 137, 138, 139, 142, 144], "variant": [53, 61, 76], "variat": [10, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 76, 87, 129, 130, 139], "variou": [53, 78, 88, 97, 145], "varoquaux": [140, 142], "vasili": [88, 143], "vector": [11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 25, 29, 31, 32, 33, 34, 35, 37, 38, 39, 51, 54, 55, 57, 58, 67, 73, 74, 75, 79, 80, 83, 86, 89, 98, 100, 109, 110, 111, 128, 142, 144], "vee": [113, 115, 117], "venv": 141, "verbos": [55, 59, 60, 63, 68, 77, 78, 85, 88], "veri": [53, 54, 56, 61, 75, 77, 79, 86, 88, 113, 140], "verifi": 82, "versa": [77, 82, 129, 135], "version": [9, 42, 43, 46, 47, 54, 55, 56, 58, 59, 88, 90, 96, 98, 101, 102, 103, 113, 128, 129, 131, 132, 133, 134, 136, 137, 140, 144], "versoin": 88, "versu": 74, "vertic": [54, 66, 79], "via": [9, 10, 20, 22, 23, 26, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 53, 57, 58, 59, 60, 61, 62, 63, 71, 72, 73, 74, 75, 76, 77, 85, 87, 89, 90, 92, 94, 95, 96, 97, 98, 99, 100, 101, 103, 112, 119, 127, 128, 129, 130, 135, 139, 140, 141, 142, 143, 144, 145], "viabl": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "vice": [77, 82, 129, 135], "victor": [72, 88, 112, 140, 143], "view": [63, 74], "vignett": [53, 144], "villa": [51, 67], "violat": [60, 63], "violet": [71, 81, 84], "vira": 143, "virtual": [61, 141], "virtualenv": 141, "visibl": [81, 85, 88], "visit": [140, 145], "visual": [21, 54, 60, 61, 62, 63, 75, 76, 78, 79, 85], "vmax": 63, "vmin": 63, "vol": 53, "volum": [88, 140], "voluntari": 82, "vv740": 79, "vv760g": 79, "w": [7, 8, 9, 10, 17, 25, 26, 27, 28, 42, 43, 46, 47, 54, 72, 79, 82, 83, 86, 90, 91, 98, 101, 102, 103, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 142], "w24678": 112, "w30302": 143, "w_": [25, 26, 54, 79, 83, 98], "w_1": [25, 26, 83], "w_2": [25, 26, 83], "w_3": [25, 26], "w_4": [25, 26], "w_df": 83, "w_i": [19, 58, 83, 85, 90, 96, 98, 112, 113, 115, 117, 128], "wa": [54, 59, 78, 79, 88, 144], "wage": 61, "wager": 143, "wai": [55, 77, 78, 80, 86, 88, 97, 113, 141], "wander": 18, "wang": 143, "want": [51, 54, 55, 56, 58, 67, 71, 77, 79, 84, 85, 97, 98, 140, 141, 143], "warn": [21, 24, 51, 52, 53, 54, 55, 56, 57, 60, 62, 63, 68, 80, 88, 90, 97, 98, 112, 113, 128, 142, 144], "wayon": 54, "we": [33, 45, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 96, 97, 98, 99, 101, 102, 103, 104, 108, 112, 113, 115, 117, 120, 128, 129, 130, 139, 141, 142, 144, 145], "weak": [32, 129, 130, 143, 144], "weakest": [60, 63], "wealth": [7, 87], "websit": [55, 56, 61, 97, 140], "wedg": [54, 79], "week": 144, "wei": 128, "weight": [21, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 42, 43, 46, 47, 54, 55, 56, 57, 60, 61, 62, 63, 66, 75, 76, 79, 80, 85, 89, 95, 97, 98, 99, 113, 118, 121, 128, 129, 136, 137, 144], "weights_bar": [29, 33, 76], "weights_dict": 76, "weiss": [140, 142], "well": [4, 5, 6, 46, 47, 52, 54, 68, 72, 77, 78, 79, 86, 90, 91, 94, 112, 141, 142], "were": [55, 57, 80, 81, 89, 145], "what": [53, 72, 77, 143], "when": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 55, 58, 61, 74, 76, 80, 82, 86, 98, 108, 111, 113, 128, 140, 141, 142, 144], "whenev": [55, 80], "whera": [129, 137], "where": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 43, 44, 45, 47, 51, 52, 54, 55, 57, 58, 59, 60, 61, 62, 63, 66, 67, 68, 71, 75, 79, 80, 82, 83, 84, 85, 86, 88, 89, 90, 91, 96, 97, 98, 99, 100, 102, 103, 104, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 135, 136, 137, 139, 141, 142, 144, 145], "wherea": [11, 57, 58, 60, 63, 66, 86, 88, 89, 98, 101, 103, 113, 121, 129, 136, 145], "whether": [4, 5, 6, 11, 14, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 55, 59, 77, 80, 81, 85, 86, 88, 92, 94, 97, 98, 129, 130, 144], "which": [4, 5, 6, 9, 11, 20, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 46, 51, 52, 53, 55, 56, 57, 58, 60, 61, 62, 63, 65, 66, 67, 68, 72, 74, 75, 77, 78, 80, 81, 83, 85, 87, 88, 89, 91, 92, 94, 96, 97, 98, 102, 113, 128, 129, 130, 135, 136, 137, 139, 141, 144, 145], "while": [51, 67, 98], "white": [54, 73, 74, 79, 88], "whitegrid": [80, 81], "whitnei": [88, 143], "who": [53, 55, 80, 88], "whole": [52, 58, 68, 85, 91, 97, 113, 114, 129, 130], "whom": 98, "why": 61, "widehat": [60, 61, 62, 63, 98], "width": [21, 24, 52, 54, 69, 70, 72], "wiki": 144, "wiksel": 143, "wild": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 128], "window": 141, "wise": [73, 74], "wish": 141, "within": [40, 54, 60, 61, 62, 63, 73, 74, 79, 83, 85], "without": [14, 20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 40, 51, 52, 60, 61, 62, 63, 67, 68, 77, 78, 88, 91, 95, 97, 98, 129, 130, 141, 144], "wolf": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 128], "won": 88, "word": [40, 85, 98, 144, 145], "work": [42, 43, 46, 47, 60, 61, 62, 63, 65, 66, 74, 75, 77, 82, 87, 88, 97, 98, 128, 141, 143], "workflow": [140, 144], "workspac": 80, "world": 143, "worri": 88, "wors": [43, 47], "would": [43, 47, 53, 55, 56, 60, 61, 62, 63, 69, 70, 72, 77, 80, 81, 85, 87, 88, 96, 97, 129, 139, 145], "wrapper": [53, 85, 97], "wright": 86, "write": [52, 53, 57, 58, 68, 74, 89, 91, 129, 139], "written": [98, 113, 129, 136, 137], "wrong": [77, 82], "wspace": 77, "wurd": [54, 55, 56], "www": [140, 141], "x": [4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 31, 32, 33, 35, 37, 38, 39, 40, 41, 42, 43, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 94, 96, 97, 98, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 134, 136, 137, 138, 139, 142, 145], "x0": [66, 82, 85], "x1": [54, 56, 57, 58, 66, 76, 78, 79, 82, 85, 87, 88, 89, 92, 94, 96, 97, 98, 113, 128, 129, 130, 142], "x10": [54, 56, 57, 76, 78, 79, 89, 92, 94, 97, 98, 113, 128, 142], "x100": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x11": [54, 56, 57, 79, 89, 92, 94, 97, 98, 113, 128, 142], "x12": [54, 56, 57, 79, 89, 92, 94, 97, 98, 113, 128, 142], "x13": [54, 56, 57, 79, 89, 92, 94, 97, 98, 113, 128, 142], "x14": [54, 56, 57, 79, 89, 92, 94, 97, 98, 113, 128, 142], "x15": [54, 56, 57, 79, 89, 92, 94, 97, 98, 113, 128, 142], "x16": [54, 56, 57, 79, 89, 92, 94, 97, 98, 113, 128, 142], "x17": [54, 56, 57, 79, 89, 92, 94, 97, 98, 113, 128, 142], "x18": [54, 56, 57, 79, 89, 92, 94, 97, 98, 113, 128, 142], "x19": [54, 56, 57, 79, 89, 92, 94, 97, 98, 113, 128, 142], "x1x2x3x4x5x6x7x8x9x10": 54, "x2": [54, 56, 57, 58, 66, 76, 78, 79, 85, 87, 88, 89, 92, 94, 96, 97, 98, 113, 128, 142], "x20": [54, 56, 57, 79, 89, 92, 94, 97, 98, 113, 128, 142], "x21": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x22": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x23": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x24": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x25": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x26": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x27": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x28": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x29": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x2_dummi": 88, "x2_preds_control": 88, "x2_preds_treat": 88, "x3": [54, 56, 57, 58, 66, 76, 78, 79, 87, 88, 89, 92, 94, 96, 97, 98, 113, 128, 142], "x30": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x31": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x32": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x33": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x34": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x35": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x36": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x37": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x38": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x39": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x4": [54, 56, 57, 58, 66, 76, 78, 79, 87, 88, 89, 92, 94, 97, 98, 113, 128, 142], "x40": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x41": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x42": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x43": [54, 56, 57, 78, 79, 89, 92, 94, 98, 142], "x44": [54, 56, 57, 78, 79, 89, 92, 94, 98, 142], "x45": [54, 56, 57, 78, 79, 89, 92, 94, 98, 142], "x46": [54, 56, 57, 78, 79, 89, 92, 94, 98, 142], "x47": [54, 56, 57, 78, 79, 89, 92, 94, 98, 142], "x48": [54, 56, 57, 78, 79, 89, 92, 94, 98, 142], "x49": [54, 56, 57, 78, 79, 89, 92, 94, 98, 142], "x5": [54, 56, 57, 76, 78, 79, 88, 89, 92, 94, 97, 98, 113, 128, 142], "x50": [54, 56, 57, 78, 79, 89, 92, 94, 98, 142], "x51": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x52": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x53": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x54": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x55": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x56": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x57": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x58": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x59": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x6": [54, 56, 57, 76, 78, 79, 89, 92, 94, 97, 98, 113, 128, 142], "x60": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x61": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x62": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x63": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x64": [54, 56, 57, 60, 61, 62, 63, 79, 80, 89, 92, 94, 98, 142], "x65": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x66": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x67": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x68": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x69": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x7": [54, 56, 57, 76, 78, 79, 89, 92, 94, 97, 98, 113, 128, 142], "x70": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x71": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x72": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x73": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x74": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x75": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x76": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x77": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x78": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x79": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x8": [54, 56, 57, 76, 78, 79, 89, 92, 94, 97, 98, 113, 128, 142], "x80": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x81": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x82": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x83": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x84": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x85": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x86": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x87": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x88": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x89": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x9": [54, 56, 57, 76, 78, 79, 89, 92, 94, 97, 98, 113, 128, 142], "x90": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x91": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x92": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x93": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x94": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x95": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x96": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x96x97x98x99x100ydcluster_var_icluster_var_jz": 54, "x97": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x98": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x99": [54, 56, 57, 79, 89, 92, 94, 98, 142], "x_": [16, 17, 25, 52, 54, 59, 68, 79, 88, 91], "x_0": [59, 69, 70, 73, 74, 75], "x_1": [9, 10, 14, 25, 26, 38, 39, 59, 69, 70, 71, 73, 74, 75, 84, 88, 98, 109, 110, 129, 130, 142], "x_1x_3": [71, 84], "x_2": [9, 10, 14, 25, 26, 59, 69, 70, 71, 73, 74, 75, 84, 88, 129, 130], "x_3": [9, 10, 14, 25, 26, 59, 69, 70, 73, 74, 75, 129, 130], "x_4": [9, 10, 14, 25, 26, 69, 70, 71, 73, 74, 75, 84], "x_5": [9, 10, 14, 69, 70, 73, 74], "x_6": [69, 70, 73, 74], "x_7": [69, 70, 73, 74], "x_8": [69, 70, 73, 74], "x_9": [69, 70, 73, 74], "x_binary_control": 88, "x_binary_tr": 88, "x_col": [4, 5, 6, 24, 51, 54, 55, 56, 60, 61, 62, 63, 67, 72, 79, 80, 81, 83, 85, 86, 87, 88, 92, 93, 94, 97, 98, 99, 101, 103, 142, 144, 145], "x_cols_bench": 88, "x_cols_binari": 88, "x_cols_poli": 79, "x_conf": 84, "x_conf_tru": 84, "x_df": 59, "x_domain": 56, "x_i": [11, 12, 13, 15, 17, 18, 19, 41, 52, 57, 58, 68, 71, 73, 74, 82, 84, 85, 89, 91, 96, 98, 100, 101, 103, 104, 111, 113, 115, 117], "x_p": [38, 39, 98, 109, 110, 142], "x_train": 78, "x_true": [71, 84], "x_var": 56, "xaxis_titl": [69, 70, 72, 85, 88], "xformla": 53, "xgb": 78, "xgb_untuned_l": 78, "xgb_untuned_m": 78, "xgbclassifi": [77, 80, 82, 145], "xgboost": [52, 55, 77, 80, 82, 145], "xgbregressor": [77, 78, 80, 82, 145], "xi": [14, 25, 26, 98], "xi_": 128, "xi_0": [16, 54, 79], "xi_i": [57, 89], "xiaoji": 143, "xintercept": [52, 57], "xlab": [52, 54, 55], "xlabel": [59, 60, 61, 63, 66, 69, 70, 71, 73, 74, 78, 80, 81, 84], "xlim": [52, 55], "xtick": [66, 78], "xval": [56, 97], "xx": 68, "y": [4, 5, 6, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 105, 107, 108, 109, 110, 112, 113, 114, 115, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 135, 136, 137, 138, 139, 142, 145], "y0": [53, 60, 63, 66, 71, 84], "y0_cvar": 71, "y0_quant": [71, 84], "y1": [53, 60, 63, 71, 84], "y1_cvar": 71, "y1_quant": [71, 84], "y_": [16, 24, 25, 54, 57, 58, 59, 60, 63, 79, 89, 98, 100, 101, 103, 104, 111, 113, 115, 117], "y_0": [20, 22, 26, 41, 113, 116], "y_1": [20, 22, 26, 41, 113, 116], "y_col": [4, 5, 6, 24, 51, 52, 54, 55, 56, 57, 60, 61, 62, 63, 67, 69, 70, 72, 73, 74, 76, 79, 80, 81, 83, 85, 86, 87, 90, 91, 92, 93, 94, 97, 98, 99, 101, 103, 112, 113, 142, 144, 145], "y_df": [59, 83], "y_diff": 59, "y_i": [11, 12, 13, 15, 17, 18, 19, 52, 57, 58, 68, 71, 82, 83, 84, 85, 89, 91, 98, 100, 111], "y_label": [21, 24], "y_lower_quantil": [60, 63], "y_mean": [60, 63], "y_pred": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 77, 97], "y_train": 78, "y_true": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 77, 97], "y_upper_quantil": [60, 63], "ya": 143, "yasui": 143, "yata": 143, "yaxis_titl": [69, 70, 72, 85, 88], "year": [61, 140], "yerr": [59, 66, 73, 74, 78, 80, 82, 85], "yet": [54, 60, 62, 63, 65, 98, 101, 103, 104], "yggvpl": 79, "yield": 98, "yintercept": 55, "ylab": [52, 54, 55], "ylabel": [59, 60, 61, 63, 66, 69, 70, 71, 73, 74, 78, 80, 81, 84], "ylim": 80, "ymax": 55, "ymin": 55, "yname": 53, "york": 143, "you": [42, 43, 46, 47, 51, 52, 59, 60, 61, 62, 63, 67, 74, 79, 87, 98, 140, 141, 145], "your": [77, 141], "ython": 140, "yukun": 143, "yusuk": 143, "yuya": 143, "yy": 68, "z": [4, 5, 6, 9, 10, 12, 14, 15, 16, 19, 25, 26, 32, 34, 37, 38, 51, 54, 55, 57, 60, 63, 67, 69, 70, 72, 79, 80, 84, 86, 88, 89, 96, 98, 107, 109, 113, 120, 122, 124, 127, 128, 144], "z1": [6, 24, 38, 60, 63, 93, 94, 98, 99, 100, 101, 103], "z2": [6, 24, 60, 63, 93, 94, 98, 99, 100, 101, 103], "z3": [6, 24, 60, 63, 93, 94, 98, 99, 100, 101, 103], "z4": [6, 24, 60, 63, 93, 94, 98, 99, 100, 101, 103], "z_": [16, 54, 79], "z_1": [9, 10, 14, 60, 63], "z_2": [9, 10, 14], "z_3": [9, 10, 14], "z_4": [9, 10, 14, 60, 63], "z_5": 9, "z_col": [4, 5, 6, 32, 34, 38, 51, 54, 55, 57, 67, 79, 80, 81, 86, 89, 92, 94, 96, 98, 144], "z_i": [15, 19, 57, 84, 89, 98], "z_j": [9, 10, 14, 25, 26], "z_true": 84, "zadik": 143, "zaxis_titl": [69, 70, 72], "zero": [22, 26, 41, 58, 59, 60, 63, 71, 76, 77, 83, 84, 87, 88, 98, 113, 115, 117, 128, 129, 132, 134], "zeros_lik": 84, "zeta": [32, 38, 39, 55, 80, 96, 98, 107, 109, 110, 142], "zeta_": [16, 54, 79], "zeta_0": [16, 54, 79], "zeta_i": [13, 15, 17, 52, 68, 91], "zeta_j": 128, "zhang": 143, "zhao": [9, 10, 14, 20, 22, 23, 26, 53, 58, 60, 63, 98, 100, 104, 143], "zimmert": [58, 98, 104, 143], "zip": [69, 70], "zorder": 66, "\u03c4_x0": 82, "\u03c4_x1": 82, "\u2139": 52}, "titles": ["API Reference", "<span class=\"section-number\">1. </span>DoubleML Data Class", "<span class=\"section-number\">3. </span>Datasets", "<span class=\"section-number\">2. </span>DoubleML Models", "<span class=\"section-number\">1.2. </span>doubleml.data.DoubleMLClusterData", "<span class=\"section-number\">1.1. </span>doubleml.data.DoubleMLData", "<span class=\"section-number\">1.3. </span>doubleml.data.DoubleMLPanelData", "<span class=\"section-number\">3.1.1. </span>doubleml.datasets.fetch_401K", "<span class=\"section-number\">3.1.2. </span>doubleml.datasets.fetch_bonus", "<span class=\"section-number\">3.2.9. </span>doubleml.datasets.make_confounded_irm_data", "<span class=\"section-number\">3.2.8. </span>doubleml.datasets.make_confounded_plr_data", "<span class=\"section-number\">3.2.10. </span>doubleml.datasets.make_heterogeneous_data", "<span class=\"section-number\">3.2.4. </span>doubleml.datasets.make_iivm_data", "<span class=\"section-number\">3.2.3. </span>doubleml.datasets.make_irm_data", "<span class=\"section-number\">3.2.11. </span>doubleml.datasets.make_irm_data_discrete_treatments", "<span class=\"section-number\">3.2.2. </span>doubleml.datasets.make_pliv_CHS2015", "<span class=\"section-number\">3.2.6. </span>doubleml.datasets.make_pliv_multiway_cluster_CKMS2021", "<span class=\"section-number\">3.2.1. </span>doubleml.datasets.make_plr_CCDDHNR2018", "<span class=\"section-number\">3.2.5. </span>doubleml.datasets.make_plr_turrell2018", "<span class=\"section-number\">3.2.7. </span>doubleml.datasets.make_ssm_data", "<span class=\"section-number\">2.3.4. </span>doubleml.did.DoubleMLDID", "<span class=\"section-number\">2.3.2. </span>doubleml.did.DoubleMLDIDAggregation", "<span class=\"section-number\">2.3.3. </span>doubleml.did.DoubleMLDIDBinary", "<span class=\"section-number\">2.3.5. </span>doubleml.did.DoubleMLDIDCS", "<span class=\"section-number\">2.3.1. </span>doubleml.did.DoubleMLDIDMulti", "<span class=\"section-number\">3.2.13. </span>doubleml.did.datasets.make_did_CS2021", "<span class=\"section-number\">3.2.12. </span>doubleml.did.datasets.make_did_SZ2020", "<span class=\"section-number\">5.1. </span>doubleml.double_ml_score_mixins.LinearScoreMixin", "<span class=\"section-number\">5.2. </span>doubleml.double_ml_score_mixins.NonLinearScoreMixin", "<span class=\"section-number\">2.2.2. </span>doubleml.irm.DoubleMLAPO", "<span class=\"section-number\">2.2.3. </span>doubleml.irm.DoubleMLAPOS", "<span class=\"section-number\">2.2.7. </span>doubleml.irm.DoubleMLCVAR", "<span class=\"section-number\">2.2.4. </span>doubleml.irm.DoubleMLIIVM", "<span class=\"section-number\">2.2.1. </span>doubleml.irm.DoubleMLIRM", "<span class=\"section-number\">2.2.6. </span>doubleml.irm.DoubleMLLPQ", "<span class=\"section-number\">2.2.5. </span>doubleml.irm.DoubleMLPQ", "<span class=\"section-number\">2.2.8. </span>doubleml.irm.DoubleMLQTE", "<span class=\"section-number\">2.2.9. </span>doubleml.irm.DoubleMLSSM", "<span class=\"section-number\">2.1.2. </span>doubleml.plm.DoubleMLPLIV", "<span class=\"section-number\">2.1.1. </span>doubleml.plm.DoubleMLPLR", "<span class=\"section-number\">2.4.1. </span>doubleml.rdd.RDFlex", "<span class=\"section-number\">3.2.14. </span>doubleml.rdd.datasets.make_simple_rdd_data", "<span class=\"section-number\">4.1.2. </span>doubleml.utils.DMLDummyClassifier", "<span class=\"section-number\">4.1.1. </span>doubleml.utils.DMLDummyRegressor", "<span class=\"section-number\">4.1.3. </span>doubleml.utils.DoubleMLBLP", "<span class=\"section-number\">4.1.4. </span>doubleml.utils.DoubleMLPolicyTree", "<span class=\"section-number\">4.1.6. </span>doubleml.utils.GlobalClassifier", "<span class=\"section-number\">4.1.5. </span>doubleml.utils.GlobalRegressor", "<span class=\"section-number\">4.2.1. </span>doubleml.utils.gain_statistics", "<span class=\"section-number\">5. </span>Score Mixin Classes for DoubleML Models", "<span class=\"section-number\">4. </span>Utility Classes and Functions", "R: Basic Instrumental Variables Calculation", "R: Basics of Double Machine Learning", "R: DoubleML for Difference-in-Differences", "R: Cluster Robust Double Machine Learning", "R: Impact of 401(k) on Financial Wealth", "R: Ensemble Learners and More with <code class=\"docutils literal notranslate\"><span class=\"pre\">mlr3pipelines</span></code>", "R: Sample Selection Models", "Python: Difference-in-Differences", "Python: Difference-in-Differences Pre-Testing", "Python: Panel Data with Multiple Time Periods", "Python: Real-Data Example for Multi-Period Difference-in-Differences", "Python: Panel Data Introduction", "Python: Pepeated Cross-Sectional Data with Multiple Time Periods", "DML: Bonus Data", "Examples", "Python: Average Potential Outcome (APO) Models", "Python: Basic Instrumental Variables calculation", "Python: Basics of Double Machine Learning", "Python: Conditional Average Treatment Effects (CATEs) for IRM models", "Python: Conditional Average Treatment Effects (CATEs) for PLR models", "Python: Conditional Value at Risk of potential outcomes", "Python: First Stage and Causal Estimation", "Python: Group Average Treatment Effects (GATEs) for IRM models", "Python: Group Average Treatment Effects (GATEs) for PLR models", "Python: GATE Sensitivity Analysis", "Python: IRM and APO Model Comparison", "Python: Choice of learners", "DoubleML meets FLAML - How to tune learners automatically within <code class=\"docutils literal notranslate\"><span class=\"pre\">DoubleML</span></code>", "Python: Cluster Robust Double Machine Learning", "Python: Impact of 401(k) on Financial Wealth", "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)", "Python: PLM and IRM for Multiple Treatments", "Python: Policy Learning with Trees", "Python: Potential Quantiles and Quantile Treatment Effects", "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)", "Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments", "Python: Sensitivity Analysis", "Example: Sensitivity Analysis for Causal ML", "Python: Sample Selection Models", "<span class=\"section-number\">6. </span>Double machine learning algorithms", "<span class=\"section-number\">1. </span>The basics of double/debiased machine learning", "DoubleMLData from dataframes", "&lt;no title&gt;", "<span class=\"section-number\">2. </span>Data Backend", "User Guide", "<span class=\"section-number\">4. </span>Heterogeneous treatment effects", "<span class=\"section-number\">7. </span>Learners, hyperparameters and hyperparameter tuning", "<span class=\"section-number\">3. </span>Models", "&lt;no title&gt;", "Panel Data", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">9. </span>Sample-splitting, cross-fitting and repeated cross-fitting", "<span class=\"section-number\">5. </span>Score functions", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">8. </span>Variance estimation and confidence intervals", "<span class=\"section-number\">10. </span>Sensitivity analysis", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "DoubleML", "Installing DoubleML", "Getting Started", "Double Machine Learning Literature", "Release Notes", "DoubleML Workflow"], "titleterms": {"": 78, "0": 145, "1": [78, 88, 145], "2": [78, 88, 145], "2011": 88, "2023": 88, "3": [78, 88, 145], "4": [88, 145], "401": [55, 80, 81, 87], "5": [88, 145], "6": 145, "7": 145, "95": 78, "A": [54, 79], "ATE": [57, 75, 82, 89], "No": [54, 79], "One": [54, 69, 70, 79], "That": 86, "The": [55, 80, 82, 91, 142], "acknowledg": [53, 140], "acycl": [51, 67], "addit": 82, "adjust": [60, 63, 85], "advanc": [85, 97, 128], "aggreg": [60, 61, 62, 63, 98], "al": 88, "algorithm": [90, 129, 140, 142], "all": [60, 63], "altern": 113, "analysi": [60, 63, 66, 75, 76, 87, 88, 129, 145], "anticip": [60, 63], "api": [0, 78], "apo": [66, 76, 98, 113, 129], "applic": [54, 79, 87], "approach": [52, 68, 77, 91], "ar": 86, "arah": 88, "arbitrari": 82, "archiv": 65, "arrai": [92, 94], "asset": [55, 80], "assumpt": 88, "att": [58, 60, 61, 62, 63], "augment": 82, "automat": 78, "automl": 78, "averag": [55, 66, 69, 70, 73, 74, 76, 80, 96, 98, 113, 129], "backend": [54, 55, 79, 80, 94, 142, 145], "band": 128, "base": [56, 60, 63], "basic": [51, 52, 60, 63, 67, 68, 91], "benchmark": [87, 88, 129], "bia": [52, 68, 91], "binari": [98, 113], "bonu": 64, "bootstrap": 128, "build": 141, "calcul": [51, 67], "call": 78, "callabl": 113, "case": 65, "cate": [69, 70, 82, 96], "causal": [61, 64, 66, 72, 88, 113, 142, 145], "chernozhukov": 88, "choic": 77, "citat": 140, "class": [1, 49, 50, 54, 79], "cluster": [54, 79], "code": 140, "coeffici": 78, "combin": [60, 63, 72], "compar": [77, 78], "comparison": [53, 76, 78], "comput": [77, 78], "conclus": [78, 88], "conda": 141, "condit": [61, 69, 70, 71, 81, 96, 113], "confid": [78, 86, 128], "construct": 97, "contrast": 66, "control": [60, 63], "covari": [60, 63, 85], "coverag": [58, 72], "cran": 141, "creat": 78, "cross": [54, 58, 63, 79, 98, 100, 112, 113, 129, 142], "custom": [77, 78], "cvar": [71, 81, 96, 113], "dag": [51, 67], "data": [1, 4, 5, 6, 51, 52, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 78, 79, 80, 81, 83, 84, 85, 87, 88, 89, 91, 94, 98, 100, 113, 129, 142, 145], "datafram": [92, 94], "dataset": [2, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 41, 64], "debias": [52, 68, 91, 142], "default": 78, "defin": [54, 79], "demo": 53, "depend": 141, "descript": [60, 63], "design": [85, 98], "detail": [53, 60, 61, 62, 63, 98], "develop": 141, "dgp": [52, 66, 68], "did": [3, 20, 21, 22, 23, 24, 25, 26, 53, 98], "differ": [53, 58, 59, 61, 65, 77, 98, 113, 128, 129], "dimension": [69, 70], "direct": [51, 67], "disclaim": 88, "discontinu": [85, 98], "distribut": [57, 89], "dml": [54, 64, 79, 112, 142, 145], "dml1": 90, "dml2": 90, "dmldummyclassifi": 42, "dmldummyregressor": 43, "doubl": [52, 54, 68, 79, 90, 91, 140, 142, 143], "double_ml_score_mixin": [27, 28], "doubleml": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 53, 55, 56, 67, 78, 80, 87, 88, 128, 140, 141, 145], "doublemlapo": [29, 30], "doublemlblp": 44, "doublemlclusterdata": [4, 54, 79], "doublemlcvar": 31, "doublemldata": [5, 55, 80, 92, 94, 142], "doublemldid": 20, "doublemldidaggreg": 21, "doublemldidbinari": 22, "doublemldidc": 23, "doublemldidmulti": 24, "doublemliivm": 32, "doublemlirm": 33, "doublemllpq": 34, "doublemlpaneldata": [6, 60, 63, 94], "doublemlpliv": [38, 54, 79], "doublemlplr": 39, "doublemlpolicytre": 45, "doublemlpq": 35, "doublemlqt": 36, "doublemlssm": 37, "effect": [55, 60, 61, 62, 63, 65, 69, 70, 71, 73, 74, 76, 80, 81, 82, 84, 87, 88, 96, 98], "elig": [55, 80], "empir": 72, "ensembl": [56, 85], "error": [54, 79], "estim": [51, 55, 57, 58, 60, 61, 62, 63, 64, 67, 72, 75, 78, 80, 81, 82, 84, 87, 88, 89, 112, 113, 128, 142, 145], "et": 88, "evalu": [77, 78, 97], "event": [60, 61, 62, 63], "exampl": [53, 54, 61, 65, 69, 70, 79, 87, 88], "exploit": [53, 56], "extern": [97, 112], "featur": [56, 140], "fetch_401k": 7, "fetch_bonu": 8, "figur": 82, "file": 141, "final": 53, "financi": [55, 80, 81], "first": 72, "fit": [54, 78, 79, 112, 142], "flaml": 78, "flexibl": 85, "fold": [78, 112], "forest": 64, "formul": [88, 145], "from": [53, 56, 92, 94, 141], "full": 78, "function": [50, 53, 54, 79, 113, 142], "fuzzi": [85, 98], "gain_statist": 48, "gate": [73, 74, 75, 96], "gatet": 75, "gener": [2, 52, 65, 66, 68, 78, 85, 91, 129], "get": 142, "github": 141, "global": 85, "globalclassifi": 46, "globalregressor": 47, "graph": [51, 67], "group": [60, 62, 63, 73, 74, 96], "guid": 95, "helper": [54, 79], "heterogen": [65, 76, 82, 96], "how": [56, 78], "hyperparamet": [76, 97], "identif": 88, "iivm": [55, 80, 98, 113], "impact": [55, 80, 81], "implement": [90, 98, 113, 129], "induc": [52, 68, 91], "infer": [128, 145], "initi": [54, 78, 79], "instal": 141, "instrument": [51, 67, 86], "integr": 53, "interact": [55, 73, 80, 83, 98, 113, 129], "interv": [78, 86, 128], "introduct": 62, "invers": 82, "irm": [3, 29, 30, 31, 32, 33, 34, 35, 36, 37, 55, 64, 69, 73, 76, 80, 82, 83, 87, 96, 98, 113, 129], "iv": [51, 55, 67, 80, 98, 113], "k": [55, 80, 81, 87, 112], "lambda": 72, "lasso": [64, 72], "latest": 141, "lear": [54, 79], "learn": [52, 54, 68, 79, 83, 90, 91, 96, 140, 142, 143], "learner": [56, 64, 76, 77, 78, 85, 97, 142], "less": 78, "level": 98, "linear": [55, 60, 63, 74, 80, 82, 85, 98, 113, 129], "linearscoremixin": 27, "literatur": 143, "load": [54, 64, 79, 88], "loader": 2, "local": [55, 80, 81, 84, 85, 113], "loss": 72, "lpq": [84, 113], "lqte": [81, 84], "m": 112, "machin": [52, 54, 68, 79, 90, 91, 140, 142, 143], "main": 140, "mainten": 140, "make_confounded_irm_data": 9, "make_confounded_plr_data": 10, "make_did_cs2021": 25, "make_did_sz2020": 26, "make_heterogeneous_data": 11, "make_iivm_data": 12, "make_irm_data": 13, "make_irm_data_discrete_treat": 14, "make_pliv_chs2015": 15, "make_pliv_multiway_cluster_ckms2021": 16, "make_plr_ccddhnr2018": 17, "make_plr_turrell2018": 18, "make_simple_rdd_data": 41, "make_ssm_data": 19, "mar": [57, 89], "market": [54, 79], "matric": [92, 94], "meet": 78, "method": [78, 145], "metric": [77, 78], "minimum": 97, "miss": [57, 89], "missing": [98, 113], "mixin": 49, "ml": [52, 53, 68, 88, 91, 145], "mlr3": 56, "mlr3extralearn": 56, "mlr3learner": 56, "mlr3pipelin": 56, "model": [3, 49, 55, 57, 64, 66, 69, 70, 73, 74, 76, 78, 80, 82, 83, 86, 88, 89, 96, 98, 112, 113, 128, 129, 142, 145], "modul": 64, "more": 56, "motiv": [54, 79], "multi": 61, "multipl": [60, 63, 66, 82, 98], "multipli": 128, "naiv": [51, 67], "net": [55, 80], "neyman": [113, 142], "nonignor": [57, 89, 98, 113], "nonlinearscoremixin": 28, "nonrespons": [57, 89, 98, 113], "note": 144, "nuisanc": [78, 142], "object": [54, 79, 87], "option": 141, "orthogon": [52, 68, 91, 113, 142], "out": [52, 68, 91], "outcom": [57, 58, 66, 71, 89, 96, 98, 113, 129], "over": 128, "overcom": [52, 68, 91], "overfit": [52, 68, 91], "overlap": 82, "packag": [53, 55, 80, 141], "panel": [58, 60, 62, 98, 100, 113, 129], "parallel": 61, "paramet": [56, 64, 78, 98, 113], "partial": [52, 55, 68, 74, 80, 82, 91, 98, 113, 129], "particip": [55, 80], "partit": 112, "penalti": 72, "pepeat": 63, "perform": [53, 82], "period": [60, 61, 63, 98, 113, 129], "pip": 141, "pipelin": 97, "pliv": [98, 113], "plm": [3, 38, 39, 82, 98, 113, 129], "plot": [54, 78, 79], "plr": [55, 64, 70, 74, 80, 96, 98, 113, 129], "polici": [83, 96], "potenti": [66, 71, 81, 84, 96, 98, 113, 129], "pq": [84, 96, 113], "pre": 59, "predict": [53, 97], "preprocess": 56, "problem": 145, "process": [52, 54, 66, 68, 79, 91], "product": [54, 79], "propens": 82, "provid": 112, "python": [58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 86, 87, 89, 97, 141], "qte": [84, 96], "qualiti": 72, "quantil": [81, 84, 96, 113], "question": 61, "r": [51, 52, 53, 54, 55, 56, 57, 65, 97, 141], "random": [57, 64, 89, 98, 113], "rank": 82, "rdd": [3, 40, 41, 85, 98], "rdflex": 40, "real": [54, 61, 79], "refer": [0, 51, 53, 54, 56, 67, 72, 77, 78, 79, 82, 86, 88, 91, 97, 112, 128, 140, 142], "regress": [55, 73, 74, 80, 83, 85, 98, 113, 129], "regular": [52, 68, 91], "releas": [141, 144], "remark": 53, "remov": [52, 68, 91], "repeat": [58, 98, 100, 112, 113, 129], "repetit": 112, "requir": 97, "research": 61, "respect": [54, 79], "result": [54, 55, 79, 80, 82], "risk": [71, 81, 96, 113], "robust": [54, 79, 86], "run": 86, "sampl": [52, 57, 68, 78, 89, 91, 98, 112, 113], "sandbox": 65, "score": [49, 52, 68, 82, 91, 113, 142], "section": [58, 63, 98, 100, 113, 129], "select": [57, 60, 63, 89, 98, 113], "sensit": [60, 63, 66, 75, 76, 87, 88, 129, 145], "set": [56, 97], "sharp": [85, 98], "simpl": [52, 68, 91], "simul": [51, 54, 58, 67, 79, 86, 87], "simultan": 128, "singl": 66, "small": 86, "sourc": [140, 141], "special": 94, "specif": [129, 145], "specifi": [64, 97, 113], "split": [52, 68, 91, 112], "ssm": 98, "stack": 85, "stage": 72, "standard": [54, 77, 79], "start": 142, "step": 78, "studi": [60, 61, 62, 63, 65], "summari": [55, 78, 80, 82], "test": 59, "theori": 129, "time": [60, 62, 63, 77, 78], "train": 78, "treat": 76, "treatment": [55, 69, 70, 71, 73, 74, 76, 80, 81, 82, 84, 96, 98, 113, 129], "tree": [83, 96], "trend": 61, "tune": [56, 78, 97], "two": [54, 69, 70, 79, 98, 113, 129], "type": 94, "uncondit": 61, "under": [57, 82, 89], "univers": [60, 63], "untun": 78, "up": 56, "us": [51, 53, 56, 64, 67, 78, 97], "user": 95, "util": [42, 43, 44, 45, 46, 47, 48, 50], "v": 72, "valid": 128, "valu": [71, 81, 96, 113], "vanderweel": 88, "variabl": [51, 67, 86], "varianc": 128, "version": 141, "via": 113, "wai": [54, 79], "weak": 86, "wealth": [55, 80, 81], "weight": [82, 96], "when": 78, "whl": 141, "within": 78, "without": [85, 112], "workflow": 145, "xgboost": 78, "zero": [54, 79]}})