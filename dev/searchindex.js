Search.setIndex({"alltitles": {"(One-Way) Cluster Robust Double Machine Learing": [[54, "(One-Way)-Cluster-Robust-Double-Machine-Learing"], [78, "(One-Way)-Cluster-Robust-Double-Machine-Learing"]], "0. Problem Formulation": [[143, "problem-formulation"]], "1. Data Backend": [[143, "data-backend"]], "1. Formulation of Causal Model & Identification Assumptions": [[87, "1.-Formulation-of-Causal-Model-&-Identification-Assumptions"]], "2. Causal Model": [[143, "causal-model"]], "2. Estimation of Causal Effect": [[87, "2.-Estimation-of-Causal-Effect"]], "3. ML Methods": [[143, "ml-methods"]], "3. Sensitivity Analysis": [[87, "3.-Sensitivity-Analysis"]], "4. Benchmarking Analysis": [[87, "4.-Benchmarking-Analysis"]], "4. DML Specifications": [[143, "dml-specifications"]], "5. Conclusion": [[87, "5.-Conclusion"]], "5. Estimation": [[143, "estimation"]], "6. Inference": [[143, "inference"]], "7. Sensitivity Analysis": [[143, "sensitivity-analysis"]], "A Motivating Example: Two-Way Cluster Robust DML": [[54, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"], [78, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"]], "API Reference": [[0, null]], "ATE Estimation and Sensitivity": [[74, "ATE-Estimation-and-Sensitivity"]], "ATE estimates distribution": [[57, "ATE-estimates-distribution"], [57, "id3"], [88, "ATE-estimates-distribution"], [88, "id3"]], "ATT Estimation": [[60, "ATT-Estimation"], [60, "id1"], [62, "ATT-Estimation"]], "ATT Estimation: Conditional Parallel Trends": [[61, "ATT-Estimation:-Conditional-Parallel-Trends"]], "ATT Estimation: Unconditional Parallel Trends": [[61, "ATT-Estimation:-Unconditional-Parallel-Trends"]], "ATTE Estimation": [[58, "ATTE-Estimation"], [58, "id2"]], "Acknowledgements": [[138, "acknowledgements"]], "Acknowledgements and Final Remarks": [[53, "Acknowledgements-and-Final-Remarks"]], "Additional Results: CATE estimates": [[81, "Additional-Results:-CATE-estimates"]], "Advanced: External Predictions": [[96, "advanced-external-predictions"]], "Advanced: Global and Local Learners, Stacked Ensembles": [[84, "Advanced:-Global-and-Local-Learners,-Stacked-Ensembles"]], "Aggregated Effects": [[60, "Aggregated-Effects"]], "Aggregation Details": [[60, "Aggregation-Details"], [61, "Aggregation-Details"], [62, "Aggregation-Details"]], "Algorithm DML1": [[89, "algorithm-dml1"]], "Algorithm DML2": [[89, "algorithm-dml2"]], "All combinations": [[60, "All-combinations"]], "Anticipation": [[60, "Anticipation"]], "Application Results": [[54, "Application-Results"], [78, "Application-Results"]], "Application: 401(k)": [[86, "Application:-401(k)"]], "AutoML with less Computation time": [[77, "AutoML-with-less-Computation-time"]], "Average Potential Outcome (APOs)": [[65, "Average-Potential-Outcome-(APOs)"]], "Average Potential Outcomes (APOs)": [[97, "average-potential-outcomes-apos"], [111, "average-potential-outcomes-apos"], [127, "average-potential-outcomes-apos"]], "Average Potential Outcomes (APOs) for Multiple Treatment Levels": [[97, "average-potential-outcomes-apos-for-multiple-treatment-levels"]], "Average Treatment Effect": [[75, "Average-Treatment-Effect"]], "Average Treatment Effect on the Treated": [[75, "Average-Treatment-Effect-on-the-Treated"]], "Basics": [[60, "Basics"]], "Benchmarking": [[127, "benchmarking"]], "Benchmarking Analysis": [[86, "Benchmarking-Analysis"]], "Binary Interactive Regression Model (IRM)": [[97, "binary-interactive-regression-model-irm"], [111, "binary-interactive-regression-model-irm"]], "CATEs for IRM models": [[95, "cates-for-irm-models"]], "CATEs for PLR models": [[95, "cates-for-plr-models"]], "CVaR Treatment Effects": [[70, "CVaR-Treatment-Effects"]], "CVaR of potential outcomes": [[95, "cvar-of-potential-outcomes"]], "CVaR treatment effects": [[95, "cvar-treatment-effects"]], "Causal Analysis with DoubleML": [[87, "Causal-Analysis-with-DoubleML"]], "Causal Contrasts": [[65, "Causal-Contrasts"]], "Causal Research Question": [[61, "Causal-Research-Question"]], "Causal estimation vs. lasso penalty \\lambda": [[71, "Causal-estimation-vs.-lasso-penalty-\\lambda"]], "Chernozhukov et al. (2023): Benchmarking": [[87, "Chernozhukov-et-al.-(2023):-Benchmarking"]], "Citation": [[138, "citation"]], "Cluster Robust Cross Fitting": [[54, "Cluster-Robust-Cross-Fitting"], [78, "Cluster-Robust-Cross-Fitting"]], "Cluster Robust Standard Errors": [[54, "Cluster-Robust-Standard-Errors"], [78, "Cluster-Robust-Standard-Errors"]], "Clustering and double machine learning": [[54, "Clustering-and-double-machine-learning"], [78, "Clustering-and-double-machine-learning"]], "Combined loss vs. lasso penalty \\lambda": [[71, "Combined-loss-vs.-lasso-penalty-\\lambda"]], "Compare Metrics for Nuisance Estimation": [[77, "Compare-Metrics-for-Nuisance-Estimation"]], "Comparing different learners": [[76, "Comparing-different-learners"]], "Comparison and summary": [[77, "Comparison-and-summary"]], "Comparison to AutoML with less Computation time and Untuned XGBoost Learners": [[77, "Comparison-to-AutoML-with-less-Computation-time-and-Untuned-XGBoost-Learners"]], "Comparison to did package": [[53, "Comparison-to-did-package"]], "Computation time": [[76, "Computation-time"]], "Conclusion": [[77, "Conclusion"]], "Conditional Value at Risk (CVaR)": [[70, "Conditional-Value-at-Risk-(CVaR)"]], "Conditional average treatment effects (CATEs)": [[95, "conditional-average-treatment-effects-cates"]], "Conditional value at risk (CVaR)": [[95, "conditional-value-at-risk-cvar"], [111, "conditional-value-at-risk-cvar"]], "Confidence bands and multiplier bootstrap for valid simultaneous inference": [[126, "confidence-bands-and-multiplier-bootstrap-for-valid-simultaneous-inference"]], "Control Groups": [[60, "Control-Groups"]], "Coverage Simulation": [[58, "Coverage-Simulation"], [58, "id3"]], "Cross-fitting with K folds": [[110, "cross-fitting-with-k-folds"]], "Cross-fitting, DML algorithms and Neyman-orthogonal score functions": [[140, "cross-fitting-dml-algorithms-and-neyman-orthogonal-score-functions"]], "Custom evaluation metrics": [[76, "Custom-evaluation-metrics"]], "DML: Bonus Data": [[63, null]], "Data": [[55, "Data"], [57, "Data"], [57, "id1"], [58, "Data"], [58, "id1"], [60, "Data"], [61, "Data"], [62, "Data"], [68, "Data"], [69, "Data"], [70, "Data"], [72, "Data"], [73, "Data"], [74, "Data"], [75, "Data"], [79, "Data"], [80, "Data"], [82, "Data"], [83, "Data"], [83, "id1"], [86, "Data"], [88, "Data"], [88, "id1"], [140, "data"]], "Data Backend": [[93, null]], "Data Description": [[60, "Data-Description"]], "Data Details": [[60, "Data-Details"]], "Data Generating Process (DGP)": [[52, "Data-Generating-Process-(DGP)"], [65, "Data-Generating-Process-(DGP)"], [67, "Data-Generating-Process-(DGP)"]], "Data Generation": [[77, "Data-Generation"]], "Data Simulation": [[51, "Data-Simulation"], [66, "Data-Simulation"]], "Data and Effect Estimation": [[86, "Data-and-Effect-Estimation"]], "Data generating process": [[90, "data-generating-process"]], "Data preprocessing": [[56, "Data-preprocessing"]], "Data with Anticipation": [[60, "Data-with-Anticipation"]], "Data-Backend for Cluster Data": [[54, "Data-Backend-for-Cluster-Data"], [78, "Data-Backend-for-Cluster-Data"]], "Dataset Generators": [[2, "dataset-generators"]], "Dataset Loaders": [[2, "dataset-loaders"]], "Datasets": [[2, null]], "Define Helper Functions for Plotting": [[54, "Define-Helper-Functions-for-Plotting"], [78, "Define-Helper-Functions-for-Plotting"]], "Demo Example from did": [[53, "Demo-Example-from-did"]], "Details on Predictive Performance": [[53, "Details-on-Predictive-Performance"]], "Difference-in-Differences": [[64, "difference-in-differences"]], "Difference-in-Differences Models": [[111, "difference-in-differences-models"], [127, "difference-in-differences-models"]], "Difference-in-Differences Models (DID)": [[97, "difference-in-differences-models-did"]], "Difference-in-Differences for Panel Data": [[127, "difference-in-differences-for-panel-data"]], "Difference-in-Differences for repeated cross-sections": [[127, "difference-in-differences-for-repeated-cross-sections"]], "Disclaimer": [[87, "Disclaimer"]], "Double Machine Learning Algorithm": [[138, "double-machine-learning-algorithm"]], "Double Machine Learning Literature": [[141, null]], "Double machine learning algorithms": [[89, null]], "Double/debiased machine learning": [[52, "Double/debiased-machine-learning"], [67, "Double/debiased-machine-learning"], [90, "double-debiased-machine-learning"]], "DoubleML": [[138, null]], "DoubleML Data Class": [[1, null]], "DoubleML Models": [[3, null]], "DoubleML Object": [[86, "DoubleML-Object"]], "DoubleML Workflow": [[143, null]], "DoubleML meets FLAML - How to tune learners automatically within DoubleML": [[77, null]], "DoubleMLData": [[93, "doublemldata"]], "DoubleMLData from arrays and matrices": [[91, "doublemldata-from-arrays-and-matrices"], [93, "doublemldata-from-arrays-and-matrices"]], "DoubleMLData from dataframes": [[91, null], [93, "doublemldata-from-dataframes"]], "DoubleMLPanelData": [[60, "DoubleMLPanelData"], [93, "doublemlpaneldata"]], "Effect Aggregation": [[61, "Effect-Aggregation"], [62, "Effect-Aggregation"], [97, "effect-aggregation"]], "Effect Heterogeneity": [[64, "effect-heterogeneity"], [75, "Effect-Heterogeneity"]], "Empirical coverage vs. lasso penalty \\lambda": [[71, "Empirical-coverage-vs.-lasso-penalty-\\lambda"]], "Estimate DML models without sample-splitting": [[110, "estimate-dml-models-without-sample-splitting"]], "Estimate double/debiased machine learning models": [[140, "estimate-double-debiased-machine-learning-models"]], "Estimating Potential Quantiles and Quantile Treatment Effects": [[80, "Estimating-Potential-Quantiles-and-Quantile-Treatment-Effects"]], "Estimating local quantile treatment effects (LQTEs)": [[80, "Estimating-local-quantile-treatment-effects-(LQTEs)"]], "Estimating the Average Treatment Effect of 401(k) Eligibility on Net Financial Assets": [[55, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"], [79, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"]], "Estimating the treatment effect on the Conditional Value a Risk (CVaR)": [[80, "Estimating-the-treatment-effect-on-the-Conditional-Value-a-Risk-(CVaR)"]], "Estimation": [[57, "Estimation"], [57, "id2"], [88, "Estimation"], [88, "id2"]], "Estimation quality vs. \\lambda": [[71, "Estimation-quality-vs.-\\lambda"]], "Evaluate learners": [[96, "evaluate-learners"]], "Event Study Aggregation": [[60, "Event-Study-Aggregation"], [61, "Event-Study-Aggregation"], [62, "Event-Study-Aggregation"]], "Example: Sensitivity Analysis for Causal ML": [[87, null]], "Examples": [[64, null]], "Exploiting the Functionalities of did": [[53, "Exploiting-the-Functionalities-of-did"]], "Externally provide a sample splitting / partition": [[110, "externally-provide-a-sample-splitting-partition"]], "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)": [[84, null]], "Fuzzy RDD": [[84, "Fuzzy-RDD"]], "Fuzzy RDD Without Adjustment": [[84, "Fuzzy-RDD-Without-Adjustment"]], "Fuzzy RDD with Flexible Adjustment": [[84, "Fuzzy-RDD-with-Flexible-Adjustment"]], "Fuzzy RDD with Linear Adjustment": [[84, "Fuzzy-RDD-with-Linear-Adjustment"]], "Fuzzy Regression Discontinuity Design": [[97, "fuzzy-regression-discontinuity-design"]], "GATE Estimation and Sensitivity": [[74, "GATE-Estimation-and-Sensitivity"]], "GATET Estimation and Sensitivity": [[74, "GATET-Estimation-and-Sensitivity"]], "GATEs for IRM models": [[95, "gates-for-irm-models"]], "GATEs for PLR models": [[95, "gates-for-plr-models"]], "General Examples": [[64, "general-examples"]], "General algorithm": [[127, "general-algorithm"]], "Generate Fuzzy Data": [[84, "Generate-Fuzzy-Data"]], "Generate Sharp Data": [[84, "Generate-Sharp-Data"]], "Getting Started": [[140, null]], "Group Aggregation": [[60, "Group-Aggregation"], [62, "Group-Aggregation"]], "Group Average Treatment Effects (GATEs)": [[72, "Group-Average-Treatment-Effects-(GATEs)"], [73, "Group-Average-Treatment-Effects-(GATEs)"]], "Group average treatment effects (GATEs)": [[95, "group-average-treatment-effects-gates"]], "Group-Time Combinations": [[60, "Group-Time-Combinations"]], "Heterogeneous treatment effects": [[95, null]], "How to exploit more features of mlr3pipelines in DoubleML": [[56, "How-to-exploit-more-features-of-mlr3pipelines-in-DoubleML"]], "Hyperparameter tuning": [[96, "hyperparameter-tuning"], [96, "id16"]], "Hyperparameter tuning with pipelines": [[96, "hyperparameter-tuning-with-pipelines"]], "Implementation": [[127, "implementation"]], "Implementation Details": [[97, "implementation-details"]], "Implementation of the double machine learning algorithms": [[89, "implementation-of-the-double-machine-learning-algorithms"]], "Implementation of the score function and the estimate of the causal parameter": [[111, "implementation-of-the-score-function-and-the-estimate-of-the-causal-parameter"]], "Implemented Neyman orthogonal score functions": [[111, "implemented-neyman-orthogonal-score-functions"]], "Initialize DoubleMLClusterData object": [[54, "Initialize-DoubleMLClusterData-object"], [78, "Initialize-DoubleMLClusterData-object"]], "Initialize the objects of class DoubleMLPLIV": [[54, "Initialize-the-objects-of-class-DoubleMLPLIV"], [78, "Initialize-the-objects-of-class-DoubleMLPLIV"]], "Installing DoubleML": [[139, null]], "Instrumental Variables Directed Acyclic Graph (IV - DAG)": [[51, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"], [66, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"]], "Interactive IV Model (IIVM)": [[55, "Interactive-IV-Model-(IIVM)"], [79, "Interactive-IV-Model-(IIVM)"]], "Interactive IV model (IIVM)": [[97, "interactive-iv-model-iivm"], [111, "interactive-iv-model-iivm"]], "Interactive Regression Model (IRM)": [[55, "Interactive-Regression-Model-(IRM)"], [72, "Interactive-Regression-Model-(IRM)"], [79, "Interactive-Regression-Model-(IRM)"], [82, "Interactive-Regression-Model-(IRM)"]], "Interactive regression model (IRM)": [[127, "interactive-regression-model-irm"]], "Interactive regression models (IRM)": [[97, "interactive-regression-models-irm"], [111, "interactive-regression-models-irm"], [127, "interactive-regression-models-irm"]], "Learners and Hyperparameters": [[75, "Learners-and-Hyperparameters"]], "Learners to estimate the nuisance models": [[140, "learners-to-estimate-the-nuisance-models"]], "Learners, hyperparameters and hyperparameter tuning": [[96, null]], "Linear Covariate Adjustment": [[60, "Linear-Covariate-Adjustment"]], "Load Data": [[87, "Load-Data"]], "Load and Process Data": [[54, "Load-and-Process-Data"], [78, "Load-and-Process-Data"]], "Load bonus data using the dml datasets module": [[63, "Load-bonus-data-using-the-dml-datasets-module"]], "Local Average Treatment Effects of 401(k) Participation on Net Financial Assets": [[55, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"], [79, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"]], "Local Potential Quantile Estimation": [[83, "Local-Potential-Quantile-Estimation"]], "Local Potential Quantiles (LPQs)": [[83, "Local-Potential-Quantiles-(LPQs)"]], "Local Quantile Treatment Effects (LQTEs)": [[83, "Local-Quantile-Treatment-Effects-(LQTEs)"]], "Local potential quantiles (LPQs)": [[111, "local-potential-quantiles-lpqs"]], "Main Features": [[138, "main-features"]], "Minimum requirements for learners": [[96, "minimum-requirements-for-learners"], [96, "id2"]], "Missingness at Random": [[97, "missingness-at-random"], [111, "missingness-at-random"]], "Model-specific implementations": [[127, "model-specific-implementations"]], "Models": [[97, null]], "Motivation": [[54, "Motivation"], [78, "Motivation"]], "Multiple Average Potential Outcome Models (APOS)": [[65, "Multiple-Average-Potential-Outcome-Models-(APOS)"]], "Naive estimation": [[51, "Naive-estimation"], [66, "Naive-estimation"]], "No Clustering / Zero-Way Clustering": [[54, "No-Clustering-/-Zero-Way-Clustering"], [78, "No-Clustering-/-Zero-Way-Clustering"]], "Nonignorable Nonresponse": [[97, "nonignorable-nonresponse"], [111, "nonignorable-nonresponse"]], "One-Way Clustering with Respect to the Market": [[54, "One-Way-Clustering-with-Respect-to-the-Market"], [78, "One-Way-Clustering-with-Respect-to-the-Market"]], "One-Way Clustering with Respect to the Product": [[54, "One-Way-Clustering-with-Respect-to-the-Product"], [78, "One-Way-Clustering-with-Respect-to-the-Product"]], "One-dimensional Example": [[68, "One-dimensional-Example"], [69, "One-dimensional-Example"]], "Outcome missing at random (MAR)": [[57, "Outcome-missing-at-random-(MAR)"], [88, "Outcome-missing-at-random-(MAR)"]], "Outcome missing under nonignorable nonresponse": [[57, "Outcome-missing-under-nonignorable-nonresponse"], [88, "Outcome-missing-under-nonignorable-nonresponse"]], "Overcoming regularization bias by orthogonalization": [[52, "Overcoming-regularization-bias-by-orthogonalization"], [67, "Overcoming-regularization-bias-by-orthogonalization"], [90, "overcoming-regularization-bias-by-orthogonalization"]], "Panel Data": [[97, "id9"], [99, null], [111, "panel-data"], [111, "id3"], [127, "panel-data"]], "Panel Data (Repeated Outcomes)": [[58, "Panel-Data-(Repeated-Outcomes)"]], "Panel data": [[97, "panel-data"]], "Parameter tuning": [[56, "Parameter-tuning"]], "Partialling out score": [[52, "Partialling-out-score"], [67, "Partialling-out-score"], [90, "partialling-out-score"]], "Partially Linear Regression Model (PLR)": [[55, "Partially-Linear-Regression-Model-(PLR)"], [73, "Partially-Linear-Regression-Model-(PLR)"], [79, "Partially-Linear-Regression-Model-(PLR)"]], "Partially linear IV regression model (PLIV)": [[97, "partially-linear-iv-regression-model-pliv"], [111, "partially-linear-iv-regression-model-pliv"]], "Partially linear models (PLM)": [[97, "partially-linear-models-plm"], [111, "partially-linear-models-plm"], [127, "partially-linear-models-plm"]], "Partially linear regression model (PLR)": [[97, "partially-linear-regression-model-plr"], [111, "partially-linear-regression-model-plr"], [127, "partially-linear-regression-model-plr"]], "Plot Coefficients and 95% Confidence Intervals": [[77, "Plot-Coefficients-and-95%-Confidence-Intervals"]], "Policy Learning with Trees": [[82, "Policy-Learning-with-Trees"], [95, "policy-learning-with-trees"]], "Potential Quantile Estimation": [[83, "Potential-Quantile-Estimation"]], "Potential Quantiles (PQs)": [[83, "Potential-Quantiles-(PQs)"]], "Potential quantiles (PQs)": [[95, "potential-quantiles-pqs"], [111, "potential-quantiles-pqs"]], "Python: Average Potential Outcome (APO) Models": [[65, null]], "Python: Basic Instrumental Variables calculation": [[66, null]], "Python: Basics of Double Machine Learning": [[67, null]], "Python: Building the package from source": [[139, "python-building-the-package-from-source"]], "Python: Case studies": [[64, "python-case-studies"]], "Python: Choice of learners": [[76, null]], "Python: Cluster Robust Double Machine Learning": [[78, null]], "Python: Conditional Average Treatment Effects (CATEs) for IRM models": [[68, null]], "Python: Conditional Average Treatment Effects (CATEs) for PLR models": [[69, null]], "Python: Conditional Value at Risk of potential outcomes": [[70, null]], "Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments": [[85, null]], "Python: Difference-in-Differences": [[58, null]], "Python: Difference-in-Differences Pre-Testing": [[59, null]], "Python: First Stage and Causal Estimation": [[71, null]], "Python: GATE Sensitivity Analysis": [[74, null]], "Python: Group Average Treatment Effects (GATEs) for IRM models": [[72, null]], "Python: Group Average Treatment Effects (GATEs) for PLR models": [[73, null]], "Python: IRM and APO Model Comparison": [[75, null]], "Python: Impact of 401(k) on Financial Wealth": [[79, null]], "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)": [[80, null]], "Python: Installing DoubleML": [[139, "python-installing-doubleml"]], "Python: Installing a released version from a .whl file": [[139, "python-installing-a-released-version-from-a-whl-file"]], "Python: Installing the latest release from pip or conda": [[139, "python-installing-the-latest-release-from-pip-or-conda"]], "Python: Learners and hyperparameters": [[96, "python-learners-and-hyperparameters"]], "Python: Optional Dependencies": [[139, "python-optional-dependencies"]], "Python: PLM and IRM for Multiple Treatments": [[81, null]], "Python: Panel Data Introduction": [[62, null]], "Python: Panel Data with Multiple Time Periods": [[60, null]], "Python: Policy Learning with Trees": [[82, null]], "Python: Potential Quantiles and Quantile Treatment Effects": [[83, null]], "Python: Real-Data Example for Multi-Period Difference-in-Differences": [[61, null]], "Python: Sample Selection Models": [[88, null]], "Python: Sensitivity Analysis": [[86, null]], "Quantile Treatment Effects (QTEs)": [[83, "Quantile-Treatment-Effects-(QTEs)"]], "Quantile treatment effects (QTEs)": [[95, "quantile-treatment-effects-qtes"]], "Quantiles": [[95, "quantiles"]], "R: Basic Instrumental Variables Calculation": [[51, null]], "R: Basics of Double Machine Learning": [[52, null]], "R: Case studies": [[64, "r-case-studies"]], "R: Cluster Robust Double Machine Learning": [[54, null]], "R: DoubleML for Difference-in-Differences": [[53, null]], "R: Ensemble Learners and More with mlr3pipelines": [[56, null]], "R: Impact of 401(k) on Financial Wealth": [[55, null]], "R: Installing DoubleML": [[139, "r-installing-doubleml"]], "R: Installing the development version from GitHub": [[139, "r-installing-the-development-version-from-github"]], "R: Installing the latest release from CRAN": [[139, "r-installing-the-latest-release-from-cran"]], "R: Learners and hyperparameters": [[96, "r-learners-and-hyperparameters"]], "R: Sample Selection Models": [[57, null]], "Ranking Treatment Effects under Treatment Propensity and Treatment Effect Heterogeneity": [[81, "Ranking-Treatment-Effects-under-Treatment-Propensity-and-Treatment-Effect-Heterogeneity"]], "Real-Data Application": [[54, "Real-Data-Application"], [78, "Real-Data-Application"]], "References": [[51, "References"], [53, "References"], [54, "References"], [56, "References"], [66, "References"], [71, "References"], [76, "References"], [77, "References"], [78, "References"], [81, "References"], [85, "References"], [87, "References"], [90, "references"], [96, "references"], [110, "references"], [126, "references"], [138, "references"], [140, "references"]], "Regression Discontinuity Designs (RDD)": [[97, "regression-discontinuity-designs-rdd"]], "Regularization Bias in Simple ML-Approaches": [[52, "Regularization-Bias-in-Simple-ML-Approaches"], [67, "Regularization-Bias-in-Simple-ML-Approaches"]], "Regularization bias in simple ML-approaches": [[90, "regularization-bias-in-simple-ml-approaches"]], "Release Notes": [[142, null]], "Repeated Cross-Sectional Data": [[58, "Repeated-Cross-Sectional-Data"], [111, "repeated-cross-sectional-data"], [111, "id4"], [127, "repeated-cross-sectional-data"]], "Repeated cross-fitting with K folds and M repetitions": [[110, "repeated-cross-fitting-with-k-folds-and-m-repetitions"]], "Repeated cross-sections": [[97, "repeated-cross-sections"], [97, "id10"], [99, "repeated-cross-sections"]], "Running a small simulation": [[85, "Running-a-small-simulation"]], "Sample Selection Models": [[111, "sample-selection-models"]], "Sample Selection Models (SSM)": [[97, "sample-selection-models-ssm"]], "Sample splitting to remove bias induced by overfitting": [[52, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [67, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [90, "sample-splitting-to-remove-bias-induced-by-overfitting"]], "Sample-splitting without cross-fitting": [[110, "sample-splitting-without-cross-fitting"]], "Sample-splitting, cross-fitting and repeated cross-fitting": [[110, null]], "Sandbox/Archive": [[64, "sandbox-archive"]], "Score Mixin Classes for DoubleML Models": [[49, null]], "Score functions": [[111, null]], "Selected Combinations": [[60, "Selected-Combinations"]], "Sensitivity Analysis": [[60, "Sensitivity-Analysis"], [65, "Sensitivity-Analysis"], [75, "Sensitivity-Analysis"], [86, "Sensitivity-Analysis"], [86, "id1"]], "Sensitivity Analysis with IRM": [[86, "Sensitivity-Analysis-with-IRM"]], "Sensitivity analysis": [[127, null]], "Set up learners based on mlr3pipelines": [[56, "Set-up-learners-based-on-mlr3pipelines"]], "Sharp RDD": [[84, "Sharp-RDD"]], "Sharp RDD Without Adjustment": [[84, "Sharp-RDD-Without-Adjustment"]], "Sharp RDD with Flexible Adjustment": [[84, "Sharp-RDD-with-Flexible-Adjustment"]], "Sharp RDD with Linear Adjustment": [[84, "Sharp-RDD-with-Linear-Adjustment"]], "Sharp Regression Discontinuity Design": [[97, "sharp-regression-discontinuity-design"]], "Simulate two-way cluster data": [[54, "Simulate-two-way-cluster-data"], [78, "Simulate-two-way-cluster-data"]], "Simulation Example": [[86, "Simulation-Example"]], "Simultaneous inference over different DoubleML models (advanced)": [[126, "simultaneous-inference-over-different-doubleml-models-advanced"]], "Single Average Potential Outcome Models (APO)": [[65, "Single-Average-Potential-Outcome-Models-(APO)"]], "Source code and maintenance": [[138, "source-code-and-maintenance"]], "Special Data Types": [[93, "special-data-types"]], "Specify learner and estimate causal parameter: IRM model with Lasso as learner": [[63, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: IRM model with random forest as learner": [[63, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-random-forest-as-learner"]], "Specify learner and estimate causal parameter: PLR model with Lasso as learner": [[63, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: PLR model with random forest as learner": [[63, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-random-forest-as-learner"]], "Specifying alternative score functions via callables": [[111, "specifying-alternative-score-functions-via-callables"]], "Specifying learners and set hyperparameters": [[96, "specifying-learners-and-set-hyperparameters"], [96, "id9"]], "Standard approach": [[76, "Standard-approach"]], "Step 1: Custom API for FLAML Models within DoubleML": [[77, "Step-1:-Custom-API-for-FLAML-Models-within-DoubleML"]], "Step 1: Initialize and Train the AutoML Models:": [[77, "Step-1:-Initialize-and-Train-the-AutoML-Models:"]], "Step 2: Evaluate the Tuned Models": [[77, "Step-2:-Evaluate-the-Tuned-Models"]], "Step 2: Using the API when calling DoubleML\u2019s .fit() Method": [[77, "Step-2:-Using-the-API-when-calling-DoubleML's-.fit()-Method"]], "Step 3: Create and Fit DoubleML Model": [[77, "Step-3:-Create-and-Fit-DoubleML-Model"]], "Summary Figure": [[81, "Summary-Figure"]], "Summary of Results": [[55, "Summary-of-Results"], [79, "Summary-of-Results"]], "The Augmented Inverse Propensity Weighting Model estimates the ATE under arbitrary effect and propensity score heterogeneity": [[81, "The-Augmented-Inverse-Propensity-Weighting-Model-estimates-the-ATE-under-arbitrary-effect-and-propensity-score-heterogeneity"]], "The Data Backend: DoubleMLData": [[55, "The-Data-Backend:-DoubleMLData"], [79, "The-Data-Backend:-DoubleMLData"]], "The DoubleML package": [[55, "The-DoubleML-package"], [79, "The-DoubleML-package"]], "The Partially Linear Model performs overlap weighting": [[81, "The-Partially-Linear-Model-performs-overlap-weighting"]], "The basics of double/debiased machine learning": [[90, null]], "The causal model": [[140, "the-causal-model"]], "The data-backend DoubleMLData": [[140, "the-data-backend-doublemldata"]], "Theory": [[127, "theory"]], "Time Aggregation": [[60, "Time-Aggregation"], [62, "Time-Aggregation"]], "Tuning on the Folds": [[77, "Tuning-on-the-Folds"]], "Tuning on the full Sample": [[77, "Tuning-on-the-full-Sample"]], "Two treatment periods": [[97, "two-treatment-periods"], [111, "two-treatment-periods"], [127, "two-treatment-periods"]], "Two-Dimensional Example": [[68, "Two-Dimensional-Example"], [69, "Two-Dimensional-Example"]], "Two-Way Clustering with Respect to Product and Market": [[54, "Two-Way-Clustering-with-Respect-to-Product-and-Market"], [78, "Two-Way-Clustering-with-Respect-to-Product-and-Market"]], "Untuned (default parameter) XGBoost": [[77, "Untuned-(default-parameter)-XGBoost"]], "Use ensemble learners based on mlr3pipelines": [[56, "Use-ensemble-learners-based-on-mlr3pipelines"]], "User Guide": [[94, null]], "Using DoubleML": [[51, "Using-DoubleML"], [66, "Using-DoubleML"]], "Using ML for DiD: Integrating DoubleML in did": [[53, "Using-ML-for-DiD:-Integrating-DoubleML-in-did"]], "Using learners from mlr3, mlr3learners and mlr3extralearners": [[56, "Using-learners-from-mlr3,-mlr3learners-and-mlr3extralearners"]], "Using pipelines to construct learners": [[96, "using-pipelines-to-construct-learners"]], "Utility Classes": [[50, "utility-classes"]], "Utility Classes and Functions": [[50, null]], "Utility Functions": [[50, "utility-functions"]], "VanderWeele and Arah (2011): Benchmarking": [[87, "VanderWeele-and-Arah-(2011):-Benchmarking"]], "Variance estimation": [[126, "variance-estimation"]], "Variance estimation and confidence intervals": [[126, null]], "Weighted Average Treatment Effects": [[95, "weighted-average-treatment-effects"]], "doubleml.data.DoubleMLClusterData": [[4, null]], "doubleml.data.DoubleMLData": [[5, null]], "doubleml.data.DoubleMLPanelData": [[6, null]], "doubleml.datasets.fetch_401K": [[7, null]], "doubleml.datasets.fetch_bonus": [[8, null]], "doubleml.datasets.make_confounded_irm_data": [[9, null]], "doubleml.datasets.make_confounded_plr_data": [[10, null]], "doubleml.datasets.make_heterogeneous_data": [[11, null]], "doubleml.datasets.make_iivm_data": [[12, null]], "doubleml.datasets.make_irm_data": [[13, null]], "doubleml.datasets.make_irm_data_discrete_treatments": [[14, null]], "doubleml.datasets.make_pliv_CHS2015": [[15, null]], "doubleml.datasets.make_pliv_multiway_cluster_CKMS2021": [[16, null]], "doubleml.datasets.make_plr_CCDDHNR2018": [[17, null]], "doubleml.datasets.make_plr_turrell2018": [[18, null]], "doubleml.datasets.make_ssm_data": [[19, null]], "doubleml.did": [[3, "doubleml-did"]], "doubleml.did.DoubleMLDID": [[20, null]], "doubleml.did.DoubleMLDIDAggregation": [[21, null]], "doubleml.did.DoubleMLDIDBinary": [[22, null]], "doubleml.did.DoubleMLDIDCS": [[23, null]], "doubleml.did.DoubleMLDIDMulti": [[24, null]], "doubleml.did.datasets.make_did_CS2021": [[25, null]], "doubleml.did.datasets.make_did_SZ2020": [[26, null]], "doubleml.double_ml_score_mixins.LinearScoreMixin": [[27, null]], "doubleml.double_ml_score_mixins.NonLinearScoreMixin": [[28, null]], "doubleml.irm": [[3, "doubleml-irm"]], "doubleml.irm.DoubleMLAPO": [[29, null]], "doubleml.irm.DoubleMLAPOS": [[30, null]], "doubleml.irm.DoubleMLCVAR": [[31, null]], "doubleml.irm.DoubleMLIIVM": [[32, null]], "doubleml.irm.DoubleMLIRM": [[33, null]], "doubleml.irm.DoubleMLLPQ": [[34, null]], "doubleml.irm.DoubleMLPQ": [[35, null]], "doubleml.irm.DoubleMLQTE": [[36, null]], "doubleml.irm.DoubleMLSSM": [[37, null]], "doubleml.plm": [[3, "doubleml-plm"]], "doubleml.plm.DoubleMLPLIV": [[38, null]], "doubleml.plm.DoubleMLPLR": [[39, null]], "doubleml.rdd": [[3, "doubleml-rdd"]], "doubleml.rdd.RDFlex": [[40, null]], "doubleml.rdd.datasets.make_simple_rdd_data": [[41, null]], "doubleml.utils.DMLDummyClassifier": [[42, null]], "doubleml.utils.DMLDummyRegressor": [[43, null]], "doubleml.utils.DoubleMLBLP": [[44, null]], "doubleml.utils.DoubleMLPolicyTree": [[45, null]], "doubleml.utils.GlobalClassifier": [[46, null]], "doubleml.utils.GlobalRegressor": [[47, null]], "doubleml.utils.gain_statistics": [[48, null]]}, "docnames": ["api/api", "api/data_class", "api/datasets", "api/dml_models", "api/generated/doubleml.data.DoubleMLClusterData", "api/generated/doubleml.data.DoubleMLData", "api/generated/doubleml.data.DoubleMLPanelData", "api/generated/doubleml.datasets.fetch_401K", "api/generated/doubleml.datasets.fetch_bonus", "api/generated/doubleml.datasets.make_confounded_irm_data", "api/generated/doubleml.datasets.make_confounded_plr_data", "api/generated/doubleml.datasets.make_heterogeneous_data", "api/generated/doubleml.datasets.make_iivm_data", "api/generated/doubleml.datasets.make_irm_data", "api/generated/doubleml.datasets.make_irm_data_discrete_treatments", "api/generated/doubleml.datasets.make_pliv_CHS2015", "api/generated/doubleml.datasets.make_pliv_multiway_cluster_CKMS2021", "api/generated/doubleml.datasets.make_plr_CCDDHNR2018", "api/generated/doubleml.datasets.make_plr_turrell2018", "api/generated/doubleml.datasets.make_ssm_data", "api/generated/doubleml.did.DoubleMLDID", "api/generated/doubleml.did.DoubleMLDIDAggregation", "api/generated/doubleml.did.DoubleMLDIDBinary", "api/generated/doubleml.did.DoubleMLDIDCS", "api/generated/doubleml.did.DoubleMLDIDMulti", "api/generated/doubleml.did.datasets.make_did_CS2021", "api/generated/doubleml.did.datasets.make_did_SZ2020", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin", "api/generated/doubleml.irm.DoubleMLAPO", "api/generated/doubleml.irm.DoubleMLAPOS", "api/generated/doubleml.irm.DoubleMLCVAR", "api/generated/doubleml.irm.DoubleMLIIVM", "api/generated/doubleml.irm.DoubleMLIRM", "api/generated/doubleml.irm.DoubleMLLPQ", "api/generated/doubleml.irm.DoubleMLPQ", "api/generated/doubleml.irm.DoubleMLQTE", "api/generated/doubleml.irm.DoubleMLSSM", "api/generated/doubleml.plm.DoubleMLPLIV", "api/generated/doubleml.plm.DoubleMLPLR", "api/generated/doubleml.rdd.RDFlex", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data", "api/generated/doubleml.utils.DMLDummyClassifier", "api/generated/doubleml.utils.DMLDummyRegressor", "api/generated/doubleml.utils.DoubleMLBLP", "api/generated/doubleml.utils.DoubleMLPolicyTree", "api/generated/doubleml.utils.GlobalClassifier", "api/generated/doubleml.utils.GlobalRegressor", "api/generated/doubleml.utils.gain_statistics", "api/mixins", "api/utility", "examples/R_double_ml_basic_iv", "examples/R_double_ml_basics", "examples/R_double_ml_did", "examples/R_double_ml_multiway_cluster", "examples/R_double_ml_pension", "examples/R_double_ml_pipeline", "examples/R_double_ml_ssm", "examples/did/py_did", "examples/did/py_did_pretest", "examples/did/py_panel", "examples/did/py_panel_data_example", "examples/did/py_panel_simple", "examples/double_ml_bonus_data", "examples/index", "examples/py_double_ml_apo", "examples/py_double_ml_basic_iv", "examples/py_double_ml_basics", "examples/py_double_ml_cate", "examples/py_double_ml_cate_plr", "examples/py_double_ml_cvar", "examples/py_double_ml_firststage", "examples/py_double_ml_gate", "examples/py_double_ml_gate_plr", "examples/py_double_ml_gate_sensitivity", "examples/py_double_ml_irm_vs_apo", "examples/py_double_ml_learner", "examples/py_double_ml_meets_flaml", "examples/py_double_ml_multiway_cluster", "examples/py_double_ml_pension", "examples/py_double_ml_pension_qte", "examples/py_double_ml_plm_irm_hetfx", "examples/py_double_ml_policy_tree", "examples/py_double_ml_pq", "examples/py_double_ml_rdflex", "examples/py_double_ml_robust_iv", "examples/py_double_ml_sensitivity", "examples/py_double_ml_sensitivity_booking", "examples/py_double_ml_ssm", "guide/algorithms", "guide/basics", "guide/data/base_data", "guide/data/panel_data", "guide/data_backend", "guide/guide", "guide/heterogeneity", "guide/learners", "guide/models", "guide/models/did/did_aggregation", "guide/models/did/did_binary", "guide/models/did/did_cs", "guide/models/did/did_pa", "guide/models/did/did_setup", "guide/models/irm/apo", "guide/models/irm/apos", "guide/models/irm/iivm", "guide/models/irm/irm", "guide/models/plm/pliv", "guide/models/plm/plr", "guide/models/ssm/ssm", "guide/resampling", "guide/scores", "guide/scores/did/did_cs_binary_score", "guide/scores/did/did_cs_score", "guide/scores/did/did_pa_binary_score", "guide/scores/did/did_pa_score", "guide/scores/irm/apo_score", "guide/scores/irm/cvar_score", "guide/scores/irm/iivm_score", "guide/scores/irm/irm_score", "guide/scores/irm/lpq_score", "guide/scores/irm/pq_score", "guide/scores/plm/pliv_score", "guide/scores/plm/plr_score", "guide/scores/ssm/mar_score", "guide/scores/ssm/nr_score", "guide/se_confint", "guide/sensitivity", "guide/sensitivity/benchmarking", "guide/sensitivity/did/did_cs_binary_sensitivity", "guide/sensitivity/did/did_cs_sensitivity", "guide/sensitivity/did/did_pa_binary_sensitivity", "guide/sensitivity/did/did_pa_sensitivity", "guide/sensitivity/implementation", "guide/sensitivity/irm/apo_sensitivity", "guide/sensitivity/irm/irm_sensitivity", "guide/sensitivity/plm/plr_sensitivity", "guide/sensitivity/theory", "index", "intro/install", "intro/intro", "literature/literature", "release/release", "workflow/workflow"], "envversion": {"nbsphinx": 4, "sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1}, "filenames": ["api/api.rst", "api/data_class.rst", "api/datasets.rst", "api/dml_models.rst", "api/generated/doubleml.data.DoubleMLClusterData.rst", "api/generated/doubleml.data.DoubleMLData.rst", "api/generated/doubleml.data.DoubleMLPanelData.rst", "api/generated/doubleml.datasets.fetch_401K.rst", "api/generated/doubleml.datasets.fetch_bonus.rst", "api/generated/doubleml.datasets.make_confounded_irm_data.rst", "api/generated/doubleml.datasets.make_confounded_plr_data.rst", "api/generated/doubleml.datasets.make_heterogeneous_data.rst", "api/generated/doubleml.datasets.make_iivm_data.rst", "api/generated/doubleml.datasets.make_irm_data.rst", "api/generated/doubleml.datasets.make_irm_data_discrete_treatments.rst", "api/generated/doubleml.datasets.make_pliv_CHS2015.rst", "api/generated/doubleml.datasets.make_pliv_multiway_cluster_CKMS2021.rst", "api/generated/doubleml.datasets.make_plr_CCDDHNR2018.rst", "api/generated/doubleml.datasets.make_plr_turrell2018.rst", "api/generated/doubleml.datasets.make_ssm_data.rst", "api/generated/doubleml.did.DoubleMLDID.rst", "api/generated/doubleml.did.DoubleMLDIDAggregation.rst", "api/generated/doubleml.did.DoubleMLDIDBinary.rst", "api/generated/doubleml.did.DoubleMLDIDCS.rst", "api/generated/doubleml.did.DoubleMLDIDMulti.rst", "api/generated/doubleml.did.datasets.make_did_CS2021.rst", "api/generated/doubleml.did.datasets.make_did_SZ2020.rst", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin.rst", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin.rst", "api/generated/doubleml.irm.DoubleMLAPO.rst", "api/generated/doubleml.irm.DoubleMLAPOS.rst", "api/generated/doubleml.irm.DoubleMLCVAR.rst", "api/generated/doubleml.irm.DoubleMLIIVM.rst", "api/generated/doubleml.irm.DoubleMLIRM.rst", "api/generated/doubleml.irm.DoubleMLLPQ.rst", "api/generated/doubleml.irm.DoubleMLPQ.rst", "api/generated/doubleml.irm.DoubleMLQTE.rst", "api/generated/doubleml.irm.DoubleMLSSM.rst", "api/generated/doubleml.plm.DoubleMLPLIV.rst", "api/generated/doubleml.plm.DoubleMLPLR.rst", "api/generated/doubleml.rdd.RDFlex.rst", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data.rst", "api/generated/doubleml.utils.DMLDummyClassifier.rst", "api/generated/doubleml.utils.DMLDummyRegressor.rst", "api/generated/doubleml.utils.DoubleMLBLP.rst", "api/generated/doubleml.utils.DoubleMLPolicyTree.rst", "api/generated/doubleml.utils.GlobalClassifier.rst", "api/generated/doubleml.utils.GlobalRegressor.rst", "api/generated/doubleml.utils.gain_statistics.rst", "api/mixins.rst", "api/utility.rst", "examples/R_double_ml_basic_iv.ipynb", "examples/R_double_ml_basics.ipynb", "examples/R_double_ml_did.ipynb", "examples/R_double_ml_multiway_cluster.ipynb", "examples/R_double_ml_pension.ipynb", "examples/R_double_ml_pipeline.ipynb", "examples/R_double_ml_ssm.ipynb", "examples/did/py_did.ipynb", "examples/did/py_did_pretest.ipynb", "examples/did/py_panel.ipynb", "examples/did/py_panel_data_example.ipynb", "examples/did/py_panel_simple.ipynb", "examples/double_ml_bonus_data.ipynb", "examples/index.rst", "examples/py_double_ml_apo.ipynb", "examples/py_double_ml_basic_iv.ipynb", "examples/py_double_ml_basics.ipynb", "examples/py_double_ml_cate.ipynb", "examples/py_double_ml_cate_plr.ipynb", "examples/py_double_ml_cvar.ipynb", "examples/py_double_ml_firststage.ipynb", "examples/py_double_ml_gate.ipynb", "examples/py_double_ml_gate_plr.ipynb", "examples/py_double_ml_gate_sensitivity.ipynb", "examples/py_double_ml_irm_vs_apo.ipynb", "examples/py_double_ml_learner.ipynb", "examples/py_double_ml_meets_flaml.ipynb", "examples/py_double_ml_multiway_cluster.ipynb", "examples/py_double_ml_pension.ipynb", "examples/py_double_ml_pension_qte.ipynb", "examples/py_double_ml_plm_irm_hetfx.ipynb", "examples/py_double_ml_policy_tree.ipynb", "examples/py_double_ml_pq.ipynb", "examples/py_double_ml_rdflex.ipynb", "examples/py_double_ml_robust_iv.ipynb", "examples/py_double_ml_sensitivity.ipynb", "examples/py_double_ml_sensitivity_booking.ipynb", "examples/py_double_ml_ssm.ipynb", "guide/algorithms.rst", "guide/basics.rst", "guide/data/base_data.rst", "guide/data/panel_data.rst", "guide/data_backend.rst", "guide/guide.rst", "guide/heterogeneity.rst", "guide/learners.rst", "guide/models.rst", "guide/models/did/did_aggregation.rst", "guide/models/did/did_binary.rst", "guide/models/did/did_cs.rst", "guide/models/did/did_pa.rst", "guide/models/did/did_setup.rst", "guide/models/irm/apo.rst", "guide/models/irm/apos.rst", "guide/models/irm/iivm.rst", "guide/models/irm/irm.rst", "guide/models/plm/pliv.rst", "guide/models/plm/plr.rst", "guide/models/ssm/ssm.rst", "guide/resampling.rst", "guide/scores.rst", "guide/scores/did/did_cs_binary_score.rst", "guide/scores/did/did_cs_score.rst", "guide/scores/did/did_pa_binary_score.rst", "guide/scores/did/did_pa_score.rst", "guide/scores/irm/apo_score.rst", "guide/scores/irm/cvar_score.rst", "guide/scores/irm/iivm_score.rst", "guide/scores/irm/irm_score.rst", "guide/scores/irm/lpq_score.rst", "guide/scores/irm/pq_score.rst", "guide/scores/plm/pliv_score.rst", "guide/scores/plm/plr_score.rst", "guide/scores/ssm/mar_score.rst", "guide/scores/ssm/nr_score.rst", "guide/se_confint.rst", "guide/sensitivity.rst", "guide/sensitivity/benchmarking.rst", "guide/sensitivity/did/did_cs_binary_sensitivity.rst", "guide/sensitivity/did/did_cs_sensitivity.rst", "guide/sensitivity/did/did_pa_binary_sensitivity.rst", "guide/sensitivity/did/did_pa_sensitivity.rst", "guide/sensitivity/implementation.rst", "guide/sensitivity/irm/apo_sensitivity.rst", "guide/sensitivity/irm/irm_sensitivity.rst", "guide/sensitivity/plm/plr_sensitivity.rst", "guide/sensitivity/theory.rst", "index.rst", "intro/install.rst", "intro/intro.rst", "literature/literature.rst", "release/release.rst", "workflow/workflow.rst"], "indexentries": {"aggregate() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.aggregate", false]], "aggregate_over_splits() (doubleml.rdd.rdflex method)": [[40, "doubleml.rdd.RDFlex.aggregate_over_splits", false]], "bootstrap() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.bootstrap", false]], "bootstrap() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.bootstrap", false]], "capo() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.capo", false]], "cate() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.cate", false]], "cate() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.cate", false]], "causal_contrast() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.causal_contrast", false]], "confint() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.confint", false]], "confint() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.confint", false]], "confint() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.confint", false]], "confint() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.confint", false]], "confint() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.confint", false]], "confint() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.confint", false]], "confint() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.confint", false]], "confint() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.confint", false]], "confint() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.confint", false]], "confint() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.confint", false]], "confint() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.confint", false]], "confint() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.confint", false]], "confint() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.confint", false]], "confint() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.confint", false]], "confint() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.confint", false]], "confint() (doubleml.rdd.rdflex method)": [[40, "doubleml.rdd.RDFlex.confint", false]], "confint() (doubleml.utils.doublemlblp method)": [[44, "doubleml.utils.DoubleMLBLP.confint", false]], "construct_framework() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.construct_framework", false]], "construct_framework() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.construct_framework", false]], "dmldummyclassifier (class in doubleml.utils)": [[42, "doubleml.utils.DMLDummyClassifier", false]], "dmldummyregressor (class in doubleml.utils)": [[43, "doubleml.utils.DMLDummyRegressor", false]], "doublemlapo (class in doubleml.irm)": [[29, "doubleml.irm.DoubleMLAPO", false]], "doublemlapos (class in doubleml.irm)": [[30, "doubleml.irm.DoubleMLAPOS", false]], "doublemlblp (class in doubleml.utils)": [[44, "doubleml.utils.DoubleMLBLP", false]], "doublemlclusterdata (class in doubleml.data)": [[4, "doubleml.data.DoubleMLClusterData", false]], "doublemlcvar (class in doubleml.irm)": [[31, "doubleml.irm.DoubleMLCVAR", false]], "doublemldata (class in doubleml.data)": [[5, "doubleml.data.DoubleMLData", false]], "doublemldid (class in doubleml.did)": [[20, "doubleml.did.DoubleMLDID", false]], "doublemldidaggregation (class in doubleml.did)": [[21, "doubleml.did.DoubleMLDIDAggregation", false]], "doublemldidbinary (class in doubleml.did)": [[22, "doubleml.did.DoubleMLDIDBinary", false]], "doublemldidcs (class in doubleml.did)": [[23, "doubleml.did.DoubleMLDIDCS", false]], "doublemldidmulti (class in doubleml.did)": [[24, "doubleml.did.DoubleMLDIDMulti", false]], "doublemliivm (class in doubleml.irm)": [[32, "doubleml.irm.DoubleMLIIVM", false]], "doublemlirm (class in doubleml.irm)": [[33, "doubleml.irm.DoubleMLIRM", false]], "doublemllpq (class in doubleml.irm)": [[34, "doubleml.irm.DoubleMLLPQ", false]], "doublemlpaneldata (class in doubleml.data)": [[6, "doubleml.data.DoubleMLPanelData", false]], "doublemlpliv (class in doubleml.plm)": [[38, "doubleml.plm.DoubleMLPLIV", false]], "doublemlplr (class in doubleml.plm)": [[39, "doubleml.plm.DoubleMLPLR", false]], "doublemlpolicytree (class in doubleml.utils)": [[45, "doubleml.utils.DoubleMLPolicyTree", false]], "doublemlpq (class in doubleml.irm)": [[35, "doubleml.irm.DoubleMLPQ", false]], "doublemlqte (class in doubleml.irm)": [[36, "doubleml.irm.DoubleMLQTE", false]], "doublemlssm (class in doubleml.irm)": [[37, "doubleml.irm.DoubleMLSSM", false]], "draw_sample_splitting() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.draw_sample_splitting", false]], "evaluate_learners() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.evaluate_learners", false]], "fetch_401k() (in module doubleml.datasets)": [[7, "doubleml.datasets.fetch_401K", false]], "fetch_bonus() (in module doubleml.datasets)": [[8, "doubleml.datasets.fetch_bonus", false]], "fit() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.fit", false]], "fit() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.fit", false]], "fit() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.fit", false]], "fit() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.fit", false]], "fit() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.fit", false]], "fit() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.fit", false]], "fit() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.fit", false]], "fit() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.fit", false]], "fit() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.fit", false]], "fit() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.fit", false]], "fit() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.fit", false]], "fit() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.fit", false]], "fit() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.fit", false]], "fit() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.fit", false]], "fit() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.fit", false]], "fit() (doubleml.rdd.rdflex method)": [[40, "doubleml.rdd.RDFlex.fit", false]], "fit() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.fit", false]], "fit() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.fit", false]], "fit() (doubleml.utils.doublemlblp method)": [[44, "doubleml.utils.DoubleMLBLP.fit", false]], "fit() (doubleml.utils.doublemlpolicytree method)": [[45, "doubleml.utils.DoubleMLPolicyTree.fit", false]], "fit() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.fit", false]], "fit() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.fit", false]], "from_arrays() (doubleml.data.doublemlclusterdata class method)": [[4, "doubleml.data.DoubleMLClusterData.from_arrays", false]], "from_arrays() (doubleml.data.doublemldata class method)": [[5, "doubleml.data.DoubleMLData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlpaneldata class method)": [[6, "doubleml.data.DoubleMLPanelData.from_arrays", false]], "gain_statistics() (in module doubleml.utils)": [[48, "doubleml.utils.gain_statistics", false]], "gapo() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.gapo", false]], "gate() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.gate", false]], "gate() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.gate", false]], "get_metadata_routing() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.get_metadata_routing", false]], "get_params() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.get_params", false]], "get_params() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.get_params", false]], "get_params() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.get_params", false]], "get_params() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.get_params", false]], "get_params() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.get_params", false]], "get_params() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.get_params", false]], "get_params() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.get_params", false]], "get_params() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.get_params", false]], "get_params() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.get_params", false]], "get_params() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.get_params", false]], "get_params() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.get_params", false]], "get_params() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.get_params", false]], "get_params() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.get_params", false]], "get_params() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.get_params", false]], "globalclassifier (class in doubleml.utils)": [[46, "doubleml.utils.GlobalClassifier", false]], "globalregressor (class in doubleml.utils)": [[47, "doubleml.utils.GlobalRegressor", false]], "linearscoremixin (class in doubleml.double_ml_score_mixins)": [[27, "doubleml.double_ml_score_mixins.LinearScoreMixin", false]], "make_confounded_irm_data() (in module doubleml.datasets)": [[9, "doubleml.datasets.make_confounded_irm_data", false]], "make_confounded_plr_data() (in module doubleml.datasets)": [[10, "doubleml.datasets.make_confounded_plr_data", false]], "make_did_cs2021() (in module doubleml.did.datasets)": [[25, "doubleml.did.datasets.make_did_CS2021", false]], "make_did_sz2020() (in module doubleml.did.datasets)": [[26, "doubleml.did.datasets.make_did_SZ2020", false]], "make_heterogeneous_data() (in module doubleml.datasets)": [[11, "doubleml.datasets.make_heterogeneous_data", false]], "make_iivm_data() (in module doubleml.datasets)": [[12, "doubleml.datasets.make_iivm_data", false]], "make_irm_data() (in module doubleml.datasets)": [[13, "doubleml.datasets.make_irm_data", false]], "make_irm_data_discrete_treatments() (in module doubleml.datasets)": [[14, "doubleml.datasets.make_irm_data_discrete_treatments", false]], "make_pliv_chs2015() (in module doubleml.datasets)": [[15, "doubleml.datasets.make_pliv_CHS2015", false]], "make_pliv_multiway_cluster_ckms2021() (in module doubleml.datasets)": [[16, "doubleml.datasets.make_pliv_multiway_cluster_CKMS2021", false]], "make_plr_ccddhnr2018() (in module doubleml.datasets)": [[17, "doubleml.datasets.make_plr_CCDDHNR2018", false]], "make_plr_turrell2018() (in module doubleml.datasets)": [[18, "doubleml.datasets.make_plr_turrell2018", false]], "make_simple_rdd_data() (in module doubleml.rdd.datasets)": [[41, "doubleml.rdd.datasets.make_simple_rdd_data", false]], "make_ssm_data() (in module doubleml.datasets)": [[19, "doubleml.datasets.make_ssm_data", false]], "nonlinearscoremixin (class in doubleml.double_ml_score_mixins)": [[28, "doubleml.double_ml_score_mixins.NonLinearScoreMixin", false]], "p_adjust() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.p_adjust", false]], "p_adjust() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.p_adjust", false]], "plot_effects() (doubleml.did.doublemldidaggregation method)": [[21, "doubleml.did.DoubleMLDIDAggregation.plot_effects", false]], "plot_effects() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.plot_effects", false]], "plot_tree() (doubleml.utils.doublemlpolicytree method)": [[45, "doubleml.utils.DoubleMLPolicyTree.plot_tree", false]], "policy_tree() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.policy_tree", false]], "predict() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.predict", false]], "predict() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.predict", false]], "predict() (doubleml.utils.doublemlpolicytree method)": [[45, "doubleml.utils.DoubleMLPolicyTree.predict", false]], "predict() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.predict", false]], "predict() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.predict", false]], "predict_proba() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.predict_proba", false]], "predict_proba() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.predict_proba", false]], "rdflex (class in doubleml.rdd)": [[40, "doubleml.rdd.RDFlex", false]], "robust_confset() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.robust_confset", false]], "score() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.score", false]], "score() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.score", false]], "score() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.score", false]], "score() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.score", false]], "sensitivity_analysis() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_analysis", false]], "sensitivity_benchmark() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_benchmark", false]], "sensitivity_plot() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_plot", false]], "set_fit_request() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.set_fit_request", false]], "set_fit_request() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.set_fit_request", false]], "set_ml_nuisance_params() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.set_ml_nuisance_params", false]], "set_params() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.set_params", false]], "set_params() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.set_params", false]], "set_params() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.set_params", false]], "set_params() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.set_params", false]], "set_sample_splitting() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.set_sample_splitting", false]], "set_score_request() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.set_score_request", false]], "set_score_request() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.set_score_request", false]], "set_x_d() (doubleml.data.doublemlclusterdata method)": [[4, "doubleml.data.DoubleMLClusterData.set_x_d", false]], "set_x_d() (doubleml.data.doublemldata method)": [[5, "doubleml.data.DoubleMLData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlpaneldata method)": [[6, "doubleml.data.DoubleMLPanelData.set_x_d", false]], "tune() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.tune", false]], "tune() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.tune", false]], "tune() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.tune", false]], "tune() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.tune", false]], "tune() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.tune", false]], "tune() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.tune", false]], "tune() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.tune", false]], "tune() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.tune", false]], "tune() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.tune", false]], "tune() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.tune", false]], "tune() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.tune", false]], "tune() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.tune", false]]}, "objects": {"doubleml.data": [[4, 0, 1, "", "DoubleMLClusterData"], [5, 0, 1, "", "DoubleMLData"], [6, 0, 1, "", "DoubleMLPanelData"]], "doubleml.data.DoubleMLClusterData": [[4, 1, 1, "", "from_arrays"], [4, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLData": [[5, 1, 1, "", "from_arrays"], [5, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLPanelData": [[6, 1, 1, "", "from_arrays"], [6, 1, 1, "", "set_x_d"]], "doubleml.datasets": [[7, 2, 1, "", "fetch_401K"], [8, 2, 1, "", "fetch_bonus"], [9, 2, 1, "", "make_confounded_irm_data"], [10, 2, 1, "", "make_confounded_plr_data"], [11, 2, 1, "", "make_heterogeneous_data"], [12, 2, 1, "", "make_iivm_data"], [13, 2, 1, "", "make_irm_data"], [14, 2, 1, "", "make_irm_data_discrete_treatments"], [15, 2, 1, "", "make_pliv_CHS2015"], [16, 2, 1, "", "make_pliv_multiway_cluster_CKMS2021"], [17, 2, 1, "", "make_plr_CCDDHNR2018"], [18, 2, 1, "", "make_plr_turrell2018"], [19, 2, 1, "", "make_ssm_data"]], "doubleml.did": [[20, 0, 1, "", "DoubleMLDID"], [21, 0, 1, "", "DoubleMLDIDAggregation"], [22, 0, 1, "", "DoubleMLDIDBinary"], [23, 0, 1, "", "DoubleMLDIDCS"], [24, 0, 1, "", "DoubleMLDIDMulti"]], "doubleml.did.DoubleMLDID": [[20, 1, 1, "", "bootstrap"], [20, 1, 1, "", "confint"], [20, 1, 1, "", "construct_framework"], [20, 1, 1, "", "draw_sample_splitting"], [20, 1, 1, "", "evaluate_learners"], [20, 1, 1, "", "fit"], [20, 1, 1, "", "get_params"], [20, 1, 1, "", "p_adjust"], [20, 1, 1, "", "sensitivity_analysis"], [20, 1, 1, "", "sensitivity_benchmark"], [20, 1, 1, "", "sensitivity_plot"], [20, 1, 1, "", "set_ml_nuisance_params"], [20, 1, 1, "", "set_sample_splitting"], [20, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDAggregation": [[21, 1, 1, "", "plot_effects"]], "doubleml.did.DoubleMLDIDBinary": [[22, 1, 1, "", "bootstrap"], [22, 1, 1, "", "confint"], [22, 1, 1, "", "construct_framework"], [22, 1, 1, "", "draw_sample_splitting"], [22, 1, 1, "", "evaluate_learners"], [22, 1, 1, "", "fit"], [22, 1, 1, "", "get_params"], [22, 1, 1, "", "p_adjust"], [22, 1, 1, "", "sensitivity_analysis"], [22, 1, 1, "", "sensitivity_benchmark"], [22, 1, 1, "", "sensitivity_plot"], [22, 1, 1, "", "set_ml_nuisance_params"], [22, 1, 1, "", "set_sample_splitting"], [22, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDCS": [[23, 1, 1, "", "bootstrap"], [23, 1, 1, "", "confint"], [23, 1, 1, "", "construct_framework"], [23, 1, 1, "", "draw_sample_splitting"], [23, 1, 1, "", "evaluate_learners"], [23, 1, 1, "", "fit"], [23, 1, 1, "", "get_params"], [23, 1, 1, "", "p_adjust"], [23, 1, 1, "", "sensitivity_analysis"], [23, 1, 1, "", "sensitivity_benchmark"], [23, 1, 1, "", "sensitivity_plot"], [23, 1, 1, "", "set_ml_nuisance_params"], [23, 1, 1, "", "set_sample_splitting"], [23, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDMulti": [[24, 1, 1, "", "aggregate"], [24, 1, 1, "", "bootstrap"], [24, 1, 1, "", "confint"], [24, 1, 1, "", "fit"], [24, 1, 1, "", "p_adjust"], [24, 1, 1, "", "plot_effects"], [24, 1, 1, "", "sensitivity_analysis"], [24, 1, 1, "", "sensitivity_benchmark"], [24, 1, 1, "", "sensitivity_plot"]], "doubleml.did.datasets": [[25, 2, 1, "", "make_did_CS2021"], [26, 2, 1, "", "make_did_SZ2020"]], "doubleml.double_ml_score_mixins": [[27, 0, 1, "", "LinearScoreMixin"], [28, 0, 1, "", "NonLinearScoreMixin"]], "doubleml.irm": [[29, 0, 1, "", "DoubleMLAPO"], [30, 0, 1, "", "DoubleMLAPOS"], [31, 0, 1, "", "DoubleMLCVAR"], [32, 0, 1, "", "DoubleMLIIVM"], [33, 0, 1, "", "DoubleMLIRM"], [34, 0, 1, "", "DoubleMLLPQ"], [35, 0, 1, "", "DoubleMLPQ"], [36, 0, 1, "", "DoubleMLQTE"], [37, 0, 1, "", "DoubleMLSSM"]], "doubleml.irm.DoubleMLAPO": [[29, 1, 1, "", "bootstrap"], [29, 1, 1, "", "capo"], [29, 1, 1, "", "confint"], [29, 1, 1, "", "construct_framework"], [29, 1, 1, "", "draw_sample_splitting"], [29, 1, 1, "", "evaluate_learners"], [29, 1, 1, "", "fit"], [29, 1, 1, "", "gapo"], [29, 1, 1, "", "get_params"], [29, 1, 1, "", "p_adjust"], [29, 1, 1, "", "sensitivity_analysis"], [29, 1, 1, "", "sensitivity_benchmark"], [29, 1, 1, "", "sensitivity_plot"], [29, 1, 1, "", "set_ml_nuisance_params"], [29, 1, 1, "", "set_sample_splitting"], [29, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLAPOS": [[30, 1, 1, "", "bootstrap"], [30, 1, 1, "", "causal_contrast"], [30, 1, 1, "", "confint"], [30, 1, 1, "", "draw_sample_splitting"], [30, 1, 1, "", "fit"], [30, 1, 1, "", "sensitivity_analysis"], [30, 1, 1, "", "sensitivity_benchmark"], [30, 1, 1, "", "sensitivity_plot"], [30, 1, 1, "", "set_sample_splitting"]], "doubleml.irm.DoubleMLCVAR": [[31, 1, 1, "", "bootstrap"], [31, 1, 1, "", "confint"], [31, 1, 1, "", "construct_framework"], [31, 1, 1, "", "draw_sample_splitting"], [31, 1, 1, "", "evaluate_learners"], [31, 1, 1, "", "fit"], [31, 1, 1, "", "get_params"], [31, 1, 1, "", "p_adjust"], [31, 1, 1, "", "sensitivity_analysis"], [31, 1, 1, "", "sensitivity_benchmark"], [31, 1, 1, "", "sensitivity_plot"], [31, 1, 1, "", "set_ml_nuisance_params"], [31, 1, 1, "", "set_sample_splitting"], [31, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLIIVM": [[32, 1, 1, "", "bootstrap"], [32, 1, 1, "", "confint"], [32, 1, 1, "", "construct_framework"], [32, 1, 1, "", "draw_sample_splitting"], [32, 1, 1, "", "evaluate_learners"], [32, 1, 1, "", "fit"], [32, 1, 1, "", "get_params"], [32, 1, 1, "", "p_adjust"], [32, 1, 1, "", "robust_confset"], [32, 1, 1, "", "sensitivity_analysis"], [32, 1, 1, "", "sensitivity_benchmark"], [32, 1, 1, "", "sensitivity_plot"], [32, 1, 1, "", "set_ml_nuisance_params"], [32, 1, 1, "", "set_sample_splitting"], [32, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLIRM": [[33, 1, 1, "", "bootstrap"], [33, 1, 1, "", "cate"], [33, 1, 1, "", "confint"], [33, 1, 1, "", "construct_framework"], [33, 1, 1, "", "draw_sample_splitting"], [33, 1, 1, "", "evaluate_learners"], [33, 1, 1, "", "fit"], [33, 1, 1, "", "gate"], [33, 1, 1, "", "get_params"], [33, 1, 1, "", "p_adjust"], [33, 1, 1, "", "policy_tree"], [33, 1, 1, "", "sensitivity_analysis"], [33, 1, 1, "", "sensitivity_benchmark"], [33, 1, 1, "", "sensitivity_plot"], [33, 1, 1, "", "set_ml_nuisance_params"], [33, 1, 1, "", "set_sample_splitting"], [33, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLLPQ": [[34, 1, 1, "", "bootstrap"], [34, 1, 1, "", "confint"], [34, 1, 1, "", "construct_framework"], [34, 1, 1, "", "draw_sample_splitting"], [34, 1, 1, "", "evaluate_learners"], [34, 1, 1, "", "fit"], [34, 1, 1, "", "get_params"], [34, 1, 1, "", "p_adjust"], [34, 1, 1, "", "sensitivity_analysis"], [34, 1, 1, "", "sensitivity_benchmark"], [34, 1, 1, "", "sensitivity_plot"], [34, 1, 1, "", "set_ml_nuisance_params"], [34, 1, 1, "", "set_sample_splitting"], [34, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLPQ": [[35, 1, 1, "", "bootstrap"], [35, 1, 1, "", "confint"], [35, 1, 1, "", "construct_framework"], [35, 1, 1, "", "draw_sample_splitting"], [35, 1, 1, "", "evaluate_learners"], [35, 1, 1, "", "fit"], [35, 1, 1, "", "get_params"], [35, 1, 1, "", "p_adjust"], [35, 1, 1, "", "sensitivity_analysis"], [35, 1, 1, "", "sensitivity_benchmark"], [35, 1, 1, "", "sensitivity_plot"], [35, 1, 1, "", "set_ml_nuisance_params"], [35, 1, 1, "", "set_sample_splitting"], [35, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLQTE": [[36, 1, 1, "", "bootstrap"], [36, 1, 1, "", "confint"], [36, 1, 1, "", "draw_sample_splitting"], [36, 1, 1, "", "fit"], [36, 1, 1, "", "p_adjust"], [36, 1, 1, "", "set_sample_splitting"]], "doubleml.irm.DoubleMLSSM": [[37, 1, 1, "", "bootstrap"], [37, 1, 1, "", "confint"], [37, 1, 1, "", "construct_framework"], [37, 1, 1, "", "draw_sample_splitting"], [37, 1, 1, "", "evaluate_learners"], [37, 1, 1, "", "fit"], [37, 1, 1, "", "get_params"], [37, 1, 1, "", "p_adjust"], [37, 1, 1, "", "sensitivity_analysis"], [37, 1, 1, "", "sensitivity_benchmark"], [37, 1, 1, "", "sensitivity_plot"], [37, 1, 1, "", "set_ml_nuisance_params"], [37, 1, 1, "", "set_sample_splitting"], [37, 1, 1, "", "tune"]], "doubleml.plm": [[38, 0, 1, "", "DoubleMLPLIV"], [39, 0, 1, "", "DoubleMLPLR"]], "doubleml.plm.DoubleMLPLIV": [[38, 1, 1, "", "bootstrap"], [38, 1, 1, "", "confint"], [38, 1, 1, "", "construct_framework"], [38, 1, 1, "", "draw_sample_splitting"], [38, 1, 1, "", "evaluate_learners"], [38, 1, 1, "", "fit"], [38, 1, 1, "", "get_params"], [38, 1, 1, "", "p_adjust"], [38, 1, 1, "", "sensitivity_analysis"], [38, 1, 1, "", "sensitivity_benchmark"], [38, 1, 1, "", "sensitivity_plot"], [38, 1, 1, "", "set_ml_nuisance_params"], [38, 1, 1, "", "set_sample_splitting"], [38, 1, 1, "", "tune"]], "doubleml.plm.DoubleMLPLR": [[39, 1, 1, "", "bootstrap"], [39, 1, 1, "", "cate"], [39, 1, 1, "", "confint"], [39, 1, 1, "", "construct_framework"], [39, 1, 1, "", "draw_sample_splitting"], [39, 1, 1, "", "evaluate_learners"], [39, 1, 1, "", "fit"], [39, 1, 1, "", "gate"], [39, 1, 1, "", "get_params"], [39, 1, 1, "", "p_adjust"], [39, 1, 1, "", "sensitivity_analysis"], [39, 1, 1, "", "sensitivity_benchmark"], [39, 1, 1, "", "sensitivity_plot"], [39, 1, 1, "", "set_ml_nuisance_params"], [39, 1, 1, "", "set_sample_splitting"], [39, 1, 1, "", "tune"]], "doubleml.rdd": [[40, 0, 1, "", "RDFlex"]], "doubleml.rdd.RDFlex": [[40, 1, 1, "", "aggregate_over_splits"], [40, 1, 1, "", "confint"], [40, 1, 1, "", "fit"]], "doubleml.rdd.datasets": [[41, 2, 1, "", "make_simple_rdd_data"]], "doubleml.utils": [[42, 0, 1, "", "DMLDummyClassifier"], [43, 0, 1, "", "DMLDummyRegressor"], [44, 0, 1, "", "DoubleMLBLP"], [45, 0, 1, "", "DoubleMLPolicyTree"], [46, 0, 1, "", "GlobalClassifier"], [47, 0, 1, "", "GlobalRegressor"], [48, 2, 1, "", "gain_statistics"]], "doubleml.utils.DMLDummyClassifier": [[42, 1, 1, "", "fit"], [42, 1, 1, "", "get_metadata_routing"], [42, 1, 1, "", "get_params"], [42, 1, 1, "", "predict"], [42, 1, 1, "", "predict_proba"], [42, 1, 1, "", "score"], [42, 1, 1, "", "set_params"], [42, 1, 1, "", "set_score_request"]], "doubleml.utils.DMLDummyRegressor": [[43, 1, 1, "", "fit"], [43, 1, 1, "", "get_metadata_routing"], [43, 1, 1, "", "get_params"], [43, 1, 1, "", "predict"], [43, 1, 1, "", "score"], [43, 1, 1, "", "set_params"], [43, 1, 1, "", "set_score_request"]], "doubleml.utils.DoubleMLBLP": [[44, 1, 1, "", "confint"], [44, 1, 1, "", "fit"]], "doubleml.utils.DoubleMLPolicyTree": [[45, 1, 1, "", "fit"], [45, 1, 1, "", "plot_tree"], [45, 1, 1, "", "predict"]], "doubleml.utils.GlobalClassifier": [[46, 1, 1, "", "fit"], [46, 1, 1, "", "get_metadata_routing"], [46, 1, 1, "", "get_params"], [46, 1, 1, "", "predict"], [46, 1, 1, "", "predict_proba"], [46, 1, 1, "", "score"], [46, 1, 1, "", "set_fit_request"], [46, 1, 1, "", "set_params"], [46, 1, 1, "", "set_score_request"]], "doubleml.utils.GlobalRegressor": [[47, 1, 1, "", "fit"], [47, 1, 1, "", "get_metadata_routing"], [47, 1, 1, "", "get_params"], [47, 1, 1, "", "predict"], [47, 1, 1, "", "score"], [47, 1, 1, "", "set_fit_request"], [47, 1, 1, "", "set_params"], [47, 1, 1, "", "set_score_request"]]}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function"}, "terms": {"": [4, 5, 6, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 46, 47, 51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 65, 71, 72, 73, 74, 76, 78, 79, 80, 84, 86, 87, 88, 89, 91, 92, 93, 96, 97, 99, 101, 102, 109, 111, 124, 125, 126, 127, 128, 138, 140, 141, 142, 143], "0": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 124, 125, 126, 127, 128, 129, 131, 132, 133, 135, 139, 140, 142], "00": [60, 73, 75, 79, 80, 110], "000": [81, 85, 126, 143], "00000": 75, "000000": [60, 62, 63, 65, 75, 79, 80, 91, 93, 95, 140], "0000000": 126, "0000000000000010000100": [56, 91, 93, 140], "000000e": [60, 73, 75, 79, 80], "00000591": 83, "000006": [65, 83], "000017": 83, "000025": 78, "000034": 79, "000038": 61, "000039": 78, "000064": 66, "000067": 78, "000076": 97, "000084": 60, "000091": 78, "0001": [63, 79], "000104": 61, "000115": 61, "000177": 61, "0002": 61, "000219": [35, 95], "000242": [36, 95], "000273": 61, "000341": 78, "000389": 60, "000403": 60, "000442": 78, "000443": 60, "000457": 60, "000475": 60, "00047580260495": 51, "000488": 78, "000494": 74, "0005": 63, "000522": 78, "000542": 61, "000566": 60, "000573": 60, "0005a80b528f": 56, "000641": 60, "000670": 78, "000722": 60, "000743": 86, "000795": 110, "000915799": 126, "0009157990": 126, "000943": [68, 69], "000977": 61, "000986": 61, "001": [25, 51, 53, 54, 55, 56, 57, 67, 96, 97, 110, 111, 126, 140, 143], "0010": 61, "001049": [97, 101], "001051": 78, "001102": 61, "001234": 80, "001284": 61, "00133": 56, "00138944": [89, 111], "001403": 84, "001471": 75, "001494": [95, 96, 97], "0015": 61, "001504": 61, "0016": [55, 79], "001603": [97, 101], "001698": 75, "001714": 95, "0018": [55, 79], "001805": 61, "0019": 63, "001907": 75, "002122": 60, "002169338": 126, "0021693380": 126, "0021693381": 126, "002213": 61, "002241": 61, "002277": 68, "002290": 59, "0023": 53, "002388": 77, "002436": 74, "002503": 61, "0026": 63, "002601": [97, 98], "0027": 61, "002723": 61, "002779": 86, "0028": [53, 55, 61, 79], "002821": 87, "0028213335041910427": 87, "002983": 78, "003": [9, 10, 26, 85], "003045": 75, "003074": 97, "003134": 83, "003187": 68, "0032": 61, "003211": 61, "003220": 65, "003232": 61, "003267": 61, "003284": 62, "003328": 83, "0034": 71, "003404": 65, "003415": 65, "003427": 78, "003493175": 110, "00356023": 60, "003607": 69, "003644": 61, "003645": 61, "003779": 74, "0038": 61, "003836": 83, "003924": 74, "003944": 68, "003975": 68, "004028": 61, "00409412": [89, 111], "0042": [55, 79], "004236": 61, "004253": 65, "004392": 74, "004475": 61, "004526": 65, "004542": 75, "004543": 61, "0046": 61, "004688": 32, "0047": [55, 79], "004846": 87, "005103": 61, "00518448": [97, 101], "005339": [68, 69], "005517": 61, "005857": 78, "005965": 61, "005e": 97, "006055": 65, "006267": 69, "006318": 60, "006425": 80, "0065": 61, "0066": 61, "0068101213851626": 77, "006819": 61, "006922": 63, "006958": [68, 69], "007210e": 80, "00728": 140, "0073": 63, "007332": 70, "007332393760465": 70, "007394": 60, "007421": 95, "00778625": 110, "007801": 60, "0078540263583833": 77, "007890": 61, "008": 87, "008023": 80, "008223": [68, 69], "008266e": 80, "008487": 63, "0084871742256079": 77, "008518": 60, "008642": 95, "008838": 61, "008883698": 111, "00888458890362062": 89, "008884589": 89, "008dbd": 81, "008e80": 81, "009": [81, 87], "009122": 83, "009255": 68, "0093": 61, "009329847": 111, "009428": 70, "00944171905420782": 87, "00950122695463054": 89, "009501226954630540": 89, "009501227": 89, "009645422": 54, "009654": 60, "009656": 83, "00972": 63, "009727": [97, 98], "009790": 80, "009951": 81, "009986": 83, "01": [20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 45, 51, 54, 55, 56, 57, 60, 68, 69, 75, 79, 80, 81, 82, 83, 84, 96, 97, 101, 110, 111, 126, 140, 143], "010213": 86, "010268": 81, "010269": 78, "010450": 54, "0105": 61, "010940": 78, "010970": 60, "011131": 83, "0112": 53, "011204": 75, "01128": 63, "011323": [97, 98], "011598": 83, "011770": 61, "0118095": 54, "011823": 86, "011828": 61, "011884": 81, "011897": 61, "011988e": 83, "012": 81, "01219": 56, "01274": 87, "012780": 80, "012831": 87, "012938": 62, "013034": 87, "013128": 68, "013290": 81, "013302": 61, "013313": 77, "013369": 61, "013450": 75, "01351638": 54, "013593": 86, "013617": 80, "013677": 84, "013712": 68, "013898": 60, "01398951": 54, "013990": 126, "014": 84, "01403089": 54, "014080": [68, 69], "014157": 61, "014209": 61, "014432": 59, "014637": 78, "014681": 86, "014873e": 68, "015": [56, 81], "015038": 70, "015083": 61, "0151": 61, "015110": 61, "015552": 68, "015565": 83, "015668": 61, "0156853566737638": 77, "015698": 83, "01574297": 83, "015743": 83, "015780": 61, "015831": 68, "016011": 69, "016154": 78, "016169": 60, "016200": [68, 69], "016211": 61, "016315": 72, "0164": 61, "01643": 141, "016471": 61, "016503": 60, "0167": 61, "016828": 61, "016857": 61, "017": [56, 81], "017140": 68, "017152": 60, "017348": 61, "017393e": 126, "017660": 75, "01772": 128, "017777e": 69, "017788": 61, "017799": 61, "017800092": 126, "0178000920": 126, "0179": [61, 110], "017908": 61, "017919": 61, "018": [51, 56, 81], "018023": 82, "018092": 95, "018148": 83, "018251": 61, "018409": 61, "018508": 68, "01857332": 110, "018699": 61, "018909": 60, "01903": [56, 96, 138, 140], "019076": 61, "01916030e": 110, "01924359": 110, "01925597": 54, "019439633": 126, "0194396330": 126, "0194396331": 126, "019464": 61, "019596": 70, "0196": 61, "019660": [36, 95], "019699": 61, "019736898": 110, "019795": 61, "01990373": 88, "019972": 61, "019974": 80, "02": [60, 68, 69, 79, 80, 83, 95, 97, 101, 110], "020033": 61, "020143": 61, "02016117": 140, "020166": 83, "0202": 61, "020250": 61, "020271": 78, "020272": 75, "020360838": 126, "0203608380": 126, "0203608381": 126, "02052929": [89, 111], "02079162e": 110, "020809": 61, "020819": 95, "02092": 140, "021098": 62, "021269": [72, 73], "021582": 61, "02163217": 54, "021690": 73, "021823": 77, "021866": 82, "021926": 70, "02202472": 110, "022057": 61, "022181": 68, "022258": 75, "022295e": 68, "02247976": 54, "02260304": [97, 101], "022736": 61, "022768": 63, "022783": 86, "022915": 78, "022969": 80, "022982": 61, "023": 81, "023020e": [79, 80], "023052": 69, "023211": 61, "023256": 83, "0233": 61, "0234": 61, "023537": 77, "023563": 126, "0239": 61, "023955": 80, "024111": 60, "024266": 75, "024346": 68, "024355": 59, "024364": 127, "024401": [72, 73], "024581": 61, "024604": 78, "024782": 83, "024926": 59, "025": [68, 69, 72, 73, 75, 81], "025077": [69, 126], "025122": 60, "02528067": 76, "0253": [56, 61], "025300e": 69, "025323": 61, "025443": 63, "025496": 68, "025529": 61, "0257": 53, "025813114": 126, "0258131140": 126, "02584": 56, "025841": 75, "02589406": 110, "025964": 75, "026": 143, "0261": 61, "026153": 60, "026463": 61, "026473": 61, "026599": 81, "026613": 61, "026669": 80, "026723": 70, "026763": 61, "026822": 77, "026966": 75, "027011": 60, "027375": 61, "027879": 61, "02791": 63, "0281": 56, "028128": 61, "028405": 61, "028520": [68, 69], "028630": [97, 98], "028639": 61, "028731": 95, "028909": 61, "028931": 61, "028953": 61, "02897287": 58, "02900983": 83, "029010": 83, "029022": 69, "029031": 60, "029209": 143, "029364": [127, 133], "0295": 61, "029831": 83, "029910e": [79, 80], "02e": 55, "03": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 57, 60, 65, 68, 69, 70, 74, 75, 79, 80, 83, 84, 86, 87, 97, 98, 101, 110, 127, 133, 143], "030059": 95, "0301": 56, "03018": 34, "030320": 61, "030346": 140, "03045": 57, "0305": 61, "030548": 61, "030564": 61, "030636": 62, "0307": 56, "030934": 83, "030962": 83, "031007": 77, "031072": 61, "03113": 88, "031134": 96, "031156": 69, "0312": 61, "031269": 63, "031639": 83, "031720": 60, "031820": 69, "03191": 141, "032185": 61, "03220": 142, "0323": 53, "03244552": 96, "0325": 140, "03258": 77, "032580": 77, "032738": 77, "032744": 60, "03279666": 110, "032883": 61, "032941": 68, "032953": 86, "033265": 77, "033292": 61, "0336210": 110, "0337": 61, "03371939": 110, "033723": 61, "033756": 70, "033946": [72, 73], "034045": 81, "034065": 69, "03411": 140, "034135": 62, "0342": 61, "034226": 80, "034227": 61, "034242": 61, "034277": 60, "034347": 61, "03438": 57, "034469": 60, "034690": 70, "0348": 61, "034812763": 126, "0348127630": 126, "0348127631": 126, "034846": 79, "03489": [16, 54, 78], "0349": 61, "034945": 61, "035": 81, "035054": 62, "035119185": 126, "0351191850": 126, "0351191851": 126, "0352": 61, "035264": 69, "03536": 140, "03538": 56, "03539": 56, "035391": 63, "0354": 56, "035411": 140, "035441": 69, "03545": 56, "035545": 63, "035572": 63, "0356": 61, "035689": [81, 97, 101], "035730": 83, "03574": 63, "035762": 83, "035785": 69, "0359": 56, "035927": 61, "035961": 61, "036": 81, "036047": 60, "036101": 60, "036119": 60, "036129015": 126, "0361290150": 126, "0361290151": 126, "036143": 83, "036147": 83, "036222": 61, "036240": 65, "036250": 60, "036396": 61, "036492": 61, "036593": 61, "0366": 61, "036729": 78, "0368": 53, "036945": 80, "03698487": 83, "036985": 83, "037008": [72, 73], "037056": 60, "037114": 75, "0374": 56, "037504": 69, "037509": 88, "037529": 75, "037577": [97, 98], "037747": [68, 69], "037979": 61, "038103": 75, "038162": 61, "03868": 62, "038845": 68, "039036": 68, "039141": 65, "03917696": [111, 126], "0392": 61, "03920960e": 110, "039310e": 70, "039407": 61, "039539": 62, "0396": 61, "039661": 75, "039726": 60, "03973849": 110, "039895": 75, "039991e": 68, "04": [10, 40, 55, 60, 65, 68, 69, 79, 80, 83, 84, 86, 97, 98, 101, 110, 143], "040010": 75, "040079": 69, "040112": 126, "040129": 81, "040139": [68, 69], "040301": 61, "040402": 62, "040533": [111, 126], "04053339": 126, "040538": 81, "040562": 68, "040629": 39, "040688": 68, "040740": 61, "040784": 65, "0408": 68, "040821": 61, "040912": 69, "040919": 69, "0411": 61, "041147": 70, "041170": 60, "0412": 61, "041284": 70, "041387": 70, "041459": 80, "041491e": 70, "041597": 61, "04165": 97, "041754": 61, "0418": 53, "041831": 70, "0419": 61, "041925": 68, "042034": 87, "042249": 69, "042255": 61, "042265": 70, "042413": 61, "0425": 96, "04268168": 110, "0428": [61, 88], "042804": 75, "042822e": 80, "042834": 61, "042844e": 83, "043082": [97, 101], "043108": 80, "043155": 61, "0433": 53, "043386": 61, "043493": 61, "0434e374": 56, "04359081": 110, "04387": 96, "043949": 60, "043998": 69, "044": 81, "044113": 70, "044114": 61, "04415": 56, "044176": 75, "044208": 61, "044239": 75, "04424": 56, "044287": 61, "04444978": 126, "044449780": 126, "0445": 96, "04465": 54, "044704": 69, "04486": 140, "04487585": [127, 133], "04491": 97, "044929": 75, "04497975": [127, 133], "04501612": 126, "04502": [96, 111, 126], "045144": 78, "045172": 75, "045220": 61, "045247": 62, "045305": 60, "045313": 68, "045379": 140, "04552": 78, "045553": 70, "045624": 59, "04563": 96, "045638": 68, "045694": 62, "045697": 60, "045725": 61, "045754": 83, "045766": 61, "04586": 96, "045932": 83, "045984": 75, "045993": 96, "04607367": 110, "04625": 96, "046451": 75, "046507": [97, 101], "046520": 62, "046525": 95, "046527": 70, "04653976": 83, "046540": 83, "046587": 69, "0466028": 54, "046728": 86, "04682310e": 110, "046922": 96, "047194": 32, "047199": 60, "047239": 75, "047288": 95, "047431": 60, "047652e": 69, "047724": 68, "047873": 75, "047954": 78, "048015": 62, "048220": 77, "048231": 60, "048308": 73, "048476": 75, "048699": 88, "048723": 96, "048853": 69, "048891": 61, "049": 81, "049264": 65, "049573": [97, 101], "04973": 69, "05": [40, 51, 53, 54, 55, 56, 57, 60, 68, 69, 70, 71, 76, 78, 79, 80, 81, 83, 84, 87, 96, 97, 98, 101, 110, 111, 126, 140, 143], "05023695": 110, "05039": 86, "050494e": 69, "050538": 69, "050957": 61, "051": [51, 56], "05126336": 110, "05126770": 110, "051359": 62, "051651": 84, "051867e": 70, "052": 84, "052000e": 80, "052023": 75, "052255": 62, "052298": 83, "0523": 61, "052352": 61, "052380": 68, "052429": 61, "052488": 73, "052502": 83, "052703": 60, "052745": 70, "052900": 62, "053": [56, 97], "053049": 69, "0533": 53, "053331": 70, "053342": 80, "053365": 60, "053389": 126, "053436": 33, "053541": 83, "053558": 70, "053849e": 68, "054": [56, 84], "054068": 78, "054162": 78, "054210": 61, "054348": 126, "054370": 70, "054529": 126, "054771e": 83, "054882": 61, "055081": 61, "055165": 86, "055171": 69, "055174": 62, "055439": [77, 80], "055493": 87, "055668": 61, "055680": 126, "055880": 62, "056": 84, "0562": 61, "056232": 62, "05639798": 60, "056499": 73, "056745": 68, "056764": 68, "056915": 75, "056990": 61, "057095": 83, "0576": [55, 79], "057762": 83, "057792": 68, "057874": 61, "057962": 70, "05803906": 110, "058042": 126, "058276": 80, "058375": 65, "058463": 83, "058508": 88, "058595": 68, "0588": 61, "05891": 97, "0590": 53, "059128": 68, "059190": 61, "059384": 83, "059627": 80, "059630": 59, "059685": 83, "059901": 61, "06": [9, 10, 26, 60, 65, 68, 69, 70, 79, 80, 83, 95, 96], "060": 51, "060016": 65, "06008533": 97, "0601": 61, "060110": 61, "060201": 83, "060212": [79, 80], "060417": 68, "060581": 76, "060845": 126, "060860": 61, "060933": 68, "0611": 53, "06111111": 56, "0615": 53, "061584": 61, "06161": 97, "061788": 62, "061991": 60, "062": [81, 84, 97], "062362": 61, "062414": 80, "062507": 83, "06269": 85, "062710": 62, "0628": 53, "062854": 60, "062964": 126, "062988": 68, "063017": 65, "063185": 62, "0632": 53, "063234e": 69, "063327": 75, "0635": 53, "063593": 69, "0636": 53, "063700": 68, "0638": 53, "063881": 97, "063995": 62, "0640": 53, "064161": 80, "064213": 69, "06426604": 110, "06428": 79, "064280": 79, "064436": 62, "0645": 53, "0646222": 55, "0647": 53, "064711": 62, "064725": 61, "06480228": 60, "0649": 53, "065": [81, 87], "065249368": 110, "0653": 53, "065356": [72, 73], "065368": 77, "065393": 62, "0654": 53, "065451": 80, "065483": 61, "065495": 62, "0655": [53, 61], "065725": 70, "0659": 53, "065938": 62, "065969": 97, "065976": 75, "066172": 61, "0662": 53, "066248": 62, "066295": 75, "0663": 61, "066317": 62, "066452": 60, "066464": 86, "06671293": 110, "066889": 83, "0669": 53, "06692492": 110, "067046e": 68, "0671": 53, "067212": 75, "067240": 83, "06724028": 83, "067269": 62, "0673": 53, "067359": 62, "067417": 60, "0675": 53, "067528": 87, "067721": 126, "068073": 69, "06827": 86, "06829038": 110, "06834315": 58, "068377": 80, "068404": 62, "068514": 68, "068700": 95, "068803": 60, "068934": 65, "06895837": 54, "069443": 65, "069569": 60, "0695854": 54, "069589": 75, "069600": 80, "069882e": 68, "07": [68, 69, 80, 83, 84, 87, 110], "070020": 83, "070035": 61, "070106": 61, "070196": 70, "0701961897676835": 70, "0702127": 54, "0704": 53, "070433": 75, "070497": 87, "0705": 61, "070534": 37, "070552": 68, "070574e": 80, "0707": 53, "070751": 68, "07085301": 97, "070884": 83, "0711": 53, "071285": 126, "07136": [54, 78], "071362": 68, "071488e": 70, "0716": 53, "07168291": 54, "071777": 96, "071782": [36, 95], "0719": 53, "07202564": [72, 73], "07222222": 56, "072235": 61, "072293": 82, "07229774": [97, 101], "072516": 75, "072605": 65, "0727": 97, "072994": 61, "073": [81, 84], "073013": 83, "073035": 61, "073207": 78, "073275": 68, "073384": 75, "073447": [97, 101], "07347676": 54, "07350015": [16, 19, 54, 78], "073520": 70, "0736": 53, "07366": [56, 96], "073694": 69, "0739130271918385": 77, "073929": 77, "074176": 60, "074255": 61, "0743": 53, "074304": 126, "07436521": 110, "074372": 61, "074426": 83, "07456127": 54, "074617": 69, "07479278": 86, "074927": 65, "075261": 59, "075384": 83, "07538443": 83, "07544271e": 110, "07546936": 60, "07561": 140, "07564554e": 110, "075760": 62, "0758": 87, "075809": 65, "075869": 96, "0759405": 110, "075942": 77, "076019": 79, "076151": 61, "076156": 126, "076179312": 126, "0761793120": 126, "076322": 83, "076347": 70, "0765": 56, "076596": 68, "076653": [97, 101], "076684": 140, "076804": 61, "07685043": 110, "07689": 56, "07691847": 110, "076953": [72, 73], "076971": 63, "077144e": 69, "077161": 80, "077242": 61, "07727773e": 110, "077319": 83, "077502": [127, 133], "077555": 68, "077592": 75, "077702": 65, "0777777777777778": 96, "07777778": [56, 96], "07781396": 110, "077840": 80, "077883": 83, "07788588": [97, 101], "077923e": 68, "07796": 97, "078017": 68, "078096": 126, "0781132": 110, "078207": 63, "07828372": 126, "078426": 95, "078474": 126, "078709": 69, "07875331": 110, "078810": 83, "079": [51, 81], "079085": 63, "079098": 61, "07915": 56, "07919896": 110, "07942v3": 141, "079441": 60, "079458e": 79, "079500e": 68, "0795844": 110, "07961": 86, "079685": 60, "07978296": 110, "08": [70, 79, 80, 83, 87, 97, 110], "080070": 60, "080118": 60, "08031571": 110, "0806": 61, "08084888": 110, "080854": 80, "0809": 61, "08091581": 110, "080947": 63, "081": 56, "081100": 83, "0812": 110, "081230": [68, 69], "081396": 73, "081488": 78, "08154161": 110, "081563": 61, "081807": 60, "08181827e": 110, "0820": 53, "082103": 61, "082197": 75, "082263": 38, "082297": 110, "08233112": 110, "08239902": 110, "082400e": 68, "082574": 33, "082715": 61, "082732": 61, "082804": 59, "082858": 69, "082905": [97, 101], "082934": 80, "08296601": 110, "082973": 78, "083258": 126, "08327224": 110, "083318": 126, "08333333": 56, "08333617": 110, "08342246": 110, "0835771416": 54, "0836": 75, "083628": 61, "08364": 75, "083673": 60, "083706": 87, "083750": 80, "083949": 87, "083996": 61, "0839987": 110, "084": 54, "084007": 74, "084156": 69, "084184": 70, "0841842065698133": 70, "084269": 80, "084323": 68, "084337": 97, "084633": 72, "084706": 60, "08496796": 110, "085317": 62, "0853505": 54, "085395": 68, "085566": 70, "085583": 60, "085592": 75, "085671": 68, "08591058": 61, "085965": 80, "086004": 75, "08602774e": 110, "0861406": 61, "0862": 138, "086264": 70, "08632": 110, "086499": 60, "08664208": 110, "086679": 96, "086684": 60, "08687429": 110, "086889": 72, "0872": 53, "087222": 69, "087561": 68, "087634": 68, "087745": 69, "087947": 83, "088048": 83, "088282": 73, "088357": 83, "088372": 60, "08848": 96, "088482": [36, 95], "088504e": 38, "08888889": 56, "08939231": 110, "0894": 53, "08968939": 54, "089698": 62, "089964": 77, "08e": 55, "09": [68, 69, 70, 79, 80, 83, 95], "09000000000000001": 96, "0900043": 110, "090025": 80, "09015": 53, "090255": 83, "090436": 69, "090566": 60, "091179e": 68, "091263": 77, "091319": 61, "091391": 126, "091406": 127, "091535": 68, "0916": 53, "091824": 69, "091992": 82, "092125": 62, "092229": 87, "092247": 83, "092263": 97, "092365": 126, "09251664": 110, "092706": 60, "092919": 128, "092935": 68, "093043": 83, "093088": 60, "09310496": 126, "093153": 83, "093474": 83, "09347419": 83, "0935": 97, "09351167": 97, "093746": 126, "093950": 78, "094": 81, "094026": 78, "094118": 83, "094336": 60, "094378e": 68, "094381": 78, "094425": 61, "09444444": 56, "094581e": 69, "09458971": 110, "094829": 97, "094999": 83, "095065": 60, "095104": 65, "095654": 68, "095781": 31, "095785": 65, "09603": 138, "096337": 78, "096418": 65, "096550": 72, "096616": 95, "096688": 69, "09669583": 110, "096741": 58, "09682314": 97, "096824": 62, "096915": 87, "097": 84, "097009": 75, "097157": 87, "097468": 70, "09756": 85, "097581": 61, "09779675": 126, "097796750": 126, "098": 55, "098188": 61, "098256": 83, "09830758": 86, "098308": 86, "098317": 80, "098319": 83, "0986": 53, "098712": 83, "09879814e": 110, "098901": 75, "099": 84, "099001": 69, "099307": 69, "099313": 60, "099485": [97, 98], "099647": 82, "099670": 80, "099731": [68, 69], "09980311": 126, "09988": 141, "0_": 15, "0ff823b17d45": 56, "0x1747bdd4520": 63, "0x1747bdd6b90": 63, "0x2920d7b7150": 82, "0x7f94b641a000": 143, "0x7f94b69cd3a0": 97, "0x7f94b6d2eae0": 98, "0x7f94b6d2fe90": 97, "0x7f94b6d913d0": 96, "0x7f94b6d960f0": 96, "0x7f94b6dbf140": 126, "0x7f94b6dd5040": 126, "0x7f94b6dd50a0": 97, "0x7f94b6e43470": 96, "0x7f94b7d51400": 97, "0x7f94b7dd6660": 98, "0x7f94b7dfbb30": 97, "0x7f94b7e23440": 97, "0x7f94b7e444a0": 126, "0x7f94b7e52570": 126, "0x7f94b7f495e0": 127, "0x7f94bd85e8d0": 133, "0x7ff194c5ade0": 87, "1": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 102, 103, 105, 106, 109, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 124, 125, 126, 127, 128, 129, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142], "10": [7, 8, 9, 10, 12, 14, 16, 17, 19, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 133, 138, 140, 141, 142, 143], "100": [16, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 56, 57, 58, 68, 69, 71, 74, 75, 76, 78, 81, 85, 87, 88, 89, 91, 93, 95, 96, 97, 99, 110, 111, 126, 127, 133, 140, 142], "1000": [25, 32, 34, 52, 58, 59, 66, 67, 72, 73, 74, 76, 77, 79, 80, 84, 86, 87, 90, 97], "10000": [51, 59, 68, 69, 79, 80, 83], "100000e": 80, "100044": 73, "100154": 77, "100208": 95, "100356": 70, "10038": 86, "100385": 77, "1003891": 110, "10039862": [88, 97], "100517": 126, "100715": 68, "10079785": 97, "1008": 61, "100807": [68, 69], "100836": 61, "100858": 86, "10089588": 83, "100896": 83, "10092": 80, "100923": 83, "100_000": 81, "101": [9, 10, 26, 53, 84, 95, 141, 142], "10108403": 61, "10126": 80, "10127930": 126, "101279300": 126, "1015": [55, 79], "1015575": 110, "1016": [9, 10, 25, 26, 53], "1016010": 55, "1018": [80, 87], "101971": 60, "101998": 65, "102": [91, 93, 95, 140, 142], "10235": 80, "10247013": 61, "10258": 80, "102611": 61, "102616": 70, "102775": 70, "10299": 79, "103": [68, 78, 84, 88, 95, 142], "10307": 126, "1031": 80, "103189": 80, "10348": 79, "103497": 83, "1038": 80, "103806": 70, "103951906910721": 70, "103952": 70, "10396": 79, "104": [55, 79, 88, 95, 142], "10406": 80, "104087": 68, "1041": 53, "10414": 80, "104492": 75, "1045303": 54, "104787": 78, "104849": 68, "105": [15, 54, 75, 78, 95, 142], "105318": 83, "1054": 56, "105461": 95, "1055": [53, 85], "105913": 60, "106": [56, 95, 142], "10607": [63, 91, 93, 140], "10613589": 61, "10618": 80, "10637173e": 110, "106391": 126, "1065": [71, 76, 77], "106553": 60, "106595": [97, 99], "106715": 74, "106746": 83, "10684450": 110, "106857": 81, "107": [56, 61, 87, 95, 142], "107073": 70, "107156": 75, "107295": 126, "1073": 80, "107413": 68, "10747": [63, 91, 93, 140], "10799": 80, "108": [51, 95, 138, 141, 142], "1080": [16, 19, 53, 54, 78], "108050": 62, "10824": [63, 91, 93, 140], "108257e": 80, "108259": 65, "10831": [63, 91, 93, 140], "108637": 61, "10874788": 61, "10878571": 83, "108786": 83, "108870": [97, 101], "109": [68, 81, 95], "109005": 83, "10903": 79, "109069": 126, "109079e": 83, "109149": 60, "109273": 78, "109277": 75, "10928": 80, "1093": 71, "109454": 80, "109470": 69, "1096": [53, 85], "10967": 79, "109861": 140, "109933": 60, "1099472942084532": 66, "10e": [70, 83], "11": [39, 51, 54, 55, 56, 57, 58, 60, 61, 62, 63, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 83, 84, 86, 87, 89, 91, 93, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 133, 140, 143], "110": [95, 142], "110005": 60, "110081": 75, "1101": 80, "11019365749799062": 87, "110194": 87, "110359": 78, "110365": 87, "110656": 62, "110681": 86, "1107": 80, "11071087": [88, 97], "110717": 126, "1109": 80, "110902": 70, "110902411746278": 70, "111": [69, 95, 97, 142], "1111": [7, 8, 17, 52, 54, 67, 71, 78, 87, 90, 97, 127, 133, 138], "111164": 82, "11120": 80, "111610": 60, "1117": [71, 76, 77], "1118": 55, "11199615e": 110, "112": [56, 95, 142], "1120": 79, "112078": 95, "11208236": [89, 111], "1122": 80, "11221294": 61, "112216": 70, "112332": 61, "11237082": 110, "112661": 61, "1129": 80, "113": [7, 95, 142], "113022": 75, "11308172": 61, "11311": 79, "113149": 75, "113207": 83, "113270": 70, "113360": 62, "113415": 80, "113533": 60, "1135818": 110, "113702": 60, "11375": 80, "113780": 78, "113952": 75, "11399": 79, "114": [95, 142], "1144500": 54, "11447": 86, "114530": 72, "1145370": 54, "114570": 69, "11458": 80, "114647": 70, "1147": 53, "1148": 80, "114834": 80, "11488": 80, "11495": 80, "114989": 65, "115": [95, 142], "11500": [79, 143], "115060e": 83, "1151610541568202": 77, "115296e": 80, "115297e": 79, "1155142425200442": 77, "11552911": 86, "11559": 80, "115636": 69, "11570": 79, "115792e": 80, "115901": 65, "115972": 68, "116": [95, 142], "116027": 70, "11617": 80, "116274": 70, "116506": 61, "116569": 80, "1166": [79, 141], "1167": 79, "11673": 80, "11675": 80, "116758": 60, "117": [68, 95], "11700": 143, "117072": 72, "117103": 60, "117112": 69, "117242": 83, "11724226": 83, "1173": 61, "117366": 83, "11743": 143, "11750": 80, "1176": 53, "1177": 53, "117710": 70, "11792": 55, "11796": 80, "118": 95, "11802": 80, "1182": 55, "11823404": 87, "118255": 83, "1186": 55, "118601": 78, "11861": 55, "11866566": 110, "1187": 86, "1187339840850312": 78, "11879": 80, "118799": 80, "118845": 60, "118938e": 97, "118952": 78, "119": [87, 95, 142], "11932": 80, "11935": 86, "11942111": [97, 101], "119670": 60, "119766": 83, "1198": [54, 78], "12": [21, 24, 25, 37, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 65, 67, 68, 69, 70, 72, 73, 74, 75, 77, 78, 79, 80, 81, 83, 84, 86, 87, 89, 91, 93, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 133, 138, 140, 141, 142, 143], "120": [57, 58, 77, 88, 95, 142], "12002": 79, "1200x600": [60, 61], "1200x800": [60, 61], "1202": 141, "120381": 60, "120410": 81, "120468": 83, "12046836": 83, "120503": 61, "120567": [72, 73], "120718": 62, "120721": 78, "12097": [7, 8, 17, 54, 71, 78, 90, 138], "121": [80, 95, 142], "1210": 80, "12101": 80, "12105472": 126, "121054720": 126, "1211": 80, "1213405": 54, "121399": 80, "1214": 126, "121584e": 83, "121711": 80, "121774": 74, "121824": 69, "121857": 61, "12196389e": 110, "122": [9, 10, 26, 53, 60, 84, 91, 93, 95, 141, 142], "1221252731353637424344565980818488909399": 110, "12214": 55, "12223182e": 110, "122408": 70, "122421": 75, "122777": 126, "123": [40, 55, 56, 60, 79, 87, 95, 142, 143], "1230": 80, "123192": 87, "12323": 80, "1234": [51, 52, 53, 63, 66, 67, 84, 85, 90, 96, 110, 126], "12348": 87, "1238": 80, "123917": 80, "123950": [97, 98, 101], "124": 95, "12410": 80, "124306": 77, "124480": 77, "12458467": 61, "124805": 79, "124825": 69, "125": [95, 142], "12500": 79, "125065": 126, "12539340": 126, "1255": 80, "12579": 80, "1258": 54, "126": [95, 142], "12606": 80, "12612": 80, "1262607": 110, "126685": 62, "126777": 126, "126802": 80, "12689": 80, "127": [9, 95, 142], "127006": 80, "12705095": [111, 126], "12707800": 54, "1272404618426184": 77, "127337": 75, "12752825": 126, "127563": 86, "1277": 81, "127778": 80, "127889": 95, "127975": 61, "128": [55, 95, 142], "12802": 55, "12814": 80, "128229": 75, "128300e": 69, "128312": 83, "128408": 78, "1285": 53, "12861": 80, "128651": 69, "129": [78, 95, 142], "129207": 60, "12945": 141, "129477": 60, "1295": [53, 80], "129514": 80, "12955": 79, "129606": 68, "129798": 68, "1298": 80, "12980769e": 110, "12983057": 97, "12e": 110, "13": [10, 12, 14, 25, 26, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 67, 68, 69, 70, 72, 73, 74, 75, 77, 78, 79, 80, 81, 83, 84, 86, 87, 89, 91, 93, 95, 96, 97, 99, 110, 111, 126, 127, 133, 140, 143], "130": [56, 72, 78, 95, 142], "130045": 61, "130122": 86, "13034980e": 110, "130370": 70, "1306": 86, "130785": 62, "130829": 83, "13084": 97, "13091": 80, "1309844442144665": 77, "131": [95, 142], "13102231": 97, "131024": 77, "13119": 86, "1312": 143, "131211": 80, "131265": 60, "1313": [55, 143], "13137893e": 110, "131483": 75, "1316": 61, "1318": 53, "131842": 75, "132": [56, 68, 78, 95, 142], "13208": 143, "1321": [79, 143], "132248": 95, "1324": [55, 79], "13243772": 61, "132454": 59, "132481": 95, "1325": 55, "13257": 79, "132671": 70, "1328": 86, "132903": 80, "132917": 62, "132982": 68, "133": [56, 91, 93, 95, 141, 142], "13300": 80, "133202": 80, "133421": 80, "133465": 61, "13356": 80, "133596": 83, "133839": 75, "13398": 87, "133f5a": 81, "134": [78, 88, 95, 142], "134037": 62, "1340371": 53, "1341": 55, "134146": 80, "1342": 80, "134211": 83, "134244": 60, "1343": 79, "134542": 68, "134567": 80, "134573": 60, "1346035": 55, "134661": 60, "134687": 80, "13474": 80, "134765": 80, "134784": 97, "134784e": 68, "1348": 79, "13490": 80, "13494756": 61, "135": [56, 95, 97, 142], "13505272": 54, "135142": 69, "135344": 65, "135352": 20, "135379": 126, "135665": 69, "135707": 96, "135856": 83, "13585644": 83, "135871": 78, "136": [63, 78, 87, 95, 142], "1360": 55, "13602": 87, "136089": 78, "1361": 80, "136102": 68, "136208": 60, "1362430723104844": 77, "13642": 80, "136442": 78, "1366": 81, "136836": 78, "136925": 60, "137": [9, 56, 63, 95, 142], "1371": 80, "137165": 97, "1372": 61, "137204": 61, "137213": 69, "137354": 62, "13738818": 61, "137396": 83, "1378": 80, "137999": 97, "138": [95, 142], "1380": 79, "138068": 72, "13809": 80, "138132": 60, "138264": 87, "138378": 70, "1384": 79, "1386": 53, "13868238": 126, "138682380": 126, "138698": 126, "1387": 53, "138851": 72, "13893": 80, "139": [87, 95, 140], "139117e": 68, "139491": 126, "139508": 75, "13956": 86, "139605": 62, "1398": 80, "139830": [97, 101], "1399": 53, "14": [52, 54, 55, 56, 57, 58, 60, 61, 62, 63, 65, 67, 68, 69, 70, 72, 73, 74, 75, 77, 78, 79, 80, 83, 84, 86, 87, 89, 91, 93, 95, 96, 97, 99, 110, 111, 126, 127, 133, 140, 141, 143], "140": [51, 57, 58, 75, 80, 88, 95, 142], "1400": 80, "14000073": 110, "140073": 68, "140081": 77, "1401": 53, "1401127": 110, "14018": 97, "140770": [68, 69], "140833": 70, "140861": 54, "140926": 83, "14095264": 61, "141": [80, 95, 142], "1410": 61, "141002": 69, "14103422": 61, "141098e": 80, "14114": 86, "141227e": 60, "1413": 110, "141376": 60, "14141": 80, "141424": 60, "141460": 65, "141525": 60, "141546": 126, "141575": 61, "141628": 61, "141820": 70, "142": [95, 142], "14200098": 126, "142119": 68, "142270": 59, "142382": 68, "1424": 96, "142631": 60, "14268": 97, "14281403493938022": 96, "142841": 62, "14289": 80, "143": [91, 93, 95, 142], "143124": 60, "143342": 69, "143495": 95, "1435": 80, "143534": 68, "14368145": 126, "144": [95, 142], "14400": 79, "14405": 80, "14406": 80, "144084": 70, "1441": 53, "144137": 58, "144241": 72, "1443": 80, "144500e": 80, "144669": 83, "1447": 80, "14478002": 110, "144800": 70, "144908": 82, "144971": 79, "145": [95, 142], "145027": 65, "145245": 83, "14532650": 126, "145625": 83, "145748": 126, "14587": 80, "1458881": 110, "146": [95, 142], "146037": 83, "146087": 140, "146142808990006": 70, "146143": 70, "14625": 80, "146435": 69, "1465": 55, "146641": 126, "14667": 80, "1468115": 54, "146973": 70, "1469734445741286": 70, "147": [95, 142], "1470": 60, "147015e": 80, "14702": 63, "147111": 60, "147121": 83, "14727738": 61, "14744": 80, "14772": 80, "1479": 80, "14790924": 126, "147909240": 126, "147927": 63, "14798": 80, "148": [95, 142], "148005": 65, "14803": 80, "148134": [68, 69], "148161": 83, "14845": 63, "1485": 80, "148750e": [79, 80], "148790": 80, "148802": 80, "149": [95, 142], "1492": 51, "149215e": 69, "149228": 87, "149285": 83, "14929799": 110, "149472": 87, "149714": 78, "14984": 80, "149858": [35, 95], "149898": 83, "15": [8, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 54, 55, 56, 58, 60, 65, 67, 68, 69, 70, 72, 74, 75, 76, 77, 78, 79, 80, 83, 84, 85, 86, 87, 91, 93, 95, 96, 97, 99, 110, 111, 126, 127, 133, 140, 143], "150": [15, 56, 87, 95, 142], "15000": [55, 79], "150000": 55, "15000000000000002": [70, 80, 83, 96], "150000e": 80, "1501": 110, "1502": 54, "150200": 78, "150334": 80, "150408": 54, "15045117": 61, "150504": 60, "150614": 63, "150719e": 79, "151": [95, 142], "151047e": 72, "151063": 68, "15113": 80, "15127517": 61, "15155808": 61, "151629": 60, "151636": 70, "15176833": 61, "151819": 83, "15194": 79, "152": [95, 142], "152034": 80, "15207102": 61, "152148": [68, 69], "152353": 69, "152451": 62, "152706": 97, "15285": 80, "152896": 77, "152926": 59, "153": [81, 87, 95, 142], "1530959776797396": 70, "153096": 70, "153119": 70, "153195": 62, "153314": 69, "15347": 80, "15354": 86, "153587": 78, "153633": 63, "153639": 110, "153746": 60, "153935": 69, "154": 95, "15430": 143, "154421": 126, "1545": 80, "154557": 83, "154758": 126, "154781": 60, "154801": 60, "154828": 70, "154890": 77, "155": [95, 142], "155000": 79, "155025": 83, "155120": 83, "155160": 65, "155174": 65, "155227": 60, "155423": 65, "155516": 82, "15556": 80, "1557093": 54, "156": [95, 142], "1560": 80, "156021": 83, "156169": 69, "156202": [68, 69], "15629255": 61, "156317": [68, 69], "1564": 126, "156545": 126, "156684": 69, "1569": 80, "156969": 70, "157": [69, 95, 142], "157091": 126, "157154": 68, "157437": 60, "1576": 80, "157733": 77, "1577657": 54, "157e": 97, "158": [95, 142], "158007": 83, "15815035": 55, "158178": 70, "1582": 80, "158277": 61, "15844662": 61, "1586": 80, "15863686": 61, "158697": 126, "158726": 95, "15887433": 61, "1589": 80, "15891559": 83, "158916": 83, "159": [81, 142], "15915": 62, "15916": [53, 62], "159386": 86, "1596": 56, "159633e": 69, "159841": 68, "159880": 60, "159959": 80, "16": [31, 51, 52, 54, 55, 56, 57, 60, 62, 65, 68, 69, 70, 73, 74, 75, 78, 79, 80, 83, 84, 86, 87, 91, 93, 95, 96, 97, 99, 110, 111, 126, 127, 133, 140, 143], "160": [57, 58, 88, 142], "160152": 60, "1604": 55, "160836": 75, "160932": 70, "161": [56, 141, 142], "161049": 69, "16112324264046485153555860626673838696": 110, "161141": 78, "16114186": 61, "161198": 82, "161236": 83, "161243": 83, "161269": 68, "161288": [69, 77], "161314": 60, "16132331": 61, "161543": 80, "161810": 60, "16182230343845495052576372757677788294100": 110, "161822303438454950525763727576777882941002345891013141720416465676885929798122125273135363742434456598081848890939916112324264046485153555860626673838696": 110, "16186997": 61, "1619": 55, "162": 142, "162009": 60, "16201": 80, "16211": 79, "162153": 83, "1622": 80, "16241": 80, "162436": 87, "162593": 77, "1626685": 54, "162683": 87, "162710": 70, "1628": 79, "162930": 80, "163": [80, 142], "163194": 83, "16354285": 61, "163566": 80, "163895": 70, "16394431": 61, "164": [65, 84, 142], "164034": 126, "1644": 61, "164467": 79, "164608": 83, "164698": 74, "1648": 53, "164801": 83, "164805": 70, "164864": 78, "165": 142, "16500": 79, "165178": 83, "16536299": 126, "165362990": 126, "16539906e": 110, "1654": 80, "165419": 83, "165515": 62, "16553": 79, "165549": 140, "165696": 60, "165707": 65, "16590": 80, "16597": 80, "166": 142, "1661": 79, "166238": 77, "166375": 95, "166517": 75, "167": [55, 79, 142], "167060": 61, "167094": 60, "16725": 80, "167547": 83, "167581e": 68, "1676": 80, "167765": 80, "167766": 60, "1679": 61, "167949": 61, "167993": 126, "168": 142, "16803512": 126, "168089": 77, "168092": 126, "1681": [53, 79], "168195": 86, "168614": 83, "168775": 62, "168931": 83, "169": [56, 142], "1691": [53, 80], "16910": 80, "169117": 87, "169196": 83, "169230e": 70, "16951": 80, "16984": 80, "17": [52, 54, 55, 56, 60, 65, 68, 69, 74, 75, 78, 79, 80, 83, 84, 86, 87, 91, 93, 95, 96, 97, 99, 110, 111, 126, 127, 133, 140, 143], "170": 142, "1704": 80, "170651": 60, "170705": 95, "170709e": 69, "170794": 60, "17083": 80, "170933": [97, 101], "171": 142, "1712": 141, "1714": 55, "171575": 83, "171696": 95, "171815": 96, "171833": 69, "171848e": 68, "1719": [60, 61, 62], "171942": 80, "172": [84, 142], "172022": 126, "172083": 69, "17210583": 61, "17233265": 61, "17251086": 61, "17257204": 61, "172628": 77, "1727829": 110, "172793": 83, "173": 142, "173504": 73, "17361339": 61, "17372": 80, "1738": 80, "17385178": 96, "173888": 61, "17393818": 61, "173941": 60, "173969": 126, "173e": 84, "174": 142, "174185": 83, "174499": 126, "174516e": 83, "17453": 80, "1746": 80, "174835": 69, "174968": 79, "17499": 80, "175": 142, "1751": 79, "175176": 83, "17522": 80, "175254": 79, "175284": 70, "175369": 69, "175635027": 54, "17576": 80, "175894": 87, "175931": [95, 96, 97], "176003": 60, "176495": 83, "17655394": 83, "176554": 83, "176752": 60, "176929": 126, "177": [141, 142], "177007": 83, "17700723": 83, "177043": [68, 69], "1773": 80, "1774": 53, "177463": 82, "177496": 83, "177537": 60, "177611": 83, "177740": 68, "177751": 83, "17778": 80, "177830": 69, "17799": 80, "177995": 83, "178": 142, "178169": 72, "178218": 69, "17823": 56, "17869": 81, "178704": 126, "178763": 83, "178934": 126, "179": [72, 142], "179026": 69, "179036": 60, "179101": 95, "179188": 60, "1795850": 54, "179588e": 83, "179777": 69, "1798913180930109556": 81, "18": [52, 54, 55, 56, 60, 63, 68, 69, 74, 75, 76, 78, 79, 80, 83, 84, 86, 87, 91, 93, 95, 96, 97, 99, 110, 126, 140, 143], "180": [57, 58, 81, 88, 142], "180143": 65, "18015": 80, "180176e": 80, "180262": 69, "180271": 75, "1803": 53, "18030": 80, "180575": [72, 73], "18062": 60, "1807": [53, 80], "1809": 141, "180951": 83, "181": 142, "1812": 80, "1814": 53, "18141": 80, "181446": 126, "18159804": 61, "182": [60, 142], "1820": 53, "18236423": 61, "182427": 69, "182633": 83, "182849": 83, "183": [56, 79, 97, 142], "183324": 60, "183339": 68, "183373": 97, "183526": 70, "183553": 84, "18356413": 97, "18368": 80, "183855": 96, "183888": 78, "184": [56, 141, 142], "184224": 65, "184247": 68, "184248": 62, "184303": [97, 101], "184347": 69, "184929": 60, "185": [55, 56], "18500": 80, "185130": 84, "18516129": [97, 101], "185220": 60, "1855": 80, "185916": 62, "185984": 68, "186": [80, 142], "18604": 80, "1862": 53, "186237": 69, "18631": 80, "18637": 138, "186433": 60, "186589": 65, "18666": 80, "186689": 97, "186735": 83, "186775": [97, 98, 101], "18678094e": 110, "186836": 83, "187": 142, "187153": 126, "187335": 60, "187601": 60, "187664": 68, "187690": 83, "18789": 80, "188": [60, 81, 142], "188175": 83, "1881752": 83, "188223": 83, "188400": 69, "188541": [97, 98], "1887": [95, 96, 97], "188760": 75, "18888149e": 110, "188882": 69, "188991": 126, "189": [56, 84, 142], "189195": 80, "189248": 68, "189293": 80, "1895815": [16, 54, 78], "189737": 83, "189739": 65, "189927": 80, "189944": 60, "189998": 83, "19": [52, 54, 55, 56, 60, 68, 69, 75, 77, 78, 79, 80, 83, 84, 86, 87, 95, 96, 97, 99, 110, 126, 140, 143], "190": [56, 142], "19000": 80, "190025": 60, "190096": 126, "190103": 60, "19031969": 83, "190320": 83, "19033538": 54, "190648": 32, "19073905e": 110, "190809": 83, "190892": 87, "1909": [16, 54, 78], "190915": 70, "190921": 73, "190976": 84, "190982": 83, "191": [56, 60, 141, 142], "191192": 68, "1912": 141, "191223": 69, "1912705": 90, "191294": 69, "191330": 60, "191534": 79, "191716": 80, "1918": 53, "192": 142, "1922": 80, "192240": 126, "192485": 62, "192505": 82, "192526": 86, "19252647": 86, "192539": [36, 95], "192587": 83, "192689": 62, "192952": 65, "193": 142, "193060": 83, "193253": 68, "193285": 68, "193308": [36, 95], "193341": 69, "193375": 75, "19374710e": 110, "19382": 80, "193849": 84, "19385": 80, "19398": 60, "193f0d909729": 56, "194": [60, 76, 80, 142], "194092": 68, "1941": 55, "19411225": 110, "19413": [79, 80], "194303": 69, "194601": 58, "194694": 60, "194786": [97, 98], "195": [92, 93, 142], "19508": 87, "19508031003642462": 87, "19509680e": 110, "195377": 83, "195396": 83, "195547": 80, "195564": 78, "19559": [55, 79], "195761": 83, "195781": 68, "1959": 141, "195963": 75, "196": [60, 142], "196189": 83, "196437": 80, "196478e": 69, "196655": 75, "19680840": 126, "196e": 40, "197": 142, "1970": 80, "197000e": 80, "19705": 80, "197190": 60, "197197": 61, "197225": [63, 91, 93, 140], "1972250000001000100001": [56, 91, 93, 140], "1974": 80, "197424": 96, "197484": 126, "19756": 80, "19758": 80, "197600": 59, "197711": 80, "197920": 68, "19793": 80, "19794": 80, "198": [60, 142], "198218": 78, "19824": 80, "198351": 83, "198493": 75, "198503": 84, "198549": 63, "198687": 55, "1988": [52, 67, 90, 97], "199": [60, 142], "1990": [55, 79, 80], "199066": 60, "1991": [55, 79, 80, 143], "199281e": 83, "199282e": 80, "199412": 69, "199458": 126, "1995": [54, 78], "1998": 81, "19983954": 88, "199893": 72, "1999": [81, 88], "1_": [70, 83], "1d": 21, "1e": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 75, 80], "1f77b4": 59, "1x_4x_3": 59, "2": [7, 9, 10, 11, 12, 13, 14, 15, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 45, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 102, 105, 110, 111, 126, 127, 128, 133, 134, 135, 136, 137, 139, 140, 141, 142], "20": [9, 10, 12, 13, 14, 16, 17, 18, 20, 22, 23, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 55, 56, 57, 58, 60, 68, 69, 70, 72, 74, 75, 76, 78, 79, 80, 83, 86, 87, 88, 89, 90, 95, 96, 97, 99, 110, 111, 126, 127, 133, 140, 143], "200": [11, 14, 15, 25, 53, 57, 58, 60, 70, 71, 76, 81, 82, 83, 88, 90, 96, 141, 142], "2000": [8, 37, 55, 57, 65, 68, 69, 70, 75, 79, 80, 83, 85, 88, 95, 97], "20000": [55, 79], "20000000000000004": [70, 80, 83], "200000e": 80, "200049": 68, "2001": 61, "20010": 80, "200110": 80, "2003": [7, 61, 141], "200303": 140, "2004": [61, 97, 101], "20046005": 61, "2005": [58, 61], "20055": 80, "2006": [61, 80, 97, 101], "20066346": 61, "2007": [61, 97, 101], "20073763": 76, "20074": 80, "201": [56, 60, 80, 142], "2010": [54, 78], "20109813": 110, "2011": [54, 78, 138, 140], "20120267": 61, "2013": [71, 126, 141], "201399": 60, "2014": [126, 141], "20146894": 61, "2015": [15, 141], "201528": [68, 69], "20158": 80, "2016": 81, "2017": [13, 141], "201768": 78, "201788e": 80, "2018": [7, 8, 17, 18, 52, 54, 55, 58, 67, 71, 76, 78, 79, 80, 85, 86, 90, 97, 102, 110, 111, 119, 126, 138, 141, 142], "2019": [11, 56, 68, 69, 70, 72, 73, 80, 83, 86, 96, 111, 117, 120, 121, 138, 140, 141], "201e": 84, "202": [60, 142], "2020": [9, 10, 12, 14, 20, 22, 23, 25, 26, 53, 56, 58, 60, 87, 96, 97, 99, 102, 127, 128, 141], "2020435": 54, "2021": [16, 25, 53, 54, 56, 60, 61, 62, 68, 69, 78, 97, 98, 101, 102, 141, 142], "20219609": 54, "2022": [86, 87, 97, 99, 102, 127, 128, 137, 138, 141], "2023": [19, 57, 85, 88, 97, 109, 111, 124, 125, 141], "2024": [51, 66, 71, 76, 77, 81, 84, 87, 97, 138, 141], "2025": [25, 60, 97, 98, 101], "202603": 69, "202650e": 70, "20269": 80, "20274": 80, "202846": 69, "203": [55, 60, 68, 79, 142], "203284": 70, "20329": 80, "203517": 60, "2036": 80, "203828": 80, "204": [60, 142], "204007": 83, "20400735": 83, "204362": 87, "20443454": 110, "204455": 69, "204482": 83, "204794": 83, "204893": 65, "205": [60, 84, 86, 142], "205187": 70, "205224": 86, "205333e": 79, "2057531": 110, "205773": 60, "205938": 78, "206": [60, 142], "2061": 80, "206253": [79, 80], "206256": 75, "206340": 60, "2064": 80, "206614": 83, "20664767": 61, "207": [60, 81, 84, 97, 142], "20749732": 61, "2075": 53, "207834": 69, "20783816": 54, "207840": 73, "207885": 79, "207912": 126, "208": [60, 65, 142], "208034e": 80, "2080787": 54, "20823898": 54, "2086": 80, "208894": 61, "2089": 61, "208926": 61, "209": [60, 65], "209014": 83, "209219e": 86, "209257": 20, "209546e": 80, "209894": 83, "21": [7, 8, 17, 52, 54, 55, 56, 60, 68, 69, 71, 78, 79, 80, 83, 85, 86, 87, 90, 95, 96, 97, 99, 110, 126, 138, 140, 141, 143], "210": [10, 14, 25, 26, 60, 65], "2103": [80, 138], "2103034": 54, "210319": [68, 69], "210323": 83, "2104": 142, "210613": 60, "2107": 141, "21078": 80, "211": [60, 65, 84, 142], "211002": [97, 101], "21105": [56, 96, 138, 140], "2112": 87, "21142": 80, "211534": 70, "211544": 62, "21155656": 83, "211557": 83, "211604": 60, "212": [60, 65, 142], "2122": 80, "21257396e": 110, "212811": 65, "212844": 78, "212863": 68, "213": [60, 65, 84, 141, 142], "213026": 80, "213070": 69, "213135": 69, "213150": 61, "213199": 97, "21361": 80, "213743e": 69, "2139": 12, "214": [60, 142], "214458": 77, "214764": 86, "214769": 75, "215": [65, 110, 142], "215069": 83, "215342": 83, "2155": 80, "21550": 80, "21562": 80, "21573": 80, "215967": 126, "216": [65, 142], "216207": 96, "2162245": 110, "216230": 60, "21624417": 54, "2163": 80, "216344": 83, "21669513e": 110, "216761": 82, "216943": 95, "217": [60, 65, 81, 84, 141, 142], "21716": 80, "2171802": [54, 78], "217244": 34, "217291": 60, "217684": 75, "218": [60, 65, 142], "21804": [55, 79], "218051": 60, "218383": 65, "218767": 80, "2189": 80, "218938": 80, "219": [9, 10, 26, 53, 65, 141, 142], "2191274": 54, "219585": 65, "2197237644227434": 77, "21e": 110, "22": [52, 54, 55, 56, 60, 68, 69, 77, 78, 79, 83, 84, 86, 87, 95, 96, 97, 99, 110, 126, 140, 143], "220": [40, 60, 65, 142], "220088": 80, "220398": 68, "220407": 77, "220446e": 60, "220646": 60, "220772": 83, "221": [65, 142], "221247": 61, "2213": 78, "2214": 78, "221419": 80, "2215": 78, "2216": 78, "2217": [54, 78], "2218": 141, "221918": 62, "222": [60, 142], "222182": 60, "2222": [52, 54, 67, 97], "22222": 80, "222261": 95, "22272803e": 110, "222843": 83, "222882": [69, 77], "223": [84, 142], "223158": 77, "223230": 60, "22336235": 54, "223485956098176": [72, 73], "223617": 77, "22375856": 54, "22390": 79, "223928": 77, "223982": 81, "223995": 60, "224": [60, 84, 142], "224127": 60, "2244": 141, "224897": [68, 69], "225": [25, 53, 88, 141, 142], "225034": 58, "225051": 60, "22505965": 54, "22507006e": 110, "225175": 83, "225222": 83, "22522221": 83, "22528": 80, "225350": 69, "225427": 65, "225459760731946": 70, "225460": 70, "225574": 78, "2256": 80, "22562": 80, "225670": 77, "225776": 87, "225899": [97, 101], "226": [81, 142, 143], "2264": 53, "22640698": 110, "226479": 75, "226489": 60, "226524": 83, "226581e": 60, "226598": 78, "226846": 60, "226938": 73, "226969": 65, "227": [60, 80, 142], "2271071": 19, "2276": 53, "2279": 80, "227932e": 79, "227974": 60, "228": 142, "228035": 80, "2281": 80, "22829": 60, "228404": 77, "228597e": 69, "228630": 69, "228648": 55, "228756": 60, "228925": 60, "229": [55, 60, 142], "22913496": 61, "22913523": 61, "22914086": 61, "22914165": 61, "22925": 80, "22937": 80, "229443": 83, "229452": [95, 96, 97], "229472": 79, "2295": 80, "22973134": 61, "229759": 96, "2298": 53, "229961": [68, 69], "229994": [68, 69], "23": [23, 43, 47, 54, 55, 56, 58, 60, 68, 69, 76, 78, 79, 80, 83, 86, 87, 91, 93, 95, 96, 97, 110, 126, 138, 140, 141, 143], "230": [25, 53, 60, 141, 142], "230009": [72, 73], "23007098": 61, "2302": 85, "23029701": 61, "2307": [54, 78, 85, 90], "2308": 86, "230842": 68, "230956": 59, "231": [7, 142], "23113": 97, "231153": 69, "23118901": 61, "231310": 83, "231343": 60, "231430": 126, "231467": 97, "231472": 60, "231507": 62, "231734": 97, "231798": 97, "231986": 83, "231e": 84, "232": 142, "232134": [68, 69], "232157": 69, "2328": 80, "232868e": 69, "232959": [72, 73], "232e": 97, "233": [13, 60, 142], "233029": 68, "233154": 143, "2335": 53, "233705": 69, "23391813": 61, "234": [60, 141, 142], "234137": 87, "234153": 87, "234205": 80, "234431": 77, "234534": 70, "2345891013141720416465676885929798": 110, "234605": 63, "234798": 80, "234910": 78, "235": [141, 142], "235291": 68, "2359": 143, "23590": 80, "236": 142, "236008": 70, "236015e": 68, "236217": 62, "236309": 80, "236884": 77, "23690345e": 110, "236963": 60, "237": [56, 142], "237035": 62, "237115": 69, "237200e": 68, "237252": 80, "237341": 68, "237461": 86, "23748": 80, "23751359e": 110, "237896": 83, "23789633": 83, "238": [54, 78, 142], "238101": 83, "238225": 126, "238251": 70, "238522": 81, "238529": 33, "23856": 80, "238619": 65, "238794": 83, "239": 142, "239019": 75, "239243": 68, "239267": 77, "239313": 69, "23965": 80, "23e": 55, "24": [54, 55, 56, 60, 68, 69, 76, 77, 78, 79, 80, 83, 86, 87, 88, 95, 96, 97, 110, 126, 140, 141, 142, 143], "240": 51, "240127": [68, 69], "240146": 69, "240182": 61, "240295": 86, "240532": [68, 69], "2407": 53, "24080030a4d": 56, "240813": 74, "241049": 83, "241064": 69, "241503": 95, "2416": 53, "241609": 80, "241645": 69, "241678": 68, "241827": 69, "241962": 87, "24199": 80, "241e": 84, "242": 141, "242000": 80, "242124": [79, 80], "242139": 126, "242158": [79, 80], "242190": 60, "2424": 75, "242427": 75, "2424596822": 73, "242815": 126, "242902": 83, "243056": 62, "2430561": 53, "243246": 83, "2438": 80, "2439": 80, "243e": 84, "244": [40, 80], "244090": 80, "244455": 83, "244495": 60, "244554": 60, "244622": 126, "24469564": 140, "245": [141, 142], "245062": 83, "2451": 53, "24510393": 55, "245370": 78, "245512": 83, "245531": 68, "245663": 60, "245720": 59, "246": 142, "246137": 62, "246624": 95, "2467506": 54, "246753": 83, "246879": 83, "247": [84, 142], "247020": 70, "247057e": 83, "2471": 80, "2472": 80, "247617": 95, "247717": 80, "24774": [79, 80], "247826": 78, "247977": 68, "248": 81, "248171": 83, "248441": 95, "248638": 70, "249": [54, 78, 81, 142], "2491": 80, "24917": 80, "249601": [97, 98, 101], "2499": [61, 92, 93], "249986": 77, "25": [9, 10, 14, 15, 16, 17, 25, 26, 36, 37, 54, 55, 56, 59, 60, 68, 69, 70, 71, 76, 77, 78, 79, 80, 83, 87, 88, 95, 96, 97, 110, 126, 140, 143], "250": [81, 142], "2500": [61, 80, 92, 93], "25000000000000006": [70, 80, 83], "250073": 80, "250210": 70, "2503": 80, "250354": 83, "250425": 70, "251": [79, 80, 86], "251412": 69, "251454": 60, "251480": 69, "251953": 80, "252133": 80, "252253": 86, "25240463": 97, "252524": 83, "252601": 126, "253026": [68, 69], "2532": 80, "253437": 82, "253675": 79, "253724": 83, "25374": 80, "254": [80, 142], "25401679": 54, "254035": 77, "254038": 73, "254083": 69, "2543": 80, "254324": 70, "254400": 126, "255": [80, 142], "255034e": 69, "255424": 61, "255995": 68, "256": [80, 96], "256082": 95, "2561": 61, "256416": 83, "256567": 78, "25672": 80, "256944": 83, "256992": 80, "257019": 69, "257207": 54, "257377": 59, "257523": 68, "257937": 60, "25802682": 61, "25807933": 61, "258083": 69, "258158": [68, 69], "2582": 61, "2583": 80, "25838295": 61, "258522": 68, "258541e": 37, "258869": 60, "258951": 83, "259164": 69, "25925768": 61, "259367": 95, "259395": 74, "2594": [55, 79], "259828": [68, 69], "259875": 69, "25x_3": 59, "26": [54, 55, 56, 58, 60, 63, 68, 69, 76, 78, 79, 80, 88, 91, 93, 95, 96, 97, 110, 126, 140], "26016": 80, "260161": [35, 95], "260211": [68, 69], "260356": 79, "260360": 83, "260737": 60, "260738": [97, 101], "260762": 65, "261": 84, "2610": 80, "261184": 60, "2613": 80, "261624": [79, 80], "261685": 80, "26175": 80, "261777": 80, "261903": 78, "2619317": 54, "262159": 60, "262423e": 80, "262621": 78, "262829": 110, "263": [7, 80, 142], "263167": 61, "2633": 80, "263942e": 69, "263974e": 83, "264": [141, 142], "264077": 60, "264086": 59, "264274e": 80, "264884": 80, "265": 142, "2651": 97, "265119": 82, "2652": [56, 79, 80], "265547": 80, "265744": 77, "2658": 73, "265929": 84, "266": 142, "266147": 84, "266295": 60, "266686": 68, "266922": 126, "267": 81, "2670691": 54, "267500": 78, "267581": 80, "267950": 83, "268": 142, "268055": 80, "268343": 77, "268628e": 69, "268942": 83, "268943": 68, "268998": 55, "269043": 83, "269112": 97, "269262e": 60, "269977": 80, "26bd56a6": 56, "26e": 55, "27": [10, 14, 25, 26, 52, 54, 55, 56, 57, 58, 60, 63, 68, 69, 76, 78, 79, 80, 88, 91, 93, 95, 96, 97, 110, 126, 140, 141], "270": 142, "2700": 56, "270248": [97, 101], "270359": 61, "270479": 60, "270644": [68, 69], "271": 142, "271004": [79, 80], "271083": 80, "271183": 79, "271556": [97, 98], "2717624": 110, "272296": 80, "272332e": 68, "272408": 69, "272662": 80, "273": 56, "273299": 69, "273356": 70, "27371": [55, 79], "27372": [55, 79], "273779": 60, "274": [56, 80], "2740991": 53, "274251e": 79, "274267": 78, "27429763": [97, 99], "27461519": [97, 101], "274793": 83, "274825": [36, 95], "27487": 80, "275": 81, "2754": 53, "27540205": 110, "275596": 126, "276": [56, 142], "276004": 81, "276148": 83, "276189e": 78, "276258": 60, "2764": 80, "2766091": 55, "27713": 80, "277299": 63, "27751": 80, "277512": 69, "277561e": 78, "277968": 83, "278": [86, 142], "2780": 54, "278000": 78, "278035": 65, "278391": 80, "278434": 72, "278454": 75, "2786": 126, "278804": 69, "279": 142, "2793": 61, "279337": 60, "27951256e": 110, "279595": 65, "27986": 80, "279933e": 69, "28": [54, 55, 56, 60, 62, 68, 69, 71, 76, 78, 79, 88, 95, 96, 97, 110, 126, 140, 142], "280196": 73, "280454dd": 56, "280514": 126, "280963": 82, "281": [84, 142], "281024": 83, "28111364": 55, "2815": 80, "2818": 53, "2819": 126, "282": [84, 141, 142], "282200": 73, "2825": [138, 140], "28251": 80, "2827198": 110, "282870": 80, "2830": [138, 140], "283041": 68, "283207": 68, "28326": 80, "283386": 68, "2836": 53, "2836059": 54, "28382": 80, "283974": 83, "283992": 68, "283994": 83, "283e": 84, "284": 142, "28425026": 86, "284271": 74, "284312": 60, "284397": 143, "28452": [55, 79], "2849": 80, "284987": 80, "285": [84, 97, 142], "285001": 65, "285483": 75, "285e": 84, "286": 142, "286203": 68, "286371": 68, "2865": [53, 80], "286507": 70, "286563e": 80, "286593": 80, "287": [51, 142], "287041": 83, "287123": 95, "287196": 68, "287353": 60, "287815": 86, "287926": 83, "288": [81, 142], "288921": 62, "288976": 80, "289": [141, 142], "28905071": 110, "289062": 79, "289357": 69, "289440": [68, 69], "289555": 75, "29": [54, 55, 56, 60, 68, 69, 76, 78, 79, 86, 88, 95, 96, 97, 110, 126, 140], "290": 97, "290068": 60, "290565": 69, "290736e": 69, "290901": 65, "290987": 79, "291": [80, 84, 142], "2910": 80, "291008": 68, "291011": [97, 99], "291071": 83, "29107127": 83, "291405": 83, "291406": 83, "291434": 69, "291500e": [79, 80], "291517": [68, 69], "29168951": [97, 101], "291963": 83, "292": [82, 142], "292028": 70, "292047": 126, "292105": 83, "292302995303554": 70, "292303": 70, "2925": 56, "2927": 80, "292997": 83, "29299726": 83, "293218": 83, "293617e": 80, "294": 142, "294067": [68, 69], "294123": 95, "294449": 68, "295": [141, 142], "295307": 68, "295481": 83, "29548121": 83, "295837": [63, 91, 93, 140], "2958370000000100000100": [56, 91, 93, 140], "2958370001000010011100": [56, 91, 93, 140], "2958371000000010010100": [56, 91, 93, 140], "296099": 65, "296228": 80, "296729": 78, "29678199": [89, 111], "296901": 68, "296954": 60, "297": 142, "297276": [97, 101], "297287": [68, 69], "2973": 80, "297349": [72, 73], "297682": 83, "297687": 80, "297749": 80, "29784405": 86, "298": [13, 56], "298076": 68, "298120": 70, "298228e": 80, "299": [56, 84], "299537": 73, "299672": 61, "299712": 72, "2999": 65, "29999": 60, "2_": [19, 57, 88, 127, 128, 137], "2_x": [19, 57, 88], "2d": [21, 111, 120], "2dx_5": [70, 83], "2e": [51, 53, 54, 55, 56, 57, 96, 97, 111, 126, 140], "2f": 74, "2m": [127, 133, 137], "2n_t": 59, "2x": 83, "2x_0": [11, 68, 69, 72, 73], "2x_4": 59, "3": [9, 10, 11, 12, 13, 14, 17, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 102, 110, 111, 126, 127, 133, 138, 139, 140, 141, 142], "30": [11, 51, 52, 54, 56, 57, 58, 60, 62, 65, 66, 67, 68, 69, 70, 76, 78, 79, 80, 83, 84, 88, 95, 96, 97, 110, 126, 140], "300": [52, 67, 70, 80, 83, 90, 141], "3000": 65, "30000": 60, "30000000000000004": [70, 80, 83], "300031": 68, "30031116e": 110, "300744": 60, "300886": 60, "300892e": 69, "30093956": 86, "301": 56, "301201": 60, "301366": 126, "301371": 83, "3016": 79, "301737": 68, "30189": 80, "302149": 68, "302253": 60, "302357": 83, "302382": 77, "302648": 78, "303007": 68, "303324": 78, "303489": 83, "303613": 83, "30361321": 83, "30383": 80, "303835": 78, "303f00f0bd62": 56, "304130": 83, "304159": 83, "304201": 59, "30527": 80, "305341": 83, "305612": 78, "305761": 60, "305775": 83, "305b": 56, "306052": 60, "306297": 68, "306437": 61, "30645": 80, "30672815": 54, "306915": 78, "306963": 83, "307200": 60, "307407": 83, "308": 80, "308568": 69, "308774": 68, "309103": 60, "30917769": [72, 73], "309539": 77, "309772": 78, "309823e": 80, "30982972": 83, "309830": 83, "31": [54, 55, 56, 57, 60, 62, 68, 69, 76, 78, 79, 80, 88, 95, 96, 97, 110, 126, 140, 143], "310000e": 80, "310145": 77, "3104921": 110, "310761": 82, "311": 84, "311253": 80, "311321": 69, "311667": 69, "311712": 72, "311869": 95, "3120": 80, "312652": 84, "313": 97, "313056": 126, "313209": 70, "313324": 80, "31337878": 80, "313535": 83, "313683": 60, "31378": 56, "313870": 75, "314": 110, "3141": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 56, 57, 63, 78, 89, 91, 93, 95, 96, 97, 111, 126, 140], "314247": 87, "314341": 68, "3146": 37, "314625": 69, "314651": 72, "31476": [79, 80], "315": 142, "315031": 87, "315036": 68, "3151": 80, "315155": 69, "315290": [72, 73], "315310": 68, "315760": 60, "315769e": 68, "316": [56, 142], "316193": 83, "31632": 80, "316407": 97, "316540": 78, "316717": [68, 69], "316826": 68, "316863": 69, "317394": 59, "317487": 83, "317607": 83, "3179150": 110, "318": [56, 142], "318000e": 80, "318438": 80, "318552": 80, "318584": 126, "318753": [72, 73], "319": [56, 142], "319100": [72, 73], "31910229": [97, 101], "319176": 60, "319211": 60, "319420": 75, "319759": 83, "319850": 83, "32": [39, 54, 55, 56, 60, 62, 68, 69, 76, 77, 78, 79, 80, 88, 95, 96, 97, 99, 110, 111, 126, 140], "320": [80, 81], "320242": 62, "320314": 79, "320633": 70, "321": 142, "321686": 126, "32170857": 60, "322136": 60, "322186e": 68, "32236455588136": 58, "322404": 86, "322751": 69, "322899": 60, "3234": 80, "323636": 79, "323679": 78, "324": [55, 80, 142], "324243": 60, "324518": 82, "32458367": 54, "3245837": 83, "324857": 60, "325056": 83, "325090": 80, "325486": 68, "325599": 68, "326": 84, "326148": 69, "326721": 69, "326740": 83, "32674263": 62, "326871": 87, "3268714482135234": 87, "327257": 68, "327428": 60, "327597": 60, "327803": 95, "327958": 65, "328471": 68, "32875335": 62, "329": 51, "329052": 60, "329339": 58, "32950022e": 110, "329791": 60, "33": [54, 55, 56, 60, 68, 69, 72, 76, 77, 78, 79, 80, 88, 95, 96, 97, 110, 126, 140, 141], "3300": [55, 79], "330068": 68, "330100": 68, "330143": 83, "33014346": 83, "330163": 69, "330285": [68, 69], "3304269": 54, "330615": 83, "330731": [36, 95], "33117059": 60, "331365": 72, "331521": 83, "331602": 80, "33175566": 83, "331756": 83, "332502": 69, "332782": [36, 95], "3329": 80, "332996": 78, "3333": [52, 54, 67, 95, 96, 97], "3333333": 56, "33333333": [60, 62], "33335939e": 110, "3335": 80, "333581": 79, "333655": 68, "333704": 69, "333955": 69, "334": [55, 81], "334750": 70, "33500": 80, "335176": 80, "335446": 65, "335609e": 83, "335846": 83, "335853": 80, "33613": [97, 98], "336382": 69, "336461": 80, "336612": 59, "336870": 61, "337380": 83, "337399": 60, "3376": 53, "337619": 58, "338": 86, "33849": 80, "3386": 110, "338603": 68, "338775": 70, "338908": 70, "339083": 60, "339269": 86, "33928": 80, "339443": 69, "339570": 83, "339875": [72, 73], "34": [52, 53, 54, 55, 56, 57, 60, 62, 68, 69, 73, 76, 78, 79, 80, 86, 88, 95, 96, 97, 110, 126, 143], "340": [55, 80], "340029": 69, "340142": 97, "340217": 61, "340235": 75, "340274": 86, "340978": 60, "341336": 34, "341472": 77, "341755e": 68, "3420": 80, "342117": 75, "342362": 65, "342632": 77, "342675": 54, "34287815": 86, "342989": 80, "342992": 78, "343": [80, 84], "343552": 60, "343620": 60, "343639": 95, "343685": 68, "34375": 79, "343828": 68, "344212": 143, "344348": 62, "344440": 95, "34450402": 62, "344505": [79, 80], "344640": 83, "344787": [68, 69], "34479700": 110, "344834": 59, "345065e": 80, "345070": 60, "345381": 70, "3453813031813522": 70, "3454": 80, "345852": 69, "345903": 83, "345989": 68, "346107": 79, "346206": 83, "346238": 86, "346269": 69, "346678": 82, "346964": 68, "34712084": 60, "34716044": 61, "347310": [36, 95], "34732709": 61, "3476148": 61, "347696": 70, "34769649731686": 70, "347929": 80, "348": 84, "348319": 69, "34858240261807": 58, "348617": 83, "348622": 84, "348700": 69, "348980e": 69, "349213": 62, "3492131": 53, "349383": 78, "34943627": 76, "349638": 69, "34967621": 54, "349772": 73, "35": [55, 56, 60, 68, 69, 70, 78, 79, 80, 83, 95, 96, 97, 110, 126, 127, 133, 143], "3500000000000001": [70, 80, 83], "350165": 96, "350203": 60, "350208": 68, "3503568": 110, "350518": 83, "350712": [72, 73], "35077502": [127, 133], "35088866": 61, "351220": 69, "351629": 80, "351766": 82, "352": [55, 78], "352058": 60, "352250e": 79, "352259e": 80, "3522697": 54, "352889": 60, "35292": 80, "352990": 80, "352998": 80, "353105": 37, "353412": 83, "35341202": 83, "35365143": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "353748e": 83, "3538": 53, "354": 80, "354188": 59, "354371": 83, "354400": 60, "354688": 38, "355": 110, "355065": 65, "355209": 83, "3555": 110, "355627": 95, "355651": 69, "35595723": 61, "35596044": 61, "35596283": 61, "35597068": 61, "355998": 60, "356136e": 80, "356167": 73, "356183": 80, "35620768e": 110, "3564": 80, "3565": 80, "3568": 97, "357": 80, "357170": 68, "35731523": [97, 99], "358158": [79, 143], "358289": 78, "358395": 86, "358799": 126, "359": [84, 143], "359100": 80, "359161e": 68, "359229": 65, "359299": 60, "3593": 86, "359307": 69, "35th": 141, "36": [55, 56, 62, 68, 69, 78, 79, 95, 96, 97, 110, 126], "360004": 83, "360065": 126, "360249": 74, "360475": [68, 69], "360572": 69, "360584": 62, "360655": 80, "360683": 70, "360801": 70, "361": 84, "361518": 70, "361518457569366": 70, "361521": 38, "361623": 75, "3619201": 12, "362157": 68, "36231307e": 110, "363276": 54, "363328": 60, "364221": 68, "364294": 60, "3643": 126, "364595": 54, "3647": 56, "364800": 83, "36501": 80, "365266": 60, "365551": 69, "36557195e": 110, "36566025e": 110, "366": 80, "36616": 80, "366306": 60, "366310": 68, "366325": 60, "366529": 82, "366708": 60, "366718627": 54, "366950": 68, "36696349": [97, 101], "367": [40, 84], "367181": 68, "367323": 83, "367366": 75, "367398": 77, "367507": 60, "367571": 70, "367625": 83, "368": [60, 62, 110], "368152": 78, "3682": [55, 79, 80], "368324": 78, "368499": 70, "3684990272106954": 70, "368577": 95, "369251": 60, "369556": 70, "3696": 86, "369796": 83, "369819": 62, "369869": 79, "369981": 78, "37": [55, 62, 68, 69, 75, 78, 79, 80, 95, 96, 97, 110, 126], "370205": 60, "370254e": 79, "3702770": 54, "370736": 78, "3707775": 54, "370908": 68, "371": 51, "3710": 80, "371357": [79, 80], "371429": 70, "371850e": 69, "372": [81, 141], "37200": [79, 80], "372097": 70, "3722": 80, "37231324": 88, "3724": 80, "372427": 69, "3727679": 54, "373218e": 77, "373406": 62, "373451": 95, "3738573": 54, "374364": 83, "37436439": 83, "3745": 80, "3745905": 110, "374821e": 80, "374862": 68, "374917e": 68, "375081": 80, "375274": 68, "375465": 83, "375621": 77, "375844": 77, "3766": 75, "376617": 75, "376760": 69, "376806": 69, "377060": 80, "377147": 95, "377311": 83, "377669": 69, "378161": 61, "378351": 32, "378588": 68, "378596": 78, "378688": 83, "378727": 68, "378834": 83, "3788859": 54, "379": 141, "379038": 83, "37939": 80, "379499e": 81, "379614": 83, "379626": 68, "37993119": 110, "379981e": 69, "38": [56, 68, 69, 79, 95, 96, 97, 110, 126], "3800694": 54, "380837": [79, 80], "381": 84, "381072": 83, "381603": 68, "381685e": [79, 80], "381689": 83, "3817": 80, "382286": 80, "382582e": 31, "382872": 70, "383297": 83, "383531": 77, "384": 80, "38412381": 62, "384443": 69, "384677": 65, "38470495": [97, 101], "384777": 80, "384865": 69, "384928": 68, "3851": 80, "385160": 69, "385240": 126, "385917": 78, "386": [56, 80], "386102": 70, "386502": 80, "386831": 65, "386988": 58, "387": 56, "3871": 53, "387212": 60, "387426": 83, "387780": 83, "3878614": 110, "388026": 68, "388071": 83, "388185": 65, "38818693": 110, "388216e": 96, "388500": 60, "388668": 83, "38866808": 83, "388871": 80, "389": 56, "389126": 97, "389164": 77, "389430": 60, "389489": [97, 98], "389566": 82, "389640": 62, "38973512e": 110, "389755": 68, "38990574": 110, "39": [51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 65, 68, 69, 73, 74, 76, 78, 79, 80, 81, 86, 87, 88, 95, 96, 97, 110, 126], "39010121e": 110, "390379": 83, "391377": 87, "39160522": 110, "392128": 69, "392242": 74, "39236801": 76, "392400": 80, "392623": 69, "392752": 58, "392833": 86, "392847": 60, "392864e": [79, 80], "392917": 68, "393604": 70, "393654": 65, "394226": 69, "39425708": 54, "39494559": 110, "395076e": 80, "395136": 78, "395268": 95, "395569": 68, "395603": 68, "3958": 97, "395889": 80, "396": [40, 97], "396102": 60, "39611477": 55, "396173": 72, "39621961e": 110, "396300": 72, "3964": 80, "396531": 80, "39697345": 62, "396985": 78, "396992": [68, 69], "397140": 70, "397155": 69, "39727": 80, "397313": 53, "397536": 77, "397578": 74, "397811": 86, "3979": 62, "398": [91, 93, 140], "398166": 65, "3985": 80, "398770": 83, "398999": 95, "399": 55, "399056": 83, "39906818": 62, "399223": 59, "399343e": 68, "399355": 59, "399679": 97, "399692": 83, "399858": 87, "3cd0": 56, "3dx_1": [70, 83], "3e1c": 56, "3ec2": 56, "3f5d93": 81, "3x_": 83, "3x_4": [70, 83], "4": [10, 14, 19, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 102, 110, 111, 126, 127, 133, 140, 142], "40": [51, 54, 57, 58, 68, 69, 70, 77, 79, 80, 81, 83, 88, 91, 93, 95, 96, 97, 110, 126, 127, 133], "400": 78, "4000": 60, "4000000000000001": 96, "40000000000000013": [70, 80, 83], "400113": 75, "40029364": [127, 133], "400823": 83, "400855956463958": 70, "400856": 70, "400905": 65, "401": [7, 143], "401247": [111, 126], "40127723e": 110, "401690e": 69, "40174304": 62, "401931": [72, 73], "402077": 80, "402113": 126, "402200": 60, "402301e": 96, "402619": 39, "402902": 80, "403": 84, "403425": 83, "403626490670169": 89, "4036264906701690": 89, "403626491": 89, "403715": 31, "403771948": 111, "4039": 53, "40395989": 62, "404267": 68, "404280": 60, "404300": 65, "404318": 53, "40438371": 62, "404411": 68, "40452": 80, "404550": 82, "405050": 69, "405203": 59, "405374": 80, "40583": 53, "405890": [36, 95], "406285": 83, "406446": 70, "4065173": 110, "40676": 53, "407": 84, "40732": 75, "407558": 68, "407565": 68, "40827544": 62, "408324": 62, "408476": [127, 133], "40847623": [127, 133], "408479": 78, "408509": 69, "408539": 83, "408565": 83, "409154": 53, "4093": 86, "409328": 80, "409395": 83, "409746": 70, "409848": [68, 69], "41": [51, 68, 69, 79, 80, 95, 96, 97, 110, 126, 143], "410100": 68, "410393": 70, "410622": 60, "410667": 95, "410681": 59, "410682": 68, "410795": 78, "41093655": 110, "410984": 62, "411146e": 69, "411190": [68, 69], "411291": 82, "411295": 83, "411304": [68, 69], "411447": 80, "411582": 83, "411587": 62, "411768": 69, "412004": 72, "412127": 83, "412304": 87, "412477": 59, "412653": 78, "412714": 70, "412726": 69, "41277093": 62, "412941e": 69, "413247e": 68, "41336": 96, "413376": 97, "41341040": 54, "413608": 83, "413797": 60, "414": 84, "414073": 33, "41443295": 62, "414533": 69, "415034": 60, "41525168e": 110, "415375": 68, "415556": 95, "41566": 97, "415812": 143, "415988": 80, "416052": 65, "41605732": 62, "416118": 60, "416132": 69, "4166": 80, "4166667": 56, "416757": 83, "416899": 68, "416919": 69, "416e": 84, "417640": 68, "417727": 79, "417736": 77, "417767": [72, 73], "417834": 65, "41798768e": 110, "418": 40, "418056": 83, "41805621": 83, "418400": 75, "418741": 65, "418806e": 70, "418969": 95, "41918406e": 110, "419371": 83, "419380": 60, "419871": 65, "41989983e": 110, "4199952": 54, "41e5": 56, "42": [14, 20, 23, 24, 57, 58, 59, 62, 65, 67, 68, 69, 70, 72, 73, 74, 75, 76, 80, 82, 83, 86, 87, 88, 92, 93, 95, 96, 97, 98, 99, 101, 110, 126, 141], "4200": 80, "42019724": 62, "420316e": 80, "420381": 62, "420625": 60, "42073312": 54, "42094064": [97, 101], "420967": 70, "421083": 53, "4211349413": 54, "421163": 68, "421200": 87, "421297e": 69, "421357": [72, 73], "421576e": 80, "421793": 86, "421919": 80, "422007": 86, "422266": 80, "422293e": 95, "422325": 70, "422591": 69, "4228272": 62, "42312391": 62, "42338": 80, "42339993": 62, "4235839": [72, 73], "42362131": 62, "42388745": 88, "423921e": 95, "423951": 53, "424083": 60, "424108": 70, "424127": 110, "42412729": 54, "424292": 68, "424328": 83, "424651": 97, "424717": 70, "424748": 87, "42492580": 110, "425": 78, "425103": 53, "42510453": 110, "425208": 80, "42526965": 62, "425325": 75, "425416": 60, "425493": 53, "42550": 80, "425636": 75, "426046": 60, "426055": 53, "426540": 78, "426540301": 54, "426560": 60, "426736": 80, "427": 80, "427486": [68, 69], "42755087": 86, "427551": 86, "427573": 78, "427654": 75, "427725": 83, "428": [126, 143], "428046": 82, "42811700": 143, "428255": 83, "428411": [79, 80], "428467": 83, "4284675": 83, "428705": 60, "428771": [36, 95], "4290": 53, "429057": 69, "429230": 68, "429705": 68, "42ba": 56, "43": [55, 62, 65, 68, 69, 95, 96, 97, 110, 126], "430164": 60, "430298e": [79, 80], "430465": [97, 101], "430595": 69, "430608": 68, "431061e": 68, "4311947070055128": 96, "431253": 77, "431306": 83, "431437": 86, "431701914": 138, "431927": 60, "431998": 65, "432130e": 79, "432300e": 83, "43231359e": 110, "432707": 84, "43294": 56, "432f": 56, "433": [56, 84], "433221": 70, "4336": 80, "433711": 61, "43374433": 88, "433750": 68, "433753": 77, "4339": 53, "434054e": 75, "434121": 77, "434535": 83, "43453524": 83, "43484890": 110, "435": 56, "43503345": 97, "43511": 80, "435401": 78, "4357": 80, "435927": 80, "435967": 78, "43597565": 83, "435976": 83, "436": [56, 80], "436016": 77, "43627032": 58, "436327": 80, "436394": 77, "4364247": 110, "436764": 84, "436806": 80, "436817": 77, "437667": 79, "43782985": 62, "437924": 80, "438": 78, "438219": 83, "438289": 80, "438569": 80, "438578e": 80, "43883": 73, "438834": 69, "4389": 80, "438960": 78, "439401e": 69, "439541": [79, 80], "439675": 84, "439699": 65, "43989": 95, "439958": 77, "43f0": 56, "44": [58, 62, 65, 68, 69, 95, 96, 97, 99, 110, 126], "440320": 80, "440605": 96, "440747": 68, "440a": 56, "441153": 83, "441187": 60, "441209": 83, "441219": 72, "44124313": 97, "441282": 68, "4416552": 54, "441849": 68, "441886": 60, "442847": [97, 98], "443016": 70, "443032": 79, "44312177": 55, "443672": [97, 101], "443686": 83, "4437": 80, "443e": 84, "444046": 80, "444060": 60, "4444": [52, 54, 67, 97], "444500": [79, 80], "444850": 80, "4449272": 80, "445": 84, "445476": 68, "44563945e": 110, "4461928741399595": 70, "446193": 70, "4462": 56, "446456": 60, "44647451": 86, "44713577e": 110, "447492": 80, "447624": [68, 69], "447706": 70, "447849": 58, "447975": 62, "448": 80, "448252": 69, "448456e": 69, "448569": 68, "448587": 70, "448745": 83, "448842": 69, "4489": 80, "44890536": 110, "448923": 74, "449107": 23, "449150": [36, 95], "44950": 80, "449677": 75, "44fa97767be8": 56, "45": [68, 69, 70, 72, 74, 77, 80, 83, 95, 96, 97, 110, 126], "4500": 79, "45000000000000007": [70, 80, 83, 96], "450031": 77, "450152": 78, "45050707": 110, "450812e": 69, "450870601": 54, "451312e": 68, "4515349": 110, "4518": 61, "452": 56, "452091": 80, "452114": 95, "452488701": 54, "452489": 78, "452623": 69, "453": 56, "453279": 68, "4535": 80, "4539": 56, "454081": 80, "454397": 83, "454406": 65, "45467447": 110, "455": 56, "45500": 80, "455078": 70, "455091": 69, "455107": 70, "455120": 83, "4552": 56, "455293": 70, "4552b8af": 56, "455448": 86, "455672": 80, "455981": 127, "456370": 78, "456458e": 68, "4566031": 126, "45660310": 126, "456625": 81, "4567": 86, "456892": 70, "457088": 83, "45737664": 62, "457667": 80, "458114": 80, "458307": 128, "458420": 80, "4584447": 54, "458784": 68, "458814": 75, "458855": 55, "4592": 54, "459200": 78, "459383": 70, "459418": 69, "459436": 77, "45957837": 110, "459760": 80, "459812": 70, "459991": 81, "46": [62, 68, 69, 74, 76, 95, 96, 97, 110, 126], "460": 80, "4601": 80, "460207": [68, 69], "460218": 70, "460289": 83, "460570": 60, "460744": 79, "4610": 143, "461227e": 68, "461412": 95, "461469": 61, "461629": 87, "462075": 60, "462292": 60, "462451": 70, "462567": 69, "462979": 68, "463325": 83, "4634": 80, "463418": 87, "463537": 60, "463668": 80, "463766": 73, "463816": [97, 101], "463857": 80, "463903": 69, "463b": 56, "464": 97, "464076": 70, "464284": 78, "46448227": 110, "464668": 34, "4647010": 110, "465": 65, "46507214": 86, "465424": 77, "465649": 87, "465730": 87, "4659651": 89, "465965114589023": 89, "4659651145890230": 89, "466047": 83, "46618738": 110, "466440": 70, "466692": 60, "466756": 83, "467": 80, "46709481": 110, "46722576e": 110, "467613": 78, "467613401": 54, "467681": [68, 69], "467770": 70, "468072": 69, "468075": 83, "46807543": 83, "46811985": 83, "468120": 83, "468406": 80, "468449": [97, 101], "468907": 65, "468919": 80, "468d": 56, "469": 56, "469474": 87, "469676": 65, "469825": 70, "469846": 60, "469895": 69, "469905": 69, "47": [55, 58, 65, 68, 69, 79, 81, 86, 95, 96, 97, 110, 126, 142], "470055": 69, "470904": 68, "471": 65, "471435": 84, "471622": 68, "471752": 62, "472": 80, "47222159": 88, "472255": 80, "4725888": 110, "472699": 69, "472891": 83, "472e": 56, "473099": 70, "47319": 97, "4740": 110, "47419634": 140, "474214": [72, 73], "474731": 95, "475304": 80, "4754987": 110, "475517": 77, "475569": 68, "475e": 84, "476856": 70, "477041e": 60, "477130": [68, 69], "477150": 83, "477247": 69, "477357": 84, "477383": 81, "477474": 78, "47759584": 110, "47761563": 58, "47776257": 110, "478": 51, "478032": 80, "478059": 69, "478064": 69, "4781": 80, "47857478": 110, "4790352": 110, "479655": 77, "47966100e": 110, "479689": 62, "479722": 69, "479860": 80, "479876": [72, 73], "479882": 69, "479928": 83, "479959": [97, 101], "47be": 56, "48": [56, 65, 68, 69, 73, 79, 80, 95, 96, 97, 110, 126], "480": 65, "480133e": 83, "480199": 75, "48029755": 86, "480579": 69, "48069071": [89, 111, 126], "480691": [111, 126], "480800e": 83, "481172": 83, "481218": 80, "481399": [79, 80], "481705": 97, "481713": 75, "481761e": 80, "482": [56, 65], "482012": 72, "482038": 70, "48208358": 83, "482084": 83, "4821487": 110, "482179": 68, "482251": 39, "482461": [127, 133], "48246134": [127, 133], "482483": 83, "482616": 77, "482790": 59, "482898e": 69, "48296": 86, "483": [84, 97], "48315": 86, "483186": 59, "483192": [79, 80], "48331": 86, "4835": 80, "483711": 83, "483717": 70, "48390784": 97, "48404": 54, "484303": 69, "4845": 80, "484640": 83, "4849": 56, "485": [56, 80], "485197": 68, "48550": 87, "485617": [79, 80], "485812e": 80, "48583": [79, 80], "485871": 73, "486": [15, 80], "4861548": 110, "486178e": 68, "486202": 70, "486532": 83, "48661": 80, "487": [65, 80], "487117": 60, "487284": 62, "487352": 61, "487467": 80, "487524": 75, "487641e": 83, "487793": 69, "487872": 66, "488394": 68, "488442": 60, "488460": 80, "488485": 80, "48873663": 58, "488811": 83, "488909": [79, 80], "488982e": 70, "489488": [97, 101], "4895498": 83, "489550": 83, "489567": 85, "489699": 70, "489951": 69, "49": [56, 65, 68, 69, 95, 96, 97, 110, 126, 143], "490000e": 80, "49004369": 110, "490070931": 54, "4901609": 110, "490488e": 79, "490504e": 80, "490700": 83, "490896": 65, "490941": 80, "49098": 86, "491034": 68, "491155": 60, "491245": 78, "49135": 37, "4915707": [97, 99], "49193": 110, "492": 80, "4923156": 89, "49231564722955": 89, "492315647229550": 89, "492347": 60, "492417e": 97, "492637": 74, "492656": 69, "49270769e": 110, "493": [84, 97, 141], "493102e": 77, "493144": 87, "493195": 75, "493219": 83, "493313": 80, "493325": 20, "493426": 84, "493456": 60, "494": 84, "494089": 69, "494129": 83, "494324": 78, "494324401": 54, "4949243": 110, "495": 82, "495108": 77, "49530782": 54, "495657": 70, "495752": 83, "4959226": 110, "49596416e": 110, "496": 82, "496262": 60, "49650883": 86, "496551": 83, "496591": [97, 101], "496640": 81, "496714": 87, "496777": 143, "49693": 96, "497": 82, "497168": 77, "497422": 69, "497560": 60, "497655": 23, "497674": 58, "497964": 95, "498": [51, 82], "498122": 75, "498286": 77, "498606": 60, "498921": 83, "498979": 80, "498992": 68, "498f": 56, "499": [80, 82, 91, 93, 140], "499000e": [79, 80], "499348": 81, "499505": 60, "499776": 80, "49d4": 56, "4a53": 56, "4b8f": 56, "4dba": 56, "4dd2": 56, "4e": [54, 55], "4ecd": 56, "4fee": 56, "4x": 83, "4x_0": [11, 68, 69, 72, 73], "4x_1": [11, 68, 69], "5": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 110, 111, 119, 126, 127, 133, 139, 140, 142], "50": [36, 54, 56, 57, 59, 65, 70, 73, 76, 77, 79, 80, 81, 83, 95, 96, 110, 126, 143], "500": [6, 9, 10, 12, 13, 17, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 52, 56, 60, 61, 62, 63, 67, 68, 69, 72, 73, 76, 79, 82, 84, 86, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 111, 126, 127, 133, 140, 143], "5000": [41, 60, 61, 62, 68, 69, 70, 83, 85], "50000": 78, "500000": [79, 80], "5000000000000001": [70, 80, 83], "500084": 83, "500267": 74, "5003517412": 54, "500517": 83, "500652": [81, 110], "50093148e": 110, "501021": 80, "501047e": 68, "5012711": 110, "501983": 83, "502005": 95, "502016": 77, "502084": 97, "502205": 68, "502494": 70, "5025850": 54, "502595": 69, "502612": 83, "502901": 69, "502995": 83, "503": 40, "503360": 81, "503374": 77, "503504": 96, "503511": 80, "503700": 65, "50398782e": 110, "504286": 78, "5042861": 54, "504548e": 69, "5047": 110, "5050973": 54, "505264": 68, "505353": 69, "50556276": 110, "506050": 68, "506644": 68, "506659": 80, "506687": 80, "50672034": 54, "506900e": 83, "506903": 70, "507": 84, "507285": 75, "50768b": 81, "508153": 82, "508211": 60, "508433": 68, "508459": 78, "5085": 80, "508947": [97, 99], "509059": 80, "509196": 83, "509461": 83, "50967": 87, "5097": 87, "5098": [63, 91, 93, 140], "509853": 83, "5099": [56, 63, 91, 93, 140], "509951": 70, "509958": 78, "51": [53, 55, 56, 62, 65, 72, 77, 95, 96, 110, 126, 142], "510000e": [79, 80], "510121": 68, "510326": 60, "510385": 78, "510555": 65, "51079110": 54, "510806": 60, "511257": 77, "511515": 80, "511540": 80, "5115547": 89, "5115547181877": 89, "51155471818770": 89, "511665": 68, "511668": 87, "5116683753999614": 87, "511862": 83, "5119623": 110, "512": 78, "512108": 83, "512149": 83, "51214922": 83, "51243406e": 110, "512519": 78, "512572": 83, "512672": [97, 127, 133], "5131": 79, "51312780": 110, "513222": 75, "513624": 95, "513992": 83, "514": 56, "514173": 69, "514539": 60, "514545": 80, "51494845": [97, 101], "515031": 68, "515338e": 68, "515358": 70, "5154": 80, "5154789948092002": 78, "5155": 56, "515672": 69, "516": 56, "516125": 70, "516222": 83, "516242": 69, "516255": 83, "516256": 83, "516528": 83, "516797": 69, "517": [51, 56, 78], "517279": 69, "5175": 80, "517753": 68, "517798": 65, "518175": 78, "518354": 60, "518375": 69, "518446": 80, "518478": 65, "518610": 97, "518782": 80, "518846": 78, "518854": 65, "51966955": 54, "519710": 83, "52": [51, 53, 56, 74, 77, 84, 95, 96, 110, 126, 143], "520": 80, "520415": 68, "520641": 86, "520930": 70, "521002": 70, "521233": 65, "521611": 69, "521632": 68, "521788": 68, "522753": 38, "522835": 59, "523030": 87, "523163": 70, "5232": 76, "52343523e": 110, "523794e": 83, "523807": 82, "523977545": 54, "52424539": 54, "524657": 83, "524934": [68, 69], "5250": 80, "525064": 65, "52510803": 55, "5251546891842586": 87, "5255": 56, "525722": 68, "52590": [55, 79], "526": 78, "526532": 80, "526582": 75, "526769": [68, 69], "526907": 60, "526984": 69, "527226": 68, "52732": 96, "527452": 69, "527540": 68, "528000e": 86, "528381e": 88, "528580": 83, "528763": 69, "528937": [72, 73], "528996901": 54, "528997": 78, "529": 78, "529405": 53, "529468": 95, "529488": 60, "529782": 53, "53": [53, 56, 65, 74, 91, 93, 95, 96, 97, 99, 110, 126, 138, 141, 143], "530659": 74, "530793": 68, "530940": 83, "53094017": 83, "531": 56, "531223": 70, "531594": 80, "53209683": 97, "532266": 70, "53257": 96, "532738": 83, "53273833": 83, "532751": 72, "53288302": 110, "5329": 80, "533": 84, "533283": 97, "533489": 59, "533900": 83, "534139": 65, "5346": 56, "534739": 60, "534788": 60, "535179": 83, "535318": 83, "535609": 80, "535625": 61, "535718e": 80, "536": 51, "53606675": 83, "536067": 83, "536078": 81, "536143": 80, "536746": 83, "536778e": 69, "536798e": [79, 80], "537240": 83, "53724023": 83, "537438": [97, 98], "53791422": 97, "538": 56, "538013": 80, "538105": 69, "5382": 86, "538937": [79, 80], "539455": 83, "539475": 83, "53947541": 83, "539491": [72, 73], "539767": 70, "54": [53, 55, 56, 58, 84, 90, 95, 96, 110, 126, 142, 143], "540240": 80, "540375": 75, "5405": 75, "540549": 75, "5408": 53, "541": 84, "541060": 75, "541159": 83, "54163": 86, "5416844": 89, "541684435562712": 89, "541821": 80, "541990": 80, "542": 84, "542136": 68, "542159": 69, "542170": 69, "542268": [97, 98, 101], "542333": 80, "542446": 73, "542451": 83, "542560": 87, "542584": 70, "5425843074324594": 70, "542647": 83, "542648": 95, "542671": 78, "542883": [127, 133], "5428834": [127, 133], "542919": 95, "542925": 60, "542989": 83, "543": [78, 80], "543052": 65, "543075": 70, "543136": 70, "543380": 78, "5434231": 89, "543423145188043": 89, "543440": 60, "5436005": 54, "543691": 69, "543764": 73, "54378": 86, "543832": 83, "544097": 83, "544383": 87, "544555": 78, "544669": 72, "54483": [111, 126], "5448331": [111, 126], "54517706e": 110, "545492": 65, "54550506": 84, "545605e": 83, "545919": 80, "545930": 95, "546294": 80, "546455": 60, "5467606094959261": 70, "546761": 70, "546953": 69, "547039": 68, "54716": 86, "547324": 69, "547431": 82, "5476": 80, "5479": 80, "547909": 80, "548394": 62, "548636": 62, "54866026": 110, "548751": 62, "549109e": 80, "55": [55, 56, 70, 79, 80, 83, 95, 96, 110, 126, 143], "5500000000000002": [70, 80, 83], "550242": 77, "551317": 69, "551586928482123": 70, "551587": 70, "551686": 70, "55176": 96, "5518": 80, "552": 80, "552508": 80, "552694e": 68, "552727": 78, "552776": 83, "553004": 65, "553064": 60, "55307": 96, "553522": 69, "553754": 95, "553878": [35, 95], "553916": 80, "554076": 70, "554188": 62, "554793e": 110, "555": 78, "555137": 69, "555150": 80, "555445": 82, "555498": 83, "5555": [52, 67], "555536": 68, "555949e": 80, "555954": 80, "556191": [68, 69], "556792": 83, "5574dcd4": 56, "557595": 78, "557731": 82, "557999": 78, "558134": [68, 69], "5584": 78, "5585": 78, "55863386": 97, "558655": 70, "5589": 78, "559": 143, "5590": 78, "559144": 70, "559186": 70, "5592": 78, "559394": 83, "559522": 83, "559592e": 68, "559680": 80, "55dc37e31fb1": 56, "55e": 55, "56": [56, 90, 95, 96, 110, 126, 138, 141], "560135": [111, 126], "56018481": 83, "560185": 83, "5602727": 58, "560530": 69, "560689": 53, "560723": 74, "561": [51, 81], "561304": 60, "561348": 69, "5616": 79, "561711": 80, "561785": [97, 99], "561883": 39, "562013": 83, "56223": 86, "562288": 87, "562390": 95, "562452": 77, "562518": 80, "562556": 62, "5625561": 53, "562712": [68, 69], "563067": 84, "563374e": 70, "563503": 83, "563528": 80, "563563": 75, "563673": 80, "56387280e": 110, "56390147e": 110, "564045": 83, "564073": 80, "5641": 80, "564142": 70, "564232": [68, 69], "56438481": 110, "564451": 69, "564577": 80, "5646099": 110, "565": 97, "565066": 70, "565132": 60, "565373": 68, "566": 87, "566024": 83, "566091": 80, "5661": 61, "566388": 68, "567004": 86, "567215": 77, "567343": 80, "567364": 69, "567529": 83, "567695": 68, "567945": [72, 73], "568287": 75, "568932": [97, 101], "569315e": 69, "569449": 95, "569540": 69, "569590": 77, "56965663": 83, "569657": 83, "569911": 54, "5699994715": 54, "57": [56, 84, 95, 96, 110, 126, 143], "570038": 70, "5700384030890744": 70, "570111": 82, "5702": 80, "570486": 53, "570562": 53, "570722": 140, "570857": 62, "570936": 68, "571778": 53, "5718": 80, "5722": 79, "572408e": 69, "57245066": 83, "572451": 83, "572717": 85, "572991": 69, "573700": 59, "574": 56, "574160": 75, "5748": 96, "57496671": 54, "575": 8, "575236": 60, "575318": 60, "575381": 79, "575557668": 110, "57572422": 86, "575810": 68, "57585824": 86, "57592948e": 110, "57599221": 86, "575e": 84, "576": 56, "5763996": 54, "57643609": 86, "577": 56, "5770": 79, "57715074": 54, "577271": 78, "577273": 68, "5776971": 86, "57775704": 86, "577807": [68, 69], "577813": 68, "577e": 84, "578081": 80, "57818600": 110, "578307": 83, "578523": 78, "578557": 69, "578846e": 70, "579125": 79, "57914935": 55, "579197": 75, "579213": 87, "579238": 70, "57927582": 110, "579322e": 79, "579875e": 68, "57e": 55, "58": [9, 55, 79, 87, 95, 96, 110, 126, 142], "5800": 80, "58000": 79, "5804": 56, "580414": 87, "580751": 79, "580853": 68, "580922": 72, "581655": 80, "581827": 77, "581849": 69, "58195536": 110, "581965": 62, "582031": 79, "582146": 69, "58241568": 110, "582754": 85, "582761": 70, "5827635": 110, "582991": 79, "583034": 72, "583195": [68, 69], "583201": 69, "5833333": 56, "583534": 83, "583692": 77, "584012": 80, "584057e": 68, "584742": 73, "584849": 70, "584928": 68, "584942e": 78, "5852": 80, "585394": [97, 98], "585426": 110, "585479": 75, "585718e": 60, "585793": 70, "586345": 61, "586362": 83, "5864": 53, "5866": 80, "586719": 70, "586719493648897": 70, "586794": 68, "5868472": 54, "586921": 77, "587135": 69, "587292": 80, "588": 80, "58812": 96, "5882": 79, "588233": 68, "588364": 95, "588854": 68, "58893114": 110, "589147e": 77, "589440": 70, "589958": 69, "59": [60, 69, 95, 96, 110, 126], "590098": 60, "590320": 59, "5905": 79, "590736": 83, "590813": 83, "590904": 69, "590911": 70, "590991": 70, "591080": 59, "591411": 72, "591441": 31, "591652": 79, "591678": 79, "591782": 83, "59199423e": 110, "592186": 69, "592681e": 70, "593": 143, "59307502e": 110, "593648": 96, "593981": 95, "594": 8, "594316e": 83, "595353": 70, "59563003": [97, 101], "596": 80, "596005": 60, "596069e": 80, "596270": [72, 73], "5964": 76, "596460": 69, "596758": 68, "597": 55, "597098": 80, "597923": 80, "598178": 80, "59854797": 97, "5985730": 55, "598602": 60, "59861": 80, "598761e": 69, "599297": [95, 96, 97], "599334": [97, 101], "599632": 62, "5cb31a99b9cc": 56, "5d": [70, 83], "5x_2": 59, "5x_3": 59, "5z_i": 83, "6": [8, 9, 10, 14, 15, 20, 21, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 91, 93, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 133, 138, 140, 141, 142], "60": [54, 57, 58, 70, 80, 81, 83, 88, 95, 96, 110, 126, 141], "600": 78, "6000": 80, "6000000000000002": [70, 80, 83], "600000e": 80, "600195": 68, "600254": 82, "600694": 97, "600776": 65, "601": 55, "601061": 70, "601598": 78, "601783e": 69, "601984": 69, "602079": 73, "602168": 70, "602322": 95, "602386e": 68, "602492": 68, "602587": 83, "602628": 70, "6029": 80, "604016": 80, "604111": 80, "604603": 23, "60463309": 61, "60478664": 61, "604825": 80, "604841": [79, 80], "605": 80, "605195": 73, "606034": 83, "60612297": 61, "606129": 83, "606342": 70, "606759": 80, "6068": 54, "606800": 78, "606954": 70, "60721002": 61, "607264": 68, "6075": 143, "607600": 80, "607900e": 69, "608": [71, 84], "608392": 83, "60857": 53, "608818": 86, "60884217": 61, "60885305": 61, "60886666": 61, "60887807": 61, "609": 84, "609522": 77, "609575": [97, 99], "609769": 60, "61": [60, 65, 81, 95, 96, 110, 126, 142], "610318": 65, "611": 143, "6110": 80, "611269": 78, "61170069": 84, "611859": 73, "612": 84, "612246": 75, "612792": 80, "613244": 69, "6133": 55, "613314": 70, "613408": 83, "613498": 80, "613574": 74, "613622": 69, "613691": [97, 99], "614": 84, "61404894": 97, "614188": 78, "614678": 80, "615": 65, "615465": 60, "615498": 31, "615532": 81, "615863": [72, 73], "616372": 79, "616617": 69, "61669761": [127, 133], "616698": [127, 133], "616828": 80, "617": 78, "61728": 95, "617283": 80, "6173": 56, "61771229": 84, "617877": 83, "618069": 79, "61810738": 55, "618267": 61, "618574": 69, "618776": 58, "618881": 69, "618999": 81, "619": 84, "619128": 69, "619294": 65, "619351": [68, 69], "619390": [68, 69], "619443": 61, "619454": 59, "619613": 79, "619903": 69, "61e": [55, 143], "62": [31, 65, 73, 74, 95, 96, 110, 126], "620156": 83, "620616": 60, "620874e": 97, "620995": 88, "621": 143, "621094": [79, 80], "621318": 83, "62131806": 83, "621359": 95, "621490": 83, "6215": 79, "622": [80, 84], "622153": 80, "622272": 65, "6224": 54, "62245103": 110, "623024": 70, "623173": 68, "623805": 60, "624": 78, "6240": 86, "62403053": 58, "6243811": 54, "624535": 96, "624764": 69, "624798": 79, "624818": 69, "624919": 80, "624988": 80, "625": [54, 78], "625159": 74, "625477": 83, "625766": 72, "625767": 68, "625891": [72, 73], "626163": 60, "626433": 83, "6266": 80, "626633": 69, "627505": [72, 73], "627560": 83, "627564": 70, "627588e": 80, "628": 84, "628069": 78, "629346": 80, "629549": 69, "629595": 37, "629740": 68, "629e": 84, "63": [54, 65, 78, 95, 96, 110, 126, 141, 142], "630150e": 83, "630597": 60, "630880": 84, "630914": 74, "631083": 69, "63117637": 97, "631333": 83, "6318": [79, 143], "632058": 78, "63245862e": 110, "632705": 60, "632747e": 83, "632958": 82, "6330631": 126, "633433": 78, "633637": 60, "63363948": 110, "633642": 81, "634": 84, "63407762": 143, "634078": [79, 143], "634577": 126, "63499": 80, "635": 40, "635000e": [79, 80], "635199": [79, 80], "635768": 68, "63593298": 97, "636": 51, "636048": 97, "636453": 34, "636575": 70, "637326": 83, "6379": 79, "638264": 83, "638461": 77, "638488": 74, "639": 79, "639135": 78, "63916605": 55, "639345": 80, "639580": 69, "639603": 69, "64": [65, 73, 79, 80, 84, 95, 96, 110, 126, 140], "640": 80, "640900": 80, "641528": 83, "641547": 83, "64154727": 83, "64197957": 83, "641980": 83, "642": 84, "6420": 80, "642016": 83, "642329": 65, "642648": [97, 98], "64269": 86, "642938e": 60, "643133": 80, "64340": 86, "643512": 70, "643679": [97, 101], "643752": 83, "643939": 65, "644113": 95, "644371": 69, "644665": 70, "64476745e": 110, "644799": 59, "644985": 68, "645": 80, "645583": 65, "64579": 53, "6458": 54, "645800": 78, "646117": 69, "64646906": 110, "646937": 59, "647002": 80, "647004": 97, "647010": 80, "647196": 59, "64723": 86, "647254e": 68, "647269": 60, "647689": 87, "647864": 95, "647873": 83, "64797": 86, "648": 79, "648355": 68, "648690": 69, "648769": 69, "649": 141, "649158": 83, "649298": 62, "649514": 68, "649738": 68, "65": [65, 70, 74, 80, 83, 84, 95, 96, 110, 126], "650": [71, 97], "6500000000000001": [70, 80, 83], "650000e": 80, "650234": 65, "650810": 80, "650867": 70, "651127": 69, "6514": 61, "652071": 80, "6522": 141, "652312": 72, "652324": 65, "652349": 83, "652350": 78, "652450e": [79, 80], "6527": 71, "652778": 78, "6528": 80, "652998": 60, "6530": 80, "653008e": 79, "653829": 75, "653846": 70, "653901": [68, 69], "654070e": 97, "654755": 59, "654946": 60, "655284": 83, "6553": 143, "6554": 141, "655422": 80, "655547": 68, "65557405e": 110, "655959": 75, "656": [51, 81], "656697": 61, "656726": 61, "657": 56, "657283": 86, "658": 78, "658021": [97, 101], "658267": 83, "658592": 69, "6586": 53, "658702": 69, "659": 56, "659245": [68, 69], "659339": 69, "659387": 62, "6593871": 53, "659423": [68, 69], "659473": 87, "659523": 60, "659636": 70, "659735": 68, "659755": 84, "6598": 76, "659835": 69, "66": [65, 75, 76, 81, 95, 96, 110, 126, 140, 142], "660": [56, 97], "660073": 69, "660320": 73, "660479": [97, 99], "660539e": 60, "6607402": 84, "660776": 83, "66133": 97, "661369": 82, "661388": 68, "661989": 60, "66214869": 62, "66244377": 62, "6625": 80, "66262953": 62, "662975": 77, "663081975281988": 70, "663082": 70, "663177": 65, "663182": 70, "6634357241067617": 87, "663529": 83, "663533": 80, "663672": 75, "663765": 69, "664103e": 80, "664147": 80, "664409": 69, "664665": 60, "664797": 68, "664824": 80, "664850": 78, "665264": 83, "665653": [97, 101], "66601815": [97, 99], "666104": 83, "666307": 59, "6666667": 56, "666742": 95, "666959e": 75, "667": 78, "667492e": 80, "667536": 83, "667614": 70, "667614205604159": 70, "667981": 68, "667985": 74, "668337": 80, "668452": 74, "668584": 59, "66891473": 60, "668981": 72, "669579": 69, "66989604": 58, "67": [51, 56, 79, 87, 95, 96, 110, 126, 140], "670867": [36, 95], "67091031": 60, "671224": 69, "671271": [68, 69], "67136": 80, "6716717587835648": 70, "671672": 70, "671690": 68, "6722": 56, "672234": [68, 69], "672368": 70, "6723684718264447": 70, "672384": [68, 69], "67245350": 54, "672511": 68, "673092": [68, 69], "6732775": 110, "673302": 78, "673330": 69, "67410934": 54, "6745349414": 54, "674552": 80, "67456": 87, "674609": 70, "674747": 77, "675": 51, "675233": 69, "675293": 82, "675625": 95, "675653": [97, 98], "675775": 77, "676405": 70, "6765": [55, 79], "676534": 126, "676641": 68, "676756": 83, "676807": 79, "677123": 68, "677614": 83, "677980": 70, "678": 84, "678000": 60, "678117": 80, "678277": 60, "67831745": 60, "67841551": 60, "678442": 60, "678826": 70, "678878": 60, "67914471": 62, "6793455": 62, "67936506": [97, 99], "679539": 78, "679789e": 68, "67ad635a": 56, "68": [56, 65, 81, 85, 86, 95, 96, 110, 126], "680": 80, "68043075": 62, "6810775": 86, "681176": 78, "681246": 69, "681448": 80, "681521": 68, "681562": 80, "681817dcfcda": 56, "682": 97, "682122": 77, "682269": 80, "6826": 79, "682875": 70, "683487": 69, "683581": 97, "683637e": 75, "683687": 69, "683942": 83, "683984": 38, "684": 143, "68410364": 55, "68411700": [55, 143], "684128": 69, "684142": 68, "684502": 83, "685104": 20, "685107": 83, "68554404e": 110, "68562150e": 110, "685807": 83, "685922": 60, "685989": 97, "686270": 69, "686627": 68, "686753": 60, "687345": 83, "687612": 69, "687647": 83, "687854": 59, "687871": 78, "6878711": 54, "688": 141, "688014": 60, "688479": 61, "688540": 95, "688641": 77, "688747": 80, "688886": 95, "688918": 80, "688956": 68, "689088": [68, 69], "689188": 59, "689392": 83, "689524": 60, "689600": 77, "689932": 68, "69": [74, 95, 96, 110, 126, 142], "690334": 70, "6903344145051182": 70, "69047468": 62, "69058888": 62, "691097": 68, "691157": 58, "69119168": 62, "69140475e": 110, "691423": 68, "691511": 79, "691848e": 69, "691911": 95, "692177": 62, "692297": 69, "692460": 77, "692579": 69, "692725": 83, "692907": 80, "692959": 68, "693316": 80, "693349": 60, "693497e": 80, "693690": 80, "693796": 78, "69381367": 110, "694154": 70, "694561": 84, "694845e": 80, "694919": 78, "6950": 80, "695045": 68, "69508862": 97, "6952794": 110, "695581": 74, "69562150e": 110, "695711": 77, "69572427": [97, 101], "695928": 68, "696011": [35, 95], "696224": 95, "696289": [72, 73], "696516": 60, "69684828": 97, "696966": 69, "697": 78, "697000": 70, "697420": [72, 73], "697545": 83, "697616": 69, "697693": 68, "698": 51, "698223": 59, "698244": 59, "698384": 62, "69840389e": 110, "698509": 68, "698642": [97, 101], "698694": 78, "698751": 77, "699": 110, "699035": 83, "699082": 70, "69921": 56, "699259e": 83, "699333": 70, "699616": 77, "699697": 69, "6_design_1a": 71, "6_r2d_0": 71, "6_r2y_0": 71, "6b": 126, "6cea": 56, "7": [10, 14, 17, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 133, 140, 141, 142], "70": [55, 57, 70, 72, 79, 80, 83, 95, 96, 110, 126, 142], "700": [68, 69, 71, 78], "7000000000000002": [70, 80, 83], "700015": 83, "700102": 83, "700314": 65, "700458": 68, "701078": 83, "701088": 79, "701106": 74, "701265": 72, "701413": 80, "701672e": 70, "701841e": 73, "701866": 83, "7018663": 83, "701966": 80, "702489": 80, "703049": 68, "70305686": [97, 101], "703108e": 39, "7031134": 60, "70321672": 60, "703325": 75, "70344386": [97, 101], "703772": 80, "703826": 60, "7040": 80, "704482": 75, "7045": 75, "704558": 75, "704814": 68, "704896": 75, "705090": 69, "705354": 68, "70552709": 60, "70557077": [97, 101], "705581": 80, "7055958": 89, "705595810371231": 89, "7055958103712310": 89, "705794": 69, "70582": 60, "70583": 86, "7059478": 110, "706056": 80, "706077": 69, "706122": 69, "706430": 68, "706645": 70, "706657": 70, "706862": 23, "707125": 69, "707441": 69, "707738": 69, "70774361": [97, 101], "707868": 83, "70789373": 110, "707963e": 79, "708190": 78, "708235": 68, "708459": 83, "708472": 69, "708821": 65, "708837": 65, "709026": 59, "709596": 68, "709606": [36, 95], "70975084": 60, "71": [95, 96, 110, 126, 142], "710059": 65, "710319": 69, "710515": 68, "710586e": 78, "711024": 80, "711328": 80, "711383e": 68, "711518": 80, "711638": 97, "712064": 68, "712082": 80, "712095": 65, "712157": 82, "712268": 68, "712372e": 69, "712503": 86, "712592": 79, "712774": 72, "712846": 95, "712960": 70, "713": 80, "713407": 80, "713457": 68, "713986": 80, "713993": 69, "7140": 60, "714240": 78, "714250": 69, "714321": 79, "71447013": 110, "714534e": 69, "714651": 83, "71465114": 83, "715013": 80, "715180e": 80, "715192829323339475461697071747987899195": 110, "71519282932333947546169707174798789919516182230343845495052576372757677788294100122125273135363742434456598081848890939916112324264046485153555860626673838696": 110, "7151928293233394754616970717479878991951618223034384549505257637275767778829410023458910131417204164656768859297981221252731353637424344565980818488909399": 110, "71519282932333947546169707174798789919516182230343845495052576372757677788294100234589101314172041646567688592979816112324264046485153555860626673838696": 110, "7151928293233394754616970717479878991952345891013141720416465676885929798122125273135363742434456598081848890939916112324264046485153555860626673838696": 110, "7154": 80, "715407": 70, "7155": 80, "7158581": 54, "716013e": 68, "716098": 65, "7161": 80, "716387": 68, "716427e": 69, "716456": 83, "716595e": 80, "716615": 65, "716762": 70, "716793": 70, "716799": 78, "7167991": 54, "716801": 77, "717": [51, 80], "717130": 80, "717185": 83, "71727866": 60, "718548": 62, "71866289": 110, "718686": 87, "719552": 69, "71959587": 110, "719870136": 110, "72": [57, 75, 95, 96, 110, 126, 142], "7204309": [97, 101], "720559": 68, "720571": 83, "720573": 68, "720589": 84, "720664": 78, "721018": 68, "721071": 83, "721245": 69, "7215093d9089": 56, "72155839e": 110, "721609": 80, "721747": 60, "721773": 60, "722316": 83, "722634": 83, "72269685": [97, 101], "722848": 70, "722881": 83, "7229": 80, "723": 56, "723314": 83, "723342": 95, "723345e": 83, "723657": 68, "723846": 65, "7239": 80, "7241399": 54, "724338": 83, "724673": 60, "724767": [72, 73], "724918": 87, "725": 56, "725010": 65, "725061": 68, "725087": 80, "725166": 83, "725467": 81, "725565": 68, "725802": 23, "725820": 77, "725919": 68, "726": [56, 84], "726658": 77, "7268131": 54, "727159e": 69, "727543": 59, "727693": 80, "727704": 80, "727976": 70, "7282094": [97, 99], "728294": 82, "728710": 83, "72875815e": 110, "728852": 80, "728e": 84, "729365": 60, "729668": 95, "729867": 68, "72987186": [97, 101], "73": [55, 65, 95, 96, 110, 126], "730011": 61, "730023": 80, "73047111": 60, "7308": 53, "730809": 68, "73088109": 60, "731174": 68, "731317": 70, "732": 84, "732067": 68, "732137": 68, "732150": 69, "7326": 80, "732638": 83, "73274989": 60, "73285": 34, "732918": 72, "733": 80, "733047": 69, "73362378": 60, "733644": 68, "734635": 68, "734770": 69, "734948": 83, "7350": 60, "735051": 61, "735144": 61, "735369e": 95, "7357": 80, "7358226": 60, "735848": 95, "735916": 60, "735941": 33, "735964": 59, "736082": [68, 69], "736084": 83, "73608412": 83, "736823": 69, "737052": 80, "7375615": 55, "73764317e": 110, "737951": [68, 69], "738": 79, "738065": 69, "738223": 80, "738315": 80, "738659e": 80, "738876": 69, "739": 80, "739063": 68, "739089": [97, 101], "7395359436844482": 70, "739536": 70, "739595": 84, "739720": 80, "739817": 74, "74": [9, 55, 69, 79, 95, 96, 110, 126, 142], "740": 78, "740180e": 83, "740367": 68, "740417": 79, "740505": 65, "740785": 68, "740869": 70, "741104": 70, "741380": 84, "741523": 65, "741702": 83, "7418": 53, "74189": 56, "742128": 83, "742375": 68, "742407": 82, "742411": 68, "742816": 62, "742907": 83, "7432": 53, "743247": 80, "743341": 69, "743609": 68, "7437": 80, "74402577": 83, "744026": 83, "744236": 86, "74461783e": 110, "74475816": [97, 101], "745": 80, "745022": 65, "745444": 68, "745638": 79, "745881": 68, "746361": 83, "746843": 73, "7470": 80, "7473": 61, "747646": 80, "747945": 54, "747961": 80, "748084": 69, "748377": 79, "748513": 80, "748785": 62, "748880": 80, "74938952": [97, 99], "7494": 61, "749436": 61, "749443": 80, "749854893": 111, "75": [9, 26, 36, 56, 59, 65, 70, 75, 79, 80, 83, 95, 96, 110, 126, 142], "75000": 87, "7500000000000002": [70, 80, 83], "750000e": 80, "750597": 69, "750701": 65, "751013": 80, "7511540": 110, "751261": 80, "751515": 81, "751633": 80, "75171": 79, "751710": [70, 79], "751712655588833": 89, "7517126555888330": 89, "751712656": 89, "75196051": 110, "752015": 32, "752088": 60, "752283": 80, "752696": 65, "752909": 75, "7533": 79, "753323": 68, "753393": 68, "753523": 83, "753814": 60, "753866": 69, "754469": 68, "754499": 69, "754678": 77, "754692": 95, "7548": 87, "754870": 78, "755688": 68, "755701e": 68, "755885": 95, "755910": 80, "7559417564883749": 70, "755942": 70, "7560824": 54, "756200": 65, "756805": 78, "756867e": 80, "756905": 23, "756969": 70, "757": [84, 141], "757151": [68, 69], "757183": 70, "757411": 83, "757559": 75, "757819": 78, "757917e": 83, "758391": 80, "758616": 60, "758831": 69, "75887": 56, "759006": 58, "759054": 69, "759272": 61, "759833": 69, "76": [95, 96, 110, 126, 141, 142], "760104": 83, "7603": 53, "760386": [97, 99], "760778": 78, "760915": 59, "761": [54, 78], "761162": 60, "761224": 65, "761429": 69, "761714": 70, "762284": 83, "76228406": 83, "762299": [97, 101], "762748": 80, "763691": 80, "764093": [68, 69], "76419024e": 110, "764315": 83, "764427": 61, "76444177e": 110, "764478": 82, "7646": 80, "764798": 83, "764953": 79, "765202": 80, "76535102": [97, 101], "765363": [68, 69], "765500e": [79, 80], "765710e": 88, "765792": 83, "76591188": 54, "765960": 68, "7660": 53, "76608187": 61, "7663": 80, "766499": 83, "766760": 60, "766940": 65, "766986": 62, "76702611e": 110, "767188": [72, 73], "767435": 87, "767549": 69, "768071": 83, "768273": [72, 73], "768763": 69, "768798": 65, "769361": 83, "769805": 83, "76980679": 110, "77": [84, 95, 96, 110, 126], "7704": 60, "770556": 80, "770944": [72, 73], "7710": 86, "771157": 126, "771390e": 80, "7714": 81, "7716982": 55, "771741": 80, "771965": 80, "772104": 68, "772157": 75, "77227783e": 110, "772396": 69, "772444": 95, "772791": 80, "77289874e": 110, "773": 56, "773177": 70, "773339": 75, "773488": 83, "77348822": 83, "773769": 77, "77401500e": 110, "774271e": 80, "775": [56, 80], "775191": [68, 69], "775285": 68, "775969": 86, "776254e": 68, "7763": 79, "776728e": 78, "776887": 79, "777154": 62, "77746575": [97, 101], "777563": 81, "7776071": 54, "777718": 77, "777728": 95, "777867": 75, "777e": 84, "778400": 68, "7786": 53, "779": 84, "779068": 75, "779108": 68, "779167": 31, "779517": [68, 69], "779682": 70, "7799": 76, "779912": 80, "78": [84, 95, 96, 110, 126, 142], "780": 56, "7800": 61, "780068": 77, "780338": 68, "780458": 83, "7806": 60, "780856": 79, "781": 80, "781233": 80, "781530": 83, "781681": 83, "782": 56, "782050": 83, "782555": 80, "782857": 61, "783": 56, "783276": 97, "7833": 53, "7838": 53, "78386025": [97, 101], "784": 126, "784238": 78, "784405": 86, "784483": 78, "784624": 70, "784792": 77, "784872": 65, "785": 56, "785038": 69, "785153": 69, "785815": 65, "785911": 83, "785e": 40, "786": 56, "786090": 77, "786191": 74, "786237": 68, "786563": 77, "786744": 70, "786884": 61, "786986": 65, "78711285e": 110, "78777": 86, "788": 141, "78818": 56, "788868": 69, "789032": 68, "789039": 69, "789330": 69, "789355e": 86, "789671": 70, "789671060840732": 70, "79": [65, 95, 96, 110, 142], "790039e": 68, "790115": 80, "790261": 95, "790723": [72, 73], "791097": 79, "791241": 83, "791297": [36, 95], "792396": 65, "792939": 70, "792972": 95, "79330022": [97, 101], "793315": 95, "79338596e": 110, "793570": 83, "793598": 69, "793735": 83, "793818": [68, 69], "794": 97, "794366": 80, "794393": 62, "79458848e": 110, "794805": 72, "795": 84, "795647": 83, "7957": 80, "795932": 96, "796014": 69, "796384": 69, "796430": 81, "796444": 80, "796596e": 68, "796e": 84, "797": 51, "797086": 68, "797189": [97, 101], "797280": 83, "797454": 97, "797737": 126, "7978": 61, "797868": 69, "79792890e": 110, "797965": 126, "798029": 60, "798071": 20, "798308": 79, "798396": 61, "798783": [72, 73], "79920372": 110, "799403": 83, "79953099": [97, 101], "7999": 88, "7b428990": 56, "7x": 83, "8": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 67, 68, 69, 70, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 91, 93, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 133, 140, 142, 143], "80": [57, 58, 70, 80, 83, 88, 95, 96, 110, 142], "800": 78, "8000": [19, 57, 88], "8000000000000002": [70, 80, 83], "8001": 61, "800143": 68, "800326e": 68, "800351": 68, "801623": 80, "802": 84, "802289": 80, "802738": 95, "803112": 77, "803300": 68, "803492e": 83, "803563": 80, "803902e": 80, "804": 80, "804219": 83, "804284": 86, "804316": 83, "804484": 83, "8048": 55, "804828": 83, "804889": 80, "805007": 78, "805153e": [79, 80], "805293": 69, "8055563": 54, "805774": 68, "8059": 79, "806218e": 80, "806531": 80, "806554": 68, "806732": 74, "8068411": 110, "80696592e": 110, "80714504e": 110, "807393": 60, "807543": 62, "807879": 83, "808": [55, 126], "808246": 79, "808284": 80, "808640": 80, "809125": 68, "8095": 81, "809913": [68, 69], "80a8": 56, "81": [54, 68, 71, 74, 76, 95, 96, 110, 142], "810044": 79, "810134": 83, "8102": [53, 79], "810306": 65, "810322": 68, "810363": 80, "810382": [79, 80], "810419": 69, "810464": 60, "810707": 80, "810895": 69, "811011": 69, "811155": 74, "811458": 79, "811513": 69, "811684": 61, "8116912": 126, "811696": 68, "811825": 78, "811901": 83, "81190107": 83, "812": 84, "812028": [97, 98], "8132463": 54, "813293": 83, "813342": 126, "813682": 80, "814136": 70, "814246e": 69, "814351": 70, "814913": 78, "8152": 80, "815213e": 69, "815224": 126, "815226": [97, 99], "81568484": 83, "815685": 83, "815993": 83, "816176": 87, "816318": 78, "816373": 68, "816752": 80, "816982": 68, "817119": 68, "817291": 80, "8173602": 76, "817967": 97, "81827267": 83, "818273": 83, "818289": 83, "81828926": 83, "818380": [68, 69], "81856": 56, "819223": 95, "819507": 77, "819723": 81, "8197844": 110, "819977": 60, "82": [87, 95, 96, 110, 142], "8202": 55, "820366": 78, "8209": 55, "820963": 65, "821": 141, "8210": 55, "821021": 70, "821274": 62, "821457": 80, "821566": 83, "821855": 95, "821970": 77, "821995": 69, "8221": 53, "822289": [79, 143], "82228913": 143, "822482": 70, "8227": 80, "822822": 70, "823247": 83, "823273": [68, 69], "823357": 60, "824350": [68, 69], "824657": 77, "824701": 70, "824750": 70, "824889": 70, "824900": 61, "824961e": 80, "8250": 53, "825587": 69, "825617": 78, "825862": 83, "825980": 70, "8259803249536914": 70, "8260": 79, "826065": [68, 69], "826111": 60, "826426": [97, 99], "826467e": 68, "826492": 83, "826519": [36, 95], "82666866e": 110, "82684324": 86, "827": 51, "827375": 58, "827381": 83, "827735": 83, "827938162750831": [72, 73], "828058": 80, "828079": 62, "828157": 65, "828618": 75, "828778e": 68, "828912": 68, "828915": [72, 73], "829543": 70, "829730e": 69, "829764": 95, "83": [95, 96, 110, 142], "830263": 77, "830273": 65, "830301": 82, "830442": 68, "830467": 68, "831": 84, "831019": 70, "831190": 69, "831278": 68, "831741": 68, "831833": [97, 101], "832086": 83, "8324": 97, "83268209": 110, "8326928": 86, "832693": 86, "832875": 83, "83287529": 83, "833024": 78, "833065": 65, "833227e": 96, "833464": 80, "833907": 78, "834133": 75, "835": 84, "8350": 80, "835035": 77, "835239": [97, 101], "835344": 65, "835596": 80, "835750": 75, "835935": 69, "836234": 97, "838114": 83, "838235": 81, "838457": 80, "83905": 20, "839286": 60, "84": [56, 74, 84, 95, 96, 97, 110, 142], "840041": 80, "840303": 83, "84030318": 83, "840673": 68, "840718": [97, 99], "840836": 83, "840995e": 79, "841": [54, 78], "841132": 79, "8415": 55, "841847": 80, "842132": 97, "842405": 70, "842625": 78, "842682": 60, "842746": 83, "8428": 79, "842853": 83, "843016": 81, "843018": 95, "843730": 78, "843796": 68, "8440": 80, "844107e": 69, "844308": 83, "8443622": 110, "844549": [72, 73], "844663": 65, "844667": 126, "844707": 83, "844889": 78, "845241": 84, "845534": 77, "846069": 60, "846388": 70, "847029": 69, "847555": 68, "847595": [35, 95], "847948": 70, "847962": 68, "847966": 80, "848688e": 77, "848757e": 79, "848868": 70, "84929323": 110, "84930915e": 110, "849427": 95, "849747": 86, "8497f641": 56, "8499": 80, "85": [13, 70, 74, 80, 83, 88, 95, 96, 110], "8500000000000002": [70, 80, 83], "850038": 65, "850321": 78, "850338": 62, "850439": 69, "850575": [68, 69], "850656": 75, "850664": 60, "850794": 83, "851": 141, "851198": 80, "8513": 56, "851366": 78, "852": 80, "85265193": 76, "85280376": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "85355510": 110, "85397773": [97, 99], "855035": 68, "85546069": 60, "855693": 60, "855780": 83, "855829": 61, "855862": 69, "85630379": 60, "856404": 73, "856758": 95, "8571": 53, "857161": 83, "857515": 95, "857544": 78, "85761926": 110, "857765": 80, "858212e": 69, "858952": 65, "859": 80, "85911521e": 110, "85912862": 126, "859129": [111, 126], "859697": 62, "8597": 79, "85974356": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "85c5": 56, "85e": 55, "86": [95, 96, 110, 142], "860663": 126, "860804": 83, "860992": [60, 80], "861": 51, "861019": 65, "861210": [97, 101], "861519": 68, "861734": 60, "862043": [72, 73], "862359": 70, "863290": 60, "863678": 60, "863772": 79, "863982270": 111, "864": 84, "86415573": 55, "86424193e": 110, "8644": 56, "864664": 65, "864741e": 80, "865313": 80, "865562": [68, 69], "865854": 80, "865860": [79, 80], "865914": 69, "866102": [68, 69], "866179899731091": 89, "866179900": 89, "866579": 80, "866798": 80, "86680165": 110, "86684446": 60, "867": 81, "867201": 77, "867384": 60, "867565": 83, "8679": 80, "868": 56, "8680726": 110, "8685788": 83, "868579": 83, "86897905": [97, 101], "869": [56, 84], "869020": 70, "869195": 69, "869280": 61, "869398": 69, "8694": 61, "869477": 68, "869585": 39, "869586": 74, "87": [55, 68, 74, 78, 95, 96, 110, 142], "870": 62, "8700": 55, "870099": [72, 73], "870260": 83, "870332": 83, "870444": 79, "870857": 83, "871": 56, "871545e": 68, "871923": 69, "871972": 75, "872": 84, "872132": 69, "872222": 80, "872727": 68, "872768": 83, "872852": 83, "87290240e": 110, "872994": 80, "87309461": [97, 101], "873137": 60, "873198": 80, "873465": 60, "873677": [72, 73], "873848": 62, "87384812361": 53, "87384812362": 53, "87430335": 126, "874303353": 126, "874702": [72, 73], "874724": 62, "874900": 60, "8750": 80, "8759": 80, "876": 84, "876083": 80, "876233": 62, "87623301": 53, "876431e": 70, "876549": 80, "87674597e": 110, "8768": 53, "876982": 60, "8771": 80, "877153": 80, "877455": 82, "877833": [68, 69], "877903": 65, "878281": 83, "878289": 80, "878402": 68, "878746": 65, "878847e": 80, "878895": 65, "878968e": 68, "879": 97, "879049": 80, "879058": 77, "879103": 70, "879434": 62, "879509": 68, "87e": 55, "88": [55, 74, 84, 95, 110], "880106": 78, "880579": 83, "880591": 82, "880808e": 80, "880880e": 80, "880886": 79, "8810": 79, "881126": 61, "881201": 80, "88125046e": 110, "881465": 59, "881581": 33, "88173062": 54, "881937": 65, "882475": 70, "882641": 79, "882928": 65, "883485": 69, "883617": 60, "883622": 83, "883914": 70, "883953": 77, "884132": 83, "8843": 86, "8845": 53, "884821": 95, "884996": 70, "8850": 55, "885065": 83, "885832": 84, "885956": 68, "885978": [72, 73], "886041": 69, "886086": [68, 69], "886266": 80, "88629": 53, "886314": 69, "88664": 56, "887182": [97, 98], "887219": 60, "887345": 80, "887372": 60, "887556": 70, "887648": 69, "887680": 68, "8879": 61, "888146": 78, "8881461": 54, "888352": 65, "888445": 69, "888775": 73, "888804": 80, "889": 51, "889293": 83, "889326": 69, "88943475": 60, "889566": 86, "889638": 65, "889733": 83, "889792": 69, "88988263e": 110, "889913": [68, 69], "889963": 83, "88ad": 56, "89": [55, 69, 95, 110, 141, 142], "890": [54, 78], "890204": [97, 101], "89027368": 126, "890273683": 126, "890318": 68, "89035917": 74, "890372": [63, 91, 93, 140], "8903720000100010000010": [56, 91, 93, 140], "8904": 51, "890454": 96, "89052597": 60, "890665": 75, "890855": 65, "8909": [54, 79, 143], "891527": [97, 101], "891606": 79, "891752": 69, "891997": 68, "892": 56, "892648": 83, "89273": 97, "892796": [68, 69], "892828": 75, "893": 56, "8932105": 54, "89332651": 60, "893461": 65, "893649": [68, 69], "893851": 83, "893948": 60, "894": 56, "89409865": 60, "894307e": 80, "894448": 69, "8946549": 57, "89472978": [97, 101], "895106": [68, 69], "895308": 80, "895333": 83, "89542579": 60, "895442": 79, "895690": [68, 69], "895768e": 70, "896023": 83, "896182e": 75, "896263": 75, "896761": 61, "897": 81, "897220": 83, "897240": 80, "8974": 79, "897451": 68, "897495e": 69, "898183": 65, "898722": 83, "899021": 95, "899250": 65, "899460": 83, "899654": 65, "899662e": 68, "899716": 69, "8bdee1a1d83d": 56, "8da924c": 56, "8e3aa840": 56, "9": [20, 22, 23, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 89, 91, 93, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 133, 139, 140, 142, 143], "90": [15, 55, 57, 58, 70, 80, 83, 88, 95, 110, 142], "9000000000000002": [70, 80, 83], "900000e": 80, "900021": 96, "900127": [97, 101], "900829": 75, "901013": 69, "901148": 83, "90136": 79, "901360": 79, "90145324": [97, 101], "901526": 74, "901683": 80, "901705": 68, "902": 126, "9023802": 60, "902573": 70, "902920": 95, "903056e": 83, "903339": 70, "903351e": 70, "903418": 78, "903674": 68, "903681": 83, "903767": [68, 69], "904156": 70, "9041560442482157": 70, "904315": 68, "904396": 68, "905042": 69, "905494": 70, "905858": 87, "905951": 86, "906072": 95, "9061": 80, "906716732639898": [72, 73], "906757": 66, "907115": 83, "907176": 83, "907214": 60, "90724697": 60, "9073": 80, "907491": 70, "907801": 78, "907879": 65, "90794478": 126, "907944783": 126, "907961": 80, "908024": 87, "908663": 69, "908767": 77, "909304": [68, 69], "909571": 65, "90963122e": 110, "909942e": 95, "909975": 80, "909997": [79, 143], "91": [85, 95, 110, 142], "910000e": 80, "910560": 61, "910895": 69, "9109": 56, "91099967": 60, "91102953": 83, "911030": 83, "911133": 60, "9112": 79, "911277": 69, "911662": 72, "911970": 60, "912230": [68, 69], "912241": 60, "9126": [55, 143], "9127": [55, 143], "912812": 62, "912903": 68, "913": 56, "91315015": 54, "913280": 87, "913371": 69, "913415e": 68, "913485": 80, "913585": 75, "913628": 60, "913774": 70, "9142": 80, "91438767e": 110, "9145": 53, "914598": 65, "915": [55, 56, 79, 80], "915000e": [79, 80], "915260e": 68, "915488": [72, 73], "9158080176561963": 77, "916236": 53, "916247": 60, "916359": 65, "916528": 72, "9166667": 56, "916914": 83, "916930": 68, "917": 56, "917000": 69, "917066": 80, "917205e": 60, "917248": 83, "91724807": 83, "917436": 83, "917811": 60, "918": 84, "918227": 70, "918293": [97, 101], "91885333": 110, "919432": 83, "919691": 60, "9197": 80, "919820": 62, "919969": 68, "91e": 55, "92": [60, 85, 95, 96, 110, 142], "920052": 69, "920335": 80, "920337": 73, "92057994": 60, "920645": 80, "9209": 53, "9210": 80, "921061": 87, "921198": 75, "921256e": 69, "921372": 70, "921739e": 60, "921778": 75, "921913": 78, "921956": [68, 69], "921e4f0d": 56, "922160": 80, "922251": 68, "9223": 80, "922668": 75, "922996": 78, "923074e": 70, "923517": 88, "923607": 83, "92369755": 54, "923804": 70, "923943": 143, "923977": 80, "924002": 83, "9243": 80, "924329": 60, "924396": [72, 73], "924443": 65, "924634": 59, "9248": 56, "924821": 70, "924843": 78, "924921": 95, "925": 58, "925248": [72, 73], "925660": 68, "925736": 70, "925957": 72, "925994": 79, "925995": 69, "926227": 69, "926621": 70, "926901": 77, "927": 52, "927074": 83, "927178": 62, "927232": 80, "9274": 80, "927511": 62, "927950": 80, "92827999": 97, "92881435e": 110, "928947": 78, "92905": 54, "92907528": 60, "929643": 68, "92972925e": 126, "929729e": [111, 126], "93": [55, 75, 85, 95, 96, 110, 142], "9304028": 54, "93074943": [97, 101], "93085999": 110, "931": [51, 90], "93136496": 110, "931479": 83, "9314879": 110, "931540": 62, "931978": 140, "932027": 70, "932404e": 80, "9325": 53, "9327": 53, "932810": 62, "932973": 83, "93300": 97, "933322": 69, "933671": 69, "933857": 69, "933996": 70, "934058": 68, "934243": 69, "934433": [68, 69], "9345": 56, "934500": 69, "934511": 126, "934549": 80, "93458": 86, "934963": 69, "934992": 70, "935": [40, 76, 97], "935220": [97, 98], "935591": 83, "935730": 83, "935764": 69, "935989": 78, "9359891": 54, "93648": 88, "936494": 68, "936739": 83, "937": 81, "937116": 78, "937586": 80, "938": 126, "938263": [97, 101], "938836": [97, 101], "939": 81, "939068": [72, 73], "9392": 80, "939250": 68, "939354": 60, "939458": 68, "9395": 80, "93958082416": 143, "939921": 60, "94": [58, 76, 95, 110, 142, 143], "94018055": 60, "940354721701296": 70, "940355": 70, "940373": 80, "941337": 60, "941440": 68, "941724": 80, "941788": 72, "94213649": 60, "942139": 73, "942312": 83, "942377": 60, "942460e": 83, "942489": 80, "9425": 53, "942550": 80, "942655": 60, "942661": 78, "942823": 80, "94309994e": 110, "943548": 75, "943693": [97, 98], "943938": 83, "943949e": 83, "944253e": 83, "944266": [72, 73], "94427158": [97, 101], "944280": 80, "94441007e": 110, "944495": 62, "94473": 57, "9448": 61, "944810": 61, "944940": 60, "945609": 62, "94581063": 60, "945881": 68, "946": 81, "946180": 75, "94629": 88, "946297": 70, "946406": 73, "946433": 83, "946533": 68, "946658": 80, "946968": 70, "947440": 82, "947466": 96, "947613": 69, "947855": 65, "9480": 80, "948112": 84, "948154e": 72, "948785e": 68, "948868": 80, "948975": 74, "94906344": 54, "949241": 126, "949456": 83, "949866": 69, "95": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 53, 55, 57, 58, 59, 60, 61, 62, 65, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 83, 84, 86, 87, 88, 95, 97, 110, 126, 127, 133, 142, 143], "9500": 80, "950158": 68, "950545": 66, "95062986e": 110, "951502": 83, "951532": 78, "951919": 62, "951920": 82, "952": [55, 84, 143], "952146": [97, 101], "9523": 53, "952756": 62, "952839": 83, "95305": 57, "95311164": [97, 101], "953295": 62, "9534": 80, "953683": 78, "953704": 68, "95372559e": 110, "953884": 97, "954": 126, "95401167e": 110, "955005e": 80, "9551": 80, "9552": 53, "955541": [36, 95], "95559917": 96, "955701": 68, "955e": 97, "956047": 54, "9561": 53, "956574": 80, "956588": 97, "956724": 70, "9567242535070148": 70, "956877": 69, "956892": 80, "957229": 73, "957375": 78, "957745": 70, "9579": 55, "957996": 70, "958": 126, "9580": 55, "958105": 95, "958541": 80, "958636": 74, "959132": 69, "959384": 69, "959613": 95, "95969117": 60, "95e": 55, "96": [55, 68, 69, 81, 95, 110, 142], "96001375": 60, "960236": 97, "96049001": 60, "9605": 80, "960808": 70, "960834": 69, "9609": 53, "961539": 80, "961704": 62, "961962": 70, "962364": 65, "962373": 69, "962954": 69, "963055": 80, "963427e": 69, "964025e": 83, "964065e": 69, "964261e": 78, "964318": 80, "96457916": 60, "964676": 61, "9647": 53, "965": 81, "96517902": 60, "965341": 69, "965531": 97, "965696": 68, "965774": 80, "96582": 96, "966015": 83, "966097": 37, "966659": 70, "9666592590622916": 70, "967092": 69, "967467": 86, "967504": 61, "967584": 60, "968127": 65, "968134e": 83, "968258e": 68, "968577": 58, "968992": 60, "969141": [95, 96, 97], "9699": 79, "969925e": 69, "97": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 58, 59, 60, 61, 62, 63, 65, 66, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 95, 96, 97, 98, 99, 101, 110, 111, 126, 140, 142, 143], "970065": 83, "970150": 69, "970611": 62, "971058": [72, 73], "97150622": 60, "972509": 69, "972745": 68, "972748": 70, "9727508": 110, "97276281": 83, "972763": 83, "97314470": 54, "973156": 95, "973229": 69, "973241": 83, "973331": 80, "973741": 69, "973890": 69, "974202": 70, "974213": 69, "97441062": [72, 73], "974414": 70, "974487": 68, "97470872": 86, "9748910611": 54, "975": [68, 69, 72, 73, 75, 76, 81], "975289": 65, "9753": 56, "975447": 58, "975450": 69, "975461": 78, "975592": 65, "976088": 83, "97619643": [97, 101], "976548e": 69, "976562": 83, "977": 81, "977202": 69, "977280": [68, 69], "977295": 80, "977507": 69, "977715e": 60, "977820": 68, "978": 143, "978303": 77, "9787": 80, "978977": 83, "979": [81, 84], "979384": 75, "979424": 60, "979475": 68, "979702": 68, "979857": 68, "979882": 62, "979966": 75, "979971e": 68, "98": [68, 69, 80, 95, 110, 142], "980026": 80, "9802393": 54, "980440": 69, "980643e": 70, "981104": 82, "981403": 69, "981438": 68, "981634": 60, "981672": 70, "981715": 68, "982019e": 69, "982353e": 80, "982417": 70, "982720": 68, "982797": 82, "982895": 60, "983192": 83, "983253": 68, "983492": 60, "983759": 143, "98393441": 86, "984024": 82, "984083": [72, 73], "984551": 32, "984562": 83, "98457968": 60, "984866": 126, "984872": [68, 69], "984937": 70, "98505871e": 110, "985207": [68, 69], "985654": 69, "986249": 79, "986383": 80, "986417": 68, "986529": 62, "98673": 74, "9870004": 56, "987220": 80, "987307": 77, "9875": 53, "987726": 69, "9880384": 56, "988307": 60, "988421": [68, 69], "988463": 83, "988541": 68, "988624": 62, "988709": 80, "988780": 80, "989732": 81, "99": [55, 65, 68, 69, 81, 95, 110, 142], "990049": 81, "990210": 80, "990609": 61, "990903": 68, "991": 56, "9914": [79, 80, 86], "991444e": 72, "9915": [55, 79, 80, 86], "991512": 55, "991963": [68, 69], "991977": 80, "991980": 60, "991988": 68, "992": 84, "99232145": 86, "992582": [68, 69], "992855": 62, "993201": 69, "993416": 75, "993575": 80, "993934": 60, "994": [81, 84], "994168239": 54, "994208": 65, "994214": 80, "994238e": 60, "994332": 66, "994377": 68, "9944": [76, 97, 99], "9948104": 57, "994851": 80, "994937": 73, "994945": 60, "995015": 80, "9951": 53, "995248": 83, "99549118e": 110, "99571372e": 110, "9961": 79, "9961392": 54, "996313": 68, "996892": 77, "996934": 78, "997": 81, "9970": 80, "997034": 88, "997494": 88, "99754397": 60, "997571": 78, "997621": 70, "997934": [72, 73], "998056": 60, "998063": 66, "99864670889": 143, "998766": 80, "999": [58, 59, 74, 86, 143], "999207": 83, "9995": [59, 68, 69], "9996": [59, 68, 69], "9996553": 55, "9997": [59, 68, 69], "99978573": 60, "9998": [59, 68, 69], "9999": [59, 68, 69], "99c8": 56, "A": [7, 9, 10, 13, 14, 18, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 51, 52, 53, 55, 56, 60, 61, 62, 66, 67, 71, 73, 76, 77, 81, 82, 84, 85, 86, 87, 90, 91, 93, 95, 96, 97, 98, 101, 109, 126, 127, 128, 134, 135, 136, 137, 138, 140, 141, 143], "ATE": [9, 33, 37, 55, 63, 65, 75, 79, 86, 87, 95, 97, 106, 109, 111, 119, 127, 135], "ATEs": [65, 81], "And": [57, 81, 88, 127, 129], "As": [52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 65, 67, 68, 69, 70, 71, 74, 75, 76, 77, 78, 79, 80, 81, 83, 87, 88, 89, 96, 97, 101, 110, 111, 115, 117, 126, 127, 128, 137, 143], "At": [9, 10, 26, 54, 58, 59, 61, 65, 74, 76, 78, 80, 83, 143], "Being": 143, "But": [75, 76], "By": [53, 54, 78, 84, 87, 96, 97, 101, 127, 133], "For": [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 33, 39, 43, 47, 51, 53, 54, 56, 58, 60, 61, 62, 65, 66, 74, 75, 76, 77, 78, 80, 82, 84, 86, 87, 89, 90, 91, 93, 95, 96, 97, 98, 99, 101, 102, 103, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 131, 132, 135, 137, 139, 140, 143], "ITE": [14, 65], "ITEs": 65, "If": [4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 52, 54, 58, 60, 61, 62, 67, 68, 69, 75, 76, 78, 80, 84, 90, 91, 93, 95, 96, 97, 99, 104, 111, 112, 114, 115, 116, 119, 126, 127, 128, 129, 131, 132, 133, 136, 137, 138, 143], "In": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 46, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 128, 129, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143], "It": [21, 53, 54, 55, 68, 69, 71, 72, 73, 78, 79, 80, 84, 85, 87, 96, 110, 138, 142], "No": [12, 51, 53, 55, 56, 57, 58, 60, 61, 62, 63, 65, 74, 79, 80, 84, 86, 88, 91, 92, 93, 96, 97, 99, 101, 111, 126, 140, 141], "Not": [97, 102], "Of": [76, 126, 143], "On": [52, 67, 77, 81, 85, 90, 141], "One": [55, 60, 79, 80, 87, 95, 126], "Or": 40, "Such": [87, 96], "That": [40, 143], "The": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 105, 107, 108, 109, 110, 111, 116, 119, 123, 125, 126, 127, 128, 129, 131, 132, 133, 134, 135, 137, 138, 139, 141, 142, 143], "Then": [26, 70, 83, 85, 97, 126, 127, 137, 138, 139], "There": [55, 79, 87, 97, 102, 139, 143], "These": [32, 55, 56, 61, 64, 77, 79, 82, 84, 86, 95, 97, 143], "To": [28, 51, 52, 54, 55, 56, 57, 58, 60, 61, 62, 65, 66, 67, 68, 69, 70, 72, 73, 75, 76, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 90, 91, 93, 95, 96, 97, 110, 126, 127, 128, 133, 137, 139, 140, 143], "Will": [97, 100, 111, 113, 127, 130], "With": [13, 68, 69, 96, 141], "_": [52, 54, 59, 60, 62, 67, 68, 69, 70, 71, 72, 73, 78, 79, 80, 82, 83, 84, 85, 89, 90, 95, 97, 102, 110, 111, 126, 127, 128, 133], "_0": [52, 54, 67, 71, 78, 89, 90, 110, 111, 124, 125, 126, 127, 137], "_1": [9, 10, 14, 25, 26, 57, 81, 88, 111, 124, 125], "_2": [9, 10, 14, 25, 26, 81], "_3": [9, 10, 14, 25, 26], "_4": [9, 10, 14, 25, 26], "_5": [9, 14], "__": [46, 47], "__init__": [77, 85], "__version__": 139, "_a": [111, 115], "_all_coef": 110, "_all_s": 110, "_b": [111, 115], "_compute_scor": 28, "_compute_score_deriv": 28, "_coordinate_desc": 78, "_d": [84, 97], "_est_causal_pars_and_s": 142, "_estimator_typ": 77, "_h": [84, 97], "_i": [52, 57, 67, 83, 88, 90, 97, 102], "_id": 110, "_j": [9, 10, 14, 16, 25, 26, 54, 78, 126], "_l": 96, "_lower_quantil": 60, "_m": [96, 110], "_mean": 60, "_n": [111, 112, 114, 115, 119, 126, 127, 133, 136], "_n_folds_per_clust": 78, "_offset": 96, "_pred": 96, "_rmse": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "_upper_quantil": 60, "_x": 41, "_y": [84, 97], "a0": 77, "a09a": 56, "a09b": 56, "a1": 77, "a3d9": 56, "a4a147": 81, "a5e6": 56, "a5e7": 56, "a6ba": 56, "a79359d2da46": 56, "a840": 56, "a_": [57, 88], "a_0": 17, "a_1": 17, "a_j": [97, 103], "ab": [53, 85, 138], "ab71": 56, "abadi": [7, 58], "abb0fd28": 56, "abdt": [63, 91, 93, 140], "abl": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 52, 67, 76, 80, 81, 96, 127, 128, 137], "about": [22, 24, 55, 58, 59, 76, 79, 85, 97, 138, 140, 143], "abov": [52, 55, 61, 65, 67, 68, 69, 72, 73, 76, 77, 79, 81, 82, 83, 84, 87, 90, 95, 96, 97, 101, 102, 139], "absolut": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 76, 96], "abstract": [28, 53, 54, 78, 111, 138, 142], "abus": [60, 97, 101, 102], "acc": [30, 53], "accept": [25, 95, 96], "access": [42, 43, 53, 55, 60, 61, 62, 72, 73, 74, 76, 86, 96, 127, 133, 143], "accord": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 55, 57, 58, 61, 65, 67, 70, 79, 83, 84, 87, 88, 96, 97, 126, 127, 129, 131, 132, 134, 135, 143], "accordingli": [57, 58, 76, 77, 79, 84, 88], "account": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 54, 55, 78, 79, 80, 86, 87, 127, 133, 137, 143], "accumul": [55, 79, 80, 86], "accuraci": [42, 46, 53, 97], "acemoglu": 141, "achiev": [54, 75, 78, 82, 87, 97, 126], "acic_2024_post": 81, "acknowledg": [55, 56, 79], "acm": 141, "acov": 141, "across": [21, 25, 55, 79, 81, 143], "action": 142, "activ": [4, 5, 6, 139, 142], "actual": [40, 60, 74, 87], "acycl": [57, 88, 143], "ad": [4, 5, 6, 7, 8, 28, 42, 43, 46, 47, 74, 91, 93, 96, 97, 126, 127, 128, 142], "adapt": [32, 79, 142], "add": [53, 54, 57, 58, 59, 63, 65, 72, 73, 74, 81, 83, 84, 86, 87, 88, 96, 97, 141, 142], "add_trac": 87, "addit": [14, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 44, 60, 61, 62, 71, 87, 92, 93, 96, 97, 98, 111, 120, 127, 134, 135, 137, 141, 142], "addition": [9, 10, 65, 70, 80, 86, 96, 97, 110, 126, 127, 133, 140], "additional_inform": 21, "additional_paramet": 21, "address": 87, "adel": 141, "adj": [84, 87], "adj_coef_bench": 87, "adj_est": 87, "adj_vanderweelearah": 87, "adjust": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 45, 54, 59, 75, 78, 80, 86, 87, 95, 97, 102, 126, 127, 133, 141, 142, 143], "adopt": [58, 97, 99, 102], "advanc": [77, 94, 110, 141], "advantag": [52, 53, 55, 65, 67, 79, 80, 90, 139], "advers": [127, 128], "adversari": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 86, 127, 133, 137], "ae": [52, 54, 55, 57], "ae56": 56, "ae89": 56, "aesthet": 52, "aeturrel": 18, "afd9e4": 81, "affect": [65, 71, 97, 142, 143], "after": [53, 55, 56, 57, 58, 71, 79, 80, 87, 88, 95, 96, 97, 102, 127, 129, 133, 139, 143], "after_stat": 52, "ag": [55, 79, 80, 82, 86, 143], "again": [52, 53, 54, 55, 57, 58, 60, 65, 67, 74, 77, 78, 79, 84, 85, 86, 87, 88, 90, 127, 129], "against": [58, 74, 76, 82, 96], "agebra": 95, "agegt54": [56, 63, 91, 93, 140], "agelt35": [56, 63, 91, 93, 140], "agg": [53, 60, 85], "agg_df": 60, "agg_df_anticip": 60, "agg_dict": 60, "agg_dictionari": 60, "agg_did_obj": [97, 98], "aggrag": [97, 98], "aggreg": [21, 24, 53, 89, 98, 110, 142], "aggregate_over_split": 40, "aggregated_eventstudi": [60, 61, 62], "aggregated_framework": [60, 61, 62, 97, 98], "aggregated_group": 60, "aggregated_tim": [60, 62], "aggregation_0": 21, "aggregation_1": 21, "aggregation_color_idx": 21, "aggregation_method_nam": 21, "aggregation_nam": 21, "aggregation_weight": [21, 60, 61, 62], "aggt": 53, "ai": 62, "aim": 84, "aipw": 81, "aipw_est_1": 81, "aipw_est_2": 81, "aipw_obj_1": 81, "aipw_obj_2": 81, "air": [54, 78], "al": [7, 8, 11, 13, 16, 17, 52, 54, 55, 56, 58, 67, 68, 69, 70, 71, 72, 73, 76, 78, 79, 80, 83, 86, 90, 97, 99, 102, 110, 111, 117, 119, 120, 121, 126, 127, 128, 137, 138, 140, 142], "alexandr": [71, 141], "algebra": 97, "algorithm": [51, 53, 54, 56, 57, 58, 60, 61, 62, 65, 67, 70, 75, 76, 78, 80, 83, 86, 88, 94, 96, 97, 99, 101, 110, 111, 126, 142, 143], "alia": [42, 43, 46, 47], "align": [52, 54, 57, 59, 60, 67, 70, 76, 78, 79, 81, 82, 83, 88, 97, 101, 102, 111, 115, 142], "all": [4, 5, 6, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 46, 47, 48, 52, 53, 54, 55, 57, 58, 65, 67, 74, 75, 76, 77, 78, 79, 80, 82, 84, 87, 88, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 102, 110, 111, 115, 126, 127, 137, 138, 139, 142], "all_coef": 110, "all_dml1_coef": 89, "all_s": 110, "all_smpl": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 75], "all_smpls_clust": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "all_z_col": [54, 78], "allow": [4, 5, 6, 20, 21, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 55, 60, 65, 79, 80, 84, 93, 95, 96, 97, 110, 111, 126, 138, 142, 143], "almqvist": 141, "along": 96, "alpha": [15, 17, 20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 55, 57, 60, 63, 65, 67, 68, 69, 70, 71, 75, 76, 77, 78, 79, 80, 83, 89, 90, 95, 96, 97, 110, 111, 126, 127, 128, 129, 131, 132, 133, 134, 135, 136, 137], "alpha_": [16, 54, 78, 96], "alpha_0": [127, 137], "alpha_ml_l": 71, "alpha_ml_m": 71, "alpha_x": [12, 32, 97], "alreadi": [26, 57, 58, 60, 85, 88, 96, 97, 99], "also": [20, 22, 23, 24, 29, 32, 33, 39, 51, 52, 53, 54, 55, 56, 58, 60, 61, 65, 66, 67, 68, 69, 72, 73, 74, 76, 77, 78, 79, 80, 82, 84, 86, 87, 90, 95, 96, 97, 110, 111, 126, 127, 128, 139, 140, 142, 143], "alter": [54, 78], "altern": [53, 55, 56, 61, 79, 82, 94, 96, 126, 138, 140], "although": 87, "alwai": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 47, 53, 84, 142], "always_tak": [32, 55, 79], "alyssa": 141, "amamb": 78, "american": [15, 81], "amgrem": 78, "amhorn": 78, "amit": [87, 141], "amjavl": 78, "ammata": 78, "among": [55, 71, 79, 80, 86, 87], "amount": [24, 55, 77, 79, 80, 143], "amp": [51, 54, 56, 57, 58, 60, 61, 62, 78, 80, 86, 88], "an": [4, 5, 6, 9, 10, 14, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 52, 53, 54, 55, 56, 57, 60, 61, 62, 65, 67, 68, 69, 71, 74, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 95, 96, 97, 98, 101, 102, 103, 110, 111, 126, 127, 128, 133, 138, 140, 141, 142, 143], "analog": [27, 28, 54, 60, 61, 78, 80, 86, 95, 97, 99, 111, 112, 114, 115, 126, 127, 133], "analys": 143, "analysi": [20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 52, 54, 55, 67, 78, 79, 80, 85, 90, 94, 95, 97, 101, 128, 133, 137, 138, 142], "analyst": 85, "analyt": [81, 83], "analyz": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 55, 79, 80, 86, 143], "ancillari": 87, "andrea": 141, "angl": 55, "angrist": 81, "ani": [51, 52, 53, 56, 57, 58, 66, 67, 85, 87, 88, 90, 97, 139, 143], "anna": [9, 10, 14, 20, 22, 23, 25, 26, 53, 58, 60, 61, 62, 97, 98, 99, 101, 102, 141], "annal": [126, 141], "anneal": 96, "annot": 52, "annual": 141, "anoth": [52, 53, 54, 55, 67, 76, 77, 78, 85, 90, 96, 97, 104], "anticip": [22, 24, 25, 53, 61, 62, 97, 98, 101, 102], "anticipation_period": [22, 24, 25, 60], "anymor": [54, 78], "aos1161": 126, "aos1230": 126, "aos1671": 126, "ap": [55, 79], "ape_e401_uncond": 55, "ape_p401_uncond": 55, "api": [91, 93, 138, 142], "apo": [29, 30, 103, 116, 134], "apoorva": 142, "apoorva__l": 81, "apoorval": 81, "app": 142, "appeal": 87, "append": [67, 76, 85, 90], "appendix": [13, 19, 57, 60, 86, 88, 127, 128], "appli": [8, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 54, 55, 56, 57, 58, 59, 60, 62, 67, 75, 76, 78, 79, 80, 84, 87, 88, 90, 97, 110, 111, 126, 138, 140, 142, 143], "applic": [52, 58, 67, 81, 87, 90, 95, 110, 141, 143], "applicatoin": 61, "apply_along_axi": 82, "apply_cross_fit": [52, 110], "apply_crossfit": 142, "appreci": 138, "approach": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 53, 54, 65, 78, 84, 86, 87, 94, 96, 97, 110, 126, 127, 128, 139, 141, 143], "appropri": [55, 71, 79, 97, 110, 143], "approx": [95, 97, 101], "approxim": [52, 67, 68, 69, 70, 76, 83, 87, 90, 95, 97, 126, 142, 143], "april": 60, "apt": 139, "ar": [4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 67, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 93, 95, 96, 97, 98, 99, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 118, 119, 122, 123, 126, 127, 128, 129, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 142, 143], "arang": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 59, 67, 70, 80, 82, 83, 86, 87, 96], "arbitrarili": [43, 47], "architectur": [111, 141], "arellano": 141, "arg": [77, 84, 95, 97], "argmax": 85, "argmin": 76, "argu": [52, 55, 67, 79, 80, 86, 90, 143], "argument": [16, 17, 18, 20, 22, 23, 25, 26, 29, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 44, 55, 58, 60, 61, 62, 68, 69, 74, 76, 79, 80, 85, 89, 95, 96, 97, 98, 142, 143], "aris": [52, 53, 54, 61, 67, 78, 87, 90, 143], "aronow": 81, "around": [53, 55, 79, 80, 84, 97, 111], "arr": 82, "arrai": [4, 5, 6, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 45, 46, 47, 57, 58, 60, 61, 62, 65, 67, 68, 69, 70, 76, 78, 81, 82, 85, 86, 87, 88, 89, 90, 95, 96, 110, 126, 127, 133, 140, 142, 143], "arrang": 54, "array_lik": 36, "articl": [18, 138], "arxiv": [16, 53, 54, 78, 85, 87, 138, 141, 142], "as_learn": [56, 96], "asarrai": [68, 69], "aspect": [55, 79, 80], "assert": 96, "assess": 53, "asset": [80, 86, 143], "assign": [4, 5, 6, 25, 55, 60, 73, 79, 84, 95, 96, 97, 98, 109, 127, 143], "assmput": [97, 109], "associ": [55, 71, 79, 97, 126, 141], "assum": [51, 54, 58, 66, 78, 81, 82, 85, 87, 97, 99, 101, 102, 111, 112, 114, 115, 126, 127, 137, 143], "assumpt": [53, 54, 55, 57, 58, 59, 60, 61, 76, 78, 79, 81, 84, 88, 97, 99, 101, 102, 109, 126, 143], "assur": 142, "astyp": [60, 62, 66, 84, 87], "asymptot": [27, 28, 52, 54, 67, 78, 90, 110, 126, 141], "ate": 65, "ate_estim": [57, 88], "ates": 65, "athei": 141, "att": [9, 24, 25, 33, 53, 59, 74, 75, 82, 87, 95, 97, 98, 99, 101, 102, 106, 111, 119, 127, 135, 142], "att_": [97, 101], "att_gt": [53, 61, 62], "attach": 53, "atte_estim": 58, "attempt": [42, 43], "attenu": [55, 79], "attr": 55, "attribut": [4, 5, 6, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 45, 46, 47, 76, 77, 89, 92, 93, 96, 110, 111, 126], "attributeerror": [42, 43], "attrict": 97, "attrit": [37, 57, 88, 97, 109], "au": [56, 96, 138, 140], "auc": 53, "author": [53, 87, 138], "auto_ml": 77, "autodoubleml": 77, "autom": 77, "automat": [21, 24, 52, 60, 62, 67, 74, 90, 95, 127, 133], "automl": 142, "automl_l": 77, "automl_l_lesstim": 77, "automl_m": 77, "automl_m_lesstim": 77, "automobil": [54, 78], "autos": 71, "autosklearn": 77, "auxiliari": [52, 67, 90], "avail": [12, 53, 55, 56, 58, 60, 61, 65, 71, 76, 79, 80, 81, 82, 84, 87, 90, 95, 96, 97, 98, 99, 101, 127, 137, 138, 139, 142, 143], "avaiv": 45, "aver": 65, "averag": [9, 10, 25, 26, 29, 30, 32, 33, 39, 51, 53, 56, 57, 58, 59, 60, 62, 66, 74, 80, 81, 82, 84, 86, 87, 88, 94, 98, 99, 102, 103, 104, 105, 106, 109, 116, 119, 126, 134, 135, 141, 142, 143], "average_it": 65, "avoid": [52, 53, 67, 84, 97, 110, 139, 142], "awai": 86, "ax": [21, 24, 59, 60, 61, 62, 65, 67, 68, 69, 70, 72, 73, 76, 77, 78, 79, 80, 81, 83, 84], "ax1": [65, 70, 75, 80, 83], "ax2": [65, 70, 75, 80, 83], "axhlin": [59, 77, 84], "axi": [21, 24, 54, 55, 65, 71, 75, 76, 78, 79, 81, 82, 84], "axvlin": [65, 67], "b": [18, 20, 22, 23, 24, 25, 52, 54, 56, 67, 68, 69, 78, 81, 83, 84, 87, 90, 95, 96, 97, 102, 126, 127, 137, 138, 140, 141], "b208": 56, "b371": 56, "b5d34a6f42b": 56, "b5d7": 56, "b_": 97, "b_0": 17, "b_1": 17, "b_j": 18, "bach": [71, 76, 77, 87, 138, 141, 142], "backbon": 76, "backend": [4, 5, 6, 53, 80, 86, 87, 91, 92, 94, 142], "backward": 142, "bad": 81, "balanc": [55, 60, 61, 79, 80], "band": [53, 94, 143], "bandwidth": [34, 35, 36, 40, 84, 97], "bar": [74, 77, 79, 95, 97, 111, 116, 119, 127, 134], "base": [11, 14, 20, 22, 23, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 45, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 65, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 95, 96, 97, 98, 102, 109, 110, 111, 126, 127, 128, 133, 138, 140, 141, 142, 143], "base_estim": [46, 47, 84], "baseestim": 85, "baselin": [14, 22, 55, 77, 79], "basi": [29, 33, 39, 44, 68, 69, 75, 95], "basic": [53, 54, 55, 58, 62, 78, 79, 80, 81, 84, 86, 87, 94, 96], "basis_df": 75, "basis_matrix": 75, "batch": 56, "battocchi": 141, "bay": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 126], "bb2913dc": 56, "bbotk": [56, 96, 142], "bbox_inch": 67, "bbox_to_anchor": 67, "bcallaway11": [53, 61], "bd929a9e": 56, "bde4": 56, "becam": [55, 79, 80], "becaus": [43, 47, 51, 52, 53, 54, 66, 67, 73, 74, 78, 81, 85, 87, 90, 143], "becker": [56, 96], "becom": [54, 73, 77, 78, 95, 110], "bee": 59, "been": [21, 24, 54, 55, 60, 62, 77, 78, 79, 80, 86, 87, 95, 96, 97, 102, 142], "befor": [25, 53, 55, 59, 60, 62, 65, 74, 79, 83, 87, 97, 99, 143], "begin": [12, 15, 16, 52, 54, 55, 56, 57, 59, 60, 67, 70, 76, 78, 79, 81, 82, 83, 88, 89, 91, 93, 96, 97, 101, 102, 110, 111, 115, 126, 140, 143], "behav": [60, 62, 73, 85], "behavior": [55, 81, 96], "behaviour": 73, "behind": [61, 97], "being": [14, 19, 25, 27, 28, 41, 46, 47, 54, 78, 84, 87, 97, 101, 102, 110, 111, 117, 126, 127, 133, 138], "belloni": [13, 71, 126, 141], "below": [51, 55, 61, 66, 79, 81, 85, 97, 139, 140], "bench_x1": 87, "bench_x2": 87, "benchmark": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 65, 74, 128, 142], "benchmark_dict": [48, 86], "benchmark_inc": 86, "benchmark_pira": 86, "benchmark_result": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39], "benchmark_twoearn": 86, "benchmarking_set": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 65, 74, 86, 87, 127, 128], "benchmarking_vari": 74, "benefit": [52, 55, 67, 79, 90], "bernoulli": 12, "berri": [54, 78], "besid": 140, "best": [29, 33, 39, 43, 44, 47, 60, 68, 69, 72, 73, 77, 139], "best_loss": 77, "beta": [12, 13, 15, 19, 37, 55, 57, 79, 82, 84, 88, 97], "beta_": [57, 88], "beta_0": [11, 57, 82, 88, 95], "beta_a": [9, 10, 87], "beta_j": [12, 13, 15, 19], "better": [53, 60, 65, 76, 87, 97, 102], "between": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 57, 59, 60, 65, 66, 70, 71, 77, 81, 83, 85, 86, 87, 88, 97, 104, 111, 112, 114, 115, 119, 122, 123, 126, 127, 137, 140, 142], "betwen": [51, 66], "beyond": 141, "bia": [19, 51, 57, 66, 71, 84, 87, 88, 94, 97, 109, 110, 111, 124, 125, 127, 137, 141, 142], "bias": [51, 55, 60, 66, 79, 80, 86, 143], "bias_bench": 87, "bibtex": 138, "big": [71, 89, 110, 111, 112, 120, 126, 127, 129, 131, 132, 135, 136, 137], "bigg": [54, 78, 111, 118, 119, 127, 135], "bilia": 8, "bilinski": 141, "bin": [52, 65, 67, 139], "binari": [11, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 37, 39, 41, 51, 53, 55, 56, 58, 66, 74, 75, 76, 79, 81, 82, 87, 95, 96, 99, 102, 105, 106, 109, 127, 134, 135, 142, 143], "binary_outcom": 41, "binary_treat": [11, 68, 72, 74], "bind": 142, "binder": [56, 96, 138, 140, 142], "binomi": [66, 81, 82, 83, 85], "bischl": [56, 96, 138, 140], "black": [52, 56, 63, 91, 93, 140], "blob": 53, "blog": 18, "blondel": [138, 140], "blp": [44, 54, 78], "blp_data": [54, 78], "blp_model": [72, 73], "blue": [52, 54, 57, 78], "bodori": 141, "bond": [55, 79, 80], "bonferroni": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 126], "bonu": [8, 56, 91, 93, 140], "book": [56, 87, 96], "bool": [4, 5, 6, 9, 11, 14, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 74, 84], "boolean": [19, 72, 73, 91, 93, 110], "boost": [51, 55, 58, 60, 66, 76, 79], "boost_class": [55, 79], "boost_summari": 79, "boostrap": [70, 142], "bootstrap": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 60, 61, 62, 65, 68, 69, 70, 72, 73, 80, 83, 94, 95, 97, 98, 110, 111, 138, 140, 142, 143], "both": [10, 11, 20, 21, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 55, 56, 58, 59, 75, 76, 77, 79, 80, 82, 84, 85, 86, 87, 91, 93, 96, 97, 101, 126, 127, 128, 133, 136, 137, 142, 143], "bottom": [54, 55, 76, 78, 79, 80], "bound": [14, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 60, 65, 74, 75, 79, 86, 87, 127, 128, 133, 137, 142, 143], "branch": 56, "brantli": [53, 141], "break": [52, 142], "breviti": 143, "brew": 139, "brewer": 54, "bridg": 87, "brief": 90, "briefli": 61, "bring": [51, 66], "brucher": [138, 140], "bsd": 142, "bst": 79, "budget": [77, 96], "bug": [138, 142], "build": [54, 76, 78, 82], "build_design_matric": [68, 69], "build_sim_dataset": 53, "built": [45, 77, 96, 138], "bureau": [87, 110, 141], "busi": [16, 19, 54, 78, 87, 141], "b\u00fchlmann": 141, "c": [7, 8, 10, 13, 15, 17, 25, 26, 51, 52, 53, 54, 55, 56, 59, 63, 66, 67, 71, 72, 73, 78, 79, 81, 84, 85, 90, 91, 93, 96, 97, 102, 111, 115, 127, 132, 138, 139, 140, 141, 143], "c1": [7, 8, 17, 54, 71, 78, 85, 90, 138, 141], "c68": [7, 8, 17, 54, 71, 78, 85, 90, 138, 141], "c895": 56, "c_": [24, 97, 101, 102, 111, 115, 126, 127, 132], "c_d": [13, 127, 135, 136, 137], "c_y": [13, 127, 137], "ca1af7be64b2": 56, "caac5a95": 56, "calcualt": 82, "calcul": [29, 33, 39, 53, 55, 60, 65, 68, 69, 70, 72, 73, 76, 77, 79, 83, 86, 127, 133, 137], "calendar": [60, 61, 62], "calibr": [76, 77, 87], "call": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 43, 46, 47, 51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 66, 68, 69, 70, 72, 73, 78, 79, 80, 82, 83, 84, 86, 87, 88, 91, 93, 96, 110, 111, 126, 127, 133, 137, 140, 142, 143], "callabl": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 67, 68, 69, 76, 94, 96, 138], "callawai": [25, 53, 60, 61, 62, 97, 98, 101, 102, 141], "camera": 71, "cameron": [54, 78], "can": [4, 5, 6, 9, 11, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 45, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 99, 101, 102, 103, 109, 110, 111, 112, 114, 115, 116, 119, 122, 123, 126, 127, 128, 133, 134, 135, 136, 137, 138, 139, 140, 142, 143], "candid": 87, "cannot": [76, 84, 87, 97, 143], "capabl": [4, 5, 6, 51, 66], "capo": [29, 75], "capo0": 75, "capo1": 75, "capsiz": [65, 77, 81, 84], "capthick": [65, 84], "cardin": [54, 78], "care": [61, 85, 96], "carlo": [9, 10, 11, 14, 68, 69, 72, 73, 87, 141], "casalicchio": [56, 96, 138, 140], "case": [4, 5, 6, 8, 11, 22, 29, 32, 33, 40, 51, 54, 55, 60, 61, 66, 68, 69, 70, 71, 73, 74, 75, 77, 78, 82, 83, 84, 85, 86, 87, 91, 93, 95, 96, 97, 99, 102, 109, 110, 111, 115, 126, 127, 133, 140, 142, 143], "cat": [52, 142], "catboost": 76, "cate": [33, 39, 44, 75, 94, 142], "cate_obj": 95, "cattaneo": [97, 141], "caus": [52, 67, 84, 90], "causal": [12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 54, 55, 56, 57, 66, 67, 75, 76, 77, 78, 79, 81, 86, 88, 89, 90, 91, 93, 94, 97, 102, 110, 126, 127, 133, 141], "causal_contrast": [30, 65, 75, 97], "causal_contrast_att": 75, "causal_contrast_c": 75, "causal_contrast_model": [65, 97], "causaldml": 141, "causalweight": 141, "caution": 126, "caveat": [73, 87], "cbind": 54, "cbook": [60, 61, 62], "cc": 79, "ccp_alpha": [33, 45, 79], "cd": 139, "cd_fast": 78, "cda85647": 56, "cdf": 95, "cdid": [54, 78], "cdot": [9, 10, 14, 25, 26, 41, 54, 59, 60, 70, 74, 78, 81, 83, 84, 87, 95, 97, 98, 101, 102, 110, 111, 115, 116, 119, 120, 124, 125, 126, 127, 132, 134], "cdot1": 74, "cell": 77, "center": [60, 61, 71], "central": [110, 142], "certain": [73, 97, 111, 115], "cexcol": 54, "cexrow": 54, "cf_d": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 60, 65, 74, 75, 86, 87, 127, 128, 133, 134, 135, 136, 137, 143], "cf_y": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 60, 65, 74, 75, 86, 87, 127, 128, 133, 134, 135, 136, 137, 143], "cff": 142, "chad": 87, "chain": 73, "chainedassignmenterror": 73, "challeng": [54, 78, 127, 128], "chang": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 55, 57, 58, 62, 73, 80, 86, 87, 88, 97, 102, 111, 119, 126, 127, 128, 129, 131, 132, 133, 134, 135, 139, 141, 142], "channel": 143, "chapter": [27, 28, 56, 96, 127, 137], "charact": [55, 56, 96, 142], "characterist": [86, 143], "chart": 77, "check": [42, 43, 46, 47, 52, 55, 58, 59, 60, 67, 76, 77, 79, 80, 85, 89, 90, 138, 139, 142], "check_data": 142, "check_scor": 142, "checkmat": 142, "chernozhukov": [7, 8, 13, 15, 17, 52, 54, 55, 67, 71, 76, 77, 78, 79, 80, 85, 86, 90, 110, 111, 119, 126, 127, 128, 137, 138, 141, 142], "chetverikov": [7, 8, 17, 54, 71, 78, 85, 90, 126, 138, 141], "chiang": [16, 54, 78, 141], "chieh": 141, "choic": [4, 5, 6, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 55, 60, 61, 62, 71, 79, 82, 95, 96, 97, 101, 111, 115, 127, 128, 133, 137, 142], "cholecyst": 85, "choos": [51, 55, 61, 66, 67, 71, 76, 79, 80, 89, 97, 101, 110, 111, 112, 114, 115, 119, 122, 123, 126, 140, 143], "chosen": [10, 14, 29, 76, 96, 97], "chou": 81, "chr": 55, "christian": [71, 141], "christoph": 141, "chunk": 96, "ci": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 58, 59, 60, 61, 62, 65, 68, 69, 70, 72, 73, 74, 75, 77, 79, 80, 83, 84, 86, 87, 95, 97, 127, 133, 142, 143], "ci_at": 65, "ci_cvar": [70, 80], "ci_cvar_0": 70, "ci_cvar_1": 70, "ci_joint": [60, 61, 62, 65], "ci_joint_cvar": 70, "ci_joint_lqt": 83, "ci_joint_qt": 83, "ci_length": 58, "ci_low": 65, "ci_lpq_0": 83, "ci_lpq_1": 83, "ci_lqt": [80, 83], "ci_pointwis": 65, "ci_pq_0": [80, 83], "ci_pq_1": [80, 83], "ci_qt": [80, 83], "ci_upp": 65, "cinelli": [87, 127, 128, 141], "circumv": 143, "citat": 142, "claim": 56, "clarifi": [60, 61, 62], "clash": 53, "class": [0, 4, 5, 6, 20, 21, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 55, 56, 57, 58, 60, 61, 62, 63, 65, 74, 75, 77, 79, 80, 85, 86, 88, 89, 91, 92, 93, 95, 96, 97, 98, 110, 111, 126, 138, 140, 142], "class_estim": 84, "class_learn": 80, "class_learner_1": 76, "class_learner_2": 76, "classes_": [77, 85], "classic": [53, 54, 61, 78, 143], "classif": [33, 42, 46, 51, 53, 55, 56, 57, 60, 61, 62, 76, 77, 82, 86, 95, 96, 97, 99, 101, 143], "classifavg": 56, "classifi": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 39, 40, 42, 46, 56, 65, 77, 84, 96, 142], "classifiermixin": 85, "classmethod": [4, 5, 6], "claudia": [141, 142], "claus": 142, "clean": 142, "cleaner": 76, "cleanup": 142, "clear": [54, 78], "clearli": [60, 84], "clever": 76, "clone": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 56, 67, 76, 78, 80, 89, 96, 97, 110, 111, 126, 127, 133, 139, 140], "close": [53, 55, 79, 85, 87, 127, 128], "cluster": [4, 16, 20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 62, 141, 142], "cluster_col": [4, 54, 78], "cluster_var": [4, 16], "cluster_var_i": [4, 54, 78], "cluster_var_j": [4, 54, 78], "cmap": 78, "cmd": 142, "co": [18, 59], "codaci": 142, "code": [18, 29, 33, 39, 51, 53, 54, 55, 56, 57, 66, 71, 79, 90, 95, 96, 97, 110, 111, 126, 139, 140, 142, 143], "codecov": 142, "coef": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 86, 87, 88, 89, 90, 95, 96, 97, 98, 99, 101, 110, 111, 126, 140, 143], "coef_": 87, "coef_df": 54, "coef_valu": 77, "coeffici": [9, 10, 11, 24, 43, 44, 47, 55, 57, 72, 73, 76, 79, 81, 82, 84, 87, 88, 95, 126, 127, 133, 143], "coefs_t": 82, "coefs_w": 82, "coffici": [127, 133], "cofid": 44, "coincid": [59, 75, 80], "col": [52, 54, 73, 79], "col_nam": 60, "collect": [56, 57, 58, 78, 88], "colnam": [54, 76], "color": [21, 24, 55, 57, 59, 65, 67, 68, 69, 70, 77, 78, 79, 80, 81, 83, 84, 87], "color_palett": [21, 24, 60, 65, 67, 78, 79, 80], "colorbar": 78, "colorblind": [21, 24, 60, 65], "colorramppalett": 54, "colorscal": [68, 69], "colour": [52, 54], "column": [4, 5, 6, 58, 59, 60, 61, 62, 63, 65, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 84, 86, 87, 88, 91, 92, 93, 95, 96, 97, 101, 110, 140, 142, 143], "column_stack": [59, 65, 72, 73, 84, 86, 87, 97], "colv": 54, "com": [18, 53, 55, 56, 61, 62, 71, 81, 87, 96, 139], "comb": 71, "combin": [22, 24, 53, 54, 56, 58, 61, 62, 65, 75, 76, 77, 78, 87, 96, 97, 101, 110, 127, 133, 142], "combind": 80, "combined_loss": 71, "come": [89, 96, 111, 127, 128, 138, 143], "command": [139, 142], "comment": [91, 93], "commit": 142, "common": [76, 86, 87, 95, 97, 141], "companion": 141, "compar": [52, 54, 59, 60, 67, 68, 69, 70, 72, 73, 75, 78, 81, 83, 84, 85, 87, 90, 96, 97, 127, 128, 142], "comparevers": 55, "comparison": [60, 65, 76, 81], "compat": [51, 53, 66, 142], "complement": 87, "complet": [77, 90, 127, 133, 139], "complex": [33, 53, 77], "compli": [84, 97], "complianc": [83, 84, 97, 111, 120], "complic": [56, 143], "complier": [55, 79, 80, 83, 84, 95, 97], "compon": [25, 46, 47, 53, 55, 61, 71, 76, 77, 79, 82, 95, 96, 110, 111, 112, 114, 115, 116, 118, 119, 120, 122, 123, 142], "compont": 53, "composit": 141, "compris": 126, "comput": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 48, 52, 53, 55, 56, 60, 61, 62, 67, 79, 80, 85, 86, 87, 110, 111, 127, 128, 129, 131, 132, 133, 134, 135, 138, 141, 142, 143], "computation": [127, 128], "concat": [77, 78, 79, 82, 126], "concaten": [59, 79, 126], "concentr": 126, "concern": 87, "conclud": [84, 87, 143], "cond": [97, 99, 109], "conda": [78, 141, 142], "condit": [9, 10, 11, 27, 28, 29, 31, 33, 39, 52, 54, 55, 57, 58, 59, 60, 65, 67, 74, 75, 78, 79, 82, 84, 87, 88, 90, 94, 97, 101, 102, 126, 127, 134, 135, 137, 140, 141, 142, 143], "conduct": [95, 97, 99, 101, 143], "conf": [53, 83], "confer": 141, "confid": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 53, 54, 55, 57, 58, 60, 61, 62, 65, 68, 69, 70, 72, 73, 75, 78, 80, 83, 84, 86, 88, 94, 95, 97, 110, 111, 127, 133, 140, 141, 142, 143], "confidenceband": 70, "confidenti": 87, "config": 81, "configur": [56, 77], "confint": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 55, 57, 58, 59, 60, 61, 62, 65, 68, 69, 70, 72, 73, 75, 76, 80, 82, 83, 84, 85, 86, 88, 95, 110, 126, 138, 140, 143], "conflict": 139, "confound": [9, 10, 11, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 51, 55, 60, 61, 66, 74, 79, 83, 86, 87, 91, 93, 97, 107, 108, 126, 127, 128, 133, 136, 137, 140, 141, 142, 143], "congress": 141, "connect": [55, 79, 80], "consequ": [9, 10, 54, 74, 78, 86, 95, 97, 99, 127, 128, 134, 135, 137], "conserv": [86, 87, 127, 137], "consid": [31, 32, 33, 34, 35, 41, 52, 54, 55, 57, 58, 59, 60, 61, 62, 67, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 95, 96, 97, 98, 102, 105, 106, 110, 111, 126, 127, 128, 138, 143], "consider": [87, 97], "consist": [38, 39, 43, 47, 55, 58, 77, 79, 80, 81, 87, 90, 91, 93, 97, 102, 107, 108, 140, 142], "consol": [52, 142], "constant": [13, 25, 43, 47, 71, 82, 95, 97, 126], "constrained_layout": 67, "construct": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 44, 56, 59, 60, 68, 69, 70, 75, 80, 85, 86, 89, 95, 111, 117, 125, 126, 142, 143], "construct_framework": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "construct_iv": 78, "constructiv": 54, "constructor": 56, "consum": [54, 78], "cont": 14, "cont_d": 65, "contain": [14, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 46, 47, 52, 54, 55, 60, 61, 65, 67, 68, 69, 72, 73, 76, 78, 79, 85, 90, 92, 93, 95, 96, 97, 98, 101, 126, 127, 128, 133, 142], "context": [87, 97, 109, 143], "contin": [14, 77], "continu": [14, 51, 56, 65, 66, 71, 81, 84, 97, 127, 137, 142, 143], "contour": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 71, 74, 86, 87, 127, 133], "contour_plot": 87, "contours_z": [68, 69], "contrast": [30, 58, 70, 75, 97, 104], "contribut": [139, 142], "contributor": 142, "control": [15, 22, 24, 25, 41, 53, 61, 62, 71, 75, 80, 82, 84, 85, 87, 97, 98, 101, 102, 111, 115, 127, 132, 143], "control_group": [22, 24, 60, 61, 62, 97, 98, 101], "convent": [40, 55, 61, 79, 80, 84, 97, 102], "converg": [52, 67, 76, 78, 90], "convergencewarn": 78, "convers": 78, "convert": [70, 78, 83], "convex": 81, "cooper": 142, "coor": [56, 96, 138, 140], "coordin": 87, "copi": [73, 77, 79, 82, 87], "cor": [127, 137], "core": [25, 58, 60, 61, 62, 63, 65, 70, 74, 78, 79, 80, 83, 86, 88, 91, 92, 93, 96, 140, 142], "cores_us": [70, 80, 83], "correct": [74, 75, 87, 95, 126, 142], "correctli": [42, 46, 58, 81, 86, 127, 137], "correl": [20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 54, 57, 71, 78, 85, 86, 88, 97, 127, 128, 137], "correpond": [60, 97], "correspond": [9, 10, 14, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 55, 56, 58, 59, 60, 61, 62, 65, 67, 68, 69, 71, 75, 76, 78, 79, 80, 82, 83, 86, 87, 90, 95, 96, 97, 99, 101, 102, 103, 109, 110, 111, 115, 126, 127, 128, 132, 133, 135, 137, 142, 143], "correspondingli": 60, "cosh": 18, "coul": 54, "could": [51, 56, 60, 61, 66, 68, 69, 77, 87, 142, 143], "counfound": [9, 10, 83, 86, 95, 127, 137], "count": [65, 79, 80], "counti": 61, "countour": [127, 133], "countyr": 61, "coupl": [55, 79, 80], "cournapeau": [138, 140], "cours": [55, 76, 79, 87, 126, 143], "cov": [9, 37, 41, 84], "cov_nam": [84, 97], "cov_typ": [29, 33, 39, 44, 142], "covari": [4, 5, 6, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 33, 38, 39, 40, 41, 44, 45, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 65, 67, 68, 69, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 85, 86, 87, 88, 90, 91, 92, 93, 95, 96, 97, 99, 101, 107, 108, 109, 111, 112, 114, 115, 126, 127, 128, 140, 141, 142], "cover": [53, 71, 86], "coverag": [60, 76, 84, 85, 95, 142], "cp": [55, 56, 96], "cpu": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "cpu_count": [70, 80, 83], "cran": [56, 141, 142], "creat": [11, 21, 24, 25, 51, 54, 56, 60, 65, 66, 67, 68, 69, 70, 72, 73, 78, 80, 82, 83, 87, 96, 127, 128, 133, 137, 139, 142], "create_synthetic_group_data": 82, "crictial": 110, "critic": [87, 143], "cross": [20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 53, 55, 56, 57, 61, 67, 76, 77, 79, 80, 84, 87, 90, 94, 96, 112, 114, 125, 126, 129, 133, 142, 143], "cross_sectional_data": [23, 26, 58, 97, 99], "crossfit": [76, 97], "crosstab": 81, "crucial": [71, 97, 143], "csail": [138, 140], "csdid": 62, "csv": [62, 71], "cumul": 97, "current": [45, 53, 58, 59, 60, 73, 75, 111, 127, 137, 138, 139, 143], "custom": [21, 24, 52, 53, 61, 67, 87, 96], "custom_measur": 53, "cut": 82, "cutoff": [40, 41, 84, 97], "cv": [56, 79, 96, 110], "cv_glmnet": [54, 55, 56, 57, 96, 97, 126, 140], "cvar": [31, 36, 94, 117, 142], "cvar_0": 70, "cvar_1": 70, "d": [4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 103, 105, 106, 107, 108, 109, 110, 111, 112, 114, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 131, 133, 134, 135, 136, 137, 138, 140, 141, 143], "d0": [70, 83, 126], "d0_true": 83, "d0cdb0ea4795": 56, "d1": [70, 81, 83, 126], "d10": 126, "d1_true": 83, "d2": [81, 126], "d21ee5775b5f": 56, "d2cml": 62, "d3": 126, "d4": 126, "d5": 126, "d5a0c70f1d98": 56, "d6": 126, "d7": 126, "d8": 126, "d9": 126, "d_": [14, 16, 54, 59, 65, 78, 97, 99, 102, 126], "d_0": [97, 103], "d_1": [81, 126], "d_2": 81, "d_col": [4, 5, 6, 24, 51, 52, 54, 55, 56, 57, 60, 61, 62, 66, 68, 69, 72, 73, 75, 78, 79, 80, 82, 84, 85, 86, 89, 90, 91, 92, 93, 96, 97, 98, 101, 110, 111, 140, 142, 143], "d_i": [11, 12, 13, 15, 17, 18, 19, 52, 57, 58, 65, 67, 70, 81, 83, 84, 88, 90, 97, 99, 109], "d_j": [65, 97, 103, 104, 126], "d_k": [97, 104, 126], "d_l": [97, 103], "d_w": 82, "da1440": 81, "dag": [57, 87, 88, 143], "dai": 60, "dark": [52, 67], "darkblu": 54, "darkr": 54, "dash": [57, 60, 65], "dat": [91, 93], "data": [0, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 46, 47, 53, 59, 71, 76, 81, 85, 89, 91, 92, 94, 95, 96, 98, 101, 102, 110, 126, 131, 132, 133, 141, 142], "data_apo": 65, "data_cvar": 80, "data_dict": [40, 68, 69, 72, 73, 74, 84, 97], "data_dml": 86, "data_dml_bas": [55, 68, 69, 72, 73, 79, 80, 82], "data_dml_base_iv": [55, 79, 80], "data_dml_flex": [55, 79], "data_dml_flex_iv": 55, "data_dml_iv_flex": 79, "data_dml_new": 82, "data_fram": 143, "data_lqt": 80, "data_pq": 80, "data_qt": 80, "data_transf": [54, 78, 79], "datafram": [4, 5, 6, 7, 8, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 45, 54, 57, 58, 59, 60, 61, 62, 63, 65, 66, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 92, 95, 96, 97, 99, 111, 126, 127, 128, 133, 140, 143], "dataset": [0, 4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 57, 58, 60, 65, 67, 68, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 133, 140, 143], "datatyp": [62, 142], "date": [24, 25, 96], "date_format": 24, "datetim": [6, 24, 25, 60, 61, 92, 93], "datetime64": [60, 92, 93], "datetime_unit": [6, 24, 60, 92, 93, 97, 98, 101], "david": 142, "db": [55, 79, 80, 86, 143], "dbl": [53, 54, 55, 56, 91, 93, 126, 140, 143], "dc13a11076b3": 56, "ddc9": 56, "de": [51, 66, 141], "deal": [51, 66], "debias": [7, 8, 16, 17, 54, 71, 78, 85, 94, 96, 110, 138, 141, 142], "debt": [55, 79, 80], "decai": [57, 88], "decid": [55, 79, 85], "decis": [33, 51, 55, 66, 79, 80, 95, 97, 141, 143], "decision_effect": 51, "decision_impact": [51, 66], "decisiontreeclassifi": [33, 45, 79], "decisiontreeregressor": 79, "declar": 143, "decreas": 84, "deep": [42, 43, 46, 47, 77], "deeper": 33, "def": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 60, 67, 70, 76, 77, 78, 81, 82, 83, 85, 87, 96, 111], "default": [4, 5, 6, 9, 10, 11, 14, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 53, 54, 57, 58, 60, 61, 62, 72, 73, 76, 78, 82, 84, 86, 87, 88, 89, 95, 96, 97, 110, 126, 127, 133, 134, 140, 143], "default_arg": 60, "default_convert": 78, "default_jitt": 24, "defier": [84, 97], "defin": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 40, 41, 43, 47, 52, 55, 56, 58, 61, 65, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 79, 80, 82, 83, 84, 85, 86, 87, 95, 96, 97, 99, 101, 102, 109, 111, 112, 114, 115, 127, 128, 133, 137], "definit": [18, 60, 72, 73, 75, 97, 101, 127, 134, 135], "defint": 127, "degre": [41, 55, 68, 69, 75, 78, 79, 84, 95, 127, 128], "dekel": 141, "delete_origin": 56, "deliber": 81, "delta": [15, 24, 53, 58, 60, 87, 97, 99, 101, 102, 111, 115, 127, 132], "delta_bench": 87, "delta_i": 53, "delta_j": 15, "delta_t": [25, 60], "delta_theta": [48, 65, 74, 86, 87, 127, 128], "delta_v": 87, "demand": [54, 78, 127, 128], "demir": [7, 8, 17, 54, 71, 78, 85, 90, 110, 138, 141], "demo": [61, 87], "demonstr": [52, 53, 54, 61, 67, 78, 84, 87, 91, 93, 97, 126, 138, 140], "deni": 141, "denomin": [127, 128, 134, 135], "denot": [38, 54, 55, 57, 58, 59, 78, 79, 84, 87, 88, 95, 97, 99, 101, 102, 103, 107, 111, 127, 128, 133, 135, 137], "dens_net_tfa": 55, "densiti": [34, 35, 36, 52, 57, 65, 67], "dep": 63, "dep1": [56, 63, 91, 93, 140], "dep2": [56, 63, 91, 93, 140], "depend": [11, 25, 29, 31, 33, 34, 36, 56, 58, 60, 68, 69, 72, 73, 74, 76, 77, 82, 84, 89, 95, 96, 97, 101, 111, 120, 121, 127, 128, 134, 137, 140, 141], "deprec": [58, 59, 60, 61, 62, 89, 97, 110, 111, 127], "depreci": 142, "depth": [33, 45, 55, 56, 82, 89, 95, 96, 97, 110, 111, 126, 140, 143], "deriv": [20, 22, 23, 25, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 97, 126], "describ": [21, 53, 54, 60, 78, 79, 80, 87, 96, 110, 139, 142], "descript": [55, 62, 63, 86, 96, 110, 111, 115, 127, 128, 132], "deserv": 97, "design": [25, 40, 41, 65, 77, 94, 141, 142], "design_info": [68, 69], "design_matrix": [68, 69, 95], "desir": [10, 56, 82, 97, 139], "detail": [20, 22, 23, 27, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 52, 55, 56, 58, 59, 65, 67, 71, 77, 80, 84, 86, 87, 90, 91, 93, 95, 96, 98, 99, 101, 102, 109, 111, 117, 119, 120, 121, 124, 125, 126, 127, 128, 132, 137, 138, 139, 140, 142, 143], "determin": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 47, 55, 70, 79, 80, 83, 84, 86, 97, 126, 127, 137], "determinist": [82, 84, 95, 97], "deutsch": 138, "dev": [139, 142], "develop": [53, 54, 56, 78, 87, 97, 99, 102, 142], "deviat": [76, 97, 127, 137], "dezeur": 141, "df": [4, 5, 6, 24, 51, 52, 54, 57, 59, 60, 65, 66, 68, 69, 70, 73, 75, 78, 81, 83, 84, 86, 87, 88, 90, 92, 93, 95, 97, 98, 101], "df_agg": 71, "df_anticip": 60, "df_apo": 65, "df_apo_ci": 65, "df_apos_ci": 65, "df_ate": 65, "df_bench": 87, "df_binari": 87, "df_bonu": [56, 91, 93, 140], "df_capo0": 75, "df_capo1": 75, "df_cate": [68, 69, 75], "df_causal_contrast_c": 75, "df_ci": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44], "df_coef": 76, "df_cvar": 80, "df_fuzzi": 84, "df_lqte": 80, "df_ml_g0": 76, "df_ml_g1": 76, "df_ml_m": 76, "df_pa": [58, 88], "df_plot": 54, "df_post_treat": 60, "df_pq": 80, "df_qte": 80, "df_result": 71, "df_sharp": 84, "df_sort": 65, "df_summari": 79, "df_wide": 78, "dfg": 138, "dgp": [25, 26, 54, 57, 59, 60, 70, 71, 78, 81, 82, 83, 87, 88], "dgp1": [25, 26], "dgp2": [25, 26], "dgp3": [25, 26], "dgp4": [25, 26], "dgp5": [25, 26], "dgp6": [25, 26], "dgp_dict": 87, "dgp_tpye": 58, "dgp_type": [25, 26, 58, 60], "diagnost": 61, "diagon": 87, "diagram": [51, 66, 97], "dichotom": [51, 66], "dict": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 45, 46, 47, 48, 60, 68, 69, 71, 77, 87, 96], "dict_kei": [127, 133], "dictionari": [9, 10, 11, 14, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 48, 60, 68, 69, 72, 73, 86, 95, 96, 97, 98, 127, 133], "dictonari": [55, 79], "did": [0, 4, 5, 6, 52, 58, 59, 60, 61, 62, 78, 92, 93, 94, 98, 99, 101, 102, 111, 115, 142, 143], "did_aggreg": [60, 62], "did_multi": [97, 98], "diff": 79, "differ": [9, 10, 11, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 51, 52, 54, 55, 56, 57, 60, 62, 65, 66, 67, 70, 72, 73, 74, 75, 77, 78, 79, 80, 81, 83, 84, 86, 87, 88, 89, 94, 95, 96, 98, 99, 101, 102, 110, 112, 114, 115, 139, 140, 141, 142, 143], "differenti": 97, "difficult": 87, "dillon": 141, "dim": [41, 55], "dim_x": [12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 25, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 52, 54, 56, 67, 75, 76, 77, 78, 90, 95, 96, 97, 127, 133], "dim_z": [15, 38, 97], "dimens": [11, 16, 25, 54, 78, 82, 110], "dimension": [11, 13, 21, 38, 39, 71, 85, 95, 97, 107, 108, 110, 126, 127, 133, 140, 141], "direct": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 57, 59, 67, 75, 88, 90, 97, 143], "directli": [40, 52, 53, 55, 65, 67, 76, 86, 90, 127, 133, 140, 143], "discontinu": [40, 41, 94, 141, 142], "discret": [14, 30, 65, 78, 97, 103, 142], "discretis": 80, "discuss": [12, 54, 55, 78, 79, 97, 98, 102, 141, 142, 143], "disjoint": [54, 72, 73, 78], "displai": [21, 54, 60, 61, 62, 65, 78, 87, 95, 96, 127, 133], "displot": 79, "disproportion": [55, 79], "disregard": [43, 47], "dist": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "distinguish": [24, 60], "distr": 96, "distribut": [41, 52, 58, 65, 67, 76, 87, 90, 97, 99, 102, 127, 135, 139, 141, 142], "diverg": [40, 52, 67, 90], "divid": [60, 97], "dmatrix": [68, 69, 95], "dml": [20, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 56, 57, 58, 61, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 101, 111, 126, 127, 133, 139], "dml1": [94, 140, 142, 143], "dml2": [51, 54, 56, 57, 58, 63, 78, 80, 94, 97, 111, 126, 140, 142, 143], "dml_apo": 75, "dml_apo_obj": 97, "dml_apos_att": 75, "dml_apos_obj": 97, "dml_base": 78, "dml_combin": 126, "dml_cover": 85, "dml_cv_predict": 142, "dml_cvar": [70, 80], "dml_cvar_0": 70, "dml_cvar_1": 70, "dml_cvar_obj": [31, 95], "dml_data": [6, 24, 53, 54, 57, 58, 59, 60, 61, 62, 63, 65, 74, 75, 76, 78, 81, 85, 86, 87, 88, 92, 93, 95, 96, 97, 98, 101, 126, 143], "dml_data_anticip": 60, "dml_data_bench": 87, "dml_data_bonu": [56, 140], "dml_data_df": 143, "dml_data_fuzzi": 84, "dml_data_lasso": 63, "dml_data_sharp": 84, "dml_data_sim": [56, 140], "dml_df": [54, 78], "dml_did": [58, 59], "dml_did_obj": [20, 23, 24, 97, 98, 99, 101], "dml_iivm": 85, "dml_iivm_boost": [55, 79], "dml_iivm_forest": [55, 79], "dml_iivm_lasso": [55, 79], "dml_iivm_obj": [32, 66, 97], "dml_iivm_tre": [55, 79], "dml_irm": [68, 72, 75, 76, 82], "dml_irm_at": 74, "dml_irm_att": 75, "dml_irm_boost": [55, 79], "dml_irm_forest": [55, 79], "dml_irm_gat": 74, "dml_irm_gatet": 74, "dml_irm_lasso": [55, 63, 79], "dml_irm_new": 82, "dml_irm_obj": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 86, 95, 96, 97], "dml_irm_obj_ext": 96, "dml_irm_rf": 63, "dml_irm_tre": [55, 79], "dml_irm_weighted_att": 75, "dml_kwarg": 75, "dml_length": 85, "dml_long": 48, "dml_lpq_0": 83, "dml_lpq_1": 83, "dml_lpq_obj": [34, 95], "dml_lqte": [80, 83], "dml_obj": [53, 60, 61, 62, 65, 86, 87], "dml_obj_al": 60, "dml_obj_anticip": 60, "dml_obj_bench": 87, "dml_obj_lasso": 61, "dml_obj_linear": 60, "dml_obj_linear_logist": 61, "dml_obj_nyt": 60, "dml_pliv": [54, 78], "dml_pliv_obj": [38, 54, 78, 97], "dml_plr": [69, 73, 126], "dml_plr_1": 126, "dml_plr_2": 126, "dml_plr_boost": [55, 79], "dml_plr_forest": [55, 79, 143], "dml_plr_lasso": [55, 63, 79], "dml_plr_no_split": 110, "dml_plr_obj": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 86, 89, 95, 96, 97, 110, 111, 126, 127, 128, 133], "dml_plr_obj_extern": 110, "dml_plr_obj_intern": 110, "dml_plr_obj_onfold": 77, "dml_plr_obj_untun": 77, "dml_plr_rf": 63, "dml_plr_tree": [55, 79, 143], "dml_pq_0": [80, 83], "dml_pq_1": [80, 83], "dml_pq_obj": [35, 95], "dml_procedur": [63, 89, 140, 142, 143], "dml_qte": [80, 83], "dml_qte_obj": [36, 95], "dml_robust_confset": 85, "dml_robust_length": 85, "dml_short": 48, "dml_ssm": [57, 88, 97], "dml_standard_ci": 85, "dml_tune": 142, "dmldummyclassifi": 96, "dmldummyregressor": 96, "dmlmt": 141, "dnorm": 52, "do": [53, 54, 55, 56, 61, 75, 76, 78, 79, 80, 81, 87, 95, 96, 110, 127, 137, 140, 143], "doabl": 111, "doc": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 138, 142], "docu": 142, "document": [59, 60, 61, 62, 64, 68, 69, 72, 73, 75, 77, 87, 97, 102, 111, 127, 138, 142], "doe": [24, 30, 36, 53, 54, 55, 65, 75, 78, 79, 81, 86, 87, 97, 111, 115, 127, 137, 143], "doesn": [51, 66], "doi": [7, 8, 9, 10, 12, 16, 17, 19, 25, 26, 53, 54, 56, 71, 78, 87, 90, 96, 110, 126, 138, 140, 142], "domain": 82, "don": [53, 77], "done": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 61, 77, 80, 96, 110, 127, 128], "dosag": 65, "dot": [37, 59, 60, 82, 91, 93, 95, 96, 97, 101, 102, 103, 126, 140], "doubl": [4, 5, 6, 7, 8, 16, 17, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 55, 61, 71, 76, 77, 79, 81, 85, 94, 96, 110, 111, 126, 127, 128, 142], "double_ml": 60, "double_ml_bonus_data": 63, "double_ml_data_from_data_fram": [52, 90, 91, 93, 143], "double_ml_data_from_matrix": [53, 56, 91, 93, 96, 126, 140], "double_ml_framework": [97, 98], "double_ml_irm": [63, 82], "double_ml_score_mixin": 0, "doubleiivm": 138, "doubleml": [0, 52, 54, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 80, 81, 82, 83, 84, 85, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 110, 111, 127, 133, 140, 141, 142], "doubleml2022python": 138, "doubleml2024r": 138, "doubleml_did_eval_linear": 53, "doubleml_did_eval_rf": 53, "doubleml_did_linear": 53, "doubleml_did_rf": 53, "doubleml_framework": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "doublemlapo": [65, 75, 97, 111, 116, 142], "doublemlblp": [29, 33, 39, 68, 69, 75, 95, 142], "doublemlclusterdata": [0, 16], "doublemlcvar": [70, 95, 111, 117, 142], "doublemldata": [0, 6, 7, 8, 12, 13, 15, 17, 18, 19, 20, 22, 23, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 54, 56, 57, 58, 59, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 94, 95, 96, 97, 99, 110, 111, 126, 127, 133, 142, 143], "doublemldid": [58, 59, 97, 99, 111, 114, 142], "doublemldidaggreg": [60, 61, 62, 97, 98], "doublemldidc": [58, 97, 99, 111, 112, 142], "doublemldidmulti": [60, 61, 62, 97, 98, 101, 111, 115, 142], "doublemlframework": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 61, 62, 97, 98, 110, 126, 142], "doublemlframwork": 30, "doublemlidid": [97, 99], "doublemlididc": [97, 99], "doublemliivm": [51, 55, 66, 79, 85, 96, 97, 110, 111, 118, 142], "doublemlirm": [20, 22, 23, 29, 31, 32, 34, 35, 37, 38, 39, 53, 55, 63, 65, 68, 72, 74, 75, 76, 79, 81, 82, 86, 87, 95, 96, 97, 110, 111, 119, 138, 142], "doublemllpq": [83, 95, 111, 120, 142], "doublemlpaneldata": [0, 22, 24, 61, 62, 92, 97, 98, 101, 142], "doublemlpliv": [96, 97, 110, 111, 122, 138, 142], "doublemlplr": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 52, 55, 56, 63, 67, 69, 73, 77, 79, 81, 86, 89, 90, 95, 96, 97, 110, 111, 123, 126, 127, 133, 138, 140, 142, 143], "doublemlpolicytre": [33, 95], "doublemlpq": [80, 83, 95, 111, 121, 142], "doublemlqt": [70, 80, 83, 95, 126, 142], "doublemlresampl": [75, 76], "doublemlsmm": 142, "doublemlssm": [57, 88, 97, 111, 124, 125], "doubli": [9, 10, 26, 53, 61, 85, 141], "down": 87, "download": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 91, 93, 139, 140], "download_fil": 61, "downward": 87, "dpg_dict": 86, "dpi": [52, 67, 81], "dr": [97, 101], "dramat": 53, "draw": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 87, 110, 142], "draw_sample_split": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 75, 76, 110], "drawn": [11, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 55, 60, 79, 80, 82, 110], "drive": [52, 67, 90], "driven": [87, 143], "drop": [53, 77, 78, 81, 91, 93, 96, 111, 112, 114, 115, 126], "dropna": 60, "dt": [60, 111, 112, 127, 129], "dt_bonu": [91, 93], "dta": [53, 62], "dtrain": 79, "dtype": [58, 60, 61, 62, 63, 65, 72, 73, 74, 76, 78, 79, 80, 85, 86, 88, 91, 92, 93, 95, 140], "dualiti": 78, "dubourg": [138, 140], "duchesnai": [138, 140], "due": [52, 53, 61, 67, 68, 69, 74, 86, 87, 90, 97, 109, 127, 128, 142, 143], "duflo": [7, 8, 17, 54, 71, 78, 85, 90, 110, 138, 141], "dummi": [29, 33, 39, 42, 43, 44, 61, 77, 87, 95, 96, 97, 99, 142], "dummyclassifi": [42, 61], "dummyregressor": [43, 61], "duplic": 142, "durabl": [56, 63, 91, 93, 140], "durat": 8, "dure": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 54, 55, 56, 57, 77, 78, 79, 96, 110, 140, 142, 143], "dx": 12, "dynam": [53, 141], "e": [5, 6, 7, 8, 9, 10, 14, 17, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 52, 53, 54, 55, 57, 58, 60, 61, 62, 65, 67, 68, 69, 71, 74, 76, 77, 78, 79, 80, 81, 84, 85, 86, 87, 88, 90, 92, 93, 95, 96, 97, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 143], "e20ea26": 56, "e401": [55, 79, 80, 86, 143], "e4016553": 143, "e45228": 81, "e57c": 56, "each": [14, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 54, 56, 59, 60, 61, 62, 65, 72, 73, 76, 77, 78, 80, 81, 82, 85, 86, 87, 89, 91, 92, 93, 96, 97, 102, 110, 126, 127, 133, 143], "earlier": [60, 143], "earn": [55, 79, 80], "earner": [55, 79, 86], "easi": [56, 85, 111], "easier": 77, "easili": [56, 76, 77, 80, 142], "ec973f": 81, "ecolor": [59, 65, 79, 81], "econ": 141, "econml": 141, "econom": [15, 16, 18, 19, 54, 71, 78, 81, 87, 110, 141], "econometr": [7, 8, 9, 10, 17, 18, 25, 26, 53, 54, 71, 78, 85, 90, 138, 141], "econometrica": [13, 54, 78, 81, 85, 90, 141], "ecosystem": [138, 143], "ectj": [7, 8, 17, 54, 71, 78, 90, 138], "ed": 141, "edge_color": 67, "edgecolor": 67, "edit": [139, 141], "edu": [138, 140], "educ": [55, 79, 80, 86, 143], "ee97bda7": 56, "effect": [9, 10, 11, 14, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 46, 47, 51, 52, 53, 54, 56, 57, 58, 59, 65, 66, 67, 71, 74, 78, 82, 84, 85, 88, 90, 94, 96, 98, 99, 101, 102, 104, 105, 106, 109, 110, 111, 119, 126, 127, 128, 140, 141, 142, 143], "effici": [97, 141], "effort": 111, "eight": [54, 78], "either": [11, 20, 22, 23, 24, 25, 29, 31, 32, 33, 34, 35, 37, 38, 39, 56, 59, 60, 71, 82, 84, 95, 96, 97, 102, 143], "eleanor": 141, "element": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 57, 58, 60, 61, 62, 68, 69, 70, 76, 78, 80, 83, 86, 88, 97, 101, 111, 112, 114, 115, 116, 127, 132, 133, 136, 137, 142], "element_text": [54, 55], "elementari": 141, "elif": [72, 73, 82], "elig": [80, 86, 143], "eligibl": [55, 79, 86], "ell": [52, 54, 67, 71, 78, 90, 111, 122, 123, 140], "ell_0": [32, 38, 39, 52, 67, 71, 77, 90, 97, 105, 111, 123], "ell_1": 61, "ell_2": 76, "els": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 44, 53, 54, 55, 59, 72, 73, 78, 82, 87], "em": 141, "emphas": [54, 78], "empir": [27, 28, 52, 54, 61, 67, 78, 81, 87, 90, 97, 101, 110, 111, 126], "emploi": [54, 71, 78, 87, 111, 118], "employ": [55, 61, 79, 80], "employe": 143, "empti": 78, "emul": [127, 128], "enabl": [21, 65, 82, 86, 95, 127, 128, 142], "enable_metadata_rout": [42, 43, 46, 47], "encapsul": [42, 43, 46, 47], "encod": 81, "end": [12, 15, 16, 52, 53, 54, 55, 57, 59, 60, 67, 70, 71, 76, 78, 79, 81, 82, 83, 88, 89, 91, 93, 96, 97, 101, 102, 110, 111, 115, 126, 140, 143], "endogen": [55, 79, 80, 143], "enet_coordinate_descent_gram": 78, "enforc": 61, "engin": [56, 141], "enrol": [55, 79, 80], "ensembl": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 63, 65, 67, 68, 69, 72, 73, 74, 76, 79, 82, 86, 87, 89, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 133, 140, 143], "ensemble_learner_pipelin": 96, "ensemble_pipe_classif": 56, "ensemble_pipe_regr": 56, "ensur": [46, 47, 54, 73, 75, 77, 78, 82, 85], "entir": [24, 52, 55, 67, 79, 90, 127, 128], "entri": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 52, 54, 58, 60, 61, 62, 63, 65, 67, 74, 78, 79, 80, 86, 88, 90, 91, 92, 93, 96, 138, 140, 142], "enumer": [59, 65, 70, 72, 73, 76, 78, 79, 80, 83, 89, 96, 110], "env": [78, 139], "environ": 139, "ep": 81, "epanechnikov": 40, "epsilon": [55, 58, 59, 70, 79, 83, 95, 97, 99, 102], "epsilon_": [54, 59, 60, 78], "epsilon_0": 41, "epsilon_1": 41, "epsilon_i": [11, 70, 81, 82, 83], "epsilon_sampl": 82, "epsilon_tru": [70, 83], "eqnarrai": 55, "equal": [21, 25, 29, 33, 54, 57, 60, 78, 81, 85, 88, 95, 96, 97, 127, 135], "equat": [41, 54, 55, 78, 79, 87, 89, 126, 143], "equilibrium": [54, 78], "equiv": [97, 102, 111, 115], "equival": [71, 75, 110], "err": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 58, 60, 61, 62, 63, 65, 66, 68, 69, 70, 72, 73, 74, 75, 77, 78, 79, 80, 81, 83, 86, 87, 88, 95, 96, 97, 98, 99, 101, 110, 111, 126, 140, 143], "error": [4, 5, 6, 20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 51, 52, 53, 55, 56, 57, 59, 67, 71, 72, 73, 76, 77, 79, 84, 87, 90, 96, 97, 107, 108, 110, 111, 126, 127, 133, 140, 142, 143], "errorbar": [59, 65, 72, 73, 77, 79, 81, 84], "erstellt": [54, 55, 56], "es_linear_logist": 61, "es_rf": 61, "esim": 84, "especi": [76, 77], "essenti": 87, "est": 84, "est_method": 53, "esther": [110, 141], "estim": [4, 5, 7, 9, 10, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 52, 53, 54, 56, 59, 65, 67, 68, 69, 70, 72, 73, 75, 76, 78, 82, 84, 85, 89, 90, 94, 95, 96, 97, 98, 99, 101, 104, 106, 112, 114, 117, 120, 121, 125, 127, 128, 133, 138, 141, 142], "estimand": 85, "estimatior": [4, 5], "estimator_list": 77, "et": [7, 8, 11, 13, 16, 17, 52, 54, 55, 56, 58, 67, 68, 69, 70, 71, 72, 73, 76, 78, 79, 80, 83, 86, 90, 97, 99, 102, 110, 111, 117, 119, 120, 121, 126, 127, 128, 137, 138, 140, 142], "eta": [27, 28, 52, 54, 55, 59, 77, 78, 79, 83, 84, 89, 95, 97, 101, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 137, 140, 143], "eta1": 81, "eta2": 81, "eta_": [126, 127, 137], "eta_0": [40, 89, 97, 101, 111, 126], "eta_d": [84, 97], "eta_i": [11, 25, 59, 60, 82, 83, 84, 97], "eta_sampl": 82, "eta_tru": 83, "etc": [20, 21, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 76, 77, 78, 142], "ev": [52, 67, 90], "eval": [24, 56, 60, 61, 62, 96, 97, 101, 111, 115, 127, 132], "eval_metr": [55, 79, 143], "eval_pr": 53, "eval_predict": 53, "evalu": [13, 20, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 56, 59, 60, 61, 62, 68, 69, 70, 74, 75, 80, 83, 86, 89, 97, 101, 102, 141, 142], "evaluate_learn": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 76, 77, 96, 142], "evalut": 96, "even": [55, 56, 60, 79, 81, 84, 96, 97, 143], "event": [97, 98], "eventstudi": [24, 60, 61, 62, 97, 98], "eventu": [54, 78], "everi": [54, 61, 78], "everyth": 138, "evid": [74, 77], "exact": [75, 87], "exactli": [84, 87, 97], "exampl": [4, 5, 6, 11, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 55, 56, 57, 58, 59, 60, 62, 63, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 88, 89, 90, 95, 96, 97, 98, 101, 109, 110, 111, 126, 127, 133, 138, 140, 142, 143], "example_attgt": 53, "example_attgt_dml_eval_linear": 53, "example_attgt_dml_eval_rf": 53, "example_attgt_dml_linear": 53, "example_attgt_dml_rf": 53, "except": [43, 47, 71, 87, 142], "excess": 76, "exclud": 48, "exclus": [29, 33, 39, 72, 73, 95], "execut": [56, 143], "exemplarili": 140, "exemplatori": 82, "exhaust": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 96], "exhibit": [54, 78], "exist": [42, 43, 46, 47, 75, 97, 99, 102, 127, 137], "exogen": [55, 79, 80, 97, 143], "exp": [9, 10, 11, 13, 14, 17, 25, 26, 52, 59, 67, 68, 69, 72, 73, 81, 82, 90], "expect": [9, 10, 43, 47, 53, 57, 58, 60, 61, 65, 74, 76, 77, 84, 87, 88, 95, 97, 110, 126, 127, 134, 140], "experi": [8, 12, 13, 52, 55, 67, 79, 85, 87, 90, 91, 93, 110, 140, 141], "experiment": [20, 22, 23, 24, 25, 26, 111, 112, 114, 115, 127, 129, 131, 132], "expertis": 87, "explain": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 86, 127, 128, 136, 137], "explan": [54, 58, 78, 86, 127, 136, 138, 143], "explanatori": [87, 126], "explicit": 87, "explicitli": [60, 62, 74, 143], "exploit": [52, 67, 90, 97, 143], "explor": 77, "exponenti": 126, "export": [77, 142], "expos": [97, 102], "exposur": [6, 25, 59, 60, 61, 62], "express": [54, 71, 84, 127, 137], "ext": 24, "extend": [87, 93, 96, 138, 142], "extendend": [127, 137], "extens": [96, 111, 138, 141, 142], "extent": 71, "extern": [52, 67, 75, 77, 94, 127, 128, 142], "external_predict": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 67, 96], "externalptr": 55, "extra": 56, "extract": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 77], "extralearn": 56, "extrem": [55, 79], "ey": 71, "ezequiel": 142, "f": [55, 56, 58, 59, 60, 65, 67, 70, 71, 76, 78, 79, 80, 82, 83, 86, 87, 88, 96, 127, 137, 138, 140], "f00584a57972": 56, "f1718fdeb9b0": 56, "f2e7": 56, "f3d24993": 56, "f6ebc": 81, "f_": [9, 25, 26, 59, 95], "f_loc": [70, 83], "f_p": 59, "f_scale": [70, 83], "f_t": 60, "f_x": 97, "face_color": 67, "facet_wrap": 55, "facilit": 77, "fact": [55, 79, 80], "factor": [41, 52, 53, 54, 55, 56, 67, 76, 90, 96, 143], "faculti": 141, "fail": 142, "fair": 76, "fake": [51, 66, 85], "fals": [4, 5, 6, 7, 8, 9, 11, 14, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 52, 55, 56, 57, 58, 60, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 87, 88, 91, 93, 96, 97, 110, 111, 112, 114, 115, 126, 127, 129, 131, 132, 143], "famili": [55, 79, 96], "familiar": 85, "fanci": 53, "far": [55, 79], "farbmach": 12, "fast": [76, 82, 96], "faster": 71, "fb5c25fa": 56, "fc9e": 56, "fd8a": 56, "featur": [7, 8, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 43, 45, 47, 53, 60, 63, 74, 75, 76, 79, 82, 95, 96, 97], "featureless": [56, 96], "features_bas": [55, 79, 80, 86], "features_flex": 55, "featureunion": 56, "februari": [60, 87], "feder": 61, "femal": [56, 63, 91, 93, 140], "fern\u00e1ndez": [13, 110, 141], "fetch": [55, 78, 79, 80, 91, 93], "fetch_401k": [55, 79, 80, 86, 143], "fetch_bonu": [56, 63, 91, 93, 140], "few": [55, 79, 80], "ff7f0e": 59, "field": [54, 78, 96, 143], "fifteenth": 141, "fifth": [54, 60], "fig": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 59, 60, 61, 62, 65, 68, 69, 70, 71, 75, 76, 77, 80, 81, 83, 84, 87], "fig_al": 67, "fig_dml": 67, "fig_non_orth": 67, "fig_orth_nosplit": 67, "fig_po_al": 67, "fig_po_dml": 67, "fig_po_nosplit": 67, "figsiz": [21, 24, 59, 60, 63, 65, 68, 69, 70, 72, 73, 75, 76, 77, 78, 79, 80, 81, 83, 84], "figur": [17, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 54, 59, 60, 61, 63, 65, 67, 68, 69, 70, 71, 72, 73, 75, 77, 78, 79, 80, 83, 87, 90], "figure_format": 81, "file": [7, 8, 61, 71, 81, 141, 142], "filenam": 52, "fill": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 54, 55, 57, 58, 76, 79, 88], "fill_between": [68, 69, 70, 75, 80, 83], "fill_valu": 76, "fillna": 60, "filter": 56, "filterwarn": 67, "final": [52, 56, 57, 59, 60, 62, 65, 67, 68, 69, 70, 72, 73, 74, 80, 83, 84, 88, 90, 97, 109, 111, 115, 143], "final_estim": 84, "financi": [7, 86, 143], "find": [55, 58, 59, 79, 87, 95, 96, 143], "finish": 56, "finit": [52, 55], "firm": [54, 78, 86], "firmid": 78, "first": [6, 9, 10, 16, 20, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 65, 66, 67, 68, 69, 72, 73, 74, 75, 76, 78, 79, 80, 82, 83, 84, 87, 88, 90, 95, 97, 98, 101, 102, 110, 126, 127, 133, 139, 140, 142, 143], "fit": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 94, 95, 96, 97, 98, 99, 101, 102, 111, 112, 114, 125, 126, 127, 128, 133, 138, 142, 143], "fit_arg": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39], "fit_transform": [75, 78, 79], "five": 78, "fix": [59, 60, 76, 142], "flag": [26, 110, 139], "flake8": 142, "flamlclassifierdoubleml": 77, "flamlregressordoubleml": 77, "flatten": [77, 81], "flexibl": [40, 51, 53, 55, 56, 58, 66, 79, 97, 138, 141, 142, 143], "flexibli": [55, 61, 79, 86], "float": [9, 10, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 60, 61, 62, 92, 93], "float32": [79, 80, 86], "float64": [58, 60, 61, 62, 63, 65, 73, 74, 78, 79, 86, 88, 91, 92, 93, 96, 140], "floor": 56, "floor_divid": 78, "flt": 56, "flush": 52, "fmt": [59, 65, 72, 73, 77, 79, 81, 84], "fobj": 79, "focu": [54, 55, 61, 75, 78, 79, 80, 87, 95, 97, 99, 102, 109, 143], "focus": [80, 86, 87, 143], "fold": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 54, 55, 56, 57, 58, 60, 61, 62, 76, 78, 79, 80, 86, 88, 89, 94, 96, 97, 99, 101, 111, 112, 114, 126, 140, 143], "follow": [9, 10, 11, 14, 25, 26, 52, 54, 55, 57, 58, 59, 60, 61, 62, 67, 68, 69, 70, 72, 73, 77, 78, 79, 80, 83, 84, 85, 86, 87, 88, 90, 91, 93, 95, 96, 97, 98, 101, 110, 111, 115, 127, 128, 129, 131, 132, 133, 134, 135, 136, 137, 139, 140, 143], "font_scal": [78, 79, 80], "fontsiz": [60, 70, 80, 83], "force_all_d_finit": [5, 6], "force_all_x_finit": [4, 5, 6], "forest": [12, 51, 52, 53, 55, 56, 58, 66, 67, 74, 76, 79, 86, 90, 96, 140, 143], "forest_summari": 79, "forg": [139, 141, 142], "form": [9, 10, 11, 20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 46, 47, 55, 57, 58, 59, 68, 69, 70, 72, 73, 74, 76, 79, 83, 84, 86, 88, 95, 97, 98, 99, 102, 105, 106, 107, 108, 111, 115, 116, 119, 127, 128, 133, 134, 135, 136, 137, 139, 140], "format": [6, 24, 61, 67, 74, 127, 133, 142], "former": [61, 85], "formula": [54, 55, 78, 79, 84, 87, 142], "formula_flex": 55, "forschungsgemeinschaft": 138, "forthcom": [87, 141], "forum": 142, "forward": [33, 45], "found": [62, 68, 69, 71, 72, 73, 77, 90, 91, 93, 96, 97, 109, 140], "foundat": [138, 141], "four": [55, 76, 79, 142], "fourth": [54, 78], "frac": [9, 10, 12, 13, 15, 17, 18, 19, 25, 26, 28, 32, 43, 47, 52, 54, 56, 59, 67, 71, 74, 78, 81, 84, 89, 90, 95, 97, 101, 105, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 124, 125, 126, 127, 128, 129, 131, 132, 134, 135, 136, 137], "fraction": 56, "frame": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 51, 52, 54, 55, 57, 58, 60, 61, 62, 63, 65, 68, 69, 72, 73, 74, 78, 79, 80, 81, 82, 86, 88, 90, 91, 92, 93, 140, 143], "framealpha": 60, "frameon": 60, "framework": [21, 24, 28, 52, 54, 56, 67, 76, 77, 78, 81, 87, 90, 96, 126, 138, 140, 142, 143], "freez": 139, "fribourg": 141, "friendli": [60, 65], "from": [4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 47, 51, 52, 54, 55, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 133, 140, 142, 143], "from_arrai": [4, 5, 6, 37, 40, 58, 59, 67, 70, 83, 90, 91, 93, 96, 126, 140], "from_product": 78, "front": 65, "fr\u00e9chet": [127, 137], "fs_kernel": [40, 97], "fs_specif": [40, 97], "fsize": [55, 79, 80, 86, 143], "full": [58, 59, 65, 67, 70, 72, 73, 76, 79, 80, 83, 84, 85, 88, 90, 97], "fulli": [33, 55, 64, 77, 79, 85, 97, 106], "fun": 52, "func": 53, "function": [0, 4, 5, 6, 17, 18, 20, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 55, 56, 57, 58, 60, 61, 62, 66, 67, 68, 69, 70, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 94, 96, 97, 98, 99, 101, 102, 105, 110, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 131, 132, 137, 138, 141, 142, 143], "fund": [55, 79, 80, 138], "further": [9, 10, 11, 14, 16, 21, 24, 25, 26, 54, 56, 57, 58, 59, 60, 61, 62, 65, 68, 69, 70, 74, 75, 76, 78, 80, 82, 83, 84, 86, 87, 88, 96, 97, 99, 101, 109, 111, 117, 120, 121, 124, 125, 126, 127, 128, 133, 136, 137, 138, 140, 142, 143], "furthermor": [67, 92, 93, 111, 116, 119], "futur": [60, 61, 62, 97, 111, 127], "futurewarn": [60, 61, 62, 73], "fuzzi": [40, 41], "g": [5, 6, 9, 10, 20, 22, 23, 24, 25, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 52, 53, 56, 58, 59, 60, 61, 62, 63, 67, 68, 69, 71, 74, 76, 80, 81, 82, 85, 86, 88, 90, 92, 93, 95, 96, 97, 98, 99, 101, 102, 111, 112, 114, 115, 116, 118, 119, 122, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 143], "g_": [41, 65, 97, 101, 111, 112, 114, 115, 117, 120, 121, 126], "g_0": [17, 18, 20, 22, 23, 24, 29, 32, 33, 35, 38, 39, 40, 41, 52, 54, 55, 67, 76, 78, 79, 90, 95, 96, 97, 103, 104, 105, 106, 107, 108, 111, 115, 116, 123, 124, 125, 127, 134, 135, 137, 140, 143], "g_1": [41, 76], "g_all": [52, 55], "g_all_po": 52, "g_ci": 55, "g_d": [111, 117, 121], "g_dml": 52, "g_dml_po": 52, "g_hat": [38, 39, 52, 67, 111], "g_hat0": [32, 33], "g_hat1": [32, 33], "g_i": [25, 97, 101, 102, 111, 115], "g_k": 95, "g_nonorth": 52, "g_nosplit": 52, "g_nosplit_po": 52, "g_valu": 22, "g_x": 59, "gain": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 76, 127, 128, 135, 142], "gain_statist": 142, "galleri": [90, 95, 96, 97, 98, 101, 109, 138, 142], "gama": 77, "gamma": [15, 18, 19, 54, 78, 81, 82, 84, 87, 97, 111, 117, 120], "gamma_0": [11, 57, 82, 88, 111, 117, 120], "gamma_a": [9, 10, 87], "gamma_bench": 87, "gamma_v": 87, "gap": [78, 87], "gapo": 29, "gate": [29, 33, 39, 44, 81, 82, 94, 142], "gate_obj": 95, "gatet": 95, "gaussian": [34, 35, 36, 52, 67, 90, 95, 96, 126, 141], "ge": [9, 11, 26, 74, 82, 95, 97, 101, 102], "geer": 141, "gelbach": [54, 78], "gener": [0, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 43, 47, 51, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 91, 93, 94, 95, 96, 97, 101, 102, 103, 110, 111, 115, 116, 119, 126, 128, 129, 131, 132, 134, 135, 137, 141, 142, 143], "generate_treat": 83, "generate_weakiv_data": 85, "geom_bar": 55, "geom_dens": [55, 57], "geom_errorbar": 55, "geom_funct": 52, "geom_histogram": 52, "geom_hlin": 55, "geom_point": 55, "geom_til": 54, "geom_vlin": [52, 57], "geq": [25, 84, 97], "german": 138, "get": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 56, 60, 62, 65, 76, 81, 86, 87, 127, 128, 138, 139], "get_dummi": 81, "get_feature_names_out": [75, 78, 79], "get_legend_handles_label": 65, "get_level_valu": 77, "get_logg": [52, 53, 54, 55, 56, 57, 89, 96, 97, 110, 111, 126, 140], "get_metadata_rout": [42, 43, 46, 47], "get_param": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 77, 96], "get_ylim": 75, "ggdid": 53, "ggplot": [52, 54, 55, 57], "ggplot2": [52, 54, 55, 57], "ggsave": 52, "ggtitl": 55, "gh": 142, "git": 139, "github": [53, 55, 61, 71, 77, 81, 138, 141, 142], "githubusercont": [62, 71], "give": [55, 75, 79], "given": [9, 10, 13, 17, 18, 20, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 46, 47, 52, 54, 57, 59, 61, 62, 65, 67, 72, 73, 78, 80, 81, 84, 87, 88, 90, 95, 97, 101, 111, 116, 126, 127, 133, 134, 135, 136, 137, 140, 142], "glmnet": [55, 56, 96, 142], "global": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 46, 47, 96, 97], "globalclassifi": 84, "globallearn": 84, "globalregressor": 84, "glrn": 56, "glrn_lasso": 56, "gmm": 85, "gname": 53, "go": [68, 69, 71, 75, 77, 84, 87], "goal": [65, 72, 73, 97], "goe": 97, "goldman": 141, "good": [71, 127, 128, 143], "gradient": [55, 79], "gradientboostingclassifi": 76, "gradientboostingregressor": 76, "gradual": 87, "gramfort": [138, 140], "graph": [56, 57, 88, 143], "graph_ensemble_classif": 56, "graph_ensemble_regr": 56, "graph_obj": 84, "graph_object": [68, 69, 71, 87], "graphlearn": [56, 96], "grasp": [65, 127, 128], "great": [59, 143], "greater": 143, "green": [52, 68, 69, 70, 83], "greg": 141, "grei": [55, 65], "grenand": 141, "grey50": 54, "grid": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 54, 56, 60, 65, 68, 69, 70, 71, 75, 80, 81, 83, 87, 96, 127, 133], "grid_arrai": [68, 69], "grid_basi": 75, "grid_bound": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 87], "grid_search": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 56, 96], "grid_siz": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 68, 69], "gridextra": 54, "gridsearchcv": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 96], "grisel": [138, 140], "grob": 54, "group": [6, 22, 24, 25, 29, 33, 39, 51, 53, 61, 65, 66, 74, 75, 80, 81, 82, 87, 94, 97, 98, 101, 102, 111, 115, 127, 132], "group_0": 95, "group_1": [72, 73, 95], "group_2": [72, 73, 95], "group_3": [72, 73], "group_effect": 82, "group_ind": 74, "group_treat": 74, "groupbi": [60, 71, 79, 85], "gruber": 12, "gt": [51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 65, 66, 68, 69, 70, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 91, 93, 140], "gt_combin": [24, 60, 61, 97, 98, 101], "gt_dict": 60, "guarante": [54, 78], "guber": 12, "guess": [86, 127, 128], "guid": [27, 28, 42, 43, 46, 47, 52, 53, 54, 56, 59, 60, 61, 62, 65, 67, 74, 75, 78, 84, 86, 96, 138, 140, 142], "guidelin": 142, "gunion": [56, 96], "gxidclusterperiodytreat": 53, "h": [9, 10, 12, 16, 25, 26, 53, 54, 78, 84, 85, 92, 93, 97, 141], "h20": 77, "h_0": [60, 65, 74, 75, 86, 87, 127, 133, 143], "h_f": [40, 97], "ha": [20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 45, 46, 47, 52, 53, 54, 55, 61, 67, 71, 76, 77, 78, 79, 80, 81, 84, 85, 86, 87, 95, 96, 97, 99, 102, 127, 128, 133, 134, 135, 136, 137, 143], "had": 61, "half": [52, 67, 81, 90, 110], "hand": [40, 76, 77, 81, 85, 143], "handbook": 81, "handl": [53, 61, 65, 76, 92, 93, 96, 142], "hansen": [7, 8, 13, 15, 17, 54, 71, 78, 85, 90, 138, 141], "happend": 76, "hard": [86, 127, 128], "harold": 141, "harsh": [42, 46], "hasn": [21, 24, 60, 62], "hat": [52, 54, 67, 71, 74, 78, 81, 84, 89, 90, 95, 97, 110, 111, 126, 127, 128, 133, 136], "have": [11, 14, 24, 29, 30, 33, 36, 39, 44, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 65, 66, 68, 69, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 91, 93, 95, 96, 97, 101, 111, 115, 126, 127, 128, 134, 137, 139, 140, 142, 143], "hazlett": [87, 127, 128], "hc": [53, 141], "hc0": [44, 142], "hdm": [54, 78], "he": [57, 88], "head": [53, 54, 56, 60, 61, 62, 63, 68, 69, 72, 73, 75, 77, 78, 79, 81, 84, 87, 91, 93, 95, 140], "heat": [54, 78], "heatmap": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 54, 78, 87], "heavili": 76, "hei": 141, "height": [21, 24, 52, 54, 71, 77], "help": [53, 55, 61, 70, 76, 80, 82, 87, 110, 143], "helper": 142, "henc": [53, 55, 56, 79, 87, 96, 111, 143], "here": [34, 35, 36, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 68, 69, 70, 72, 73, 74, 76, 78, 79, 80, 82, 83, 84, 87, 88, 91, 93, 96, 97, 139], "heterogen": [11, 25, 33, 55, 74, 79, 80, 82, 94, 97, 106, 110, 141, 142, 143], "heteroskedast": [72, 73], "heurist": [52, 67, 90], "high": [13, 38, 39, 55, 59, 71, 79, 80, 85, 89, 97, 107, 108, 126, 138, 140, 141], "higher": [53, 55, 71, 79, 80, 81, 84, 85, 142, 143], "highli": [55, 79, 138], "highlight": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 58, 75, 77, 87, 142], "highlightcolor": [68, 69], "hint": 77, "hispan": 63, "hist": 65, "hist_e401": 55, "hist_p401": 55, "histogram": 65, "histplot": 67, "hjust": 55, "hline": [91, 93, 126, 140, 143], "hold": [19, 54, 55, 57, 77, 78, 79, 88, 95, 96, 97, 101], "holdout": [96, 110], "holm": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39], "home": [55, 60, 62, 79, 87], "homogen": 97, "hook": 142, "hopefulli": 80, "horizont": [54, 59, 78], "hostedtoolcach": [60, 61, 62, 79], "hot": 81, "hotstart_backward": [56, 96], "hotstart_forward": [56, 96], "household": [55, 79, 80, 86], "how": [21, 25, 42, 43, 46, 47, 51, 53, 54, 55, 57, 58, 59, 60, 61, 62, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 90, 96, 97, 138, 139], "howev": [52, 55, 57, 67, 77, 79, 84, 87, 88, 90, 97, 143], "hown": [55, 79, 80, 86, 143], "hpwt": [54, 78], "hpwt0": 54, "hpwtairmpdspac": 54, "href": 138, "hspace": 76, "hstack": [37, 59], "html": [56, 73, 138, 140, 142], "http": [12, 18, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 96, 138, 139, 140, 142], "huber": [19, 57, 88, 97, 109, 111, 124, 125, 141], "hue": [60, 79], "huge": 76, "hugo": 141, "husd": [56, 63, 91, 93, 140], "hyperparamet": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 55, 56, 63, 71, 76, 77, 79, 94, 140], "hypothes": [126, 141], "hypothesi": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 79, 86, 127, 133, 141], "hypothet": 87, "i": [4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 102, 103, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 119, 120, 121, 125, 126, 127, 128, 133, 134, 135, 137, 138, 139, 140, 142, 143], "i0": [58, 59, 97, 99], "i03": 138, "i1": [58, 97, 99], "i_": [15, 78, 82], "i_1": [54, 78], "i_2": [54, 78], "i_3": [54, 78], "i_4": 59, "i_est": 67, "i_fold": 54, "i_k": [54, 78, 89, 110, 126], "i_learn": 76, "i_level": 65, "i_rep": [52, 57, 58, 67, 76, 88, 90], "i_split": 78, "i_train": 67, "icp": 141, "id": [6, 24, 53, 54, 56, 60, 61, 62, 78, 92, 93, 97, 98, 101], "id_col": [6, 24, 60, 61, 62, 92, 93, 97, 98, 101], "id_var": 78, "idea": [55, 56, 79, 80, 87, 96, 97, 127, 128, 143], "ident": [9, 10, 11, 14, 15, 25, 26, 45, 56, 61, 65, 75, 77, 84, 96, 97, 102, 111, 119, 127, 133], "identfi": 87, "identif": [60, 61, 62, 84, 85, 97, 143], "identifi": [6, 54, 55, 58, 61, 74, 78, 79, 80, 84, 87, 92, 93, 95, 97, 99, 101, 102, 109, 127, 137, 142], "identifii": 95, "idnam": 53, "idx_gt_att": 24, "idx_tau": [70, 80, 83], "idx_treat": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 65, 127, 133], "ieee": 141, "ifels": 53, "ignor": [42, 43, 46, 47, 61, 67, 84], "ii": [54, 78], "iid": [60, 97, 99], "iivm": [12, 27, 28, 32, 80, 89, 95, 105, 118, 138, 142], "iivm_summari": 79, "iivmglmnet": 55, "iivmrang": 55, "iivmrpart": 55, "iivmxgboost11861": 55, "ij": [16, 54, 57, 65, 78, 88], "ilia": 141, "illustr": [52, 54, 55, 56, 57, 58, 59, 61, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 78, 79, 80, 82, 83, 86, 87, 88, 90, 96, 143], "iloc": [58, 59, 60, 61, 62, 65, 76, 78, 81, 85], "immedi": 139, "immun": [110, 141], "impact": [51, 66, 76, 81, 86], "implement": [20, 22, 23, 24, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 67, 71, 75, 76, 78, 79, 81, 84, 86, 87, 88, 90, 94, 95, 96, 98, 99, 100, 101, 102, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 140, 141, 142, 143], "impli": [9, 10, 54, 55, 78, 79, 80, 84, 95, 97, 102, 127, 129, 131, 132, 134, 135], "implicitli": [97, 101], "implment": [59, 97, 98], "import": [4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 133, 139, 140, 142, 143], "importlib": 71, "impos": 87, "improv": [58, 60, 76, 82, 97, 142], "in_sample_norm": [20, 22, 23, 24, 58, 111, 112, 114, 115, 127, 129, 131, 132], "inbuild": 76, "inbuilt": 76, "inc": [55, 79, 80, 86, 143], "includ": [20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 53, 55, 59, 60, 61, 62, 65, 72, 73, 75, 79, 84, 86, 87, 95, 97, 126, 127, 133, 134, 135, 137, 139, 142, 143], "include_bia": [75, 78, 79], "include_never_tr": 25, "include_scenario": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 87], "incom": [55, 79, 80, 82, 86, 143], "incorpor": [56, 86, 127, 133], "increas": [60, 74, 76, 78, 87, 143], "increment": 142, "ind": 79, "independ": [9, 10, 11, 20, 22, 23, 24, 26, 41, 54, 56, 59, 74, 78, 82, 97, 102, 109, 111, 112, 114, 115, 142], "index": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 54, 59, 63, 67, 71, 72, 73, 77, 78, 79, 81, 82, 90, 91, 93, 110, 111, 112, 114, 115, 140], "index_col": 71, "india": [110, 141], "indic": [4, 5, 6, 11, 14, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 54, 55, 57, 59, 60, 61, 62, 74, 78, 79, 80, 84, 85, 87, 88, 89, 91, 93, 95, 97, 99, 102, 103, 109, 110], "individu": [25, 29, 33, 46, 47, 53, 55, 59, 60, 65, 72, 73, 74, 77, 79, 80, 84, 86, 95, 97, 143], "individual_df": 59, "induc": [94, 110], "industri": [54, 78], "inf": [4, 5, 6, 53, 60, 61, 62, 85], "inf_model": 111, "infer": [13, 15, 51, 52, 54, 61, 66, 67, 71, 76, 77, 78, 85, 90, 94, 97, 110, 138, 140, 141, 142], "inferenti": 143, "infinit": [4, 5, 6, 85, 142], "influenc": [32, 43, 47, 97], "info": [51, 56, 58, 60, 61, 62, 63, 65, 74, 77, 78, 79, 80, 86, 88, 91, 92, 93, 140, 142, 143], "inform": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 46, 47, 51, 56, 60, 61, 62, 66, 68, 69, 76, 84, 85, 86, 87, 97, 98, 127, 128, 141], "infti": [52, 67, 90, 97, 102], "inher": 87, "inherit": [81, 92, 93, 142], "initi": [4, 5, 6, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 55, 56, 57, 58, 60, 61, 62, 70, 79, 80, 83, 84, 86, 87, 88, 91, 92, 93, 95, 96, 97, 110, 140, 142, 143], "inlin": [63, 81], "inlinebackend": 81, "inner": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 96], "innermost": 96, "input": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 56, 61, 86, 89, 97, 101, 126, 127, 128, 133], "insensit": 97, "insid": [42, 43, 46, 47], "insight": [71, 87], "insignific": 86, "inspect": 140, "inspir": [9, 12, 13, 19, 60, 87], "instal": [55, 77, 84, 97, 142], "install_github": 139, "instanc": [46, 47, 55, 56, 79, 96], "instanti": [54, 55, 78, 79, 96, 110], "instead": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 51, 53, 55, 60, 61, 62, 65, 66, 73, 74, 77, 79, 80, 95, 96, 97, 111, 115, 127, 129, 131, 132, 135, 136, 142], "instruct": [139, 142], "instrument": [4, 5, 6, 7, 12, 15, 32, 38, 54, 55, 56, 57, 58, 60, 61, 62, 63, 65, 74, 78, 79, 80, 83, 86, 88, 91, 92, 93, 96, 97, 99, 101, 105, 107, 111, 120, 126, 140, 143], "instrument_effect": 51, "instrument_impact": 66, "instrument_strength": 85, "insuffienct": 77, "int": [9, 10, 11, 14, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 53, 54, 55, 58, 61, 66, 70, 82, 83, 85, 87, 88, 92, 93], "int32": 61, "int64": [60, 62, 63, 76, 78, 91, 92, 93, 140], "int8": [79, 80, 86], "integ": [26, 56, 96], "integr": [77, 87, 127, 137, 142], "intend": [40, 56, 87, 143], "intent": [97, 143], "inter": 96, "interact": [9, 12, 13, 14, 29, 30, 32, 33, 40, 41, 65, 87, 94, 96, 105, 106, 134, 135, 138, 142, 143], "interchang": 126, "interest": [9, 10, 32, 33, 38, 39, 52, 55, 57, 58, 67, 71, 79, 80, 84, 85, 88, 90, 95, 97, 99, 101, 103, 104, 105, 106, 107, 108, 109, 111, 126, 140, 143], "interfac": [53, 55, 56, 91, 93, 96, 110, 140], "intermedi": [73, 87], "intern": [53, 55, 56, 65, 77, 80, 96, 141], "internet": [55, 79, 80], "interpret": [60, 61, 62, 72, 73, 85, 87, 95, 97, 101, 127, 128, 134, 135, 136, 137, 139, 143], "intersect": [87, 127, 133, 142], "interv": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 53, 54, 55, 57, 58, 60, 61, 62, 65, 68, 69, 70, 72, 73, 75, 78, 80, 83, 84, 86, 88, 94, 95, 110, 111, 127, 133, 140, 141, 142, 143], "intial": 84, "introduc": [52, 67, 90, 91, 93, 126, 142, 143], "introduct": [52, 54, 56, 67, 78, 80, 86, 96, 97, 99, 102, 127, 128], "introductori": [53, 87], "intrument": [57, 88], "intspecifi": 40, "intuit": 87, "inuidur1": [56, 63, 91, 93, 140], "inuidur1femaleblackothracedep1dep2q2q3q4q5q6agelt35agegt54durablelusdhusdtg": [56, 91, 93, 140], "inuidur2": [63, 91, 93, 140], "inv_sigmoid": 81, "invalid": [52, 67, 90], "invari": [97, 99], "invers": [29, 31, 32, 33, 34, 35, 36, 37, 57, 88, 127, 134, 135], "invert": 32, "invert_yaxi": 78, "investig": [71, 77, 87], "involv": [95, 96, 111, 143], "io": [81, 142], "ipw_norm": 142, "ipykernel_13983": 73, "ipynb": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "ira": [55, 79, 80], "irm": [0, 13, 14, 20, 22, 23, 27, 28, 38, 39, 44, 45, 76, 87, 89, 94, 96, 101, 106, 119, 134, 135, 138, 142, 143], "irm_summari": 79, "irmglmnet": 55, "irmrang": 55, "irmrpart": 55, "irmxgboost8047": 55, "irrespect": 87, "irrevers": [97, 102], "is_classifi": [20, 22, 23, 29, 32, 33, 39], "is_gat": [29, 33, 39, 44], "isfinit": [60, 61, 62], "isnan": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 76, 96], "isoton": 87, "isotonicregress": 87, "issn": 71, "issu": [21, 24, 87, 138, 141, 142], "ite": [60, 65, 72, 73, 74], "ite_lower_quantil": 60, "ite_mean": 60, "ite_upper_quantil": 60, "item": [32, 79, 89, 96, 110], "iter": [40, 51, 57, 58, 78, 79, 84, 88, 96, 126, 143], "itertool": 71, "its": [42, 43, 87, 89, 95, 96, 97, 99, 101, 110, 111, 126], "iv": [12, 15, 16, 32, 38, 39, 52, 54, 67, 78, 90, 91, 93, 105, 107, 122, 123, 127, 136, 138, 142, 143], "iv_2": 51, "iv_var": [54, 78], "iv\u00e1n": [110, 141], "j": [7, 8, 9, 10, 12, 13, 15, 16, 17, 18, 19, 25, 26, 52, 53, 54, 56, 57, 65, 67, 71, 78, 81, 85, 88, 90, 96, 97, 103, 126, 138, 140], "j_": [54, 78], "j_0": 126, "j_1": [54, 78], "j_2": [54, 78], "j_3": [54, 78], "j_k": [54, 78], "jame": 141, "janari": 60, "janni": [55, 79], "januari": 60, "jasenakova": 142, "javanmard": 141, "jbe": [54, 78], "jeconom": [9, 10, 25, 26, 53], "jerzi": 141, "jia": 87, "jitter": 24, "jitter_valu": 24, "jk": [97, 104], "jmlr": [56, 138, 140, 142], "job": [55, 79, 80], "john": 141, "joint": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 60, 61, 62, 65, 68, 69, 70, 72, 73, 75, 80, 83, 85, 97, 99, 126, 142, 143], "jointli": [83, 95], "jonathan": 141, "joss": [56, 96, 138, 140], "journal": [7, 8, 9, 10, 16, 17, 19, 25, 26, 53, 54, 56, 71, 78, 81, 85, 87, 90, 96, 138, 140, 141, 142], "jss": 138, "jump": [82, 84, 97], "jun": [53, 141], "jupyt": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "juraj": 141, "just": [53, 56, 58, 59, 60, 65, 70, 72, 73, 74, 75, 82, 83, 97, 111, 112, 114, 115, 127, 128], "justif": [110, 127, 128], "k": [7, 10, 12, 13, 15, 16, 17, 19, 25, 26, 52, 54, 56, 67, 76, 77, 78, 84, 85, 89, 90, 94, 95, 97, 126, 143], "k_h": [84, 97], "kaggl": [55, 79], "kallu": [70, 80, 83, 86, 111, 117, 120, 121, 141], "kappa": 97, "kato": [16, 54, 78, 126, 141], "kb": [58, 61, 62, 65, 74, 78, 79, 80, 86, 91, 92, 93, 140], "kde": [34, 35, 36, 79], "kdeplot": [58, 76, 88], "kdeunivari": [34, 35, 36], "kecsk\u00e9sov\u00e1": 142, "keel": 85, "keep": [43, 47, 53, 73, 75, 87, 143], "kei": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 45, 54, 55, 68, 69, 72, 73, 77, 78, 79, 80, 84, 87, 96, 97, 111, 127, 133, 142], "keith": 141, "kelz": 85, "kengo": 141, "kennedi": 85, "kernel": [34, 35, 36, 40, 43, 47, 84, 97], "kernel_regress": 84, "kernelreg": 84, "keyword": [16, 17, 18, 25, 26, 29, 33, 39, 41, 44], "kf": 110, "kfold": [78, 110], "kind": [51, 66, 79], "kj": [10, 12, 13, 15, 16, 17, 19, 25, 26, 52, 54, 67, 78, 90], "klaassen": [12, 71, 76, 77, 87, 138, 141], "klaa\u00dfen": 12, "knau": 141, "know": [58, 82, 85], "knowledg": [51, 66, 76, 81, 82], "known": [74, 76, 84, 85, 87, 96, 97, 102], "kohei": 141, "kotthof": 56, "kotthoff": [56, 96, 138, 140], "krueger": 81, "kueck": [55, 79], "kurz": [138, 141, 142], "kwarg": [9, 10, 14, 16, 17, 18, 25, 26, 29, 33, 39, 40, 41, 42, 44, 77, 97], "l": [54, 56, 57, 63, 68, 69, 78, 85, 87, 88, 96, 127, 136, 138, 140], "l1": [79, 88, 97], "l_hat": [38, 39, 52, 67, 111], "lab": 57, "label": [21, 24, 42, 46, 59, 65, 67, 68, 69, 70, 72, 73, 75, 77, 80, 81, 83, 84], "labor": 81, "laffer": 141, "laff\u00e9r": [19, 57, 88, 97, 109, 111, 124, 125], "lal": [81, 142], "lambda": [54, 55, 56, 57, 60, 79, 81, 82, 96, 97, 111, 112, 126, 140], "lambda_": 71, "lambda_0": [111, 112], "lambda_t": 26, "land": 82, "lang": [56, 96, 138, 140], "langl": [11, 82], "lappli": 110, "larg": [52, 67, 74, 76, 77, 81, 87, 97], "larger": [33, 53, 84, 87, 127, 133], "largest": 76, "largli": 76, "lasso": [54, 55, 56, 57, 61, 79, 88, 96, 140, 141], "lasso_class": [55, 79], "lasso_pip": [56, 96], "lasso_summari": 79, "lassocv": [37, 61, 71, 78, 79, 88, 96, 97, 126, 140], "last": [26, 56, 139], "late": [32, 51, 55, 79, 85, 97, 105, 111, 118], "latent": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 86, 127, 136, 137], "later": [55, 56, 84, 87, 96, 143], "latter": [46, 47, 85, 97], "layout": 71, "lbrace": [12, 13, 19, 32, 33, 54, 78, 89, 97, 103, 105, 106, 110, 111, 116, 126, 127, 134], "ldot": [38, 39, 54, 57, 78, 88, 89, 97, 107, 108, 110, 126, 140], "le": [26, 58, 82, 95, 97, 99, 111, 120, 121], "lead": [53, 87, 97], "leadsto": 126, "lear": [56, 96, 138, 140], "learn": [4, 5, 6, 7, 8, 16, 17, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 55, 56, 61, 63, 65, 66, 70, 71, 75, 76, 77, 79, 80, 81, 83, 84, 85, 87, 91, 93, 94, 96, 110, 111, 126, 127, 128, 142, 143], "learner": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55, 57, 58, 60, 61, 62, 67, 68, 69, 71, 78, 79, 80, 86, 87, 88, 89, 90, 94, 97, 99, 101, 110, 111, 126, 127, 133, 142, 143], "learner_class": [37, 142], "learner_cv": 56, "learner_forest_classif": 56, "learner_forest_regr": 56, "learner_l": 86, "learner_lasso": 56, "learner_list": 76, "learner_m": 86, "learner_nam": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 96], "learner_param_v": 56, "learner_rf": 126, "learnerclassif": 56, "learnerregr": 56, "learnerregrcvglmnet": 56, "learnerregrrang": [56, 96], "learning_r": [60, 67, 70, 80, 83, 84, 87, 90], "least": [51, 55, 66, 79, 80, 86, 97, 101, 110], "leav": [57, 87, 88], "left": [12, 13, 15, 16, 19, 25, 52, 54, 65, 67, 76, 78, 79, 80, 81, 83, 84, 90, 97, 111, 112, 114, 115, 126, 127, 129, 131, 132, 134, 135], "legend": [55, 59, 60, 65, 67, 68, 69, 70, 72, 73, 75, 76, 80, 81, 83], "lemp": 61, "len": [65, 70, 76, 77, 78, 80, 83, 85], "length": [20, 21, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 56, 58, 60, 85, 96], "leq": [54, 78], "less": [53, 55, 79, 80, 84, 87], "lester": 141, "let": [9, 10, 14, 25, 26, 52, 53, 55, 56, 57, 58, 60, 65, 67, 70, 72, 73, 75, 76, 79, 80, 83, 87, 88, 89, 90, 96, 97, 99, 102, 109, 127, 128, 137, 143], "level": [14, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 54, 55, 57, 58, 59, 60, 61, 62, 65, 68, 69, 70, 72, 73, 74, 75, 78, 79, 80, 83, 85, 86, 87, 88, 96, 103, 104, 111, 116, 127, 133, 134, 143], "level_0": [56, 78], "level_1": 78, "level_bound": 65, "levi": 85, "levinsohn": [54, 78], "lewi": 141, "lgbmclassifi": [58, 59, 60, 70, 76, 80, 83, 84, 87], "lgbmregressor": [58, 59, 60, 67, 70, 76, 80, 84, 87, 90], "lgr": [52, 53, 54, 55, 56, 57, 89, 96, 97, 110, 111, 126, 140], "lib": [60, 61, 62, 78, 79], "liblinear": [79, 88, 97], "librari": [51, 52, 53, 54, 55, 56, 57, 89, 90, 91, 93, 96, 97, 110, 111, 126, 139, 140, 143], "licens": 142, "lie": 141, "lightgbm": [58, 59, 60, 67, 70, 76, 80, 83, 84, 87], "like": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 53, 55, 56, 60, 61, 62, 71, 73, 79, 80, 87, 96, 97, 98, 110, 140, 143], "lim": 81, "lim_": [84, 97], "limegreen": [68, 69], "limit": [81, 97, 102, 141], "limits_": 95, "lin": [84, 87, 97], "line": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 59, 61, 65, 87], "linear": [9, 10, 14, 15, 16, 17, 18, 25, 27, 28, 29, 33, 38, 39, 44, 51, 52, 53, 54, 56, 58, 59, 61, 65, 66, 67, 68, 69, 71, 72, 75, 76, 77, 78, 85, 86, 87, 89, 90, 94, 95, 96, 101, 107, 108, 110, 112, 114, 115, 116, 118, 119, 122, 123, 126, 133, 135, 136, 137, 138, 140, 141, 142, 143], "linear_learn": 60, "linear_model": [29, 33, 37, 39, 44, 60, 61, 62, 63, 65, 66, 71, 75, 76, 78, 79, 84, 85, 87, 88, 96, 97, 126, 140], "linearli": [84, 97], "linearregress": [51, 60, 61, 62, 65, 66, 75, 76, 84, 85, 87], "linearscoremixin": [0, 111], "lineplot": [60, 65], "linestyl": [59, 60, 65, 77, 84], "linetyp": 57, "linewidth": [59, 60], "link": [87, 142], "linspac": [68, 69, 75, 87], "lint": 142, "linux": 139, "list": [4, 5, 6, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 47, 52, 53, 54, 55, 56, 60, 61, 67, 68, 69, 78, 80, 82, 90, 96, 110, 111, 139, 142], "list_confset": 32, "listedcolormap": 78, "literatur": [87, 97, 99, 102], "littl": [74, 85], "ll": [56, 126, 143], "lllllllllllllllll": [91, 93, 140], "lm": [51, 53, 87], "ln_alpha_ml_l": 71, "ln_alpha_ml_m": 71, "load": [51, 53, 55, 56, 71, 79, 80, 91, 93, 139, 140], "loader": 0, "loc": [59, 60, 61, 62, 65, 67, 70, 71, 73, 78, 81, 83, 86, 87], "local": [32, 34, 95, 97, 105, 141, 142], "localconvert": 78, "locat": [70, 83, 97], "log": [54, 60, 61, 62, 71, 76, 78, 81, 86, 96, 97, 99, 101], "log_odd": 82, "log_p": [54, 78], "log_reg": [51, 53], "logarithm": 71, "logic": [32, 56, 96], "logical_not": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 76, 96], "logist": [9, 41, 51, 53, 55, 57, 61, 65, 66, 79, 85, 87, 88, 143], "logisticregress": [51, 60, 61, 62, 63, 65, 66, 75, 84, 85, 87], "logisticregressioncv": [37, 61, 76, 79, 88, 97], "logit": [76, 81], "loglik": 56, "logloss": [55, 79, 143], "logo": 142, "logspac": 79, "long": [6, 9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 61, 67, 76, 86, 87, 127, 128, 137, 141], "longer": [60, 97, 101], "look": [53, 55, 56, 58, 59, 60, 61, 62, 70, 76, 79, 80, 83, 84, 86], "loop": [65, 85], "loss": [60, 61, 62, 76, 77, 84, 86, 96, 97, 99, 101], "loss_ml_g0": 76, "loss_ml_g1": 76, "loss_ml_m": 76, "low": [59, 74, 85, 95, 141], "lower": [32, 55, 56, 59, 60, 65, 70, 71, 74, 75, 80, 81, 83, 84, 85, 86, 87, 96, 127, 133, 137, 143], "lower_bound": [68, 69], "lpop": 61, "lpq": [34, 36, 80, 95, 120, 142], "lpq_0": 83, "lpq_1": 83, "lqte": 95, "lr": 84, "lrn": [51, 52, 53, 54, 55, 56, 57, 89, 96, 97, 110, 111, 126, 140, 143], "lrn_0": 56, "lt": [51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 65, 74, 78, 79, 80, 82, 86, 87, 88, 91, 93, 140], "lucien": 142, "luka": 141, "luk\u00e1\u0161": 19, "lusd": [56, 63, 91, 93, 140], "lvert": 71, "m": [6, 7, 8, 9, 15, 16, 17, 24, 37, 52, 54, 56, 60, 63, 67, 71, 74, 76, 77, 78, 81, 85, 90, 92, 93, 94, 95, 96, 97, 98, 101, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 129, 131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142], "m_": [65, 97, 101, 103, 111, 115, 116, 120, 126], "m_0": [17, 18, 20, 22, 23, 24, 29, 31, 32, 33, 35, 38, 39, 40, 52, 54, 55, 67, 71, 74, 77, 78, 79, 90, 95, 96, 97, 105, 106, 107, 108, 111, 112, 114, 115, 117, 120, 121, 123, 124, 125, 140, 143], "m_hat": [32, 33, 38, 39, 52, 67, 75, 111], "m_i": [84, 97], "ma": [16, 54, 78, 85, 97, 98, 141], "mac": 139, "machin": [4, 5, 6, 7, 8, 16, 17, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 55, 56, 57, 58, 60, 61, 62, 63, 65, 66, 70, 71, 75, 76, 77, 79, 80, 81, 83, 84, 85, 86, 87, 88, 94, 96, 97, 99, 101, 110, 111, 126, 127, 128, 142, 143], "machineri": [71, 141], "mackei": 141, "maco": 139, "made": [97, 109, 143], "mae": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 76, 96], "maggi": 141, "magnitud": [127, 128], "mai": [43, 47, 57, 58, 88], "main": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 62, 71, 80, 87, 97, 126, 127, 128, 141, 143], "mainli": 87, "maintain": [53, 138, 142], "mainten": 142, "major": [56, 87, 142], "make": [51, 65, 66, 76, 77, 87, 95, 96, 142, 143], "make_confounded_irm_data": [87, 142], "make_confounded_plr_data": 86, "make_did_cs2021": [6, 24, 60, 92, 93, 97, 98, 101], "make_did_sz2020": [20, 23, 58, 97, 99], "make_heterogeneous_data": [68, 69, 72, 73, 74], "make_iivm_data": [32, 34, 95, 97], "make_irm_data": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 75, 76, 95, 96, 97], "make_irm_data_discrete_treat": 65, "make_pipelin": 79, "make_pliv_chs2015": [38, 97], "make_pliv_multiway_cluster_ckms2021": [4, 54, 78], "make_plr_ccddhnr2018": [5, 6, 20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 67, 77, 89, 90, 95, 96, 97, 110, 111, 126, 127, 133], "make_simple_rdd_data": [40, 84, 97], "make_spd_matrix": 18, "make_ssm_data": [57, 88, 97], "malt": [138, 141], "man": [51, 66], "manag": [96, 139], "mani": [15, 27, 28, 52, 53, 54, 56, 58, 67, 77, 78, 90, 111, 126, 143], "manili": 44, "manipul": [55, 56, 84, 97], "manual": [55, 75, 77, 86, 143], "mao": 141, "map": [32, 42, 43, 46, 47, 53, 54, 78, 95, 97, 105], "mapsto": [89, 95], "mar": [19, 97], "march": [71, 76, 77], "margin": [68, 69, 87], "marit": [55, 79], "marker": [60, 65, 87], "markers": 81, "market": 81, "markettwo": 54, "markov": [18, 141], "marr": [55, 79, 80, 86, 143], "marshal": 96, "martin": [19, 87, 138, 141, 142], "masatoshi": 141, "masip": 142, "mask": 24, "maskedarrai": [97, 98], "master": [53, 61], "mat": 54, "match": [96, 127, 136], "math": [37, 60, 61, 62], "mathbb": [9, 10, 14, 25, 26, 27, 28, 32, 33, 38, 39, 54, 57, 58, 59, 60, 65, 74, 76, 77, 78, 81, 84, 88, 95, 97, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 114, 115, 116, 117, 119, 120, 121, 123, 124, 125, 126, 127, 129, 131, 132, 133, 134, 135, 136, 137, 140, 143], "mathcal": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 41, 52, 54, 57, 59, 67, 70, 78, 82, 83, 88, 90, 97, 98, 101, 102], "mathop": 95, "mathrm": [9, 10, 60, 61, 62, 84, 97, 98, 101, 102, 111, 115, 127, 132], "matia": 141, "matplotlib": [21, 24, 58, 59, 60, 61, 62, 63, 65, 67, 68, 69, 70, 72, 73, 75, 76, 77, 78, 79, 80, 81, 83, 84, 87, 88], "matric": [82, 142], "matrix": [9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 43, 47, 52, 54, 55, 56, 57, 67, 78, 88, 90, 91, 93, 96, 126, 140, 142, 143], "matt": 141, "matter": [76, 81], "max": [25, 55, 56, 75, 79, 80, 85, 89, 95, 96, 97, 110, 111, 115, 117, 126, 127, 132, 140, 143], "max_depth": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 63, 79, 86, 89, 95, 96, 97, 99, 110, 111, 126, 127, 133, 140, 143], "max_featur": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 63, 79, 86, 89, 95, 96, 97, 110, 111, 126, 127, 133, 140, 143], "max_it": [78, 79, 87], "maxim": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 82, 95, 97], "maxima": 126, "maximum": [95, 96], "mb": [60, 63, 88, 91, 93, 140], "mb706": 142, "mea": 12, "mean": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 51, 52, 54, 55, 58, 60, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 83, 85, 86, 87, 90, 96, 97, 126, 143], "mean_absolute_error": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 76, 96], "meant": [61, 95, 142], "measir": 86, "measur": [53, 56, 61, 71, 77, 85, 86, 87, 96, 97, 98, 109, 127, 128, 134, 135, 136, 137], "measure_col": 71, "measure_func": 53, "measure_pr": 53, "measures_r": 53, "mechan": [42, 43, 46, 47, 87], "median": [85, 87, 110], "melt": 54, "membership": 87, "memori": [58, 60, 61, 62, 63, 65, 74, 78, 79, 80, 86, 88, 91, 92, 93, 140], "mention": [74, 95], "merg": [55, 79], "mert": [110, 141], "meshgrid": [68, 69, 87], "messag": [52, 53, 54, 55, 56, 57, 140, 142], "meta": [42, 43, 46, 47, 96, 140], "metadata": [42, 43, 46, 47], "metadata_rout": [42, 43, 46, 47], "metadatarequest": [42, 43, 46, 47], "method": [4, 5, 6, 20, 21, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 72, 73, 75, 76, 78, 79, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 128, 133, 138, 140, 142], "methodolog": 141, "methodologi": 87, "metric": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 46, 96], "michael": 141, "michaela": 142, "michel": [138, 140], "michela": [19, 141], "mid": [55, 79, 81, 84, 97, 111, 123], "mid_point": 65, "might": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 54, 61, 70, 75, 76, 78, 82, 84, 86, 87, 96, 97], "mild": [52, 67, 90], "militari": 81, "miller": [54, 78], "mimic": 87, "min": [54, 55, 56, 57, 60, 61, 62, 70, 75, 78, 79, 80, 83, 84, 89, 96, 97, 101, 110, 111, 126, 140, 143], "min_": 95, "min_samples_leaf": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 74, 79, 86, 89, 95, 96, 97, 99, 110, 111, 126, 127, 133, 143], "min_samples_split": [79, 97, 98, 101], "minim": [33, 45, 55, 76, 79, 84, 97], "minimum": 61, "minor": [52, 61, 67, 90, 111, 142], "minsplit": 55, "minut": 77, "miruna": 141, "mislead": 142, "miss": [4, 5, 6, 37, 56, 96, 97, 111, 124, 142], "missing": [19, 57, 88], "misspecif": 58, "misspecifi": 58, "mit": [138, 140], "mixin": [0, 27, 28, 111], "ml": [18, 54, 55, 56, 61, 71, 77, 78, 79, 84, 89, 94, 96, 97, 110, 138, 141, 142], "ml_g": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 55, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 70, 72, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 95, 96, 97, 98, 99, 101, 142], "ml_g0": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 53, 55, 58, 60, 61, 62, 63, 76, 79, 86, 96, 97, 99, 101], "ml_g1": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 53, 55, 58, 60, 61, 62, 63, 76, 79, 86, 96, 97, 99, 101], "ml_g_d0": [88, 97], "ml_g_d0_t0": [58, 97, 99], "ml_g_d0_t1": [58, 97, 99], "ml_g_d1": [88, 97], "ml_g_d1_t0": [58, 97, 99], "ml_g_d1_t1": [58, 97, 99], "ml_g_d_lvl0": 97, "ml_g_d_lvl1": 97, "ml_g_sim": 37, "ml_l": [38, 39, 52, 54, 55, 56, 63, 67, 69, 73, 77, 78, 79, 81, 86, 89, 90, 96, 97, 110, 111, 126, 127, 133, 140, 142, 143], "ml_l_bonu": 140, "ml_l_forest": 56, "ml_l_forest_pip": 56, "ml_l_lasso": 56, "ml_l_lasso_pip": 56, "ml_l_rf": 143, "ml_l_sim": 140, "ml_l_tune": 96, "ml_l_xgb": 143, "ml_m": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 133, 140, 142, 143], "ml_m_bench_control": 87, "ml_m_bench_treat": 87, "ml_m_bonu": 140, "ml_m_forest": 56, "ml_m_forest_pip": 56, "ml_m_lasso": 56, "ml_m_lasso_pip": 56, "ml_m_rf": 143, "ml_m_sim": [37, 140], "ml_m_tune": 96, "ml_m_xgb": 143, "ml_pi": [37, 57, 88, 97], "ml_pi_sim": 37, "ml_r": [32, 38, 51, 54, 55, 66, 78, 79, 85, 97, 142], "ml_r0": 97, "ml_r1": [55, 79, 97], "mlr": [56, 96], "mlr3": [51, 52, 53, 54, 55, 57, 89, 96, 97, 110, 111, 126, 138, 140, 142, 143], "mlr3book": [56, 96], "mlr3extralearn": [55, 96], "mlr3filter": 56, "mlr3learner": [51, 52, 53, 54, 55, 89, 96, 97, 110, 111, 126, 140, 143], "mlr3measur": 53, "mlr3pipelin": [96, 142], "mlr3tune": [56, 96, 142], "mlr3vers": 55, "mlrmeasur": 53, "mode": [87, 139], "model": [0, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 47, 48, 51, 52, 53, 54, 56, 58, 59, 60, 61, 62, 66, 67, 70, 71, 74, 76, 78, 80, 83, 86, 89, 90, 91, 92, 93, 94, 96, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 112, 114, 115, 116, 118, 119, 122, 123, 128, 133, 134, 135, 136, 137, 138, 141, 142], "model_data": [55, 79], "model_label": 77, "model_select": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 67, 78, 96, 110], "modellist": 75, "modelmlestimatelowerupp": 55, "modern": [56, 96, 138, 140], "modul": [84, 97, 139], "moment": [27, 28, 54, 78, 97, 101, 111, 126, 127, 128, 137, 140], "monoton": 97, "mont": [9, 10, 11, 14, 68, 69, 72, 73], "montanari": 141, "month": 60, "more": [33, 44, 51, 53, 55, 60, 61, 62, 65, 66, 68, 69, 71, 75, 76, 77, 79, 80, 84, 86, 87, 89, 95, 96, 97, 98, 99, 101, 102, 109, 111, 119, 126, 127, 128, 133, 137, 140, 143], "moreov": [55, 56, 61, 71, 96, 126, 143], "mortgag": [55, 79, 80], "most": [55, 70, 76, 79, 80, 83, 87, 95, 96, 97, 127, 133, 139], "motiv": [87, 90], "motivation_example_bch": 71, "mp": 53, "mpd": [54, 78], "mpdta": 61, "mpg": 78, "mse": [56, 71, 96], "mserd": 84, "msg": 60, "msr": [56, 96], "mtry": [55, 56, 89, 96, 97, 110, 111, 126, 143], "mu": 59, "mu_": 59, "mu_0": 97, "mu_mean": 59, "much": [55, 56, 79, 84, 85, 87, 143], "muld": [63, 91, 93, 140], "multi": [24, 42, 46, 53, 54, 68, 69, 78, 111, 115, 142], "multiclass": [56, 77], "multiindex": 78, "multioutput": [43, 47], "multioutputregressor": [43, 47], "multipl": [4, 5, 6, 14, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 54, 55, 57, 58, 61, 62, 75, 78, 79, 86, 87, 88, 91, 93, 96, 101, 104, 107, 110, 126, 127, 128, 141, 142, 143], "multipletest": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39], "multipli": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 67, 94, 95, 111, 143], "multiprocess": [70, 80, 83], "multitest": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39], "multivariate_norm": 37, "multiwai": [16, 54, 78, 141], "music": 141, "must": [20, 21, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 96, 97], "mutat": 56, "mutual": [29, 33, 39, 55, 72, 73, 79, 80, 95], "my_sampl": 110, "my_task": 110, "n": [9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 25, 26, 29, 31, 32, 33, 34, 35, 37, 38, 39, 41, 51, 52, 54, 56, 57, 59, 60, 65, 66, 67, 70, 71, 74, 78, 81, 82, 83, 84, 88, 89, 90, 95, 96, 97, 102, 110, 126, 138, 139], "n_": [14, 59], "n_aggreg": 21, "n_coef": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 127, 133], "n_color": 60, "n_complier": 83, "n_core": [70, 80, 83], "n_estim": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 58, 59, 60, 63, 67, 68, 69, 70, 72, 73, 74, 79, 80, 82, 83, 84, 86, 87, 89, 90, 95, 96, 97, 99, 110, 111, 126, 127, 133, 140, 143], "n_eval": [56, 96], "n_featur": [42, 43, 46, 47], "n_fold": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 54, 55, 58, 60, 61, 63, 67, 68, 69, 70, 72, 73, 75, 76, 78, 79, 80, 81, 82, 83, 84, 86, 87, 90, 96, 110, 140, 143], "n_folds_per_clust": [54, 78], "n_folds_tun": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "n_framework": 21, "n_iter": [40, 84, 97], "n_iter_randomized_search": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 96], "n_job": 79, "n_jobs_cv": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 76], "n_jobs_model": [24, 30, 36, 70, 80, 83], "n_level": [14, 65], "n_ob": [6, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 44, 45, 52, 56, 57, 58, 59, 60, 65, 67, 68, 69, 72, 73, 74, 75, 76, 77, 84, 86, 87, 88, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 110, 126, 127, 133, 140], "n_output": [42, 43, 46, 47], "n_period": [25, 60], "n_pre_treat_period": [25, 60], "n_rep": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 54, 57, 58, 60, 63, 65, 67, 74, 75, 76, 78, 84, 86, 87, 88, 90, 96, 110, 127, 133, 140, 143], "n_rep_boot": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 60, 61, 62, 65, 68, 69, 70, 72, 73, 75, 80, 83, 126], "n_sampl": [42, 43, 46, 47, 82, 85], "n_samples_fit": [43, 47], "n_split": 110, "n_t": 59, "n_target": [46, 47], "n_theta": 21, "n_time_period": 59, "n_true": [70, 83], "n_var": [52, 56, 67, 90, 91, 93, 96, 126, 140], "n_w": 82, "n_x": [11, 68, 69, 72, 73, 74], "na": [4, 5, 6, 52, 54, 57, 90, 142], "na_real_": [54, 142], "naiv": [52, 67, 90], "name": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 52, 53, 54, 60, 61, 72, 73, 74, 77, 78, 84, 86, 87, 96, 139, 142], "namespac": 53, "nan": [4, 5, 6, 20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 58, 59, 65, 67, 70, 72, 73, 76, 77, 79, 80, 83, 88, 90, 96], "nanmean": 67, "narita": 141, "nat": 60, "nathan": 141, "nation": [87, 110, 141], "nativ": 53, "natt": 82, "natur": 87, "ncol": [54, 55, 56, 84, 91, 93, 96, 126, 140], "ncoverag": 76, "ndarrai": [4, 5, 6, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 91, 93], "nearli": 76, "necess": [54, 78], "necessari": [53, 54, 77, 78, 84, 97, 139], "need": [20, 22, 23, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 53, 55, 57, 66, 67, 77, 80, 85, 88, 96, 110, 127, 137, 142, 143], "neg": [43, 47], "neighborhood": [84, 126], "neither": [4, 5, 6, 54, 78, 91, 93], "neng": 141, "neq": [84, 97], "nest": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 46, 47, 96, 111, 125, 127, 133], "net": [80, 86, 143], "net_tfa": [55, 79, 80, 86, 143], "nev": [97, 101, 102], "never": [25, 32, 53, 54, 60, 61, 62, 73, 78, 97, 102, 142], "never_tak": [32, 55, 79], "never_tr": [22, 24, 60, 61, 62, 97, 98, 101], "nevertheless": 75, "new": [51, 52, 53, 54, 55, 56, 57, 68, 69, 77, 79, 82, 89, 90, 91, 93, 95, 96, 97, 110, 111, 126, 138, 140, 141, 142, 143], "new_data": [68, 69, 82], "newei": [7, 8, 17, 54, 71, 78, 87, 90, 138, 141], "newest": 142, "next": [53, 55, 56, 68, 69, 70, 74, 76, 79, 80, 82, 83, 85, 87, 142], "neyman": [54, 78, 89, 94, 127, 137, 138, 141], "nfold": [54, 55, 57, 97], "nh": 97, "nice": 53, "nifa": [79, 80, 86], "nil": 87, "nine": [54, 78], "nn": 84, "noack": [84, 97, 141, 142], "node": [55, 56, 89, 97, 110, 111, 126, 140, 143], "nois": [41, 81, 82], "nomin": 85, "non": [16, 17, 18, 22, 25, 26, 32, 40, 51, 52, 55, 59, 60, 66, 67, 79, 80, 82, 84, 96, 110, 111, 115, 126, 127, 132], "non_orth_scor": [52, 67, 111], "nondur": 63, "none": [4, 5, 6, 14, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 54, 55, 58, 60, 61, 62, 63, 65, 66, 74, 79, 80, 85, 86, 87, 88, 91, 92, 93, 96, 97, 99, 101, 111, 126, 139, 140], "nonignor": [37, 125], "nonlinear": [25, 28, 55, 60, 79, 84, 97, 111, 120, 121, 142], "nonlinearscoremixin": [0, 111], "nonparametr": [34, 35, 36, 84, 87, 111, 127, 128, 134, 135, 136, 137, 141], "nop": 56, "nor": [4, 5, 6, 54, 78, 91, 93], "norm": 67, "normal": [20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 57, 58, 59, 60, 62, 66, 67, 70, 74, 80, 81, 82, 83, 84, 85, 88, 90, 91, 93, 96, 97, 111, 112, 114, 115, 126, 140], "normalize_ipw": [29, 30, 31, 32, 33, 34, 35, 36, 37, 57, 75, 80, 88], "not_yet_tr": [22, 24, 60], "notat": [54, 57, 58, 78, 88, 97, 99, 101, 102, 109, 111, 115], "note": [4, 5, 6, 21, 24, 27, 28, 32, 33, 37, 38, 39, 42, 43, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 110, 111, 138, 140], "notebook": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 95, 96, 97, 143], "notic": [51, 66, 85], "now": [53, 54, 55, 57, 61, 62, 68, 69, 76, 78, 79, 82, 85, 87, 88, 140, 142], "np": [4, 5, 6, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 58, 59, 60, 61, 62, 63, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 133, 140, 143], "nround": [52, 55, 143], "nrow": [53, 54, 56, 84, 91, 93, 96, 126, 140], "nu": [18, 26, 32, 57, 88, 97, 105, 127, 128, 133, 136, 137], "nu2": [60, 127, 133], "nu_0": [127, 137], "nu_i": [57, 88], "nuis_g0": 51, "nuis_g1": 51, "nuis_l": 143, "nuis_m": [51, 143], "nuis_r0": 51, "nuis_r1": 51, "nuis_rmse_ml_l": 71, "nuis_rmse_ml_m": 71, "nuisanc": [4, 5, 6, 17, 18, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 54, 55, 56, 57, 58, 61, 67, 68, 69, 70, 71, 74, 76, 78, 79, 80, 83, 85, 86, 87, 88, 89, 90, 96, 97, 101, 110, 111, 112, 114, 115, 116, 120, 126, 127, 132, 137, 138, 142, 143], "nuisance_el": [127, 129, 131, 132, 134, 135, 136], "nuisance_loss": [76, 96, 142], "nuisance_target": 76, "null": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 53, 86, 96, 127, 133, 142], "null_hypothesi": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 86, 127, 133], "num": [55, 56, 89, 96, 97, 110, 111, 126, 140], "num_leav": [59, 70, 80, 83], "number": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 47, 52, 54, 59, 60, 61, 67, 68, 69, 70, 71, 72, 73, 76, 78, 80, 82, 83, 84, 87, 97, 101, 110, 126, 138, 140, 143], "numer": [28, 51, 56, 75, 81, 96, 111, 127, 134, 135, 142], "numeric_onli": 71, "numpi": [4, 5, 6, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 45, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 93, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 133, 140], "nuniqu": 60, "ny": 141, "nyt": [97, 101, 102], "o": [59, 65, 71, 72, 73, 76, 77, 79, 81, 84, 126, 138, 140], "ob": [53, 55, 59, 84], "obei": 111, "obj": 79, "obj_dml_data": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 54, 61, 62, 66, 67, 70, 75, 77, 78, 83, 89, 90, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 133, 142], "obj_dml_data_bonu": [91, 93], "obj_dml_data_bonus_df": [91, 93], "obj_dml_data_from_arrai": [4, 5, 6], "obj_dml_data_from_df": [4, 5], "obj_dml_data_sim": [91, 93], "obj_dml_plr": [52, 67, 90], "obj_dml_plr_bonu": [56, 140], "obj_dml_plr_bonus_pip": 56, "obj_dml_plr_bonus_pipe2": 56, "obj_dml_plr_bonus_pipe3": 56, "obj_dml_plr_bonus_pipe_ensembl": 56, "obj_dml_plr_fullsampl": 77, "obj_dml_plr_lesstim": 77, "obj_dml_plr_nonorth": [52, 67], "obj_dml_plr_orth_nosplit": [52, 67], "obj_dml_plr_sim": [56, 140], "obj_dml_plr_sim_pip": 56, "obj_dml_plr_sim_pipe_ensembl": 56, "obj_dml_plr_sim_pipe_tun": 56, "obj_dml_sim": 37, "object": [4, 5, 6, 7, 8, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 51, 55, 56, 57, 58, 60, 61, 62, 63, 65, 68, 69, 70, 73, 74, 75, 77, 79, 80, 83, 84, 88, 91, 92, 93, 95, 96, 97, 98, 99, 101, 110, 111, 126, 138, 140, 141, 142, 143], "obs_confound": [51, 66], "observ": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 37, 38, 39, 41, 44, 45, 48, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 74, 76, 77, 78, 79, 80, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 102, 109, 110, 111, 112, 114, 115, 126, 127, 128, 129, 131, 132, 140, 141, 143], "obtain": [10, 32, 51, 52, 53, 54, 57, 58, 60, 61, 62, 66, 67, 68, 69, 70, 71, 76, 78, 83, 85, 87, 88, 89, 90, 95, 96, 110, 111, 126, 127, 128, 133, 139, 140], "obvious": 60, "occur": [25, 60, 77, 142], "off": [82, 141], "offer": [25, 53, 55, 79, 80, 87, 143], "offici": 139, "offset": 96, "often": 83, "oka": 141, "ol": [29, 33, 39, 44], "olma": [84, 97, 141, 142], "omega": [74, 95, 97, 98, 111, 116, 119, 127, 134, 135], "omega_": [16, 54, 78], "omega_1": [16, 54, 78], "omega_2": [16, 54, 78], "omega_epsilon": [54, 78], "omega_v": [16, 54, 78], "omega_x": [16, 54, 78], "omit": [60, 86, 87, 97, 101, 111, 115, 127, 128, 137, 141, 142, 143], "ommit": 87, "onc": [53, 77, 87, 97, 102, 143], "one": [21, 24, 38, 48, 51, 52, 53, 54, 55, 56, 60, 61, 62, 65, 66, 67, 68, 69, 76, 78, 80, 81, 84, 85, 86, 87, 90, 95, 96, 97, 98, 101, 102, 107, 110, 111, 112, 114, 115, 119, 122, 123, 126, 127, 128, 133, 134, 135, 136, 140, 142], "ones": [56, 59, 70, 77, 83, 86, 95], "ones_lik": [65, 83], "onli": [4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 46, 47, 53, 54, 55, 60, 68, 69, 72, 73, 74, 76, 77, 78, 79, 80, 84, 89, 95, 96, 97, 101, 109, 111, 115, 117, 120, 121, 126, 127, 128, 132, 134, 135, 137, 142], "onlin": 143, "onto": 76, "oo": 77, "oob_error": [56, 96], "oop": 142, "opac": [68, 69], "open": [56, 96, 138, 140], "oper": [56, 142], "opposit": [82, 84, 97], "oprescu": [11, 68, 69, 72, 73, 141], "opt": [60, 61, 62, 79], "optim": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 40, 56, 68, 69, 77, 82, 95, 96, 141], "option": [20, 21, 22, 23, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 51, 52, 54, 55, 57, 60, 61, 65, 68, 69, 72, 73, 74, 76, 78, 79, 80, 88, 96, 97, 110, 111, 117, 120, 121, 126, 142], "oracl": [14, 41, 60, 65], "oracle_valu": [9, 10, 14, 41, 65], "orang": 52, "orcal": [9, 10], "order": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 46, 53, 54, 55, 56, 61, 75, 78, 79, 84, 96, 97, 110, 111], "org": [12, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 96, 138, 139, 142], "orient": [56, 96, 111, 138, 140, 141, 142], "origin": [42, 43, 45, 46, 47, 53, 56, 60, 61, 62, 73, 82, 86, 87, 95, 111, 119], "orign": [55, 79], "orth_sign": [44, 45, 75], "orthogon": [44, 45, 54, 55, 60, 78, 79, 89, 94, 97, 126, 127, 137, 138, 141], "orthongon": [127, 137], "osx": 139, "other": [4, 5, 6, 38, 39, 42, 43, 46, 47, 52, 54, 55, 56, 57, 58, 60, 61, 62, 65, 67, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 90, 91, 93, 95, 96, 97, 107, 108, 110, 111, 119, 126, 127, 137, 138, 139, 140, 141, 142, 143], "other_ind": 78, "otherwis": [20, 22, 23, 29, 32, 33, 39, 42, 43, 46, 47, 55, 79, 80, 82, 97, 99, 111, 115], "othrac": [56, 63, 91, 93, 140], "our": [52, 53, 55, 56, 58, 60, 67, 68, 69, 70, 76, 77, 79, 80, 83, 84, 86, 87, 90, 97, 138, 140, 142, 143], "ourselv": 76, "out": [38, 39, 54, 56, 58, 59, 60, 61, 62, 63, 71, 76, 77, 78, 80, 86, 87, 88, 89, 91, 93, 94, 95, 96, 97, 98, 99, 101, 111, 122, 123, 126, 127, 128, 133, 136, 138, 140, 142, 143], "outcom": [4, 5, 6, 9, 10, 14, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 51, 53, 54, 55, 56, 59, 60, 61, 62, 63, 66, 71, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 91, 92, 93, 96, 99, 101, 102, 103, 107, 108, 109, 115, 116, 126, 128, 133, 134, 136, 137, 140, 142, 143], "outcome_0": 66, "outcome_1": 66, "outer": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 96], "output": [53, 60, 62, 76, 85, 89, 97, 101, 126, 143], "output_list": 85, "outshr": 78, "outsid": 52, "over": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 37, 38, 39, 52, 60, 61, 65, 67, 71, 76, 90, 94, 96, 127, 133, 142], "overal": [21, 60, 61, 62, 82, 87, 97, 98], "overall_aggregation_weight": [21, 60, 61, 62], "overcom": [94, 111], "overfit": [77, 94, 110], "overlap": [58, 87, 97, 99, 102], "overrid": [96, 142], "overridden": 97, "overst": [55, 79, 80], "overview": [76, 126, 127, 133, 141], "overwrit": 142, "ownership": [55, 79], "p": [9, 10, 11, 13, 20, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 90, 95, 96, 97, 98, 99, 101, 102, 110, 111, 112, 114, 115, 116, 117, 120, 121, 124, 125, 126, 127, 134, 135, 138, 139, 140, 142], "p401": [55, 79, 80], "p_0": [111, 112, 114], "p_1": 126, "p_adjust": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 110, 126, 138, 140], "p_dbl": [56, 96], "p_hat": 75, "p_int": 96, "p_n": 15, "p_val": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39], "p_x": [16, 54, 78], "p_x0": 81, "p_x1": 81, "packag": [51, 52, 54, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 77, 78, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 95, 96, 97, 99, 102, 109, 110, 111, 126, 127, 128, 138, 140, 141, 142, 143], "packagedata": 78, "packagevers": 55, "page": [87, 138, 141], "pair": [51, 66], "pake": [54, 78], "paket": [54, 55, 56], "pal": 54, "palett": [21, 24, 60, 65], "pand": 60, "panda": [4, 5, 6, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 44, 45, 57, 58, 59, 60, 61, 62, 63, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 91, 92, 93, 95, 97, 127, 128, 140], "pandas2ri": 78, "panel": [5, 6, 20, 22, 25, 26, 61, 92, 93, 101, 102, 131, 132, 141, 142], "paper": [12, 15, 56, 77, 81, 84, 86, 87, 127, 137, 138, 140, 141, 142], "par": 63, "par_grid": [56, 96], "paradox": [56, 96, 142], "parallel": [53, 58, 59, 60, 65, 70, 76, 83, 97, 99, 101, 102], "param": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 77, 96], "param_grid": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 96], "param_nam": 53, "param_set": [56, 96], "param_v": 56, "paramet": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 55, 57, 58, 60, 61, 65, 67, 68, 69, 70, 71, 74, 75, 76, 78, 79, 82, 83, 84, 85, 86, 87, 88, 89, 90, 93, 94, 95, 96, 97, 99, 101, 102, 103, 104, 105, 106, 109, 110, 120, 121, 126, 127, 128, 133, 135, 137, 138, 140, 141, 142, 143], "parametr": [32, 53, 87, 90, 96, 143], "params_exact": 96, "params_nam": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 53], "parenttoc": 138, "part": [18, 52, 54, 55, 56, 62, 67, 76, 77, 78, 79, 90, 96, 110, 127, 137, 142, 143], "parti": 18, "partial": [10, 15, 16, 17, 18, 28, 38, 39, 54, 56, 63, 71, 77, 78, 86, 89, 94, 96, 107, 108, 110, 122, 123, 126, 133, 134, 135, 136, 137, 138, 140, 142, 143], "partial_": [111, 126], "partiallli": 86, "particip": [7, 80, 86, 143], "particular": [97, 138], "particularli": 77, "partion": [54, 78], "partit": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 78, 89, 94], "partli": 143, "pass": [29, 33, 39, 42, 43, 44, 46, 47, 53, 56, 61, 77, 96, 143], "passo": [138, 140], "past": 54, "paste0": [54, 57], "pastel": 67, "path": [96, 97], "path_to_r": 71, "patsi": [68, 69, 95], "pattern": 87, "paul": 141, "pd": [4, 5, 6, 7, 8, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 58, 59, 60, 61, 62, 65, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 95, 97], "pdf": [67, 81], "pedregosa": [138, 140], "pedregosa11a": [138, 140], "pedro": [53, 141], "penal": [57, 61, 88], "penalti": [55, 56, 61, 66, 79, 85, 87, 88, 96, 97], "pennsylvania": [8, 91, 93, 140], "pension": [55, 79, 80, 143], "peopl": [55, 79, 80], "pep8": 142, "per": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 60, 61, 62, 78], "percent": 96, "percentag": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39], "perf_count": 76, "perfectli": [84, 97], "perform": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 45, 52, 54, 56, 58, 60, 61, 62, 67, 71, 73, 74, 76, 77, 78, 80, 86, 87, 88, 90, 96, 97, 99, 101, 110, 111, 126, 138, 140, 141, 143], "perfrom": 74, "perhap": 143, "period": [20, 22, 24, 25, 53, 58, 59, 62, 92, 93, 98, 99, 101, 102, 115, 141, 142], "perp": [97, 109], "perrot": [138, 140], "person": 143, "pessimist": 87, "peter": 141, "petra": 142, "petronelaj": 142, "pfister": [56, 96, 138, 140], "phi": [54, 78, 95, 126], "philipp": [87, 138, 141], "philippbach": [138, 142], "pi": [13, 15, 18, 37, 95, 97, 111, 124, 125], "pi_": [16, 54, 78], "pi_0": [111, 124, 125], "pi_i": [57, 88, 97], "pick": [84, 143], "pip": [84, 97], "pip3": 139, "pipe": 56, "pipe_forest_classif": 56, "pipe_forest_regr": 56, "pipe_lasso": 56, "pipelin": [42, 43, 46, 47, 56, 79, 142], "pipeop": 56, "pira": [55, 79, 80, 86, 143], "pivot": [71, 78, 141], "plai": [77, 143], "plan": [7, 55, 79, 80, 143], "plausibl": [87, 127], "pleas": [42, 43, 46, 47, 53, 58, 59, 61, 65, 77, 87, 110, 139], "plim": 81, "pliv": [27, 28, 38, 54, 78, 89, 95, 107, 122, 138, 142], "plm": [0, 94, 96, 126, 133, 143], "plot": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 45, 52, 53, 55, 56, 57, 59, 60, 61, 62, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 79, 80, 81, 83, 84, 86, 87, 88, 95, 127, 133], "plot_data": 60, "plot_effect": [21, 24, 60, 61, 62], "plot_tre": [45, 82, 95], "plotli": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 68, 69, 71, 84, 87], "plr": [27, 28, 39, 56, 77, 81, 86, 89, 96, 108, 110, 123, 126, 133, 135, 136, 137, 138, 140, 142, 143], "plr_est": 81, "plr_est1": 81, "plr_est2": 81, "plr_obj": 81, "plr_obj_1": 81, "plr_obj_2": 81, "plr_summari": 79, "plrglmnet": 55, "plrranger": 55, "plrrpart": 55, "plrxgboost8700": 55, "plt": [58, 59, 60, 63, 65, 67, 68, 69, 70, 72, 73, 75, 76, 77, 78, 79, 80, 81, 83, 84, 87, 88], "plt_smpl": [54, 78], "plt_smpls_cluster": [54, 78], "plug": [74, 127, 129, 131, 132, 133, 134, 135], "pm": [40, 54, 78, 126, 127, 133, 137], "pmatrix": [57, 88], "pmlr": [71, 76, 77], "po": [56, 96], "poe": 141, "point": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 53, 54, 72, 73, 78, 87, 95, 97, 143], "pointwis": [44, 70, 72, 73, 83], "poli": [55, 75, 78, 79], "polici": [33, 38, 39, 45, 94, 97, 107, 108, 140, 141, 142], "policy_tre": [33, 82, 95], "policy_tree_2": 82, "policy_tree_obj": 95, "policytre": 82, "polit": 81, "poly_dict": 79, "polynomi": [7, 8, 41, 55, 63, 75, 79, 84], "polynomial_featur": [7, 8, 55, 63], "polynomialfeatur": [75, 78, 79], "poor": 85, "popul": [87, 97, 101, 111, 115], "popular": [76, 97, 127, 128], "porport": 86, "posit": [18, 55, 60, 62, 81, 87, 143], "posixct": [56, 96], "possibl": [4, 5, 6, 43, 46, 47, 53, 56, 60, 62, 68, 69, 72, 73, 74, 75, 76, 77, 82, 84, 85, 86, 87, 96, 97, 101, 103, 104, 126, 127, 128, 142, 143], "possibli": [85, 127, 128], "post": [15, 18, 24, 97, 99, 101, 102, 126, 141], "postdoubl": 141, "poster": 81, "potenti": [9, 14, 25, 29, 30, 31, 34, 35, 37, 41, 57, 58, 60, 75, 81, 84, 88, 99, 102, 103, 109, 116, 117, 126, 134, 139, 142, 143], "potential_level": 65, "power": [56, 77, 87, 96, 141], "pp": [53, 71, 76, 77], "pq": [34, 35, 36, 80, 121, 142], "pq_0": [80, 83], "pq_1": [80, 83], "pr": [37, 51, 54, 55, 56, 57, 96, 97, 110, 111, 126, 140, 143], "practic": [76, 87, 141], "pre": [22, 24, 25, 53, 57, 58, 60, 61, 62, 88, 96, 97, 99, 101, 111, 115, 142], "precis": [53, 97, 127, 135, 143], "precomput": [43, 47], "pred": [53, 77], "pred_df": 82, "pred_dict": 96, "pred_treat": 82, "predict": [4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 52, 54, 55, 56, 61, 67, 70, 71, 75, 76, 77, 78, 79, 82, 85, 87, 90, 95, 110, 127, 128, 133, 135, 142, 143], "predict_proba": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 39, 40, 42, 46, 77, 85, 96], "predictor": [29, 33, 39, 44, 45, 68, 69, 72, 73, 87, 89], "prefer": [55, 79, 80, 143], "preliminari": [31, 52, 67, 84, 111, 117, 120, 121, 125], "prepar": [53, 54, 78, 142], "preprint": [85, 141], "preprocess": [55, 61, 75, 78, 79, 80, 96], "presenc": [55, 79, 80], "present": [53, 61, 87, 96, 111, 119, 143], "prespecifi": 86, "pretest": 53, "pretreat": [20, 22, 23, 24, 53, 58], "prettenhof": [138, 140], "preval": 87, "prevent": [110, 142], "previou": [59, 74, 75, 81, 139, 143], "previous": [61, 85, 96, 143], "price": [54, 78], "priliminari": [34, 36], "primari": 65, "principl": [127, 128], "print": [22, 24, 40, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 91, 92, 93, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 133, 139, 140, 142, 143], "print_detail": 53, "print_period": [22, 24], "prior": [76, 97, 109], "privat": 142, "prob": 56, "prob_dist": 85, "prob_dist_": 85, "probabilit": 74, "probabl": [14, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 46, 52, 53, 57, 58, 60, 65, 67, 74, 81, 83, 84, 87, 88, 90, 97, 111, 112, 114, 115, 120, 141], "problem": [55, 61, 79, 80, 95, 96], "procedur": [52, 54, 55, 67, 76, 78, 79, 86, 87, 96, 126, 139, 142], "proceed": [15, 141], "process": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 41, 53, 57, 58, 59, 62, 68, 69, 70, 71, 72, 73, 76, 77, 82, 83, 87, 88, 94, 126, 127, 128, 141, 142], "produc": 81, "product": [68, 69, 71, 76, 87, 127, 137], "producton": 54, "program": [13, 55, 79, 80, 141, 143], "progress": 64, "project": [56, 68, 69, 95, 138, 142], "project_z": [68, 69], "prone": 111, "pronounc": 84, "propens": [9, 10, 25, 34, 36, 55, 57, 58, 60, 61, 74, 75, 76, 79, 80, 87, 88, 95, 97, 101, 103, 111, 115, 127, 134], "properli": [77, 143], "properti": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 56, 60, 61, 62, 76, 79, 80, 81, 85, 86, 96, 97, 127, 133, 140, 142], "proport": [86, 127, 128, 136, 137], "propos": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 54, 56, 78, 84, 127, 128, 141, 142], "provid": [4, 5, 6, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 53, 54, 55, 56, 61, 68, 69, 72, 73, 75, 77, 78, 79, 84, 87, 89, 90, 91, 92, 93, 94, 96, 126, 138, 140, 142, 143], "prune": [33, 45], "ps911c": 78, "ps944": 78, "pscore1": 81, "pscore2": 81, "psi": [27, 28, 52, 53, 54, 78, 89, 97, 101, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 137, 140], "psi_": [126, 127, 133, 136, 137], "psi_a": [27, 32, 33, 38, 39, 52, 54, 67, 78, 97, 101, 110, 111, 112, 114, 115, 116, 118, 119, 122, 123, 126], "psi_b": [27, 32, 33, 38, 39, 52, 67, 95, 97, 101, 110, 111, 112, 114, 115, 116, 118, 119, 122, 123], "psi_el": [110, 111], "psi_j": 126, "psi_nu2": [127, 133], "psi_sigma2": [127, 133], "public": [51, 66, 142], "publish": [87, 142], "pull": [55, 142], "purchas": 87, "pure": 87, "purp": [68, 69], "purpos": [52, 67, 74, 86, 87, 111, 115, 127, 128, 140], "pval": 126, "px": [71, 84], "py": [60, 61, 62, 73, 78, 79, 87, 138, 139, 142], "py3": 139, "py_al": 67, "py_did": 58, "py_did_pretest": 59, "py_dml": 67, "py_dml_nosplit": 67, "py_dml_po": 67, "py_dml_po_nosplit": 67, "py_double_ml_apo": 65, "py_double_ml_bas": 67, "py_double_ml_basic_iv": 66, "py_double_ml_c": 68, "py_double_ml_cate_plr": 69, "py_double_ml_cvar": 70, "py_double_ml_firststag": 71, "py_double_ml_g": 72, "py_double_ml_gate_plr": 73, "py_double_ml_gate_sensit": 74, "py_double_ml_irm_vs_apo": 75, "py_double_ml_learn": 76, "py_double_ml_meets_flaml": 77, "py_double_ml_multiway_clust": 78, "py_double_ml_pens": 79, "py_double_ml_pension_qt": 80, "py_double_ml_plm_irm_hetfx": 81, "py_double_ml_policy_tre": 82, "py_double_ml_pq": 83, "py_double_ml_rdflex": 84, "py_double_ml_robust_iv": 85, "py_double_ml_sensit": 86, "py_double_ml_sensitivity_book": 87, "py_double_ml_ssm": 88, "py_non_orthogon": 67, "py_panel": 60, "py_panel_data_exampl": 61, "py_panel_simpl": 62, "py_po_al": 67, "pydata": 73, "pypi": [141, 142], "pyplot": [58, 59, 60, 63, 65, 67, 68, 69, 70, 72, 73, 75, 76, 77, 78, 79, 80, 81, 83, 84, 87, 88], "pyproject": 142, "pyreadr": 61, "python": [18, 53, 77, 87, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 101, 110, 111, 126, 127, 128, 133, 138, 140, 141, 142, 143], "python3": [60, 61, 62, 79, 139], "q": [56, 70, 83, 84, 96, 138, 140], "q2": [56, 63, 91, 93, 140], "q3": [56, 63, 91, 93, 140], "q4": [56, 63, 91, 93, 140], "q5": [56, 63, 91, 93, 140], "q6": [56, 63, 91, 93, 140], "q_i": [84, 97], "qquad": 13, "qte": [70, 80, 142], "quad": [25, 26, 55, 57, 58, 79, 82, 84, 88, 95, 97, 99, 102, 109, 111, 120, 126, 127, 129], "quadrat": [57, 88], "qualiti": [86, 89, 142], "quanitl": 80, "quant": 70, "quantifi": [87, 97, 102], "quantil": [14, 24, 30, 31, 34, 35, 36, 60, 65, 70, 75, 86, 94, 96, 117, 120, 121, 141, 142], "quantiti": [51, 66, 87], "queri": 79, "question": [87, 143], "quick": 80, "quit": [76, 82, 86, 127, 128], "r": [12, 32, 42, 43, 46, 47, 59, 60, 61, 62, 67, 68, 69, 71, 78, 81, 84, 85, 87, 89, 90, 91, 93, 94, 97, 105, 110, 111, 118, 122, 126, 127, 128, 134, 135, 136, 137, 138, 140, 141, 142, 143], "r2_d": [13, 76], "r2_score": [43, 47], "r2_y": [13, 76], "r6": [56, 142], "r_0": [32, 38, 55, 79, 97, 105], "r_all": 52, "r_d": 13, "r_df": 78, "r_dml": 52, "r_dml_nosplit": 52, "r_dml_po": 52, "r_dml_po_nosplit": 52, "r_double_ml_bas": 52, "r_double_ml_basic_iv": 51, "r_double_ml_did": 53, "r_double_ml_multiway_clust": 54, "r_double_ml_pens": 55, "r_double_ml_pipelin": 56, "r_double_ml_ssm": 57, "r_hat": 38, "r_hat0": 32, "r_hat1": 32, "r_non_orthogon": 52, "r_po_al": 52, "r_y": 13, "rais": [4, 5, 6, 42, 43, 46, 47, 60, 61, 62, 96], "randint": 81, "randn": 37, "random": [11, 14, 18, 19, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 51, 52, 53, 55, 56, 58, 59, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 95, 96, 98, 99, 101, 102, 110, 124, 126, 127, 133, 137, 140, 141, 143], "random_search": 96, "random_st": [14, 60, 67, 74, 75, 82], "randomforest": [55, 76, 79], "randomforest_class": [55, 68, 79, 82], "randomforest_reg": [68, 82], "randomforestclassifi": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 63, 65, 68, 69, 72, 73, 74, 76, 79, 82, 84, 86, 87, 95, 96, 97, 98, 99, 101, 143], "randomforestregressor": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 63, 65, 67, 68, 69, 72, 73, 74, 76, 79, 82, 84, 86, 87, 89, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 133, 140, 143], "randomized_search": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 96], "randomizedsearchcv": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 96], "randomli": [52, 54, 67, 78, 90, 110, 143], "rang": [52, 58, 59, 65, 67, 70, 72, 73, 76, 77, 78, 80, 82, 83, 84, 85, 87, 88, 90, 96, 97], "rangeindex": [58, 60, 61, 62, 63, 65, 74, 78, 79, 80, 86, 88, 91, 92, 93, 140], "ranger": [53, 55, 56, 89, 96, 97, 110, 111, 126, 140, 143], "rangl": [11, 82], "rank": 142, "rate": [71, 76, 97], "rather": [84, 87, 97], "ratio": [96, 110, 127, 128], "rational": 61, "ravel": [68, 69], "raw": [55, 61, 62, 71, 79], "raw_res_manual_lasso_r_100_n_100_p_200_rho_0": 71, "rbind": 55, "rbindlist": 55, "rbinom": 51, "rbrace": [12, 13, 19, 32, 33, 54, 78, 89, 97, 103, 105, 106, 110, 111, 116, 126, 127, 134], "rcolorbrew": 54, "rcparam": [59, 63, 68, 69, 70, 72, 73, 75, 78, 79, 80, 83], "rd": [97, 142], "rda": 61, "rdbu": 54, "rdbu_r": 78, "rdbwselect": 97, "rdd": [0, 4, 5, 6, 94, 139], "rdflex": [84, 97, 142], "rdflex_fuzzi": 84, "rdflex_fuzzy_stack": 84, "rdflex_obj": [40, 97], "rdflex_sharp": 84, "rdflex_sharp_stack": 84, "rdrobust": [40, 84, 97, 139, 142], "rdrobust_fuzzi": 84, "rdrobust_fuzzy_noadj": 84, "rdrobust_sharp": 84, "rdrobust_sharp_noadj": 84, "rdt044": 71, "re": [60, 78, 87, 139], "read": [61, 139], "read_csv": [62, 71], "read_r": 61, "readabl": 142, "reader": [61, 85], "readili": 138, "real": [55, 79, 80, 86, 127, 128], "realat": 97, "realiz": [84, 97, 109], "reason": [4, 5, 6, 51, 66, 71, 76, 77, 86, 87, 127, 128, 143], "recal": [63, 127, 137], "receiv": [25, 60, 65, 84, 97, 99, 101], "recent": [77, 97, 99, 102, 141], "recogn": [55, 79, 80], "recommend": [56, 60, 76, 84, 87, 89, 97, 110, 127, 139, 141, 142], "recov": [51, 53, 66, 81], "recsi": 141, "red": [54, 57, 72, 73, 77, 78], "reduc": [55, 74, 77, 79, 84, 86, 87, 97, 142], "redund": 142, "reemploy": [8, 91, 93, 140], "ref": 61, "refactor": 142, "refer": [7, 8, 9, 10, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 59, 60, 65, 74, 79, 80, 84, 86, 91, 93, 94, 95, 97, 98, 101, 102, 111, 127, 128, 133, 141, 142], "reference_level": [30, 65, 75, 97], "refin": 142, "refit": [127, 128], "reflect": [82, 87, 95], "reg": [25, 26, 55, 79, 143], "reg_estim": 84, "reg_learn": 80, "reg_learner_1": 76, "reg_learner_2": 76, "regard": [87, 138], "regener": 142, "region": [54, 70, 78, 126, 141], "regr": [51, 52, 53, 54, 55, 56, 57, 89, 96, 97, 110, 111, 126, 140, 143], "regravg": [56, 96], "regress": [9, 10, 12, 13, 14, 15, 16, 17, 18, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 44, 51, 53, 54, 56, 57, 60, 61, 62, 65, 66, 71, 77, 78, 81, 85, 86, 87, 88, 89, 90, 94, 95, 96, 99, 101, 105, 106, 107, 108, 110, 115, 126, 128, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143], "regressor": [43, 47, 52, 55, 65, 67, 70, 76, 77, 79, 90], "regular": [15, 94, 96, 111, 126, 141], "reich": [56, 96], "reinforc": 141, "reject": [55, 79], "rel": [55, 61, 79, 85, 127, 128, 134, 135], "relat": [75, 87, 143], "relationship": [51, 66, 71, 87, 126], "relev": [4, 5, 6, 11, 20, 22, 23, 24, 42, 43, 44, 46, 47, 60, 70, 82, 83, 97, 127, 143], "reli": [58, 59, 60, 68, 69, 74, 75, 95, 96, 97, 99, 111, 115, 127, 128, 143], "reload": 55, "remain": [53, 126, 143], "remark": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 52, 58, 59, 60, 61, 62, 65, 67, 68, 69, 70, 72, 73, 74, 75, 76, 80, 86, 95, 96, 97, 99, 101, 110, 111, 112, 114, 115, 120, 121, 126, 127, 132, 135], "remot": 139, "remov": [55, 75, 87, 94, 97, 110, 111, 127, 142], "renam": [60, 79, 142], "render": [86, 87], "reorgan": 142, "rep": [52, 57, 90, 96, 126], "repeat": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 54, 55, 56, 57, 60, 61, 62, 67, 74, 78, 79, 80, 81, 84, 86, 88, 90, 94, 96, 101, 126, 129, 140, 142, 143], "repeatedkfold": 78, "repet": 86, "repetit": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 61, 68, 69, 71, 72, 73, 74, 76, 94, 96, 126, 140, 142, 143], "repetiton": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40], "replac": [82, 87, 142], "replic": [7, 8, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 55, 61, 62, 67, 71, 85, 87], "repo": 142, "report": [55, 77, 79, 138, 142], "repositori": [60, 71, 84, 142], "repr": [52, 54], "repres": [25, 81, 87, 97], "represent": [10, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 86, 126, 127, 128, 129, 131, 132, 133, 134, 135, 136, 137, 140, 142], "reproduc": 14, "request": [42, 43, 46, 47, 142], "requir": [38, 39, 42, 46, 51, 55, 56, 60, 61, 62, 65, 74, 79, 80, 86, 97, 98, 101, 111, 115, 126, 127, 128, 133, 139, 142, 143], "requirenamespac": 53, "rerun": 61, "res_df": 78, "res_dict": [9, 10, 11, 14, 41], "resampl": [51, 54, 56, 57, 58, 60, 61, 62, 78, 80, 86, 88, 96, 97, 99, 101, 110, 111, 126, 138, 140, 143], "research": [54, 56, 78, 81, 87, 110, 138, 140, 141, 143], "resembl": [57, 88], "reset": 53, "reset_index": [60, 71, 78, 79], "reshap": [59, 67, 68, 69, 75], "reshape2": 54, "residu": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 86, 127, 128, 136, 137], "resolut": [56, 96], "resourc": 76, "resourcewis": 76, "respect": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 60, 61, 62, 65, 79, 80, 84, 95, 97, 101, 105, 109, 110, 127, 137, 143], "respons": [7, 56, 96], "rest": 97, "restart": 139, "restrict": 76, "restructur": 142, "restud": 71, "result": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 51, 52, 53, 56, 57, 58, 59, 60, 61, 62, 65, 67, 68, 69, 71, 74, 75, 76, 82, 84, 85, 86, 87, 88, 90, 96, 110, 111, 112, 114, 115, 127, 128, 133, 140, 142], "result_iivm": 55, "result_irm": 55, "result_plr": 55, "results_df": 85, "retain": [42, 43, 46, 47], "retina": 81, "retir": [55, 79, 80, 86], "return": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 56, 57, 60, 61, 62, 67, 70, 73, 76, 77, 78, 81, 82, 83, 85, 86, 87, 88, 89, 96, 111, 127, 128, 142], "return_count": [65, 76], "return_tune_r": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "return_typ": [4, 5, 6, 7, 8, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 55, 56, 57, 58, 67, 75, 76, 77, 79, 80, 86, 88, 89, 90, 91, 93, 95, 96, 97, 99, 110, 111, 126, 127, 133, 140, 143], "rev": 54, "reveal": 74, "review": [15, 71, 141], "revist": [54, 78], "rf": 84, "rho": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 60, 65, 74, 75, 84, 86, 87, 127, 128, 133, 137, 143], "rho_val": 87, "richter": [56, 96, 138, 140], "riesz": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 86, 127, 128, 129, 131, 132, 133, 136, 137], "riesz_rep": [127, 133], "right": [12, 13, 15, 16, 19, 25, 52, 54, 67, 76, 78, 79, 80, 81, 83, 84, 87, 90, 97, 111, 112, 114, 115, 126, 127, 129, 131, 132, 134, 135], "rightarrow_": [52, 67, 90], "risk": [31, 94, 142], "ritov": 141, "rival": 78, "rival_ind": 78, "rmd": 53, "rmse": [53, 58, 60, 61, 62, 76, 77, 80, 86, 88, 96, 97, 99, 101, 111, 126, 140, 142], "rmse_dml_ml_l_fullsampl": 77, "rmse_dml_ml_l_lesstim": 77, "rmse_dml_ml_l_onfold": 77, "rmse_dml_ml_l_untun": 77, "rmse_dml_ml_m_fullsampl": 77, "rmse_dml_ml_m_lesstim": 77, "rmse_dml_ml_m_onfold": 77, "rmse_dml_ml_m_untun": 77, "rmse_oos_ml_l": 77, "rmse_oos_ml_m": 77, "rmse_oos_onfolds_ml_l": 77, "rmse_oos_onfolds_ml_m": 77, "rnorm": [51, 56, 91, 93, 96, 126, 140], "robin": [7, 8, 17, 54, 71, 78, 90, 138, 141], "robinson": [52, 67, 90], "robject": 78, "robu": [72, 73], "robust": [9, 10, 16, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 53, 60, 61, 65, 74, 75, 84, 86, 87, 97, 127, 133, 141, 142, 143], "robust_confset": [32, 85, 142], "robust_cov": 85, "robust_length": 85, "roc\u00edo": 141, "role": [4, 5, 6, 52, 67, 77, 90, 143], "romano": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 126], "root": [20, 22, 23, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 71, 90, 96, 111, 141], "rotat": [77, 84], "roth": [84, 97, 99, 102, 141], "rough": [87, 143], "roughli": [60, 87], "round": [55, 61, 65, 75, 76, 81, 87], "rout": [42, 43, 46, 47], "row": [21, 52, 55, 59, 61, 63, 68, 69, 77, 78, 82, 91, 93, 97, 101, 110, 140, 143], "row_index": 73, "rownam": 54, "rowv": 54, "roxygen2": 142, "royal": [87, 141], "rpart": [55, 56, 96], "rpart_cv": 56, "rprocess": 76, "rpy2": 78, "rpy2pi": 78, "rskf": 75, "rsmp": [56, 96, 110], "rsmp_tune": [56, 96], "rssb": 87, "rtype": 30, "ruben": 141, "ruiz": [51, 66], "rule": [53, 95], "run": [53, 61, 84, 97, 139, 142], "runif": 51, "runner": [60, 62, 87], "runtime_learn": 56, "rv": [60, 65, 74, 75, 86, 87, 127, 133, 143], "rva": [60, 65, 74, 75, 86, 87, 127, 133, 143], "rvert": 71, "rvert_": 71, "s1": 77, "s2": 77, "s_": [16, 54, 78, 97, 109], "s_1": 17, "s_2": 17, "s_col": [4, 5, 57, 84, 88, 97], "s_i": [19, 57, 84, 88, 97], "s_x": [16, 54, 78], "safeguard": [58, 96], "sake": [55, 79, 87, 143], "same": [14, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 44, 52, 54, 57, 60, 61, 67, 68, 69, 74, 75, 76, 78, 80, 82, 84, 85, 86, 87, 88, 96, 97, 101, 111, 112, 114, 115, 126, 127, 135, 142], "samii": 81, "sampl": [9, 10, 16, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 51, 53, 54, 56, 58, 60, 61, 62, 66, 72, 73, 75, 76, 78, 80, 82, 85, 86, 94, 96, 99, 101, 102, 109, 126, 140, 141, 142], "sample_weight": [40, 42, 43, 46, 47, 84], "sant": [9, 10, 14, 20, 22, 23, 25, 26, 53, 58, 60, 61, 62, 97, 98, 99, 101, 102, 141], "sara": 141, "sasaki": [16, 54, 78, 141], "satisfi": [57, 61, 88, 96, 111, 126], "save": [52, 55, 61, 67, 72, 73, 76, 77, 79, 80, 96, 127, 133, 143], "savefig": 67, "saveguard": 76, "saver": [55, 79, 80], "sc": 60, "scalar": 97, "scale": [25, 52, 54, 59, 70, 75, 81, 83, 87, 126, 127, 137], "scale_color_manu": 52, "scale_fill_manu": [52, 54], "scaled_psi": 75, "scatter": [59, 65, 72, 73, 81, 84, 87], "scatterplot": 65, "scenario": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 65, 74, 75, 86, 87, 97, 127, 133, 143], "scene": [68, 69, 71], "scene_camera": 71, "schacht": [71, 76, 77], "schaefer": 81, "schedul": 142, "scheme": [54, 78, 96, 97, 98, 110, 138], "schneider": 56, "schratz": [56, 96, 138, 140], "scienc": [18, 51, 66, 81, 141], "scikit": [61, 76, 79, 96, 138, 140, 142, 143], "scipi": 67, "score": [0, 4, 5, 6, 9, 10, 20, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 68, 69, 70, 71, 74, 75, 76, 77, 78, 79, 80, 83, 84, 86, 87, 88, 89, 94, 95, 96, 97, 98, 99, 101, 102, 103, 110, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 131, 132, 133, 135, 136, 137, 138, 142, 143], "scoring_method": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "script": 139, "sd": 51, "se": [52, 54, 67, 86, 90, 96, 110, 126, 127, 133, 141, 143], "se_df": 54, "se_dml": [52, 67, 90], "se_dml_po": [52, 67, 90], "se_nonorth": [52, 67], "se_orth_nosplit": [52, 67], "se_orth_po_nosplit": [52, 67], "seaborn": [21, 24, 58, 60, 63, 65, 67, 76, 78, 79, 80, 87, 88], "search": [20, 22, 23, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 96, 111], "search_mod": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 96], "searchabl": 55, "second": [16, 20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 56, 67, 76, 77, 78, 89, 90, 110, 126, 127, 128, 137, 140], "secondari": 65, "section": [23, 26, 53, 54, 55, 56, 59, 74, 77, 78, 80, 87, 98, 101, 119, 129, 142], "secur": 81, "see": [7, 8, 13, 19, 20, 22, 23, 27, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 46, 47, 51, 53, 54, 55, 56, 58, 60, 61, 62, 65, 66, 68, 69, 73, 75, 77, 78, 80, 81, 82, 84, 85, 86, 87, 96, 97, 99, 102, 110, 111, 117, 119, 120, 121, 124, 125, 127, 128, 132, 133, 137, 139, 140, 142], "seed": [14, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55, 56, 57, 58, 59, 63, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 133, 140, 143], "seek": 81, "seem": [53, 55, 74, 79, 80, 127, 143], "seen": [72, 73, 75], "sel_cols_chiang": 78, "select": [4, 5, 6, 14, 15, 19, 25, 37, 51, 71, 76, 84, 87, 89, 91, 93, 94, 96, 109, 126, 140, 141, 142, 143], "selected_coef": 76, "selected_featur": [56, 96], "selected_learn": 76, "self": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 76, 77, 85, 143], "selfref": 55, "semenova": [68, 69, 141], "semi": 90, "semiparametr": 7, "sens": [86, 87], "sensemakr": [127, 128], "sensit": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 94, 95, 97, 101, 128, 133, 137, 142], "sensitivity_analysi": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 65, 74, 75, 86, 87, 127, 133, 143], "sensitivity_benchmark": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 65, 74, 86, 87, 127, 128], "sensitivity_el": [127, 133], "sensitivity_param": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 86, 87, 127, 128, 133], "sensitivity_plot": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 65, 74, 86, 87, 127, 133], "sensitivity_summari": [60, 65, 74, 75, 86, 87, 127, 133, 143], "sensitv": 75, "sensitvity_benchmark": 65, "sensiv": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39], "senstiv": [127, 136], "sep": 52, "separ": [21, 81, 86, 96, 97, 110, 142], "seper": [77, 84, 86, 126, 127, 128], "seq_len": [52, 57, 90], "sequenti": 8, "ser": [60, 61, 62], "seri": [60, 61, 62, 73, 87, 141], "serv": [25, 91, 92, 93, 140, 142], "serverless": [141, 142], "servic": 81, "set": [4, 5, 6, 7, 8, 9, 11, 16, 17, 18, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 45, 46, 47, 51, 52, 53, 54, 55, 57, 58, 60, 61, 62, 63, 65, 66, 67, 68, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 84, 85, 86, 87, 88, 89, 90, 91, 93, 95, 97, 98, 101, 102, 110, 111, 112, 114, 115, 116, 119, 126, 127, 128, 134, 135, 136, 139, 140, 142, 143], "set_as_param": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "set_config": [42, 43, 46, 47], "set_fit_request": [46, 47], "set_fold_specif": 96, "set_index": 79, "set_ml_nuisance_param": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 55, 63, 79, 96, 142], "set_param": [42, 43, 46, 47, 77, 96], "set_sample_split": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 75, 76, 110, 142], "set_score_request": [42, 43, 46, 47], "set_styl": [79, 80], "set_text": 76, "set_threshold": [52, 53, 54, 55, 56, 57, 89, 96, 97, 110, 111, 126, 140], "set_tick": 78, "set_ticklabel": 78, "set_titl": [65, 75, 77, 78, 84], "set_x_d": [4, 5, 6], "set_xlabel": [65, 67, 75, 77, 78, 84], "set_xlim": 67, "set_xtick": 81, "set_xticklabel": 81, "set_ylabel": [65, 75, 77, 78, 81, 84], "set_ylim": [70, 75, 77, 78, 83], "setdiff": 142, "setdiff1d": 78, "setminu": [54, 78, 126], "settings_l": 77, "settings_m": 77, "setup": [139, 142], "seven": [54, 78], "sever": [48, 55, 56, 60, 61, 62, 76, 77, 79, 80, 86, 87, 90, 96, 143], "shape": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 45, 46, 47, 59, 60, 65, 68, 69, 72, 73, 76, 78, 79, 82, 84, 86, 87, 96, 97], "share": [54, 55, 78, 79], "sharma": [87, 141], "sharp": 40, "shift": 60, "shock": [54, 78], "short": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 86, 87, 97, 101, 127, 128, 141, 142, 143], "shortcut": 55, "shortli": [54, 56, 78, 96], "shota": 141, "should": [4, 5, 6, 20, 21, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 55, 57, 60, 61, 65, 72, 73, 76, 79, 84, 85, 86, 88, 91, 93, 95, 96, 97, 104, 126, 127, 128, 138], "show": [51, 52, 54, 57, 58, 60, 63, 65, 66, 67, 68, 69, 71, 74, 75, 76, 77, 78, 81, 84, 85, 87, 88, 90, 127, 136, 139], "showcas": 82, "showlabel": 87, "showlegend": 87, "shown": [51, 66, 81, 140], "showscal": [68, 69, 71], "shrink": 84, "shuffl": 110, "side": [84, 97, 127, 133], "sigma": [9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 37, 52, 54, 57, 67, 78, 88, 90, 95, 110, 126, 127, 128, 133, 136, 137], "sigma2": [127, 133], "sigma_": [10, 12, 13, 15, 16, 17, 19, 25, 26, 52, 54, 67, 78, 90], "sigma_0": [127, 137], "sigma_j": 126, "sigmoid": 81, "sign": [85, 87], "signal": [44, 45], "signatur": [32, 33, 34, 35, 36, 38, 39, 111], "signif": [51, 53, 54, 55, 56, 57, 96, 97, 110, 111, 126, 140, 143], "signific": [51, 54, 55, 56, 57, 60, 65, 74, 75, 79, 82, 84, 86, 87, 96, 97, 110, 111, 126, 127, 133, 140, 143], "silverman": [34, 35, 36], "sim": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 41, 52, 53, 54, 57, 59, 67, 70, 78, 82, 83, 88, 90, 97], "sim_data": 62, "similar": [10, 14, 53, 56, 61, 68, 69, 74, 77, 80, 84, 85, 86, 87, 97], "similarli": [61, 77, 85], "simpl": [11, 20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 46, 47, 53, 56, 68, 69, 72, 73, 74, 75, 82, 87, 94, 97, 127, 128], "simplest": 95, "simpli": [56, 58, 143], "simplic": [55, 76, 79, 82, 87], "simplif": [127, 129], "simplifi": [60, 75, 81, 87, 95, 127, 136], "simul": [9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 25, 26, 52, 56, 57, 60, 62, 67, 68, 69, 70, 71, 72, 73, 76, 77, 83, 84, 87, 88, 90, 96, 126, 140], "simul_data": 37, "simulaten": [97, 104], "simulation_run": 71, "simult": 53, "simultan": [61, 94, 143], "sin": [11, 14, 18, 59, 68, 69, 72, 73], "sinc": [9, 10, 42, 46, 55, 57, 58, 59, 65, 72, 73, 74, 76, 77, 79, 81, 88, 96, 97, 99, 127, 133, 135, 139, 142], "singl": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 58, 60, 61, 62, 72, 73, 80, 81, 96, 126], "single_learner_pipelin": 96, "singleton": 110, "sinh": 18, "sipp": [55, 79, 80], "site": [60, 61, 62, 78, 79], "situat": [54, 78], "six": [25, 54], "sixth": 78, "size": [21, 24, 37, 52, 54, 55, 56, 59, 60, 61, 62, 67, 70, 71, 74, 76, 77, 79, 81, 82, 83, 85, 87, 89, 91, 93, 96, 97, 98, 110, 111, 126, 140, 143], "sizeabl": 87, "skill": 141, "sklearn": [18, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 45, 46, 47, 60, 61, 62, 63, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 84, 85, 86, 87, 88, 89, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 133, 140, 143], "skotara": 87, "slide": 81, "slight": [97, 101], "slightli": [59, 60, 72, 73, 74, 76, 95, 97, 101, 102, 111, 112, 114, 115, 127, 128], "sligthli": [20, 22, 23], "slow": [52, 67, 90], "slower": [52, 67, 90], "small": [11, 57, 58, 59, 75, 82, 88, 97, 127, 128, 135], "smaller": [55, 58, 72, 73, 74, 77, 79, 84, 87, 97, 143], "smallest": [22, 76], "smpl": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 67, 76, 78, 110, 111], "smpls_cluster": [54, 78], "smucler": 142, "sn": [58, 60, 63, 65, 67, 76, 78, 79, 80, 87, 88], "so": [46, 47, 51, 55, 56, 57, 58, 60, 61, 66, 77, 79, 81, 87, 88, 96, 126, 143], "social": [81, 141], "societi": [54, 78, 87, 141], "softwar": [56, 96, 138, 140, 141, 142], "solari": 142, "sole": [61, 87], "solut": [89, 95, 111], "solv": [27, 54, 78, 95, 96, 97, 101, 126], "solver": [79, 88, 97], "some": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 55, 56, 57, 58, 59, 61, 63, 76, 77, 79, 80, 84, 85, 86, 88, 95, 96, 97, 105, 139, 142], "sometim": [76, 97, 102], "sonabend": [56, 96], "soon": [97, 100, 111, 113, 127, 130], "sophist": 96, "sort": [21, 79, 85, 97], "sort_bi": 21, "sort_valu": 65, "sourc": [56, 96, 140, 142], "sourcefileload": 71, "sp": 53, "space": [54, 78, 96], "spars": [71, 96, 126, 140, 141], "sparsiti": 141, "spec": 141, "special": [54, 78, 94, 97], "specif": [20, 22, 23, 25, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 40, 54, 55, 60, 61, 62, 65, 75, 76, 78, 79, 87, 91, 93, 94, 95, 96, 97, 101, 110, 111, 119, 126, 133, 137, 138, 140], "specifi": [4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 54, 55, 56, 57, 58, 60, 61, 62, 65, 66, 68, 69, 70, 72, 73, 75, 77, 78, 79, 80, 82, 83, 84, 86, 87, 88, 89, 91, 92, 93, 94, 95, 97, 116, 119, 139, 140, 142, 143], "specifii": 80, "speed": [24, 30, 36, 76], "speedup": 76, "spefici": 32, "spindler": [15, 71, 76, 77, 87, 138, 141, 142], "spine": [79, 80], "spline": [68, 69, 95], "spline_basi": [68, 69, 95], "spline_grid": [68, 69], "split": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 54, 56, 57, 58, 60, 61, 62, 75, 76, 78, 80, 82, 86, 88, 94, 95, 96, 97, 99, 101, 111, 126, 140, 142], "split_sampl": [75, 76], "sponsor": [55, 79, 80], "sprintf": 52, "sq_error": 71, "sqrt": [9, 10, 13, 14, 25, 26, 52, 54, 56, 63, 67, 70, 78, 83, 90, 110, 126, 127, 128, 140], "squar": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 55, 71, 79, 96, 97, 127, 137, 141], "squarederror": [55, 79, 143], "squeez": [58, 70, 83, 88], "src": 79, "ssm": [4, 5, 6, 19, 94, 109], "ssrn": 12, "stabil": 74, "stabl": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 138], "stack": [56, 96], "stackingclassifi": 84, "stackingregressor": 84, "stacklrn": 56, "stackrel": 97, "stage": [40, 61, 68, 69, 72, 73, 82, 84, 96, 97, 142, 143], "stagger": [97, 102], "stai": [97, 102], "standard": [24, 26, 53, 56, 60, 61, 62, 70, 72, 73, 84, 85, 97, 98, 101, 110, 111, 126, 127, 133, 137, 142, 143], "standard_norm": [91, 93, 96, 126, 140], "standardscal": 79, "star": 97, "start": [25, 53, 55, 56, 60, 61, 62, 68, 69, 71, 74, 76, 77, 78, 79, 83, 87, 97, 99, 138, 143], "start_dat": 25, "stat": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 67, 84, 91, 93, 96, 97, 126, 138, 141], "stat_bin": 52, "stat_dens": 55, "state": 143, "stationar": 58, "stationari": [97, 99], "statist": [16, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 54, 78, 85, 86, 87, 126, 127, 133, 138, 140, 141, 142, 143], "statsmodel": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 84], "statu": [53, 55, 57, 58, 79, 81, 84, 88, 97, 102], "std": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 65, 66, 68, 69, 70, 72, 73, 74, 75, 77, 78, 79, 80, 81, 83, 86, 87, 88, 95, 96, 97, 98, 99, 101, 110, 111, 126, 140, 143], "stefan": 141, "step": [52, 55, 56, 67, 72, 73, 74, 79, 82, 90, 96, 97, 126, 138, 143], "stepdown": 126, "stick": [55, 79], "still": [57, 58, 68, 69, 72, 73, 74, 80, 84, 86, 88, 96, 97, 101], "stochast": [38, 39, 97, 107, 108, 140], "stock": [55, 79, 80, 85], "store": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 85, 89, 96, 110, 111, 126, 127, 133, 142], "store_model": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 77], "store_predict": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 79, 82], "stori": [87, 141], "str": [4, 5, 6, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 55, 60, 65, 72, 73, 83, 84, 95, 97, 142], "straightforward": [72, 73, 76, 95], "strategi": [81, 87, 97, 143], "stratifi": [75, 76], "stratum": 81, "strength": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 85, 86, 87, 127, 128, 133, 136], "strftime": 60, "strictli": 97, "string": [20, 21, 22, 23, 24, 29, 31, 32, 33, 34, 35, 37, 38, 39, 95, 126, 127, 133, 140, 142], "string_label": 81, "strong": [57, 85, 88, 127, 128], "stronger": [85, 97, 101, 126, 143], "structur": [7, 8, 17, 25, 54, 55, 57, 61, 71, 78, 79, 85, 88, 90, 96, 138, 141, 143], "student": 141, "studi": [19, 54, 55, 71, 76, 77, 78, 79, 80, 85, 86, 97, 98, 140, 143], "style": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 77, 142], "styler": 142, "styliz": 87, "sub": [42, 43, 46, 47, 54, 78], "subclass": [92, 93, 142], "subfold": 96, "subgroup": [32, 55, 79, 142], "subject": [54, 78], "submiss": 142, "submit": 60, "submodul": 142, "subobject": [42, 43, 46, 47], "subplot": [54, 59, 65, 67, 68, 69, 70, 72, 73, 75, 76, 77, 78, 79, 80, 81, 83, 84], "subplots_adjust": 76, "subpopul": [97, 109], "subsampl": [56, 76], "subscript": [97, 101, 111, 115, 127, 128], "subsequ": [54, 78], "subset": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 46, 54, 76, 78, 82, 89, 95, 96, 127, 128], "subseteq": 95, "substanti": [55, 79, 81], "substract": 126, "subtract": 126, "sudo": 139, "suffic": 87, "suffici": [76, 77, 87], "suggest": [54, 55, 78, 79, 87, 142], "suitabl": [57, 68, 69, 88, 97, 101], "sum": [43, 47, 54, 55, 78, 79, 80, 83, 84, 95, 126], "sum_": [25, 41, 52, 54, 67, 78, 84, 89, 90, 95, 97, 98, 102, 126], "sum_i": 81, "sum_oth": 78, "sum_riv": 78, "summar": [21, 53, 60, 61, 62, 65, 81, 87, 89, 127, 133], "summari": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 53, 54, 56, 57, 58, 60, 61, 62, 63, 65, 66, 68, 69, 70, 72, 73, 74, 75, 78, 80, 83, 85, 86, 87, 88, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 110, 111, 126, 127, 140, 142, 143], "summary_df": 85, "summary_result": 55, "suppli": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 68, 69, 72, 73, 74, 82, 95, 127, 128, 133, 134], "support": [11, 32, 40, 53, 54, 60, 61, 62, 76, 78, 82, 84, 93, 96, 97, 105, 143], "support_s": [11, 68, 69, 72, 73, 82], "support_t": 82, "support_w": 82, "suppos": 87, "suppress": [53, 55, 56, 57], "suppresswarn": 52, "suprema": 126, "suptitl": [70, 76, 77, 80, 83], "supxlabel": [70, 80, 83], "supylabel": [70, 80, 83], "sure": [65, 96, 142], "surfac": [68, 69, 71], "surgic": 85, "surpress": [54, 140], "survei": [55, 79, 80, 143], "susan": 141, "sven": [87, 138, 141], "svenk": 78, "svenklaassen": [138, 142], "svg": [52, 67], "switch": [52, 67, 87, 90], "symbol": 87, "symmetr": 18, "syntax": [84, 97], "synthesi": 141, "synthet": [11, 25, 41, 51, 66, 68, 69, 70, 72, 73, 77, 82, 83, 85], "syrgkani": [87, 141], "system": 141, "szita": 141, "t": [4, 5, 6, 9, 10, 14, 20, 21, 23, 24, 25, 26, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 68, 69, 70, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 92, 93, 95, 96, 97, 98, 99, 101, 102, 110, 111, 112, 126, 127, 129, 140, 143], "t_": [24, 60, 61, 62, 97, 101, 111, 115, 127, 132], "t_1_start": 76, "t_1_stop": 76, "t_2_start": 76, "t_2_stop": 76, "t_3_start": 76, "t_3_stop": 76, "t_col": [4, 5, 6, 23, 24, 60, 61, 62, 92, 93, 97, 98, 99, 101], "t_df": 82, "t_diff": 59, "t_dml": 52, "t_g": 25, "t_i": [58, 82, 84, 97, 99], "t_idx": 59, "t_nonorth": 52, "t_orth_nosplit": 52, "t_sigmoid": 82, "t_stat": 126, "t_value_ev": 22, "t_value_pr": 22, "tabl": [52, 54, 55, 56, 57, 65, 85, 89, 91, 93, 96, 97, 110, 111, 126, 140, 143], "tabular": [76, 91, 93, 126, 140, 143], "taddi": 141, "takatsu": 85, "take": [9, 10, 11, 32, 33, 38, 39, 57, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 76, 80, 83, 84, 85, 86, 88, 89, 95, 96, 97, 98, 102, 105, 106, 107, 108, 111, 116, 119, 127, 134, 135, 136, 140], "taken": [55, 79, 80, 143], "taker": [32, 142], "talk": 143, "target": [20, 22, 23, 27, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 46, 47, 51, 54, 55, 56, 57, 68, 69, 76, 78, 95, 96, 97, 99, 101, 102, 103, 104, 105, 106, 109, 110, 111, 120, 121, 126, 127, 135, 137, 138, 140, 142, 143], "task": [51, 77, 91, 93, 110, 143], "task_typ": 142, "tau": [41, 59, 70, 80, 81, 83, 84, 95, 97, 111, 117, 120, 121], "tau_": [81, 84, 97], "tau_0": [84, 97], "tau_1": 81, "tau_2": 81, "tau_vec": [70, 80, 83], "tax": [55, 79, 80], "te": [53, 68, 69, 82], "techniqu": [52, 67, 90, 110, 143], "teen": 61, "templat": 142, "ten": 77, "tend": [55, 79, 80, 97], "tensor": [68, 69], "tenth": 141, "term": [6, 22, 52, 54, 55, 56, 59, 67, 71, 78, 79, 81, 87, 90, 97, 102, 138, 143], "termin": [56, 96], "terminatorev": 56, "test": [8, 12, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 60, 61, 62, 67, 74, 78, 85, 87, 90, 96, 97, 110, 111, 126, 140, 141, 142, 143], "test_id": [54, 110], "test_ind": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "test_set": 110, "test_siz": 67, "text": [9, 10, 12, 14, 24, 25, 26, 40, 41, 54, 55, 60, 61, 62, 70, 71, 81, 82, 83, 84, 87, 95, 97, 101, 102, 110, 111, 115, 127, 132], "textbf": [89, 96, 143], "textposit": 87, "textrm": [127, 128, 134, 135, 136, 137], "tg": [56, 63, 91, 93, 140], "th": [54, 78], "than": [33, 52, 53, 55, 67, 71, 75, 76, 79, 80, 81, 84, 85, 86, 87, 90, 97, 101, 127, 133, 143], "thank": [53, 55, 56, 79, 142], "thatw": 59, "thei": [53, 55, 59, 72, 73, 79, 81, 85, 97, 127, 137], "them": [21, 55, 56, 68, 69, 70, 74, 77, 79, 83, 97], "theme": [54, 55], "theme_minim": [52, 55, 57], "theorem": [97, 102, 127, 137], "theoret": [76, 87, 110, 141], "theori": [95, 141], "therebi": [54, 56, 78, 143], "therefor": [62, 65, 81, 84, 86, 110, 111, 127, 136], "theta": [9, 10, 12, 13, 14, 16, 18, 19, 20, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 56, 57, 58, 59, 60, 65, 67, 71, 74, 75, 76, 78, 84, 86, 87, 88, 89, 90, 91, 93, 95, 96, 97, 98, 101, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 133, 136, 137, 140, 143], "theta_": [25, 60, 65, 84, 87, 95, 97, 103, 104, 126, 127, 137], "theta_0": [11, 32, 33, 38, 39, 52, 54, 55, 57, 65, 67, 68, 69, 71, 72, 73, 78, 79, 87, 88, 90, 95, 97, 99, 105, 106, 107, 108, 109, 111, 120, 121, 123, 126, 127, 134, 135, 137, 140], "theta_dml": [52, 67, 90], "theta_dml_po": [52, 67, 90], "theta_initi": 67, "theta_nonorth": [52, 67], "theta_orth_nosplit": [52, 67], "theta_orth_po_nosplit": [52, 67], "theta_resc": 52, "theta_t": 59, "thi": [9, 10, 20, 21, 22, 23, 24, 25, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 95, 96, 97, 101, 102, 103, 104, 105, 106, 110, 111, 112, 114, 115, 117, 119, 120, 121, 126, 127, 128, 133, 134, 135, 138, 139, 140, 141, 142, 143], "think": 56, "third": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 61, 62, 67, 78, 90, 110], "thirion": [138, 140], "this_df": [71, 79], "this_split_ind": 78, "those": [53, 55, 61, 79, 80, 85], "though": [51, 66, 81], "thread": [81, 96], "three": [54, 56, 72, 73, 139, 142], "threshold": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 84, 87, 97], "through": [53, 61, 70, 72, 73, 83, 84, 96, 97], "throughout": [74, 85], "thu": [77, 84, 95, 97], "tibbl": 53, "tick": 24, "tick_param": 84, "tight": 67, "tight_layout": [60, 77, 78, 84], "tighter": 84, "tild": [9, 10, 14, 25, 26, 54, 78, 81, 89, 95, 110, 111, 115, 120, 121, 124, 125, 126, 127, 136, 137], "tile": 85, "time": [4, 5, 6, 15, 16, 20, 22, 24, 25, 52, 53, 54, 55, 57, 58, 59, 61, 67, 71, 72, 73, 78, 79, 80, 84, 86, 87, 88, 92, 93, 97, 98, 99, 101, 102, 111, 127, 141, 142, 143], "time_budget": 77, "time_df": 59, "time_period": 59, "time_typ": [25, 60], "titiunik": [97, 141], "titl": [21, 24, 54, 55, 57, 60, 61, 65, 68, 69, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 83, 84, 87, 138], "title_fonts": 60, "tmp": 73, "tname": 53, "tnr": [56, 96], "to_datetim": 60, "to_fram": 82, "to_numpi": [70, 74, 80, 83], "todo": [54, 63], "toeplitz": 71, "togeth": [72, 73, 126], "toler": 78, "tomasz": [141, 142], "toml": 142, "too": 76, "tool": [53, 56, 86, 143], "top": [54, 76, 78, 79, 80, 84, 87, 97, 138], "total": [43, 47, 55, 77, 79], "tpot": 77, "tracker": 138, "tradit": 126, "train": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 46, 47, 52, 54, 56, 67, 68, 69, 70, 72, 73, 75, 76, 78, 79, 82, 83, 89, 90, 110], "train_id": [54, 110], "train_ind": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "train_set": 110, "train_test_split": 67, "transact": 141, "transform": [9, 10, 41, 75, 81, 87, 143], "translat": 71, "transpos": 59, "treament": 82, "treat": [24, 25, 26, 33, 53, 58, 59, 60, 61, 62, 65, 74, 82, 84, 87, 95, 97, 99, 101, 102, 106, 111, 115, 126, 143], "treat1_param": 81, "treat2_param": 81, "treat_var": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 96], "treatment": [4, 5, 6, 7, 8, 9, 10, 11, 14, 17, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 71, 74, 76, 77, 78, 82, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 96, 98, 99, 101, 102, 103, 104, 105, 106, 109, 110, 112, 114, 115, 116, 117, 119, 120, 121, 126, 132, 133, 134, 136, 138, 140, 141, 142, 143], "treatment_df": 59, "treatment_effect": [11, 68, 69], "treatment_level": [29, 30, 65, 75, 97], "treatment_var": [4, 5, 6], "tree": [33, 45, 55, 56, 58, 59, 60, 76, 79, 89, 94, 96, 97, 110, 111, 126, 140, 142], "tree_param": [33, 45], "tree_summari": 79, "trees_class": [55, 79], "trend": [53, 58, 59, 60, 78, 97, 99, 101, 102, 141], "tri": [71, 127, 128], "triangular": [40, 84, 97], "trim": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 55, 79, 80, 87], "trimming_rul": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 80], "trimming_threshold": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 55, 68, 75, 79, 80, 82, 83, 87], "trm": [56, 96], "true": [4, 5, 6, 7, 8, 9, 14, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 93, 96, 97, 99, 110, 111, 116, 117, 120, 121, 124, 125, 126, 127, 129, 131, 132, 137, 140, 143], "true_effect": [59, 68, 69, 72, 73, 85], "true_gatet_effect": 74, "true_group_effect": 74, "true_tau": 84, "truemfunct": 85, "truncat": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 80], "try": [76, 86], "tune": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 71, 76, 84, 94, 97, 138, 140, 142], "tune_on_fold": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 96], "tune_r": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "tune_set": [56, 96], "tuned_model": 77, "tuner": 96, "tunergridsearch": 56, "tupl": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 60, 97, 101], "turn": 87, "turrel": 18, "tutori": 55, "tw": [79, 80], "twice": 97, "twinx": 65, "two": [11, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 55, 56, 58, 60, 61, 62, 66, 67, 70, 75, 76, 77, 79, 80, 81, 82, 83, 85, 86, 87, 89, 90, 95, 96, 99, 101, 110, 120, 126, 143], "twoclass": 56, "twoearn": [55, 79, 80, 86, 143], "type": [6, 9, 10, 11, 14, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 55, 56, 60, 67, 76, 77, 78, 84, 87, 90, 94, 96, 97, 111, 122, 123, 126, 127, 136, 142, 143], "typeerror": [60, 61, 62], "typic": [73, 97, 102, 138], "u": [9, 10, 11, 13, 19, 26, 32, 33, 34, 35, 36, 43, 47, 52, 53, 54, 55, 58, 59, 60, 61, 65, 67, 70, 72, 73, 75, 76, 78, 79, 80, 82, 83, 85, 86, 87, 90, 97, 103, 105, 106, 127, 128, 139, 143], "u_hat": [52, 67, 111], "u_i": [12, 15, 18, 19], "u_t": 26, "uehara": 141, "uhash": 56, "ulf": 141, "unambigu": 87, "uncertainti": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 72, 73, 75, 84, 86, 127, 133, 143], "unchang": [42, 43, 46, 47], "uncondit": [55, 60, 79, 143], "unconfounded": [87, 141], "under": [32, 37, 52, 55, 58, 61, 67, 79, 82, 84, 87, 90, 97, 101, 102, 109, 126, 141], "underbrac": [52, 59, 67, 90, 95], "underfit": 77, "underli": [9, 14, 55, 56, 60, 65, 72, 73, 81, 82, 97, 102, 127, 128, 143], "underlin": [54, 78], "underset": [84, 97], "understand": [60, 61, 87], "undesir": 96, "unevenli": 110, "uniform": [26, 40, 41, 59, 66, 68, 69, 70, 82, 83, 126], "uniform_averag": [43, 47], "uniformli": [32, 60, 70, 80, 126], "union": 32, "uniqu": [6, 51, 60, 61, 62, 65, 66, 76, 84, 92, 93, 111, 127, 137], "unique_label": 77, "unit": [6, 25, 52, 53, 57, 58, 59, 60, 61, 62, 74, 84, 88, 92, 93, 97, 99, 101, 102, 111, 112, 114, 115, 127, 132, 142], "univari": [11, 68, 69], "univers": 141, "unknown": 97, "unlik": [55, 79, 80, 87], "unobserv": [9, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 51, 55, 60, 66, 79, 80, 86, 87, 97, 127, 128, 137, 143], "unpen": 53, "unstabl": [127, 128], "unter": [54, 55, 56], "untest": 87, "until": [97, 99, 142], "untreat": [87, 97, 99], "up": [24, 30, 36, 55, 71, 76, 77, 79, 80, 86, 87, 96, 97, 99, 110, 127, 128, 139, 142, 143], "upcom": 142, "updat": [42, 43, 46, 47, 54, 73, 78, 79, 141, 142], "update_layout": [68, 69, 71, 84, 87], "update_trac": [68, 69], "upload": 142, "upon": [111, 142], "upper": [32, 55, 56, 59, 60, 65, 67, 70, 74, 75, 80, 83, 84, 86, 87, 96, 127, 133, 137, 143], "upper_bound": [68, 69], "upsilon": [57, 88], "upsilon_i": [57, 88], "upward": [55, 79, 80, 87], "upweight": 81, "url": [61, 71, 138, 141], "us": [4, 5, 6, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 52, 54, 55, 57, 58, 59, 60, 61, 62, 65, 67, 68, 69, 70, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 93, 95, 97, 101, 110, 111, 112, 114, 115, 126, 127, 128, 133, 135, 136, 137, 138, 139, 140, 142, 143], "usa": 141, "usabl": 76, "usag": [53, 58, 60, 61, 62, 63, 65, 74, 78, 79, 80, 86, 88, 91, 92, 93, 140, 142], "use_label_encod": [79, 143], "use_other_treat_as_covari": [4, 5, 6, 91, 93], "use_pred_offset": 96, "usecolormap": [68, 69], "user": [27, 28, 42, 43, 46, 47, 52, 53, 54, 55, 56, 61, 65, 67, 74, 75, 76, 78, 79, 84, 86, 95, 96, 97, 111, 126, 138, 139, 140, 142, 143], "user_guid": 73, "userwarn": [60, 62, 79, 87], "usual": [54, 58, 60, 61, 62, 68, 69, 76, 78, 84, 86, 87, 95, 96, 110, 127, 137], "util": [0, 28, 75, 76, 77, 81, 84, 96, 97, 142], "v": [7, 8, 13, 15, 16, 17, 19, 32, 33, 38, 39, 43, 47, 52, 54, 55, 60, 65, 67, 74, 75, 76, 77, 78, 79, 81, 84, 85, 89, 90, 95, 97, 103, 105, 106, 107, 108, 126, 138, 140, 141, 142, 143], "v108": 138, "v12": [138, 140], "v22": 56, "v23": 138, "v_": [16, 54, 78, 97], "v_i": [12, 13, 17, 18, 19, 52, 67, 90, 97], "v_j": 126, "val": [13, 60, 61, 62, 110, 141], "val_list": 71, "valid": [4, 5, 6, 12, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 53, 54, 55, 58, 61, 67, 70, 76, 77, 78, 79, 80, 83, 90, 94, 95, 96, 110, 111, 117, 120, 121, 127, 128, 141, 143], "valu": [4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 46, 47, 48, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 65, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 84, 85, 86, 87, 89, 92, 93, 94, 96, 97, 98, 102, 103, 109, 110, 117, 120, 121, 124, 125, 126, 127, 128, 133, 137, 140, 142, 143], "value_count": 79, "van": 141, "vanderpla": [138, 140], "vanish": [52, 67, 90], "var": [9, 10, 14, 25, 26, 54, 78, 81, 84, 127, 128, 134, 135, 136, 137], "var_ep": 87, "varepsilon": [9, 10, 16, 32, 54, 57, 78, 88, 95, 97, 105], "varepsilon_": [16, 25, 54, 60, 78], "varepsilon_0": 26, "varepsilon_1": 26, "varepsilon_d": [10, 14], "varepsilon_i": [14, 15, 57, 70, 83, 88], "vari": [25, 55, 59, 60, 76, 79, 81, 87], "variabl": [4, 5, 6, 7, 10, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 54, 55, 56, 57, 58, 60, 61, 62, 63, 65, 71, 74, 77, 78, 79, 80, 84, 86, 87, 88, 91, 92, 93, 95, 96, 97, 99, 101, 102, 103, 105, 106, 107, 108, 110, 111, 126, 127, 128, 133, 137, 140, 141, 142, 143], "varianc": [27, 28, 54, 56, 78, 84, 86, 87, 94, 97, 110, 127, 128, 133, 135, 136, 137, 140, 142], "variant": [53, 61, 75], "variat": [10, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 75, 86, 127, 128, 137], "variou": [53, 77, 87, 96, 143], "varoquaux": [138, 140], "vasili": [87, 141], "vector": [11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 25, 29, 31, 32, 33, 34, 35, 37, 38, 39, 51, 54, 55, 57, 58, 66, 72, 73, 74, 78, 79, 82, 85, 88, 97, 99, 107, 108, 109, 126, 140, 142], "vee": [111, 115, 127, 132], "venv": 139, "verbos": [55, 59, 60, 67, 76, 77, 84, 87], "veri": [53, 54, 56, 61, 74, 76, 78, 85, 87, 111, 138], "verifi": 81, "versa": [76, 81, 127, 133], "version": [9, 42, 43, 46, 47, 54, 55, 56, 58, 59, 87, 89, 95, 97, 101, 111, 126, 127, 129, 131, 132, 134, 135, 142], "versoin": 87, "versu": 73, "vertic": [54, 65, 78], "via": [9, 10, 20, 22, 23, 26, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 53, 57, 58, 59, 60, 61, 62, 70, 71, 72, 73, 74, 75, 76, 84, 86, 88, 89, 91, 93, 94, 95, 96, 97, 98, 99, 101, 110, 117, 125, 126, 127, 128, 133, 137, 138, 139, 140, 141, 142, 143], "viabl": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "vice": [76, 81, 127, 133], "victor": [71, 87, 110, 138, 141], "view": 73, "vignett": [53, 142], "villa": [51, 66], "violat": 60, "violet": [70, 80, 83], "vira": 141, "virtual": [61, 139], "virtualenv": 139, "visibl": [80, 84, 87], "visit": [138, 143], "visual": [21, 54, 60, 61, 62, 74, 75, 77, 78, 84], "vol": 53, "volum": [87, 138], "voluntari": 81, "vv740": 78, "vv760g": 78, "w": [7, 8, 9, 10, 17, 25, 26, 27, 28, 42, 43, 46, 47, 54, 71, 78, 81, 82, 85, 89, 90, 97, 101, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 131, 132, 133, 134, 135, 136, 137, 138, 140], "w24678": 110, "w30302": 141, "w_": [25, 26, 54, 78, 82, 97], "w_1": [25, 26, 82], "w_2": [25, 26, 82], "w_3": [25, 26], "w_4": [25, 26], "w_df": 82, "w_i": [19, 58, 82, 84, 89, 95, 97, 110, 111, 115, 126], "wa": [54, 59, 77, 78, 87, 142], "wage": 61, "wager": 141, "wai": [55, 76, 77, 79, 85, 87, 96, 111, 139], "wander": 18, "wang": 141, "want": [51, 54, 55, 56, 58, 66, 70, 76, 78, 83, 84, 96, 97, 138, 139, 141], "warn": [21, 24, 51, 52, 53, 54, 55, 56, 57, 60, 62, 67, 79, 87, 89, 96, 97, 110, 111, 126, 140, 142], "wayon": 54, "we": [33, 45, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 95, 96, 97, 98, 101, 102, 106, 110, 111, 115, 118, 126, 127, 128, 137, 139, 140, 142, 143], "weak": [32, 127, 128, 141, 142], "weakest": 60, "wealth": [7, 86], "websit": [55, 56, 61, 96, 138], "wedg": [54, 78], "week": 142, "wei": 126, "weight": [21, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 42, 43, 46, 47, 54, 55, 56, 57, 60, 61, 62, 65, 74, 75, 78, 79, 84, 88, 94, 96, 97, 98, 111, 116, 119, 126, 127, 134, 135, 142], "weights_bar": [29, 33, 75], "weights_dict": 75, "weiss": [138, 140], "well": [4, 5, 6, 46, 47, 52, 54, 67, 71, 76, 77, 78, 85, 89, 90, 93, 110, 139, 140], "were": [55, 57, 79, 80, 88, 143], "what": [53, 71, 76, 141], "when": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 55, 58, 61, 73, 75, 79, 81, 85, 97, 106, 109, 111, 126, 138, 139, 140, 142], "whenev": [55, 79], "whera": [127, 135], "where": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 43, 44, 45, 47, 51, 52, 54, 55, 57, 58, 59, 60, 61, 62, 65, 66, 67, 70, 74, 78, 79, 81, 82, 83, 84, 85, 87, 88, 89, 90, 95, 96, 97, 98, 99, 101, 102, 103, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 133, 134, 135, 137, 139, 140, 142, 143], "wherea": [11, 57, 58, 60, 65, 85, 87, 88, 97, 101, 111, 119, 127, 134, 143], "whether": [4, 5, 6, 11, 14, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 55, 59, 76, 79, 80, 84, 85, 87, 91, 93, 96, 97, 127, 128, 142], "which": [4, 5, 6, 9, 11, 20, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 46, 51, 52, 53, 55, 56, 57, 58, 60, 61, 62, 64, 65, 66, 67, 71, 73, 74, 76, 77, 79, 80, 82, 84, 86, 87, 88, 90, 91, 93, 95, 96, 97, 101, 111, 126, 127, 128, 133, 134, 135, 137, 139, 142, 143], "while": [51, 66, 97], "white": [54, 72, 73, 78, 87], "whitegrid": [79, 80], "whitnei": [87, 141], "who": [53, 55, 79, 87], "whole": [52, 58, 67, 84, 90, 96, 127, 128], "whom": 97, "why": 61, "widehat": [60, 61, 62, 97], "width": [21, 24, 52, 54, 68, 69, 71], "wiki": 142, "wiksel": 141, "wild": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 126], "window": 139, "wise": [72, 73], "wish": 139, "within": [40, 54, 60, 61, 62, 72, 73, 78, 82, 84], "without": [14, 20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 40, 51, 52, 60, 61, 62, 66, 67, 76, 77, 87, 90, 94, 96, 97, 127, 128, 139, 142], "wolf": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 126], "won": 87, "word": [40, 84, 97, 142, 143], "work": [42, 43, 46, 47, 60, 61, 62, 64, 65, 73, 74, 76, 81, 86, 87, 96, 97, 126, 139, 141], "workflow": [138, 142], "workspac": 79, "world": 141, "worri": 87, "wors": [43, 47], "would": [43, 47, 53, 55, 56, 60, 61, 62, 68, 69, 71, 76, 79, 80, 84, 86, 87, 95, 96, 127, 137, 143], "wrapper": [53, 84, 96], "wright": 85, "write": [52, 53, 57, 58, 67, 73, 88, 90, 127, 137], "written": [97, 111, 127, 134, 135], "wrong": [76, 81], "wspace": 76, "wurd": [54, 55, 56], "www": [138, 139], "x": [4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 31, 32, 33, 35, 37, 38, 39, 40, 41, 42, 43, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 93, 95, 96, 97, 99, 101, 103, 104, 105, 106, 107, 108, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 131, 132, 134, 135, 136, 137, 140, 143], "x0": [65, 81, 84], "x1": [54, 56, 57, 58, 65, 75, 77, 78, 81, 84, 86, 87, 88, 91, 93, 95, 96, 97, 111, 126, 127, 128, 140], "x10": [54, 56, 57, 75, 77, 78, 88, 91, 93, 96, 97, 111, 126, 140], "x100": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x11": [54, 56, 57, 78, 88, 91, 93, 96, 97, 111, 126, 140], "x12": [54, 56, 57, 78, 88, 91, 93, 96, 97, 111, 126, 140], "x13": [54, 56, 57, 78, 88, 91, 93, 96, 97, 111, 126, 140], "x14": [54, 56, 57, 78, 88, 91, 93, 96, 97, 111, 126, 140], "x15": [54, 56, 57, 78, 88, 91, 93, 96, 97, 111, 126, 140], "x16": [54, 56, 57, 78, 88, 91, 93, 96, 97, 111, 126, 140], "x17": [54, 56, 57, 78, 88, 91, 93, 96, 97, 111, 126, 140], "x18": [54, 56, 57, 78, 88, 91, 93, 96, 97, 111, 126, 140], "x19": [54, 56, 57, 78, 88, 91, 93, 96, 97, 111, 126, 140], "x1x2x3x4x5x6x7x8x9x10": 54, "x2": [54, 56, 57, 58, 65, 75, 77, 78, 84, 86, 87, 88, 91, 93, 95, 96, 97, 111, 126, 140], "x20": [54, 56, 57, 78, 88, 91, 93, 96, 97, 111, 126, 140], "x21": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x22": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x23": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x24": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x25": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x26": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x27": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x28": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x29": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x2_dummi": 87, "x2_preds_control": 87, "x2_preds_treat": 87, "x3": [54, 56, 57, 58, 65, 75, 77, 78, 86, 87, 88, 91, 93, 95, 96, 97, 111, 126, 140], "x30": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x31": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x32": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x33": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x34": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x35": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x36": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x37": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x38": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x39": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x4": [54, 56, 57, 58, 65, 75, 77, 78, 86, 87, 88, 91, 93, 96, 97, 111, 126, 140], "x40": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x41": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x42": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x43": [54, 56, 57, 77, 78, 88, 91, 93, 97, 140], "x44": [54, 56, 57, 77, 78, 88, 91, 93, 97, 140], "x45": [54, 56, 57, 77, 78, 88, 91, 93, 97, 140], "x46": [54, 56, 57, 77, 78, 88, 91, 93, 97, 140], "x47": [54, 56, 57, 77, 78, 88, 91, 93, 97, 140], "x48": [54, 56, 57, 77, 78, 88, 91, 93, 97, 140], "x49": [54, 56, 57, 77, 78, 88, 91, 93, 97, 140], "x5": [54, 56, 57, 75, 77, 78, 87, 88, 91, 93, 96, 97, 111, 126, 140], "x50": [54, 56, 57, 77, 78, 88, 91, 93, 97, 140], "x51": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x52": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x53": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x54": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x55": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x56": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x57": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x58": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x59": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x6": [54, 56, 57, 75, 77, 78, 88, 91, 93, 96, 97, 111, 126, 140], "x60": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x61": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x62": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x63": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x64": [54, 56, 57, 60, 61, 62, 78, 79, 88, 91, 93, 97, 140], "x65": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x66": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x67": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x68": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x69": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x7": [54, 56, 57, 75, 77, 78, 88, 91, 93, 96, 97, 111, 126, 140], "x70": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x71": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x72": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x73": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x74": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x75": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x76": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x77": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x78": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x79": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x8": [54, 56, 57, 75, 77, 78, 88, 91, 93, 96, 97, 111, 126, 140], "x80": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x81": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x82": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x83": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x84": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x85": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x86": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x87": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x88": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x89": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x9": [54, 56, 57, 75, 77, 78, 88, 91, 93, 96, 97, 111, 126, 140], "x90": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x91": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x92": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x93": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x94": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x95": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x96": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x96x97x98x99x100ydcluster_var_icluster_var_jz": 54, "x97": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x98": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x99": [54, 56, 57, 78, 88, 91, 93, 97, 140], "x_": [16, 17, 25, 52, 54, 59, 67, 78, 87, 90], "x_0": [59, 68, 69, 72, 73, 74], "x_1": [9, 10, 14, 25, 26, 38, 39, 59, 68, 69, 70, 72, 73, 74, 83, 87, 97, 107, 108, 127, 128, 140], "x_1x_3": [70, 83], "x_2": [9, 10, 14, 25, 26, 59, 68, 69, 70, 72, 73, 74, 83, 87, 127, 128], "x_3": [9, 10, 14, 25, 26, 59, 68, 69, 72, 73, 74, 127, 128], "x_4": [9, 10, 14, 25, 26, 68, 69, 70, 72, 73, 74, 83], "x_5": [9, 10, 14, 68, 69, 72, 73], "x_6": [68, 69, 72, 73], "x_7": [68, 69, 72, 73], "x_8": [68, 69, 72, 73], "x_9": [68, 69, 72, 73], "x_binary_control": 87, "x_binary_tr": 87, "x_col": [4, 5, 6, 24, 51, 54, 55, 56, 60, 61, 62, 66, 71, 78, 79, 80, 82, 84, 85, 86, 87, 91, 92, 93, 96, 97, 98, 101, 140, 142, 143], "x_cols_bench": 87, "x_cols_binari": 87, "x_cols_poli": 78, "x_conf": 83, "x_conf_tru": 83, "x_df": 59, "x_domain": 56, "x_i": [11, 12, 13, 15, 17, 18, 19, 41, 52, 57, 58, 67, 70, 72, 73, 81, 83, 84, 88, 90, 95, 97, 99, 101, 102, 109, 111, 115], "x_p": [38, 39, 97, 107, 108, 140], "x_train": 77, "x_true": [70, 83], "x_var": 56, "xaxis_titl": [68, 69, 71, 84, 87], "xformla": 53, "xgb": 77, "xgb_untuned_l": 77, "xgb_untuned_m": 77, "xgbclassifi": [76, 79, 81, 143], "xgboost": [52, 55, 76, 79, 81, 143], "xgbregressor": [76, 77, 79, 81, 143], "xi": [14, 25, 26, 97], "xi_": 126, "xi_0": [16, 54, 78], "xi_i": [57, 88], "xiaoji": 141, "xintercept": [52, 57], "xlab": [52, 54, 55], "xlabel": [59, 60, 61, 65, 68, 69, 70, 72, 73, 77, 79, 80, 83], "xlim": [52, 55], "xtick": [65, 77], "xval": [56, 96], "xx": 67, "y": [4, 5, 6, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 101, 103, 105, 106, 107, 108, 110, 111, 112, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 133, 134, 135, 136, 137, 140, 143], "y0": [53, 60, 65, 70, 83], "y0_cvar": 70, "y0_quant": [70, 83], "y1": [53, 60, 70, 83], "y1_cvar": 70, "y1_quant": [70, 83], "y_": [16, 24, 25, 54, 57, 58, 59, 60, 78, 88, 97, 99, 101, 102, 109, 111, 115], "y_0": [20, 22, 26, 41, 111, 114], "y_1": [20, 22, 26, 41, 111, 114], "y_col": [4, 5, 6, 24, 51, 52, 54, 55, 56, 57, 60, 61, 62, 66, 68, 69, 71, 72, 73, 75, 78, 79, 80, 82, 84, 85, 86, 89, 90, 91, 92, 93, 96, 97, 98, 101, 110, 111, 140, 142, 143], "y_df": [59, 82], "y_diff": 59, "y_i": [11, 12, 13, 15, 17, 18, 19, 52, 57, 58, 67, 70, 81, 82, 83, 84, 88, 90, 97, 99, 109], "y_label": [21, 24], "y_lower_quantil": 60, "y_mean": 60, "y_pred": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 76, 96], "y_train": 77, "y_true": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 76, 96], "y_upper_quantil": 60, "ya": 141, "yasui": 141, "yata": 141, "yaxis_titl": [68, 69, 71, 84, 87], "year": [61, 138], "yerr": [59, 65, 72, 73, 77, 79, 81, 84], "yet": [54, 60, 62, 64, 97, 101, 102], "yggvpl": 78, "yield": 97, "yintercept": 55, "ylab": [52, 54, 55], "ylabel": [59, 60, 61, 65, 68, 69, 70, 72, 73, 77, 79, 80, 83], "ylim": 79, "ymax": 55, "ymin": 55, "yname": 53, "york": 141, "you": [42, 43, 46, 47, 51, 52, 59, 60, 61, 62, 66, 73, 78, 86, 97, 138, 139, 143], "your": [76, 139], "ython": 138, "yukun": 141, "yusuk": 141, "yuya": 141, "yy": 67, "z": [4, 5, 6, 9, 10, 12, 14, 15, 16, 19, 25, 26, 32, 34, 37, 38, 51, 54, 55, 57, 60, 66, 68, 69, 71, 78, 79, 83, 85, 87, 88, 95, 97, 105, 107, 111, 118, 120, 122, 125, 126, 142], "z1": [6, 24, 38, 60, 92, 93, 97, 98, 99, 101], "z2": [6, 24, 60, 92, 93, 97, 98, 99, 101], "z3": [6, 24, 60, 92, 93, 97, 98, 99, 101], "z4": [6, 24, 60, 92, 93, 97, 98, 99, 101], "z_": [16, 54, 78], "z_1": [9, 10, 14, 60], "z_2": [9, 10, 14], "z_3": [9, 10, 14], "z_4": [9, 10, 14, 60], "z_5": 9, "z_col": [4, 5, 6, 32, 34, 38, 51, 54, 55, 57, 66, 78, 79, 80, 85, 88, 91, 93, 95, 97, 142], "z_i": [15, 19, 57, 83, 88, 97], "z_j": [9, 10, 14, 25, 26], "z_true": 83, "zadik": 141, "zaxis_titl": [68, 69, 71], "zero": [22, 26, 41, 58, 59, 60, 70, 75, 76, 82, 83, 86, 87, 97, 111, 115, 126, 127, 132], "zeros_lik": 83, "zeta": [32, 38, 39, 55, 79, 95, 97, 105, 107, 108, 140], "zeta_": [16, 54, 78], "zeta_0": [16, 54, 78], "zeta_i": [13, 15, 17, 52, 67, 90], "zeta_j": 126, "zhang": 141, "zhao": [9, 10, 14, 20, 22, 23, 26, 53, 58, 60, 97, 99, 102, 141], "zimmert": [58, 97, 102, 141], "zip": [68, 69], "zorder": 65, "\u03c4_x0": 81, "\u03c4_x1": 81, "\u2139": 52}, "titles": ["API Reference", "<span class=\"section-number\">1. </span>DoubleML Data Class", "<span class=\"section-number\">3. </span>Datasets", "<span class=\"section-number\">2. </span>DoubleML Models", "<span class=\"section-number\">1.2. </span>doubleml.data.DoubleMLClusterData", "<span class=\"section-number\">1.1. </span>doubleml.data.DoubleMLData", "<span class=\"section-number\">1.3. </span>doubleml.data.DoubleMLPanelData", "<span class=\"section-number\">3.1.1. </span>doubleml.datasets.fetch_401K", "<span class=\"section-number\">3.1.2. </span>doubleml.datasets.fetch_bonus", "<span class=\"section-number\">3.2.9. </span>doubleml.datasets.make_confounded_irm_data", "<span class=\"section-number\">3.2.8. </span>doubleml.datasets.make_confounded_plr_data", "<span class=\"section-number\">3.2.10. </span>doubleml.datasets.make_heterogeneous_data", "<span class=\"section-number\">3.2.4. </span>doubleml.datasets.make_iivm_data", "<span class=\"section-number\">3.2.3. </span>doubleml.datasets.make_irm_data", "<span class=\"section-number\">3.2.11. </span>doubleml.datasets.make_irm_data_discrete_treatments", "<span class=\"section-number\">3.2.2. </span>doubleml.datasets.make_pliv_CHS2015", "<span class=\"section-number\">3.2.6. </span>doubleml.datasets.make_pliv_multiway_cluster_CKMS2021", "<span class=\"section-number\">3.2.1. </span>doubleml.datasets.make_plr_CCDDHNR2018", "<span class=\"section-number\">3.2.5. </span>doubleml.datasets.make_plr_turrell2018", "<span class=\"section-number\">3.2.7. </span>doubleml.datasets.make_ssm_data", "<span class=\"section-number\">2.3.4. </span>doubleml.did.DoubleMLDID", "<span class=\"section-number\">2.3.2. </span>doubleml.did.DoubleMLDIDAggregation", "<span class=\"section-number\">2.3.3. </span>doubleml.did.DoubleMLDIDBinary", "<span class=\"section-number\">2.3.5. </span>doubleml.did.DoubleMLDIDCS", "<span class=\"section-number\">2.3.1. </span>doubleml.did.DoubleMLDIDMulti", "<span class=\"section-number\">3.2.13. </span>doubleml.did.datasets.make_did_CS2021", "<span class=\"section-number\">3.2.12. </span>doubleml.did.datasets.make_did_SZ2020", "<span class=\"section-number\">5.1. </span>doubleml.double_ml_score_mixins.LinearScoreMixin", "<span class=\"section-number\">5.2. </span>doubleml.double_ml_score_mixins.NonLinearScoreMixin", "<span class=\"section-number\">2.2.2. </span>doubleml.irm.DoubleMLAPO", "<span class=\"section-number\">2.2.3. </span>doubleml.irm.DoubleMLAPOS", "<span class=\"section-number\">2.2.7. </span>doubleml.irm.DoubleMLCVAR", "<span class=\"section-number\">2.2.4. </span>doubleml.irm.DoubleMLIIVM", "<span class=\"section-number\">2.2.1. </span>doubleml.irm.DoubleMLIRM", "<span class=\"section-number\">2.2.6. </span>doubleml.irm.DoubleMLLPQ", "<span class=\"section-number\">2.2.5. </span>doubleml.irm.DoubleMLPQ", "<span class=\"section-number\">2.2.8. </span>doubleml.irm.DoubleMLQTE", "<span class=\"section-number\">2.2.9. </span>doubleml.irm.DoubleMLSSM", "<span class=\"section-number\">2.1.2. </span>doubleml.plm.DoubleMLPLIV", "<span class=\"section-number\">2.1.1. </span>doubleml.plm.DoubleMLPLR", "<span class=\"section-number\">2.4.1. </span>doubleml.rdd.RDFlex", "<span class=\"section-number\">3.2.14. </span>doubleml.rdd.datasets.make_simple_rdd_data", "<span class=\"section-number\">4.1.2. </span>doubleml.utils.DMLDummyClassifier", "<span class=\"section-number\">4.1.1. </span>doubleml.utils.DMLDummyRegressor", "<span class=\"section-number\">4.1.3. </span>doubleml.utils.DoubleMLBLP", "<span class=\"section-number\">4.1.4. </span>doubleml.utils.DoubleMLPolicyTree", "<span class=\"section-number\">4.1.6. </span>doubleml.utils.GlobalClassifier", "<span class=\"section-number\">4.1.5. </span>doubleml.utils.GlobalRegressor", "<span class=\"section-number\">4.2.1. </span>doubleml.utils.gain_statistics", "<span class=\"section-number\">5. </span>Score Mixin Classes for DoubleML Models", "<span class=\"section-number\">4. </span>Utility Classes and Functions", "R: Basic Instrumental Variables Calculation", "R: Basics of Double Machine Learning", "R: DoubleML for Difference-in-Differences", "R: Cluster Robust Double Machine Learning", "R: Impact of 401(k) on Financial Wealth", "R: Ensemble Learners and More with <code class=\"docutils literal notranslate\"><span class=\"pre\">mlr3pipelines</span></code>", "R: Sample Selection Models", "Python: Difference-in-Differences", "Python: Difference-in-Differences Pre-Testing", "Python: Panel Data with Multiple Time Periods", "Python: Real-Data Example for Multi-Period Difference-in-Differences", "Python: Panel Data Introduction", "DML: Bonus Data", "Examples", "Python: Average Potential Outcome (APO) Models", "Python: Basic Instrumental Variables calculation", "Python: Basics of Double Machine Learning", "Python: Conditional Average Treatment Effects (CATEs) for IRM models", "Python: Conditional Average Treatment Effects (CATEs) for PLR models", "Python: Conditional Value at Risk of potential outcomes", "Python: First Stage and Causal Estimation", "Python: Group Average Treatment Effects (GATEs) for IRM models", "Python: Group Average Treatment Effects (GATEs) for PLR models", "Python: GATE Sensitivity Analysis", "Python: IRM and APO Model Comparison", "Python: Choice of learners", "DoubleML meets FLAML - How to tune learners automatically within <code class=\"docutils literal notranslate\"><span class=\"pre\">DoubleML</span></code>", "Python: Cluster Robust Double Machine Learning", "Python: Impact of 401(k) on Financial Wealth", "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)", "Python: PLM and IRM for Multiple Treatments", "Python: Policy Learning with Trees", "Python: Potential Quantiles and Quantile Treatment Effects", "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)", "Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments", "Python: Sensitivity Analysis", "Example: Sensitivity Analysis for Causal ML", "Python: Sample Selection Models", "<span class=\"section-number\">6. </span>Double machine learning algorithms", "<span class=\"section-number\">1. </span>The basics of double/debiased machine learning", "DoubleMLData from dataframes", "&lt;no title&gt;", "<span class=\"section-number\">2. </span>Data Backend", "User Guide", "<span class=\"section-number\">4. </span>Heterogeneous treatment effects", "<span class=\"section-number\">7. </span>Learners, hyperparameters and hyperparameter tuning", "<span class=\"section-number\">3. </span>Models", "&lt;no title&gt;", "Panel Data", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">9. </span>Sample-splitting, cross-fitting and repeated cross-fitting", "<span class=\"section-number\">5. </span>Score functions", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">8. </span>Variance estimation and confidence intervals", "<span class=\"section-number\">10. </span>Sensitivity analysis", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "DoubleML", "Installing DoubleML", "Getting Started", "Double Machine Learning Literature", "Release Notes", "DoubleML Workflow"], "titleterms": {"": 77, "0": 143, "1": [77, 87, 143], "2": [77, 87, 143], "2011": 87, "2023": 87, "3": [77, 87, 143], "4": [87, 143], "401": [55, 79, 80, 86], "5": [87, 143], "6": 143, "7": 143, "95": 77, "A": [54, 78], "ATE": [57, 74, 81, 88], "No": [54, 78], "One": [54, 68, 69, 78], "That": 85, "The": [55, 79, 81, 90, 140], "acknowledg": [53, 138], "acycl": [51, 66], "addit": 81, "adjust": [60, 84], "advanc": [84, 96, 126], "aggreg": [60, 61, 62, 97], "al": 87, "algorithm": [89, 127, 138, 140], "all": 60, "altern": 111, "analysi": [60, 65, 74, 75, 86, 87, 127, 143], "anticip": 60, "api": [0, 77], "apo": [65, 75, 97, 111, 127], "applic": [54, 78, 86], "approach": [52, 67, 76, 90], "ar": 85, "arah": 87, "arbitrari": 81, "archiv": 64, "arrai": [91, 93], "asset": [55, 79], "assumpt": 87, "att": [58, 60, 61, 62], "augment": 81, "automat": 77, "automl": 77, "averag": [55, 65, 68, 69, 72, 73, 75, 79, 95, 97, 111, 127], "backend": [54, 55, 78, 79, 93, 140, 143], "band": 126, "base": 56, "basic": [51, 52, 60, 66, 67, 90], "benchmark": [86, 87, 127], "bia": [52, 67, 90], "binari": [97, 111], "bonu": 63, "bootstrap": 126, "build": 139, "calcul": [51, 66], "call": 77, "callabl": 111, "case": 64, "cate": [68, 69, 81, 95], "causal": [61, 63, 65, 71, 87, 111, 140, 143], "chernozhukov": 87, "choic": 76, "citat": 138, "class": [1, 49, 50, 54, 78], "cluster": [54, 78], "code": 138, "coeffici": 77, "combin": [60, 71], "compar": [76, 77], "comparison": [53, 75, 77], "comput": [76, 77], "conclus": [77, 87], "conda": 139, "condit": [61, 68, 69, 70, 80, 95, 111], "confid": [77, 85, 126], "construct": 96, "contrast": 65, "control": 60, "covari": [60, 84], "coverag": [58, 71], "cran": 139, "creat": 77, "cross": [54, 58, 78, 97, 99, 110, 111, 127, 140], "custom": [76, 77], "cvar": [70, 80, 95, 111], "dag": [51, 66], "data": [1, 4, 5, 6, 51, 52, 54, 55, 56, 57, 58, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 77, 78, 79, 80, 82, 83, 84, 86, 87, 88, 90, 93, 97, 99, 111, 127, 140, 143], "datafram": [91, 93], "dataset": [2, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 41, 63], "debias": [52, 67, 90, 140], "default": 77, "defin": [54, 78], "demo": 53, "depend": 139, "descript": 60, "design": [84, 97], "detail": [53, 60, 61, 62, 97], "develop": 139, "dgp": [52, 65, 67], "did": [3, 20, 21, 22, 23, 24, 25, 26, 53, 97], "differ": [53, 58, 59, 61, 64, 76, 97, 111, 126, 127], "dimension": [68, 69], "direct": [51, 66], "disclaim": 87, "discontinu": [84, 97], "distribut": [57, 88], "dml": [54, 63, 78, 110, 140, 143], "dml1": 89, "dml2": 89, "dmldummyclassifi": 42, "dmldummyregressor": 43, "doubl": [52, 54, 67, 78, 89, 90, 138, 140, 141], "double_ml_score_mixin": [27, 28], "doubleml": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 53, 55, 56, 66, 77, 79, 86, 87, 126, 138, 139, 143], "doublemlapo": [29, 30], "doublemlblp": 44, "doublemlclusterdata": [4, 54, 78], "doublemlcvar": 31, "doublemldata": [5, 55, 79, 91, 93, 140], "doublemldid": 20, "doublemldidaggreg": 21, "doublemldidbinari": 22, "doublemldidc": 23, "doublemldidmulti": 24, "doublemliivm": 32, "doublemlirm": 33, "doublemllpq": 34, "doublemlpaneldata": [6, 60, 93], "doublemlpliv": [38, 54, 78], "doublemlplr": 39, "doublemlpolicytre": 45, "doublemlpq": 35, "doublemlqt": 36, "doublemlssm": 37, "effect": [55, 60, 61, 62, 64, 68, 69, 70, 72, 73, 75, 79, 80, 81, 83, 86, 87, 95, 97], "elig": [55, 79], "empir": 71, "ensembl": [56, 84], "error": [54, 78], "estim": [51, 55, 57, 58, 60, 61, 62, 63, 66, 71, 74, 77, 79, 80, 81, 83, 86, 87, 88, 110, 111, 126, 140, 143], "et": 87, "evalu": [76, 77, 96], "event": [60, 61, 62], "exampl": [53, 54, 61, 64, 68, 69, 78, 86, 87], "exploit": [53, 56], "extern": [96, 110], "featur": [56, 138], "fetch_401k": 7, "fetch_bonu": 8, "figur": 81, "file": 139, "final": 53, "financi": [55, 79, 80], "first": 71, "fit": [54, 77, 78, 110, 140], "flaml": 77, "flexibl": 84, "fold": [77, 110], "forest": 63, "formul": [87, 143], "from": [53, 56, 91, 93, 139], "full": 77, "function": [50, 53, 54, 78, 111, 140], "fuzzi": [84, 97], "gain_statist": 48, "gate": [72, 73, 74, 95], "gatet": 74, "gener": [2, 52, 64, 65, 67, 77, 84, 90, 127], "get": 140, "github": 139, "global": 84, "globalclassifi": 46, "globalregressor": 47, "graph": [51, 66], "group": [60, 62, 72, 73, 95], "guid": 94, "helper": [54, 78], "heterogen": [64, 75, 81, 95], "how": [56, 77], "hyperparamet": [75, 96], "identif": 87, "iivm": [55, 79, 97, 111], "impact": [55, 79, 80], "implement": [89, 97, 111, 127], "induc": [52, 67, 90], "infer": [126, 143], "initi": [54, 77, 78], "instal": 139, "instrument": [51, 66, 85], "integr": 53, "interact": [55, 72, 79, 82, 97, 111, 127], "interv": [77, 85, 126], "introduct": 62, "invers": 81, "irm": [3, 29, 30, 31, 32, 33, 34, 35, 36, 37, 55, 63, 68, 72, 75, 79, 81, 82, 86, 95, 97, 111, 127], "iv": [51, 55, 66, 79, 97, 111], "k": [55, 79, 80, 86, 110], "lambda": 71, "lasso": [63, 71], "latest": 139, "lear": [54, 78], "learn": [52, 54, 67, 78, 82, 89, 90, 95, 138, 140, 141], "learner": [56, 63, 75, 76, 77, 84, 96, 140], "less": 77, "level": 97, "linear": [55, 60, 73, 79, 81, 84, 97, 111, 127], "linearscoremixin": 27, "literatur": 141, "load": [54, 63, 78, 87], "loader": 2, "local": [55, 79, 80, 83, 84, 111], "loss": 71, "lpq": [83, 111], "lqte": [80, 83], "m": 110, "machin": [52, 54, 67, 78, 89, 90, 138, 140, 141], "main": 138, "mainten": 138, "make_confounded_irm_data": 9, "make_confounded_plr_data": 10, "make_did_cs2021": 25, "make_did_sz2020": 26, "make_heterogeneous_data": 11, "make_iivm_data": 12, "make_irm_data": 13, "make_irm_data_discrete_treat": 14, "make_pliv_chs2015": 15, "make_pliv_multiway_cluster_ckms2021": 16, "make_plr_ccddhnr2018": 17, "make_plr_turrell2018": 18, "make_simple_rdd_data": 41, "make_ssm_data": 19, "mar": [57, 88], "market": [54, 78], "matric": [91, 93], "meet": 77, "method": [77, 143], "metric": [76, 77], "minimum": 96, "miss": [57, 88], "missing": [97, 111], "mixin": 49, "ml": [52, 53, 67, 87, 90, 143], "mlr3": 56, "mlr3extralearn": 56, "mlr3learner": 56, "mlr3pipelin": 56, "model": [3, 49, 55, 57, 63, 65, 68, 69, 72, 73, 75, 77, 79, 81, 82, 85, 87, 88, 95, 97, 110, 111, 126, 127, 140, 143], "modul": 63, "more": 56, "motiv": [54, 78], "multi": 61, "multipl": [60, 65, 81, 97], "multipli": 126, "naiv": [51, 66], "net": [55, 79], "neyman": [111, 140], "nonignor": [57, 88, 97, 111], "nonlinearscoremixin": 28, "nonrespons": [57, 88, 97, 111], "note": 142, "nuisanc": [77, 140], "object": [54, 78, 86], "option": 139, "orthogon": [52, 67, 90, 111, 140], "out": [52, 67, 90], "outcom": [57, 58, 65, 70, 88, 95, 97, 111, 127], "over": 126, "overcom": [52, 67, 90], "overfit": [52, 67, 90], "overlap": 81, "packag": [53, 55, 79, 139], "panel": [58, 60, 62, 97, 99, 111, 127], "parallel": 61, "paramet": [56, 63, 77, 111], "partial": [52, 55, 67, 73, 79, 81, 90, 97, 111, 127], "particip": [55, 79], "partit": 110, "penalti": 71, "perform": [53, 81], "period": [60, 61, 97, 111, 127], "pip": 139, "pipelin": 96, "pliv": [97, 111], "plm": [3, 38, 39, 81, 97, 111, 127], "plot": [54, 77, 78], "plr": [55, 63, 69, 73, 79, 95, 97, 111, 127], "polici": [82, 95], "potenti": [65, 70, 80, 83, 95, 97, 111, 127], "pq": [83, 95, 111], "pre": 59, "predict": [53, 96], "preprocess": 56, "problem": 143, "process": [52, 54, 65, 67, 78, 90], "product": [54, 78], "propens": 81, "provid": 110, "python": [58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 85, 86, 88, 96, 139], "qte": [83, 95], "qualiti": 71, "quantil": [80, 83, 95, 111], "question": 61, "r": [51, 52, 53, 54, 55, 56, 57, 64, 96, 139], "random": [57, 63, 88, 97, 111], "rank": 81, "rdd": [3, 40, 41, 84, 97], "rdflex": 40, "real": [54, 61, 78], "refer": [0, 51, 53, 54, 56, 66, 71, 76, 77, 78, 81, 85, 87, 90, 96, 110, 126, 138, 140], "regress": [55, 72, 73, 79, 82, 84, 97, 111, 127], "regular": [52, 67, 90], "releas": [139, 142], "remark": 53, "remov": [52, 67, 90], "repeat": [58, 97, 99, 110, 111, 127], "repetit": 110, "requir": 96, "research": 61, "respect": [54, 78], "result": [54, 55, 78, 79, 81], "risk": [70, 80, 95, 111], "robust": [54, 78, 85], "run": 85, "sampl": [52, 57, 67, 77, 88, 90, 97, 110, 111], "sandbox": 64, "score": [49, 52, 67, 81, 90, 111, 140], "section": [58, 97, 99, 111, 127], "select": [57, 60, 88, 97, 111], "sensit": [60, 65, 74, 75, 86, 87, 127, 143], "set": [56, 96], "sharp": [84, 97], "simpl": [52, 67, 90], "simul": [51, 54, 58, 66, 78, 85, 86], "simultan": 126, "singl": 65, "small": 85, "sourc": [138, 139], "special": 93, "specif": [127, 143], "specifi": [63, 96, 111], "split": [52, 67, 90, 110], "ssm": 97, "stack": 84, "stage": 71, "standard": [54, 76, 78], "start": 140, "step": 77, "studi": [60, 61, 62, 64], "summari": [55, 77, 79, 81], "test": 59, "theori": 127, "time": [60, 62, 76, 77], "train": 77, "treat": 75, "treatment": [55, 68, 69, 70, 72, 73, 75, 79, 80, 81, 83, 95, 97, 111, 127], "tree": [82, 95], "trend": 61, "tune": [56, 77, 96], "two": [54, 68, 69, 78, 97, 111, 127], "type": 93, "uncondit": 61, "under": [57, 81, 88], "untun": 77, "up": 56, "us": [51, 53, 56, 63, 66, 77, 96], "user": 94, "util": [42, 43, 44, 45, 46, 47, 48, 50], "v": 71, "valid": 126, "valu": [70, 80, 95, 111], "vanderweel": 87, "variabl": [51, 66, 85], "varianc": 126, "version": 139, "via": 111, "wai": [54, 78], "weak": 85, "wealth": [55, 79, 80], "weight": [81, 95], "when": 77, "whl": 139, "within": 77, "without": [84, 110], "workflow": 143, "xgboost": 77, "zero": [54, 78]}})