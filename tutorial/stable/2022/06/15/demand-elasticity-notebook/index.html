<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.100.1" />


<title>Demand elasticity notebook - A Hugo website</title>
<meta property="og:title" content="Demand elasticity notebook - A Hugo website">




  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/rstudio/blogdown">GitHub</a></li>
    
    <li><a href="https://twitter.com/rstudio">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">21 min read</span>
    

    <h1 class="article-title">Demand elasticity notebook</h1>

    
    <span class="article-date">2022-06-15</span>
    

    <div class="article-content">
      


<div id="r-ab-testing-with-doubleml" class="section level1">
<h1>R: A/B Testing with DoubleML</h1>
<p><img src="figures/ab_testing.png" alt="An illustration of A/B testing." style="width: 400px;"/></p>
<p>In this notebook, we demontrate exemplarily how the <a href="https://docs.doubleml.org/stable/index.html">DoubleML</a> package can be used to estimate the causal effect of seeing a new ad design on customers’ purchases in a webshop. We base the estimation steps of our analysis according to the <a href="https://docs.doubleml.org/stable/workflow/workflow.html">DoubleML workflow</a>.</p>
<div id="problem-formulation-ab-testing" class="section level2">
<h2>0. Problem Formulation: A/B Testing</h2>
<div id="the-ab-testing-scenario" class="section level3">
<h3>The A/B Testing Scenario</h3>
<p>Let’s consider the following stylized scenario. The manager of a webshop performs an A/B test to estimate the effect a new ad design <span class="math inline">\(A\)</span> has on customers’ purchases (in <span class="math inline">\(100\$\)</span>), <span class="math inline">\(Y\)</span>, on average. This effect is called the <strong>A</strong>verage <strong>T</strong>reatment <strong>E</strong>ffect (<strong>ATE</strong>). The treatment is assigned randomly conditional on the visitors’ characteristics, which we call <span class="math inline">\(V\)</span>. Such characteristics could be collected from a customer’s shoppers account, for example. These might include the number of previous purchases, time since the last purchase, length of stay on a page as well as whether a customer has a rewards card, among other characteristics. <br></p>
<p>In the following, we use a <strong>D</strong>irected <strong>A</strong>cyclical <strong>G</strong>raph (DAG) to illustrate our assumptions on the causal structure of the scenario. As not only the outcome, but also the treatment is dependent on the individual characteristics, there are arrows going from <span class="math inline">\(V\)</span> to both <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span>. In our example, we also assume that the treatment <span class="math inline">\(A\)</span> is a direct cause of the customers’ purchases <span class="math inline">\(Y\)</span>.</p>
<div class="figure">
<img src="figures/DAG.svg" alt="" />
<p class="caption">Scenario illustration with a DAG</p>
</div>
<p>Let’s assume the conditional randomization has been conducted properly, such that a tidy data set has been collected. Now, a data scientist wants to evaluate whether the new ad design causally affected the sales, by using the <a href="https://docs.doubleml.org/stable/index.html">DoubleML</a> package.</p>
</div>
<div id="why-control-for-individual-characteristics" class="section level3">
<h3>Why control for individual characteristics?</h3>
<p>Before we start the case study, let us briefly address the question why we need to include individual characteristics in our analysis at all. There are mainly two reasons why we want to control for observable characteristics. First, so-called confounders, i.e., variables that have a causal effect on both the treatment variable and the outcome variable, possibly create a bias in our estimate. In order to uncover the true causal effect of the treatment, it is necessary that our causal framework takes all confounding variables into account. Otherwise, the average causal effect of the treatment on the outcome is not identified. A second reason to include individual characteristics is efficiency. The more variation can be explained within our causal framework, the more precise will be the resulting estimate. In practical terms, greater efficiency leads to tighter confidence intervals and smaller standard errors and p-values. This might help to improve the power of A/B tests even if the treatment variable is unconditionally assigned to individuals.</p>
</div>
<div id="why-use-machine-learning-to-analyze-ab-tests" class="section level3">
<h3>Why use machine learning to analyze A/B tests?</h3>
<p>ML methods have turned out to be very flexible in terms of modeling complex relationships of explanatory variables and dependent variables and, thus, have exhibited a great predictive performance in many applications. In the double machine learning approach (<a href="https://arxiv.org/abs/1608.00060">Chernozhukov et al. (2018)</a>), ML methods are used for modelling so-called nuisance functions. In terms of the A/B case study considered here, ML tools can be used to flexibly control for confounding variables. For example, a linear parametric specification as in a standard linear regression model might not be correct and, hence, not sufficient to account for the underlying confounding. Moreover, by using powerful ML techniques, the causal model will likely be able to explain a greater share of the total variation and, hence, lead to more precise estimation.</p>
</div>
</div>
<div id="data-backend" class="section level2">
<h2>1. Data-Backend</h2>
<div id="the-data-set" class="section level3">
<h3>The data set</h3>
<p>As an illustrative example we use a data set from the <a href="https://sites.google.com/view/acic2019datachallenge/data-challenge">ACIC 2019 Data Challenge</a>. In this challenge, a great number of data sets have been generated in a way that they mimic distributional relationships that are found in many economic real data applications. Although the data have not been generated explicitly to address an A/B testing case study, they are well-suited for demonstration purposes. We will focus on one of the many different data genereting processes (DGP) that we picked at random, in this particualar case a data set called <code>high42</code>. An advantage of using the synthetic <a href="https://sites.google.com/view/acic2019datachallenge/data-challenge">ACIC 2019 data</a> is that we know the true average treatment effect which is 0.8 in our data set.</p>
<pre class="r"><code># Load required packages for this tutorial
library(DoubleML)
library(mlr3)
library(mlr3learners)
library(data.table)
library(ggplot2)

# suppress messages during fitting
lgr::get_logger(&quot;mlr3&quot;)$set_threshold(&quot;warn&quot;)</code></pre>
<p>First we load the data.</p>
<pre class="r"><code># Load data set from url (internet connection required)
url = &quot;https://raw.githubusercontent.com/DoubleML/doubleml-docs/master/doc/examples/data/high42.CSV&quot;
df = fread(url)</code></pre>
<pre class="r"><code>dim(df)</code></pre>
<pre><code>## [1] 1000  202</code></pre>
<pre class="r"><code>head(df)</code></pre>
<pre><code>##           Y A V1 V2 V3 V4       V5       V6 V7         V8        V9 V10 V11 V12
## 1: 7.358185 1 10  0  0  7 192.7938 23.67695  8 0.18544294 15.853240   6   1   0
## 2: 8.333672 1 12  0  1  4 199.6536 19.28127  7 0.51484172  9.244882   5   0   0
## 3: 7.472758 0 14  1  1  2 194.2078 24.58933  5 0.30919878 10.791593   4   1   0
## 4: 6.502319 1  0  1  0  9 201.8380 25.51392  4 0.16016010 22.639362   8   0   1
## 5: 7.043758 1 12  0  0  9 201.3604 31.16064  6 0.29197555 25.793415   3   1   1
## 6: 5.658337 0  8  0  1  6 193.2195 20.46564  9 0.05673076  9.277520   6   1   0
##          V13        V14      V15       V16      V17 V18        V19 V20 V21
## 1: 0.2008238  1.1701569 1.217621 -16.81330 8.070488   1  1.0398440  30   0
## 2: 0.2230785 -1.5365715 1.118535 -15.08600 4.482310   0 -0.5417638  -1   0
## 3: 0.1751244 -1.2845893 1.265334 -27.76753 6.190104   1  1.0213073  59   0
## 4: 0.1487592  0.3477511 1.141496 -15.75004 4.752172   0  1.2265897  25   1
## 5: 0.1381315  0.3454147 1.298493 -30.29079 7.794285   0  1.6554302   8   1
## 6: 0.2511722  0.4423286 1.124828 -33.89765 6.324069   0  0.7876613  25   0
##          V22        V23         V24        V25          V26       V27 V28
## 1: 0.2793573 -1.4206026 0.055594684 0.04032890 7.901696e-05 -58.99537  20
## 2: 0.2561681  2.7890151 0.014976614 0.04844543 8.593516e-03 -20.40609   7
## 3: 0.7500856 -0.6466704 0.006374132 0.06934194 2.614655e-05 -50.77884  16
## 4: 0.5371091  4.6331190 0.057029086 0.12232294 8.090068e-03 -58.68011  27
## 5: 0.7368710 -3.5655872 0.031656106 0.10157513 1.200738e-03 -12.73111  14
## 6: 0.4499631 -0.4609568 0.043176027 0.02658478 2.442900e-02 -48.19671  10
##         V29      V30       V31         V32        V33      V34 V35       V36
## 1: 16.64241 34.71766  4.951458  0.17461066  0.2324233 2.070571   0 20.061676
## 2: 46.53248 21.68320  7.369149  0.43136246  1.9259106 1.833840   1 28.059975
## 3: 14.45965 24.99987 14.157243  0.82646459  6.0069024 1.774421   0 11.849514
## 4: 13.68466 25.50238  7.705198 -0.77489925 32.1862806 1.873539   0 20.509810
## 5: 18.46981 28.61574  8.432233 -0.96055871  0.4755487 1.391923   0  9.656802
## 6: 12.99067 32.42152  9.737143  0.03840029 19.1461227 2.300713   0 16.668824
##             V37 V38       V39       V40 V41      V42 V43 V44 V45 V46       V47
## 1: 0.0004402816   1 0.4810670 1.2442739   5 21.34893   1  27   1   1 20.794542
## 2: 0.0007701103   0 2.1610440 1.0118577  12 12.07421   0  27   1   0 24.425323
## 3: 0.0018029212   0 1.0183895 1.2272869   8 15.07926   0  26   0   1 19.125459
## 4: 0.0026928483   0 2.3844421 0.6641995  15 29.66762   0  23   1   1  6.958642
## 5: 0.0023956495   0 0.2408289 1.1954664   8 34.22849   1  28   0   0 20.872313
## 6: 0.0003555247   1 1.0684471 1.3538031   7 42.12471   1  31   0   1 23.213361
##    V48 V49 V50         V51      V52 V53 V54 V55      V56 V57        V58
## 1:   0   1   1  0.58347949 3.585321   1   0   0 2.143131   0 -4.5760397
## 2:   0   0   1 -0.89160276 2.599451   1   0   1 2.392858   1 -0.2151697
## 3:   0   1   0  1.51972438 4.510590   1   0   1 2.113959   1 -4.0197275
## 4:   0   1   1 -0.20967451 2.151854   1   0   0 2.277861   0 -4.1827249
## 5:   0   1   1 -0.09122315 4.605654   1   0   1 2.076338   0 -3.1929933
## 6:   0   0   1  0.08143847 2.747924   1   0   0 2.123266   0 -1.7701957
##           V59 V60          V61      V62 V63 V64 V65 V66      V67       V68
## 1: -2.3393634   1 1.138766e-03 11.53770   0   4   1   1 3.904367 177.51886
## 2:  0.5914007   1 1.370902e-05 11.26127   0   2   1   0 6.468333  70.21335
## 3:  2.0021808   0 2.299705e-02 10.67112   0   2   1   0 4.796108  28.38070
## 4: -0.8959613   1 1.224616e-03 12.09844   1   5   1   0 2.624500  19.75818
## 5: -0.6357487   0 8.927531e-04 10.82328   0   0   1   0 5.195238  55.38393
## 6:  2.4273998   1 2.320485e-03 10.96484   0   2   1   0 4.032135 160.05336
##             V69        V70          V71       V72      V73 V74      V75
## 1: -0.977834002 -15.655234 -0.339091443 -4.859462 46.77102   7 27.35320
## 2: -0.008146874   2.425168  0.250900654 -4.879361 44.25859   4 22.95139
## 3: -0.093093555  -4.628594  0.493405176 -2.582342 59.11854   3 21.71225
## 4:  0.293489224 -12.201118 -0.319991248 -4.524812 11.21414   3 25.32508
## 5: -0.329338305 -19.628055 -0.007397628 -3.060636 12.19038   5 23.33296
## 6: -0.963812494 -13.956940 -0.071165907 -4.836254 21.10611   9 28.26248
##           V76 V77 V78 V79         V80        V81 V82 V83        V84
## 1: -2.2034941   0   6   1 -0.95657964 0.05625706  29   0 0.03737981
## 2: -0.9763126   0   7   0 -0.91832528 0.08790430  24   0 0.03721510
## 3: -0.7862249   0  11   0 -0.92248578 0.06542868  22   0 0.03099046
## 4: -0.2882286   0  14   0  0.05925364 0.11128509  13   0 0.03540332
## 5:  0.3681190   0   7   0 -0.07175302 0.05468009  16   0 0.02777697
## 6: -2.8403231   0   5   0  0.04554488 0.08101242  34   0 0.03868584
##             V85         V86 V87 V88 V89      V90 V91         V92 V93       V94
## 1: -0.157240114 -0.19498800   0   0  17 1.078362   1  1.25262806   0 0.9756274
## 2: -0.138141504  1.18997769   0   1  17 1.262945   1 -0.49464176   0 0.9756161
## 3:  0.996975342 -4.13585291   0   1  22 2.439541   0 -0.12586824   0 0.9808798
## 4: -2.097800685 -0.02922322   0   0   8 1.039004   0  1.06547295   0 0.9817917
## 5:  0.007976423  0.50863166   0   0  14 1.836204   1 -0.02935486   0 0.9882583
## 6:  0.179009664  0.70442709   0   1  11 1.613274   0  1.21502580   0 0.9839163
##    V95      V96      V97 V98 V99     V100     V101 V102 V103     V104      V105
## 1:   5 2.214363 1.266806   1   0 3.858811 2.248257    7    1 35.61543 0.9860864
## 2:   6 5.377556 1.252839   0   0 3.977629 1.821898    3    0 23.06962 0.9936143
## 3:   6 3.865115 1.279053   1   0 3.745750 2.142148    6    2 55.44521 0.9914764
## 4:   4 2.458335 1.286735   0   0 3.894206 1.742324    5    3 28.14201 0.9307358
## 5:   4 3.042121 1.163366   1   0 3.739791 2.188678   10    2 49.26435 0.9991959
## 6:   5 3.616383 1.276404   1   0 3.959868 2.634676    3    2 43.16583 0.9964038
##           V106       V107 V108      V109 V110     V111 V112      V113
## 1:   0.8109243 -0.9703621    0 0.2730344    0 73.50169    1 0.6634900
## 2:  10.9250096 -1.0017012    0 0.4120395    0 51.62268    1 0.2838329
## 3:  13.6599136 -0.9802746    0 0.5236362    0 62.99163    1 0.9516340
## 4: -25.5088824 -0.9934119    1 0.6180330    0 43.44839    1 0.7741553
## 5:   7.0802934 -0.8977199    1 0.5246819    0 72.31038    1 0.6754245
## 6:  -3.3171809 -0.9993140    1 0.7486420    0 60.74483    0 0.1145096
##          V114      V115     V116       V117 V118 V119 V120     V121       V122
## 1: 0.43676411 0.9991616 10.79522 -0.5385633    1    4  934 33.04630  0.4615909
## 2: 0.36474425 0.9989004 11.55450  0.2812390   15    1  941 33.86293 -5.6026849
## 3: 0.06739379 0.9989085 11.84418 -1.4635959   11    5  874 22.15443 -1.8215083
## 4: 0.13928599 0.9996213 14.81413 -0.3407824   43    8  941 45.65891 -2.9436803
## 5: 0.27403924 0.9202706 18.14229 -1.2608972   50    1  952 19.93158 -1.1604592
## 6: 0.25122805 0.9854695 14.57774 -0.8166704    9    2 1111 22.45532 -1.5620386
##    V123      V124       V125 V126 V127 V128 V129 V130 V131      V132 V133
## 1:    0 0.2877762  0.1592139    2    0    1    0    0    6 -14.26363    1
## 2:    0 0.3517710  0.5749188    0    0    0    0    0   20  44.22123    0
## 3:    1 0.4206429 -0.4930869    0    0    0    1    0   15  44.42188    1
## 4:    1 0.3484142  0.9731821    2    0    1    1    0    4  25.29488    1
## 5:    0 0.4275244 -2.7654479    1    5    0    1    1    7  15.44793    3
## 6:    0 0.4941055 -0.4603852    1    2    0    1    0   14  16.02116    1
##         V134      V135 V136 V137        V138 V139     V140 V141 V142 V143
## 1: 0.6480628 4.9721815    1    1 -0.01270437   14 2.211044    0  -88    1
## 2: 0.5464590 2.8346019    0    0 -3.34385582   15 2.780464    0 -103    1
## 3: 0.4751264 1.4166685    0    1  0.70178508   12 2.583958    0 -104    1
## 4: 0.8665180 2.3020696    0    1  3.17621778    6 2.593933    0  -94    1
## 5: 0.4346036 0.5164262    0    0  1.13260721   11 3.313170    0  -62    1
## 6: 0.4303166 1.7248318    0    1 -0.99296580    7 2.070250    0  -80    1
##         V144      V145       V146     V147     V148 V149 V150      V151
## 1: 14.899569 0.2973792 -1.4045348 22.93680 1.112065    1    0 0.7522654
## 2:  9.283158 1.5241806 -0.5923406 19.84037 1.107726    1    0 0.7412935
## 3:  8.126532 1.9441522  1.7871841 17.53077 1.109680    1    0 0.7495357
## 4:  7.990564 1.3594995  0.2679640 24.41190 1.112921    0    0 0.7961531
## 5:  4.432384 0.9491806 -0.7068222 24.55891 1.101129    0    0 0.7781383
## 6:  5.261740 0.8242018 -0.6108768 20.55625 1.109204    0    1 0.8590800
##          V152 V153     V154 V155      V156      V157 V158       V159 V160
## 1: 0.09487609    1 61.75640    0 0.1555205  1.299214    0  8.9809734    1
## 2: 0.23032948    1 45.97932    1 0.1844182  3.095729    3 14.0603393    1
## 3: 0.13815855    1 51.36620    0 0.2829257  0.810593    4  0.9319917    0
## 4: 0.14440331    0 48.24623    0 0.1138707  2.181276    5  2.5809724    0
## 5: 0.13364686    1 47.29790    1 0.2961928 -0.434977    4  7.2345673    1
## 6: 0.33345130    0 56.33525    0 0.1476424 -1.926538    3  4.4139180    1
##         V161 V162       V163 V164 V165     V166 V167       V168       V169 V170
## 1: 14.597886    0  0.3639882    0    0 2.733640    0 0.16228697  2.2205500    0
## 2:  3.380854    0  0.5129260    0    0 2.724039    0 0.10602104 -0.6852898    0
## 3:  3.109170    1 -1.2629269    0    0 2.715319    0 0.05108764  1.9828793    1
## 4: 57.611061    0  0.5685741    0    0 2.711964    0 0.13866903  1.4471618    2
## 5: 32.457049    0 -0.5958445    0    1 2.713607    0 0.06787200  0.4291680    4
## 6: 24.568721    0 -0.2389066    0    0 2.742426    0 0.11932443  2.4901435    2
##          V171 V172     V173      V174 V175 V176 V177       V178       V179 V180
## 1:  30.147837    0 5.890918 0.5035608    1    0    0 0.64153940 -0.7871431   31
## 2: -86.355520    0 6.902555 0.2127434    4    0    0 0.01900934 -1.0393244   21
## 3:  -2.545955    0 6.704886 0.4259085    3    0    0 0.80010433  0.2331956   21
## 4: -33.742974    1 7.528415 0.3282203    6    0    1 1.35364593  0.1319536   32
## 5: -20.346347    1 5.539800 0.4322122    4    0    0 0.90278942 -0.6498747   30
## 6: -17.029889    1 7.514143 0.2331520    1    0    0 1.22747635  0.8908464   20
##    V181 V182 V183 V184        V185       V186       V187        V188 V189 V190
## 1:    3    0    1   17  0.21751273 0.16146110 -0.1212995  0.46743435    3    0
## 2:    3    0    0   16  0.54973231 0.22095415 -0.5137855 -0.11908883    3    0
## 3:    0    0    0   15  0.42325170 0.11036085  0.2995553 -0.25991994    3    0
## 4:    6    1    0    0 -0.18960878 0.02418432 -0.1869971 -0.08153095    2    0
## 5:    3    0    0   17 -0.03580849 0.24367914 -0.6039507  0.53261710    1    0
## 6:    2    0    0    0 -0.70241461 0.57273510  1.8515487  0.64336691    0    0
##        V191 V192     V193 V194 V195     V196       V197 V198 V199       V200
## 1: 1.462837    1 1627.274    0    0 4.683956  0.5656669    0    3 0.02433804
## 2: 1.330522    1 1661.484    1    0 6.766661 -0.3954021    0    4 0.05651780
## 3: 1.384151    1 1658.939    0    0 5.647794  1.1127661    0    0 0.01344207
## 4: 1.220303    1 1650.802    0    0 5.370363 -0.3058420    0    4 0.03463235
## 5: 1.170094    1 1676.819    0    0 3.446532  2.4406606    0    1 0.01751381
## 6: 1.802945    1 1634.093    1    0 5.294410  1.0869714    1    0 0.04712806</code></pre>
<p>We see that the data set consists of 1000 observations (= website visitors) and 202 variables:</p>
<ul>
<li><code>Y</code>: A customer’s purchases (in <span class="math inline">\(100\$\)</span>)</li>
<li><code>A</code>: Binary treatment variable with a value 1 indicating that a customer has been exposed to the new ad design (and value 0 otherwise).</li>
<li><code>V1</code>,…, <code>V200</code>: The remaining 200 columns <span class="math inline">\(V\)</span> represent individual characteristics of the customers (=confounders).</li>
</ul>
<p>To start our analysis, we initialize the data backend from the previously loaded data set, i.e., we create a new instance of a <a href="https://docs.doubleml.org/stable/guide/data_backend.html">DoubleMLData</a> object. During initialization, we specify the roles of the variables in the data set, i.e., in our example the outcome variable <span class="math inline">\(Y\)</span> via the parameter <code>y_col</code>, the treatment variable <span class="math inline">\(A\)</span> via <code>d_cols</code> and the confounding variables <span class="math inline">\(V\)</span> via <code>x_cols</code>.</p>
<pre class="r"><code># Specify explanatory variables for data-backend
features_base = colnames(df)[grep(&quot;V&quot;, colnames(df))]

# TODO: Initialize DoubleMLData (data-backend of DoubleML)</code></pre>
<p>We can print the data-backend to see the variables, which we have assigned as outcome, treatment and controls.</p>
<pre class="r"><code># TODO: print data backend</code></pre>
</div>
</div>
<div id="causal-model" class="section level2">
<h2>2. Causal Model</h2>
<p>The inference problem is to determine the causal effect of seeing the new ad design <span class="math inline">\(A\)</span> on customers’ purchases <span class="math inline">\(Y\)</span> once we control for individual characteristics <span class="math inline">\(V\)</span>. In our example, we are interested in the average treatment effect. Basically, there are two causal models available in <a href="https://docs.doubleml.org/stable/">DoubleML</a> that can be used to estimate the ATE.</p>
<p>The so-called <strong>interactive regression model</strong> (IRM) called by <a href="https://docs.doubleml.org/stable/guide/models.html#interactive-regression-model-irm">DoubleMLIRM</a> is a flexible (nonparametric) model to estimate this causal quantity. The model does not impose functional form restrictions on the underlying regression relationships, for example, linearity or additivity as in a standard linear regression model. This means that the model hosts heterogeneous treatment effects, i.e., account for variation in the effect of the new ad design across customers. Moreover, it is possible to also estimate other causal parameters with the IRM, for example, the average treatment effect on the treated (= those customers who have been exposed to the new ad), which might be of interest too.</p>
<div id="interactive-regression-model-irm" class="section level3">
<h3>2.1. Interactive regression model (IRM)</h3>
<p>We briefly introduce the <a href="https://docs.doubleml.org/stable/guide/models.html#interactive-regression-model-irm">interactive regression model</a> where the main regression relationship of interest is provided by</p>
<p><span class="math display">\[Y = g_0(A, V) + U_1, \quad E(U_1 | V, A) = 0,\]</span></p>
<p>where the treatment variable is binary, <span class="math inline">\(A \in \lbrace 0,1 \rbrace\)</span>. We consider estimation of the average treatment effect (ATE):</p>
<p><span class="math display">\[\theta_0 = \mathbb{E}[g_0(1, V) - g_0(0,V)],\]</span></p>
<p>when treatment effects are heterogeneous. In order to be able to use ML methods, the estimation framework generally requires a property called “double robustness” or “Neyman orthogonality”. In the IRM, double robustness can be achieved by including the first-stage estimation</p>
<p><span class="math display">\[A = m_0(V) + U_2, \quad E(U_2| V) = 0,\]</span></p>
<p>which amounts to estimation of the propensity score, i.e., the probability that a customer is exposed to the treatment provided her observed characteristics. Both predictions are then combined in <a href="https://docs.doubleml.org/stable/guide/scores.html#interactive-regression-model-irm">the doubly robust score for the average treatment effect</a> which is given by</p>
<p><span class="math display">\[\psi(W; \theta, \eta) := g(1,V) - g(0,V) + \frac{A (Y - g(1,V))}{m(V)} - \frac{(1 - A)(Y - g(0,V))}{1 - m(V)} - \theta.\]</span></p>
</div>
<div id="naive-approach-unconditional-estimate-of-ate" class="section level3">
<h3>2.2. Naive Approach: Unconditional estimate of ATE</h3>
<p>As a naive estimate, we could calculate the unconditional average treatment effect. In other words, we simply take the difference between <span class="math inline">\(Y\)</span> observed for the customers who have been exposed to the treatment <span class="math inline">\((A=1)\)</span> and those who haven’t been exposed <span class="math inline">\((A=0)\)</span>.</p>
<p>Since the unconditional ATE does not account for the confounding variables, it will generally not correspond to the true ATE (only in the case of unconditionally random treatment assignment, the unconditional ATE will correspond to the true ATE). For example, if the unconditional ATE estimate is greater than the actual ATE, the manager would erroneously overinterpret the effect of the new ad design and probably make misleading decisions for the marketing budget in the future.</p>
<pre class="r"><code># TODO: Calculate unconditional average treatment effect</code></pre>
</div>
</div>
<div id="ml-methods" class="section level2">
<h2>3. ML Methods</h2>
<p>In this step, we define the learners that will be used for estimation of the nuisance functions later.</p>
<div id="benchmark-using-linear-and-logistic-regression" class="section level3">
<h3>3.1. Benchmark using linear and logistic regression</h3>
<p>Let us first start with a benchmark model that is based on (unpenalized) linear and logistic regression. Hence, we estimate the functions <span class="math inline">\(g_0(A,V)\)</span> using a linear regression model and <span class="math inline">\(m_0(V)\)</span> by using an (unpenalized) logistic regression. In both cases, we include all available characteristics <span class="math inline">\(V\)</span>. We will later compare the performance of this model to that using more advanced ML methods.</p>
<pre class="r"><code># TODO: Initialize Linear and Logistic Regression learners</code></pre>
</div>
<div id="instantiate-one-or-several-ml-learners-of-your-choice" class="section level3">
<h3>3.2. Instantiate one or several ML learners of your choice</h3>
<pre class="r"><code># TODO: Initialize one ML learner of your choice</code></pre>
<pre class="r"><code># TODO: Initialize a second ML learner of your choice
#      (proceed as long as you like)</code></pre>
</div>
</div>
<div id="dml-specifications" class="section level2">
<h2>4. DML Specifications</h2>
<p>At this stage, we instantiate a causal model object of the class <a href="https://docs.doubleml.org/stable/guide/models.html#interactive-regression-model-irm">DoubleMLIRM</a>. Provide the learners via parameters <code>ml_g</code> and <code>ml_m</code>. You can either stick with the default setting or change the parameters. The documentation for the <a href="https://docs.doubleml.org/r/stable/reference/DoubleMLIRM.html">DoubleMLIRM</a> class is available <a href="https://docs.doubleml.org/r/stable/reference/DoubleMLIRM.html">here</a>. Also have a look at the documentation of the abstract base class <a href="https://docs.doubleml.org/r/stable/reference/DoubleML.html">DoubleML</a></p>
<p><strong>Hint</strong>: Use <code>set.seed()</code> to set a random seed prior to your initialization. This makes the sample splits of the different models comparable. Also try to use the same DML specifications in all models to attain some comparability.</p>
<div id="linear-and-logistic-benchmark-model" class="section level3">
<h3>4.1. Linear and logistic benchmark model</h3>
<pre class="r"><code># TODO: Initialize benchmark DoubleMLIRM model</code></pre>
</div>
<div id="ml-model-of-your-choice" class="section level3">
<h3>4.2. ML Model of your choice</h3>
<pre class="r"><code># TODO: Initialize a DoubleMLIRM model using the ML learners of your choice</code></pre>
</div>
<div id="x.-ml-model-of-your-choice" class="section level3">
<h3>4.3. - 4.X. ML Model of your choice</h3>
<p>Proceed with the models using the other ML learners.</p>
<pre class="r"><code># TODO: Initialize a DoubleMLIRM model using the ML learners of your choice</code></pre>
</div>
<div id="estimation" class="section level3">
<h3>5. Estimation</h3>
</div>
<div id="estimation-for-the-benchmark-irm" class="section level3">
<h3>5.1. Estimation for the Benchmark IRM</h3>
<pre class="r"><code># TODO: Fit benchmark DoubleMLIRM model using the fit() method</code></pre>
</div>
<div id="estimation-diagnostics-for-the-benchmark-irm" class="section level3">
<h3>5.2. Estimation Diagnostics for the Benchmark IRM</h3>
<div id="assess-the-predictive-performance-in-the-benchmark-irm" class="section level4">
<h4>5.2.1. Assess the Predictive Performance in the benchmark IRM</h4>
<p>To evaluate the different models we can compare how well the employed estimators fit the nuisance functions <span class="math inline">\(g_0(\cdot)\)</span> and <span class="math inline">\(m_0(\cdot)\)</span>. Use the following helper function to compare the predictive performance of your models.</p>
<pre class="r"><code># A function to calculate prediction accuracy values for every repetition
# of a Double Machine Learning model using IRM, DoubleMLIRM
pred_acc_irm = function(obj, prop) {
  # obj : DoubleML::DoubleMLIRM
  # The IRM Double Machine Learning model
  # prop : logical
  # Indication if RMSE values have to be computed for main regression or
  # log loss values for propensity score  
  
  if (obj$data$n_treat &gt; 1) {
    stop(&quot;Number of treatment variable is &gt; 1. Helper function for nuisance accuracy is only implemented for 1 treatment variable.&quot;)
  }
  h = obj$data$n_obs
  w = obj$n_rep
  
  y = obj$data$data_model[[obj$data$y_col]]
  d = obj$data$data_model[[obj$data$treat_col]]
  g0 = matrix(obj$predictions[[&#39;ml_g0&#39;]][,,1], ncol = w)
  g1 = matrix(obj$predictions[[&#39;ml_g1&#39;]][,,1], ncol = w)
  m = matrix(obj$predictions[[&#39;ml_m&#39;]][,,1], ncol = w)
  
  if (!all(unique(d) %in% c(0,1))) {
    stop(&quot;Treatment must be a binary variable.&quot;)
  }

  if (!prop) {
    export_pred = d*g1 + (1-d) * g0
    # Calculate MSE for every repetition
    pred_acc = apply(export_pred, 2,
                     function(x) mlr3measures::rmse(y,x))
  } else {
    pred_acc = rep(NA, w)
      for (j in seq_len(w)) {
          class_probs = matrix(c(1-m[,j],m[,j]), ncol = 2)
          colnames(class_probs) = c(&quot;0&quot;, &quot;1&quot;)
          pred_acc[j] = mlr3measures::logloss(as.factor(d),class_probs)
    }
  }
  return(pred_acc)
}</code></pre>
<pre class="r"><code># TODO: Evaluate the predictive performance for `ml_g` and `ml_m` using the
#       helper function `pred_acc_irm()`.</code></pre>
</div>
<div id="optional-5.2.2.-evaluation-of-propensity-score-estimates-in-the-benchmark-irm" class="section level4">
<h4>Optional: 5.2.2. Evaluation of Propensity Score Estimates in the Benchmark IRM</h4>
<p>The propensity score <span class="math inline">\(m_0(A,V)\)</span> plays an important role in the <a href="https://docs.doubleml.org/stable/guide/scores.html#interactive-regression-model-irm">score of the IRM model</a>. Try to summarize the estimates for <span class="math inline">\(m_0(A,V)\)</span> using some descriptive statistics or visualization. You can use the following helper function to generate a histogram for the propensity score.</p>
<pre class="r"><code># Function to plot propensity scores
rep_propscore_plot = function(obj) {
  # obj : doubleml
  # The Double Machine Learning model
  if (obj$data$n_treat &gt; 1) {
    stop(&quot;Number of treatment variable is &gt; 1. Helper function for nuisance accuracy is only implemented for 1 treatment variable.&quot;)
  }
  m = data.table(obj$predictions[[&#39;ml_m&#39;]][,,1])
  colnames(m) = paste(&quot;Repetition&quot;, 1:obj$n_rep)
  m = melt(m,
           measure.vars = names(m))
  
  hist_ps = ggplot(m) +
    geom_histogram(aes(y = ..count.., x = value),
                   bins = 25, fill = &quot;darkblue&quot;,
                   col= &quot;darkblue&quot;, alpha = 0.5) + 
    xlim(c(0,1)) + theme_minimal() + 
    facet_grid(. ~ variable )
  return(hist_ps)
}</code></pre>
<pre class="r"><code># (TODO): Summarize the propensity score estimates</code></pre>
</div>
</div>
<div id="estimation-for-ml-model" class="section level3">
<h3>5.3. Estimation for ML Model</h3>
<pre class="r"><code># TODO: Fit the ML DoubleMLIRM model using the fit() method</code></pre>
</div>
<div id="estimation-diagnostics-for-the-irm-using-ml-methods" class="section level3">
<h3>5.3. Estimation Diagnostics for the IRM using ML Methods</h3>
<div id="assess-the-predictive-performance-in-the-irm-using-ml-methods" class="section level4">
<h4>5.3.1. Assess the Predictive Performance in the IRM using ML methods</h4>
<pre class="r"><code># TODO: Evaluate the predictive performance for `ml_g` and `ml_m` using the
#       helper function `pred_acc_irm()`.</code></pre>
</div>
<div id="optional-5.3.2.-evaluation-of-propensity-score-estimates-in-the-benchmark-irm" class="section level4">
<h4>Optional: 5.3.2. Evaluation of Propensity Score Estimates in the Benchmark IRM</h4>
<pre class="r"><code># (TODO): Summarize the propensity score estimates</code></pre>
</div>
</div>
<div id="x.-ml-model-of-your-choice-1" class="section level3">
<h3>5.4. - 5.X. ML Model of your choice</h3>
<p>Proceed with the models using the other ML learners.</p>
</div>
<div id="x1-summarize-your-results-on-the-quality-of-estimation" class="section level3">
<h3>5.X+1 Summarize your Results on the Quality of Estimation</h3>
<p>Provide a brief summary of your estimation results, for example by creating a table or figure.</p>
<pre class="r"><code># TODO: Summarize the results on the nuisance estimation in a table or figure</code></pre>
</div>
</div>
<div id="inference" class="section level2">
<h2>6. Inference</h2>
<p>Summarize your results on the <strong>coefficient estimate</strong> for <span class="math inline">\(\theta_0\)</span> as well as the <strong>standard errors</strong> and / or <strong>confidence intervals</strong>, respectively. You can create a table or a figure illustrating your findings.</p>
<p>Try to answer the following questions:</p>
<ul>
<li>Can you reject the <span class="math inline">\(H_0\)</span> that the new add (<span class="math inline">\(A\)</span>) has no effect on sales (<span class="math inline">\(Y\)</span>) at common significance levels?</li>
<li>How close is your estimate to the true value of <span class="math inline">\(\theta_0=0.8\)</span>?</li>
<li>Do the confidence intervals cover the true effect <span class="math inline">\(\theta_0 = 0.8\)</span>?</li>
</ul>
<p><strong>Solution:</strong></p>
<ul>
<li>In all ML based models, the null hypothesis <span class="math inline">\(H_0: \theta_0 = 0\)</span> can be rejected at all common significance levels</li>
<li>The linear/logistic benchmarmk model seems to suffer from numerical instabilities/overfitting; the results in terms of the quality of fit for the nuisance functions are worse than for the ML methods leading to imprecise and instable estimation</li>
<li>The bias of the benchmark model is quite substantial, the results for the ML-based models are closer to the true effect of <span class="math inline">\(\theta_0=0.8\)</span>. The associated confidence intervals do cover the true effect <span class="math inline">\(\theta_0=0.8\)</span>.</li>
</ul>
<div id="inference-for-the-benchmark-irm" class="section level3">
<h3>6.1. Inference for the benchmark IRM</h3>
<pre class="r"><code>## TODO: After calling fit(), access the coefficient parameter,
##      the standard error and confidence interva by calling the method
##      `summary()` and `confint().</code></pre>
</div>
<div id="inference-for-the-irm-using-ml-methods" class="section level3">
<h3>6.2. Inference for the IRM using ML methods</h3>
<pre class="r"><code>## TODO: After calling fit(), access the coefficient parameter,
##      the standard error and confidence interval by calling the methods
##      `summary()` and `confint()`.</code></pre>
</div>
<div id="x.-ml-model-of-your-choice-2" class="section level3">
<h3>6.3. - 6.X. ML Model of your choice</h3>
<p>Proceed with the models using the other ML learners.</p>
<hr />
</div>
</div>
<div id="variation-scope-for-extensions" class="section level2">
<h2>Variation / Scope for Extensions</h2>
<div id="variation-1-partially-linear-regression" class="section level3">
<h3>Variation 1: Partially linear regression</h3>
<p>As an alternative to the (nonparametric) IRM model, the <a href="https://docs.doubleml.org/stable/index.html">DoubleML</a> package also includes the <a href="https://docs.doubleml.org/stable/guide/models.html#partially-linear-regression-model-plr">partial linear regression (PLR)</a> model, which assumes the population regression has a linear and additive structure. Although in reality, we never know if this structure really holds for the underlying data generating process, we can apply this model and see how the estimates compare to those from the IRM.</p>
<p>We can estimate the nuisance functions <span class="math inline">\(g_0\)</span> and <span class="math inline">\(m_0\)</span> in the following PLR model:</p>
<p><span class="math display">\[\begin{eqnarray}
&amp; Y = A\theta_0 + g_0(V) + \zeta, &amp;\quad E[\zeta \mid A,V]= 0,\\
&amp; A = m_0(V) +  U_3, &amp;\quad E[U_3 \mid V] = 0.
\end{eqnarray}\]</span></p>
</div>
<div id="variation-2-employ-an-alternative-learner" class="section level3">
<h3>Variation 2: Employ an alternative learner</h3>
<p>Instead of the learners used above, we can experiment with different learners that are available from the <code>mlr3</code> ecosystem. A searchable list of all implemented learners is available <a href="https://mlr3extralearners.mlr-org.com/articles/learners/list_learners.html">here</a>.</p>
</div>
<div id="variation-3-tune-a-learner-or-experiment-with-pipelines" class="section level3">
<h3>Variation 3: Tune a learner or experiment with pipelines</h3>
<p>The <a href="https://docs.doubleml.org/stable/guide/learners.html#r-learners-and-hyperparameters">learner section of the user guide</a> explains how to perform parameter tuning using the <a href="https://mlr3tuning.mlr-org.com/">mlr3tuning</a> package.</p>
<p>It is also possible to implement pipelines using the <a href="https://mlr3pipelines.mlr-org.com/index.html">mlr3pipelines</a> package. You can find an experimental notebook <a href="https://docs.doubleml.org/dev/examples/R_double_ml_pipeline.html">here</a>.</p>
<hr />
<p><strong>Notes and Acknowledgement</strong></p>
<p>We would like to thank the organizers of the <a href="https://sites.google.com/view/acic2019datachallenge/data-challenge">ACIC 2019 Data Challenge</a> for setting up this data challenge and making the numerous synthetic data examples publicly available. Although the data examples in the <a href="https://sites.google.com/view/acic2019datachallenge/data-challenge">ACIC 2019 Data Challenge</a> do not explicitly adress A/B testing, we put the data example here in this context to give a tractable example on the use of causal machine learning in practice. The parameters for the random forests and extreme gradient boosting learners have been tuned externally. The corresponding tuning notebook will be uploaded in the <a href="https://docs.doubleml.org/dev/examples/index.html">examples gallery</a> in the future.</p>
</div>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W. and Robins, J. (2018), Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21: C1-C68. <a href="doi:10.1111/ectj.12097" class="uri">doi:10.1111/ectj.12097</a>.</p>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

