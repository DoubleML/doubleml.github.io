
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>3. Models &#8212; DoubleML  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../_static/thebelab.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=c49bcd98" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=1ecd1175"></script>
    <script src="../_static/thebelab-helper.js"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'guide/models';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://docs.doubleml.org/dev/_static/switcher.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'stable';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = false;
        </script>
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4. Heterogeneous treatment effects" href="heterogeneity.html" />
    <link rel="prev" title="2. Data Backend" href="data_backend.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>
<aside class="bd-header-announcement" aria-label="Announcement">
  <div class="bd-header-announcement__content">Interested to learn more? We offer <a href='https://trainings.doubleml.org/'>DoubleML Trainings!</a></div>
</aside>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt=""/>
    <script>document.write(`<img src="../_static/logo_dark.png" class="logo__image only-dark" alt=""/>`);</script>
  
  
    <p class="title logo__title">DoubleML</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro/install.html">
     Install
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro/intro.html">
     Getting Started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../workflow/workflow.html">
     Workflow
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="guide.html">
     User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../examples/index.html">
     Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api/api.html">
     Python API
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-external" href="https://docs.doubleml.org/r/stable/">
     R API
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-external" href="https://docs.doubleml.org/doubleml-coverage/">
     Coverage Repository
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../literature/literature.html">
     Literature
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../release/release.html">
     Release notes
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
        </div>
      
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/DoubleML/doubleml-for-py" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/DoubleML/" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/DoubleML/doubleml-for-py/discussions" title="Discussions" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-comments fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discussions</span></a>
        </li>
</ul></div>
      
        <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro/install.html">
     Install
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro/intro.html">
     Getting Started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../workflow/workflow.html">
     Workflow
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="guide.html">
     User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../examples/index.html">
     Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api/api.html">
     Python API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://docs.doubleml.org/r/stable/">
     R API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://docs.doubleml.org/doubleml-coverage/">
     Coverage Repository
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../literature/literature.html">
     Literature
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../release/release.html">
     Release notes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/DoubleML/doubleml-for-py" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/DoubleML/" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/DoubleML/doubleml-for-py/discussions" title="Discussions" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-comments fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discussions</span></a>
        </li>
</ul></div>
        
          <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-3"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-3"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-3"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-3">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item"><p class="logo" style="text-align:center;"><a href="../index.html">
    <img class="logo" src="../logo.png" alt="Logo" width="65%" height="65%">
</a></p>

<script type="text/javascript">
    // Change the logo depending on the theme
    var logo = document.querySelector('img.logo');
    var observer = new MutationObserver(function(mutations) {
        const dark = document.documentElement.dataset.theme == 'dark';
        if (dark) {
            logo.src = "../logo_dark.png";
        } else {
            logo.src = "../logo.png";
        }
    });
    observer.observe(document.documentElement, {attributes: true, attributeFilter: ['data-theme']});
</script></div>
        <div class="sidebar-primary-item">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="basics.html">1. The basics of double/debiased machine learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_backend.html">2. Data Backend</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">3. Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="heterogeneity.html">4. Heterogeneous Treatment Effects</a></li>
<li class="toctree-l1"><a class="reference internal" href="scores.html">5. Score functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="algorithms.html">6. Double machine learning algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="learners.html">7. Learners, hyperparameters and hyperparameter tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="se_confint.html">8. Variance estimation and confidence intervals</a></li>
<li class="toctree-l1"><a class="reference internal" href="resampling.html">9. Sample-splitting, cross-fitting and repeated cross-fitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="sensitivity.html">10. Sensitivity Analysis</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="guide.html" class="nav-link">User Guide</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="models">
<span id="id1"></span><h1><span class="section-number">3. </span>Models<a class="headerlink" href="#models" title="Link to this heading">#</a></h1>
<p>The <a class="reference internal" href="../index.html#doubleml-package"><span class="std std-ref">DoubleML</span></a>-package includes the following models.</p>
<section id="partially-linear-models-plm">
<span id="plm-models"></span><h2><span class="section-number">3.1. </span>Partially linear models (PLM)<a class="headerlink" href="#partially-linear-models-plm" title="Link to this heading">#</a></h2>
<p>The partially linear models (PLM) take the form</p>
<div class="math notranslate nohighlight">
\[Y = D \theta_0 + g_0(X) + \zeta,\]</div>
<p>where treatment effects are additive with some sort of linear form.</p>
<section id="partially-linear-regression-model-plr">
<span id="plr-model"></span><h3><span class="section-number">3.1.1. </span>Partially linear regression model (PLR)<a class="headerlink" href="#partially-linear-regression-model-plr" title="Link to this heading">#</a></h3>
<p><strong>Partially linear regression (PLR)</strong> models take the form</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}Y = D \theta_0 + g_0(X) + \zeta, &amp; &amp;\mathbb{E}(\zeta | D,X) = 0,\\D = m_0(X) + V, &amp; &amp;\mathbb{E}(V | X) = 0,\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(Y\)</span> is the outcome variable and <span class="math notranslate nohighlight">\(D\)</span> is the policy variable of interest.
The high-dimensional vector <span class="math notranslate nohighlight">\(X = (X_1, \ldots, X_p)\)</span> consists of other confounding covariates,
and <span class="math notranslate nohighlight">\(\zeta\)</span> and <span class="math notranslate nohighlight">\(V\)</span> are stochastic errors.</p>
<figure class="align-center" id="id20">
<div class="graphviz"><img src="../_images/graphviz-8852e5db087f49410d0a5212d9b7fdcb58f0aaf9.png" alt="digraph {
     nodesep=1;
     ranksep=1;
     rankdir=LR;
     { node [shape=circle, style=filled]
       Y [fillcolor=&quot;#56B4E9&quot;]
       D [fillcolor=&quot;#F0E442&quot;]
       V [fillcolor=&quot;#F0E442&quot;]
       X [fillcolor=&quot;#D55E00&quot;]
     }
     Y -&gt; D -&gt; V [dir=&quot;back&quot;];
     X -&gt; D;
     Y -&gt; X [dir=&quot;back&quot;];
}" class="graphviz" /></div>
<figcaption>
<p><span class="caption-text">Causal diagram</span><a class="headerlink" href="#id20" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><code class="docutils literal notranslate"><span class="pre">DoubleMLPLR</span></code> implements PLR models. Estimation is conducted via its <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Remark that the standard approach with <code class="docutils literal notranslate"><span class="pre">score='partialling</span> <span class="pre">out'</span></code> does not rely on a direct estimate of <span class="math notranslate nohighlight">\(g_0(X)\)</span>,
but <span class="math notranslate nohighlight">\(\ell_0(X) := \mathbb{E}[Y \mid X] = \theta_0 \mathbb{E}[D \mid X] + g(X)\)</span>.</p>
</div>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="py" for="sd-tab-item-0">
Python</label><div class="sd-tab-content docutils">
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="gp">In [2]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">doubleml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dml</span>

<span class="gp">In [3]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">doubleml.plm.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_plr_CCDDHNR2018</span>

<span class="gp">In [4]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span>

<span class="gp">In [5]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">clone</span>

<span class="gp">In [6]: </span><span class="n">learner</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="gp">In [7]: </span><span class="n">ml_l</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">learner</span><span class="p">)</span>

<span class="gp">In [8]: </span><span class="n">ml_m</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">learner</span><span class="p">)</span>

<span class="gp">In [9]: </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1111</span><span class="p">)</span>

<span class="gp">In [10]: </span><span class="n">data</span> <span class="o">=</span> <span class="n">make_plr_CCDDHNR2018</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">n_obs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">dim_x</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;DataFrame&#39;</span><span class="p">)</span>

<span class="gp">In [11]: </span><span class="n">obj_dml_data</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLData</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">)</span>

<span class="gp">In [12]: </span><span class="n">dml_plr_obj</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLPLR</span><span class="p">(</span><span class="n">obj_dml_data</span><span class="p">,</span> <span class="n">ml_l</span><span class="p">,</span> <span class="n">ml_m</span><span class="p">)</span>

<span class="gp">In [13]: </span><span class="nb">print</span><span class="p">(</span><span class="n">dml_plr_obj</span><span class="o">.</span><span class="n">fit</span><span class="p">())</span>
<span class="go">================== DoubleMLPLR Object ==================</span>

<span class="go">------------------ Data Summary      ------------------</span>
<span class="go">Outcome variable: y</span>
<span class="go">Treatment variable(s): [&#39;d&#39;]</span>
<span class="go">Covariates: [&#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;X4&#39;, &#39;X5&#39;, &#39;X6&#39;, &#39;X7&#39;, &#39;X8&#39;, &#39;X9&#39;, &#39;X10&#39;, &#39;X11&#39;, &#39;X12&#39;, &#39;X13&#39;, &#39;X14&#39;, &#39;X15&#39;, &#39;X16&#39;, &#39;X17&#39;, &#39;X18&#39;, &#39;X19&#39;, &#39;X20&#39;]</span>
<span class="go">Instrument variable(s): None</span>
<span class="go">No. Observations: 500</span>

<span class="go">------------------ Score &amp; Algorithm ------------------</span>
<span class="go">Score function: partialling out</span>

<span class="go">------------------ Machine Learner   ------------------</span>
<span class="go">Learner ml_l: RandomForestRegressor(max_depth=5, max_features=20, min_samples_leaf=2)</span>
<span class="go">Learner ml_m: RandomForestRegressor(max_depth=5, max_features=20, min_samples_leaf=2)</span>
<span class="go">Out-of-sample Performance:</span>
<span class="go">Regression:</span>
<span class="go">Learner ml_l RMSE: [[1.18356413]]</span>
<span class="go">Learner ml_m RMSE: [[1.06008533]]</span>

<span class="go">------------------ Resampling        ------------------</span>
<span class="go">No. folds: 5</span>
<span class="go">No. repeated sample splits: 1</span>

<span class="go">------------------ Fit Summary       ------------------</span>
<span class="go">       coef  std err         t         P&gt;|t|     2.5 %    97.5 %</span>
<span class="go">d  0.512672  0.04491  11.41566  3.492417e-30  0.424651  0.600694</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="r" for="sd-tab-item-1">
R</label><div class="sd-tab-content docutils">
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">DoubleML</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">mlr3</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">mlr3learners</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">data.table</span><span class="p">)</span>
<span class="n">lgr</span><span class="o">::</span><span class="nf">get_logger</span><span class="p">(</span><span class="s">&quot;mlr3&quot;</span><span class="p">)</span><span class="o">$</span><span class="nf">set_threshold</span><span class="p">(</span><span class="s">&quot;warn&quot;</span><span class="p">)</span>

<span class="n">learner</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">lrn</span><span class="p">(</span><span class="s">&quot;regr.ranger&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">num.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">mtry</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="n">min.node.size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">max.depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span>
<span class="n">ml_l</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">learner</span><span class="o">$</span><span class="nf">clone</span><span class="p">()</span>
<span class="n">ml_m</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">learner</span><span class="o">$</span><span class="nf">clone</span><span class="p">()</span>
<span class="nf">set.seed</span><span class="p">(</span><span class="m">1111</span><span class="p">)</span>
<span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">make_plr_CCDDHNR2018</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">,</span><span class="w"> </span><span class="n">n_obs</span><span class="o">=</span><span class="m">500</span><span class="p">,</span><span class="w"> </span><span class="n">dim_x</span><span class="o">=</span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="n">return_type</span><span class="o">=</span><span class="s">&#39;data.table&#39;</span><span class="p">)</span>
<span class="n">obj_dml_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DoubleMLData</span><span class="o">$</span><span class="nf">new</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">y_col</span><span class="o">=</span><span class="s">&quot;y&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">d_cols</span><span class="o">=</span><span class="s">&quot;d&quot;</span><span class="p">)</span>
<span class="n">dml_plr_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DoubleMLPLR</span><span class="o">$</span><span class="nf">new</span><span class="p">(</span><span class="n">obj_dml_data</span><span class="p">,</span><span class="w"> </span><span class="n">ml_l</span><span class="p">,</span><span class="w"> </span><span class="n">ml_m</span><span class="p">)</span>
<span class="n">dml_plr_obj</span><span class="o">$</span><span class="nf">fit</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">dml_plr_obj</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>================= DoubleMLPLR Object ==================


------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): d
Covariates: X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, X20
Instrument(s): 
Selection variable: 
No. Observations: 500

------------------ Score &amp; algorithm ------------------
Score function: partialling out
DML algorithm: dml2

------------------ Machine learner   ------------------
ml_l: regr.ranger
ml_m: regr.ranger

------------------ Resampling        ------------------
No. folds: 5
No. repeated sample splits: 1
Apply cross-fitting: TRUE

------------------ Fit summary       ------------------
 Estimates and significance testing of the effect of target variables
  Estimate. Std. Error t value Pr(&gt;|t|)    
d   0.47319    0.04165   11.36   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


</pre></div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="logistic-partially-linear-regression-model-lplr">
<span id="lplr-model"></span><h3><span class="section-number">3.1.2. </span>Logistic partially linear regression model (LPLR)<a class="headerlink" href="#logistic-partially-linear-regression-model-lplr" title="Link to this heading">#</a></h3>
<p><strong>Logistic partially linear regression (LPLR)</strong> models take the form</p>
<div class="math notranslate nohighlight">
\[\mathbb{E} [Y | D, X] = \mathbb{P} (Y=1 | D, X) = \text{expit} \{\beta_0 D + r_0 (X) \}\]</div>
<p>where <span class="math notranslate nohighlight">\(Y\)</span> is the binary outcome variable and <span class="math notranslate nohighlight">\(D\)</span> is the policy variable of interest.
The high-dimensional vector <span class="math notranslate nohighlight">\(X = (X_1, \ldots, X_p)\)</span> consists of confounding covariates and
<span class="math notranslate nohighlight">\(\text{expit}\)</span> is the logistic link function</p>
<div class="math notranslate nohighlight">
\[\text{expit} ( X ) = \frac{1}{1 + e^{-x}}.\]</div>
<figure class="align-center" id="id21">
<div class="graphviz"><img src="../_images/graphviz-8852e5db087f49410d0a5212d9b7fdcb58f0aaf9.png" alt="digraph {
     nodesep=1;
     ranksep=1;
     rankdir=LR;
     { node [shape=circle, style=filled]
       Y [fillcolor=&quot;#56B4E9&quot;]
       D [fillcolor=&quot;#F0E442&quot;]
       V [fillcolor=&quot;#F0E442&quot;]
       X [fillcolor=&quot;#D55E00&quot;]
     }
     Y -&gt; D -&gt; V [dir=&quot;back&quot;];
     X -&gt; D;
     Y -&gt; X [dir=&quot;back&quot;];
}" class="graphviz" /></div>
<figcaption>
<p><span class="caption-text">Causal diagram</span><a class="headerlink" href="#id21" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><code class="docutils literal notranslate"><span class="pre">DoubleMLLPLR</span></code> implements LPLR models. Estimation is conducted via its <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Remark that the treatment effects are not additive in this model. The partial linear term enters the model through a logistic link function.</p>
</div>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-2" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="py" for="sd-tab-item-2">
Python</label><div class="sd-tab-content docutils">
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [14]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="gp">In [15]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">doubleml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dml</span>

<span class="gp">In [16]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">doubleml.plm.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_lplr_LZZ2020</span>

<span class="gp">In [17]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span><span class="p">,</span> <span class="n">RandomForestClassifier</span>

<span class="gp">In [18]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">clone</span>

<span class="gp">In [19]: </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">3141</span><span class="p">)</span>

<span class="gp">In [20]: </span><span class="n">ml_t</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="gp">In [21]: </span><span class="n">ml_m</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="gp">In [22]: </span><span class="n">ml_M</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="gp">In [23]: </span><span class="n">obj_dml_data</span> <span class="o">=</span> <span class="n">make_lplr_LZZ2020</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">n_obs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">dim_x</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>

<span class="gp">In [24]: </span><span class="n">dml_lplr_obj</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLLPLR</span><span class="p">(</span><span class="n">obj_dml_data</span><span class="p">,</span> <span class="n">ml_M</span><span class="p">,</span> <span class="n">ml_t</span><span class="p">,</span> <span class="n">ml_m</span><span class="p">)</span>

<span class="gp">In [25]: </span><span class="n">dml_lplr_obj</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span>
<span class="gh">Out[25]: </span>
<span class="go">       coef  std err         t     P&gt;|t|     2.5 %   97.5 %</span>
<span class="go">d  0.404834   0.1065  3.801269  0.000144  0.196099  0.61357</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="partially-linear-panel-regression-model-plpr">
<span id="plpr-model"></span><h3><span class="section-number">3.1.3. </span>Partially linear panel regression model (PLPR)<a class="headerlink" href="#partially-linear-panel-regression-model-plpr" title="Link to this heading">#</a></h3>
<p>Suppose a panel study observes each of <span class="math notranslate nohighlight">\(N\)</span> individuals over <span class="math notranslate nohighlight">\(T\)</span> time periods (or waves).
For each individual <span class="math notranslate nohighlight">\(i=1,\dots,N\)</span> and each period <span class="math notranslate nohighlight">\(t=1,\dots,T\)</span>, the data consists of the
triple <span class="math notranslate nohighlight">\((Y_{it}, D_{it}, X_{it})\)</span>. Let <span class="math notranslate nohighlight">\(\{(Y_{it}, D_{it}, X_{it}) : t = 1, \dots , T\}_{i=1}^N\)</span>
denote <span class="math notranslate nohighlight">\(N\)</span> independent and identically distributed (iid) random vectors, where each vector
corresponds to individual <span class="math notranslate nohighlight">\(i\)</span> observed across all T waves.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The notation and identifying assumptions are based on <a class="reference external" href="https://doi.org/10.1093/ectj/utaf011">Clarke and Polselli (2025)</a>, with some small adjustments to better fit into the general package documentation conventions, sometimes slightly abusing notation.
See also the R package <a class="reference external" href="https://github.com/POLSEAN/XTDML">xtdml</a> implementation and corresponding reference <a class="reference external" href="https://arxiv.org/abs/2512.15965">Polselli (2025)</a> for further details.</p>
</div>
<p><strong>Partially linear panel regression (PLPR)</strong> models <a class="reference external" href="https://doi.org/10.1093/ectj/utaf011">(Clarke and Polselli, 2025)</a> take the form</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}Y_{it} = \theta_0 D_{it} + g_1(X_{it}) + \alpha_i^* + U_{it}, &amp; &amp;\mathbb{E}(U_{it} | D_{it}, X_{it}, \alpha_i^*) = 0,\\D_{it} = m_1(X_{it}) + \gamma_i + V_{it}, &amp; &amp;\mathbb{E}(V_{it} | X_{it}, \gamma_i) = 0,\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(Y_{it}\)</span> is the outcome variable and <span class="math notranslate nohighlight">\(D_{it}\)</span> is the policy variable of interest.
Further note that <span class="math notranslate nohighlight">\(\mathbb{E}[\alpha_i^* | D_{it}, X_{it}] \neq 0\)</span>. The high-dimensional
vector <span class="math notranslate nohighlight">\(X_{it} = (X_{it,1}, \ldots, X_{it,p})\)</span> consists of other confounding covariates.
<span class="math notranslate nohighlight">\(\alpha_i^*\)</span> and <span class="math notranslate nohighlight">\(\gamma_i\)</span> represent unobserved individual heterogeneity terms,
correlated with the covariates. <span class="math notranslate nohighlight">\(U_{it}\)</span> and <span class="math notranslate nohighlight">\(V_{it}\)</span> are stochastic errors.</p>
<p>Alternatively one can write the <em>partialling-out</em> PLPR model as</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}Y_{it} = \theta_0 V_{it} + \ell_1(X_{it}) + \alpha_i + U_{it},\\V_{it} = D_{it} - m_1(X_{it}) - \gamma_i,\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha_i\)</span> is a fixed effect.</p>
<p>To account for the presence of unobserved heterogeneity, <a class="reference external" href="https://doi.org/10.1093/ectj/utaf011">Clarke and Polselli (2025)</a>
use different panel data approaches, under the following assumptions.</p>
<p>Define potential outcomes <span class="math notranslate nohighlight">\(Y_{it}(d)\)</span> for individual <span class="math notranslate nohighlight">\(i\)</span> at wave <span class="math notranslate nohighlight">\(t\)</span>, where realizations are
linked to the observed outcome by the consistency assumption <span class="math notranslate nohighlight">\(Y_{it}(d_{it}) = Y_{it}\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\xi_i\)</span> represent time-invariant heterogeneity terms influencing outcome and treatment.
Further, define <span class="math notranslate nohighlight">\(L_{t-1}(W_i) = \{W_{i1}, \dots, W_{it-1}\}\)</span> as lags of a random variable <span class="math notranslate nohighlight">\(W_{it}\)</span>
at wave <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>Assumptions <a class="reference external" href="https://doi.org/10.1093/ectj/utaf011">(Clarke and Polselli, 2025)</a>:</p>
<ul>
<li><dl class="simple">
<dt><strong>No feedback to predictors</strong></dt><dd><p><span class="math notranslate nohighlight">\(X_{it} \perp L_{t-1} (Y_i, D_i) | L_{t-1} (X_i), \xi_i\)</span></p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Static panel</strong></dt><dd><p><span class="math notranslate nohighlight">\(Y_{it}, D_{it} \perp L_{t-1} (Y_i, X_i, D_i) | X_{it}, \xi_i\)</span></p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Selection on observables and omitted time-invariant variables</strong></dt><dd><p><span class="math notranslate nohighlight">\(Y_{it} (.) \perp D_{it} | X_{it}, \xi_i\)</span></p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Homogeneity and linearity of the treatment effect</strong></dt><dd><p><span class="math notranslate nohighlight">\(\mathbb{E} [Y_{it}(d) - Y_{it}(0) | X_{it}, \xi_i] = d \theta_0\)</span></p>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Additive Separability</strong></dt><dd><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp;\mathbb{E} [Y_{it}(0) | X_{it}, \xi_i] = g_1(X_{it}) + \alpha^*_i, &amp; &amp;\alpha^*_i = \alpha^*(\xi_i),\\&amp;\mathbb{E} [D_{it} | X_{it}, \xi_i] = m_1(X_{it}) + \gamma_i, &amp; &amp;\gamma_i = \gamma(\xi_i)\end{aligned}\end{align} \]</div>
</dd>
</dl>
</li>
</ul>
<p><strong>Correlated Random Effect (CRE) Approaches</strong></p>
<p>These approaches convert the fixed-effects model into a random-effects specification using the
Mundlak device <a class="reference external" href="https://doi.org/10.2307/1913646">(Mundlak, 1978)</a>.</p>
<p>Given the set of assumptions, the PLPR model under the CRE approaches take the form</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}Y_{it} = \theta_0 D_{it} + \tilde{g}_1 (X_{it}, \bar{X}_i) + a_i + U_{it},\\D_{it} = \tilde{m}_1(X_{it}, \bar{X}_i) + c_i + V_{it}.\end{aligned}\end{align} \]</div>
<p>For the <em>partialling-out</em> PLPR</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}Y_{it} = \theta_0 V_{it} + \tilde{\ell}_1(X_{it}, \bar{X}_i) + a_i + U_{it},\\V_{it} = D_{it} - \tilde{m}_1(X_{it}, \bar{X}_i) - c_i,\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(a_i\)</span>, <span class="math notranslate nohighlight">\(c_i\)</span> are random effects and covariate unit means
<span class="math notranslate nohighlight">\(\bar{X}_i = T^{-1} \sum_{t=1}^{T} X_{it}\)</span>.</p>
<p><strong>Transformation Approaches</strong></p>
<p>These approaches remove individual heterogeneity from the model by transforming the data.
For some random variable <span class="math notranslate nohighlight">\(W_{it}\)</span>, define the First-Difference (FD) transformation
<span class="math notranslate nohighlight">\(Q(W_{it}) = W_{it} - W_{it-1}\)</span> (for <span class="math notranslate nohighlight">\(t=2, \dots, T\)</span>), and the Within-Group (WG)
transformation <span class="math notranslate nohighlight">\(Q(W_{it}) = X_{it} - \bar{X}_{i}\)</span>, where <span class="math notranslate nohighlight">\(\bar{W}_{i} = T^{-1} \sum_{t=1}^T W_{it}\)</span>.</p>
<p>The PLPR model under the transformation approaches takes the form</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}Q(Y_{it}) = \theta_0 Q(D_{it}) + Q(g_1(X_{it})) + Q(U_{it}),\\Q(D_{it}) = Q(m_1(X_{it})) + Q(V_{it}).\end{aligned}\end{align} \]</div>
<p>For the <em>partialling-out</em> PLPR</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}Q(Y_{it}) = \theta_0 Q(V_{it}) + Q(\ell_1(X_{it})) + Q(U_{it}),\\Q(V_{it}) = Q(D_{it}) - Q(m_1(X_{it})).\end{aligned}\end{align} \]</div>
<p>These transformations remove the fixed effect terms, as <span class="math notranslate nohighlight">\(Q(\alpha_i^*)=Q(\alpha_i)=Q(\gamma_i)=0\)</span>.</p>
<p><strong>Implementation</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">DoubleMLPLPR</span></code> implements the estimation and requires <a class="reference internal" href="data_backend.html#dml-panel-data"><span class="std std-ref">DoubleMLPanelData</span></a> with parameter <code class="docutils literal notranslate"><span class="pre">static_panel=True</span></code> as input.
Unit identifier and time period columns are set with <code class="docutils literal notranslate"><span class="pre">id_col</span></code> and <code class="docutils literal notranslate"><span class="pre">t_col</span></code> in <a class="reference internal" href="data_backend.html#dml-panel-data"><span class="std std-ref">DoubleMLPanelData</span></a>.</p>
<p>The model described in <a class="reference external" href="https://doi.org/10.1093/ectj/utaf011">Clarke and Polselli (2025)</a> uses block-k-fold sample splitting, where the entire time series
of the sampled unit is allocated to one fold to allow for possible serial correlation within each unit, which is often the case for panel data. Furthermore,
cluster robust standard error are employed. <code class="docutils literal notranslate"><span class="pre">DoubleMLPLPR</span></code> implements both of these aspects by using <code class="docutils literal notranslate"><span class="pre">id_col</span></code> as the cluster variable internally, see the example notebook
<a class="reference external" href="https://docs.doubleml.org/stable/examples/py_double_ml_multiway_cluster.html">Python: Cluster Robust Double Machine Learning</a>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">DoubleMLPLPR</span></code> model inlcudes four different estimation approaches. The first two are correlated random effects (CRE) variants, the latter
two are transformation approaches. This can be selected with the <code class="docutils literal notranslate"><span class="pre">approach</span></code> parameter.</p>
<p><code class="docutils literal notranslate"><span class="pre">approach='cre_general'</span></code>:</p>
<ul>
<li><p>Learn <span class="math notranslate nohighlight">\(\tilde{\ell}_1 (X_{it}, \bar{X}_i)\)</span> from <span class="math notranslate nohighlight">\(\{ Y_{it}, X_{it}, \bar{X}_i : t=1,\dots, T \}_{i=1}^N\)</span>,</p></li>
<li><p>First learn <span class="math notranslate nohighlight">\(\tilde{m}_1(X_{it}, \bar{X}_i)\)</span> from <span class="math notranslate nohighlight">\(\{ D_{it}, X_{it}, \bar{X}_i : t=1,\dots, T \}_{i=1}^N\)</span>, with predictions <span class="math notranslate nohighlight">\(\hat{m}_{1,it} = \tilde{m}_1 (X_{it}, \bar{X}_i)\)</span></p>
<ul class="simple">
<li><p>Calculate <span class="math notranslate nohighlight">\(\hat{\bar{m}}_i = T^{-1} \sum_{t=1}^T \hat{m}_{1,it}\)</span>,</p></li>
<li><p>Calculate final nuisance part as <span class="math notranslate nohighlight">\(\hat{m}^*_1 (X_{it}, \bar{X}_i, \bar{D}_i) = \hat{m}_{1,it} + \bar{D}_i - \hat{\bar{m}}_i\)</span>,</p></li>
</ul>
<p>where <span class="math notranslate nohighlight">\(\hat{m}^*_1 (X_{it}, \bar{X}_i, \bar{D}_i) = \mathbb{E}[D_{it} | X_{it}, \bar{X}_i] + c_i\)</span>.</p>
</li>
<li><p><span class="math notranslate nohighlight">\(g_1\)</span> can be learnt iteratively from <span class="math notranslate nohighlight">\(\{ Y_{it}, X_{it}, \bar{X}_i : t=1,\dots, T \}_{i=1}^N\)</span> using estimates for <span class="math notranslate nohighlight">\(\tilde{\ell}_1, \tilde{m}_1\)</span>.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">approach='cre_normal'</span></code></p>
<p>Under the assumption that the conditional distribution <span class="math notranslate nohighlight">\(D_{i1}, \dots, D_{iT} | X_{i1}, \dots X_{iT}\)</span> is multivariate normal (see <a class="reference external" href="https://doi.org/10.1093/ectj/utaf011">Clarke and Polselli (2025)</a> for further details):</p>
<ul class="simple">
<li><p>Learn <span class="math notranslate nohighlight">\(\tilde{\ell}_1 (X_{it}, \bar{X}_i)\)</span> from <span class="math notranslate nohighlight">\(\{ Y_{it}, X_{it}, \bar{X}_i : t=1,\dots, T \}_{i=1}^N\)</span>,</p></li>
<li><p>Learn <span class="math notranslate nohighlight">\(m^*_{1}\)</span> from <span class="math notranslate nohighlight">\(\{ D_{it}, X_{it}, \bar{X}_i, \bar{D}_i: t=1,\dots, T \}_{i=1}^N\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(g_1\)</span> can be learnt iteratively from <span class="math notranslate nohighlight">\(\{ Y_{it}, X_{it}, \bar{X}_i : t=1,\dots, T \}_{i=1}^N\)</span> using estimates for <span class="math notranslate nohighlight">\(\tilde{\ell}_1, \tilde{m}_1\)</span>.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">approach='fd_exact'</span></code></p>
<p>Consider First-Difference (FD) transformation <span class="math notranslate nohighlight">\(Q(W_{it})= W_{it} - W_{it-1}\)</span>, under the assumptions from above,
<a class="reference external" href="https://doi.org/10.1093/ectj/utaf011">Clarke and Polselli (2025)</a> show that <span class="math notranslate nohighlight">\(\mathbb{E}[Y_{it}-Y_{it-1} | X_{it-1},X_{it}] =\Delta \ell_1 (X_{it-1}, X_{it})\)</span> and
<span class="math notranslate nohighlight">\(\mathbb{E}[D_{it}-D_{it-1} | X_{it-1},X_{it}] =\Delta m_1 (X_{it-1}, X_{it})\)</span>. Therefore, the transformed nuisance function can be learnt as</p>
<ul class="simple">
<li><p>Learn <span class="math notranslate nohighlight">\(\Delta \ell_1 (X_{it-1}, X_{it})\)</span> from <span class="math notranslate nohighlight">\(\{ Y_{it}-Y_{it-1}, X_{it-1}, X_{it} : t=2, \dots , T \}_{i=1}^N\)</span>,</p></li>
<li><p>Learn <span class="math notranslate nohighlight">\(\Delta m_1 (X_{it-1}, X_{it})\)</span> from <span class="math notranslate nohighlight">\(\{ D_{it}-D_{it-1}, X_{it-1}, X_{it} : t=2, \dots , T \}_{i=1}^N\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta g_1 (X_{it-1}, X_{it})\)</span> can be learnt iteratively from <span class="math notranslate nohighlight">\(\{ Y_{it}-Y_{it-1}, X_{it-1}, X_{it} : t=2, \dots , T \}_{i=1}^N\)</span> using estimates for <span class="math notranslate nohighlight">\(\Delta \ell_1, \Delta m_1\)</span>.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">approach='wg_approx'</span></code></p>
<p>For the Within-Group (WG) transformation <span class="math notranslate nohighlight">\(Q(W_{it})= W_{it} - \bar{W}_{i}\)</span>, where <span class="math notranslate nohighlight">\(\bar{W}_{i} = T^{-1} \sum_{t=1}^T W_{it}\)</span>.
Approximating the model gives</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
Q(Y_{it}) &amp;\approx \theta_0 Q(D_{it}) + g_1 (Q(X_{it})) + Q(U_{it}), \\
Q(D_{it}) &amp;\approx m_1 (Q(X_{it})) + Q(V_{it}).
\end{align*}\end{split}\]</div>
<p>Similarly,</p>
<div class="math notranslate nohighlight">
\[Q(Y_{it}) \approx \theta_0 Q(V_{it}) + \ell_1 (Q(X_{it})) + Q(U_{it}).\]</div>
<ul class="simple">
<li><p>Learn <span class="math notranslate nohighlight">\(\ell_1\)</span> from transformed data <span class="math notranslate nohighlight">\(\{ Q(Y_{it}), Q(X_{it}) : t=1,\dots,T \}_{i=1}^N\)</span>,</p></li>
<li><p>Learn <span class="math notranslate nohighlight">\(m_1\)</span> from transformed data <span class="math notranslate nohighlight">\(\{ Q(D_{it}), Q(X_{it}) : t=1,\dots,T \}_{i=1}^N\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(g_1\)</span> can be learnt iteratively from <span class="math notranslate nohighlight">\(\{ Q(Y_{it}), Q(X_{it}) : t=1,\dots,T \}_{i=1}^N\)</span>, using estimates for <span class="math notranslate nohighlight">\(\ell_1, m_1\)</span>.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">DoubleMLPLPR</span></code> implements PLPR models. Estimation is conducted via its <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-3" name="sd-tab-set-2" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="py" for="sd-tab-item-3">
Python</label><div class="sd-tab-content docutils">
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [26]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="gp">In [27]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">doubleml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dml</span>

<span class="gp">In [28]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">doubleml.plm.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_plpr_CP2025</span>

<span class="gp">In [29]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LassoCV</span>

<span class="gp">In [30]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">clone</span>

<span class="gp">In [31]: </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">3142</span><span class="p">)</span>

<span class="gp">In [32]: </span><span class="n">learner</span> <span class="o">=</span> <span class="n">LassoCV</span><span class="p">()</span>

<span class="gp">In [33]: </span><span class="n">ml_l</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">learner</span><span class="p">)</span>

<span class="gp">In [34]: </span><span class="n">ml_m</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">learner</span><span class="p">)</span>

<span class="gp">In [35]: </span><span class="n">data</span> <span class="o">=</span> <span class="n">make_plpr_CP2025</span><span class="p">(</span><span class="n">num_id</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> <span class="n">num_t</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">dim_x</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">dgp_type</span><span class="o">=</span><span class="s1">&#39;dgp1&#39;</span><span class="p">)</span>

<span class="gp">In [36]: </span><span class="n">obj_dml_data</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DoubleMLPanelData</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;time&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="n">static_panel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="gp">In [37]: </span><span class="n">dml_plpr_obj</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLPLPR</span><span class="p">(</span><span class="n">obj_dml_data</span><span class="p">,</span> <span class="n">ml_l</span><span class="p">,</span> <span class="n">ml_m</span><span class="p">)</span>

<span class="gp">In [38]: </span><span class="n">dml_plpr_obj</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="gh">Out[38]: </span><span class="go">&lt;doubleml.plm.plpr.DoubleMLPLPR at 0x7feaed459d60&gt;</span>

<span class="gp">In [39]: </span><span class="nb">print</span><span class="p">(</span><span class="n">dml_plpr_obj</span><span class="p">)</span>
<span class="go">================== DoubleMLPLPR Object ==================</span>

<span class="go">------------------ Data Summary      ------------------</span>
<span class="go">Outcome variable: y_diff</span>
<span class="go">Treatment variable(s): [&#39;d_diff&#39;]</span>
<span class="go">Covariates: [&#39;x1&#39;, &#39;x2&#39;, &#39;x3&#39;, &#39;x4&#39;, &#39;x5&#39;, &#39;x6&#39;, &#39;x7&#39;, &#39;x8&#39;, &#39;x9&#39;, &#39;x10&#39;, &#39;x11&#39;, &#39;x12&#39;, &#39;x13&#39;, &#39;x14&#39;, &#39;x15&#39;, &#39;x16&#39;, &#39;x17&#39;, &#39;x18&#39;, &#39;x19&#39;, &#39;x20&#39;, &#39;x21&#39;, &#39;x22&#39;, &#39;x23&#39;, &#39;x24&#39;, &#39;x25&#39;, &#39;x26&#39;, &#39;x27&#39;, &#39;x28&#39;, &#39;x29&#39;, &#39;x30&#39;, &#39;x1_lag&#39;, &#39;x2_lag&#39;, &#39;x3_lag&#39;, &#39;x4_lag&#39;, &#39;x5_lag&#39;, &#39;x6_lag&#39;, &#39;x7_lag&#39;, &#39;x8_lag&#39;, &#39;x9_lag&#39;, &#39;x10_lag&#39;, &#39;x11_lag&#39;, &#39;x12_lag&#39;, &#39;x13_lag&#39;, &#39;x14_lag&#39;, &#39;x15_lag&#39;, &#39;x16_lag&#39;, &#39;x17_lag&#39;, &#39;x18_lag&#39;, &#39;x19_lag&#39;, &#39;x20_lag&#39;, &#39;x21_lag&#39;, &#39;x22_lag&#39;, &#39;x23_lag&#39;, &#39;x24_lag&#39;, &#39;x25_lag&#39;, &#39;x26_lag&#39;, &#39;x27_lag&#39;, &#39;x28_lag&#39;, &#39;x29_lag&#39;, &#39;x30_lag&#39;]</span>
<span class="go">Instrument variable(s): None</span>
<span class="go">Time variable: time</span>
<span class="go">Id variable: id</span>
<span class="go">Static panel data: True</span>
<span class="go">No. Unique Ids: 250</span>
<span class="go">No. Observations: 2250</span>

<span class="go">------------------ Score &amp; Algorithm ------------------</span>
<span class="go">Score function: partialling out</span>
<span class="go">Static panel model approach: fd_exact</span>

<span class="go">------------------ Machine Learner   ------------------</span>
<span class="go">Learner ml_l: LassoCV()</span>
<span class="go">Learner ml_m: LassoCV()</span>
<span class="go">Out-of-sample Performance:</span>
<span class="go">Regression:</span>
<span class="go">Learner ml_l RMSE: [[1.5816666]]</span>
<span class="go">Learner ml_m RMSE: [[1.4005743]]</span>

<span class="go">------------------ Resampling        ------------------</span>
<span class="go">No. folds per cluster: 5</span>
<span class="go">No. folds: 5</span>
<span class="go">No. repeated sample splits: 1</span>

<span class="go">------------------ Fit Summary       ------------------</span>
<span class="go">            coef   std err          t         P&gt;|t|     2.5 %    97.5 %</span>
<span class="go">d_diff  0.511626  0.024615  20.784933  5.924636e-96  0.463381  0.559871</span>

<span class="go">------------------ Additional Information -------------</span>
<span class="go">Cluster variable(s): [&#39;id&#39;]</span>

<span class="go">Pre-Transformation Data Summary: </span>
<span class="go">Outcome variable: y</span>
<span class="go">Treatment variable(s): [&#39;d&#39;]</span>
<span class="go">Covariates: [&#39;x1&#39;, &#39;x2&#39;, &#39;x3&#39;, &#39;x4&#39;, &#39;x5&#39;, &#39;x6&#39;, &#39;x7&#39;, &#39;x8&#39;, &#39;x9&#39;, &#39;x10&#39;, &#39;x11&#39;, &#39;x12&#39;, &#39;x13&#39;, &#39;x14&#39;, &#39;x15&#39;, &#39;x16&#39;, &#39;x17&#39;, &#39;x18&#39;, &#39;x19&#39;, &#39;x20&#39;, &#39;x21&#39;, &#39;x22&#39;, &#39;x23&#39;, &#39;x24&#39;, &#39;x25&#39;, &#39;x26&#39;, &#39;x27&#39;, &#39;x28&#39;, &#39;x29&#39;, &#39;x30&#39;]</span>
<span class="go">No. Observations: 2500</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="partially-linear-iv-regression-model-pliv">
<span id="pliv-model"></span><h3><span class="section-number">3.1.4. </span>Partially linear IV regression model (PLIV)<a class="headerlink" href="#partially-linear-iv-regression-model-pliv" title="Link to this heading">#</a></h3>
<p><strong>Partially linear IV regression (PLIV)</strong> models take the form</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}Y - D \theta_0 =  g_0(X) + \zeta, &amp; &amp;\mathbb{E}(\zeta | Z, X) = 0,\\Z = m_0(X) + V, &amp; &amp;\mathbb{E}(V | X) = 0.\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(Y\)</span> is the outcome variable, <span class="math notranslate nohighlight">\(D\)</span> is the policy variable of interest and <span class="math notranslate nohighlight">\(Z\)</span>
denotes one or multiple instrumental variables. The high-dimensional vector
<span class="math notranslate nohighlight">\(X = (X_1, \ldots, X_p)\)</span> consists of other confounding covariates, and <span class="math notranslate nohighlight">\(\zeta\)</span> and
<span class="math notranslate nohighlight">\(V\)</span> are stochastic errors.</p>
<figure class="align-center" id="id22">
<div class="graphviz"><img src="../_images/graphviz-21b721be23729673da52c6a08d58e43dd1769d11.png" alt="digraph {
     nodesep=1;
     ranksep=1;
     rankdir=LR;
     { node [shape=circle, style=filled]
       Y [fillcolor=&quot;#56B4E9&quot;]
       D [fillcolor=&quot;#56B4E9&quot;]
       Z [fillcolor=&quot;#F0E442&quot;]
       V [fillcolor=&quot;#F0E442&quot;]
       X [fillcolor=&quot;#D55E00&quot;]
     }

     Z -&gt; V [dir=&quot;back&quot;];
     D -&gt; X [dir=&quot;back&quot;];
     Y -&gt; D [dir=&quot;both&quot;];
     X -&gt; Y;
     Z -&gt; X [dir=&quot;back&quot;];
     Z -&gt; D;

     { rank=same; Y D }
     { rank=same; Z X }
         { rank=same; V }
}" class="graphviz" /></div>
<figcaption>
<p><span class="caption-text">Causal diagram</span><a class="headerlink" href="#id22" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><code class="docutils literal notranslate"><span class="pre">DoubleMLPLIV</span></code> implements PLIV models.
Estimation is conducted via its <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-4" name="sd-tab-set-3" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="py" for="sd-tab-item-4">
Python</label><div class="sd-tab-content docutils">
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [40]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="gp">In [41]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">doubleml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dml</span>

<span class="gp">In [42]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">doubleml.plm.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_pliv_CHS2015</span>

<span class="gp">In [43]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span>

<span class="gp">In [44]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">clone</span>

<span class="gp">In [45]: </span><span class="n">learner</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="gp">In [46]: </span><span class="n">ml_l</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">learner</span><span class="p">)</span>

<span class="gp">In [47]: </span><span class="n">ml_m</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">learner</span><span class="p">)</span>

<span class="gp">In [48]: </span><span class="n">ml_r</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">learner</span><span class="p">)</span>

<span class="gp">In [49]: </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2222</span><span class="p">)</span>

<span class="gp">In [50]: </span><span class="n">data</span> <span class="o">=</span> <span class="n">make_pliv_CHS2015</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">n_obs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">dim_x</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">dim_z</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;DataFrame&#39;</span><span class="p">)</span>

<span class="gp">In [51]: </span><span class="n">obj_dml_data</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLData</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">z_cols</span><span class="o">=</span><span class="s1">&#39;Z1&#39;</span><span class="p">)</span>

<span class="gp">In [52]: </span><span class="n">dml_pliv_obj</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLPLIV</span><span class="p">(</span><span class="n">obj_dml_data</span><span class="p">,</span> <span class="n">ml_l</span><span class="p">,</span> <span class="n">ml_m</span><span class="p">,</span> <span class="n">ml_r</span><span class="p">)</span>

<span class="gp">In [53]: </span><span class="nb">print</span><span class="p">(</span><span class="n">dml_pliv_obj</span><span class="o">.</span><span class="n">fit</span><span class="p">())</span>
<span class="go">================== DoubleMLPLIV Object ==================</span>

<span class="go">------------------ Data Summary      ------------------</span>
<span class="go">Outcome variable: y</span>
<span class="go">Treatment variable(s): [&#39;d&#39;]</span>
<span class="go">Covariates: [&#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;X4&#39;, &#39;X5&#39;]</span>
<span class="go">Instrument variable(s): [&#39;Z1&#39;]</span>
<span class="go">No. Observations: 500</span>

<span class="go">------------------ Score &amp; Algorithm ------------------</span>
<span class="go">Score function: partialling out</span>

<span class="go">------------------ Machine Learner   ------------------</span>
<span class="go">Learner ml_l: RandomForestRegressor(max_depth=5, max_features=5, min_samples_leaf=5)</span>
<span class="go">Learner ml_m: RandomForestRegressor(max_depth=5, max_features=5, min_samples_leaf=5)</span>
<span class="go">Learner ml_r: RandomForestRegressor(max_depth=5, max_features=5, min_samples_leaf=5)</span>
<span class="go">Out-of-sample Performance:</span>
<span class="go">Regression:</span>
<span class="go">Learner ml_l RMSE: [[1.44412537]]</span>
<span class="go">Learner ml_m RMSE: [[0.52544332]]</span>
<span class="go">Learner ml_r RMSE: [[1.19962187]]</span>

<span class="go">------------------ Resampling        ------------------</span>
<span class="go">No. folds: 5</span>
<span class="go">No. repeated sample splits: 1</span>

<span class="go">------------------ Fit Summary       ------------------</span>
<span class="go">       coef   std err         t         P&gt;|t|     2.5 %    97.5 %</span>
<span class="go">d  0.474709  0.089459  5.306416  1.118018e-07  0.299372  0.650046</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-5" name="sd-tab-set-3" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="r" for="sd-tab-item-5">
R</label><div class="sd-tab-content docutils">
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">DoubleML</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">mlr3</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">mlr3learners</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">data.table</span><span class="p">)</span>

<span class="n">learner</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">lrn</span><span class="p">(</span><span class="s">&quot;regr.ranger&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">num.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">mtry</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="n">min.node.size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">max.depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span>
<span class="n">ml_l</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">learner</span><span class="o">$</span><span class="nf">clone</span><span class="p">()</span>
<span class="n">ml_m</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">learner</span><span class="o">$</span><span class="nf">clone</span><span class="p">()</span>
<span class="n">ml_r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">learner</span><span class="o">$</span><span class="nf">clone</span><span class="p">()</span>
<span class="nf">set.seed</span><span class="p">(</span><span class="m">2222</span><span class="p">)</span>
<span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">make_pliv_CHS2015</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">,</span><span class="w"> </span><span class="n">n_obs</span><span class="o">=</span><span class="m">500</span><span class="p">,</span><span class="w"> </span><span class="n">dim_x</span><span class="o">=</span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="n">dim_z</span><span class="o">=</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">return_type</span><span class="o">=</span><span class="s">&quot;data.table&quot;</span><span class="p">)</span>
<span class="n">obj_dml_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DoubleMLData</span><span class="o">$</span><span class="nf">new</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">y_col</span><span class="o">=</span><span class="s">&quot;y&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">d_col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;d&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">z_cols</span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Z1&quot;</span><span class="p">)</span>
<span class="n">dml_pliv_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DoubleMLPLIV</span><span class="o">$</span><span class="nf">new</span><span class="p">(</span><span class="n">obj_dml_data</span><span class="p">,</span><span class="w"> </span><span class="n">ml_l</span><span class="p">,</span><span class="w"> </span><span class="n">ml_m</span><span class="p">,</span><span class="w"> </span><span class="n">ml_r</span><span class="p">)</span>
<span class="n">dml_pliv_obj</span><span class="o">$</span><span class="nf">fit</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">dml_pliv_obj</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>================= DoubleMLPLIV Object ==================


------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): d
Covariates: X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, X20
Instrument(s): Z1
Selection variable: 
No. Observations: 500

------------------ Score &amp; algorithm ------------------
Score function: partialling out
DML algorithm: dml2

------------------ Machine learner   ------------------
ml_l: regr.ranger
ml_m: regr.ranger
ml_r: regr.ranger

------------------ Resampling        ------------------
No. folds: 5
No. repeated sample splits: 1
Apply cross-fitting: TRUE

------------------ Fit summary       ------------------
 Estimates and significance testing of the effect of target variables
  Estimate. Std. Error t value Pr(&gt;|t|)    
d   0.66133    0.07796   8.483   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


</pre></div>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="interactive-regression-models-irm">
<span id="irm-models"></span><h2><span class="section-number">3.2. </span>Interactive regression models (IRM)<a class="headerlink" href="#interactive-regression-models-irm" title="Link to this heading">#</a></h2>
<p>The interactive regression model (IRM) take the form</p>
<div class="math notranslate nohighlight">
\[Y = g_0(D, X) + U,\]</div>
<p>where treatment effects are fully heterogeneous.</p>
<section id="binary-interactive-regression-model-irm">
<span id="irm-model"></span><h3><span class="section-number">3.2.1. </span>Binary Interactive Regression Model (IRM)<a class="headerlink" href="#binary-interactive-regression-model-irm" title="Link to this heading">#</a></h3>
<p><strong>Interactive regression (IRM)</strong> models take the form</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}Y = g_0(D, X) + U, &amp; &amp;\mathbb{E}(U | X, D) = 0,\\D = m_0(X) + V, &amp; &amp;\mathbb{E}(V | X) = 0,\end{aligned}\end{align} \]</div>
<p>where the treatment variable is binary, <span class="math notranslate nohighlight">\(D \in \lbrace 0,1 \rbrace\)</span>.
We consider estimation of the average treatment effects when treatment effects are fully heterogeneous.</p>
<p>Target parameters of interest in this model are the average treatment effect (ATE),</p>
<div class="math notranslate nohighlight">
\[\theta_0 = \mathbb{E}[g_0(1, X) - g_0(0,X)]\]</div>
<p>and the average treatment effect of the treated (ATTE),</p>
<div class="math notranslate nohighlight">
\[\theta_0 = \mathbb{E}[g_0(1, X) - g_0(0,X) | D=1].\]</div>
<figure class="align-center" id="id23">
<div class="graphviz"><img src="../_images/graphviz-8852e5db087f49410d0a5212d9b7fdcb58f0aaf9.png" alt="digraph {
     nodesep=1;
     ranksep=1;
     rankdir=LR;
     { node [shape=circle, style=filled]
       Y [fillcolor=&quot;#56B4E9&quot;]
       D [fillcolor=&quot;#F0E442&quot;]
       V [fillcolor=&quot;#F0E442&quot;]
       X [fillcolor=&quot;#D55E00&quot;]
     }
     Y -&gt; D -&gt; V [dir=&quot;back&quot;];
     X -&gt; D;
     Y -&gt; X [dir=&quot;back&quot;];
}" class="graphviz" /></div>
<figcaption>
<p><span class="caption-text">Causal diagram</span><a class="headerlink" href="#id23" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><code class="docutils literal notranslate"><span class="pre">DoubleMLIRM</span></code> implements IRM models.
Estimation is conducted via its <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-6" name="sd-tab-set-4" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="py" for="sd-tab-item-6">
Python</label><div class="sd-tab-content docutils">
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="gp">In [2]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">doubleml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dml</span>

<span class="gp">In [3]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">doubleml.irm.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_irm_data</span>

<span class="gp">In [4]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span><span class="p">,</span> <span class="n">RandomForestClassifier</span>

<span class="gp">In [5]: </span><span class="n">ml_g</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="gp">In [6]: </span><span class="n">ml_m</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="gp">In [7]: </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">3333</span><span class="p">)</span>

<span class="gp">In [8]: </span><span class="n">data</span> <span class="o">=</span> <span class="n">make_irm_data</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">n_obs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">dim_x</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;DataFrame&#39;</span><span class="p">)</span>

<span class="gp">In [9]: </span><span class="n">obj_dml_data</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLData</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">)</span>

<span class="gp">In [10]: </span><span class="n">dml_irm_obj</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLIRM</span><span class="p">(</span><span class="n">obj_dml_data</span><span class="p">,</span> <span class="n">ml_g</span><span class="p">,</span> <span class="n">ml_m</span><span class="p">)</span>

<span class="gp">In [11]: </span><span class="nb">print</span><span class="p">(</span><span class="n">dml_irm_obj</span><span class="o">.</span><span class="n">fit</span><span class="p">())</span>
<span class="go">================== DoubleMLIRM Object ==================</span>

<span class="go">------------------ Data Summary      ------------------</span>
<span class="go">Outcome variable: y</span>
<span class="go">Treatment variable(s): [&#39;d&#39;]</span>
<span class="go">Covariates: [&#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;X4&#39;, &#39;X5&#39;, &#39;X6&#39;, &#39;X7&#39;, &#39;X8&#39;, &#39;X9&#39;, &#39;X10&#39;]</span>
<span class="go">Instrument variable(s): None</span>
<span class="go">No. Observations: 500</span>

<span class="go">------------------ Score &amp; Algorithm ------------------</span>
<span class="go">Score function: ATE</span>

<span class="go">------------------ Machine Learner   ------------------</span>
<span class="go">Learner ml_g: RandomForestRegressor(max_depth=5, max_features=10, min_samples_leaf=2)</span>
<span class="go">Learner ml_m: RandomForestClassifier(max_depth=5, max_features=10, min_samples_leaf=2)</span>
<span class="go">Out-of-sample Performance:</span>
<span class="go">Regression:</span>
<span class="go">Learner ml_g0 RMSE: [[1.07085301]]</span>
<span class="go">Learner ml_g1 RMSE: [[1.09682314]]</span>
<span class="go">Classification:</span>
<span class="go">Learner ml_m Log Loss: [[0.55863386]]</span>

<span class="go">------------------ Resampling        ------------------</span>
<span class="go">No. folds: 5</span>
<span class="go">No. repeated sample splits: 1</span>

<span class="go">------------------ Fit Summary       ------------------</span>
<span class="go">       coef  std err         t     P&gt;|t|     2.5 %    97.5 %</span>
<span class="go">d  0.599297   0.1887  3.175931  0.001494  0.229452  0.969141</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-7" name="sd-tab-set-4" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="r" for="sd-tab-item-7">
R</label><div class="sd-tab-content docutils">
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">DoubleML</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">mlr3</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">mlr3learners</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">data.table</span><span class="p">)</span>

<span class="nf">set.seed</span><span class="p">(</span><span class="m">3333</span><span class="p">)</span>
<span class="n">ml_g</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">lrn</span><span class="p">(</span><span class="s">&quot;regr.ranger&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">num.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">mtry</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">min.node.size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">max.depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span>
<span class="n">ml_m</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">lrn</span><span class="p">(</span><span class="s">&quot;classif.ranger&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">num.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">mtry</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">min.node.size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">max.depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span>
<span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">make_irm_data</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="m">0.5</span><span class="p">,</span><span class="w"> </span><span class="n">n_obs</span><span class="o">=</span><span class="m">500</span><span class="p">,</span><span class="w"> </span><span class="n">dim_x</span><span class="o">=</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">return_type</span><span class="o">=</span><span class="s">&quot;data.table&quot;</span><span class="p">)</span>
<span class="n">obj_dml_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DoubleMLData</span><span class="o">$</span><span class="nf">new</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">y_col</span><span class="o">=</span><span class="s">&quot;y&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">d_cols</span><span class="o">=</span><span class="s">&quot;d&quot;</span><span class="p">)</span>
<span class="n">dml_irm_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DoubleMLIRM</span><span class="o">$</span><span class="nf">new</span><span class="p">(</span><span class="n">obj_dml_data</span><span class="p">,</span><span class="w"> </span><span class="n">ml_g</span><span class="p">,</span><span class="w"> </span><span class="n">ml_m</span><span class="p">)</span>
<span class="n">dml_irm_obj</span><span class="o">$</span><span class="nf">fit</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">dml_irm_obj</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>================= DoubleMLIRM Object ==================


------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): d
Covariates: X1, X2, X3, X4, X5, X6, X7, X8, X9, X10
Instrument(s): 
Selection variable: 
No. Observations: 500

------------------ Score &amp; algorithm ------------------
Score function: ATE
DML algorithm: dml2

------------------ Machine learner   ------------------
ml_g: regr.ranger
ml_m: classif.ranger

------------------ Resampling        ------------------
No. folds: 5
No. repeated sample splits: 1
Apply cross-fitting: TRUE

------------------ Fit summary       ------------------
 Estimates and significance testing of the effect of target variables
  Estimate. Std. Error t value Pr(&gt;|t|)
d    0.3958     0.2651   1.493    0.135


</pre></div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="average-potential-outcomes-apos">
<span id="irm-apo-model"></span><h3><span class="section-number">3.2.2. </span>Average Potential Outcomes (APOs)<a class="headerlink" href="#average-potential-outcomes-apos" title="Link to this heading">#</a></h3>
<p>For general discrete-values treatments <span class="math notranslate nohighlight">\(D \in \lbrace d_0, \dots, d_l \rbrace\)</span> the model can be generalized to</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}Y = g_0(D, X) + U, &amp; &amp;\mathbb{E}(U | X, D) = 0,\\A_j = m_{0,j}(X) + V, &amp; &amp;\mathbb{E}(V | X) = 0,\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(A_j := 1\lbrace D = d_j\rbrace\)</span> is an indicator variable for treatment level <span class="math notranslate nohighlight">\(d_j\)</span> and <span class="math notranslate nohighlight">\(m_{0,j}(X)\)</span> denotes
the corresponding propensity score.</p>
<p>Possible target parameters of interest in this model are the average potential outcomes (APOs)</p>
<div class="math notranslate nohighlight">
\[\theta_{0,j} = \mathbb{E}[g_0(d_j, X)].\]</div>
<p><code class="docutils literal notranslate"><span class="pre">DoubleMLAPO</span></code> implements the estimation of average potential outcomes.
Estimation is conducted via its <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-8" name="sd-tab-set-5" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="py" for="sd-tab-item-8">
Python</label><div class="sd-tab-content docutils">
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [12]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="gp">In [13]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">doubleml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dml</span>

<span class="gp">In [14]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">doubleml.irm.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_irm_data</span>

<span class="gp">In [15]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span><span class="p">,</span> <span class="n">RandomForestClassifier</span>

<span class="gp">In [16]: </span><span class="n">ml_g</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="gp">In [17]: </span><span class="n">ml_m</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="gp">In [18]: </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">3333</span><span class="p">)</span>

<span class="gp">In [19]: </span><span class="n">data</span> <span class="o">=</span> <span class="n">make_irm_data</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">n_obs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">dim_x</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;DataFrame&#39;</span><span class="p">)</span>

<span class="gp">In [20]: </span><span class="n">obj_dml_data</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLData</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">)</span>

<span class="gp">In [21]: </span><span class="n">dml_apo_obj</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLAPO</span><span class="p">(</span><span class="n">obj_dml_data</span><span class="p">,</span> <span class="n">ml_g</span><span class="p">,</span> <span class="n">ml_m</span><span class="p">,</span> <span class="n">treatment_level</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="gp">In [22]: </span><span class="nb">print</span><span class="p">(</span><span class="n">dml_apo_obj</span><span class="o">.</span><span class="n">fit</span><span class="p">())</span>
<span class="go">================== DoubleMLAPO Object ==================</span>

<span class="go">------------------ Data Summary      ------------------</span>
<span class="go">Outcome variable: y</span>
<span class="go">Treatment variable(s): [&#39;d&#39;]</span>
<span class="go">Covariates: [&#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;X4&#39;, &#39;X5&#39;, &#39;X6&#39;, &#39;X7&#39;, &#39;X8&#39;, &#39;X9&#39;, &#39;X10&#39;]</span>
<span class="go">Instrument variable(s): None</span>
<span class="go">No. Observations: 500</span>

<span class="go">------------------ Score &amp; Algorithm ------------------</span>
<span class="go">Score function: APO</span>

<span class="go">------------------ Machine Learner   ------------------</span>
<span class="go">Learner ml_g: RandomForestRegressor(max_depth=5, max_features=10, min_samples_leaf=2)</span>
<span class="go">Learner ml_m: RandomForestClassifier(max_depth=5, max_features=10, min_samples_leaf=2)</span>
<span class="go">Out-of-sample Performance:</span>
<span class="go">Regression:</span>
<span class="go">Learner ml_g_d_lvl0 RMSE: [[1.09351167]]</span>
<span class="go">Learner ml_g_d_lvl1 RMSE: [[1.07085301]]</span>
<span class="go">Classification:</span>
<span class="go">Learner ml_m Log Loss: [[0.55863386]]</span>

<span class="go">------------------ Resampling        ------------------</span>
<span class="go">No. folds: 5</span>
<span class="go">No. repeated sample splits: 1</span>

<span class="go">------------------ Fit Summary       ------------------</span>
<span class="go">      coef   std err         t     P&gt;|t|     2.5 %    97.5 %</span>
<span class="go">d -0.13084  0.137165 -0.953884  0.340142 -0.399679  0.137999</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="average-potential-outcomes-apos-for-multiple-treatment-levels">
<span id="irm-apos-model"></span><h3><span class="section-number">3.2.3. </span>Average Potential Outcomes (APOs) for Multiple Treatment Levels<a class="headerlink" href="#average-potential-outcomes-apos-for-multiple-treatment-levels" title="Link to this heading">#</a></h3>
<p>If multiple treatment levels should be estimated simulatenously, another possible target parameter of interest in this model
are contrasts (or average treatment effects) between treatment levels <span class="math notranslate nohighlight">\(d_j\)</span> and <span class="math notranslate nohighlight">\(d_k\)</span>:</p>
<div class="math notranslate nohighlight">
\[\theta_{0,jk} = \mathbb{E}[g_0(d_j, X) - g_0(d_k, X)].\]</div>
<p><code class="docutils literal notranslate"><span class="pre">DoubleMLAPOS</span></code> implements the estimation of average potential outcomes for multiple treatment levels.
Estimation is conducted via its <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method. The <code class="docutils literal notranslate"><span class="pre">causal_contrast()</span></code> method allows to estimate causal contrasts between treatment levels:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-9" name="sd-tab-set-6" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="py" for="sd-tab-item-9">
Python</label><div class="sd-tab-content docutils">
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [23]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="gp">In [24]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">doubleml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dml</span>

<span class="gp">In [25]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">doubleml.irm.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_irm_data</span>

<span class="gp">In [26]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span><span class="p">,</span> <span class="n">RandomForestClassifier</span>

<span class="gp">In [27]: </span><span class="n">ml_g</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="gp">In [28]: </span><span class="n">ml_m</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="gp">In [29]: </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">3333</span><span class="p">)</span>

<span class="gp">In [30]: </span><span class="n">data</span> <span class="o">=</span> <span class="n">make_irm_data</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">n_obs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">dim_x</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;DataFrame&#39;</span><span class="p">)</span>

<span class="gp">In [31]: </span><span class="n">obj_dml_data</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLData</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">)</span>

<span class="gp">In [32]: </span><span class="n">dml_apos_obj</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLAPOS</span><span class="p">(</span><span class="n">obj_dml_data</span><span class="p">,</span> <span class="n">ml_g</span><span class="p">,</span> <span class="n">ml_m</span><span class="p">,</span> <span class="n">treatment_levels</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="gp">In [33]: </span><span class="nb">print</span><span class="p">(</span><span class="n">dml_apos_obj</span><span class="o">.</span><span class="n">fit</span><span class="p">())</span>
<span class="go">================== DoubleMLAPOS Object ==================</span>

<span class="go">------------------ Fit summary       ------------------</span>
<span class="go">       coef   std err         t     P&gt;|t|     2.5 %    97.5 %</span>
<span class="go">0 -0.152706  0.186689 -0.817967  0.413376 -0.518610  0.213199</span>
<span class="go">1  0.533283  0.134784  3.956588  0.000076  0.269112  0.797454</span>

<span class="gp">In [34]: </span><span class="n">causal_contrast_model</span> <span class="o">=</span> <span class="n">dml_apos_obj</span><span class="o">.</span><span class="n">causal_contrast</span><span class="p">(</span><span class="n">reference_levels</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="gp">In [35]: </span><span class="nb">print</span><span class="p">(</span><span class="n">causal_contrast_model</span><span class="o">.</span><span class="n">summary</span><span class="p">)</span>
<span class="go">            coef   std err         t     P&gt;|t|     2.5 %   97.5 %</span>
<span class="go">1 vs 0  0.685989  0.231734  2.960236  0.003074  0.231798  1.14018</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="interactive-iv-model-iivm">
<span id="iivm-model"></span><h3><span class="section-number">3.2.4. </span>Interactive IV model (IIVM)<a class="headerlink" href="#interactive-iv-model-iivm" title="Link to this heading">#</a></h3>
<p><strong>Interactive IV regression (IIVM)</strong> models take the form</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}Y = \ell_0(D, X) + \zeta, &amp; &amp;\mathbb{E}(\zeta | Z, X) = 0,\\Z = m_0(X) + V, &amp; &amp;\mathbb{E}(V | X) = 0,\end{aligned}\end{align} \]</div>
<p>where the treatment variable is binary, <span class="math notranslate nohighlight">\(D \in \lbrace 0,1 \rbrace\)</span>
and the instrument is binary, <span class="math notranslate nohighlight">\(Z \in \lbrace 0,1 \rbrace\)</span>.
Consider the functions <span class="math notranslate nohighlight">\(g_0\)</span>, <span class="math notranslate nohighlight">\(r_0\)</span> and <span class="math notranslate nohighlight">\(m_0\)</span>, where <span class="math notranslate nohighlight">\(g_0\)</span> maps the support of <span class="math notranslate nohighlight">\((Z,X)\)</span> to
<span class="math notranslate nohighlight">\(\mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(r_0\)</span> and <span class="math notranslate nohighlight">\(m_0\)</span> respectively map the support of <span class="math notranslate nohighlight">\((Z,X)\)</span> and <span class="math notranslate nohighlight">\(X\)</span> to
<span class="math notranslate nohighlight">\((\varepsilon, 1-\varepsilon)\)</span> for some <span class="math notranslate nohighlight">\(\varepsilon \in (0, 1/2)\)</span>, such that</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}Y = g_0(Z, X) + \nu, &amp; &amp;\mathbb{E}(\nu | Z, X) = 0,\\D = r_0(Z, X) + U, &amp; &amp;\mathbb{E}(U | Z, X) = 0,\\Z = m_0(X) + V, &amp; &amp;\mathbb{E}(V | X) = 0.\end{aligned}\end{align} \]</div>
<p>The target parameter of interest in this model is the local average treatment effect (LATE),</p>
<div class="math notranslate nohighlight">
\[\theta_0 = \frac{\mathbb{E}[g_0(1, X)] - \mathbb{E}[g_0(0,X)]}{\mathbb{E}[r_0(1, X)] - \mathbb{E}[r_0(0,X)]}.\]</div>
<figure class="align-center" id="id24">
<div class="graphviz"><img src="../_images/graphviz-21b721be23729673da52c6a08d58e43dd1769d11.png" alt="digraph {
     nodesep=1;
     ranksep=1;
     rankdir=LR;
     { node [shape=circle, style=filled]
       Y [fillcolor=&quot;#56B4E9&quot;]
       D [fillcolor=&quot;#56B4E9&quot;]
       Z [fillcolor=&quot;#F0E442&quot;]
       V [fillcolor=&quot;#F0E442&quot;]
       X [fillcolor=&quot;#D55E00&quot;]
     }

     Z -&gt; V [dir=&quot;back&quot;];
     D -&gt; X [dir=&quot;back&quot;];
     Y -&gt; D [dir=&quot;both&quot;];
     X -&gt; Y;
     Z -&gt; X [dir=&quot;back&quot;];
     Z -&gt; D;

     { rank=same; Y D }
     { rank=same; Z X }
         { rank=same; V }
}" class="graphviz" /></div>
<figcaption>
<p><span class="caption-text">Causal diagram</span><a class="headerlink" href="#id24" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><code class="docutils literal notranslate"><span class="pre">DoubleMLIIVM</span></code> implements IIVM models.
Estimation is conducted via its <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-10" name="sd-tab-set-7" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="py" for="sd-tab-item-10">
Python</label><div class="sd-tab-content docutils">
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [36]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="gp">In [37]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">doubleml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dml</span>

<span class="gp">In [38]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">doubleml.irm.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_iivm_data</span>

<span class="gp">In [39]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span><span class="p">,</span> <span class="n">RandomForestClassifier</span>

<span class="gp">In [40]: </span><span class="n">ml_g</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="gp">In [41]: </span><span class="n">ml_m</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="gp">In [42]: </span><span class="n">ml_r</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="gp">In [43]: </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">4444</span><span class="p">)</span>

<span class="gp">In [44]: </span><span class="n">data</span> <span class="o">=</span> <span class="n">make_iivm_data</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">n_obs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">dim_x</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">alpha_x</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;DataFrame&#39;</span><span class="p">)</span>

<span class="gp">In [45]: </span><span class="n">obj_dml_data</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLData</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">z_cols</span><span class="o">=</span><span class="s1">&#39;z&#39;</span><span class="p">)</span>

<span class="gp">In [46]: </span><span class="n">dml_iivm_obj</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLIIVM</span><span class="p">(</span><span class="n">obj_dml_data</span><span class="p">,</span> <span class="n">ml_g</span><span class="p">,</span> <span class="n">ml_m</span><span class="p">,</span> <span class="n">ml_r</span><span class="p">)</span>

<span class="gp">In [47]: </span><span class="nb">print</span><span class="p">(</span><span class="n">dml_iivm_obj</span><span class="o">.</span><span class="n">fit</span><span class="p">())</span>
<span class="go">================== DoubleMLIIVM Object ==================</span>

<span class="go">------------------ Data Summary      ------------------</span>
<span class="go">Outcome variable: y</span>
<span class="go">Treatment variable(s): [&#39;d&#39;]</span>
<span class="go">Covariates: [&#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;X4&#39;, &#39;X5&#39;, &#39;X6&#39;, &#39;X7&#39;, &#39;X8&#39;, &#39;X9&#39;, &#39;X10&#39;, &#39;X11&#39;, &#39;X12&#39;, &#39;X13&#39;, &#39;X14&#39;, &#39;X15&#39;, &#39;X16&#39;, &#39;X17&#39;, &#39;X18&#39;, &#39;X19&#39;, &#39;X20&#39;]</span>
<span class="go">Instrument variable(s): [&#39;z&#39;]</span>
<span class="go">No. Observations: 1000</span>

<span class="go">------------------ Score &amp; Algorithm ------------------</span>
<span class="go">Score function: LATE</span>

<span class="go">------------------ Machine Learner   ------------------</span>
<span class="go">Learner ml_g: RandomForestRegressor(max_depth=5, max_features=20, min_samples_leaf=2)</span>
<span class="go">Learner ml_m: RandomForestClassifier(max_depth=5, max_features=20, min_samples_leaf=2)</span>
<span class="go">Learner ml_r: RandomForestClassifier(max_depth=5, max_features=20, min_samples_leaf=2)</span>
<span class="go">Out-of-sample Performance:</span>
<span class="go">Regression:</span>
<span class="go">Learner ml_g0 RMSE: [[1.12983057]]</span>
<span class="go">Learner ml_g1 RMSE: [[1.13102231]]</span>
<span class="go">Classification:</span>
<span class="go">Learner ml_m Log Loss: [[0.69684828]]</span>
<span class="go">Learner ml_r0 Log Loss: [[0.69505403]]</span>
<span class="go">Learner ml_r1 Log Loss: [[0.43503345]]</span>

<span class="go">------------------ Resampling        ------------------</span>
<span class="go">No. folds: 5</span>
<span class="go">No. repeated sample splits: 1</span>

<span class="go">------------------ Fit Summary       ------------------</span>
<span class="go">       coef   std err         t     P&gt;|t|    2.5 %    97.5 %</span>
<span class="go">d  0.389186  0.231165  1.683582  0.092262 -0.06389  0.842262</span>

<span class="go">------------------ Additional Information -------------</span>
<span class="go">Robust Confidence Set: [-0.0935, 0.8325]</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-11" name="sd-tab-set-7" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="r" for="sd-tab-item-11">
R</label><div class="sd-tab-content docutils">
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">DoubleML</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">mlr3</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">mlr3learners</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">data.table</span><span class="p">)</span>

<span class="nf">set.seed</span><span class="p">(</span><span class="m">4444</span><span class="p">)</span>
<span class="n">ml_g</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">lrn</span><span class="p">(</span><span class="s">&quot;regr.ranger&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">num.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">mtry</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="n">min.node.size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">max.depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span>
<span class="n">ml_m</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">lrn</span><span class="p">(</span><span class="s">&quot;classif.ranger&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">num.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">mtry</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="n">min.node.size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">max.depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span>
<span class="n">ml_r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ml_m</span><span class="o">$</span><span class="nf">clone</span><span class="p">()</span>
<span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">make_iivm_data</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="m">0.5</span><span class="p">,</span><span class="w"> </span><span class="n">n_obs</span><span class="o">=</span><span class="m">1000</span><span class="p">,</span><span class="w"> </span><span class="n">dim_x</span><span class="o">=</span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="n">alpha_x</span><span class="o">=</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">return_type</span><span class="o">=</span><span class="s">&quot;data.table&quot;</span><span class="p">)</span>
<span class="n">obj_dml_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DoubleMLData</span><span class="o">$</span><span class="nf">new</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">y_col</span><span class="o">=</span><span class="s">&quot;y&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">d_cols</span><span class="o">=</span><span class="s">&quot;d&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">z_cols</span><span class="o">=</span><span class="s">&quot;z&quot;</span><span class="p">)</span>
<span class="n">dml_iivm_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DoubleMLIIVM</span><span class="o">$</span><span class="nf">new</span><span class="p">(</span><span class="n">obj_dml_data</span><span class="p">,</span><span class="w"> </span><span class="n">ml_g</span><span class="p">,</span><span class="w"> </span><span class="n">ml_m</span><span class="p">,</span><span class="w"> </span><span class="n">ml_r</span><span class="p">)</span>
<span class="n">dml_iivm_obj</span><span class="o">$</span><span class="nf">fit</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">dml_iivm_obj</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>================= DoubleMLIIVM Object ==================


------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): d
Covariates: x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20
Instrument(s): z
Selection variable: 
No. Observations: 1000

------------------ Score &amp; algorithm ------------------
Score function: LATE
DML algorithm: dml2

------------------ Machine learner   ------------------
ml_g: regr.ranger
ml_m: classif.ranger
ml_r: classif.ranger

------------------ Resampling        ------------------
No. folds: 5
No. repeated sample splits: 1
Apply cross-fitting: TRUE

------------------ Fit summary       ------------------
 Estimates and significance testing of the effect of target variables
  Estimate. Std. Error t value Pr(&gt;|t|)  
d    0.3568     0.1988   1.794   0.0727 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


</pre></div>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="difference-in-differences-models-did">
<span id="did-models"></span><h2><span class="section-number">3.3. </span>Difference-in-Differences Models (DID)<a class="headerlink" href="#difference-in-differences-models-did" title="Link to this heading">#</a></h2>
<p><strong>Difference-in-Differences Models (DID)</strong> implemented in the package focus on the the binary treatment case with staggered adoption.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The notation and identifying assumptions are based on <a class="reference external" href="https://doi.org/10.1016/j.jeconom.2020.12.001">Callaway and Sant’Anna (2021)</a>, but adjusted to better fit into the general package documentation conventions, sometimes slightly abusing notation.
The underlying score functions are based on <a class="reference external" href="https://doi.org/10.1016/j.jeconom.2020.06.003">Sant’Anna and Zhao (2020)</a>, <a class="reference external" href="https://arxiv.org/abs/1809.01643">Zimmert (2018)</a> and <a class="reference external" href="https://doi.org/10.1093/ectj/utaa001">Chang (2020)</a>.
For a more detailed introduction and recent developments of the difference-in-differences literature see e.g. <a class="reference external" href="https://arxiv.org/abs/2201.01194">Roth et al. (2022)</a>.</p>
</div>
<p>We consider <span class="math notranslate nohighlight">\(n\)</span> observed units at time periods <span class="math notranslate nohighlight">\(t=1,\dots, \mathcal{T}\)</span>.
The treatment status for unit <span class="math notranslate nohighlight">\(i\)</span> at time period <span class="math notranslate nohighlight">\(t\)</span> is denoted by the binary variable <span class="math notranslate nohighlight">\(D_{i,t}=1\)</span>. The package considers the staggered adoption setting,
where a unit stays treated after it has been treated once (<em>Irreversibility of Treatment</em>).</p>
<p>Let <span class="math notranslate nohighlight">\(G^{\mathrm{g}}_i\)</span> be an indicator variable that takes value one if unit <span class="math notranslate nohighlight">\(i\)</span> is treated at time period <span class="math notranslate nohighlight">\(t=\mathrm{g}\)</span>, <span class="math notranslate nohighlight">\(G^{\mathrm{g}}_i=1\{G_i=\mathrm{g}\}\)</span> with <span class="math notranslate nohighlight">\(G_i\)</span> refering to the first post-treatment period.
I units are never exposed to the treatment, define <span class="math notranslate nohighlight">\(G_i=\infty\)</span>.</p>
<p>The target parameters are defined in terms of differences in potential outcomes. The observed and potential outcome for each unit <span class="math notranslate nohighlight">\(i\)</span> at time period <span class="math notranslate nohighlight">\(t\)</span> are assumed to be of the form</p>
<div class="math notranslate nohighlight">
\[Y_{i,t} = Y_{i,t}(0) + \sum_{\mathrm{g}=2}^{\mathcal{T}} (Y_{i,t}(\mathrm{g}) - Y_{i,t}(0)) \cdot G^{\mathrm{g}}_i,\]</div>
<p>such that we observe one consistent potential outcome for each unit at each time period.</p>
<p>The corresponding target parameters are the average causal effects of the treatment</p>
<div class="math notranslate nohighlight">
\[ATT(\mathrm{g},t):= \mathbb{E}[Y_{i,t}(\mathrm{g}) - Y_{i,t}(0)|G^{\mathrm{g}}_i=1].\]</div>
<p>This target parameter quantifies the average change in potential outcomes for units that are treated the first time in period <span class="math notranslate nohighlight">\(\mathrm{g}\)</span> with the difference in outcome being evaluated for time period <span class="math notranslate nohighlight">\(t\)</span>.
The corresponding control groups, defined by an indicator <span class="math notranslate nohighlight">\(C\)</span>, can be typically set as either the <em>never treated</em> or <em>not yet treated</em> units.
Let</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
C_{i,t}^{(\text{nev})} \equiv C_{i}^{(\text{nev})} &amp;:= 1\{G_i=\infty\} \quad \text{(never treated)}, \\
C_{i,t}^{(\text{nyt})} &amp;:= 1\{G_i &gt; t\} \quad \text{(not yet treated)}.
\end{align}\end{split}\]</div>
<p>The corresponding identifying assumptions are:</p>
<ol class="arabic">
<li><dl class="simple">
<dt><strong>Irreversibility of Treatment:</strong></dt><dd><p><span class="math notranslate nohighlight">\(D_{i,1} = 0 \quad a.s.\)</span>
For all <span class="math notranslate nohighlight">\(t=2,\dots,\mathcal{T}\)</span>, <span class="math notranslate nohighlight">\(D_{i,t-1} = 1\)</span> implies <span class="math notranslate nohighlight">\(D_{i,t} = 1 \quad a.s.\)</span></p>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Data:</strong></dt><dd><p>The observed data are generated according to the following mechanisms:</p>
<p>a. <strong>Panel Data (Random Sampling):</strong>
The sample <span class="math notranslate nohighlight">\((Y_{i,1},\dots, Y_{i,\mathcal{T}}, X_i, D_{i,1}, \dots, D_{i,\mathcal{T}})_{i=1}^n\)</span> is independent and identically distributed.</p>
<p>b. <strong>Repeated Cross Sections:</strong>
The sample consists of <span class="math notranslate nohighlight">\((Y_{i,t},G^{2}_i,\dots,G^{\mathcal{T}}_i, C_i,T_i, X_i)_{i=1}^n\)</span>, where <span class="math notranslate nohighlight">\(T_i\in \{1,\dots,\mathcal{T}\}\)</span> denotes the time period of unit <span class="math notranslate nohighlight">\(i\)</span> being observed.
Conditional on <span class="math notranslate nohighlight">\(T=t\)</span>, the data are independent and identically distributed from the distribution of <span class="math notranslate nohighlight">\((Y_{t},G^{2},\dots,G^{\mathcal{T}}, C, X)\)</span>, with <span class="math notranslate nohighlight">\((G^{2},\dots,G^{\mathcal{T}}, C, X)\)</span> being invariant to <span class="math notranslate nohighlight">\(T\)</span>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Limited Treatment Anticipation:</strong></dt><dd><p>There is a known <span class="math notranslate nohighlight">\(\delta\ge 0\)</span> such that
<span class="math notranslate nohighlight">\(\mathbb{E}[Y_{i,t}(\mathrm{g})|X_i, G_i^{\mathrm{g}}=1] = \mathbb{E}[Y_{i,t}(0)|X_i, G_i^{\mathrm{g}}=1]\quad a.s.\)</span> for all <span class="math notranslate nohighlight">\(\mathrm{g}\in\mathcal{G}, t\in\{1,\dots,\mathcal{T}\}\)</span> such that <span class="math notranslate nohighlight">\(t&lt; \mathrm{g}-\delta\)</span>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Conditional Parallel Trends:</strong></dt><dd><p>Let <span class="math notranslate nohighlight">\(\delta\)</span> be defined as in Assumption 3.\
For each <span class="math notranslate nohighlight">\(\mathrm{g}\in\mathcal{G}\)</span> and <span class="math notranslate nohighlight">\(t\in\{2,\dots,\mathcal{T}\}\)</span> such that <span class="math notranslate nohighlight">\(t\ge \mathrm{g}-\delta\)</span>:</p>
<ol class="loweralpha simple">
<li><dl class="simple">
<dt><strong>Never Treated:</strong></dt><dd><p><span class="math notranslate nohighlight">\(\mathbb{E}[Y_{i,t}(0) - Y_{i,t-1}(0)|X_i, G_i^{\mathrm{g}}=1] = \mathbb{E}[Y_{i,t}(0) - Y_{i,t-1}(0)|X_i,C_{i}^{(\text{nev})}=1] \quad a.s.\)</span></p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Not Yet Treated:</strong></dt><dd><p><span class="math notranslate nohighlight">\(\mathbb{E}[Y_{i,t}(0) - Y_{i,t-1}(0)|X_i, G_i^{\mathrm{g}}=1] = \mathbb{E}[Y_{i,t}(0) - Y_{i,t-1}(0)|X_i,C_{i,t+\delta}^{(\text{nyt})}=1] \quad a.s.\)</span></p>
</dd>
</dl>
</li>
</ol>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Overlap:</strong></dt><dd><p>For each time period <span class="math notranslate nohighlight">\(t=2,\dots,\mathcal{T}\)</span> and <span class="math notranslate nohighlight">\(\mathrm{g}\in\mathcal{G}\)</span> there exists a <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span> such that
<span class="math notranslate nohighlight">\(P(G_i^{\mathrm{g}}=1) &gt; \epsilon\)</span> and <span class="math notranslate nohighlight">\(P(G_i^{\mathrm{g}}=1|X_i, G_i^{\mathrm{g}} + C_{i,t}^{(\text{nyt})}=1) &lt; 1-\epsilon\quad a.s.\)</span></p>
</dd>
</dl>
</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For a detailed discussion of the assumptions see <a class="reference external" href="https://doi.org/10.1016/j.jeconom.2020.12.001">Callaway and Sant’Anna (2021)</a>.</p>
</div>
<p>Under the assumptions above (either Assumption a. or b.), the target parameter <span class="math notranslate nohighlight">\(ATT(\mathrm{g},t)\)</span> is identified see Theorem 1. <a class="reference external" href="https://doi.org/10.1016/j.jeconom.2020.12.001">Callaway and Sant’Anna (2021)</a>.</p>
<section id="parameters-implementation">
<span id="did-implementation-model"></span><h3><span class="section-number">3.3.1. </span>Parameters &amp; Implementation<a class="headerlink" href="#parameters-implementation" title="Link to this heading">#</a></h3>
<p>To estimate the target parameter <span class="math notranslate nohighlight">\(ATT(\mathrm{g},t_\text{eval})\)</span>, the implementation (both for panel data or repeated cross sections) is based on the following parameters:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathrm{g}\)</span> is the first post-treatment period of interest, i.e. the treatment group.</p></li>
<li><p><span class="math notranslate nohighlight">\(t_\text{pre}\)</span> is the pre-treatment period, i.e. the time period from which the conditional parallel trends are assumed.</p></li>
<li><p><span class="math notranslate nohighlight">\(t_\text{eval}\)</span> is the time period of interest or evaluation period, i.e. the time period where the treatment effect is evaluated.</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta\)</span> is number of anticipation periods, i.e. the number of time periods for which units are assumed to anticipate the treatment.</p></li>
</ul>
<p>Under the assumptions above the target parameter <span class="math notranslate nohighlight">\(ATT(\mathrm{g},t_\text{eval})\)</span> can be estimated by choosing a suitable combination
of <span class="math notranslate nohighlight">\((\mathrm{g}, t_\text{pre}, t_\text{eval}, \delta)\)</span> if <span class="math notranslate nohighlight">\(t_\text{eval} - t_\text{pre} \ge 1 + \delta\)</span>, i.e. the parallel trends are assumed to hold at least one period more than the anticipation period.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The choice <span class="math notranslate nohighlight">\(t_\text{pre}= \min(\mathrm{g},t_\text{eval}) -\delta-1\)</span> corresponds to the definition of <span class="math notranslate nohighlight">\(ATT_{dr}(\mathrm{g},t_\text{eval};\delta)\)</span> from <a class="reference external" href="https://doi.org/10.1016/j.jeconom.2020.12.001">Callaway and Sant’Anna (2021)</a>.</p>
<p>As an example, if the target parameter is the effect on the group receiving treatment in <span class="math notranslate nohighlight">\(2006\)</span> but evaluated in <span class="math notranslate nohighlight">\(2007\)</span> with an anticipation period of <span class="math notranslate nohighlight">\(\delta=1\)</span>, then the pre-treatment period is <span class="math notranslate nohighlight">\(2004\)</span>.
The parallel trend assumption is slightly stronger with anticipation as the trends have to parallel for a longer periods, i.e. <span class="math notranslate nohighlight">\(ATT_{dr}(2006,2007;1)=ATT(2006,2004;2006)\)</span>.</p>
</div>
<p>In the following, we will omit the subscript <span class="math notranslate nohighlight">\(\delta\)</span> in the notation of the nuisance functions and the control group (implicitly assuming <span class="math notranslate nohighlight">\(\delta=0\)</span>).</p>
<p>For a given tuple <span class="math notranslate nohighlight">\((\mathrm{g}, t_\text{pre}, t_\text{eval})\)</span> the target parameter <span class="math notranslate nohighlight">\(ATT(\mathrm{g},t)\)</span> is estimated by solving the empirical version of the the following linear moment condition:</p>
<div class="math notranslate nohighlight">
\[ATT(\mathrm{g}, t_\text{pre}, t_\text{eval}):= -\frac{\mathbb{E}[\psi_b(W,\eta_0)]}{\mathbb{E}[\psi_a(W,\eta_0)]}\]</div>
<p>with nuisance elements <span class="math notranslate nohighlight">\(\eta_0\)</span> which depend on the parameter combination <span class="math notranslate nohighlight">\((\mathrm{g}, t_\text{pre}, t_\text{eval})\)</span> and score function <span class="math notranslate nohighlight">\(\psi(W,\theta, \eta)\)</span> (for details, see <a class="reference internal" href="#did-pa-model"><span class="std std-ref">Panel Data Details</span></a> or <a class="reference internal" href="#did-cs-model"><span class="std std-ref">Repeated Cross-Section Details</span></a>).
Under the identifying assumptions above</p>
<div class="math notranslate nohighlight">
\[ATT(\mathrm{g}, t_\text{pre}, t_\text{eval}) = ATT(\mathrm{g},t).\]</div>
<p><code class="docutils literal notranslate"><span class="pre">DoubleMLDIDMulti</span></code> implements the estimation of <span class="math notranslate nohighlight">\(ATT(\mathrm{g}, t_\text{pre}, t_\text{eval})\)</span> for multiple time periods and requires <a class="reference internal" href="data_backend.html#dml-panel-data"><span class="std std-ref">DoubleMLPanelData</span></a> as input.</p>
<p>Setting <code class="docutils literal notranslate"><span class="pre">gt_combinations='standard'</span></code> will estimate the target parameter for all (possible) combinations of <span class="math notranslate nohighlight">\((\mathrm{g}, t_\text{pre}, t_\text{eval})\)</span> with <span class="math notranslate nohighlight">\(\mathrm{g}\in\{2,\dots,\mathcal{T}\}\)</span> and <span class="math notranslate nohighlight">\((t_\text{pre}, t_\text{eval})\)</span> with <span class="math notranslate nohighlight">\(t_\text{eval}\in\{2,\dots,\mathcal{T}\}\)</span> and
<span class="math notranslate nohighlight">\(t_\text{pre}= \min(\mathrm{g},t_\text{eval}) -\delta-1\)</span>.
This corresponds to the setting where all trends are set as short as possible, but still respecting the anticipation period.</p>
</section>
<section id="panel-data">
<span id="did-pa-model"></span><h3><span class="section-number">3.3.2. </span>Panel data<a class="headerlink" href="#panel-data" title="Link to this heading">#</a></h3>
<p>For the estimation of the target parameters <span class="math notranslate nohighlight">\(ATT(\mathrm{g},t)\)</span> the following nuisance functions are required:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
g_{0, \mathrm{g}, t_\text{pre}, t_\text{eval}, \delta}(X_i) &amp;:= \mathbb{E}[Y_{i,t_\text{eval}} - Y_{i,t_\text{pre}}|X_i, C_{i,t_\text{eval} + \delta}^{(\cdot)} = 1], \\
m_{0, \mathrm{g}, t_\text{eval} + \delta}(X_i) &amp;:= P(G_i^{\mathrm{g}}=1|X_i, G_i^{\mathrm{g}} + C_{i,t_\text{eval} + \delta}^{(\cdot)}=1).
\end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(g_{0, \mathrm{g}, t_\text{pre}, t_\text{eval},\delta}(\cdot)\)</span> denotes the population outcome change regression function and <span class="math notranslate nohighlight">\(m_{0, \mathrm{g}, t_\text{eval} + \delta}(\cdot)\)</span> the generalized propensity score.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Remark that the nuisance functions depend on the control group used for the estimation of the target parameter.
By slight abuse of notation we use the same notation for both control groups <span class="math notranslate nohighlight">\(C_{i,t}^{(\text{nev})}\)</span> and <span class="math notranslate nohighlight">\(C_{i,t}^{(\text{nyt})}\)</span>. More specifically, the
control group only depends on <span class="math notranslate nohighlight">\(\delta\)</span> for <em>not yet treated</em> units.</p>
</div>
<p>For a given tuple <span class="math notranslate nohighlight">\((\mathrm{g}, t_\text{pre}, t_\text{eval})\)</span> the target parameter <span class="math notranslate nohighlight">\(ATT(\mathrm{g},t)\)</span> is estimated by solving the empirical version of the the following linear moment condition:</p>
<div class="math notranslate nohighlight">
\[ATT(\mathrm{g}, t_\text{pre}, t_\text{eval}):= -\frac{\mathbb{E}[\psi_b(W,\eta_0)]}{\mathbb{E}[\psi_a(W,\eta_0)]}\]</div>
<p>with nuisance elements <span class="math notranslate nohighlight">\(\eta_0=(g_{0, \mathrm{g}, t_\text{pre}, t_\text{eval}}, m_{0, \mathrm{g}, t_\text{eval}})\)</span> and score function <span class="math notranslate nohighlight">\(\psi(W,\theta, \eta)\)</span> being defined in the <a class="reference internal" href="scores.html#did-pa-score"><span class="std std-ref">DiD Score Section</span></a>.</p>
<p>Estimation is conducted via its <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-12" name="sd-tab-set-8" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="py" for="sd-tab-item-12">
Python</label><div class="sd-tab-content docutils">
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="gp">In [2]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">doubleml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dml</span>

<span class="gp">In [3]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">doubleml.did.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_did_CS2021</span>

<span class="gp">In [4]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span><span class="p">,</span> <span class="n">RandomForestClassifier</span>

<span class="gp">In [5]: </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="gp">In [6]: </span><span class="n">df</span> <span class="o">=</span> <span class="n">make_did_CS2021</span><span class="p">(</span><span class="n">n_obs</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>

<span class="gp">In [7]: </span><span class="n">dml_data</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DoubleMLPanelData</span><span class="p">(</span>
<span class="gp">   ...: </span>    <span class="n">df</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">y_col</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">d_cols</span><span class="o">=</span><span class="s2">&quot;d&quot;</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">id_col</span><span class="o">=</span><span class="s2">&quot;id&quot;</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">t_col</span><span class="o">=</span><span class="s2">&quot;t&quot;</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">x_cols</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Z1&quot;</span><span class="p">,</span> <span class="s2">&quot;Z2&quot;</span><span class="p">,</span> <span class="s2">&quot;Z3&quot;</span><span class="p">,</span> <span class="s2">&quot;Z4&quot;</span><span class="p">],</span>
<span class="gp">   ...: </span>    <span class="n">datetime_unit</span><span class="o">=</span><span class="s2">&quot;M&quot;</span>
<span class="gp">   ...: </span><span class="p">)</span>
<span class="gp">   ...: </span>

<span class="gp">In [8]: </span><span class="n">dml_did_obj</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">did</span><span class="o">.</span><span class="n">DoubleMLDIDMulti</span><span class="p">(</span>
<span class="gp">   ...: </span>    <span class="n">obj_dml_data</span><span class="o">=</span><span class="n">dml_data</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">ml_g</span><span class="o">=</span><span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">min_samples_split</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
<span class="gp">   ...: </span>    <span class="n">ml_m</span><span class="o">=</span><span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">min_samples_split</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
<span class="gp">   ...: </span>    <span class="n">gt_combinations</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">control_group</span><span class="o">=</span><span class="s2">&quot;never_treated&quot;</span><span class="p">,</span>
<span class="gp">   ...: </span><span class="p">)</span>
<span class="gp">   ...: </span>

<span class="gp">In [9]: </span><span class="nb">print</span><span class="p">(</span><span class="n">dml_did_obj</span><span class="o">.</span><span class="n">fit</span><span class="p">())</span>
<span class="go">================== DoubleMLDIDMulti Object ==================</span>

<span class="go">------------------ Data summary      ------------------</span>
<span class="go">Outcome variable: y</span>
<span class="go">Treatment variable(s): [&#39;d&#39;]</span>
<span class="go">Covariates: [&#39;Z1&#39;, &#39;Z2&#39;, &#39;Z3&#39;, &#39;Z4&#39;]</span>
<span class="go">Instrument variable(s): None</span>
<span class="go">Time variable: t</span>
<span class="go">Id variable: id</span>
<span class="go">Static panel data: False</span>
<span class="go">No. Unique Ids: 500</span>
<span class="go">No. Observations: 2500</span>
<span class="go">------------------ Score &amp; algorithm ------------------</span>
<span class="go">Score function: observational</span>
<span class="go">Control group: never_treated</span>
<span class="go">Anticipation periods: 0</span>

<span class="go">------------------ Machine learner   ------------------</span>
<span class="go">Learner ml_g: RandomForestRegressor(min_samples_split=10)</span>
<span class="go">Learner ml_m: RandomForestClassifier(min_samples_split=10)</span>
<span class="go">Out-of-sample Performance:</span>
<span class="go">Regression:</span>
<span class="go">Learner ml_g0 RMSE: [[ 3.78240482  3.93965493  6.92248736 10.42365741  3.84609957  3.89460366</span>
<span class="go">   3.96896106  7.47699623  3.73766741  3.869787    3.72369688  3.89814202]]</span>
<span class="go">Learner ml_g1 RMSE: [[3.29001936 3.36274369 6.55021688 9.79228725 3.81850929 3.47805578</span>
<span class="go">  3.60013741 6.97436783 3.99883095 3.55346908 3.7992264  4.02008393]]</span>
<span class="go">Classification:</span>
<span class="go">Learner ml_m Log Loss: [[0.75466617 0.72330647 0.73602094 0.73949306 0.72993015 0.69404895</span>
<span class="go">  0.72017804 0.67913613 0.75899873 0.73649108 0.76023347 0.7271188 ]]</span>

<span class="go">------------------ Resampling        ------------------</span>
<span class="go">No. folds: 5</span>
<span class="go">No. repeated sample splits: 1</span>

<span class="go">------------------ Fit summary       ------------------</span>
<span class="go">                                  coef   std err  ...     2.5 %    97.5 %</span>
<span class="go">ATT(2025-03,2025-01,2025-02) -0.797078  0.434393  ... -1.648472  0.054315</span>
<span class="go">ATT(2025-03,2025-02,2025-03)  0.664108  0.501988  ... -0.319771  1.647986</span>
<span class="go">ATT(2025-03,2025-02,2025-04)  1.293611  0.881254  ... -0.433614  3.020837</span>
<span class="go">ATT(2025-03,2025-02,2025-05)  1.019904  1.261031  ... -1.451671  3.491479</span>
<span class="go">ATT(2025-04,2025-01,2025-02) -0.292843  0.431639  ... -1.138841  0.553154</span>
<span class="go">ATT(2025-04,2025-02,2025-03) -0.139659  0.466269  ... -1.053530  0.774212</span>
<span class="go">ATT(2025-04,2025-03,2025-04)  1.428612  0.477834  ...  0.492074  2.365150</span>
<span class="go">ATT(2025-04,2025-03,2025-05)  2.053898  0.782147  ...  0.520918  3.586878</span>
<span class="go">ATT(2025-05,2025-01,2025-02) -0.182826  0.454513  ... -1.073654  0.708003</span>
<span class="go">ATT(2025-05,2025-02,2025-03)  0.281980  0.445238  ... -0.590670  1.154630</span>
<span class="go">ATT(2025-05,2025-03,2025-04)  0.095835  0.439257  ... -0.765092  0.956762</span>
<span class="go">ATT(2025-05,2025-04,2025-05)  1.394554  0.481344  ...  0.451137  2.337970</span>

<span class="go">[12 rows x 6 columns]</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Remark that the output contains two different outcome regressions <span class="math notranslate nohighlight">\(g(0,X)\)</span> and <span class="math notranslate nohighlight">\(g(1,X)\)</span>. As in the <a class="reference internal" href="#irm-model"><span class="std std-ref">IRM model</span></a>
the outcome regression <span class="math notranslate nohighlight">\(g(0,X)\)</span> refers to the control group, whereas <span class="math notranslate nohighlight">\(g(1,X)\)</span> refers to the outcome change regression for the treatment group, i.e.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
g(0,X) &amp;\approx g_{0, \mathrm{g}, t_\text{pre}, t_\text{eval}, \delta}(X_i) = \mathbb{E}[Y_{i,t_\text{eval}} - Y_{i,t_\text{pre}}|X_i, C_{i,t_\text{eval} + \delta}^{(\cdot)} = 1],\\
g(1,X) &amp;\approx \mathbb{E}[Y_{i,t_\text{eval}} - Y_{i,t_\text{pre}}|X_i, G_i^{\mathrm{g}} = 1].
\end{align}\end{split}\]</div>
<p>Further, <span class="math notranslate nohighlight">\(g(1,X)\)</span> is only required for <a class="reference internal" href="sensitivity.html#sensitivity-did-pa"><span class="std std-ref">Sensitivity Analysis</span></a> and is not used for the estimation of the target parameter.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A more detailed example is available in the <a class="reference internal" href="../examples/index.html#did-examplegallery"><span class="std std-ref">Example Gallery</span></a>.</p>
</div>
</section>
<section id="repeated-cross-sections">
<span id="did-cs-model"></span><h3><span class="section-number">3.3.3. </span>Repeated cross-sections<a class="headerlink" href="#repeated-cross-sections" title="Link to this heading">#</a></h3>
<p>For the estimation of the target parameters <span class="math notranslate nohighlight">\(ATT(\mathrm{g},t)\)</span> the following nuisance functions are required:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
g^{\text{treat}}_{0,\mathrm{g}, t, \text{eval} + \delta}(X_i) &amp;:= \mathbb{E}[Y_{i,t} |X_i, G_i^{\mathrm{g}}=1, T_i=t], \\
g^{\text{control}}_{0,\mathrm{g}, t, \text{eval} + \delta}(X_i) &amp;:= \mathbb{E}[Y_{i,t} |X_i, C_{i,t_\text{eval} + \delta}^{(\cdot)}=1, T_i=t], \\
m_{0, \mathrm{g}, t_\text{eval} + \delta}(X_i) &amp;:= P(G_i^{\mathrm{g}}=1|X_i, G_i^{\mathrm{g}} + C_{i,t_\text{eval} + \delta}^{(\cdot)}=1).
\end{align}\end{split}\]</div>
<p>for <span class="math notranslate nohighlight">\(t\in\{t_\text{pre}, t_\text{eval}\}\)</span>.
Here, <span class="math notranslate nohighlight">\(g^{(\cdot)}_{\mathrm{g}, t, \text{eval} + \delta}(\cdot)\)</span> denotes the population outcome regression function (for either treatment or control group at time period <span class="math notranslate nohighlight">\(t\)</span>) and <span class="math notranslate nohighlight">\(m_{0, \mathrm{g}, t_\text{eval} + \delta}(\cdot)\)</span> the generalized propensity score.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Remark that the nuisance functions depend on the control group used for the estimation of the target parameter.
By slight abuse of notation we use the same notation for both control groups <span class="math notranslate nohighlight">\(C_{i,t}^{(\text{nev})}\)</span> and <span class="math notranslate nohighlight">\(C_{i,t}^{(\text{nyt})}\)</span>. More specifically, the
control group only depends on <span class="math notranslate nohighlight">\(\delta\)</span> for <em>not yet treated</em> units.</p>
</div>
<p>For a given tuple <span class="math notranslate nohighlight">\((\mathrm{g}, t_\text{pre}, t_\text{eval})\)</span> the target parameter <span class="math notranslate nohighlight">\(ATT(\mathrm{g},t)\)</span> is estimated by solving the empirical version of the the following linear moment condition:</p>
<div class="math notranslate nohighlight">
\[ATT(\mathrm{g}, t_\text{pre}, t_\text{eval}):= -\frac{\mathbb{E}[\psi_b(W,\eta_0)]}{\mathbb{E}[\psi_a(W,\eta_0)]}\]</div>
<p>with nuisance elements <span class="math notranslate nohighlight">\(\eta_0=(g^{0,\text{treat}}_{\mathrm{g}, t_\text{pre}, t_\text{eval} + \delta}, g^{0,\text{control}}_{\mathrm{g}, t_\text{pre}, t_\text{eval} + \delta}, g^{0,\text{treat}}_{\mathrm{g}, t_\text{eval}, t_\text{eval} + \delta}, g^{0,\text{control}}_{\mathrm{g}, t_\text{eval}, t_\text{eval} + \delta}, m_{0, \mathrm{g}, t_\text{eval}})\)</span> and score function <span class="math notranslate nohighlight">\(\psi(W,\theta, \eta)\)</span> defined in the <a class="reference internal" href="scores.html#did-cs-score"><span class="std std-ref">DiD Score Section</span></a>.</p>
<p>Setting <code class="docutils literal notranslate"><span class="pre">panel=False</span></code> will estimate the target parameter for repeated cross sections. Estimation is conducted via its <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-13" name="sd-tab-set-9" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="py" for="sd-tab-item-13">
Python</label><div class="sd-tab-content docutils">
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="gp">In [2]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">doubleml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dml</span>

<span class="gp">In [3]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">doubleml.did.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_did_cs_CS2021</span>

<span class="gp">In [4]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span><span class="p">,</span> <span class="n">RandomForestClassifier</span>

<span class="gp">In [5]: </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="gp">In [6]: </span><span class="n">df</span> <span class="o">=</span> <span class="n">make_did_cs_CS2021</span><span class="p">(</span><span class="n">n_obs</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>

<span class="gp">In [7]: </span><span class="n">dml_data</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DoubleMLPanelData</span><span class="p">(</span>
<span class="gp">   ...: </span>    <span class="n">df</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">y_col</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">d_cols</span><span class="o">=</span><span class="s2">&quot;d&quot;</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">id_col</span><span class="o">=</span><span class="s2">&quot;id&quot;</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">t_col</span><span class="o">=</span><span class="s2">&quot;t&quot;</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">x_cols</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Z1&quot;</span><span class="p">,</span> <span class="s2">&quot;Z2&quot;</span><span class="p">,</span> <span class="s2">&quot;Z3&quot;</span><span class="p">,</span> <span class="s2">&quot;Z4&quot;</span><span class="p">],</span>
<span class="gp">   ...: </span>    <span class="n">datetime_unit</span><span class="o">=</span><span class="s2">&quot;M&quot;</span>
<span class="gp">   ...: </span><span class="p">)</span>
<span class="gp">   ...: </span>

<span class="gp">In [8]: </span><span class="n">dml_did_obj</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">did</span><span class="o">.</span><span class="n">DoubleMLDIDMulti</span><span class="p">(</span>
<span class="gp">   ...: </span>    <span class="n">obj_dml_data</span><span class="o">=</span><span class="n">dml_data</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">ml_g</span><span class="o">=</span><span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">min_samples_split</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
<span class="gp">   ...: </span>    <span class="n">ml_m</span><span class="o">=</span><span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">min_samples_split</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
<span class="gp">   ...: </span>    <span class="n">gt_combinations</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">control_group</span><span class="o">=</span><span class="s2">&quot;never_treated&quot;</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">panel</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="gp">   ...: </span><span class="p">)</span>
<span class="gp">   ...: </span>

<span class="gp">In [9]: </span><span class="nb">print</span><span class="p">(</span><span class="n">dml_did_obj</span><span class="o">.</span><span class="n">fit</span><span class="p">())</span>
<span class="go">================== DoubleMLDIDMulti Object ==================</span>

<span class="go">------------------ Data summary      ------------------</span>
<span class="go">Outcome variable: y</span>
<span class="go">Treatment variable(s): [&#39;d&#39;]</span>
<span class="go">Covariates: [&#39;Z1&#39;, &#39;Z2&#39;, &#39;Z3&#39;, &#39;Z4&#39;]</span>
<span class="go">Instrument variable(s): None</span>
<span class="go">Time variable: t</span>
<span class="go">Id variable: id</span>
<span class="go">Static panel data: False</span>
<span class="go">No. Unique Ids: 975</span>
<span class="go">No. Observations: 2575</span>
<span class="go">------------------ Score &amp; algorithm ------------------</span>
<span class="go">Score function: observational</span>
<span class="go">Control group: never_treated</span>
<span class="go">Anticipation periods: 0</span>

<span class="go">------------------ Machine learner   ------------------</span>
<span class="go">Learner ml_g: RandomForestRegressor(min_samples_split=10)</span>
<span class="go">Learner ml_m: RandomForestClassifier(min_samples_split=10)</span>
<span class="go">Out-of-sample Performance:</span>
<span class="go">Regression:</span>
<span class="go">Learner ml_g_d0_t0 RMSE: [[ 3.82372418  8.03968904  7.9497455   7.51233332  3.6236967   7.64117907</span>
<span class="go">   8.84014899  9.39454997  3.69850969  7.28448202  9.24197136 14.70793174]]</span>
<span class="go">Learner ml_g_d0_t1 RMSE: [[ 7.61250258  8.7889255  14.44404522 19.58859711  7.87484243  9.13993416</span>
<span class="go">  14.56521754 20.16118746  7.78980657  9.53291002 13.88415932 17.87640238]]</span>
<span class="go">Learner ml_g_d1_t0 RMSE: [[ 3.9289008   7.24835916  7.09218894  7.53148135  3.23124913  8.16747866</span>
<span class="go">  11.02708816 11.23689448  3.38402797  7.09174584 10.84813353 16.13355896]]</span>
<span class="go">Learner ml_g_d1_t1 RMSE: [[ 6.90678968 11.20877105 14.01900305 15.86975793  8.52714188 10.56915009</span>
<span class="go">  12.06278811 15.5607573   7.68183347  9.11684973 16.45613995 18.27431071]]</span>
<span class="go">Classification:</span>
<span class="go">Learner ml_m Log Loss: [[0.56633984 0.5780788  0.60712561 0.58631757 0.60337339 0.59172611</span>
<span class="go">  0.61448202 0.60883043 0.55390846 0.55662098 0.59769369 0.57002385]]</span>

<span class="go">------------------ Resampling        ------------------</span>
<span class="go">No. folds: 5</span>
<span class="go">No. repeated sample splits: 1</span>

<span class="go">------------------ Fit summary       ------------------</span>
<span class="go">                                   coef   std err  ...     2.5 %     97.5 %</span>
<span class="go">ATT(2025-03,2025-01,2025-02)  -2.062167  1.352047  ... -4.712130   0.587796</span>
<span class="go">ATT(2025-03,2025-02,2025-03)   1.556333  2.009763  ... -2.382730   5.495396</span>
<span class="go">ATT(2025-03,2025-02,2025-04)   3.536658  2.441567  ... -1.248725   8.322041</span>
<span class="go">ATT(2025-03,2025-02,2025-05)  10.086666  3.435192  ...  3.353813  16.819519</span>
<span class="go">ATT(2025-04,2025-01,2025-02)  -0.942777  1.219287  ... -3.332535   1.446981</span>
<span class="go">ATT(2025-04,2025-02,2025-03)   0.989364  2.087370  ... -3.101805   5.080533</span>
<span class="go">ATT(2025-04,2025-03,2025-04)  -0.228648  2.630290  ... -5.383921   4.926626</span>
<span class="go">ATT(2025-04,2025-03,2025-05)   4.732706  3.426133  ... -1.982392  11.447803</span>
<span class="go">ATT(2025-05,2025-01,2025-02)  -0.060672  1.104913  ... -2.226262   2.104917</span>
<span class="go">ATT(2025-05,2025-02,2025-03)   1.145082  1.528255  ... -1.850242   4.140406</span>
<span class="go">ATT(2025-05,2025-03,2025-04)  -1.332028  2.230951  ... -5.704613   3.040556</span>
<span class="go">ATT(2025-05,2025-04,2025-05)   0.484545  3.096257  ... -5.584007   6.553097</span>

<span class="go">[12 rows x 6 columns]</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Remark that the output contains four different outcome regressions <span class="math notranslate nohighlight">\(g(d,t, X)\)</span> for <span class="math notranslate nohighlight">\(d,t\in\{0,1\}\)</span> . As in the <a class="reference internal" href="#irm-model"><span class="std std-ref">IRM model</span></a>
the outcome regression with <span class="math notranslate nohighlight">\(d=0\)</span> refers to the control group, whereas <span class="math notranslate nohighlight">\(t=0\)</span> refers to the pre-treatment period, i.e.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
g(0,0,X) &amp;\approx g^{\text{control}}_{0,\mathrm{g}, t_\text{pre}, \text{eval} + \delta}(X_i) = \mathbb{E}[Y_{i,t_\text{pre}} |X_i, C_{i,t_\text{eval} + \delta}^{(\cdot)}=1, T_i=t_\text{pre}],\\
g(0,1,X) &amp;\approx g^{\text{control}}_{0,\mathrm{g}, t_\text{eval}, \text{eval} + \delta}(X_i) = \mathbb{E}[Y_{i,t_\text{eval}} |X_i, C_{i,t_\text{eval} + \delta}^{(\cdot)}=1, T_i=t_\text{eval}],\\
g(1,0,X) &amp;\approx g^{\text{treat}}_{0,\mathrm{g}, t_\text{pre}, \text{eval} + \delta}(X_i) = \mathbb{E}[Y_{i,t_\text{pre}} |X_i, G_i^{\mathrm{g}}=1,, T_i=t_\text{pre}],\\
g(1,1,X) &amp;\approx g^{\text{treat}}_{0,\mathrm{g}, t_\text{eval}, \text{eval} + \delta}(X_i) = \mathbb{E}[Y_{i,t_\text{eval}} |X_i, G_i^{\mathrm{g}}=1,, T_i=t_\text{eval}].
\end{align}\end{split}\]</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A more detailed example is available in the <a class="reference internal" href="../examples/index.html#did-examplegallery"><span class="std std-ref">Example Gallery</span></a>.</p>
</div>
</section>
<section id="effect-aggregation">
<span id="did-aggregation"></span><h3><span class="section-number">3.3.4. </span>Effect Aggregation<a class="headerlink" href="#effect-aggregation" title="Link to this heading">#</a></h3>
<p>The following section considers the aggregation of different <span class="math notranslate nohighlight">\(ATT(\mathrm{g},t)\)</span> to summary measures based on <a class="reference external" href="https://doi.org/10.1016/j.jeconom.2020.12.001">Callaway and Sant’Anna (2021)</a>.
All implemented aggregation schemes take the form of a weighted average of the <span class="math notranslate nohighlight">\(ATT(\mathrm{g},t)\)</span> estimates</p>
<div class="math notranslate nohighlight">
\[\theta = \sum_{\mathrm{g}\in \mathcal{G}} \sum_{t=2}^{\mathcal{T}} \omega(\mathrm{g},t) \cdot ATT(\mathrm{g},t)\]</div>
<p>where <span class="math notranslate nohighlight">\(\omega(\mathrm{g},t)\)</span> is a weight function based on the treatment group <span class="math notranslate nohighlight">\(\mathrm{g}\)</span> and time period <span class="math notranslate nohighlight">\(t\)</span>.
The aggragation schemes are implmented via the <code class="docutils literal notranslate"><span class="pre">aggregate()</span></code> method of the <code class="docutils literal notranslate"><span class="pre">DoubleMLDIDMulti</span></code> class.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-14" name="sd-tab-set-10" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="py" for="sd-tab-item-14">
Python</label><div class="sd-tab-content docutils">
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="gp">In [2]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">doubleml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dml</span>

<span class="gp">In [3]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">doubleml.did.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_did_CS2021</span>

<span class="gp">In [4]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span><span class="p">,</span> <span class="n">RandomForestClassifier</span>

<span class="gp">In [5]: </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="gp">In [6]: </span><span class="n">df</span> <span class="o">=</span> <span class="n">make_did_CS2021</span><span class="p">(</span><span class="n">n_obs</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>

<span class="gp">In [7]: </span><span class="n">dml_data</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DoubleMLPanelData</span><span class="p">(</span>
<span class="gp">   ...: </span>    <span class="n">df</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">y_col</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">d_cols</span><span class="o">=</span><span class="s2">&quot;d&quot;</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">id_col</span><span class="o">=</span><span class="s2">&quot;id&quot;</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">t_col</span><span class="o">=</span><span class="s2">&quot;t&quot;</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">x_cols</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Z1&quot;</span><span class="p">,</span> <span class="s2">&quot;Z2&quot;</span><span class="p">,</span> <span class="s2">&quot;Z3&quot;</span><span class="p">,</span> <span class="s2">&quot;Z4&quot;</span><span class="p">],</span>
<span class="gp">   ...: </span>    <span class="n">datetime_unit</span><span class="o">=</span><span class="s2">&quot;M&quot;</span>
<span class="gp">   ...: </span><span class="p">)</span>
<span class="gp">   ...: </span>

<span class="gp">In [8]: </span><span class="n">dml_did_obj</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">did</span><span class="o">.</span><span class="n">DoubleMLDIDMulti</span><span class="p">(</span>
<span class="gp">   ...: </span>    <span class="n">obj_dml_data</span><span class="o">=</span><span class="n">dml_data</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">ml_g</span><span class="o">=</span><span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">min_samples_split</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
<span class="gp">   ...: </span>    <span class="n">ml_m</span><span class="o">=</span><span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">min_samples_split</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
<span class="gp">   ...: </span>    <span class="n">gt_combinations</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
<span class="gp">   ...: </span>    <span class="n">control_group</span><span class="o">=</span><span class="s2">&quot;never_treated&quot;</span><span class="p">,</span>
<span class="gp">   ...: </span><span class="p">)</span>
<span class="gp">   ...: </span>

<span class="gp">In [9]: </span><span class="n">dml_did_obj</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="gh">Out[9]: </span><span class="go">&lt;doubleml.did.did_multi.DoubleMLDIDMulti at 0x7feaec08cb60&gt;</span>

<span class="gp">In [10]: </span><span class="n">agg_did_obj</span> <span class="o">=</span> <span class="n">dml_did_obj</span><span class="o">.</span><span class="n">aggregate</span><span class="p">(</span><span class="n">aggregation</span><span class="o">=</span><span class="s2">&quot;group&quot;</span><span class="p">)</span>

<span class="gp">In [11]: </span><span class="n">agg_did_obj</span><span class="o">.</span><span class="n">aggregated_frameworks</span><span class="o">.</span><span class="n">bootstrap</span><span class="p">()</span>
<span class="gh">Out[11]: </span><span class="go">&lt;doubleml.double_ml_framework.DoubleMLFramework at 0x7feafd9db860&gt;</span>

<span class="gp">In [12]: </span><span class="nb">print</span><span class="p">(</span><span class="n">agg_did_obj</span><span class="p">)</span>
<span class="go">================== DoubleMLDIDAggregation Object ==================</span>
<span class="go"> Group Aggregation </span>

<span class="go">------------------ Overall Aggregated Effects ------------------</span>
<span class="go">    coef  std err       t    P&gt;|t|   2.5 %   97.5 %</span>
<span class="go">1.368191 0.490882 2.78721 0.005316 0.40608 2.330301</span>
<span class="go">------------------ Aggregated Effects         ------------------</span>
<span class="go">             coef   std err         t     P&gt;|t|     2.5 %    97.5 %</span>
<span class="go">2025-03  0.992541  0.833510  1.190796  0.233734 -0.641109  2.626192</span>
<span class="go">2025-04  1.741255  0.592124  2.940693  0.003275  0.580713  2.901797</span>
<span class="go">2025-05  1.394554  0.481344  2.897208  0.003765  0.451137  2.337970</span>
<span class="go">------------------ Additional Information     ------------------</span>
<span class="go">Score function: observational</span>
<span class="go">Control group: never_treated</span>
<span class="go">Anticipation periods: 0</span>
</pre></div>
</div>
</div>
</div>
<p>The method <code class="docutils literal notranslate"><span class="pre">aggregate()</span></code> requires the <code class="docutils literal notranslate"><span class="pre">aggregation</span></code> argument to be set to one of the following values:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'group'</span></code>: aggregates <span class="math notranslate nohighlight">\(ATT(\mathrm{g},t)\)</span> estimates by the treatment group <span class="math notranslate nohighlight">\(\mathrm{g}\)</span>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'time'</span></code>: aggregates <span class="math notranslate nohighlight">\(ATT(\mathrm{g},t)\)</span> estimates by the time period <span class="math notranslate nohighlight">\(t\)</span> (based on group size).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'eventstudy'</span></code>: aggregates <span class="math notranslate nohighlight">\(ATT(\mathrm{g},t)\)</span> estimates based on time difference to first treatment assignment like an event study (based on group size).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dictionary</span></code>: a dictionary with values containing the aggregation weights (as <code class="docutils literal notranslate"><span class="pre">numpy.ma.MaskedArray</span></code>).</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Remark that <code class="docutils literal notranslate"><span class="pre">'time'</span></code> and <code class="docutils literal notranslate"><span class="pre">'eventstudy'</span></code> aggregation use internal group reweighting according to the total group size (e.g. the group decomposition should be relatively stable over time, as assumed in Assumption 2).
It can be helpful to check the aggregation weights as in the <a class="reference internal" href="../examples/index.html#did-examplegallery"><span class="std std-ref">example gallery</span></a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A more detailed example on effect aggregation is available in the <a class="reference internal" href="../examples/index.html#did-examplegallery"><span class="std std-ref">example gallery</span></a>.
For a detailed discussion on different aggregation schemes, we refer to of <a class="reference external" href="https://doi.org/10.1016/j.jeconom.2020.12.001">Callaway and Sant’Anna (2021)</a>.</p>
</div>
</section>
<section id="two-treatment-periods">
<span id="did-binary-model"></span><h3><span class="section-number">3.3.5. </span>Two treatment periods<a class="headerlink" href="#two-treatment-periods" title="Link to this heading">#</a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This documentation refers to the deprecated implementation for two time periods.
This functionality will be removed in a future version.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We recommend using the implementation <a class="reference internal" href="#did-pa-model"><span class="std std-ref">Panel data</span></a> and <a class="reference internal" href="#did-cs-model"><span class="std std-ref">Repeated cross-sections</span></a>.</p>
</div>
<p><strong>Difference-in-Differences Models (DID)</strong> implemented in the package focus on the the binary treatment case with
with two treatment periods.</p>
<p>Adopting the notation from <a class="reference external" href="https://doi.org/10.1016/j.jeconom.2020.06.003">Sant’Anna and Zhao (2020)</a>,
let <span class="math notranslate nohighlight">\(Y_{it}\)</span> be the outcome of interest for unit <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>. Further, let <span class="math notranslate nohighlight">\(D_{it}=1\)</span> indicate
if unit <span class="math notranslate nohighlight">\(i\)</span> is treated before time <span class="math notranslate nohighlight">\(t\)</span> (otherwise <span class="math notranslate nohighlight">\(D_{it}=0\)</span>). Since all units start as untreated (<span class="math notranslate nohighlight">\(D_{i0}=0\)</span>), define
<span class="math notranslate nohighlight">\(D_{i}=D_{i1}.\)</span> Relying on the potential outcome notation, denote <span class="math notranslate nohighlight">\(Y_{it}(0)\)</span> as the outcome of unit <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> if the unit did not receive
treatment up until time <span class="math notranslate nohighlight">\(t\)</span> and analogously for <span class="math notranslate nohighlight">\(Y_{it}(1)\)</span> with treatment. Consequently, the observed outcome
for unit is <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> is <span class="math notranslate nohighlight">\(Y_{it}=D_{it} Y_{it}(1) + (1-D_{it}) Y_{it}(0)\)</span>. Further, let
<span class="math notranslate nohighlight">\(X_i\)</span> be a vector of pre-treatment covariates.</p>
<p>Target parameter of interest is the average treatment effect on the treated (ATTE)</p>
<div class="math notranslate nohighlight">
\[\theta_0 = \mathbb{E}[Y_{i1}(1)- Y_{i1}(0)|D_i=1].\]</div>
<p>The corresponding identifying assumptions are</p>
<ul class="simple">
<li><p><strong>(Cond.) Parallel Trends:</strong> <span class="math notranslate nohighlight">\(\mathbb{E}[Y_{i1}(0) - Y_{i0}(0)|X_i, D_i=1] = \mathbb{E}[Y_{i1}(0) - Y_{i0}(0)|X_i, D_i=0]\quad a.s.\)</span></p></li>
<li><p><strong>Overlap:</strong> <span class="math notranslate nohighlight">\(\exists\epsilon &gt; 0\)</span>: <span class="math notranslate nohighlight">\(P(D_i=1) &gt; \epsilon\)</span> and <span class="math notranslate nohighlight">\(P(D_i=1|X_i) \le 1-\epsilon\quad a.s.\)</span></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For a more detailed introduction and recent developments of the difference-in-differences literature see e.g. <a class="reference external" href="https://arxiv.org/abs/2201.01194">Roth et al. (2022)</a>.</p>
</div>
<section id="id15">
<h4><span class="section-number">3.3.5.1. </span>Panel Data<a class="headerlink" href="#id15" title="Link to this heading">#</a></h4>
<p>If panel data are available, the observations are assumed to be iid. of form <span class="math notranslate nohighlight">\((Y_{i0}, Y_{i1}, D_i, X_i)\)</span>.
Remark that the difference <span class="math notranslate nohighlight">\(\Delta Y_i= Y_{i1}-Y_{i0}\)</span> has to be defined as the outcome <code class="docutils literal notranslate"><span class="pre">y</span></code> in the <code class="docutils literal notranslate"><span class="pre">DoubleMLData</span></code> object.</p>
<p><code class="docutils literal notranslate"><span class="pre">DoubleMLIDID</span></code> implements difference-in-differences models for panel data.
Estimation is conducted via its <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-15" name="sd-tab-set-11" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="py" for="sd-tab-item-15">
Python</label><div class="sd-tab-content docutils">
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="gp">In [2]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">doubleml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dml</span>

<span class="gp">In [3]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">doubleml.did.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_did_SZ2020</span>

<span class="gp">In [4]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span><span class="p">,</span> <span class="n">RandomForestClassifier</span>

<span class="gp">In [5]: </span><span class="n">ml_g</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="gp">In [6]: </span><span class="n">ml_m</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="gp">In [7]: </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="gp">In [8]: </span><span class="n">data</span> <span class="o">=</span> <span class="n">make_did_SZ2020</span><span class="p">(</span><span class="n">n_obs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;DataFrame&#39;</span><span class="p">)</span>

<span class="go"># y is already defined as the difference of observed outcomes</span>
<span class="gp">In [9]: </span><span class="n">obj_dml_data</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLDIDData</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">)</span>

<span class="gp">In [10]: </span><span class="n">dml_did_obj</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLDID</span><span class="p">(</span><span class="n">obj_dml_data</span><span class="p">,</span> <span class="n">ml_g</span><span class="p">,</span> <span class="n">ml_m</span><span class="p">)</span>

<span class="gp">In [11]: </span><span class="nb">print</span><span class="p">(</span><span class="n">dml_did_obj</span><span class="o">.</span><span class="n">fit</span><span class="p">())</span>
<span class="go">================== DoubleMLDID Object ==================</span>

<span class="go">------------------ Data Summary      ------------------</span>
<span class="go">Outcome variable: y</span>
<span class="go">Treatment variable(s): [&#39;d&#39;]</span>
<span class="go">Covariates: [&#39;Z1&#39;, &#39;Z2&#39;, &#39;Z3&#39;, &#39;Z4&#39;]</span>
<span class="go">Instrument variable(s): None</span>
<span class="go">No. Observations: 500</span>

<span class="go">------------------ Score &amp; Algorithm ------------------</span>
<span class="go">Score function: observational</span>

<span class="go">------------------ Machine Learner   ------------------</span>
<span class="go">Learner ml_g: RandomForestRegressor(max_depth=5, min_samples_leaf=5)</span>
<span class="go">Learner ml_m: RandomForestClassifier(max_depth=5, min_samples_leaf=5)</span>
<span class="go">Out-of-sample Performance:</span>
<span class="go">Regression:</span>
<span class="go">Learner ml_g0 RMSE: [[16.27429763]]</span>
<span class="go">Learner ml_g1 RMSE: [[13.35731523]]</span>
<span class="go">Classification:</span>
<span class="go">Learner ml_m Log Loss: [[0.66601815]]</span>

<span class="go">------------------ Resampling        ------------------</span>
<span class="go">No. folds: 5</span>
<span class="go">No. repeated sample splits: 1</span>

<span class="go">------------------ Fit Summary       ------------------</span>
<span class="go">       coef   std err         t     P&gt;|t|     2.5 %    97.5 %</span>
<span class="go">d -2.840718  1.760386 -1.613691  0.106595 -6.291011  0.609575</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id16">
<h4><span class="section-number">3.3.5.2. </span>Repeated cross-sections<a class="headerlink" href="#id16" title="Link to this heading">#</a></h4>
<p>For repeated cross-sections, the observations are assumed to be iid. of form <span class="math notranslate nohighlight">\((Y_{i}, D_i, X_i, T_i)\)</span>,
where <span class="math notranslate nohighlight">\(T_i\)</span> is a dummy variable if unit <span class="math notranslate nohighlight">\(i\)</span> is observed pre- or post-treatment period, such
that the observed outcome can be defined as</p>
<div class="math notranslate nohighlight">
\[Y_i = T_i Y_{i1} + (1-T_i) Y_{i0}.\]</div>
<p>Further, treatment and covariates are assumed to be stationary, such that the joint distribution of <span class="math notranslate nohighlight">\((D,X)\)</span> is invariant to <span class="math notranslate nohighlight">\(T\)</span>.</p>
<p><code class="docutils literal notranslate"><span class="pre">DoubleMLIDIDCS</span></code> implements difference-in-differences models for repeated cross-sections.
Estimation is conducted via its <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-16" name="sd-tab-set-12" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="py" for="sd-tab-item-16">
Python</label><div class="sd-tab-content docutils">
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [12]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="gp">In [13]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">doubleml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dml</span>

<span class="gp">In [14]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">doubleml.did.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_did_SZ2020</span>

<span class="gp">In [15]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span><span class="p">,</span> <span class="n">RandomForestClassifier</span>

<span class="gp">In [16]: </span><span class="n">ml_g</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="gp">In [17]: </span><span class="n">ml_m</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="gp">In [18]: </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="gp">In [19]: </span><span class="n">data</span> <span class="o">=</span> <span class="n">make_did_SZ2020</span><span class="p">(</span><span class="n">n_obs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">cross_sectional_data</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;DataFrame&#39;</span><span class="p">)</span>

<span class="gp">In [20]: </span><span class="n">obj_dml_data</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLDIDData</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">t_col</span><span class="o">=</span><span class="s1">&#39;t&#39;</span><span class="p">)</span>

<span class="gp">In [21]: </span><span class="n">dml_did_obj</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLDIDCS</span><span class="p">(</span><span class="n">obj_dml_data</span><span class="p">,</span> <span class="n">ml_g</span><span class="p">,</span> <span class="n">ml_m</span><span class="p">)</span>

<span class="gp">In [22]: </span><span class="nb">print</span><span class="p">(</span><span class="n">dml_did_obj</span><span class="o">.</span><span class="n">fit</span><span class="p">())</span>
<span class="go">================== DoubleMLDIDCS Object ==================</span>

<span class="go">------------------ Data Summary      ------------------</span>
<span class="go">Outcome variable: y</span>
<span class="go">Treatment variable(s): [&#39;d&#39;]</span>
<span class="go">Covariates: [&#39;Z1&#39;, &#39;Z2&#39;, &#39;Z3&#39;, &#39;Z4&#39;]</span>
<span class="go">Instrument variable(s): None</span>
<span class="go">No. Observations: 500</span>

<span class="go">------------------ Score &amp; Algorithm ------------------</span>
<span class="go">Score function: observational</span>

<span class="go">------------------ Machine Learner   ------------------</span>
<span class="go">Learner ml_g: RandomForestRegressor(max_depth=5, min_samples_leaf=5)</span>
<span class="go">Learner ml_m: RandomForestClassifier(max_depth=5, min_samples_leaf=5)</span>
<span class="go">Out-of-sample Performance:</span>
<span class="go">Regression:</span>
<span class="go">Learner ml_g_d0_t0 RMSE: [[17.4915707]]</span>
<span class="go">Learner ml_g_d0_t1 RMSE: [[44.85397773]]</span>
<span class="go">Learner ml_g_d1_t0 RMSE: [[32.74938952]]</span>
<span class="go">Learner ml_g_d1_t1 RMSE: [[53.7282094]]</span>
<span class="go">Classification:</span>
<span class="go">Learner ml_m Log Loss: [[0.67936506]]</span>

<span class="go">------------------ Resampling        ------------------</span>
<span class="go">No. folds: 5</span>
<span class="go">No. repeated sample splits: 1</span>

<span class="go">------------------ Fit Summary       ------------------</span>
<span class="go">     coef   std err         t     P&gt;|t|      2.5 %    97.5 %</span>
<span class="go">d -4.9944  7.561785 -0.660479  0.508947 -19.815226  9.826426</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="sample-selection-models-ssm">
<span id="ssm-models"></span><h2><span class="section-number">3.4. </span>Sample Selection Models (SSM)<a class="headerlink" href="#sample-selection-models-ssm" title="Link to this heading">#</a></h2>
<p><strong>Sample Selection Models (SSM)</strong> implemented in the package focus on the the binary treatment case when outcomes are only observed for a subpopulation
due to sample selection or outcome attrition.</p>
<p>The implementation and notation is based on <a class="reference external" href="https://doi.org/10.1080/07350015.2023.2271071">Bia, Huber and Lafférs (2023)</a>.
Let <span class="math notranslate nohighlight">\(D_i\)</span> be the binary treatment indicator and <span class="math notranslate nohighlight">\(Y_{i}(d)\)</span> the potential outcome under treatment value <span class="math notranslate nohighlight">\(d\)</span>. Further, define
<span class="math notranslate nohighlight">\(Y_{i}:=Y_{i}(D)\)</span> to be the realized outcome and <span class="math notranslate nohighlight">\(S_{i}\)</span> as a binary selection indicator. The outcome <span class="math notranslate nohighlight">\(Y_{i}\)</span> is only observed if <span class="math notranslate nohighlight">\(S_{i}=1\)</span>.
Finally, let <span class="math notranslate nohighlight">\(X_i\)</span> be a vector of observed covariates, measures prior to treatment assignment.</p>
<p>Target parameter of interest is the average treatment effect (ATE)</p>
<div class="math notranslate nohighlight">
\[\theta_0 = \mathbb{E}[Y_{i}(1)- Y_{i}(0)].\]</div>
<p>The corresponding identifying assumption is</p>
<ul class="simple">
<li><p><strong>Cond. Independence of Treatment:</strong> <span class="math notranslate nohighlight">\(Y_i(d) \perp D_i|X_i\quad a.s.\)</span> for <span class="math notranslate nohighlight">\(d=0,1\)</span></p></li>
</ul>
<p>where further assmputions are made in the context of the respective sample selection model.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A more detailed example can be found in the <a class="reference internal" href="../examples/index.html#examplegallery"><span class="std std-ref">Example Gallery</span></a>.</p>
</div>
<section id="missingness-at-random">
<span id="ssm-mar-model"></span><h3><span class="section-number">3.4.1. </span>Missingness at Random<a class="headerlink" href="#missingness-at-random" title="Link to this heading">#</a></h3>
<p>Consider the following two additional assumptions for the sample selection model:</p>
<ul class="simple">
<li><p><strong>Cond. Independence of Selection:</strong> <span class="math notranslate nohighlight">\(Y_i(d) \perp S_i|D_i=d, X_i\quad a.s.\)</span> for <span class="math notranslate nohighlight">\(d=0,1\)</span></p></li>
<li><p><strong>Common Support:</strong> <span class="math notranslate nohighlight">\(P(D_i=1|X_i)&gt;0\)</span> and <span class="math notranslate nohighlight">\(P(S_i=1|D_i=d, X_i)&gt;0\)</span> for <span class="math notranslate nohighlight">\(d=0,1\)</span></p></li>
</ul>
<p>such that outcomes are missing at random (for the score see <a class="reference internal" href="scores.html#ssm-mar-score"><span class="std std-ref">Scores</span></a>).</p>
<p><code class="docutils literal notranslate"><span class="pre">DoubleMLSSM</span></code> implements sample selection models. The score <code class="docutils literal notranslate"><span class="pre">score='missing-at-random'</span></code> refers to the correponding score
relying on the assumptions above. The <code class="docutils literal notranslate"><span class="pre">DoubleMLData</span></code> object has to be defined with the additional argument <code class="docutils literal notranslate"><span class="pre">s_col</span></code> for the selection indicator.
Estimation is conducted via its <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-17" name="sd-tab-set-13" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="py" for="sd-tab-item-17">
Python</label><div class="sd-tab-content docutils">
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="gp">In [2]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LassoCV</span><span class="p">,</span> <span class="n">LogisticRegressionCV</span>

<span class="gp">In [3]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">doubleml.irm.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_ssm_data</span>

<span class="gp">In [4]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">doubleml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dml</span>

<span class="gp">In [5]: </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="gp">In [6]: </span><span class="n">n_obs</span> <span class="o">=</span> <span class="mi">2000</span>

<span class="gp">In [7]: </span><span class="n">df</span> <span class="o">=</span> <span class="n">make_ssm_data</span><span class="p">(</span><span class="n">n_obs</span><span class="o">=</span><span class="n">n_obs</span><span class="p">,</span> <span class="n">mar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;DataFrame&#39;</span><span class="p">)</span>

<span class="gp">In [8]: </span><span class="n">dml_data</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLSSMData</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">s_col</span><span class="o">=</span><span class="s1">&#39;s&#39;</span><span class="p">)</span>

<span class="gp">In [9]: </span><span class="n">ml_g</span> <span class="o">=</span> <span class="n">LassoCV</span><span class="p">()</span>

<span class="gp">In [10]: </span><span class="n">ml_m</span> <span class="o">=</span> <span class="n">LogisticRegressionCV</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;liblinear&#39;</span><span class="p">)</span>

<span class="gp">In [11]: </span><span class="n">ml_pi</span> <span class="o">=</span> <span class="n">LogisticRegressionCV</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;liblinear&#39;</span><span class="p">)</span>

<span class="gp">In [12]: </span><span class="n">dml_ssm</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLSSM</span><span class="p">(</span><span class="n">dml_data</span><span class="p">,</span> <span class="n">ml_g</span><span class="p">,</span> <span class="n">ml_m</span><span class="p">,</span> <span class="n">ml_pi</span><span class="p">,</span> <span class="n">score</span><span class="o">=</span><span class="s1">&#39;missing-at-random&#39;</span><span class="p">)</span>

<span class="gp">In [13]: </span><span class="n">dml_ssm</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="gh">Out[13]: </span><span class="go">&lt;doubleml.irm.ssm.DoubleMLSSM at 0x7feafd7a3c20&gt;</span>

<span class="gp">In [14]: </span><span class="nb">print</span><span class="p">(</span><span class="n">dml_ssm</span><span class="p">)</span>
<span class="go">================== DoubleMLSSM Object ==================</span>

<span class="go">------------------ Data Summary      ------------------</span>
<span class="go">Outcome variable: y</span>
<span class="go">Treatment variable(s): [&#39;d&#39;]</span>
<span class="go">Covariates: [&#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;X4&#39;, &#39;X5&#39;, &#39;X6&#39;, &#39;X7&#39;, &#39;X8&#39;, &#39;X9&#39;, &#39;X10&#39;, &#39;X11&#39;, &#39;X12&#39;, &#39;X13&#39;, &#39;X14&#39;, &#39;X15&#39;, &#39;X16&#39;, &#39;X17&#39;, &#39;X18&#39;, &#39;X19&#39;, &#39;X20&#39;, &#39;X21&#39;, &#39;X22&#39;, &#39;X23&#39;, &#39;X24&#39;, &#39;X25&#39;, &#39;X26&#39;, &#39;X27&#39;, &#39;X28&#39;, &#39;X29&#39;, &#39;X30&#39;, &#39;X31&#39;, &#39;X32&#39;, &#39;X33&#39;, &#39;X34&#39;, &#39;X35&#39;, &#39;X36&#39;, &#39;X37&#39;, &#39;X38&#39;, &#39;X39&#39;, &#39;X40&#39;, &#39;X41&#39;, &#39;X42&#39;, &#39;X43&#39;, &#39;X44&#39;, &#39;X45&#39;, &#39;X46&#39;, &#39;X47&#39;, &#39;X48&#39;, &#39;X49&#39;, &#39;X50&#39;, &#39;X51&#39;, &#39;X52&#39;, &#39;X53&#39;, &#39;X54&#39;, &#39;X55&#39;, &#39;X56&#39;, &#39;X57&#39;, &#39;X58&#39;, &#39;X59&#39;, &#39;X60&#39;, &#39;X61&#39;, &#39;X62&#39;, &#39;X63&#39;, &#39;X64&#39;, &#39;X65&#39;, &#39;X66&#39;, &#39;X67&#39;, &#39;X68&#39;, &#39;X69&#39;, &#39;X70&#39;, &#39;X71&#39;, &#39;X72&#39;, &#39;X73&#39;, &#39;X74&#39;, &#39;X75&#39;, &#39;X76&#39;, &#39;X77&#39;, &#39;X78&#39;, &#39;X79&#39;, &#39;X80&#39;, &#39;X81&#39;, &#39;X82&#39;, &#39;X83&#39;, &#39;X84&#39;, &#39;X85&#39;, &#39;X86&#39;, &#39;X87&#39;, &#39;X88&#39;, &#39;X89&#39;, &#39;X90&#39;, &#39;X91&#39;, &#39;X92&#39;, &#39;X93&#39;, &#39;X94&#39;, &#39;X95&#39;, &#39;X96&#39;, &#39;X97&#39;, &#39;X98&#39;, &#39;X99&#39;, &#39;X100&#39;]</span>
<span class="go">Instrument variable(s): None</span>
<span class="go">No. Observations: 2000</span>

<span class="go">------------------ Score &amp; Algorithm ------------------</span>
<span class="go">Score function: missing-at-random</span>

<span class="go">------------------ Machine Learner   ------------------</span>
<span class="go">Learner ml_g: LassoCV()</span>
<span class="go">Learner ml_pi: LogisticRegressionCV(penalty=&#39;l1&#39;, solver=&#39;liblinear&#39;)</span>
<span class="go">Learner ml_m: LogisticRegressionCV(penalty=&#39;l1&#39;, solver=&#39;liblinear&#39;)</span>
<span class="go">Out-of-sample Performance:</span>
<span class="go">Regression:</span>
<span class="go">Learner ml_g_d0 RMSE: [[1.10039862]]</span>
<span class="go">Learner ml_g_d1 RMSE: [[1.11071087]]</span>
<span class="go">Classification:</span>
<span class="go">Learner ml_pi Log Loss: [[0.53791422]]</span>
<span class="go">Learner ml_m Log Loss: [[0.63593298]]</span>

<span class="go">------------------ Resampling        ------------------</span>
<span class="go">No. folds: 5</span>
<span class="go">No. repeated sample splits: 1</span>

<span class="go">------------------ Fit Summary       ------------------</span>
<span class="go">       coef   std err          t         P&gt;|t|     2.5 %    97.5 %</span>
<span class="go">d  0.965531  0.065969  14.636048  1.654070e-48  0.836234  1.094829</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-18" name="sd-tab-set-13" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="r" for="sd-tab-item-18">
R</label><div class="sd-tab-content docutils">
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">DoubleML</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">mlr3</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">data.table</span><span class="p">)</span>

<span class="nf">set.seed</span><span class="p">(</span><span class="m">3141</span><span class="p">)</span>
<span class="n">n_obs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2000</span>
<span class="n">df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">make_ssm_data</span><span class="p">(</span><span class="n">n_obs</span><span class="o">=</span><span class="n">n_obs</span><span class="p">,</span><span class="w"> </span><span class="n">mar</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">return_type</span><span class="o">=</span><span class="s">&quot;data.table&quot;</span><span class="p">)</span>
<span class="n">dml_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DoubleMLData</span><span class="o">$</span><span class="nf">new</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="w"> </span><span class="n">y_col</span><span class="o">=</span><span class="s">&quot;y&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">d_cols</span><span class="o">=</span><span class="s">&quot;d&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">s_col</span><span class="o">=</span><span class="s">&quot;s&quot;</span><span class="p">)</span>

<span class="n">ml_g</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">lrn</span><span class="p">(</span><span class="s">&quot;regr.cv_glmnet&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">nfolds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;lambda.min&quot;</span><span class="p">)</span>
<span class="n">ml_m</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">lrn</span><span class="p">(</span><span class="s">&quot;classif.cv_glmnet&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">nfolds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;lambda.min&quot;</span><span class="p">)</span>
<span class="n">ml_pi</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">lrn</span><span class="p">(</span><span class="s">&quot;classif.cv_glmnet&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">nfolds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;lambda.min&quot;</span><span class="p">)</span>

<span class="n">dml_ssm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DoubleMLSSM</span><span class="o">$</span><span class="nf">new</span><span class="p">(</span><span class="n">dml_data</span><span class="p">,</span><span class="w"> </span><span class="n">ml_g</span><span class="p">,</span><span class="w"> </span><span class="n">ml_m</span><span class="p">,</span><span class="w"> </span><span class="n">ml_pi</span><span class="p">,</span><span class="w"> </span><span class="n">score</span><span class="o">=</span><span class="s">&quot;missing-at-random&quot;</span><span class="p">)</span>
<span class="n">dml_ssm</span><span class="o">$</span><span class="nf">fit</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">dml_ssm</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>================= DoubleMLSSM Object ==================


------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): d
Covariates: X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, X20, X21, X22, X23, X24, X25, X26, X27, X28, X29, X30, X31, X32, X33, X34, X35, X36, X37, X38, X39, X40, X41, X42, X43, X44, X45, X46, X47, X48, X49, X50, X51, X52, X53, X54, X55, X56, X57, X58, X59, X60, X61, X62, X63, X64, X65, X66, X67, X68, X69, X70, X71, X72, X73, X74, X75, X76, X77, X78, X79, X80, X81, X82, X83, X84, X85, X86, X87, X88, X89, X90, X91, X92, X93, X94, X95, X96, X97, X98, X99, X100
Instrument(s): 
Selection variable: s
No. Observations: 2000

------------------ Score &amp; algorithm ------------------
Score function: missing-at-random
DML algorithm: dml2

------------------ Machine learner   ------------------
ml_g: regr.cv_glmnet
ml_pi: classif.cv_glmnet
ml_m: classif.cv_glmnet

------------------ Resampling        ------------------
No. folds: 5
No. repeated sample splits: 1
Apply cross-fitting: TRUE

------------------ Fit summary       ------------------
 Estimates and significance testing of the effect of target variables
  Estimate. Std. Error t value Pr(&gt;|t|)    
d   0.93300    0.05891   15.84   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


</pre></div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="nonignorable-nonresponse">
<span id="ssm-nr-model"></span><h3><span class="section-number">3.4.2. </span>Nonignorable Nonresponse<a class="headerlink" href="#nonignorable-nonresponse" title="Link to this heading">#</a></h3>
<p>When sample selection or outcome attriction is realated to unobservables, identification generally requires an instrument for the selection indicator <span class="math notranslate nohighlight">\(S_i\)</span>.
Consider the following additional assumptions for the instrumental variable:</p>
<ul class="simple">
<li><p><strong>Cond. Correlation:</strong> <span class="math notranslate nohighlight">\(\exists Z: \mathbb{E}[Z\cdot S|D,X] \neq 0\)</span></p></li>
<li><p><strong>Cond. Independence:</strong> <span class="math notranslate nohighlight">\(Y_i(d,z)=Y_i(d)\)</span> and <span class="math notranslate nohighlight">\(Y_i \perp Z_i|D_i=d, X_i\quad a.s.\)</span> for <span class="math notranslate nohighlight">\(d=0,1\)</span></p></li>
</ul>
<p>This requires the instrumental variable <span class="math notranslate nohighlight">\(Z_i\)</span>, which must not affect <span class="math notranslate nohighlight">\(Y_i\)</span> or be associated
with unobservables affecting <span class="math notranslate nohighlight">\(Y_i\)</span> conditional on <span class="math notranslate nohighlight">\(D_i\)</span> and <span class="math notranslate nohighlight">\(X_i\)</span>. Further, the selection is determined via
a (unknown) threshold model:</p>
<ul class="simple">
<li><p><strong>Threshold:</strong> <span class="math notranslate nohighlight">\(S_i = 1\{V_i \le \xi(D,X,Z)\}\)</span> where <span class="math notranslate nohighlight">\(\xi\)</span> is a general function and <span class="math notranslate nohighlight">\(V_i\)</span> is a scalar with strictly monotonic cumulative distribution function conditional on <span class="math notranslate nohighlight">\(X_i\)</span>.</p></li>
<li><p><strong>Cond. Independence:</strong> <span class="math notranslate nohighlight">\(V_i \perp (Z_i, D_i)|X_i\)</span>.</p></li>
</ul>
<p>Let <span class="math notranslate nohighlight">\(\Pi_i := P(S_i=1|D_i, X_i, Z_i)\)</span> denote the selection probability.
Additionally, the following assumptions are required:</p>
<ul class="simple">
<li><p><strong>Common Support for Treatment:</strong> <span class="math notranslate nohighlight">\(P(D_i=1|X_i, \Pi)&gt;0\)</span></p></li>
<li><p><strong>Cond. Effect Homogeneity:</strong> <span class="math notranslate nohighlight">\(\mathbb{E}[Y_i(1)-Y_i(0)|S_i=1, X_i=x, V_i=v] = \mathbb{E}[Y_i(1)-Y_i(0)|X_i=x, V_i=v]\)</span></p></li>
<li><p><strong>Common Support for Selection:</strong> <span class="math notranslate nohighlight">\(P(S_i=1|D_i=d, X_i=x, Z_i=z)&gt;0\quad a.s.\)</span> for <span class="math notranslate nohighlight">\(d=0,1\)</span></p></li>
</ul>
<p>For further details, see <a class="reference external" href="https://doi.org/10.1080/07350015.2023.2271071">Bia, Huber and Lafférs (2023)</a>.</p>
<figure class="align-center" id="id25">
<a class="reference internal image-reference" href="../_images/py_ssm1.svg"><img alt="DAG" src="../_images/py_ssm1.svg" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-text">Causal paths under nonignorable nonresponse</span><a class="headerlink" href="#id25" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><code class="docutils literal notranslate"><span class="pre">DoubleMLSSM</span></code> implements sample selection models. The score <code class="docutils literal notranslate"><span class="pre">score='nonignorable'</span></code> refers to the correponding score
relying on the assumptions above. The <code class="docutils literal notranslate"><span class="pre">DoubleMLData</span></code> object has to be defined with the additional argument <code class="docutils literal notranslate"><span class="pre">s_col</span></code> for the selection indicator
and <code class="docutils literal notranslate"><span class="pre">z_cols</span></code> for the instrument.
Estimation is conducted via its <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-19" name="sd-tab-set-14" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="py" for="sd-tab-item-19">
Python</label><div class="sd-tab-content docutils">
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [15]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="gp">In [16]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LassoCV</span><span class="p">,</span> <span class="n">LogisticRegressionCV</span>

<span class="gp">In [17]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">doubleml.irm.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_ssm_data</span>

<span class="gp">In [18]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">doubleml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dml</span>

<span class="gp">In [19]: </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="gp">In [20]: </span><span class="n">n_obs</span> <span class="o">=</span> <span class="mi">2000</span>

<span class="gp">In [21]: </span><span class="n">df</span> <span class="o">=</span> <span class="n">make_ssm_data</span><span class="p">(</span><span class="n">n_obs</span><span class="o">=</span><span class="n">n_obs</span><span class="p">,</span> <span class="n">mar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;DataFrame&#39;</span><span class="p">)</span>

<span class="gp">In [22]: </span><span class="n">dml_data</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLSSMData</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">z_cols</span><span class="o">=</span><span class="s1">&#39;z&#39;</span><span class="p">,</span> <span class="n">s_col</span><span class="o">=</span><span class="s1">&#39;s&#39;</span><span class="p">)</span>

<span class="gp">In [23]: </span><span class="n">ml_g</span> <span class="o">=</span> <span class="n">LassoCV</span><span class="p">()</span>

<span class="gp">In [24]: </span><span class="n">ml_m</span> <span class="o">=</span> <span class="n">LogisticRegressionCV</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;liblinear&#39;</span><span class="p">)</span>

<span class="gp">In [25]: </span><span class="n">ml_pi</span> <span class="o">=</span> <span class="n">LogisticRegressionCV</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;liblinear&#39;</span><span class="p">)</span>

<span class="gp">In [26]: </span><span class="n">dml_ssm</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLSSM</span><span class="p">(</span><span class="n">dml_data</span><span class="p">,</span> <span class="n">ml_g</span><span class="p">,</span> <span class="n">ml_m</span><span class="p">,</span> <span class="n">ml_pi</span><span class="p">,</span> <span class="n">score</span><span class="o">=</span><span class="s1">&#39;nonignorable&#39;</span><span class="p">)</span>

<span class="gp">In [27]: </span><span class="n">dml_ssm</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="gh">Out[27]: </span><span class="go">&lt;doubleml.irm.ssm.DoubleMLSSM at 0x7feaec08c590&gt;</span>

<span class="gp">In [28]: </span><span class="nb">print</span><span class="p">(</span><span class="n">dml_ssm</span><span class="p">)</span>
<span class="go">================== DoubleMLSSM Object ==================</span>

<span class="go">------------------ Data Summary      ------------------</span>
<span class="go">Outcome variable: y</span>
<span class="go">Treatment variable(s): [&#39;d&#39;]</span>
<span class="go">Covariates: [&#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;X4&#39;, &#39;X5&#39;, &#39;X6&#39;, &#39;X7&#39;, &#39;X8&#39;, &#39;X9&#39;, &#39;X10&#39;, &#39;X11&#39;, &#39;X12&#39;, &#39;X13&#39;, &#39;X14&#39;, &#39;X15&#39;, &#39;X16&#39;, &#39;X17&#39;, &#39;X18&#39;, &#39;X19&#39;, &#39;X20&#39;, &#39;X21&#39;, &#39;X22&#39;, &#39;X23&#39;, &#39;X24&#39;, &#39;X25&#39;, &#39;X26&#39;, &#39;X27&#39;, &#39;X28&#39;, &#39;X29&#39;, &#39;X30&#39;, &#39;X31&#39;, &#39;X32&#39;, &#39;X33&#39;, &#39;X34&#39;, &#39;X35&#39;, &#39;X36&#39;, &#39;X37&#39;, &#39;X38&#39;, &#39;X39&#39;, &#39;X40&#39;, &#39;X41&#39;, &#39;X42&#39;, &#39;X43&#39;, &#39;X44&#39;, &#39;X45&#39;, &#39;X46&#39;, &#39;X47&#39;, &#39;X48&#39;, &#39;X49&#39;, &#39;X50&#39;, &#39;X51&#39;, &#39;X52&#39;, &#39;X53&#39;, &#39;X54&#39;, &#39;X55&#39;, &#39;X56&#39;, &#39;X57&#39;, &#39;X58&#39;, &#39;X59&#39;, &#39;X60&#39;, &#39;X61&#39;, &#39;X62&#39;, &#39;X63&#39;, &#39;X64&#39;, &#39;X65&#39;, &#39;X66&#39;, &#39;X67&#39;, &#39;X68&#39;, &#39;X69&#39;, &#39;X70&#39;, &#39;X71&#39;, &#39;X72&#39;, &#39;X73&#39;, &#39;X74&#39;, &#39;X75&#39;, &#39;X76&#39;, &#39;X77&#39;, &#39;X78&#39;, &#39;X79&#39;, &#39;X80&#39;, &#39;X81&#39;, &#39;X82&#39;, &#39;X83&#39;, &#39;X84&#39;, &#39;X85&#39;, &#39;X86&#39;, &#39;X87&#39;, &#39;X88&#39;, &#39;X89&#39;, &#39;X90&#39;, &#39;X91&#39;, &#39;X92&#39;, &#39;X93&#39;, &#39;X94&#39;, &#39;X95&#39;, &#39;X96&#39;, &#39;X97&#39;, &#39;X98&#39;, &#39;X99&#39;, &#39;X100&#39;]</span>
<span class="go">Instrument variable(s): [&#39;z&#39;]</span>
<span class="go">No. Observations: 2000</span>

<span class="go">------------------ Score &amp; Algorithm ------------------</span>
<span class="go">Score function: nonignorable</span>

<span class="go">------------------ Machine Learner   ------------------</span>
<span class="go">Learner ml_g: LassoCV()</span>
<span class="go">Learner ml_pi: LogisticRegressionCV(penalty=&#39;l1&#39;, solver=&#39;liblinear&#39;)</span>
<span class="go">Learner ml_m: LogisticRegressionCV(penalty=&#39;l1&#39;, solver=&#39;liblinear&#39;)</span>
<span class="go">Out-of-sample Performance:</span>
<span class="go">Regression:</span>
<span class="go">Learner ml_g_d0 RMSE: [[0.92827999]]</span>
<span class="go">Learner ml_g_d1 RMSE: [[1.10079785]]</span>
<span class="go">Classification:</span>
<span class="go">Learner ml_pi Log Loss: [[0.44124313]]</span>
<span class="go">Learner ml_m Log Loss: [[0.59854797]]</span>

<span class="go">------------------ Resampling        ------------------</span>
<span class="go">No. folds: 5</span>
<span class="go">No. repeated sample splits: 1</span>

<span class="go">------------------ Fit Summary       ------------------</span>
<span class="go">      coef   std err         t         P&gt;|t|     2.5 %    97.5 %</span>
<span class="go">d  1.14268  0.183373  6.231467  4.620874e-10  0.783276  1.502084</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-20" name="sd-tab-set-14" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="r" for="sd-tab-item-20">
R</label><div class="sd-tab-content docutils">
<div class="jupyter_cell jupyter_container docutils container">
<div class="cell_input code_cell docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">DoubleML</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">mlr3</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">data.table</span><span class="p">)</span>

<span class="nf">set.seed</span><span class="p">(</span><span class="m">3141</span><span class="p">)</span>
<span class="n">n_obs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2000</span>
<span class="n">df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">make_ssm_data</span><span class="p">(</span><span class="n">n_obs</span><span class="o">=</span><span class="n">n_obs</span><span class="p">,</span><span class="w"> </span><span class="n">mar</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">return_type</span><span class="o">=</span><span class="s">&quot;data.table&quot;</span><span class="p">)</span>
<span class="n">dml_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DoubleMLData</span><span class="o">$</span><span class="nf">new</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="w"> </span><span class="n">y_col</span><span class="o">=</span><span class="s">&quot;y&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">d_cols</span><span class="o">=</span><span class="s">&quot;d&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">z_cols</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;z&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">s_col</span><span class="o">=</span><span class="s">&quot;s&quot;</span><span class="p">)</span>

<span class="n">ml_g</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">lrn</span><span class="p">(</span><span class="s">&quot;regr.cv_glmnet&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">nfolds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;lambda.min&quot;</span><span class="p">)</span>
<span class="n">ml_m</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">lrn</span><span class="p">(</span><span class="s">&quot;classif.cv_glmnet&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">nfolds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;lambda.min&quot;</span><span class="p">)</span>
<span class="n">ml_pi</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">lrn</span><span class="p">(</span><span class="s">&quot;classif.cv_glmnet&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">nfolds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;lambda.min&quot;</span><span class="p">)</span>

<span class="n">dml_ssm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DoubleMLSSM</span><span class="o">$</span><span class="nf">new</span><span class="p">(</span><span class="n">dml_data</span><span class="p">,</span><span class="w"> </span><span class="n">ml_g</span><span class="p">,</span><span class="w"> </span><span class="n">ml_m</span><span class="p">,</span><span class="w"> </span><span class="n">ml_pi</span><span class="p">,</span><span class="w"> </span><span class="n">score</span><span class="o">=</span><span class="s">&quot;nonignorable&quot;</span><span class="p">)</span>
<span class="n">dml_ssm</span><span class="o">$</span><span class="nf">fit</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">dml_ssm</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>================= DoubleMLSSM Object ==================


------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): d
Covariates: X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, X20, X21, X22, X23, X24, X25, X26, X27, X28, X29, X30, X31, X32, X33, X34, X35, X36, X37, X38, X39, X40, X41, X42, X43, X44, X45, X46, X47, X48, X49, X50, X51, X52, X53, X54, X55, X56, X57, X58, X59, X60, X61, X62, X63, X64, X65, X66, X67, X68, X69, X70, X71, X72, X73, X74, X75, X76, X77, X78, X79, X80, X81, X82, X83, X84, X85, X86, X87, X88, X89, X90, X91, X92, X93, X94, X95, X96, X97, X98, X99, X100
Instrument(s): z
Selection variable: s
No. Observations: 2000

------------------ Score &amp; algorithm ------------------
Score function: nonignorable
DML algorithm: dml2

------------------ Machine learner   ------------------
ml_g: regr.cv_glmnet
ml_pi: classif.cv_glmnet
ml_m: classif.cv_glmnet

------------------ Resampling        ------------------
No. folds: 5
No. repeated sample splits: 1
Apply cross-fitting: TRUE

------------------ Fit summary       ------------------
 Estimates and significance testing of the effect of target variables
  Estimate. Std. Error t value Pr(&gt;|t|)    
d   0.92566    0.06811   13.59   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


</pre></div>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="regression-discontinuity-designs-rdd">
<span id="rdd-models"></span><h2><span class="section-number">3.5. </span>Regression Discontinuity Designs (RDD)<a class="headerlink" href="#regression-discontinuity-designs-rdd" title="Link to this heading">#</a></h2>
<p><strong>Regression Discontinuity Designs (RDD)</strong> are causal inference methods used when treatment assignment is determined by a continuous running variable (“score”) crossing a known threshold (“cutoff”). These designs exploit discontinuities in the probability of receiving treatment at the cutoff to estimate the average treatment effect. RDDs are divided into two main types: <strong>Sharp</strong> and <strong>Fuzzy</strong>.</p>
<p>The key idea behind RDD is that units just above and just below the threshold are assumed to be comparable, differing only in the treatment assignment. This allows estimating the causal effect at the threshold by comparing outcomes of treated and untreated units.</p>
<p>Our implementation follows work from <a class="reference external" href="https://arxiv.org/abs/2107.07942">Noack, Olma and Rothe (2024)</a>.</p>
<p>Let <span class="math notranslate nohighlight">\(Y_i\)</span> be the observed outcome of an individual and <span class="math notranslate nohighlight">\(D_i\)</span> the treatment it received. By using a set of additional covariates <span class="math notranslate nohighlight">\(X_i\)</span> for each observation, <span class="math notranslate nohighlight">\(Y_i\)</span> and <span class="math notranslate nohighlight">\(D_i\)</span> can be adjusted in a first stage, to reduce the standard deviation in the estimation of the causal effect.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<dl class="simple">
<dt>To fit into the package syntax, our notation differs as follows from the one used in most standard RDD works (as for example <a class="reference external" href="https://doi.org/10.1146/annurev-economics-051520-021409">Cattaneo and Titiunik (2022)</a>):</dt><dd><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S_i\)</span> the <strong>score</strong> (instead of <span class="math notranslate nohighlight">\(X_i\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(X_i\)</span> the <strong>covariates</strong> (instead of <span class="math notranslate nohighlight">\(Z_i\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(D_i\)</span> the <strong>treatment received</strong> (in sharp RDD instead of <span class="math notranslate nohighlight">\(T_i\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(T_i\)</span> the <strong>treatment assigned</strong> (only relevant in fuzzy RDD)</p></li>
</ul>
</dd>
</dl>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">doubleml.rdd</span></code> module depends on <code class="docutils literal notranslate"><span class="pre">rdrobust</span></code> which can be installed via <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">rdrobust</span></code> or <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">doubleml[rdd]</span></code>.</p>
</div>
<section id="sharp-regression-discontinuity-design">
<h3><span class="section-number">3.5.1. </span>Sharp Regression Discontinuity Design<a class="headerlink" href="#sharp-regression-discontinuity-design" title="Link to this heading">#</a></h3>
<p>In a <strong>Sharp RDD</strong>, the treatment <span class="math notranslate nohighlight">\(D_i\)</span> is deterministically assigned at the cutoff (<span class="math notranslate nohighlight">\(D_i = \mathbb{1}\{S_i \geq c\}\)</span>).</p>
<p>Let <span class="math notranslate nohighlight">\(S_i\)</span> represent the score, and let <span class="math notranslate nohighlight">\(c\)</span> denote the cutoff point. Further, let <span class="math notranslate nohighlight">\(Y_i(1)\)</span> and <span class="math notranslate nohighlight">\(Y_i(0)\)</span> denote the potential outcomes with and without treatment, respectively. Then, the treatment effect at the cutoff</p>
<div class="math notranslate nohighlight">
\[\tau_0 = \mathbb{E}[Y_i(1)-Y_i(0)\mid S_i = c]\]</div>
<p>is identified as the difference in the conditional expectation of <span class="math notranslate nohighlight">\(Y_i\)</span> at the cutoff from both sides</p>
<div class="math notranslate nohighlight">
\[\tau_0 = \lim_{s \to c^+} \mathbb{E}[Y_i \mid S_i = s] - \lim_{s \to c^-} \mathbb{E}[Y_i \mid S_i = s]\]</div>
<p>The key assumption for identifying this effect in a sharp RDD is:</p>
<ul class="simple">
<li><p><strong>Continuity:</strong> The conditional mean of the potential outcomes <span class="math notranslate nohighlight">\(\mathbb{E}[Y_i(d)\mid S_i=s]\)</span> for <span class="math notranslate nohighlight">\(d \in \{0, 1\}\)</span> is continuous at the cutoff level <span class="math notranslate nohighlight">\(c\)</span>.</p></li>
</ul>
<p>This includes the necessary condition of exogeneity, implying units cannot perfectly manipulate their value of <span class="math notranslate nohighlight">\(S_i\)</span> to either receive or avoid treatment exactly at the cutoff.</p>
<p>Without the use of covariates, <span class="math notranslate nohighlight">\(\tau_{0}\)</span> is typically estimated by running separate local linear regressions on each side of the cutoff, yielding an estimator of the form:</p>
<div class="math notranslate nohighlight">
\[\hat{\tau}_{\text{base}}(h) = \sum_{i=1}^n w_i(h)Y_i,\]</div>
<p>where the <span class="math notranslate nohighlight">\(w_i(h)\)</span> are local linear regression weights that depend on the data through the realizations of the running variable only and <span class="math notranslate nohighlight">\(h &gt; 0\)</span> is a bandwidth.</p>
<p>Under standard conditions, which include that the running variable is continuously distributed, and that the bandwidth <span class="math notranslate nohighlight">\(h\)</span> tends to zero at an appropriate rate, the estimator <span class="math notranslate nohighlight">\(\hat{\tau}_{\text{base}}(h)\)</span> is approximately normally distributed in large samples, with bias of order <span class="math notranslate nohighlight">\(h^2\)</span> and variance of order <span class="math notranslate nohighlight">\((nh)^{-1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\hat{\tau}_{\text{base}}(h) \stackrel{a}{\sim} N\left(\tau + h^2  B_{\text{base}},(nh)^{-1}V_{\text{base}}\right).\]</div>
<p>If covariates are available, they can be used to improve the accuracy of empirical RD estimates. The most popular strategy is to include them linearly and without kernel localization in the local linear regression. By simple least squares algebra, this “linear adjustment” estimator can be written as a no-covariates estimator with the covariate-adjusted outcome <span class="math notranslate nohighlight">\(Y_i - X_i^{\top} \widehat{\gamma}_h\)</span>:</p>
<div class="math notranslate nohighlight">
\[\widehat{\tau}_{\text{lin}}(h) = \sum_{i=1}^n w_i(h)\left(Y_i - X_i^{\top} \widehat{\gamma}_h\right).\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\widehat{\gamma}_h\)</span> is the minimizer from the regression</p>
<div class="math notranslate nohighlight">
\[\underset{\beta,\gamma}{\mathrm{arg\,min}} \, \sum_{i=1}^n K_h(S_i) (Y_i - Q_i^\top\beta- X_i^{\top}\gamma )^2,\]</div>
<p>with <span class="math notranslate nohighlight">\(Q_i =(D_i, S_i, D_i S_i, 1)^T\)</span> (see <code class="docutils literal notranslate"><span class="pre">fs_specification</span></code> in <a class="reference internal" href="#rdd-imp-details"><span class="std std-ref">Implementation Details</span></a>), <span class="math notranslate nohighlight">\(K_h(v)=K(v/h)/h\)</span> with <span class="math notranslate nohighlight">\(K(\cdot)\)</span> a kernel function.</p>
<p>If <span class="math notranslate nohighlight">\(\mathbb{E}[X_i | S_i = s]\)</span> is twice continuously differentiable around the cutoff, then the distribution of <span class="math notranslate nohighlight">\(\widehat{\tau}_{\text{lin}}(h)\)</span> is similar to the one of the base estimator with potentially smaller variance term <span class="math notranslate nohighlight">\(V_{\text{lin}}\)</span>.</p>
<p>As this linear adjustment might not exploit the available covariate information efficiently, DoubleML features an RDD estimator with flexible covariate adjustment based on potentially nonlinear adjustment functions <span class="math notranslate nohighlight">\(\eta\)</span>. The estimator takes the following form:</p>
<div class="math notranslate nohighlight">
\[\widehat{\tau}_{\text{RDFlex}}(h; \eta) = \sum_{i=1}^n w_i(h) M_i(\eta), \quad M_i(\eta) = Y_i - \eta(X_i).\]</div>
<p>Similar to other algorithms in DoubleML, <span class="math notranslate nohighlight">\(\eta\)</span> is estimated by ML methods and with crossfitting. Different than in other models, there is no orthogonal score, but a similar global insensitivity property holds (for details see <a class="reference external" href="https://arxiv.org/abs/2107.07942">Noack, Olma and Rothe (2024)</a>). We adjust the outcome variable by the influence of the covariates.</p>
<p>This reduces the variance in the estimation potentially even further to:</p>
<div class="math notranslate nohighlight">
\[V(\eta) = \frac{\bar{\kappa}}{f_X(0)} \left( \mathbb{V}[M_i(\eta) | S_i = 0^+] + \mathbb{V}[M_i(\eta) | S_i = 0^-] \right).\]</div>
<p>with <span class="math notranslate nohighlight">\(\bar{\kappa}\)</span> being a kernel constant. To maximize the precision of the estimator <span class="math notranslate nohighlight">\(\widehat\tau(h;\eta)\)</span> for any particular bandwidth <span class="math notranslate nohighlight">\(h\)</span>, <span class="math notranslate nohighlight">\(\eta\)</span> has to be chosen such that <span class="math notranslate nohighlight">\(V(\eta)\)</span> is as small as possible. The equally-weighted average of the left and right limits of the conditional expectation function <span class="math notranslate nohighlight">\(\mathbb{E}[Y_i|S_i=s,X_i=x]\)</span> at the cutoff achieves this goal. According to <a class="reference external" href="https://arxiv.org/abs/2107.07942">Noack, Olma and Rothe (2024)</a>, it holds:</p>
<div class="math notranslate nohighlight">
\[V(\eta) \geq V(\eta_0) \text{ for all } \eta,\]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[\eta_0(x) = \frac{1}{2} \left( \mu_0^+(x) + \mu_0^-(x) \right), \quad \mu_0^\star(x) = \mathbb{E}[Y_i | S_i = 0^\star, X_i = x] \text{ for } \star \in \{+, -\}.\]</div>
<p><code class="docutils literal notranslate"><span class="pre">RDFlex</span></code> implements this regression discontinuity design with <span class="math notranslate nohighlight">\(\eta_0\)</span> being estimated by user-specified ML methods. The indicator <code class="docutils literal notranslate"><span class="pre">fuzzy=False</span></code> indicates a sharp design. The <code class="docutils literal notranslate"><span class="pre">DoubleMLRDDData</span></code> object has to be defined with the arguments:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">y_col</span></code> refers to the observed outcome, on which we want to estimate the effect at the cutoff</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">score_col</span></code> refers to the score</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x_cols</span></code> refers to the covariates to be adjusted for</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">d_cols</span></code> is an indicator of whether an observation is treated or not. In the sharp design, this should be identical to an indicator of whether an observation is left or right of the cutoff (<span class="math notranslate nohighlight">\(D_i = \mathbb{I}[S_i &gt; c]\)</span>)</p></li>
</ul>
</div></blockquote>
<p>Estimation is conducted via its <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-21" name="sd-tab-set-15" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="py" for="sd-tab-item-21">
Python</label><div class="sd-tab-content docutils">
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="gp">In [2]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="gp">In [3]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LassoCV</span>

<span class="gp">In [4]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">doubleml.rdd.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_simple_rdd_data</span>

<span class="gp">In [5]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">doubleml.rdd</span><span class="w"> </span><span class="kn">import</span> <span class="n">RDFlex</span>

<span class="gp">In [6]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">doubleml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dml</span>

<span class="gp">In [7]: </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="gp">In [8]: </span><span class="n">data_dict</span> <span class="o">=</span> <span class="n">make_simple_rdd_data</span><span class="p">(</span><span class="n">n_obs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">fuzzy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="gp">In [9]: </span><span class="n">cov_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;x&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">data_dict</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

<span class="gp">In [10]: </span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">data_dict</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">],</span> <span class="n">data_dict</span><span class="p">[</span><span class="s1">&#39;D&#39;</span><span class="p">],</span> <span class="n">data_dict</span><span class="p">[</span><span class="s1">&#39;score&#39;</span><span class="p">],</span> <span class="n">data_dict</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">])),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;score&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">cov_names</span><span class="p">)</span>

<span class="gp">In [11]: </span><span class="n">dml_data</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLRDDData</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">y_col</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">d_cols</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">x_cols</span><span class="o">=</span><span class="n">cov_names</span><span class="p">,</span> <span class="n">score_col</span><span class="o">=</span><span class="s1">&#39;score&#39;</span><span class="p">)</span>

<span class="gp">In [12]: </span><span class="n">ml_g</span> <span class="o">=</span> <span class="n">LassoCV</span><span class="p">()</span>

<span class="gp">In [13]: </span><span class="n">rdflex_obj</span> <span class="o">=</span> <span class="n">RDFlex</span><span class="p">(</span><span class="n">dml_data</span><span class="p">,</span> <span class="n">ml_g</span><span class="p">,</span> <span class="n">fuzzy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="gp">In [14]: </span><span class="n">rdflex_obj</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="gh">Out[14]: </span><span class="go">&lt;doubleml.rdd.rdd.RDFlex at 0x7feafd43a780&gt;</span>

<span class="gp">In [15]: </span><span class="nb">print</span><span class="p">(</span><span class="n">rdflex_obj</span><span class="p">)</span>
<span class="go">Method             Coef.     S.E.     t-stat       P&gt;|t|           95% CI</span>
<span class="gt">-------------------------------------------------------------------------</span>
<span class="n">Conventional</span>      <span class="mf">1.290</span>     <span class="mf">0.564</span>     <span class="mf">2.285</span>    <span class="mf">2.232e-02</span>  <span class="p">[</span><span class="mf">0.183</span><span class="p">,</span> <span class="mf">2.396</span><span class="p">]</span>
<span class="n">Robust</span>                 <span class="o">-</span>        <span class="o">-</span>     <span class="mf">2.053</span>    <span class="mf">4.005e-02</span>  <span class="p">[</span><span class="mf">0.062</span><span class="p">,</span> <span class="mf">2.660</span><span class="p">]</span>
<span class="n">Design</span> <span class="n">Type</span><span class="p">:</span>        <span class="n">Sharp</span>
<span class="ne">Cutoff</span>:             0
<span class="n">First</span> <span class="n">Stage</span> <span class="n">Kernel</span><span class="p">:</span> <span class="n">triangular</span>
<span class="n">Final</span> <span class="n">Bandwidth</span><span class="p">:</span>    <span class="p">[</span><span class="mf">0.63117311</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="fuzzy-regression-discontinuity-design">
<h3><span class="section-number">3.5.2. </span>Fuzzy Regression Discontinuity Design<a class="headerlink" href="#fuzzy-regression-discontinuity-design" title="Link to this heading">#</a></h3>
<p>In a <strong>Fuzzy RDD</strong>, treatment assignment <span class="math notranslate nohighlight">\(T_i\)</span> is identical to the sharp RDD (<span class="math notranslate nohighlight">\(T_i = \mathbb{1}\{S_i \geq c\}\)</span>), however, compliance is limited around the cutoff which leads to a different treatment received <span class="math notranslate nohighlight">\(D_i\)</span> than assigned (<span class="math notranslate nohighlight">\(D_i \neq T_i\)</span>) for some units.</p>
<p>The parameter of interest in the Fuzzy RDD is the average treatment effect at the cutoff, for all individuals that comply with the assignment</p>
<div class="math notranslate nohighlight">
\[\theta_{0} = \mathbb{E}[Y_i(1)-Y_i(0)\mid S_i = c, \{i\in \text{compliers}\}]\]</div>
<p>with <span class="math notranslate nohighlight">\(Y_i(D_i(T_i))\)</span> being the potential outcome under the potential treatments. This effect is identified by</p>
<div class="math notranslate nohighlight">
\[\theta_{0} = \frac{\lim_{s \to c^+} \mathbb{E}[Y_i \mid S_i = s] - \lim_{s \to c^-} \mathbb{E}[Y_i \mid S_i = s]}{\lim_{s \to c^+} \mathbb{E}[D_i \mid S_i = s] - \lim_{s \to c^-} \mathbb{E}[D_i \mid S_i = s]}\]</div>
<p>The assumptions for identifying the ATT in a fuzzy RDD are:</p>
<ul class="simple">
<li><p><strong>Continuity of Potential Outcomes:</strong> Similar to sharp RDD, the conditional mean of the potential outcomes <span class="math notranslate nohighlight">\(\mathbb{E}[Y_i(d)\mid S_i=s]\)</span> for <span class="math notranslate nohighlight">\(d \in \{0, 1\}\)</span> is continuous at the cutoff level <span class="math notranslate nohighlight">\(c\)</span>.</p></li>
<li><p><strong>Continuity of Treatment Assignment Probability:</strong> The probability of receiving treatment <span class="math notranslate nohighlight">\(\mathbb{E}[D_i | S_i = s]\)</span> must change discontinuously at the cutoff, but there should be no other jumps in the probability.</p></li>
<li><p><strong>Monotonicity:</strong> There must be no “defiers”, meaning individuals for whom the treatment assignment goes in the opposite direction of the score.</p></li>
</ul>
<p>Under similar considerations as in the sharp case, an estimator using flexible covariate adjustment can be derived as:</p>
<div class="math notranslate nohighlight">
\[\hat{\theta}(h; \widehat{\eta}_Y, \widehat{\eta}_D) = \frac{\hat{\tau}_Y(h; \widehat{\eta}_Y)}{\hat{\tau}_D(h; \widehat{\eta}_D)}
= \frac{\sum_{i=1}^n w_{i}(h) (Y_i - \widehat{\eta}_{Y}(X_i))}{\sum_{i=1}^n w_{i}(h) (T_i - \widehat{\eta}_{D}(X_i))},\]</div>
<p>where <span class="math notranslate nohighlight">\(\eta_Y\)</span> and <span class="math notranslate nohighlight">\(\eta_D\)</span> are defined as in the sharp RDD setting, with the respective outcome.</p>
<p><code class="docutils literal notranslate"><span class="pre">RDFlex</span></code> implements this fuzzy RDD with flexible covariate adjustment. The indicator <code class="docutils literal notranslate"><span class="pre">fuzzy=True</span></code> indicates a fuzzy design. The <code class="docutils literal notranslate"><span class="pre">DoubleMLRDDData</span></code> object has to be defined with the arguments:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">y_col</span></code> refers to the observed outcome, on which we want to estimate the effect at the cutoff</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">score_col</span></code> refers to the score</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x_cols</span></code> refers to the covariates to be adjusted for</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">d_cols</span></code> is an indicator of whether an observation is treated or not. In the fuzzy design, this should <strong>not</strong> be identical to an indicator of whether an observation is left or right of the cutoff (<span class="math notranslate nohighlight">\(D_i \neq \mathbb{I}[S_i &gt; c]\)</span>)</p></li>
</ul>
</div></blockquote>
<p>Estimation is conducted via its <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-22" name="sd-tab-set-16" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="py" for="sd-tab-item-22">
Python</label><div class="sd-tab-content docutils">
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [16]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="gp">In [17]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="gp">In [18]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LassoCV</span><span class="p">,</span> <span class="n">LogisticRegressionCV</span>

<span class="gp">In [19]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">doubleml.rdd.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_simple_rdd_data</span>

<span class="gp">In [20]: </span><span class="kn">from</span><span class="w"> </span><span class="nn">doubleml.rdd</span><span class="w"> </span><span class="kn">import</span> <span class="n">RDFlex</span>

<span class="gp">In [21]: </span><span class="kn">import</span><span class="w"> </span><span class="nn">doubleml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dml</span>

<span class="gp">In [22]: </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="gp">In [23]: </span><span class="n">data_dict</span> <span class="o">=</span> <span class="n">make_simple_rdd_data</span><span class="p">(</span><span class="n">n_obs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">fuzzy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="gp">In [24]: </span><span class="n">cov_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;x&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">data_dict</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

<span class="gp">In [25]: </span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">data_dict</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">],</span> <span class="n">data_dict</span><span class="p">[</span><span class="s1">&#39;D&#39;</span><span class="p">],</span> <span class="n">data_dict</span><span class="p">[</span><span class="s1">&#39;score&#39;</span><span class="p">],</span> <span class="n">data_dict</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">])),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;score&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">cov_names</span><span class="p">)</span>

<span class="gp">In [26]: </span><span class="n">dml_data</span> <span class="o">=</span> <span class="n">dml</span><span class="o">.</span><span class="n">DoubleMLRDDData</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">y_col</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">d_cols</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">x_cols</span><span class="o">=</span><span class="n">cov_names</span><span class="p">,</span> <span class="n">score_col</span><span class="o">=</span><span class="s1">&#39;score&#39;</span><span class="p">)</span>

<span class="gp">In [27]: </span><span class="n">ml_g</span> <span class="o">=</span> <span class="n">LassoCV</span><span class="p">()</span>

<span class="gp">In [28]: </span><span class="n">ml_m</span> <span class="o">=</span> <span class="n">LogisticRegressionCV</span><span class="p">()</span>

<span class="gp">In [29]: </span><span class="n">rdflex_obj</span> <span class="o">=</span> <span class="n">RDFlex</span><span class="p">(</span><span class="n">dml_data</span><span class="p">,</span> <span class="n">ml_g</span><span class="p">,</span> <span class="n">ml_m</span><span class="p">,</span> <span class="n">fuzzy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="gp">In [30]: </span><span class="n">rdflex_obj</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="gh">Out[30]: </span><span class="go">&lt;doubleml.rdd.rdd.RDFlex at 0x7feafdb31790&gt;</span>

<span class="gp">In [31]: </span><span class="nb">print</span><span class="p">(</span><span class="n">rdflex_obj</span><span class="p">)</span>
<span class="go">Method             Coef.     S.E.     t-stat       P&gt;|t|           95% CI</span>
<span class="gt">-------------------------------------------------------------------------</span>
<span class="n">Conventional</span>      <span class="mf">3.343</span>     <span class="mf">5.164</span>     <span class="mf">0.647</span>    <span class="mf">5.174e-01</span>  <span class="p">[</span><span class="o">-</span><span class="mf">6.779</span><span class="p">,</span> <span class="mf">13.465</span><span class="p">]</span>
<span class="n">Robust</span>                 <span class="o">-</span>        <span class="o">-</span>     <span class="mf">0.657</span>    <span class="mf">5.110e-01</span>  <span class="p">[</span><span class="o">-</span><span class="mf">7.789</span><span class="p">,</span> <span class="mf">15.649</span><span class="p">]</span>
<span class="n">Design</span> <span class="n">Type</span><span class="p">:</span>        <span class="n">Fuzzy</span>
<span class="ne">Cutoff</span>:             0
<span class="n">First</span> <span class="n">Stage</span> <span class="n">Kernel</span><span class="p">:</span> <span class="n">triangular</span>
<span class="n">Final</span> <span class="n">Bandwidth</span><span class="p">:</span>    <span class="p">[</span><span class="mf">0.61458483</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="implementation-details">
<span id="rdd-imp-details"></span><h3><span class="section-number">3.5.3. </span>Implementation Details<a class="headerlink" href="#implementation-details" title="Link to this heading">#</a></h3>
<p>There are some specialities in the <code class="docutils literal notranslate"><span class="pre">RDFlex</span></code> implementation that differ from the rest of the package and thus deserve to be pointed out here.</p>
<ol class="arabic simple">
<li><p><strong>Bandwidth Selection</strong>: The bandwidth is a crucial tuning parameter for RDD algorithms. By default, our implementation uses the <code class="docutils literal notranslate"><span class="pre">rdbwselect</span></code> method from the <code class="docutils literal notranslate"><span class="pre">rdrobust</span></code> library for an initial selection. This can be overridden by the user using the parameter <code class="docutils literal notranslate"><span class="pre">h_fs</span></code>. Since covariate adjustment and RDD fitting are interacting, by default, we repeat the bandwidth selection and nuisance estimation steps once in the <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method. This can be adjusted by <code class="docutils literal notranslate"><span class="pre">n_iterations</span></code>.</p></li>
<li><p><strong>Kernel Selection</strong>: Another crucial decision when estimating with RDD is the kernel determining the weights for observations around the cutoff. For this, the parameters <code class="docutils literal notranslate"><span class="pre">fs_kernel</span></code> and <code class="docutils literal notranslate"><span class="pre">kernel</span></code> are important. The latter is a key-worded argument and is used in the RDD estimation, while the <code class="docutils literal notranslate"><span class="pre">fs_kernel</span></code> specifies the kernel used in the nuisance estimation. By default, both of them are <code class="docutils literal notranslate"><span class="pre">triangular</span></code>.</p></li>
<li><p><strong>Local and Global Learners</strong>: <code class="docutils literal notranslate"><span class="pre">RDFlex</span></code> estimates the nuisance functions locally around the cutoff. In certain scenarios, it can be desirable to rather perform a global fit on the full support of the score <span class="math notranslate nohighlight">\(S\)</span>. For this, the <code class="docutils literal notranslate"><span class="pre">Global</span> <span class="pre">Learners</span></code> in <code class="docutils literal notranslate"><span class="pre">doubleml.utils</span></code> can be used (see our example notebook in the <a class="reference internal" href="../examples/index.html#examplegallery"><span class="std std-ref">Example Gallery</span></a>).</p></li>
<li><p><strong>First Stage Specifications</strong>: In nuisance estimation, we have to add variable(s) to add information about the location of the observation left or right of the cutoff. Available options are:
In the default case <code class="docutils literal notranslate"><span class="pre">fs_specification=&quot;cutoff&quot;</span></code>, this is an indicator of whether the observation is left or right.
If <code class="docutils literal notranslate"><span class="pre">fs_specification=&quot;cutoff</span> <span class="pre">and</span> <span class="pre">score&quot;</span></code>, additionally the score is added.
In the case of <code class="docutils literal notranslate"><span class="pre">fs_specification=&quot;interacted</span> <span class="pre">cutoff</span> <span class="pre">and</span> <span class="pre">score&quot;</span></code>, also an interaction term of the cutoff indicator and the score is added.</p></li>
<li><p><strong>Intention-to-Treat Effects</strong>: Above, we demonstrated how to estimate the ATE at the cutoff in a fuzzy RDD. To estimate an Intention-to-Treat effect instead, the parameter <code class="docutils literal notranslate"><span class="pre">fuzzy=False</span></code> can be selected.</p></li>
<li><p><strong>Key-worded Arguments</strong>: <code class="docutils literal notranslate"><span class="pre">rdrobust</span></code> as the underlying RDD library has additional parameters to tune the estimation. You can use <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> to add them via <code class="docutils literal notranslate"><span class="pre">RDFlex</span></code>.</p></li>
</ol>
</section>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="data_backend.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">2. </span>Data Backend</p>
      </div>
    </a>
    <a class="right-next"
       href="heterogeneity.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4. </span>Heterogeneous treatment effects</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partially-linear-models-plm">3.1. Partially linear models (PLM)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partially-linear-regression-model-plr">3.1.1. Partially linear regression model (PLR)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-partially-linear-regression-model-lplr">3.1.2. Logistic partially linear regression model (LPLR)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partially-linear-panel-regression-model-plpr">3.1.3. Partially linear panel regression model (PLPR)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partially-linear-iv-regression-model-pliv">3.1.4. Partially linear IV regression model (PLIV)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interactive-regression-models-irm">3.2. Interactive regression models (IRM)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-interactive-regression-model-irm">3.2.1. Binary Interactive Regression Model (IRM)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#average-potential-outcomes-apos">3.2.2. Average Potential Outcomes (APOs)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#average-potential-outcomes-apos-for-multiple-treatment-levels">3.2.3. Average Potential Outcomes (APOs) for Multiple Treatment Levels</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interactive-iv-model-iivm">3.2.4. Interactive IV model (IIVM)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-in-differences-models-did">3.3. Difference-in-Differences Models (DID)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters-implementation">3.3.1. Parameters &amp; Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#panel-data">3.3.2. Panel data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#repeated-cross-sections">3.3.3. Repeated cross-sections</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#effect-aggregation">3.3.4. Effect Aggregation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#two-treatment-periods">3.3.5. Two treatment periods</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">3.3.5.1. Panel Data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">3.3.5.2. Repeated cross-sections</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-selection-models-ssm">3.4. Sample Selection Models (SSM)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#missingness-at-random">3.4.1. Missingness at Random</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nonignorable-nonresponse">3.4.2. Nonignorable Nonresponse</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-discontinuity-designs-rdd">3.5. Regression Discontinuity Designs (RDD)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sharp-regression-discontinuity-design">3.5.1. Sharp Regression Discontinuity Design</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fuzzy-regression-discontinuity-design">3.5.2. Fuzzy Regression Discontinuity Design</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-details">3.5.3. Implementation Details</a></li>
</ul>
</li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/DoubleML/doubleml-docs/edit/dev/doc/guide/models.rst">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>

  <div class="sidebar-secondary-item">

  <div class="tocsection sourcelink">
    <a href="../_sources/guide/models.rst.txt">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2023, Bach, P., Chernozhukov, V., Klaassen, S., Kurz, M. S., and Spindler, M..
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.3.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>