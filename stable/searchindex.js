Search.setIndex({"alltitles": {"(One-Way) Cluster Robust Double Machine Learing": [[62, "(One-Way)-Cluster-Robust-Double-Machine-Learing"], [89, "(One-Way)-Cluster-Robust-Double-Machine-Learing"]], "0. Problem Formulation": [[160, "problem-formulation"]], "1. Data Backend": [[160, "data-backend"]], "1. Formulation of Causal Model & Identification Assumptions": [[98, "1.-Formulation-of-Causal-Model-&-Identification-Assumptions"]], "2. Causal Model": [[160, "causal-model"]], "2. Estimation of Causal Effect": [[98, "2.-Estimation-of-Causal-Effect"]], "3. ML Methods": [[160, "ml-methods"]], "3. Sensitivity Analysis": [[98, "3.-Sensitivity-Analysis"]], "4. Benchmarking Analysis": [[98, "4.-Benchmarking-Analysis"]], "4. DML Specifications": [[160, "dml-specifications"]], "5. Conclusion": [[98, "5.-Conclusion"]], "5. Estimation": [[160, "estimation"]], "6. Inference": [[160, "inference"]], "7. Sensitivity Analysis": [[160, "sensitivity-analysis"]], "A Motivating Example: Two-Way Cluster Robust DML": [[62, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"], [89, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"]], "API Reference": [[0, null]], "ATE Estimation and Sensitivity": [[84, "ATE-Estimation-and-Sensitivity"]], "ATE estimates distribution": [[65, "ATE-estimates-distribution"], [65, "id3"], [99, "ATE-estimates-distribution"], [99, "id3"]], "ATT Estimation": [[68, "ATT-Estimation"], [68, "id1"], [70, "ATT-Estimation"], [71, "ATT-Estimation"], [71, "id1"]], "ATT Estimation: Conditional Parallel Trends": [[69, "ATT-Estimation:-Conditional-Parallel-Trends"]], "ATT Estimation: Unconditional Parallel Trends": [[69, "ATT-Estimation:-Unconditional-Parallel-Trends"]], "ATTE Estimation": [[66, "ATTE-Estimation"], [66, "id2"]], "Acknowledgements": [[155, "acknowledgements"]], "Acknowledgements and Final Remarks": [[61, "Acknowledgements-and-Final-Remarks"]], "Additional Results: CATE estimates": [[92, "Additional-Results:-CATE-estimates"]], "Advanced: External Predictions": [[110, "advanced-external-predictions"]], "Advanced: Global and Local Learners, Stacked Ensembles": [[95, "Advanced:-Global-and-Local-Learners,-Stacked-Ensembles"]], "Aggregated Effects": [[68, "Aggregated-Effects"], [71, "Aggregated-Effects"]], "Aggregation Details": [[68, "Aggregation-Details"], [69, "Aggregation-Details"], [70, "Aggregation-Details"], [71, "Aggregation-Details"]], "Algorithm DML1": [[100, "algorithm-dml1"]], "Algorithm DML2": [[100, "algorithm-dml2"]], "All Combinations": [[68, "All-Combinations"]], "All combinations": [[71, "All-combinations"]], "Anticipation": [[68, "Anticipation"], [71, "Anticipation"]], "Application Results": [[62, "Application-Results"], [89, "Application-Results"]], "Application: 401(k)": [[97, "Application:-401(k)"]], "AutoML with less Computation time": [[88, "AutoML-with-less-Computation-time"]], "Average Potential Outcome (APOs)": [[75, "Average-Potential-Outcome-(APOs)"]], "Average Potential Outcomes (APOs)": [[111, "average-potential-outcomes-apos"], [127, "average-potential-outcomes-apos"], [144, "average-potential-outcomes-apos"]], "Average Potential Outcomes (APOs) for Multiple Treatment Levels": [[111, "average-potential-outcomes-apos-for-multiple-treatment-levels"]], "Average Treatment Effect": [[85, "Average-Treatment-Effect"]], "Average Treatment Effect on the Treated": [[85, "Average-Treatment-Effect-on-the-Treated"]], "Basics": [[68, "Basics"], [71, "Basics"]], "Benchmarking": [[144, "benchmarking"]], "Benchmarking Analysis": [[97, "Benchmarking-Analysis"]], "Binary Interactive Regression Model (IRM)": [[111, "binary-interactive-regression-model-irm"], [127, "binary-interactive-regression-model-irm"]], "CATEs for IRM models": [[109, "cates-for-irm-models"]], "CATEs for PLR models": [[109, "cates-for-plr-models"]], "CVaR Treatment Effects": [[80, "CVaR-Treatment-Effects"]], "CVaR of potential outcomes": [[109, "cvar-of-potential-outcomes"]], "CVaR treatment effects": [[109, "cvar-treatment-effects"]], "Causal Analysis with DoubleML": [[98, "Causal-Analysis-with-DoubleML"]], "Causal Contrasts": [[75, "Causal-Contrasts"]], "Causal Research Question": [[69, "Causal-Research-Question"]], "Causal estimation vs. lasso penalty \\lambda": [[81, "Causal-estimation-vs.-lasso-penalty-\\lambda"]], "Chernozhukov et al. (2023): Benchmarking": [[98, "Chernozhukov-et-al.-(2023):-Benchmarking"]], "Citation": [[155, "citation"]], "Cluster Robust Cross Fitting": [[62, "Cluster-Robust-Cross-Fitting"], [89, "Cluster-Robust-Cross-Fitting"]], "Cluster Robust Standard Errors": [[62, "Cluster-Robust-Standard-Errors"], [89, "Cluster-Robust-Standard-Errors"]], "Clustering and double machine learning": [[62, "Clustering-and-double-machine-learning"], [89, "Clustering-and-double-machine-learning"]], "Combined loss vs. lasso penalty \\lambda": [[81, "Combined-loss-vs.-lasso-penalty-\\lambda"]], "Compare Metrics for Nuisance Estimation": [[88, "Compare-Metrics-for-Nuisance-Estimation"]], "Comparing different learners": [[86, "Comparing-different-learners"]], "Comparison and summary": [[88, "Comparison-and-summary"]], "Comparison to AutoML with less Computation time and Untuned XGBoost Learners": [[88, "Comparison-to-AutoML-with-less-Computation-time-and-Untuned-XGBoost-Learners"]], "Comparison to did package": [[61, "Comparison-to-did-package"]], "Computation time": [[86, "Computation-time"]], "Conclusion": [[88, "Conclusion"]], "Conditional Value at Risk (CVaR)": [[80, "Conditional-Value-at-Risk-(CVaR)"]], "Conditional average treatment effects (CATEs)": [[109, "conditional-average-treatment-effects-cates"]], "Conditional value at risk (CVaR)": [[109, "conditional-value-at-risk-cvar"], [127, "conditional-value-at-risk-cvar"]], "Confidence bands and multiplier bootstrap for valid simultaneous inference": [[143, "confidence-bands-and-multiplier-bootstrap-for-valid-simultaneous-inference"]], "Control Groups": [[68, "Control-Groups"], [71, "Control-Groups"]], "Coverage Simulation": [[66, "Coverage-Simulation"], [66, "id3"]], "Creating the DoubleMLData Object": [[74, "Creating-the-DoubleMLData-Object"]], "Cross-fitting with K folds": [[126, "cross-fitting-with-k-folds"]], "Cross-fitting, DML algorithms and Neyman-orthogonal score functions": [[157, "cross-fitting-dml-algorithms-and-neyman-orthogonal-score-functions"]], "Custom evaluation metrics": [[86, "Custom-evaluation-metrics"]], "DML: Bonus Data": [[72, null]], "Data": [[63, "Data"], [65, "Data"], [65, "id1"], [66, "Data"], [66, "id1"], [68, "Data"], [69, "Data"], [70, "Data"], [71, "Data"], [78, "Data"], [79, "Data"], [80, "Data"], [82, "Data"], [83, "Data"], [84, "Data"], [85, "Data"], [87, "Data"], [90, "Data"], [91, "Data"], [93, "Data"], [94, "Data"], [94, "id1"], [97, "Data"], [99, "Data"], [99, "id1"], [157, "data"]], "Data Backend": [[107, null]], "Data Description": [[68, "Data-Description"], [71, "Data-Description"]], "Data Details": [[68, "Data-Details"], [71, "Data-Details"]], "Data Generating Process (DGP)": [[60, "Data-Generating-Process-(DGP)"], [74, "Data-Generating-Process-(DGP)"], [75, "Data-Generating-Process-(DGP)"], [77, "Data-Generating-Process-(DGP)"]], "Data Generation": [[88, "Data-Generation"]], "Data Simulation": [[59, "Data-Simulation"], [76, "Data-Simulation"]], "Data and Effect Estimation": [[97, "Data-and-Effect-Estimation"]], "Data generating process": [[101, "data-generating-process"]], "Data preprocessing": [[64, "Data-preprocessing"]], "Data with Anticipation": [[68, "Data-with-Anticipation"], [71, "Data-with-Anticipation"]], "Data-Backend for Cluster Data": [[62, "Data-Backend-for-Cluster-Data"], [89, "Data-Backend-for-Cluster-Data"]], "Dataset Generators": [[2, "dataset-generators"]], "Dataset Loaders": [[2, "dataset-loaders"]], "Datasets": [[2, null]], "Define Helper Functions for Plotting": [[62, "Define-Helper-Functions-for-Plotting"], [89, "Define-Helper-Functions-for-Plotting"]], "Demo Example from did": [[61, "Demo-Example-from-did"]], "Details on Predictive Performance": [[61, "Details-on-Predictive-Performance"]], "Difference-in-Differences": [[73, "difference-in-differences"]], "Difference-in-Differences Models": [[127, "difference-in-differences-models"], [144, "difference-in-differences-models"]], "Difference-in-Differences Models (DID)": [[111, "difference-in-differences-models-did"]], "Difference-in-Differences for Panel Data": [[144, "difference-in-differences-for-panel-data"]], "Difference-in-Differences for repeated cross-sections": [[144, "difference-in-differences-for-repeated-cross-sections"]], "Disclaimer": [[98, "Disclaimer"]], "Double Machine Learning Algorithm": [[155, "double-machine-learning-algorithm"]], "Double Machine Learning Literature": [[158, null]], "Double machine learning algorithms": [[100, null]], "Double/debiased machine learning": [[60, "Double/debiased-machine-learning"], [77, "Double/debiased-machine-learning"], [101, "double-debiased-machine-learning"]], "DoubleML": [[155, null]], "DoubleML Data Class": [[1, null]], "DoubleML Models": [[3, null]], "DoubleML Object": [[97, "DoubleML-Object"]], "DoubleML Workflow": [[160, null]], "DoubleML meets FLAML - How to tune learners automatically within DoubleML": [[88, null]], "DoubleML with TabPFN": [[74, "DoubleML-with-TabPFN"]], "DoubleMLDIDData": [[107, "doublemldiddata"]], "DoubleMLData": [[107, "doublemldata"]], "DoubleMLData from arrays and matrices": [[102, "doublemldata-from-arrays-and-matrices"], [107, "doublemldata-from-arrays-and-matrices"]], "DoubleMLData from dataframes": [[102, null], [107, "doublemldata-from-dataframes"]], "DoubleMLPanelData": [[68, "DoubleMLPanelData"], [71, "DoubleMLPanelData"], [107, "doublemlpaneldata"]], "DoubleMLRDDData": [[107, "doublemlrdddata"]], "DoubleMLSSMData": [[107, "doublemlssmdata"]], "Effect Aggregation": [[69, "Effect-Aggregation"], [70, "Effect-Aggregation"], [111, "effect-aggregation"]], "Effect Heterogeneity": [[73, "effect-heterogeneity"], [85, "Effect-Heterogeneity"]], "Empirical coverage vs. lasso penalty \\lambda": [[81, "Empirical-coverage-vs.-lasso-penalty-\\lambda"]], "Estimate DML models without sample-splitting": [[126, "estimate-dml-models-without-sample-splitting"]], "Estimate double/debiased machine learning models": [[157, "estimate-double-debiased-machine-learning-models"]], "Estimating Potential Quantiles and Quantile Treatment Effects": [[91, "Estimating-Potential-Quantiles-and-Quantile-Treatment-Effects"]], "Estimating local quantile treatment effects (LQTEs)": [[91, "Estimating-local-quantile-treatment-effects-(LQTEs)"]], "Estimating the Average Treatment Effect of 401(k) Eligibility on Net Financial Assets": [[63, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"], [90, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"]], "Estimating the treatment effect on the Conditional Value a Risk (CVaR)": [[91, "Estimating-the-treatment-effect-on-the-Conditional-Value-a-Risk-(CVaR)"]], "Estimation": [[65, "Estimation"], [65, "id2"], [99, "Estimation"], [99, "id2"]], "Estimation of Average Potential Outcomes": [[74, "Estimation-of-Average-Potential-Outcomes"]], "Estimation quality vs. \\lambda": [[81, "Estimation-quality-vs.-\\lambda"]], "Evaluate learners": [[110, "evaluate-learners"]], "Event Study Aggregation": [[68, "Event-Study-Aggregation"], [69, "Event-Study-Aggregation"], [70, "Event-Study-Aggregation"], [71, "Event-Study-Aggregation"]], "Example usage": [[103, "example-usage"], [104, "example-usage"], [105, "example-usage"], [106, "example-usage"], [107, "example-usage"], [107, "id6"], [107, "id8"], [107, "id10"]], "Examples": [[73, null]], "Exploiting the Functionalities of did": [[61, "Exploiting-the-Functionalities-of-did"]], "Externally provide a sample splitting / partition": [[126, "externally-provide-a-sample-splitting-partition"]], "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)": [[95, null]], "Fuzzy RDD": [[95, "Fuzzy-RDD"]], "Fuzzy RDD Without Adjustment": [[95, "Fuzzy-RDD-Without-Adjustment"]], "Fuzzy RDD with Flexible Adjustment": [[95, "Fuzzy-RDD-with-Flexible-Adjustment"]], "Fuzzy RDD with Linear Adjustment": [[95, "Fuzzy-RDD-with-Linear-Adjustment"]], "Fuzzy Regression Discontinuity Design": [[111, "fuzzy-regression-discontinuity-design"]], "GATE Estimation and Sensitivity": [[84, "GATE-Estimation-and-Sensitivity"]], "GATET Estimation and Sensitivity": [[84, "GATET-Estimation-and-Sensitivity"]], "GATEs for IRM models": [[109, "gates-for-irm-models"]], "GATEs for PLR models": [[109, "gates-for-plr-models"]], "General Examples": [[73, "general-examples"]], "General algorithm": [[144, "general-algorithm"]], "Generate Fuzzy Data": [[95, "Generate-Fuzzy-Data"]], "Generate Sharp Data": [[95, "Generate-Sharp-Data"]], "Getting Started": [[157, null]], "Group Aggregation": [[68, "Group-Aggregation"], [70, "Group-Aggregation"], [71, "Group-Aggregation"]], "Group Average Treatment Effects (GATEs)": [[82, "Group-Average-Treatment-Effects-(GATEs)"], [83, "Group-Average-Treatment-Effects-(GATEs)"]], "Group average treatment effects (GATEs)": [[109, "group-average-treatment-effects-gates"]], "Group-Time Combinations": [[68, "Group-Time-Combinations"], [71, "Group-Time-Combinations"]], "Heterogeneous treatment effects": [[109, null]], "How to exploit more features of mlr3pipelines in DoubleML": [[64, "How-to-exploit-more-features-of-mlr3pipelines-in-DoubleML"]], "Hyperparameter tuning": [[110, "hyperparameter-tuning"], [110, "id16"]], "Hyperparameter tuning with pipelines": [[110, "hyperparameter-tuning-with-pipelines"]], "Implementation": [[144, "implementation"]], "Implementation Details": [[111, "implementation-details"]], "Implementation of the double machine learning algorithms": [[100, "implementation-of-the-double-machine-learning-algorithms"]], "Implementation of the score function and the estimate of the causal parameter": [[127, "implementation-of-the-score-function-and-the-estimate-of-the-causal-parameter"]], "Implemented Neyman orthogonal score functions": [[127, "implemented-neyman-orthogonal-score-functions"]], "Imports and Setup": [[74, "Imports-and-Setup"]], "Initialize DoubleMLClusterData object": [[62, "Initialize-DoubleMLClusterData-object"]], "Initialize DoubleMLData object with clusters": [[89, "Initialize-DoubleMLData-object-with-clusters"]], "Initialize the objects of class DoubleMLPLIV": [[62, "Initialize-the-objects-of-class-DoubleMLPLIV"], [89, "Initialize-the-objects-of-class-DoubleMLPLIV"]], "Installing DoubleML": [[156, null]], "Instrumental Variables Directed Acyclic Graph (IV - DAG)": [[59, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"], [76, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"]], "Interactive IV Model (IIVM)": [[63, "Interactive-IV-Model-(IIVM)"], [90, "Interactive-IV-Model-(IIVM)"]], "Interactive IV model (IIVM)": [[111, "interactive-iv-model-iivm"], [127, "interactive-iv-model-iivm"]], "Interactive Regression Model (IRM)": [[63, "Interactive-Regression-Model-(IRM)"], [82, "Interactive-Regression-Model-(IRM)"], [90, "Interactive-Regression-Model-(IRM)"], [93, "Interactive-Regression-Model-(IRM)"]], "Interactive regression model (IRM)": [[144, "interactive-regression-model-irm"]], "Interactive regression models (IRM)": [[111, "interactive-regression-models-irm"], [127, "interactive-regression-models-irm"], [144, "interactive-regression-models-irm"]], "Key Takeaways": [[74, "Key-Takeaways"]], "Key arguments": [[103, null], [104, null], [105, null], [106, null], [107, "key-arguments"], [107, "id5"], [107, "id7"], [107, "id9"]], "Learners and Hyperparameters": [[85, "Learners-and-Hyperparameters"]], "Learners to estimate the nuisance models": [[157, "learners-to-estimate-the-nuisance-models"]], "Learners, hyperparameters and hyperparameter tuning": [[110, null]], "Linear Covariate Adjustment": [[68, "Linear-Covariate-Adjustment"], [71, "Linear-Covariate-Adjustment"]], "Load Data": [[98, "Load-Data"]], "Load and Process Data": [[62, "Load-and-Process-Data"], [89, "Load-and-Process-Data"]], "Load bonus data using the dml datasets module": [[72, "Load-bonus-data-using-the-dml-datasets-module"]], "Local Average Treatment Effects of 401(k) Participation on Net Financial Assets": [[63, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"], [90, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"]], "Local Potential Quantile Estimation": [[94, "Local-Potential-Quantile-Estimation"]], "Local Potential Quantiles (LPQs)": [[94, "Local-Potential-Quantiles-(LPQs)"]], "Local Quantile Treatment Effects (LQTEs)": [[94, "Local-Quantile-Treatment-Effects-(LQTEs)"]], "Local potential quantiles (LPQs)": [[127, "local-potential-quantiles-lpqs"]], "Logistic partial linear regression (LPLR)": [[127, "logistic-partial-linear-regression-lplr"]], "Logistic partially linear regression model (LPLR)": [[111, "logistic-partially-linear-regression-model-lplr"]], "Machine Learning Methods Comparison": [[74, "Machine-Learning-Methods-Comparison"]], "Main Features": [[155, "main-features"]], "Minimum requirements for learners": [[110, "minimum-requirements-for-learners"], [110, "id2"]], "Missingness at Random": [[111, "missingness-at-random"], [127, "missingness-at-random"]], "Model": [[87, "Model"]], "Model Performance Evaluation": [[74, "Model-Performance-Evaluation"]], "Model-specific implementations": [[144, "model-specific-implementations"]], "Models": [[111, null]], "Motivation": [[62, "Motivation"], [89, "Motivation"]], "Multiple Average Potential Outcome Models (APOS)": [[75, "Multiple-Average-Potential-Outcome-Models-(APOS)"]], "Naive estimation": [[59, "Naive-estimation"], [76, "Naive-estimation"]], "No Clustering / Zero-Way Clustering": [[62, "No-Clustering-/-Zero-Way-Clustering"], [89, "No-Clustering-/-Zero-Way-Clustering"]], "Nonignorable Nonresponse": [[111, "nonignorable-nonresponse"], [127, "nonignorable-nonresponse"]], "One-Way Clustering with Respect to the Market": [[62, "One-Way-Clustering-with-Respect-to-the-Market"], [89, "One-Way-Clustering-with-Respect-to-the-Market"]], "One-Way Clustering with Respect to the Product": [[62, "One-Way-Clustering-with-Respect-to-the-Product"], [89, "One-Way-Clustering-with-Respect-to-the-Product"]], "One-dimensional Example": [[78, "One-dimensional-Example"], [79, "One-dimensional-Example"]], "Outcome missing at random (MAR)": [[65, "Outcome-missing-at-random-(MAR)"], [99, "Outcome-missing-at-random-(MAR)"]], "Outcome missing under nonignorable nonresponse": [[65, "Outcome-missing-under-nonignorable-nonresponse"], [99, "Outcome-missing-under-nonignorable-nonresponse"]], "Overcoming regularization bias by orthogonalization": [[60, "Overcoming-regularization-bias-by-orthogonalization"], [77, "Overcoming-regularization-bias-by-orthogonalization"], [101, "overcoming-regularization-bias-by-orthogonalization"]], "Panel Data": [[111, "id9"], [113, null], [127, "panel-data"], [127, "id3"], [144, "panel-data"]], "Panel Data (Repeated Outcomes)": [[66, "Panel-Data-(Repeated-Outcomes)"]], "Panel data": [[111, "panel-data"]], "Parameter tuning": [[64, "Parameter-tuning"]], "Parameters & Implementation": [[111, "parameters-implementation"]], "Partialling out score": [[60, "Partialling-out-score"], [77, "Partialling-out-score"], [101, "partialling-out-score"]], "Partially Linear Regression Model (PLR)": [[63, "Partially-Linear-Regression-Model-(PLR)"], [83, "Partially-Linear-Regression-Model-(PLR)"], [90, "Partially-Linear-Regression-Model-(PLR)"]], "Partially linear IV regression model (PLIV)": [[111, "partially-linear-iv-regression-model-pliv"], [127, "partially-linear-iv-regression-model-pliv"]], "Partially linear models (PLM)": [[111, "partially-linear-models-plm"], [127, "partially-linear-models-plm"], [144, "partially-linear-models-plm"]], "Partially linear regression model (PLR)": [[111, "partially-linear-regression-model-plr"], [127, "partially-linear-regression-model-plr"], [144, "partially-linear-regression-model-plr"]], "Performance Summary and Insights": [[74, "Performance-Summary-and-Insights"]], "Plot Coefficients and 95% Confidence Intervals": [[88, "Plot-Coefficients-and-95%-Confidence-Intervals"]], "Policy Learning with Trees": [[93, "Policy-Learning-with-Trees"], [109, "policy-learning-with-trees"]], "Potential Quantile Estimation": [[94, "Potential-Quantile-Estimation"]], "Potential Quantiles (PQs)": [[94, "Potential-Quantiles-(PQs)"]], "Potential quantiles (PQs)": [[109, "potential-quantiles-pqs"], [127, "potential-quantiles-pqs"]], "Python: Average Potential Outcome (APO) Models": [[75, null]], "Python: Basic Instrumental Variables calculation": [[76, null]], "Python: Basics of Double Machine Learning": [[77, null]], "Python: Building the package from source": [[156, "python-building-the-package-from-source"]], "Python: Case studies": [[73, "python-case-studies"]], "Python: Causal Machine Learning with TabPFN": [[74, null]], "Python: Choice of learners": [[86, null]], "Python: Cluster Robust Double Machine Learning": [[89, null]], "Python: Conditional Average Treatment Effects (CATEs) for IRM models": [[78, null]], "Python: Conditional Average Treatment Effects (CATEs) for PLR models": [[79, null]], "Python: Conditional Value at Risk of potential outcomes": [[80, null]], "Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments": [[96, null]], "Python: Difference-in-Differences": [[66, null]], "Python: Difference-in-Differences Pre-Testing": [[67, null]], "Python: First Stage and Causal Estimation": [[81, null]], "Python: GATE Sensitivity Analysis": [[84, null]], "Python: Group Average Treatment Effects (GATEs) for IRM models": [[82, null]], "Python: Group Average Treatment Effects (GATEs) for PLR models": [[83, null]], "Python: IRM and APO Model Comparison": [[85, null]], "Python: Impact of 401(k) on Financial Wealth": [[90, null]], "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)": [[91, null]], "Python: Installing DoubleML": [[156, "python-installing-doubleml"]], "Python: Installing a released version from a .whl file": [[156, "python-installing-a-released-version-from-a-whl-file"]], "Python: Installing the latest release from pip or conda": [[156, "python-installing-the-latest-release-from-pip-or-conda"]], "Python: Learners and hyperparameters": [[110, "python-learners-and-hyperparameters"]], "Python: Log-Odds Effects for Logistic PLR models": [[87, null]], "Python: Optional Dependencies": [[156, "python-optional-dependencies"]], "Python: PLM and IRM for Multiple Treatments": [[92, null]], "Python: Panel Data Introduction": [[70, null]], "Python: Panel Data with Multiple Time Periods": [[68, null]], "Python: Policy Learning with Trees": [[93, null]], "Python: Potential Quantiles and Quantile Treatment Effects": [[94, null]], "Python: Real-Data Example for Multi-Period Difference-in-Differences": [[69, null]], "Python: Repeated Cross-Sectional Data with Multiple Time Periods": [[71, null]], "Python: Sample Selection Models": [[99, null]], "Python: Sensitivity Analysis": [[97, null]], "Python: Sensitivity Analysis for Causal ML": [[98, null]], "Quantile Treatment Effects (QTEs)": [[94, "Quantile-Treatment-Effects-(QTEs)"]], "Quantile treatment effects (QTEs)": [[109, "quantile-treatment-effects-qtes"]], "Quantiles": [[109, "quantiles"]], "R: Basic Instrumental Variables Calculation": [[59, null]], "R: Basics of Double Machine Learning": [[60, null]], "R: Case studies": [[73, "r-case-studies"]], "R: Cluster Robust Double Machine Learning": [[62, null]], "R: DoubleML for Difference-in-Differences": [[61, null]], "R: Ensemble Learners and More with mlr3pipelines": [[64, null]], "R: Impact of 401(k) on Financial Wealth": [[63, null]], "R: Installing DoubleML": [[156, "r-installing-doubleml"]], "R: Installing the development version from GitHub": [[156, "r-installing-the-development-version-from-github"]], "R: Installing the latest release from CRAN": [[156, "r-installing-the-latest-release-from-cran"]], "R: Learners and hyperparameters": [[110, "r-learners-and-hyperparameters"]], "R: Sample Selection Models": [[65, null]], "Ranking Treatment Effects under Treatment Propensity and Treatment Effect Heterogeneity": [[92, "Ranking-Treatment-Effects-under-Treatment-Propensity-and-Treatment-Effect-Heterogeneity"]], "Real-Data Application": [[62, "Real-Data-Application"], [89, "Real-Data-Application"]], "References": [[59, "References"], [61, "References"], [62, "References"], [64, "References"], [76, "References"], [81, "References"], [86, "References"], [88, "References"], [89, "References"], [92, "References"], [96, "References"], [98, "References"], [101, "references"], [110, "references"], [126, "references"], [143, "references"], [155, "references"], [157, "references"]], "Regression Discontinuity Designs (RDD)": [[111, "regression-discontinuity-designs-rdd"]], "Regularization Bias in Simple ML-Approaches": [[60, "Regularization-Bias-in-Simple-ML-Approaches"], [77, "Regularization-Bias-in-Simple-ML-Approaches"]], "Regularization bias in simple ML-approaches": [[101, "regularization-bias-in-simple-ml-approaches"]], "Release Notes": [[159, null]], "Repeated Cross-Sectional Data": [[66, "Repeated-Cross-Sectional-Data"], [127, "repeated-cross-sectional-data"], [127, "id4"], [144, "repeated-cross-sectional-data"]], "Repeated cross-fitting with K folds and M repetitions": [[126, "repeated-cross-fitting-with-k-folds-and-m-repetitions"]], "Repeated cross-sections": [[111, "repeated-cross-sections"], [111, "id10"], [113, "repeated-cross-sections"]], "Running a small simulation": [[96, "Running-a-small-simulation"]], "Sample Selection Models": [[127, "sample-selection-models"]], "Sample Selection Models (SSM)": [[111, "sample-selection-models-ssm"]], "Sample splitting to remove bias induced by overfitting": [[60, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [77, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [101, "sample-splitting-to-remove-bias-induced-by-overfitting"]], "Sample-splitting without cross-fitting": [[126, "sample-splitting-without-cross-fitting"]], "Sample-splitting, cross-fitting and repeated cross-fitting": [[126, null]], "Sandbox/Archive": [[73, "sandbox-archive"]], "Score Mixin Classes for DoubleML Models": [[57, null]], "Score functions": [[127, null]], "Selected Combinations": [[68, "Selected-Combinations"], [71, "Selected-Combinations"]], "Sensitivity Analysis": [[68, "Sensitivity-Analysis"], [71, "Sensitivity-Analysis"], [75, "Sensitivity-Analysis"], [85, "Sensitivity-Analysis"], [97, "Sensitivity-Analysis"], [97, "id1"]], "Sensitivity Analysis with IRM": [[97, "Sensitivity-Analysis-with-IRM"]], "Sensitivity analysis": [[144, null]], "Set up learners based on mlr3pipelines": [[64, "Set-up-learners-based-on-mlr3pipelines"]], "Sharp RDD": [[95, "Sharp-RDD"]], "Sharp RDD Without Adjustment": [[95, "Sharp-RDD-Without-Adjustment"]], "Sharp RDD with Flexible Adjustment": [[95, "Sharp-RDD-with-Flexible-Adjustment"]], "Sharp RDD with Linear Adjustment": [[95, "Sharp-RDD-with-Linear-Adjustment"]], "Sharp Regression Discontinuity Design": [[111, "sharp-regression-discontinuity-design"]], "Simulate two-way cluster data": [[62, "Simulate-two-way-cluster-data"], [89, "Simulate-two-way-cluster-data"]], "Simulation Example": [[97, "Simulation-Example"]], "Simultaneous inference over different DoubleML models (advanced)": [[143, "simultaneous-inference-over-different-doubleml-models-advanced"]], "Single Average Potential Outcome Models (APO)": [[75, "Single-Average-Potential-Outcome-Models-(APO)"]], "Source code and maintenance": [[155, "source-code-and-maintenance"]], "Special Data Types": [[107, "special-data-types"]], "Specify learner and estimate causal parameter: IRM model with Lasso as learner": [[72, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: IRM model with random forest as learner": [[72, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-random-forest-as-learner"]], "Specify learner and estimate causal parameter: PLR model with Lasso as learner": [[72, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: PLR model with random forest as learner": [[72, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-random-forest-as-learner"]], "Specifying alternative score functions via callables": [[127, "specifying-alternative-score-functions-via-callables"]], "Specifying learners and set hyperparameters": [[110, "specifying-learners-and-set-hyperparameters"], [110, "id9"]], "Standard approach": [[86, "Standard-approach"]], "Step 1: Custom API for FLAML Models within DoubleML": [[88, "Step-1:-Custom-API-for-FLAML-Models-within-DoubleML"]], "Step 1: Initialize and Train the AutoML Models:": [[88, "Step-1:-Initialize-and-Train-the-AutoML-Models:"]], "Step 2: Evaluate the Tuned Models": [[88, "Step-2:-Evaluate-the-Tuned-Models"]], "Step 2: Using the API when calling DoubleML\u2019s .fit() Method": [[88, "Step-2:-Using-the-API-when-calling-DoubleML's-.fit()-Method"]], "Step 3: Create and Fit DoubleML Model": [[88, "Step-3:-Create-and-Fit-DoubleML-Model"]], "Summary Figure": [[92, "Summary-Figure"]], "Summary of Results": [[63, "Summary-of-Results"], [90, "Summary-of-Results"]], "The Augmented Inverse Propensity Weighting Model estimates the ATE under arbitrary effect and propensity score heterogeneity": [[92, "The-Augmented-Inverse-Propensity-Weighting-Model-estimates-the-ATE-under-arbitrary-effect-and-propensity-score-heterogeneity"]], "The Data Backend: DoubleMLData": [[63, "The-Data-Backend:-DoubleMLData"], [90, "The-Data-Backend:-DoubleMLData"]], "The DoubleML package": [[63, "The-DoubleML-package"], [90, "The-DoubleML-package"]], "The Partially Linear Model performs overlap weighting": [[92, "The-Partially-Linear-Model-performs-overlap-weighting"]], "The basics of double/debiased machine learning": [[101, null]], "The causal model": [[157, "the-causal-model"]], "The data-backend DoubleMLData": [[157, "the-data-backend-doublemldata"]], "Theory": [[144, "theory"]], "Time Aggregation": [[68, "Time-Aggregation"], [70, "Time-Aggregation"], [71, "Time-Aggregation"]], "Tuning on the Folds": [[88, "Tuning-on-the-Folds"]], "Tuning on the full Sample": [[88, "Tuning-on-the-full-Sample"]], "Two treatment periods": [[111, "two-treatment-periods"], [127, "two-treatment-periods"], [144, "two-treatment-periods"]], "Two-Dimensional Example": [[78, "Two-Dimensional-Example"], [79, "Two-Dimensional-Example"]], "Two-Way Clustering with Respect to Product and Market": [[62, "Two-Way-Clustering-with-Respect-to-Product-and-Market"], [89, "Two-Way-Clustering-with-Respect-to-Product-and-Market"]], "Universal Base Period": [[68, "Universal-Base-Period"], [71, "Universal-Base-Period"]], "Untuned (default parameter) XGBoost": [[88, "Untuned-(default-parameter)-XGBoost"]], "Use ensemble learners based on mlr3pipelines": [[64, "Use-ensemble-learners-based-on-mlr3pipelines"]], "User Guide": [[108, null]], "Using DoubleML": [[59, "Using-DoubleML"], [76, "Using-DoubleML"]], "Using ML for DiD: Integrating DoubleML in did": [[61, "Using-ML-for-DiD:-Integrating-DoubleML-in-did"]], "Using learners from mlr3, mlr3learners and mlr3extralearners": [[64, "Using-learners-from-mlr3,-mlr3learners-and-mlr3extralearners"]], "Using pipelines to construct learners": [[110, "using-pipelines-to-construct-learners"]], "Utility Classes": [[58, "utility-classes"]], "Utility Classes and Functions": [[58, null]], "Utility Functions": [[58, "utility-functions"]], "VanderWeele and Arah (2011): Benchmarking": [[98, "VanderWeele-and-Arah-(2011):-Benchmarking"]], "Variance estimation": [[143, "variance-estimation"]], "Variance estimation and confidence intervals": [[143, null]], "Visualizations": [[87, "Visualizations"]], "Visualizing Average Potential Outcomes": [[74, "Visualizing-Average-Potential-Outcomes"]], "Visualizing Average Treatment Effects": [[74, "Visualizing-Average-Treatment-Effects"]], "Visualizing the Treatment Effect Structure": [[74, "Visualizing-the-Treatment-Effect-Structure"]], "Weighted Average Treatment Effects": [[109, "weighted-average-treatment-effects"]], "doubleml.data.DoubleMLClusterData": [[4, null]], "doubleml.data.DoubleMLDIDData": [[5, null]], "doubleml.data.DoubleMLData": [[6, null]], "doubleml.data.DoubleMLPanelData": [[7, null]], "doubleml.data.DoubleMLRDDData": [[8, null]], "doubleml.data.DoubleMLSSMData": [[9, null]], "doubleml.datasets.fetch_401K": [[10, null]], "doubleml.datasets.fetch_bonus": [[11, null]], "doubleml.did": [[3, "doubleml-did"]], "doubleml.did.DoubleMLDID": [[12, null]], "doubleml.did.DoubleMLDIDAggregation": [[13, null]], "doubleml.did.DoubleMLDIDBinary": [[14, null]], "doubleml.did.DoubleMLDIDCS": [[15, null]], "doubleml.did.DoubleMLDIDMulti": [[16, null]], "doubleml.did.datasets.make_did_CS2021": [[17, null]], "doubleml.did.datasets.make_did_SZ2020": [[18, null]], "doubleml.did.datasets.make_did_cs_CS2021": [[19, null]], "doubleml.double_ml_score_mixins.LinearScoreMixin": [[20, null]], "doubleml.double_ml_score_mixins.NonLinearScoreMixin": [[21, null]], "doubleml.irm": [[3, "doubleml-irm"]], "doubleml.irm.DoubleMLAPO": [[22, null]], "doubleml.irm.DoubleMLAPOS": [[23, null]], "doubleml.irm.DoubleMLCVAR": [[24, null]], "doubleml.irm.DoubleMLIIVM": [[25, null]], "doubleml.irm.DoubleMLIRM": [[26, null]], "doubleml.irm.DoubleMLLPQ": [[27, null]], "doubleml.irm.DoubleMLPQ": [[28, null]], "doubleml.irm.DoubleMLQTE": [[29, null]], "doubleml.irm.DoubleMLSSM": [[30, null]], "doubleml.irm.datasets.make_confounded_irm_data": [[31, null]], "doubleml.irm.datasets.make_heterogeneous_data": [[32, null]], "doubleml.irm.datasets.make_iivm_data": [[33, null]], "doubleml.irm.datasets.make_irm_data": [[34, null]], "doubleml.irm.datasets.make_irm_data_discrete_treatments": [[35, null]], "doubleml.irm.datasets.make_ssm_data": [[36, null]], "doubleml.plm": [[3, "doubleml-plm"]], "doubleml.plm.DoubleMLLPLR": [[37, null]], "doubleml.plm.DoubleMLPLIV": [[38, null]], "doubleml.plm.DoubleMLPLR": [[39, null]], "doubleml.plm.datasets.make_confounded_plr_data": [[40, null]], "doubleml.plm.datasets.make_lplr_LZZ2020": [[41, null]], "doubleml.plm.datasets.make_pliv_CHS2015": [[42, null]], "doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021": [[43, null]], "doubleml.plm.datasets.make_plr_CCDDHNR2018": [[44, null]], "doubleml.plm.datasets.make_plr_turrell2018": [[45, null]], "doubleml.rdd": [[3, "doubleml-rdd"]], "doubleml.rdd.RDFlex": [[46, null]], "doubleml.rdd.datasets.make_simple_rdd_data": [[47, null]], "doubleml.utils.DMLDummyClassifier": [[48, null]], "doubleml.utils.DMLDummyRegressor": [[49, null]], "doubleml.utils.DoubleMLBLP": [[50, null]], "doubleml.utils.DoubleMLPolicyTree": [[51, null]], "doubleml.utils.GlobalClassifier": [[52, null]], "doubleml.utils.GlobalRegressor": [[53, null]], "doubleml.utils.PSProcessor": [[54, null]], "doubleml.utils.PSProcessorConfig": [[55, null]], "doubleml.utils.gain_statistics": [[56, null]]}, "docnames": ["api/api", "api/data_class", "api/datasets", "api/dml_models", "api/generated/doubleml.data.DoubleMLClusterData", "api/generated/doubleml.data.DoubleMLDIDData", "api/generated/doubleml.data.DoubleMLData", "api/generated/doubleml.data.DoubleMLPanelData", "api/generated/doubleml.data.DoubleMLRDDData", "api/generated/doubleml.data.DoubleMLSSMData", "api/generated/doubleml.datasets.fetch_401K", "api/generated/doubleml.datasets.fetch_bonus", "api/generated/doubleml.did.DoubleMLDID", "api/generated/doubleml.did.DoubleMLDIDAggregation", "api/generated/doubleml.did.DoubleMLDIDBinary", "api/generated/doubleml.did.DoubleMLDIDCS", "api/generated/doubleml.did.DoubleMLDIDMulti", "api/generated/doubleml.did.datasets.make_did_CS2021", "api/generated/doubleml.did.datasets.make_did_SZ2020", "api/generated/doubleml.did.datasets.make_did_cs_CS2021", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin", "api/generated/doubleml.irm.DoubleMLAPO", "api/generated/doubleml.irm.DoubleMLAPOS", "api/generated/doubleml.irm.DoubleMLCVAR", "api/generated/doubleml.irm.DoubleMLIIVM", "api/generated/doubleml.irm.DoubleMLIRM", "api/generated/doubleml.irm.DoubleMLLPQ", "api/generated/doubleml.irm.DoubleMLPQ", "api/generated/doubleml.irm.DoubleMLQTE", "api/generated/doubleml.irm.DoubleMLSSM", "api/generated/doubleml.irm.datasets.make_confounded_irm_data", "api/generated/doubleml.irm.datasets.make_heterogeneous_data", "api/generated/doubleml.irm.datasets.make_iivm_data", "api/generated/doubleml.irm.datasets.make_irm_data", "api/generated/doubleml.irm.datasets.make_irm_data_discrete_treatments", "api/generated/doubleml.irm.datasets.make_ssm_data", "api/generated/doubleml.plm.DoubleMLLPLR", "api/generated/doubleml.plm.DoubleMLPLIV", "api/generated/doubleml.plm.DoubleMLPLR", "api/generated/doubleml.plm.datasets.make_confounded_plr_data", "api/generated/doubleml.plm.datasets.make_lplr_LZZ2020", "api/generated/doubleml.plm.datasets.make_pliv_CHS2015", "api/generated/doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021", "api/generated/doubleml.plm.datasets.make_plr_CCDDHNR2018", "api/generated/doubleml.plm.datasets.make_plr_turrell2018", "api/generated/doubleml.rdd.RDFlex", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data", "api/generated/doubleml.utils.DMLDummyClassifier", "api/generated/doubleml.utils.DMLDummyRegressor", "api/generated/doubleml.utils.DoubleMLBLP", "api/generated/doubleml.utils.DoubleMLPolicyTree", "api/generated/doubleml.utils.GlobalClassifier", "api/generated/doubleml.utils.GlobalRegressor", "api/generated/doubleml.utils.PSProcessor", "api/generated/doubleml.utils.PSProcessorConfig", "api/generated/doubleml.utils.gain_statistics", "api/mixins", "api/utility", "examples/R_double_ml_basic_iv", "examples/R_double_ml_basics", "examples/R_double_ml_did", "examples/R_double_ml_multiway_cluster", "examples/R_double_ml_pension", "examples/R_double_ml_pipeline", "examples/R_double_ml_ssm", "examples/did/py_did", "examples/did/py_did_pretest", "examples/did/py_panel", "examples/did/py_panel_data_example", "examples/did/py_panel_simple", "examples/did/py_rep_cs", "examples/double_ml_bonus_data", "examples/index", "examples/learners/py_tabpfn", "examples/py_double_ml_apo", "examples/py_double_ml_basic_iv", "examples/py_double_ml_basics", "examples/py_double_ml_cate", "examples/py_double_ml_cate_plr", "examples/py_double_ml_cvar", "examples/py_double_ml_firststage", "examples/py_double_ml_gate", "examples/py_double_ml_gate_plr", "examples/py_double_ml_gate_sensitivity", "examples/py_double_ml_irm_vs_apo", "examples/py_double_ml_learner", "examples/py_double_ml_lplr", "examples/py_double_ml_meets_flaml", "examples/py_double_ml_multiway_cluster", "examples/py_double_ml_pension", "examples/py_double_ml_pension_qte", "examples/py_double_ml_plm_irm_hetfx", "examples/py_double_ml_policy_tree", "examples/py_double_ml_pq", "examples/py_double_ml_rdflex", "examples/py_double_ml_robust_iv", "examples/py_double_ml_sensitivity", "examples/py_double_ml_sensitivity_booking", "examples/py_double_ml_ssm", "guide/algorithms", "guide/basics", "guide/data/base_data", "guide/data/did_data", "guide/data/panel_data", "guide/data/rdd_data", "guide/data/ssm_data", "guide/data_backend", "guide/guide", "guide/heterogeneity", "guide/learners", "guide/models", "guide/models/did/did_aggregation", "guide/models/did/did_binary", "guide/models/did/did_cs", "guide/models/did/did_implementation", "guide/models/did/did_pa", "guide/models/did/did_setup", "guide/models/irm/apo", "guide/models/irm/apos", "guide/models/irm/iivm", "guide/models/irm/irm", "guide/models/plm/lplr", "guide/models/plm/pliv", "guide/models/plm/plr", "guide/models/ssm/ssm", "guide/resampling", "guide/scores", "guide/scores/did/did_cs_binary_score", "guide/scores/did/did_cs_score", "guide/scores/did/did_pa_binary_score", "guide/scores/did/did_pa_score", "guide/scores/irm/apo_score", "guide/scores/irm/cvar_score", "guide/scores/irm/iivm_score", "guide/scores/irm/irm_score", "guide/scores/irm/lpq_score", "guide/scores/irm/pq_score", "guide/scores/plm/lplr_score", "guide/scores/plm/pliv_score", "guide/scores/plm/plr_score", "guide/scores/ssm/mar_score", "guide/scores/ssm/nr_score", "guide/se_confint", "guide/sensitivity", "guide/sensitivity/benchmarking", "guide/sensitivity/did/did_cs_binary_sensitivity", "guide/sensitivity/did/did_cs_sensitivity", "guide/sensitivity/did/did_pa_binary_sensitivity", "guide/sensitivity/did/did_pa_sensitivity", "guide/sensitivity/implementation", "guide/sensitivity/irm/apo_sensitivity", "guide/sensitivity/irm/irm_sensitivity", "guide/sensitivity/plm/plr_sensitivity", "guide/sensitivity/theory", "index", "intro/install", "intro/intro", "literature/literature", "release/release", "workflow/workflow"], "envversion": {"nbsphinx": 4, "sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1}, "filenames": ["api/api.rst", "api/data_class.rst", "api/datasets.rst", "api/dml_models.rst", "api/generated/doubleml.data.DoubleMLClusterData.rst", "api/generated/doubleml.data.DoubleMLDIDData.rst", "api/generated/doubleml.data.DoubleMLData.rst", "api/generated/doubleml.data.DoubleMLPanelData.rst", "api/generated/doubleml.data.DoubleMLRDDData.rst", "api/generated/doubleml.data.DoubleMLSSMData.rst", "api/generated/doubleml.datasets.fetch_401K.rst", "api/generated/doubleml.datasets.fetch_bonus.rst", "api/generated/doubleml.did.DoubleMLDID.rst", "api/generated/doubleml.did.DoubleMLDIDAggregation.rst", "api/generated/doubleml.did.DoubleMLDIDBinary.rst", "api/generated/doubleml.did.DoubleMLDIDCS.rst", "api/generated/doubleml.did.DoubleMLDIDMulti.rst", "api/generated/doubleml.did.datasets.make_did_CS2021.rst", "api/generated/doubleml.did.datasets.make_did_SZ2020.rst", "api/generated/doubleml.did.datasets.make_did_cs_CS2021.rst", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin.rst", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin.rst", "api/generated/doubleml.irm.DoubleMLAPO.rst", "api/generated/doubleml.irm.DoubleMLAPOS.rst", "api/generated/doubleml.irm.DoubleMLCVAR.rst", "api/generated/doubleml.irm.DoubleMLIIVM.rst", "api/generated/doubleml.irm.DoubleMLIRM.rst", "api/generated/doubleml.irm.DoubleMLLPQ.rst", "api/generated/doubleml.irm.DoubleMLPQ.rst", "api/generated/doubleml.irm.DoubleMLQTE.rst", "api/generated/doubleml.irm.DoubleMLSSM.rst", "api/generated/doubleml.irm.datasets.make_confounded_irm_data.rst", "api/generated/doubleml.irm.datasets.make_heterogeneous_data.rst", "api/generated/doubleml.irm.datasets.make_iivm_data.rst", "api/generated/doubleml.irm.datasets.make_irm_data.rst", "api/generated/doubleml.irm.datasets.make_irm_data_discrete_treatments.rst", "api/generated/doubleml.irm.datasets.make_ssm_data.rst", "api/generated/doubleml.plm.DoubleMLLPLR.rst", "api/generated/doubleml.plm.DoubleMLPLIV.rst", "api/generated/doubleml.plm.DoubleMLPLR.rst", "api/generated/doubleml.plm.datasets.make_confounded_plr_data.rst", "api/generated/doubleml.plm.datasets.make_lplr_LZZ2020.rst", "api/generated/doubleml.plm.datasets.make_pliv_CHS2015.rst", "api/generated/doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021.rst", "api/generated/doubleml.plm.datasets.make_plr_CCDDHNR2018.rst", "api/generated/doubleml.plm.datasets.make_plr_turrell2018.rst", "api/generated/doubleml.rdd.RDFlex.rst", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data.rst", "api/generated/doubleml.utils.DMLDummyClassifier.rst", "api/generated/doubleml.utils.DMLDummyRegressor.rst", "api/generated/doubleml.utils.DoubleMLBLP.rst", "api/generated/doubleml.utils.DoubleMLPolicyTree.rst", "api/generated/doubleml.utils.GlobalClassifier.rst", "api/generated/doubleml.utils.GlobalRegressor.rst", "api/generated/doubleml.utils.PSProcessor.rst", "api/generated/doubleml.utils.PSProcessorConfig.rst", "api/generated/doubleml.utils.gain_statistics.rst", "api/mixins.rst", "api/utility.rst", "examples/R_double_ml_basic_iv.ipynb", "examples/R_double_ml_basics.ipynb", "examples/R_double_ml_did.ipynb", "examples/R_double_ml_multiway_cluster.ipynb", "examples/R_double_ml_pension.ipynb", "examples/R_double_ml_pipeline.ipynb", "examples/R_double_ml_ssm.ipynb", "examples/did/py_did.ipynb", "examples/did/py_did_pretest.ipynb", "examples/did/py_panel.ipynb", "examples/did/py_panel_data_example.ipynb", "examples/did/py_panel_simple.ipynb", "examples/did/py_rep_cs.ipynb", "examples/double_ml_bonus_data.ipynb", "examples/index.rst", "examples/learners/py_tabpfn.ipynb", "examples/py_double_ml_apo.ipynb", "examples/py_double_ml_basic_iv.ipynb", "examples/py_double_ml_basics.ipynb", "examples/py_double_ml_cate.ipynb", "examples/py_double_ml_cate_plr.ipynb", "examples/py_double_ml_cvar.ipynb", "examples/py_double_ml_firststage.ipynb", "examples/py_double_ml_gate.ipynb", "examples/py_double_ml_gate_plr.ipynb", "examples/py_double_ml_gate_sensitivity.ipynb", "examples/py_double_ml_irm_vs_apo.ipynb", "examples/py_double_ml_learner.ipynb", "examples/py_double_ml_lplr.ipynb", "examples/py_double_ml_meets_flaml.ipynb", "examples/py_double_ml_multiway_cluster.ipynb", "examples/py_double_ml_pension.ipynb", "examples/py_double_ml_pension_qte.ipynb", "examples/py_double_ml_plm_irm_hetfx.ipynb", "examples/py_double_ml_policy_tree.ipynb", "examples/py_double_ml_pq.ipynb", "examples/py_double_ml_rdflex.ipynb", "examples/py_double_ml_robust_iv.ipynb", "examples/py_double_ml_sensitivity.ipynb", "examples/py_double_ml_sensitivity_booking.ipynb", "examples/py_double_ml_ssm.ipynb", "guide/algorithms.rst", "guide/basics.rst", "guide/data/base_data.rst", "guide/data/did_data.rst", "guide/data/panel_data.rst", "guide/data/rdd_data.rst", "guide/data/ssm_data.rst", "guide/data_backend.rst", "guide/guide.rst", "guide/heterogeneity.rst", "guide/learners.rst", "guide/models.rst", "guide/models/did/did_aggregation.rst", "guide/models/did/did_binary.rst", "guide/models/did/did_cs.rst", "guide/models/did/did_implementation.rst", "guide/models/did/did_pa.rst", "guide/models/did/did_setup.rst", "guide/models/irm/apo.rst", "guide/models/irm/apos.rst", "guide/models/irm/iivm.rst", "guide/models/irm/irm.rst", "guide/models/plm/lplr.rst", "guide/models/plm/pliv.rst", "guide/models/plm/plr.rst", "guide/models/ssm/ssm.rst", "guide/resampling.rst", "guide/scores.rst", "guide/scores/did/did_cs_binary_score.rst", "guide/scores/did/did_cs_score.rst", "guide/scores/did/did_pa_binary_score.rst", "guide/scores/did/did_pa_score.rst", "guide/scores/irm/apo_score.rst", "guide/scores/irm/cvar_score.rst", "guide/scores/irm/iivm_score.rst", "guide/scores/irm/irm_score.rst", "guide/scores/irm/lpq_score.rst", "guide/scores/irm/pq_score.rst", "guide/scores/plm/lplr_score.rst", "guide/scores/plm/pliv_score.rst", "guide/scores/plm/plr_score.rst", "guide/scores/ssm/mar_score.rst", "guide/scores/ssm/nr_score.rst", "guide/se_confint.rst", "guide/sensitivity.rst", "guide/sensitivity/benchmarking.rst", "guide/sensitivity/did/did_cs_binary_sensitivity.rst", "guide/sensitivity/did/did_cs_sensitivity.rst", "guide/sensitivity/did/did_pa_binary_sensitivity.rst", "guide/sensitivity/did/did_pa_sensitivity.rst", "guide/sensitivity/implementation.rst", "guide/sensitivity/irm/apo_sensitivity.rst", "guide/sensitivity/irm/irm_sensitivity.rst", "guide/sensitivity/plm/plr_sensitivity.rst", "guide/sensitivity/theory.rst", "index.rst", "intro/install.rst", "intro/intro.rst", "literature/literature.rst", "release/release.rst", "workflow/workflow.rst"], "indexentries": {"adjust_ps() (doubleml.utils.psprocessor method)": [[54, "doubleml.utils.PSProcessor.adjust_ps", false]], "aggregate() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.aggregate", false]], "aggregate_over_splits() (doubleml.rdd.rdflex method)": [[46, "doubleml.rdd.RDFlex.aggregate_over_splits", false]], "bootstrap() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.bootstrap", false]], "bootstrap() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.bootstrap", false]], "bootstrap() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.bootstrap", false]], "capo() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.capo", false]], "cate() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.cate", false]], "cate() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.cate", false]], "causal_contrast() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.causal_contrast", false]], "confint() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.confint", false]], "confint() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.confint", false]], "confint() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.confint", false]], "confint() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.confint", false]], "confint() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.confint", false]], "confint() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.confint", false]], "confint() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.confint", false]], "confint() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.confint", false]], "confint() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.confint", false]], "confint() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.confint", false]], "confint() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.confint", false]], "confint() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.confint", false]], "confint() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.confint", false]], "confint() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.confint", false]], "confint() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.confint", false]], "confint() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.confint", false]], "confint() (doubleml.rdd.rdflex method)": [[46, "doubleml.rdd.RDFlex.confint", false]], "confint() (doubleml.utils.doublemlblp method)": [[50, "doubleml.utils.DoubleMLBLP.confint", false]], "construct_framework() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.construct_framework", false]], "construct_framework() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.construct_framework", false]], "construct_framework() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.construct_framework", false]], "dmldummyclassifier (class in doubleml.utils)": [[48, "doubleml.utils.DMLDummyClassifier", false]], "dmldummyregressor (class in doubleml.utils)": [[49, "doubleml.utils.DMLDummyRegressor", false]], "doublemlapo (class in doubleml.irm)": [[22, "doubleml.irm.DoubleMLAPO", false]], "doublemlapos (class in doubleml.irm)": [[23, "doubleml.irm.DoubleMLAPOS", false]], "doublemlblp (class in doubleml.utils)": [[50, "doubleml.utils.DoubleMLBLP", false]], "doublemlclusterdata (class in doubleml.data)": [[4, "doubleml.data.DoubleMLClusterData", false]], "doublemlcvar (class in doubleml.irm)": [[24, "doubleml.irm.DoubleMLCVAR", false]], "doublemldata (class in doubleml.data)": [[6, "doubleml.data.DoubleMLData", false]], "doublemldid (class in doubleml.did)": [[12, "doubleml.did.DoubleMLDID", false]], "doublemldidaggregation (class in doubleml.did)": [[13, "doubleml.did.DoubleMLDIDAggregation", false]], "doublemldidbinary (class in doubleml.did)": [[14, "doubleml.did.DoubleMLDIDBinary", false]], "doublemldidcs (class in doubleml.did)": [[15, "doubleml.did.DoubleMLDIDCS", false]], "doublemldiddata (class in doubleml.data)": [[5, "doubleml.data.DoubleMLDIDData", false]], "doublemldidmulti (class in doubleml.did)": [[16, "doubleml.did.DoubleMLDIDMulti", false]], "doublemliivm (class in doubleml.irm)": [[25, "doubleml.irm.DoubleMLIIVM", false]], "doublemlirm (class in doubleml.irm)": [[26, "doubleml.irm.DoubleMLIRM", false]], "doublemllplr (class in doubleml.plm)": [[37, "doubleml.plm.DoubleMLLPLR", false]], "doublemllpq (class in doubleml.irm)": [[27, "doubleml.irm.DoubleMLLPQ", false]], "doublemlpaneldata (class in doubleml.data)": [[7, "doubleml.data.DoubleMLPanelData", false]], "doublemlpliv (class in doubleml.plm)": [[38, "doubleml.plm.DoubleMLPLIV", false]], "doublemlplr (class in doubleml.plm)": [[39, "doubleml.plm.DoubleMLPLR", false]], "doublemlpolicytree (class in doubleml.utils)": [[51, "doubleml.utils.DoubleMLPolicyTree", false]], "doublemlpq (class in doubleml.irm)": [[28, "doubleml.irm.DoubleMLPQ", false]], "doublemlqte (class in doubleml.irm)": [[29, "doubleml.irm.DoubleMLQTE", false]], "doublemlrdddata (class in doubleml.data)": [[8, "doubleml.data.DoubleMLRDDData", false]], "doublemlssm (class in doubleml.irm)": [[30, "doubleml.irm.DoubleMLSSM", false]], "doublemlssmdata (class in doubleml.data)": [[9, "doubleml.data.DoubleMLSSMData", false]], "draw_sample_splitting() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.draw_sample_splitting", false]], "evaluate_learners() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.evaluate_learners", false]], "fetch_401k() (in module doubleml.datasets)": [[10, "doubleml.datasets.fetch_401K", false]], "fetch_bonus() (in module doubleml.datasets)": [[11, "doubleml.datasets.fetch_bonus", false]], "fit() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.fit", false]], "fit() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.fit", false]], "fit() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.fit", false]], "fit() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.fit", false]], "fit() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.fit", false]], "fit() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.fit", false]], "fit() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.fit", false]], "fit() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.fit", false]], "fit() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.fit", false]], "fit() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.fit", false]], "fit() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.fit", false]], "fit() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.fit", false]], "fit() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.fit", false]], "fit() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.fit", false]], "fit() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.fit", false]], "fit() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.fit", false]], "fit() (doubleml.rdd.rdflex method)": [[46, "doubleml.rdd.RDFlex.fit", false]], "fit() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.fit", false]], "fit() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.fit", false]], "fit() (doubleml.utils.doublemlblp method)": [[50, "doubleml.utils.DoubleMLBLP.fit", false]], "fit() (doubleml.utils.doublemlpolicytree method)": [[51, "doubleml.utils.DoubleMLPolicyTree.fit", false]], "fit() (doubleml.utils.globalclassifier method)": [[52, "doubleml.utils.GlobalClassifier.fit", false]], "fit() (doubleml.utils.globalregressor method)": [[53, "doubleml.utils.GlobalRegressor.fit", false]], "from_arrays() (doubleml.data.doublemlclusterdata class method)": [[4, "doubleml.data.DoubleMLClusterData.from_arrays", false]], "from_arrays() (doubleml.data.doublemldata class method)": [[6, "doubleml.data.DoubleMLData.from_arrays", false]], "from_arrays() (doubleml.data.doublemldiddata class method)": [[5, "doubleml.data.DoubleMLDIDData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlpaneldata class method)": [[7, "doubleml.data.DoubleMLPanelData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlrdddata class method)": [[8, "doubleml.data.DoubleMLRDDData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlssmdata class method)": [[9, "doubleml.data.DoubleMLSSMData.from_arrays", false]], "from_config() (doubleml.utils.psprocessor class method)": [[54, "doubleml.utils.PSProcessor.from_config", false]], "gain_statistics() (in module doubleml.utils)": [[56, "doubleml.utils.gain_statistics", false]], "gapo() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.gapo", false]], "gate() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.gate", false]], "gate() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.gate", false]], "get_metadata_routing() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalclassifier method)": [[52, "doubleml.utils.GlobalClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalregressor method)": [[53, "doubleml.utils.GlobalRegressor.get_metadata_routing", false]], "get_params() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.get_params", false]], "get_params() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.get_params", false]], "get_params() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.get_params", false]], "get_params() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.get_params", false]], "get_params() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.get_params", false]], "get_params() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.get_params", false]], "get_params() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.get_params", false]], "get_params() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.get_params", false]], "get_params() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.get_params", false]], "get_params() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.get_params", false]], "get_params() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.get_params", false]], "get_params() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.get_params", false]], "get_params() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.get_params", false]], "get_params() (doubleml.utils.globalclassifier method)": [[52, "doubleml.utils.GlobalClassifier.get_params", false]], "get_params() (doubleml.utils.globalregressor method)": [[53, "doubleml.utils.GlobalRegressor.get_params", false]], "globalclassifier (class in doubleml.utils)": [[52, "doubleml.utils.GlobalClassifier", false]], "globalregressor (class in doubleml.utils)": [[53, "doubleml.utils.GlobalRegressor", false]], "linearscoremixin (class in doubleml.double_ml_score_mixins)": [[20, "doubleml.double_ml_score_mixins.LinearScoreMixin", false]], "make_confounded_irm_data() (in module doubleml.irm.datasets)": [[31, "doubleml.irm.datasets.make_confounded_irm_data", false]], "make_confounded_plr_data() (in module doubleml.plm.datasets)": [[40, "doubleml.plm.datasets.make_confounded_plr_data", false]], "make_did_cs2021() (in module doubleml.did.datasets)": [[17, "doubleml.did.datasets.make_did_CS2021", false]], "make_did_cs_cs2021() (in module doubleml.did.datasets)": [[19, "doubleml.did.datasets.make_did_cs_CS2021", false]], "make_did_sz2020() (in module doubleml.did.datasets)": [[18, "doubleml.did.datasets.make_did_SZ2020", false]], "make_heterogeneous_data() (in module doubleml.irm.datasets)": [[32, "doubleml.irm.datasets.make_heterogeneous_data", false]], "make_iivm_data() (in module doubleml.irm.datasets)": [[33, "doubleml.irm.datasets.make_iivm_data", false]], "make_irm_data() (in module doubleml.irm.datasets)": [[34, "doubleml.irm.datasets.make_irm_data", false]], "make_irm_data_discrete_treatments() (in module doubleml.irm.datasets)": [[35, "doubleml.irm.datasets.make_irm_data_discrete_treatments", false]], "make_lplr_lzz2020() (in module doubleml.plm.datasets)": [[41, "doubleml.plm.datasets.make_lplr_LZZ2020", false]], "make_pliv_chs2015() (in module doubleml.plm.datasets)": [[42, "doubleml.plm.datasets.make_pliv_CHS2015", false]], "make_pliv_multiway_cluster_ckms2021() (in module doubleml.plm.datasets)": [[43, "doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021", false]], "make_plr_ccddhnr2018() (in module doubleml.plm.datasets)": [[44, "doubleml.plm.datasets.make_plr_CCDDHNR2018", false]], "make_plr_turrell2018() (in module doubleml.plm.datasets)": [[45, "doubleml.plm.datasets.make_plr_turrell2018", false]], "make_simple_rdd_data() (in module doubleml.rdd.datasets)": [[47, "doubleml.rdd.datasets.make_simple_rdd_data", false]], "make_ssm_data() (in module doubleml.irm.datasets)": [[36, "doubleml.irm.datasets.make_ssm_data", false]], "nonlinearscoremixin (class in doubleml.double_ml_score_mixins)": [[21, "doubleml.double_ml_score_mixins.NonLinearScoreMixin", false]], "p_adjust() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.p_adjust", false]], "p_adjust() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.p_adjust", false]], "p_adjust() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.p_adjust", false]], "plot_effects() (doubleml.did.doublemldidaggregation method)": [[13, "doubleml.did.DoubleMLDIDAggregation.plot_effects", false]], "plot_effects() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.plot_effects", false]], "plot_tree() (doubleml.utils.doublemlpolicytree method)": [[51, "doubleml.utils.DoubleMLPolicyTree.plot_tree", false]], "policy_tree() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.policy_tree", false]], "predict() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.predict", false]], "predict() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.predict", false]], "predict() (doubleml.utils.doublemlpolicytree method)": [[51, "doubleml.utils.DoubleMLPolicyTree.predict", false]], "predict() (doubleml.utils.globalclassifier method)": [[52, "doubleml.utils.GlobalClassifier.predict", false]], "predict() (doubleml.utils.globalregressor method)": [[53, "doubleml.utils.GlobalRegressor.predict", false]], "predict_proba() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.predict_proba", false]], "predict_proba() (doubleml.utils.globalclassifier method)": [[52, "doubleml.utils.GlobalClassifier.predict_proba", false]], "psprocessor (class in doubleml.utils)": [[54, "doubleml.utils.PSProcessor", false]], "psprocessorconfig (class in doubleml.utils)": [[55, "doubleml.utils.PSProcessorConfig", false]], "rdflex (class in doubleml.rdd)": [[46, "doubleml.rdd.RDFlex", false]], "robust_confset() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.robust_confset", false]], "score() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.score", false]], "score() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.score", false]], "score() (doubleml.utils.globalclassifier method)": [[52, "doubleml.utils.GlobalClassifier.score", false]], "score() (doubleml.utils.globalregressor method)": [[53, "doubleml.utils.GlobalRegressor.score", false]], "sensitivity_analysis() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_analysis", false]], "sensitivity_benchmark() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_benchmark", false]], "sensitivity_plot() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidmulti method)": [[16, "doubleml.did.DoubleMLDIDMulti.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_plot", false]], "set_fit_request() (doubleml.utils.globalclassifier method)": [[52, "doubleml.utils.GlobalClassifier.set_fit_request", false]], "set_fit_request() (doubleml.utils.globalregressor method)": [[53, "doubleml.utils.GlobalRegressor.set_fit_request", false]], "set_ml_nuisance_params() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.set_ml_nuisance_params", false]], "set_params() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.set_params", false]], "set_params() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.set_params", false]], "set_params() (doubleml.utils.globalclassifier method)": [[52, "doubleml.utils.GlobalClassifier.set_params", false]], "set_params() (doubleml.utils.globalregressor method)": [[53, "doubleml.utils.GlobalRegressor.set_params", false]], "set_sample_splitting() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapos method)": [[23, "doubleml.irm.DoubleMLAPOS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlqte method)": [[29, "doubleml.irm.DoubleMLQTE.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.set_sample_splitting", false]], "set_score_request() (doubleml.utils.dmldummyclassifier method)": [[48, "doubleml.utils.DMLDummyClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.dmldummyregressor method)": [[49, "doubleml.utils.DMLDummyRegressor.set_score_request", false]], "set_score_request() (doubleml.utils.globalclassifier method)": [[52, "doubleml.utils.GlobalClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.globalregressor method)": [[53, "doubleml.utils.GlobalRegressor.set_score_request", false]], "set_x_d() (doubleml.data.doublemlclusterdata method)": [[4, "doubleml.data.DoubleMLClusterData.set_x_d", false]], "set_x_d() (doubleml.data.doublemldata method)": [[6, "doubleml.data.DoubleMLData.set_x_d", false]], "set_x_d() (doubleml.data.doublemldiddata method)": [[5, "doubleml.data.DoubleMLDIDData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlpaneldata method)": [[7, "doubleml.data.DoubleMLPanelData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlrdddata method)": [[8, "doubleml.data.DoubleMLRDDData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlssmdata method)": [[9, "doubleml.data.DoubleMLSSMData.set_x_d", false]], "tune() (doubleml.did.doublemldid method)": [[12, "doubleml.did.DoubleMLDID.tune", false]], "tune() (doubleml.did.doublemldidbinary method)": [[14, "doubleml.did.DoubleMLDIDBinary.tune", false]], "tune() (doubleml.did.doublemldidcs method)": [[15, "doubleml.did.DoubleMLDIDCS.tune", false]], "tune() (doubleml.irm.doublemlapo method)": [[22, "doubleml.irm.DoubleMLAPO.tune", false]], "tune() (doubleml.irm.doublemlcvar method)": [[24, "doubleml.irm.DoubleMLCVAR.tune", false]], "tune() (doubleml.irm.doublemliivm method)": [[25, "doubleml.irm.DoubleMLIIVM.tune", false]], "tune() (doubleml.irm.doublemlirm method)": [[26, "doubleml.irm.DoubleMLIRM.tune", false]], "tune() (doubleml.irm.doublemllpq method)": [[27, "doubleml.irm.DoubleMLLPQ.tune", false]], "tune() (doubleml.irm.doublemlpq method)": [[28, "doubleml.irm.DoubleMLPQ.tune", false]], "tune() (doubleml.irm.doublemlssm method)": [[30, "doubleml.irm.DoubleMLSSM.tune", false]], "tune() (doubleml.plm.doublemllplr method)": [[37, "doubleml.plm.DoubleMLLPLR.tune", false]], "tune() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.tune", false]], "tune() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.tune", false]]}, "objects": {"doubleml.data": [[4, 0, 1, "", "DoubleMLClusterData"], [5, 0, 1, "", "DoubleMLDIDData"], [6, 0, 1, "", "DoubleMLData"], [7, 0, 1, "", "DoubleMLPanelData"], [8, 0, 1, "", "DoubleMLRDDData"], [9, 0, 1, "", "DoubleMLSSMData"]], "doubleml.data.DoubleMLClusterData": [[4, 1, 1, "", "from_arrays"], [4, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLDIDData": [[5, 1, 1, "", "from_arrays"], [5, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLData": [[6, 1, 1, "", "from_arrays"], [6, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLPanelData": [[7, 1, 1, "", "from_arrays"], [7, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLRDDData": [[8, 1, 1, "", "from_arrays"], [8, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLSSMData": [[9, 1, 1, "", "from_arrays"], [9, 1, 1, "", "set_x_d"]], "doubleml.datasets": [[10, 2, 1, "", "fetch_401K"], [11, 2, 1, "", "fetch_bonus"]], "doubleml.did": [[12, 0, 1, "", "DoubleMLDID"], [13, 0, 1, "", "DoubleMLDIDAggregation"], [14, 0, 1, "", "DoubleMLDIDBinary"], [15, 0, 1, "", "DoubleMLDIDCS"], [16, 0, 1, "", "DoubleMLDIDMulti"]], "doubleml.did.DoubleMLDID": [[12, 1, 1, "", "bootstrap"], [12, 1, 1, "", "confint"], [12, 1, 1, "", "construct_framework"], [12, 1, 1, "", "draw_sample_splitting"], [12, 1, 1, "", "evaluate_learners"], [12, 1, 1, "", "fit"], [12, 1, 1, "", "get_params"], [12, 1, 1, "", "p_adjust"], [12, 1, 1, "", "sensitivity_analysis"], [12, 1, 1, "", "sensitivity_benchmark"], [12, 1, 1, "", "sensitivity_plot"], [12, 1, 1, "", "set_ml_nuisance_params"], [12, 1, 1, "", "set_sample_splitting"], [12, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDAggregation": [[13, 1, 1, "", "plot_effects"]], "doubleml.did.DoubleMLDIDBinary": [[14, 1, 1, "", "bootstrap"], [14, 1, 1, "", "confint"], [14, 1, 1, "", "construct_framework"], [14, 1, 1, "", "draw_sample_splitting"], [14, 1, 1, "", "evaluate_learners"], [14, 1, 1, "", "fit"], [14, 1, 1, "", "get_params"], [14, 1, 1, "", "p_adjust"], [14, 1, 1, "", "sensitivity_analysis"], [14, 1, 1, "", "sensitivity_benchmark"], [14, 1, 1, "", "sensitivity_plot"], [14, 1, 1, "", "set_ml_nuisance_params"], [14, 1, 1, "", "set_sample_splitting"], [14, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDCS": [[15, 1, 1, "", "bootstrap"], [15, 1, 1, "", "confint"], [15, 1, 1, "", "construct_framework"], [15, 1, 1, "", "draw_sample_splitting"], [15, 1, 1, "", "evaluate_learners"], [15, 1, 1, "", "fit"], [15, 1, 1, "", "get_params"], [15, 1, 1, "", "p_adjust"], [15, 1, 1, "", "sensitivity_analysis"], [15, 1, 1, "", "sensitivity_benchmark"], [15, 1, 1, "", "sensitivity_plot"], [15, 1, 1, "", "set_ml_nuisance_params"], [15, 1, 1, "", "set_sample_splitting"], [15, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDMulti": [[16, 1, 1, "", "aggregate"], [16, 1, 1, "", "bootstrap"], [16, 1, 1, "", "confint"], [16, 1, 1, "", "fit"], [16, 1, 1, "", "p_adjust"], [16, 1, 1, "", "plot_effects"], [16, 1, 1, "", "sensitivity_analysis"], [16, 1, 1, "", "sensitivity_benchmark"], [16, 1, 1, "", "sensitivity_plot"]], "doubleml.did.datasets": [[17, 2, 1, "", "make_did_CS2021"], [18, 2, 1, "", "make_did_SZ2020"], [19, 2, 1, "", "make_did_cs_CS2021"]], "doubleml.double_ml_score_mixins": [[20, 0, 1, "", "LinearScoreMixin"], [21, 0, 1, "", "NonLinearScoreMixin"]], "doubleml.irm": [[22, 0, 1, "", "DoubleMLAPO"], [23, 0, 1, "", "DoubleMLAPOS"], [24, 0, 1, "", "DoubleMLCVAR"], [25, 0, 1, "", "DoubleMLIIVM"], [26, 0, 1, "", "DoubleMLIRM"], [27, 0, 1, "", "DoubleMLLPQ"], [28, 0, 1, "", "DoubleMLPQ"], [29, 0, 1, "", "DoubleMLQTE"], [30, 0, 1, "", "DoubleMLSSM"]], "doubleml.irm.DoubleMLAPO": [[22, 1, 1, "", "bootstrap"], [22, 1, 1, "", "capo"], [22, 1, 1, "", "confint"], [22, 1, 1, "", "construct_framework"], [22, 1, 1, "", "draw_sample_splitting"], [22, 1, 1, "", "evaluate_learners"], [22, 1, 1, "", "fit"], [22, 1, 1, "", "gapo"], [22, 1, 1, "", "get_params"], [22, 1, 1, "", "p_adjust"], [22, 1, 1, "", "sensitivity_analysis"], [22, 1, 1, "", "sensitivity_benchmark"], [22, 1, 1, "", "sensitivity_plot"], [22, 1, 1, "", "set_ml_nuisance_params"], [22, 1, 1, "", "set_sample_splitting"], [22, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLAPOS": [[23, 1, 1, "", "bootstrap"], [23, 1, 1, "", "causal_contrast"], [23, 1, 1, "", "confint"], [23, 1, 1, "", "draw_sample_splitting"], [23, 1, 1, "", "fit"], [23, 1, 1, "", "sensitivity_analysis"], [23, 1, 1, "", "sensitivity_benchmark"], [23, 1, 1, "", "sensitivity_plot"], [23, 1, 1, "", "set_sample_splitting"]], "doubleml.irm.DoubleMLCVAR": [[24, 1, 1, "", "bootstrap"], [24, 1, 1, "", "confint"], [24, 1, 1, "", "construct_framework"], [24, 1, 1, "", "draw_sample_splitting"], [24, 1, 1, "", "evaluate_learners"], [24, 1, 1, "", "fit"], [24, 1, 1, "", "get_params"], [24, 1, 1, "", "p_adjust"], [24, 1, 1, "", "sensitivity_analysis"], [24, 1, 1, "", "sensitivity_benchmark"], [24, 1, 1, "", "sensitivity_plot"], [24, 1, 1, "", "set_ml_nuisance_params"], [24, 1, 1, "", "set_sample_splitting"], [24, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLIIVM": [[25, 1, 1, "", "bootstrap"], [25, 1, 1, "", "confint"], [25, 1, 1, "", "construct_framework"], [25, 1, 1, "", "draw_sample_splitting"], [25, 1, 1, "", "evaluate_learners"], [25, 1, 1, "", "fit"], [25, 1, 1, "", "get_params"], [25, 1, 1, "", "p_adjust"], [25, 1, 1, "", "robust_confset"], [25, 1, 1, "", "sensitivity_analysis"], [25, 1, 1, "", "sensitivity_benchmark"], [25, 1, 1, "", "sensitivity_plot"], [25, 1, 1, "", "set_ml_nuisance_params"], [25, 1, 1, "", "set_sample_splitting"], [25, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLIRM": [[26, 1, 1, "", "bootstrap"], [26, 1, 1, "", "cate"], [26, 1, 1, "", "confint"], [26, 1, 1, "", "construct_framework"], [26, 1, 1, "", "draw_sample_splitting"], [26, 1, 1, "", "evaluate_learners"], [26, 1, 1, "", "fit"], [26, 1, 1, "", "gate"], [26, 1, 1, "", "get_params"], [26, 1, 1, "", "p_adjust"], [26, 1, 1, "", "policy_tree"], [26, 1, 1, "", "sensitivity_analysis"], [26, 1, 1, "", "sensitivity_benchmark"], [26, 1, 1, "", "sensitivity_plot"], [26, 1, 1, "", "set_ml_nuisance_params"], [26, 1, 1, "", "set_sample_splitting"], [26, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLLPQ": [[27, 1, 1, "", "bootstrap"], [27, 1, 1, "", "confint"], [27, 1, 1, "", "construct_framework"], [27, 1, 1, "", "draw_sample_splitting"], [27, 1, 1, "", "evaluate_learners"], [27, 1, 1, "", "fit"], [27, 1, 1, "", "get_params"], [27, 1, 1, "", "p_adjust"], [27, 1, 1, "", "sensitivity_analysis"], [27, 1, 1, "", "sensitivity_benchmark"], [27, 1, 1, "", "sensitivity_plot"], [27, 1, 1, "", "set_ml_nuisance_params"], [27, 1, 1, "", "set_sample_splitting"], [27, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLPQ": [[28, 1, 1, "", "bootstrap"], [28, 1, 1, "", "confint"], [28, 1, 1, "", "construct_framework"], [28, 1, 1, "", "draw_sample_splitting"], [28, 1, 1, "", "evaluate_learners"], [28, 1, 1, "", "fit"], [28, 1, 1, "", "get_params"], [28, 1, 1, "", "p_adjust"], [28, 1, 1, "", "sensitivity_analysis"], [28, 1, 1, "", "sensitivity_benchmark"], [28, 1, 1, "", "sensitivity_plot"], [28, 1, 1, "", "set_ml_nuisance_params"], [28, 1, 1, "", "set_sample_splitting"], [28, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLQTE": [[29, 1, 1, "", "bootstrap"], [29, 1, 1, "", "confint"], [29, 1, 1, "", "draw_sample_splitting"], [29, 1, 1, "", "fit"], [29, 1, 1, "", "p_adjust"], [29, 1, 1, "", "set_sample_splitting"]], "doubleml.irm.DoubleMLSSM": [[30, 1, 1, "", "bootstrap"], [30, 1, 1, "", "confint"], [30, 1, 1, "", "construct_framework"], [30, 1, 1, "", "draw_sample_splitting"], [30, 1, 1, "", "evaluate_learners"], [30, 1, 1, "", "fit"], [30, 1, 1, "", "get_params"], [30, 1, 1, "", "p_adjust"], [30, 1, 1, "", "sensitivity_analysis"], [30, 1, 1, "", "sensitivity_benchmark"], [30, 1, 1, "", "sensitivity_plot"], [30, 1, 1, "", "set_ml_nuisance_params"], [30, 1, 1, "", "set_sample_splitting"], [30, 1, 1, "", "tune"]], "doubleml.irm.datasets": [[31, 2, 1, "", "make_confounded_irm_data"], [32, 2, 1, "", "make_heterogeneous_data"], [33, 2, 1, "", "make_iivm_data"], [34, 2, 1, "", "make_irm_data"], [35, 2, 1, "", "make_irm_data_discrete_treatments"], [36, 2, 1, "", "make_ssm_data"]], "doubleml.plm": [[37, 0, 1, "", "DoubleMLLPLR"], [38, 0, 1, "", "DoubleMLPLIV"], [39, 0, 1, "", "DoubleMLPLR"]], "doubleml.plm.DoubleMLLPLR": [[37, 1, 1, "", "bootstrap"], [37, 1, 1, "", "confint"], [37, 1, 1, "", "construct_framework"], [37, 1, 1, "", "draw_sample_splitting"], [37, 1, 1, "", "evaluate_learners"], [37, 1, 1, "", "fit"], [37, 1, 1, "", "get_params"], [37, 1, 1, "", "p_adjust"], [37, 1, 1, "", "sensitivity_analysis"], [37, 1, 1, "", "sensitivity_benchmark"], [37, 1, 1, "", "sensitivity_plot"], [37, 1, 1, "", "set_ml_nuisance_params"], [37, 1, 1, "", "set_sample_splitting"], [37, 1, 1, "", "tune"]], "doubleml.plm.DoubleMLPLIV": [[38, 1, 1, "", "bootstrap"], [38, 1, 1, "", "confint"], [38, 1, 1, "", "construct_framework"], [38, 1, 1, "", "draw_sample_splitting"], [38, 1, 1, "", "evaluate_learners"], [38, 1, 1, "", "fit"], [38, 1, 1, "", "get_params"], [38, 1, 1, "", "p_adjust"], [38, 1, 1, "", "sensitivity_analysis"], [38, 1, 1, "", "sensitivity_benchmark"], [38, 1, 1, "", "sensitivity_plot"], [38, 1, 1, "", "set_ml_nuisance_params"], [38, 1, 1, "", "set_sample_splitting"], [38, 1, 1, "", "tune"]], "doubleml.plm.DoubleMLPLR": [[39, 1, 1, "", "bootstrap"], [39, 1, 1, "", "cate"], [39, 1, 1, "", "confint"], [39, 1, 1, "", "construct_framework"], [39, 1, 1, "", "draw_sample_splitting"], [39, 1, 1, "", "evaluate_learners"], [39, 1, 1, "", "fit"], [39, 1, 1, "", "gate"], [39, 1, 1, "", "get_params"], [39, 1, 1, "", "p_adjust"], [39, 1, 1, "", "sensitivity_analysis"], [39, 1, 1, "", "sensitivity_benchmark"], [39, 1, 1, "", "sensitivity_plot"], [39, 1, 1, "", "set_ml_nuisance_params"], [39, 1, 1, "", "set_sample_splitting"], [39, 1, 1, "", "tune"]], "doubleml.plm.datasets": [[40, 2, 1, "", "make_confounded_plr_data"], [41, 2, 1, "", "make_lplr_LZZ2020"], [42, 2, 1, "", "make_pliv_CHS2015"], [43, 2, 1, "", "make_pliv_multiway_cluster_CKMS2021"], [44, 2, 1, "", "make_plr_CCDDHNR2018"], [45, 2, 1, "", "make_plr_turrell2018"]], "doubleml.rdd": [[46, 0, 1, "", "RDFlex"]], "doubleml.rdd.RDFlex": [[46, 1, 1, "", "aggregate_over_splits"], [46, 1, 1, "", "confint"], [46, 1, 1, "", "fit"]], "doubleml.rdd.datasets": [[47, 2, 1, "", "make_simple_rdd_data"]], "doubleml.utils": [[48, 0, 1, "", "DMLDummyClassifier"], [49, 0, 1, "", "DMLDummyRegressor"], [50, 0, 1, "", "DoubleMLBLP"], [51, 0, 1, "", "DoubleMLPolicyTree"], [52, 0, 1, "", "GlobalClassifier"], [53, 0, 1, "", "GlobalRegressor"], [54, 0, 1, "", "PSProcessor"], [55, 0, 1, "", "PSProcessorConfig"], [56, 2, 1, "", "gain_statistics"]], "doubleml.utils.DMLDummyClassifier": [[48, 1, 1, "", "fit"], [48, 1, 1, "", "get_metadata_routing"], [48, 1, 1, "", "get_params"], [48, 1, 1, "", "predict"], [48, 1, 1, "", "predict_proba"], [48, 1, 1, "", "score"], [48, 1, 1, "", "set_params"], [48, 1, 1, "", "set_score_request"]], "doubleml.utils.DMLDummyRegressor": [[49, 1, 1, "", "fit"], [49, 1, 1, "", "get_metadata_routing"], [49, 1, 1, "", "get_params"], [49, 1, 1, "", "predict"], [49, 1, 1, "", "score"], [49, 1, 1, "", "set_params"], [49, 1, 1, "", "set_score_request"]], "doubleml.utils.DoubleMLBLP": [[50, 1, 1, "", "confint"], [50, 1, 1, "", "fit"]], "doubleml.utils.DoubleMLPolicyTree": [[51, 1, 1, "", "fit"], [51, 1, 1, "", "plot_tree"], [51, 1, 1, "", "predict"]], "doubleml.utils.GlobalClassifier": [[52, 1, 1, "", "fit"], [52, 1, 1, "", "get_metadata_routing"], [52, 1, 1, "", "get_params"], [52, 1, 1, "", "predict"], [52, 1, 1, "", "predict_proba"], [52, 1, 1, "", "score"], [52, 1, 1, "", "set_fit_request"], [52, 1, 1, "", "set_params"], [52, 1, 1, "", "set_score_request"]], "doubleml.utils.GlobalRegressor": [[53, 1, 1, "", "fit"], [53, 1, 1, "", "get_metadata_routing"], [53, 1, 1, "", "get_params"], [53, 1, 1, "", "predict"], [53, 1, 1, "", "score"], [53, 1, 1, "", "set_fit_request"], [53, 1, 1, "", "set_params"], [53, 1, 1, "", "set_score_request"]], "doubleml.utils.PSProcessor": [[54, 1, 1, "", "adjust_ps"], [54, 1, 1, "", "from_config"]]}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function"}, "terms": {"": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 42, 43, 44, 45, 46, 50, 52, 53, 59, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 74, 75, 81, 82, 83, 84, 86, 87, 89, 90, 91, 95, 97, 98, 99, 100, 102, 104, 105, 106, 107, 110, 111, 113, 114, 116, 117, 125, 127, 141, 142, 143, 144, 145, 155, 157, 158, 159, 160], "0": [4, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 50, 51, 53, 54, 55, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 152, 156, 157, 159], "00": [68, 71, 82, 83, 85, 90, 91, 126], "000": [92, 96, 143, 160], "00000": 85, "000000": [68, 70, 72, 75, 85, 90, 91, 102, 107, 109, 157], "0000000": 143, "0000000000000010000100": [64, 102, 107, 157], "000000e": [68, 71, 82, 83, 85, 90, 91], "000006": 75, "000017": 94, "000021": 68, "000025": 89, "000034": 90, "000039": 89, "000056": 109, "000064": 76, "000067": 89, "000068": 69, "000076": 111, "000091": [69, 89], "0001": [72, 90], "000112": 69, "000124": 69, "000128": 87, "000129": 37, "000135": 69, "000144": 111, "000168": 69, "0002": 69, "000219": 28, "0002413443": 126, "000242": 29, "000320": 94, "00032016": 94, "000341": 89, "000351": 68, "000359": 71, "000391": 68, "000421": 69, "000428": 68, "000436": 68, "000442": 89, "0004443313": 126, "000447": 71, "000461": 68, "000474": 71, "00047580260495": 59, "000488": 89, "000492": 69, "000494": 84, "0005": 72, "000511": 71, "000522": 89, "000557": 69, "000562": 71, "000565": 79, "000593": 68, "0005a80b528f": 64, "000616": 68, "000621": [68, 71], "000624": 71, "000647": 68, "000650": 71, "000665": 71, "000670": 89, "000690": 69, "000743": 97, "000875": 70, "000899": 69, "000915799": 143, "0009157990": 143, "000916": [103, 107], "000943": [78, 79], "001": [17, 19, 54, 59, 61, 62, 63, 64, 65, 77, 92, 110, 111, 126, 127, 143, 157, 160], "0010": 69, "001007": 69, "001049": [111, 116], "001051": 89, "001145": 91, "00122": 126, "00133": 64, "00138944": [100, 127], "001403": 95, "001471": 85, "001494": [109, 110, 111], "001533": 69, "0016": [63, 69, 90], "001603": [111, 116], "001698": 85, "001758": 78, "0018": [63, 90], "0019": 72, "001907": 85, "002037900301454": 88, "002067": 69, "002149e": 79, "002169338": 143, "0021693380": 143, "0021693381": 143, "002225": 78, "002290": 67, "0023": 61, "002406": 69, "002436": 84, "0026": 72, "002601": [111, 112], "002687397": 126, "0027": 69, "002728e": 79, "002779": 97, "002786": 71, "0028": [61, 63, 90], "002814": 71, "002821": 98, "00282133350419121": 98, "002829": 69, "002873e": 78, "002887606": 126, "00290774": [111, 114], "002983": 89, "003": [18, 31, 40, 96], "003045": 85, "003051": 78, "003074": 111, "003134": 94, "003220": 75, "003227": 69, "003315": 69, "003319": 69, "003328": 94, "003345": 69, "0034": [69, 81], "003404": 75, "003415": 75, "003427": 89, "003445": 69, "003520": 69, "003528e": 78, "003717": 69, "003779": 84, "003836": 94, "0039": 69, "003924": 84, "004": 59, "004054": 69, "00409412": [100, 127], "004136": 69, "004170": 69, "004192": 78, "0042": [63, 90], "004253": 75, "004269": 79, "004392": 84, "004526": 75, "004542": 85, "0046": 69, "0047": [63, 90], "004811": 69, "004846": 98, "004930324": 126, "005089": 71, "00518448": [111, 116], "005339": [78, 79], "005546": 69, "005857": 89, "005e": 111, "006": 92, "006055": 75, "0060715124549546": 88, "0065": 69, "00651378": 68, "006593": 109, "006721": 71, "0068": 69, "006915": 71, "006922": 72, "006958": [78, 79], "007": 92, "007215": 69, "00728": 157, "0073": 72, "007421": 109, "00765838": 126, "00778625": 126, "007789e": 88, "008": 98, "008023": 91, "008223": [78, 79], "008239": 69, "008487": 72, "008616": 69, "008674": 16, "008825": 80, "008825189994473": 80, "008883698": 127, "00888458890362062": 100, "008884589": 100, "008941": 78, "008969034": 126, "008dbd": 92, "008e80": 92, "009": [92, 98], "009066": 68, "009102": 71, "009122": 94, "009171": [111, 114], "009329847": 127, "009428": 80, "00944171905420782": 98, "00950122695463054": 100, "009501226954630540": 100, "009501227": 100, "009645422": 62, "009656": 94, "00972": 72, "009727": [111, 112], "009790": 91, "0098": 69, "009986": 94, "01": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 51, 54, 55, 59, 62, 63, 64, 65, 68, 71, 74, 78, 79, 85, 90, 91, 92, 93, 94, 95, 110, 111, 114, 116, 126, 127, 143, 157, 160], "010045": 88, "010189": 68, "010213": 97, "010269": 89, "010314": 70, "010344": 92, "010363": 68, "010364": 109, "010385": 68, "010450": 62, "0105": 69, "010696": 92, "010799": 68, "01091843": [111, 114], "010940": 89, "011016": 74, "011131": 94, "0112": 61, "011204": 85, "01128": 72, "011285": 78, "011323": [111, 112], "011598": 94, "011751": 68, "011799": 71, "0118095": 62, "011823": 97, "011829": 69, "011899": 71, "0119793017": 126, "011988e": 94, "01219": 64, "012234": 92, "012248865": 126, "012292": 69, "0124105481660435": 88, "01274": 98, "012794": 70, "012831": 98, "013034": 98, "013088": 25, "013304": 69, "013350": 78, "013429": 92, "013450": 85, "013483": 71, "01351638": 62, "013593": 97, "013677": 95, "013848": 78, "013870": 78, "013976": 79, "01398951": 62, "013990": 143, "014": [92, 95], "01403089": 62, "014062": 68, "014080": [78, 79], "014149": 69, "014215": 69, "014397": 69, "014431": 91, "014432": 67, "014485": 68, "014496": 71, "014572": 71, "014637": 89, "014681": 97, "014721": 91, "014738": 78, "01489635": 126, "015": 64, "015038": 80, "015062": 69, "0151": 69, "015188": 69, "015565": 94, "015670": 69, "015698": 94, "01574297": 94, "015743": 94, "015866": 71, "016154": 89, "016200": [78, 79], "016289": 69, "016317": 69, "0164": 69, "016416": 69, "01643": 158, "016518": 91, "016598": 78, "0167": 69, "016702": 82, "016803": 69, "016871": 69, "016888": 92, "01695242": 71, "017": 64, "017080": 71, "017142": 69, "017185": 91, "017393e": 143, "017564": 69, "017605": 78, "017660": 85, "01772": 145, "0178": 69, "017800092": 143, "0178000920": 143, "017868": 69, "017893": 69, "0179": 69, "017941": 69, "018": [64, 92], "018063": 69, "018092": 109, "018148": 94, "018385": 69, "018452": 69, "0184989": [111, 114], "0187512020118494": 88, "018827": 71, "01903": [64, 110, 155, 157], "01916030e": 126, "01925597": 62, "019433": 69, "019439633": 143, "0194396330": 143, "0194396331": 143, "0195": 69, "019561": 69, "019584": 69, "019596": 80, "019641": 71, "019660": 29, "019687": 69, "019869": 69, "01990373": 99, "019917": 69, "019989": 69, "02": [16, 68, 71, 78, 79, 90, 91, 94, 111, 114, 116, 126], "02016117": 157, "020166": 94, "020259": 69, "020271": 89, "020272": 85, "0203": 69, "020360838": 143, "0203608380": 143, "0203608381": 143, "02052929": [100, 127], "020740": 68, "02079162e": 126, "020793": 71, "02086599": 126, "02092": 157, "020971": 69, "021073": 71, "021269": [82, 83], "021329": 71, "021496": 69, "021508": 68, "02163217": 62, "021654": 68, "021690": 83, "021728": 74, "021862": 70, "021866": 93, "021891": 69, "021926": 80, "022018": 71, "022246": 69, "022258": 85, "022358": 69, "02247976": 62, "022511": 91, "02260304": [111, 116], "022629": 109, "022643": 68, "022749": 109, "022768": 72, "022783": 97, "022820": 78, "0229": 69, "022915": 89, "022950": 91, "023": 92, "023020e": [90, 91], "0232": 69, "023256": 94, "0234": 69, "023464": 69, "023563": 143, "023728": 79, "024250": 78, "024266": 85, "024355": 67, "024364": 144, "024401": [82, 83], "024516": 69, "024604": 89, "02467": 96, "024782": 94, "024926": 67, "024985": 74, "025": [78, 79, 82, 83, 85, 92], "025077": 143, "0253": 64, "025407": 88, "025443": 72, "025518": 69, "0257": 61, "025716": 71, "02571736": 126, "025813114": 143, "0258131140": 143, "02584": 64, "025841": 85, "025964": 85, "0260": 69, "026051": 69, "026632": 69, "026680": 69, "026723": 80, "0268": 69, "026802": 69, "026932": 71, "026966": 85, "02699695": 86, "027": 59, "027419e": 79, "027474": 68, "027484": 69, "027549": 69, "027599": 71, "027735": 68, "027867": 69, "02791": 72, "028063": 71, "0281": 64, "028146": 88, "028255": 69, "028488": 69, "028510": 69, "028520": [78, 79], "028608": 69, "028630": [111, 112], "028655": 69, "028681": 109, "028731": 109, "028854": 69, "028868": 91, "029": 92, "02900983": 94, "029010": 94, "029209": 160, "029364": [144, 150], "0294": 69, "029696": 79, "029831": 94, "029910e": [90, 91], "02e": 63, "03": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 65, 68, 71, 75, 78, 79, 80, 84, 85, 90, 91, 94, 95, 97, 98, 111, 112, 114, 116, 126, 144, 150, 160], "030059": 109, "0301": 64, "030123": [111, 114], "03018": 27, "0302": 69, "030346": 157, "03045": 65, "030497": 79, "0305": 69, "030528": 69, "030647": 68, "0307": 64, "030765": 70, "030817": 79, "030900": 71, "030933": 69, "030934": 94, "030962": 94, "030964": 74, "030967": 69, "0310": 69, "031022": 69, "031075": 88, "03113": 99, "031134": 110, "0312": 69, "031269": 72, "031295": 68, "031323": 88, "031491": 88, "031639": 94, "031644": 68, "031692": 88, "031712": 91, "031864": 69, "031880": 71, "03191": 158, "032": 92, "03207942": 126, "03220": 159, "032224": 68, "0323": 61, "032403663": 126, "032416": 71, "03244552": 110, "0325": 157, "032546": 69, "032664": 78, "032675": [111, 114], "032793": 69, "032940": 69, "032953": 97, "033261": 69, "0333": 69, "033353": 26, "033402": 71, "033464": 68, "033564": 69, "033661": 88, "033779": 88, "033939": 69, "033943": 92, "033946": [82, 83], "03411": 157, "034258": 70, "03438": 65, "034468": 69, "0345": 69, "0345945426": 126, "034609": 79, "034666": 79, "034690": 80, "034807": 68, "034812763": 143, "0348127630": 143, "0348127631": 143, "034846": 90, "03489": [43, 62, 89], "0349": 69, "034957": 69, "03508": 70, "0351": 69, "035119185": 143, "0351191850": 143, "0351191851": 143, "035222": 69, "035265": 80, "03534806": [111, 114], "03536": 157, "03538": 64, "03539": 64, "035391": 72, "0354": 64, "035411": 157, "03545": 64, "035545": 72, "035572": 72, "035579": 79, "0356": 69, "035632": 69, "035689": [111, 116], "035695": 69, "035730": 94, "035735": 79, "03574": 72, "035762": 94, "0359": 64, "035916": 68, "036": 92, "036129015": 143, "0361290150": 143, "0361290151": 143, "036143": 94, "036147": 94, "036209": 68, "036214": 69, "036240": 75, "036316": 69, "036381": 78, "036473": 69, "0365": 69, "036577": 74, "036729": 89, "0368": 61, "037008": [82, 83], "037114": 85, "037271": 78, "0374": [64, 69], "037509": 99, "037513": 68, "037529": 85, "037577": [111, 112], "037747": [78, 79], "03783666": 94, "037837": 94, "037855e": 79, "038006": 79, "038056": 69, "038085": 79, "038103": 85, "0387": 70, "038735": 68, "038812": 93, "038831": 88, "038847": 78, "038878": 92, "039074": 69, "039105": 69, "039141": 75, "039154": 91, "03917696": [127, 143], "03920960e": 126, "039302e": 80, "0394": 69, "039661": 85, "039895": 85, "04": [16, 40, 63, 68, 71, 75, 78, 79, 90, 91, 94, 95, 97, 111, 112, 114, 116, 126, 160], "040010": 85, "040037": 71, "040067": 79, "040112": 143, "040118": 74, "040139": [78, 79], "040204": 69, "040450": 69, "040512": 70, "040533": [39, 127, 143], "04053339": 143, "040784": 75, "040813": 78, "040912": 79, "0410": 69, "041091": 78, "041147": 80, "041240": 69, "041262": 88, "041275e": 78, "041284": 80, "0413": 69, "041381": 78, "041387": 80, "041491e": 80, "0416": 69, "04165": 111, "041787": 69, "0418": 61, "041831": 80, "041860": 69, "042": 92, "042034": 98, "042060": 88, "042265": 80, "0425": 110, "042517": 79, "042583": 91, "042683": 69, "0427": 69, "0428": 99, "042804": 85, "042819": 68, "042844e": 94, "042854": 78, "042911": 69, "043002": 79, "043054": 69, "043082": [111, 116], "043091": 70, "0433": 61, "0434e374": 64, "043512": 68, "04387": 110, "043884": 69, "043928": 69, "043996": 79, "044": 92, "044113": 80, "04415": 64, "044176": 85, "044239": 85, "04424": 64, "044291": 69, "044399": 79, "044447": [103, 107], "04444978": 143, "044449780": 143, "0445": [74, 110], "04462225": 126, "04465": 62, "044811": 69, "04486": 157, "04487585": [144, 150], "04491": 111, "044929": 85, "04497975": [144, 150], "04501612": 143, "04502": [110, 127, 143], "045144": 89, "045172": 85, "045379": 157, "04552": 89, "045521": 71, "045553": 80, "045568": 69, "045624": 67, "04563": 110, "045662": 70, "045754": 94, "04586": 110, "045922": 69, "045932": 94, "045984": 85, "045991": 78, "045993": 110, "046": 92, "046088": 94, "04608822": 94, "046238": 78, "04625": 110, "046410": 69, "046451": 85, "046507": [111, 116], "046525": 109, "046527": 80, "0466028": 62, "046614": 70, "046632": 70, "046728": 97, "046788": 91, "04682310e": 126, "046844": 109, "046922": 110, "047123": 79, "047239": 85, "047288": 109, "0473": 78, "047390": 79, "047829770": 126, "047873": 85, "047954": 89, "048": 59, "048035": 71, "048308": 83, "04843155": 126, "048476": 85, "04863013": 126, "048699": 99, "048723": 110, "048899": 69, "049": 92, "049356": 69, "049459": 68, "049547": 71, "049563": 68, "049573": [111, 116], "049599": 69, "049629": 69, "04964": 92, "049729": 79, "05": [16, 55, 59, 61, 62, 63, 64, 65, 68, 71, 78, 79, 80, 81, 89, 90, 91, 92, 94, 95, 98, 110, 111, 112, 114, 116, 126, 127, 143, 157, 160], "050128": 68, "05039": 97, "05087778": 126, "050996": 69, "051": 64, "051186": 94, "051348": 70, "051562": 71, "051651": 95, "051712": 93, "051870e": 80, "052": 95, "052005": 68, "052023": 85, "0520233166790431": 88, "052205": 70, "052377": 69, "0524": 69, "052488": 83, "052502": 94, "052745": 80, "052798": 69, "052811": 91, "053": [64, 111], "053188": 78, "053284": 70, "0533": 61, "053331": 80, "053389": 143, "053537": 68, "053541": 94, "053558": 80, "053659": 66, "053830": 70, "054": [64, 95], "054064": 88, "054068": 89, "054162": 89, "054330": 79, "054339": 143, "054370": 80, "054529": 143, "054771e": 94, "055118": 69, "055165": 97, "055280e": 78, "055410": 79, "055430": 68, "055439": 91, "055493": 98, "055564": 69, "055599": 69, "0556": 126, "055680": 143, "055819": 69, "056": 95, "0560": 69, "056172": 74, "056309": 70, "05641568": 126, "056428": 71, "056499": 83, "056751": 71, "056915": 85, "057303": 69, "057371": 91, "057535": 78, "0576": [63, 90], "057608": 78, "057762": 94, "057845": 70, "057962": 80, "058042": 143, "058175": 94, "058233": 69, "058340": 71, "058375": 75, "058383": 78, "058443e": 68, "058463": 94, "058467": 79, "058508": 99, "058538": 25, "058676e": 78, "0587": 69, "05872875": 126, "058892": 78, "05891": 111, "058933": 71, "0590": 61, "059058": 79, "059187": 68, "059384": 94, "059630": 67, "059685": 94, "059936": 78, "059976e": 79, "06": [18, 31, 40, 68, 71, 75, 78, 79, 80, 90, 91, 94, 110], "0600": 69, "060016": 75, "060049": 69, "060074": 69, "06008533": 111, "060201": 94, "060212": [90, 91], "0603268864456956": 88, "060471": 68, "060534": 69, "060845": 143, "060963": 69, "060965": 70, "060975": 68, "0611": 61, "06111111": 64, "061322": 69, "061455": 70, "0615": 61, "062": [95, 111], "062293": 79, "062414": 91, "06269": 96, "06270135": 86, "062774": 78, "0628": 61, "062964": 143, "063017": 75, "063067": 78, "063098": 78, "063113": 70, "0632": 61, "06329768": [111, 114], "063312": 91, "063327": 85, "063428e": 91, "0635": 61, "063590": 94, "0636": 61, "063641": 79, "063686613": 126, "063712": 68, "063715": 69, "0638": 61, "06389": 111, "063927": 71, "06397789": 94, "063978": 94, "063979": 78, "064": 92, "0640": 61, "064066": 70, "064164": 91, "06417": 68, "06428": 90, "064280": 90, "0645": 61, "064589": 68, "064612": 70, "0646222": 63, "064673": 74, "0647": 61, "0649": 61, "064918": 71, "065": 98, "065277": 109, "0653": 61, "065356": [82, 83], "0654": 61, "065443": 70, "0655": 61, "065535": 30, "065589": 70, "065725": 80, "065768": 69, "0658": 69, "0659": 61, "065946": 70, "065969": 111, "065976": 85, "066003": 69, "066039": 79, "066044": 70, "0662": [61, 69], "066295": 85, "066387": 70, "066464": 97, "066478": 88, "0665228323": 126, "066788": 68, "0669": 61, "06692492": 126, "0671": 61, "067212": 85, "0673": 61, "067436": 88, "0675": 61, "067506": 70, "067528": 98, "067639": 91, "067721": 143, "067782": 71, "067878": 68, "067990": 68, "068": 59, "068036": 69, "06811": 111, "068148": 94, "068221": 93, "06827": 97, "068340": 78, "068369": 70, "068377": 91, "068567": 68, "068673": 71, "068700": 109, "068781": 69, "06888762": 126, "068934": 75, "06895837": 62, "0690504": 126, "069088": 70, "069144": 88, "069150": 70, "069223": 71, "069443": 75, "0695854": 62, "069589": 85, "069634": 78, "07": [71, 78, 79, 91, 94, 95, 98, 109, 111, 126], "070013": 71, "070020": 94, "0702127": 62, "070393": 91, "0704": [61, 69], "070427": 78, "070433": 85, "070497": 98, "0707": 61, "070797": 91, "07085301": 111, "070884": 94, "0711": 61, "071158": 68, "07123152": 126, "071279": 143, "071287": 68, "07136": [62, 89], "071369": 71, "071543e": 80, "0716": 61, "07168291": 62, "071731": 91, "071771e": 78, "071777": 110, "071782": 29, "0719": 61, "07202564": [82, 83], "072069": 26, "072153": 78, "07222222": 64, "072293": 93, "07229774": [111, 116], "072516": 85, "072583e": 79, "072605": 75, "0727": 111, "073": 95, "073013": 94, "073091": 69, "073207": 89, "073293e": 78, "073347": 69, "073384": 85, "073427": 69, "073447": [111, 116], "07347676": 62, "07350015": [36, 43, 62, 89], "073518": 80, "0735183279373635": 80, "073520": 80, "073561": 68, "0736": 61, "07366": [64, 110], "073900": 79, "074184": 71, "0743": 61, "074304": 143, "07436521": 126, "074426": 94, "07456127": 62, "074600": 71, "074700e": 91, "07479278": 97, "074927": 75, "075261": 67, "075269": 69, "075384": 94, "07538443": 94, "07544271e": 126, "07561": 157, "07564554e": 126, "0758": 98, "075809": 75, "075869": 110, "076": 92, "076019": 90, "076119": 91, "076150": 143, "076179312": 143, "0761793120": 143, "076187e": 78, "076279": 78, "076322": 94, "076347": 80, "0765": 64, "076527": 69, "076653": [111, 116], "076684": 157, "076720": 68, "07685043": 126, "07689": 64, "07691847": 126, "076951": 70, "076953": [82, 83], "076971": 72, "077090": 88, "077234": 69, "07727773e": 126, "077319": 94, "077502": [144, 150], "077525": 69, "077527e": 91, "077543": 78, "077592": 85, "077702": 75, "0777777777777778": 110, "07777778": [64, 110], "07781396": 126, "077883": 94, "07788588": [111, 116], "077920": [103, 107], "07796": 111, "078090": 143, "078095": 16, "078141": 69, "078188": 68, "078207": 72, "07828372": 143, "078382": 79, "078384e": 78, "078426": 109, "078474": 143, "078720": 68, "078810": 94, "079033": 71, "079051": 69, "079085": 72, "079137": 70, "07915": 64, "079158": 68, "07919896": 126, "07942v3": 158, "079458e": 90, "07961": 97, "079650": 71, "079674": 71, "079761": [103, 107], "07978296": 126, "08": [71, 80, 91, 94, 98], "080041e": 78, "080121": 91, "08031571": 126, "080583": 71, "080652": 68, "080712": 71, "080854": 91, "08091581": 126, "080946": 69, "080947": 72, "080987": 78, "081": 64, "0810": 69, "081100": 94, "081230": [78, 79], "08136022": 126, "081396": 83, "0814": 69, "081469": 68, "081488": 89, "08154161": 126, "08181827e": 126, "0820": 61, "082031": 69, "082197": 85, "082297": 126, "082584": 70, "082643": 16, "08278693": 126, "082804": 67, "082905": [111, 116], "082973": 89, "083079": 91, "083153": 79, "083258": 143, "083312": 143, "08333333": 64, "08333617": 126, "083416": 92, "0835771416": 62, "0836": 85, "08364": 85, "083706": 98, "083845": 69, "083929": 69, "083949": 98, "084": 62, "084007": 84, "084269": 91, "084288": 78, "084589": 69, "084714": 79, "084807": 79, "084976": 71, "085171": 68, "085279e": 82, "0853505": 62, "085372": 78, "08546947": 126, "085566": 80, "085592": 85, "085730": 74, "086004": 85, "086015": 71, "08602774e": 126, "0862": 155, "086264": 80, "086298": 69, "086353": 70, "08639008": 126, "086401570476133": 80, "086402": 80, "08664208": 126, "086679": 110, "08672": 126, "086826": 82, "086933": 71, "087184": 38, "0872": 61, "08755249": 69, "087566": 88, "087634": 78, "087826": 79, "08788274": 126, "087947": 94, "088019": 78, "088048": 94, "088082": 88, "088162": 71, "088208": 68, "088282": 83, "088288": [103, 107], "08831209": 69, "088346e": 78, "088357": 94, "088401": 78, "08848": 110, "088482": 29, "08856249": 126, "088792": 88, "088836": 88, "08888889": 64, "0889": 74, "088935": [111, 114], "089016": 71, "08903345": 126, "089191": 79, "089229": 74, "0894": 61, "08941296": 126, "089431": 71, "08945138": 126, "089459": 111, "08968939": 62, "089715": 70, "089802": 68, "089994": 68, "08e": 63, "09": [38, 59, 78, 79, 80, 90, 91, 94], "09000000000000001": 110, "09015": 61, "090255": 94, "090331": 79, "090363": 93, "090728": 87, "090831": 68, "091200": 68, "091298": 68, "091308": 109, "091327": 68, "091391": 143, "091406": 144, "091432": 78, "0916": 61, "091902": 71, "091992": 93, "092229": 98, "092262": 111, "092365": 143, "092453": 94, "09245337": 94, "092646": 94, "092919": 145, "0929369228758206": 88, "093019": 69, "093043": 94, "09310496": 143, "093153": 94, "093173": 78, "09324": 126, "0935": 111, "09351167": 111, "093607": 79, "093740": 143, "093950": 89, "093977": 69, "094026": 89, "094118": 94, "094196": 71, "09433242": 126, "094381": 89, "094420": 91, "09444444": 64, "094596": 71, "09468168": 126, "094721": 68, "094766": 93, "094776": 68, "094795": 71, "094829": 111, "094883": 78, "094968": 79, "094999": 94, "095017": 70, "095104": 75, "095785": 75, "095933": 68, "09603": 155, "096041": 71, "096053": 68, "096337": 89, "096418": 75, "096616": 24, "09682314": 111, "096890e": 109, "096915": 98, "097": 95, "097009": 85, "097140": 82, "097157": 98, "097194": 70, "097208": 93, "097468": 80, "09756": 96, "09779675": 143, "097796750": 143, "098": 63, "098046": 71, "098050": 68, "098134": 92, "09815295": 126, "098256": 94, "09829724": 126, "09830758": 97, "098308": 97, "098319": 94, "0986": 61, "098712": 94, "09879814e": 126, "098866": 69, "098901": 85, "099": 95, "099239": 68, "099485": [111, 112], "099647": 93, "099670": 91, "099731": [78, 79], "09980311": 143, "099854": 68, "09988": 158, "0_": 42, "0ff823b17d45": 64, "0x": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "0x1747bdd4520": 72, "0x1747bdd6b90": 72, "0x7f2d9cde2810": 98, "0x7f5ae18061b0": 111, "0x7f5ae18783e0": 111, "0x7f5ae187ae40": 111, "0x7f5ae18fbcb0": 111, "0x7f5ae2a29cd0": 111, "0x7f5ae2bdb2f0": 110, "0x7f5ae2c99460": 110, "0x7f5ae2c9afc0": 110, "0x7f5ae2ca6030": 111, "0x7f5aef51a4e0": 143, "0x7f5aef51b290": 143, "0x7f5aef684440": 144, "0x7f5aeffe8050": 112, "0x7f5af539e330": 160, "0x7f5af5938c50": 143, "0x7f5af593b740": 143, "0x7f5af5f0d7f0": 112, "0x7f5af62aa0f0": 150, "0x7fe636073f80": 93, "1": [8, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 52, 53, 54, 55, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159], "10": [10, 11, 12, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 35, 36, 37, 38, 39, 40, 41, 43, 44, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 104, 107, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 144, 150, 155, 157, 158, 159, 160], "100": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 43, 45, 62, 64, 65, 66, 78, 79, 81, 84, 85, 86, 89, 92, 96, 98, 99, 100, 102, 107, 109, 110, 111, 113, 126, 127, 143, 144, 150, 157, 159], "1000": [17, 19, 25, 27, 60, 66, 67, 74, 76, 77, 82, 83, 84, 86, 87, 88, 90, 91, 95, 97, 98, 101, 111], "10000": [59, 67, 71, 78, 79, 90, 94], "100000e": 91, "100044": 83, "10005": 71, "100208": 109, "10023": 71, "100356": 80, "10038": 97, "10039862": [99, 111], "100435": 71, "100510": 143, "10052": 71, "100664": 78, "10074": 91, "10079785": 111, "1008": 69, "100807": [78, 79], "100816": 69, "100858": 97, "10089588": 94, "100896": 94, "100923": 94, "100_000": 92, "101": [18, 31, 40, 61, 95, 109, 158, 159], "10105": 71, "101075": 68, "10109": 109, "101138": 68, "10126": 91, "10127930": 143, "101279300": 143, "1015": [63, 90], "1016": [17, 18, 19, 31, 40, 61], "1016010": 63, "1016338581630878": 88, "1018": 91, "101998": 75, "102": [102, 107, 109, 157, 159], "102282": 16, "10235": 91, "102616": 80, "10270522": 126, "10277": 91, "102775": 80, "102793": 71, "10291814": 69, "10299": 90, "103": [78, 89, 95, 102, 107, 109, 159], "10307": 143, "1030891095588866": 88, "1031": 91, "103179163001313": 88, "103215": 16, "10348": 90, "103494892": 126, "103497": 94, "103727": 71, "103806": 80, "1039": 96, "10396": 90, "104": [63, 90, 109, 159], "1040": 91, "104016": 78, "10406": 91, "1041": 61, "10414": 91, "10416593": 69, "104383": 79, "104475": 78, "10449": 96, "104492": 85, "1045303": 62, "1047": 96, "104787": 89, "105": [42, 62, 85, 89, 109, 159], "10521988": 69, "105227e": 79, "105318": 94, "1054": 64, "105461": 109, "1055": [61, 96], "105541": 69, "105983": 71, "106": [64, 109, 159], "10607": [72, 102, 107, 157], "106116": 91, "10627": 91, "1063": 74, "10637173e": 126, "106385": 143, "1065": [81, 86, 88, 111], "106595": [12, 111, 113], "106674": 71, "106715": 84, "107": [64, 69, 98, 109, 159], "107026e": 68, "107073": 80, "107156": 85, "107290": 143, "1073": 91, "10744879": 69, "107467": [103, 107], "10747": [72, 102, 107, 157], "107746": 94, "107809": 78, "10791": 91, "107935": 94, "108": [109, 155, 158, 159], "1080": [36, 43, 61, 62, 89], "10824": [72, 102, 107, 157], "108257e": 91, "108259": 75, "10829": 91, "10831": [72, 102, 107, 157], "108742": 16, "108783": 80, "108783087402629": 80, "10878571": 94, "108786": 94, "108870": [111, 116], "109": [78, 109], "10903": 90, "109069": 143, "109079e": 94, "109273": 89, "109277": 85, "10928": 91, "1093": [41, 81], "109373": 69, "109454": 91, "1095": 74, "10952798": 126, "1096": [61, 96], "10967": 90, "109861": 157, "1099472942084532": 76, "10e": [80, 94], "11": [39, 41, 59, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 94, 95, 97, 98, 99, 100, 102, 107, 109, 110, 111, 112, 113, 116, 126, 127, 143, 144, 150, 157, 159, 160], "110": [109, 159], "110081": 85, "1101": 91, "110179": 79, "11019365749799062": 98, "110194": 98, "110359": 89, "110365": 98, "110399": 70, "110448": 71, "110557": 88, "110681": 97, "11071087": [99, 111], "110717": 143, "110742": 88, "110777": 68, "1109": 91, "110e": 111, "111": [79, 109, 159], "111008": 68, "111043": 82, "1111": [10, 11, 44, 60, 62, 77, 81, 89, 98, 101, 111, 144, 150, 155], "11120": 91, "111352344760325": 88, "111358": 70, "111499": 68, "111577": 91, "111601": 68, "1117": [81, 86, 88], "111712": 71, "111783": 91, "1118": 63, "11199615e": 126, "112": [64, 109, 159], "1120": 90, "112078": 109, "11208236": [100, 127], "112135": 80, "1121351274811793": 80, "1122": 91, "112216": 80, "11252022": 69, "1126197": 126, "112867e": 79, "113": [10, 109, 159], "113005": 91, "113022": 85, "11311": 90, "113149": 85, "11319019": 69, "113207": 94, "113270": 80, "113418": 69, "11376": 91, "113780": 89, "113952": 85, "11399": 90, "113995": 71, "114": [109, 126, 159], "114026": 88, "114258": 78, "1144": 91, "1144500": 62, "11447": 97, "114530": 82, "1145370": 62, "114570": 79, "11458": 91, "114592": 68, "114647": 80, "1147": 61, "114834": 91, "114989": 75, "115": [92, 109, 159], "11500": [90, 160], "115060e": 94, "1151": 91, "115297e": [90, 91], "11530": 91, "11552911": 97, "115690": 71, "11570": 90, "115785": 16, "11588": 91, "115901": 75, "115910": 68, "116": [109, 159], "116090": [111, 114], "116274": 80, "116412": 69, "1166": [90, 158], "1167": 90, "116727": 70, "11673": 91, "11676": 91, "117": [78, 109], "11700": 160, "117194": 93, "11720": 91, "117242": 94, "11724226": 94, "117366": 94, "11743": 160, "117457": 79, "11750": 91, "1176": 61, "117696": 68, "1177": 61, "117710": 80, "11789998": 94, "117900": 94, "11792": 63, "11796": 91, "118": 109, "118018e": 111, "1182": 63, "11820": 91, "11823404": 98, "118255": 94, "118278": 68, "11850": 91, "118596": 80, "1186": 63, "118601": 89, "11861": 63, "1187": 97, "118708e": 89, "1187339840850312": 89, "118799": 91, "118952": 89, "119": [98, 109, 159], "119230": 68, "119306": 70, "11935": 97, "119409": 88, "11942111": [111, 116], "119434": 68, "119766": 94, "1198": [62, 89], "119820": [111, 114], "1199": 69, "12": [4, 13, 16, 17, 19, 54, 55, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 102, 107, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 144, 150, 155, 157, 158, 159, 160], "120": [65, 66, 88, 99, 109, 159], "12002": 90, "1200x600": [68, 69, 71], "1200x800": [68, 69, 71], "120165": 68, "1202": 158, "120567": [82, 83], "120721": 89, "12080467": [111, 114], "120838": 68, "120955": 71, "12097": [10, 11, 44, 62, 81, 89, 91, 101, 155], "121": [91, 109, 159], "1210": 91, "12105472": 143, "121054720": 143, "1211": 91, "121297": 91, "1213405": 62, "1214": 143, "121584e": 94, "121743": 71, "121750": 79, "121774": 84, "12196389e": 126, "122": [18, 31, 40, 61, 68, 95, 102, 107, 109, 158, 159], "12214": 63, "12223182e": 126, "122408": 80, "122421": 85, "122671e": 79, "122777": 143, "123": [46, 63, 64, 68, 71, 90, 98, 109, 159, 160], "1230": 91, "123062": 71, "123192": 98, "12323": 91, "1234": [59, 60, 61, 72, 76, 77, 95, 96, 101, 110, 126, 143], "12344330": 126, "12348": 98, "123501e": 91, "123950": [111, 112, 116], "124": 109, "12410": 91, "124456": 71, "124805": 90, "125": [109, 159], "12500": 90, "125059": 143, "12539340": 143, "125649": 109, "12572": 91, "125751": 68, "1258": [62, 91], "126": [109, 159], "12612": 91, "12616": 91, "126361": 71, "126494": 16, "126771": 143, "126802": 91, "126820": 70, "126821": 68, "12689": 91, "127": [31, 109, 159], "12705095": [127, 143], "12707800": 62, "127337": 85, "127385": 71, "127420": [111, 114], "1275": 91, "12752825": 143, "127563": 97, "1277": 92, "128": [63, 109, 159], "12802": 63, "12807557": 69, "12814": 91, "128229": 85, "128258": 71, "128312": 94, "128372": 79, "128393": 91, "128408": 89, "128412": [103, 107], "1285": 61, "12861": 91, "12863909": 69, "128846": 68, "128937": 68, "129": [89, 109, 159], "129085": 79, "129418": 68, "129446": 79, "12945": 158, "12947879": [111, 114], "1295": [61, 91], "12955": 90, "1297": 91, "12980769e": 126, "12983057": 111, "13": [17, 18, 19, 33, 35, 40, 41, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 78, 79, 80, 82, 83, 84, 85, 88, 89, 90, 91, 94, 95, 96, 97, 98, 100, 102, 107, 109, 110, 111, 113, 114, 126, 127, 143, 144, 150, 157, 160], "130": [64, 89, 109, 159], "13003274": [111, 114], "130122": 97, "13034980e": 126, "130370": 80, "1306": 97, "130609": 79, "130802": 78, "130826": 71, "130829": 94, "13084": 111, "13091": 91, "130971": 82, "131": [109, 159], "13101620222324384042464952576667708490": 126, "13102231": 111, "1311": 69, "131105": 69, "13119": 97, "1312": 160, "1313": [63, 160], "13137893e": 126, "131483": 85, "1318": 61, "131842": 85, "131928": [103, 107], "132": [64, 78, 89, 109, 159], "13208": 160, "1321": [90, 160], "132248": 109, "1324": [63, 90], "132454": 67, "132481": 109, "1325": 63, "132502": 71, "13257": 90, "13259334": 69, "132671": 80, "1328": 97, "132922": 71, "132941": 91, "133": [64, 102, 107, 109, 158, 159], "133125": 70, "133202": 91, "133321e": 68, "133421": 91, "133509": 78, "13357": 91, "133586": 70, "133596": 94, "133676": 79, "133839": 85, "133899": 68, "13398": 98, "13399086": 126, "133f5a": 92, "134": [89, 99, 109, 159], "134037": 70, "1340371": 61, "134078": 69, "1341": 63, "134211": 94, "1343": [90, 91], "134567": 91, "13459253": 69, "1346035": 63, "134664e": 78, "134687": 91, "13474": 91, "134765": 91, "134784": 111, "1348": 90, "134875": 70, "13490": 91, "135": [64, 109, 111, 159], "13505272": 62, "135074": 71, "135255": 71, "135309": 78, "135375": 75, "135378": 143, "135398": 79, "135431": 69, "135449": 68, "135596": 91, "135707": 110, "135729": 71, "135856": 94, "13585644": 94, "135871": 89, "1359": 91, "136": [72, 89, 98, 109, 159], "1360": [63, 74], "136010": 78, "13602": 98, "136089": 89, "136250": 70, "13642": 91, "136442": 89, "1366": 92, "1367095": 126, "13673608": 126, "136836": 89, "136885": 91, "136898": 78, "13692928": 69, "137": [31, 64, 72, 109, 159], "1371": 91, "137165": 111, "137230": 88, "137254": 69, "1373": 69, "137317": 68, "137396": 94, "1378": 91, "137814": 69, "137999": 111, "138": [109, 159], "1380": 90, "13809": 91, "138142": 82, "138264": 98, "138378": 80, "1384": 90, "1386": 61, "13868238": 143, "138682380": 143, "138698": 143, "1387": 61, "13892432": 69, "13893": 91, "139": [98, 109, 157], "139013": 69, "1392": 69, "139310": 68, "139491": 143, "139508": 85, "13956": 97, "139622": 91, "13972196": 69, "1398": 91, "139830": [111, 116], "1399": 61, "14": [60, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 74, 75, 78, 79, 80, 82, 83, 84, 85, 87, 88, 89, 90, 91, 94, 95, 97, 98, 100, 102, 107, 109, 110, 111, 113, 114, 126, 127, 143, 144, 150, 157, 158, 160], "140": [65, 66, 85, 91, 99, 109, 159], "1400": 91, "14000073": 126, "1401": 61, "14018": 111, "140530": 78, "140770": [78, 79], "140833": 80, "14085944": 69, "140861": 62, "141": [91, 109, 159], "141069": 79, "141101e": 91, "14114": 97, "141247": 75, "14141": 91, "141484": 78, "141546": 143, "141820": 80, "141921": 71, "142": [109, 159], "14200098": 143, "142045e": 89, "142132": 78, "142270": 67, "1424": 110, "142482": 94, "14268": 111, "14281403493938022": 110, "14289": 91, "142961": 68, "143": [102, 107, 109, 159], "1435": 91, "143593": 78, "14368145": 143, "144": [109, 159], "14400": 90, "14405": 91, "14406": 91, "144084": 80, "1441": 61, "144195": 79, "1443": [68, 91], "14463": 71, "144669": 94, "144800": 80, "144813": 75, "144908": 93, "144971": 90, "145": [109, 159], "145245": 94, "14530": 91, "14532650": 143, "145625": 94, "145699e": 78, "145737": 143, "14588": 91, "145943": 68, "146": [92, 109, 159], "146037": 94, "146087": 157, "146214": [103, 107], "14625": 91, "1465": 63, "146641": 143, "1468115": 62, "146861": 109, "147": [109, 159], "14702": 72, "147121": 94, "147383": 79, "14744": 91, "147464": 109, "147703": 68, "14772": 91, "1479": 91, "14790924": 143, "147909240": 143, "147927": 72, "14795": 91, "148": [109, 159], "148005": 75, "14803": 91, "1481": 126, "148134": [78, 79], "148161": 94, "148210": 80, "1482102407485826": 80, "14845": 72, "148455": 80, "1484554868601506": 80, "1485": 91, "148750e": [90, 91], "14876": 71, "148835": 91, "149": [59, 109, 159], "1492": 59, "149228": 98, "149265": 79, "149285": 94, "149392e": 109, "149446427": 78, "149472": 98, "149681": 78, "149714": 89, "149858": 28, "149897": 79, "149898": 94, "15": [11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 60, 62, 63, 64, 66, 68, 71, 74, 75, 78, 79, 80, 84, 85, 86, 87, 88, 89, 90, 91, 94, 95, 96, 97, 98, 102, 107, 109, 110, 111, 113, 114, 126, 127, 143, 144, 150, 157, 160], "150": [42, 64, 98, 109, 159], "15000": [63, 90], "150000": 63, "15000000000000002": [80, 91, 94, 110], "1502": 62, "150200": 89, "150234": 91, "150408": 62, "150435": 91, "150484e": 68, "150614": 72, "15066584": 126, "150719e": 90, "150783": 68, "151": [109, 159], "151036": 68, "15108": 91, "151447": 88, "151636": 80, "15167541": 69, "15168206": 69, "151779": 71, "151819": 94, "151835": 68, "15194": 90, "152": [109, 159], "15202883": 69, "152148": [78, 79], "15236311": 69, "152414": 79, "15263117": 69, "152706": 111, "15287": 91, "15292": 71, "152926": 67, "152934e": 71, "153": [98, 109, 159], "15308371": 69, "153119": 80, "153293": 91, "15339": 91, "153398": 91, "15354": 97, "153585": 68, "153587": 89, "153633": 72, "153639": 126, "153971e": 78, "153983": 80, "1539831483813536": 80, "154": [74, 109], "15430": 160, "154324": 71, "154415": 143, "1545": 91, "154557": 94, "15470": 71, "154707": 75, "154752": 143, "154821": [111, 114], "154828": 80, "155": [74, 109, 159], "155000": 90, "155025": 94, "155120": 94, "155160": 75, "1554": 91, "155423": 75, "15549": 91, "15556": 91, "1557093": 62, "155954": 68, "155995": 79, "156": [109, 159], "156021": 94, "156198": 71, "156202": [78, 79], "156209": 79, "156317": [78, 79], "156328": 88, "1564": 143, "156540": 143, "156554": 78, "156704": 91, "1569": 91, "156969": 80, "157": [79, 109, 159], "157080": 143, "157470": 93, "1576": 91, "1577657": 62, "158": [109, 159], "158007": 94, "158076": 79, "15815035": 63, "158178": 80, "158198": 91, "1582": 91, "158288": 91, "158440": 69, "1586": 91, "158682": 88, "158697": 143, "158726": 109, "15881226": 69, "15881385": 69, "1589": 91, "159": 159, "15915": 70, "15916": [61, 70], "159386": 97, "159466": 94, "15946647": 94, "159592": 79, "1596": 64, "159662e": 71, "159826": 91, "159959": 91, "159970": 71, "15e": 126, "16": [24, 59, 60, 62, 63, 64, 65, 66, 68, 70, 71, 74, 75, 78, 79, 80, 83, 84, 85, 89, 90, 91, 94, 95, 97, 98, 102, 107, 109, 110, 111, 113, 114, 126, 127, 143, 144, 150, 157, 160], "160": [65, 66, 99, 159], "160102": 68, "160285e": 79, "1604": 63, "160475": 68, "16054893": 69, "160825": 82, "160836": 85, "160932": 80, "161": [64, 74, 158, 159], "161141": 89, "161236": 94, "161243": 94, "161305": 78, "161441": 66, "1619": 63, "16192879": 69, "162": [74, 159], "16201": 91, "16202854": 69, "16211": 90, "162153": 94, "1622": 91, "16233899": 69, "162389": 78, "16241": 91, "162436": 98, "16249": 91, "1626685": 62, "162683": 98, "162710": 80, "162752": 88, "1628": 90, "162909": 89, "162930": 91, "163": 159, "163013": 16, "163039": 71, "1631553": 126, "163194": 94, "163219": 71, "16341943": 69, "163496": 71, "163519": [111, 114], "163566": 91, "163895": 80, "164": [75, 91, 95, 111, 159], "164034": 143, "164467": 90, "164608": 94, "164698": 84, "16474935": 69, "1648": 61, "164801": 94, "164805": 80, "164864": 89, "164941": 16, "164943": 88, "165": [74, 98, 159], "16500": 90, "165178": 94, "165266": 71, "1653": 91, "16536299": 143, "165362990": 143, "16539906e": 126, "1654": 69, "165419": 94, "16553": 90, "165542": 70, "165549": 157, "165573": 68, "165670e": 79, "165707": 75, "16590": 91, "166": [74, 159], "166067": 68, "166088": 74, "1661": 90, "16618": 91, "166322": [103, 107], "166517": 85, "166904": 91, "166998": 68, "167": [63, 90, 159], "167035": 91, "167547": 94, "1676": 91, "167930": 91, "167981": 143, "168": 159, "16803512": 143, "168092": 143, "1681": [61, 90], "168195": 97, "168371": 69, "1684": 69, "168614": 94, "168931": 94, "169": [64, 159], "1691": [61, 91], "16910": 91, "169117": 98, "169196": 94, "169220e": 80, "16951": 91, "169557": 68, "169677": 87, "16984": 91, "169858": 88, "17": [60, 62, 63, 64, 66, 68, 71, 74, 75, 78, 79, 84, 85, 88, 89, 90, 91, 94, 95, 97, 98, 102, 107, 109, 110, 111, 113, 126, 127, 143, 144, 150, 157, 160], "170": 159, "1705": 91, "170705": 109, "17083": 91, "170880": 68, "170933": [111, 116], "171": [74, 159], "1712": 158, "171255": 109, "1714": 63, "171493": 68, "171575": 94, "171696": 109, "17174945": 69, "171815": 110, "171860": 79, "1719": [68, 69, 70, 71], "171911": 68, "172": [95, 159], "172022": 143, "172083": 79, "17220074": 69, "17229076": 69, "17231049": 69, "17236": 69, "172535": 70, "172672": 37, "172793": 94, "173": [74, 159], "173183": 71, "173504": 83, "17372": 91, "17385178": 110, "173935": 69, "173964": 143, "173e": 95, "174": 159, "174177": 94, "174185": 94, "174499": 143, "174516e": 94, "17453": 91, "1746": 91, "17469": 91, "174707": 71, "17480932": 69, "174968": 90, "174e": 111, "175": [74, 92, 159], "17500": 91, "175080": 71, "1751": 90, "175254": 90, "175284": 80, "175355": 68, "17536": 91, "175507": 68, "175564e": 79, "175635027": 62, "175757": 68, "17576": 91, "175894": 98, "175931": [109, 110, 111], "176056e": 109, "176395": 68, "176495": 94, "17655394": 94, "176554": 94, "1765945": 71, "176719": 68, "176929": 143, "177": [158, 159], "177007": 94, "17700723": 94, "177043": [78, 79], "177207": 69, "1773": 91, "177397": 91, "1774": 61, "177496": 94, "177595": 71, "177611": 94, "177751": 94, "177865": 71, "177878": 68, "177995": 94, "178": [91, 159], "17800": 91, "17807": 91, "178169": 82, "17823": 64, "178235": 68, "178273": 79, "178661": 143, "178751": 74, "178763": 94, "178934": 143, "179": [87, 91, 103, 107, 159], "179101": 109, "179204": 79, "179276": 79, "179319": 68, "179338e": 71, "179347": 68, "179548": [103, 107], "1795850": 62, "179588e": 94, "179810": 79, "1798913180930109556": 92, "18": [60, 62, 63, 64, 66, 68, 71, 72, 74, 78, 79, 82, 84, 85, 86, 88, 89, 90, 91, 94, 95, 97, 98, 102, 107, 109, 110, 111, 113, 126, 143, 157, 160], "180": [65, 66, 99, 159], "180271": 85, "180296": 79, "1803": 61, "18030": 91, "180307": 79, "180474": 79, "180575": [82, 83], "1807": [61, 91], "1809": 158, "180951": 94, "181": 159, "1812": 91, "181330": 75, "181350": 78, "1814": 61, "18141": 91, "181432": 143, "18191547": 69, "182": 159, "1820": 61, "18201607": 69, "182018": 79, "182151": 109, "182288": 79, "182354": 79, "182383": 70, "182399": 16, "182633": 94, "182713": 71, "18273096": [111, 114], "182770": 71, "182849": 94, "183": [64, 68, 71, 74, 111, 159], "1832": 91, "183373": 111, "183526": 80, "183553": 95, "18356413": 111, "18368": 91, "183814": 91, "183855": 110, "183888": 89, "184": [59, 64, 158, 159], "184224": 75, "184303": [111, 116], "184449": 78, "184779": 70, "184806": 74, "185": [63, 64], "185035": 78, "185130": 95, "18516129": [111, 116], "185617": 79, "185930": 109, "185956e": 79, "186": [71, 159], "18604": 91, "186136": 79, "1862": 61, "18622": 91, "18637": 155, "18640728": 126, "186528": 70, "186589": 75, "18666": 91, "186689": 111, "186735": 94, "186775": [111, 112, 116], "18678094e": 126, "186836": 94, "186864": 88, "186868": 66, "187": 159, "187148": 143, "187617": 79, "187690": 94, "18773": 91, "187732": 78, "187782": 78, "1879": 74, "188": [68, 71, 159], "188175": 94, "1881752": 94, "188223": 94, "188541": [111, 112], "188545": 79, "1887": [109, 110, 111], "188760": 85, "1888": 91, "18888149e": 126, "188986e": 78, "189": [64, 91, 95, 159], "189023": 143, "189195": 91, "1895815": [43, 62, 89], "189737": 94, "189739": 75, "18976": 91, "189998": 94, "19": [15, 60, 62, 63, 64, 66, 68, 71, 74, 78, 79, 85, 88, 89, 90, 91, 94, 95, 97, 98, 102, 107, 109, 110, 111, 113, 114, 126, 143, 157, 160], "190": [64, 159], "190000e": 91, "190090": 143, "190231": 91, "19031969": 94, "190320": 94, "19033538": 62, "190500": 68, "190534": [111, 114], "19073905e": 126, "190809": 94, "190892": 98, "1909": [43, 62, 89], "190915": 80, "190921": 83, "190976": 95, "19098": 68, "190982": 94, "191": [64, 68, 158, 159], "191053": 68, "191062": 68, "191081": 70, "1912": 158, "1912705": 101, "191501": 78, "191534": 90, "191578": 25, "19179": 71, "1918": 61, "192": [71, 159], "192171": 71, "192234": 143, "1923": 91, "192526": 97, "19252647": 97, "192539": 29, "192617": 71, "192952": 75, "193": [68, 71, 159], "193060": 94, "193155": 68, "193235": 71, "193237": 68, "193308": 29, "193375": 85, "193581": 78, "193644": 91, "19374710e": 126, "19382": 91, "193833": 91, "193849": 95, "19385": 91, "193870": 78, "193f0d909729": 64, "194": [86, 159], "1941": 63, "19413": [90, 91], "194167": 78, "194253": 71, "194416": 71, "194588e": 78, "1947182": 126, "194786": [111, 112], "194945": 109, "195": [68, 104, 107, 159], "19508": 98, "19508031003642456": 98, "19509680e": 126, "195101": 78, "195132": 79, "19535571": 126, "195377": 94, "195396": 94, "19550": 91, "195564": 89, "19559": [63, 90], "195816": 78, "1959": 158, "195920": 74, "195963": 85, "196": [71, 159], "196034": 71, "196099": 111, "196189": 94, "196428": 71, "196655": 85, "19680840": 143, "196824": 94, "196835": 88, "197": [103, 107, 159], "1970": 91, "19702": 71, "19705": 91, "197225": [72, 102, 107, 157], "1972250000001000100001": [64, 102, 107, 157], "197356": 78, "197424": 110, "197462": 79, "197484": 143, "1975": 91, "19756": 91, "19758": 91, "197600": 67, "197801": 68, "19793": 91, "197947": 71, "19798": 91, "198": [68, 71, 159], "198221": 91, "19824": 91, "198493": 85, "198503": 95, "198529e": 79, "198549": 72, "198617": 93, "198687": 63, "1988": [60, 77, 101, 111], "198803": 69, "198822": 71, "199": [90, 159], "1990": [63, 90, 91], "1991": [63, 90, 91, 160], "199213": 79, "199281e": 94, "199445": 143, "1995": [62, 89], "19962187": 111, "1998": 92, "19983954": 99, "1999": 92, "19e": 126, "1_": [80, 94], "1d": 13, "1e": [12, 15, 54, 55, 85, 91], "1f77b4": 67, "1m": 110, "1mnon": 110, "1x_4x_3": 67, "2": [10, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 45, 46, 47, 49, 51, 53, 54, 55, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 120, 126, 127, 143, 144, 145, 147, 149, 150, 151, 152, 153, 154, 156, 157, 158, 159], "20": [12, 14, 15, 17, 18, 19, 22, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 37, 38, 39, 40, 41, 43, 44, 45, 60, 62, 63, 64, 65, 66, 67, 68, 71, 74, 78, 79, 80, 84, 85, 86, 87, 88, 89, 90, 91, 94, 97, 98, 99, 100, 101, 102, 107, 109, 110, 111, 113, 126, 127, 143, 144, 150, 157, 160], "200": [17, 19, 32, 35, 42, 61, 65, 66, 68, 71, 80, 81, 86, 93, 94, 99, 101, 106, 107, 110, 158, 159], "2000": [11, 30, 63, 65, 75, 78, 79, 80, 85, 90, 91, 94, 96, 99, 109, 111], "20000": [63, 90], "20000000000000004": [80, 91, 94], "200000e": 91, "200042": 78, "200081": 68, "2001": 69, "20010": 91, "200110": 91, "200225": 74, "2003": [10, 69, 158], "200303": 157, "2004": [69, 111, 115], "2005": [66, 69], "20053419": 69, "20055": 91, "2006": [69, 91, 111, 115], "2007": [69, 111, 115], "20074": 91, "20076649": 69, "200982": 82, "201": [64, 71, 159], "2010": [62, 89], "2011": [62, 89, 155, 157], "201236": 68, "2013": [81, 143, 158], "2014": [143, 158], "2015": [42, 158], "201514": [111, 114], "201528": [78, 79], "20158": 91, "20159309": 69, "2016": 92, "20167273": 86, "2017": [34, 158], "201768": 89, "2018": [10, 11, 44, 45, 60, 62, 63, 66, 77, 81, 86, 89, 90, 91, 96, 97, 101, 111, 117, 126, 127, 135, 143, 155, 158, 159], "2019": [32, 64, 78, 79, 80, 82, 83, 91, 94, 97, 110, 127, 133, 136, 137, 155, 157, 158], "20193613": 69, "201e": 95, "202": [68, 159], "2020": [12, 14, 15, 17, 18, 19, 31, 33, 35, 40, 61, 64, 66, 68, 71, 87, 98, 110, 111, 113, 117, 144, 145, 158], "2020435": 62, "2021": [17, 19, 41, 43, 61, 62, 64, 68, 69, 70, 71, 78, 79, 89, 111, 112, 115, 117, 127, 138, 158, 159], "202145": 71, "20219609": 62, "2022": [97, 98, 111, 113, 117, 144, 145, 154, 155, 158], "202298": 79, "2023": [36, 65, 96, 99, 111, 125, 127, 141, 142, 158], "2024": [59, 76, 81, 86, 88, 92, 95, 96, 98, 111, 155, 158], "2025": [16, 17, 19, 68, 71, 96, 111, 112, 114, 116], "202650e": 80, "20269": 91, "2029554862": 67, "203": [63, 78, 90, 159], "203082": 109, "2032": 74, "203284": 80, "20329": 91, "203546": 71, "203828": 91, "204": [68, 71, 159], "204007": 94, "20400735": 94, "204362": 98, "204445": 68, "204482": 94, "204495": 79, "204794": 94, "204893": 75, "205": [68, 92, 95, 97, 159], "205187": 80, "205224": 97, "205245": 91, "205333e": 90, "20534604": 126, "205402": 71, "205512": 71, "20586329": 69, "205938": 89, "205971": 69, "206": [68, 74, 159], "206253": [90, 91], "206256": 85, "20660222": 69, "206752": 74, "206802": 26, "206848": 69, "206896": 68, "207": [68, 95, 159], "207242": 79, "2075": 61, "20783816": 62, "207840": 83, "207885": 90, "207912": 143, "208": [68, 71, 74, 75, 159], "2080787": 62, "20823898": 62, "208300": 94, "208375": 71, "208485": 74, "2085167": 126, "208564": 68, "2086": 91, "2087": 69, "208741": 69, "209": [71, 74, 75], "209137e": 68, "209219e": 97, "2095505": 126, "20956": 91, "209894": 94, "209991": 69, "21": [10, 11, 44, 60, 62, 63, 64, 66, 68, 71, 74, 78, 79, 81, 88, 89, 90, 91, 94, 96, 97, 98, 101, 102, 107, 109, 110, 111, 113, 126, 143, 155, 157, 158, 160], "210": [17, 18, 19, 35, 40, 68, 71, 74, 75, 91], "210111": 68, "210292": 68, "2103": 155, "2103034": 62, "210319": [78, 79], "210323": 94, "2104": 159, "2107": 158, "21082": 91, "210945": 71, "210991": 68, "211": [68, 71, 74, 75, 95, 159], "211002": [111, 116], "21105": [64, 110, 155, 157], "2112": 98, "211219": 71, "2114026": 94, "211403": 94, "21142": 91, "211534": 80, "211682": 79, "212": [68, 71, 75, 159], "2121": 91, "212280": 71, "212317": 94, "212491": 94, "212516": 71, "21257396e": 126, "212811": 75, "212844": 89, "213": [59, 71, 74, 75, 95, 158, 159], "213199": 111, "21342": 91, "21351": 91, "2138": 74, "2139": 33, "214": [68, 71, 74, 92, 159], "214089": 68, "214440": 69, "214451": 70, "214478": 71, "214732": 78, "214764": 97, "214769": 85, "215": [71, 75, 159], "215103": 79, "215159": 91, "215342": 94, "2155": 91, "21550": 91, "21562": 91, "215958": 143, "216": [67, 71, 74, 75, 159], "216113": 94, "216207": 110, "216215e": 78, "216224": 79, "21624417": 62, "2163": 91, "216344": 94, "21669513e": 126, "2167": 91, "21677223": 126, "216925": 70, "216943": 109, "217": [68, 74, 75, 95, 158, 159], "21716": 91, "2171802": [62, 89], "2172351": 126, "217244": 27, "2175": 91, "217684": 85, "218": [68, 74, 75, 159], "21804": [63, 90], "218223": 88, "218383": 75, "218546": 94, "2189": 91, "218924": 91, "219": [18, 31, 40, 61, 71, 74, 75, 103, 107, 158, 159], "2191274": 62, "219432": 92, "219585": 75, "22": [60, 62, 63, 64, 66, 68, 71, 74, 78, 79, 88, 89, 90, 94, 95, 97, 98, 109, 110, 111, 113, 126, 143, 157, 160], "220": [68, 71, 74, 75, 159], "220171": [111, 114], "220211": 91, "22034249": 126, "220446e": 68, "220568": [111, 114], "220587": 16, "220772": 94, "221": [71, 74, 75, 159], "221127": 78, "2213": 89, "221365": 71, "2214": 89, "2215": 89, "2216": 89, "2217": [62, 89], "2218": 158, "221846": 71, "222": [68, 71, 74, 159], "2222": [60, 62, 77, 111], "222261": 109, "222264e": 79, "222306": 88, "222430": 79, "222432": 70, "22272803e": 126, "222843": 94, "223": [71, 95, 159], "223095": 66, "22319": 68, "22336235": 62, "223418": 68, "223485956098176": [82, 83], "223625": 16, "22375856": 62, "22390": 90, "224": [68, 71, 92, 95, 159], "224073": 74, "2244": 158, "224822": 79, "22485963": 126, "224897": [78, 79], "225": [17, 19, 61, 68, 99, 158, 159], "22505965": 62, "22507006e": 126, "22515044": [111, 114], "225175": 94, "22528": 91, "225427": 75, "225574": 89, "2256": 91, "22562": 91, "225635": 94, "22563538": 94, "225764": 88, "225776": 98, "225899": [111, 116], "225917": 88, "225948": 68, "226": [68, 159], "2264": 61, "226479": 85, "226524": 94, "226598": 89, "2266719": 126, "226834": 69, "226873": 69, "226919": 79, "226938": 83, "227": [68, 71, 103, 107, 159], "227005": 68, "227018": 80, "227018245501943": 80, "227086": 88, "2271071": 36, "227184e": 79, "227567": 109, "2276": 61, "227621e": 79, "2279": 91, "227932e": 90, "228": [68, 71, 159], "228002": 68, "228035": 91, "2281": 91, "228648": 63, "228725": 91, "22881897": 69, "228964": 68, "229": [63, 68, 71, 159], "22913178": 69, "22913613": 69, "22913784": 69, "22914068": 69, "229181": 71, "22925": 91, "229443": 94, "229452": [109, 110, 111], "229472": 90, "2295": [71, 91], "22959": 91, "229726": 91, "22975253": 69, "229759": 110, "2298": 61, "22995629": 69, "229961": [78, 79], "2299864": 126, "229994": [78, 79], "22m": 110, "23": [49, 53, 62, 63, 64, 66, 68, 71, 78, 79, 87, 88, 89, 90, 94, 97, 98, 102, 107, 109, 110, 111, 126, 143, 155, 157, 158, 160], "230": [17, 19, 61, 71, 158, 159], "230009": [82, 83], "230055": [71, 111, 114], "2302": 96, "230203": 71, "230368": 78, "2307": [62, 89, 96, 101], "230737": 71, "2308": 97, "230821": 91, "230956": 67, "231": [10, 103, 107, 159], "231080": 68, "231100": 71, "231165": 111, "23116703": 69, "231310": 94, "231430": 143, "231467": 111, "231734": 111, "231798": 111, "231879": 74, "231937": 70, "231986": 94, "231e": 95, "232": [71, 159], "232134": [78, 79], "232277": 79, "232395": 71, "232485": 79, "232497e": 78, "232774": 78, "232959": [82, 83], "232e": 111, "233": [34, 71, 159], "2331": 91, "233154": 160, "233415": 91, "2335": 61, "23368": 91, "23391813": 69, "234": [126, 158, 159], "23404391": [111, 114], "234137": 98, "234153": 98, "234169": 71, "234534": 80, "234605": 72, "234910": 89, "235": [68, 71, 158, 159], "235080": 71, "2351982": 126, "235250": 68, "23549489": [111, 114], "2359": 160, "236": [68, 71, 159], "236008": 80, "23611": 91, "236391": 71, "236411": 94, "23690345e": 126, "236966": 68, "237": [64, 71, 159], "237114": 71, "237158": 79, "237164": 79, "237461": 97, "23751359e": 126, "237756": 74, "238": [62, 89, 159], "238012": 94, "23801203": 94, "238101": 94, "238213": 143, "238251": 80, "238279": 92, "238287": 78, "238416": 78, "238449": 79, "23856": 91, "238619": 75, "238664": 70, "238718": 79, "239": [68, 71, 159], "239019": 85, "239352": 91, "2394222": 126, "239503": 78, "23968": 91, "239845": 91, "23e": 63, "24": [41, 62, 63, 64, 68, 71, 78, 79, 88, 89, 90, 94, 97, 98, 99, 109, 110, 111, 126, 143, 157, 158, 159, 160], "240": [71, 88], "240053": 79, "240127": [78, 79], "240194": 78, "240295": 97, "2403": 96, "240495": 109, "240532": [78, 79], "240601": [111, 114], "240693": 109, "2407": 61, "24080030a4d": 64, "240813": 84, "240944": 69, "241": 159, "241049": 94, "241064": 79, "241237": 70, "241474e": 79, "241503": 109, "2416": 61, "241841": 93, "241962": 98, "241973": 91, "241e": 95, "242": [158, 159], "242124": [90, 91], "242139": 143, "242158": [90, 91], "242351": 68, "2424": 85, "242427": 85, "242559": 66, "242815": 143, "242864": 91, "242902": 94, "243": [59, 91, 92, 159], "243056": 70, "2430561": 61, "243107": 71, "243162": 68, "243200": 71, "243246": 94, "243447": 68, "243457": 93, "2438": 91, "243e": 95, "244": [91, 159], "244275": 68, "244455": 94, "2445": 91, "244622": 143, "244667": 71, "24469564": 157, "245": [158, 159], "245062": 94, "2451": 61, "24510393": 63, "245370": 89, "245512": 94, "245552": 79, "245610": 79, "245720": 67, "246": 159, "246374": 71, "2467506": 62, "246753": 94, "2468": 91, "246879": 94, "247": [95, 159], "247020": 80, "247057e": 94, "247129": 70, "247158": 71, "2472": 91, "247617": 109, "24774": [90, 91], "247826": 89, "248": [91, 159], "248147": 68, "248171": 94, "248178": 78, "248441": 109, "248487": 88, "248638": 80, "2489": 74, "249": [62, 89, 92, 159], "249067": 79, "2491": 91, "249100": 89, "24917": 91, "249185": 71, "249306": 88, "249494": 78, "249556": 68, "249601": [111, 112, 116], "2499": [69, 104, 107], "25": [17, 18, 19, 29, 30, 31, 35, 40, 41, 42, 43, 44, 62, 63, 64, 67, 68, 71, 78, 79, 80, 81, 88, 89, 90, 91, 94, 98, 109, 110, 111, 126, 143, 157, 160], "250": [92, 159], "2500": [69, 91, 104, 107, 111, 116], "25000000000000006": [80, 91, 94], "250083": 91, "2501": 91, "250210": 80, "250341": [111, 114], "250425": 80, "250432": 71, "250460": 68, "250529": 88, "2506": 96, "250674": 71, "250762": 68, "250838": 88, "251": [90, 91, 97, 159], "251152": 91, "251638": 68, "251724": 68, "251818834": 126, "252": [91, 159], "252133": 91, "252253": 97, "252524": 94, "252601": 143, "252849": 79, "252998": 68, "253": [68, 159], "253026": [78, 79], "253610": 16, "253675": 90, "253724": 94, "25374": 91, "253828": 74, "254": [91, 159], "25401679": 62, "254038": 83, "25422089": 71, "254225": 71, "2543": 91, "254324": 80, "254400": 143, "254427": 71, "254589": 70, "254858": 74, "254957": 68, "255": [71, 91, 159], "255122": 68, "255151e": 94, "256": [71, 91, 110, 159], "256082": 109, "256216": 68, "256352": 79, "256416": 94, "256429": 71, "25643": 91, "256567": 89, "25672": 91, "256944": 94, "25699646": 126, "257": 159, "257037": 91, "257193": 71, "257207": 62, "257259": 71, "257377": 67, "257467": 71, "2575": [111, 114], "257762": 94, "25791277": 69, "258": 159, "258158": [78, 79], "258174": 71, "2584": 91, "25855": 66, "2587": 69, "25877436": 69, "25897329": 69, "259": 159, "259184": 74, "259230": 79, "259395": 84, "2594": [63, 90], "259828": [78, 79], "259886": 71, "259960": 68, "25x_3": 67, "26": [62, 63, 64, 66, 68, 71, 72, 74, 78, 79, 88, 89, 90, 102, 107, 109, 110, 111, 126, 143, 157], "2600293": 126, "26016": 91, "260161": 28, "260211": [78, 79], "260328": 78, "260356": 90, "260360": 94, "260738": [111, 116], "260762": 75, "261": 95, "2610": 91, "261140": 79, "26125255": 69, "261366": 78, "261624": [90, 91], "261635": 109, "261685": 91, "261686": 88, "261903": 89, "2619317": 62, "2620": 91, "262083": 88, "262204": 91, "262261": 78, "262513e": 71, "262829": 126, "262839": 68, "263": [10, 91, 159], "2633": 91, "263893": 71, "264": [158, 159], "26405769": 71, "264073": 68, "264086": 67, "264255": 16, "264381": 71, "264426": 88, "265": 159, "2651": 111, "265119": 93, "2652": [64, 90, 91], "265431": 71, "265547": 91, "265562": 78, "2658": 83, "265888": 74, "265929": 95, "266": 159, "266147": 95, "2663": 74, "266633": 78, "266724": 91, "266909": 143, "267": [68, 92], "267055e": 78, "2670691": 62, "267164": 88, "267500": 89, "267581": 91, "267950": 94, "268": [68, 159], "26837250": 126, "26857892": 126, "268850e": 79, "268942": 94, "268977": 91, "268998": 63, "269043": 94, "269112": 111, "269264": 71, "269425e": 78, "269713": 71, "26bd56a6": 64, "26e": 63, "27": [17, 18, 19, 35, 40, 60, 62, 63, 64, 65, 66, 68, 71, 72, 78, 79, 88, 89, 90, 92, 102, 107, 109, 110, 111, 126, 143, 157, 158, 160], "270": 159, "2700": 64, "270000e": 91, "270248": [111, 116], "270292": 68, "270311": 16, "270644": [78, 79], "27066": 91, "270669": 68, "2708052": 126, "271": 159, "271004": [90, 91], "271183": 90, "2714838731": 89, "271556": [111, 112], "271648": 68, "271775": 68, "271867": 89, "272073": 68, "272300": 71, "272384": 79, "272505": 88, "272610": [111, 114], "272643": 91, "273": 64, "273208": 91, "273237": 68, "273356": 80, "2736074": 126, "27371": [63, 90], "27372": [63, 90], "2739": [67, 68, 71, 95, 98], "274": [64, 91], "274071": 71, "274076": 71, "2740991": 61, "274231e": 71, "274251e": 90, "27429763": [111, 113], "274483": 68, "27461519": [111, 116], "27472": 91, "274793": 94, "274825": 29, "2754": 61, "275452": 71, "275454": 79, "2755": 69, "275538": [103, 107], "275596": 143, "275658": 68, "275831": 79, "276": [64, 159], "2760": 91, "276002": 74, "276148": 94, "276189e": 89, "2762": 91, "276218": 68, "276311": 91, "2766091": 63, "276863": 70, "27695": 91, "277299": 72, "277626": 109, "277808": 78, "277824": 71, "2779": 69, "27794072": [111, 114], "277987": 88, "278": [82, 97, 159], "2780": 62, "278000": 89, "278035": 75, "278391": 91, "278434": 82, "278454": 85, "278543": 78, "2786": 143, "278705": 70, "278907": 94, "279": 159, "279140": 74, "27927": 91, "27951256e": 126, "279524": 94, "279595": 75, "279824": 71, "27991": 91, "279926": 79, "28": [62, 63, 64, 68, 70, 71, 78, 79, 81, 88, 89, 90, 109, 110, 111, 126, 143, 157, 159, 160], "280122": 78, "280196": 83, "280357": 71, "280454dd": 64, "280501": 143, "280766": 79, "280963": 93, "281": [68, 95, 159], "281024": 94, "28111364": 63, "281360": 16, "2815": 91, "281677": 71, "2818": 61, "2819": 143, "281908": 78, "282": [95, 158, 159], "282151": 68, "282200": 83, "2825": [155, 157], "2830": [155, 157], "283168e": 78, "28326": 91, "2834": 126, "283521": 69, "2836": 61, "2836059": 62, "28367": 91, "2838056": 126, "2838546": [111, 114], "283e": 95, "284": 159, "284073": 94, "28425026": 97, "284271": 84, "284360": 68, "284397": 160, "28452": [63, 90], "2849": 91, "284947": 71, "284958": 71, "284987": 91, "285": [59, 95, 111, 159], "285001": 75, "285160": 89, "285276": 94, "285483": 85, "285e": 95, "286": 159, "2861": 74, "286203": 78, "2865": [61, 91], "286507": 80, "286563e": 91, "287": 159, "287041": 94, "287123": 109, "287648": 68, "287815": 97, "287926": 94, "288": [92, 159], "288105e": 78, "288107": 70, "288788e": 79, "288836": 68, "289": [158, 159], "289006": 74, "289062": [90, 91], "289081": 94, "289170": 89, "289198": 79, "289354": 71, "289440": [78, 79], "289536": 78, "289549": 74, "289555": 85, "289706": 78, "289868": 68, "289983": 88, "29": [62, 63, 64, 68, 71, 74, 78, 79, 88, 89, 90, 97, 109, 110, 111, 126, 143, 157, 160], "290": 111, "290112e": 79, "290507": 79, "290901": 75, "290987": 90, "291": [95, 159], "2910": 91, "291011": [12, 111, 113], "291063": 68, "291071": 94, "29107127": 94, "29121721313336434753586568697375788297": 126, "29121721313336434753586568697375788297131016202223243840424649525766677084907111415253541515455565962717274777986914681318283245485060616480818992939596": 126, "291342": 79, "291406": 94, "291500e": [90, 91], "291517": [78, 79], "291554": 68, "29168951": [111, 116], "291963": 94, "292": [93, 159], "2920": 91, "292028": 80, "292046": 143, "2921": 74, "292105": 94, "292179": 109, "2925": 64, "292652e": 79, "292696": 71, "292997": 94, "29299726": 94, "293218": 94, "29356613": 71, "293669": 91, "294": 159, "294067": [78, 79], "294123": 109, "2942919": 71, "2945": 74, "294642": 68, "295": [158, 159], "295508": 71, "295590": 78, "295783": 71, "295837": [72, 102, 107, 157], "2958370000000100000100": [64, 102, 107, 157], "2958370001000010011100": [64, 102, 107, 157], "2958371000000010010100": [64, 102, 107, 157], "295855642811191": 80, "295856": 80, "295868": 91, "296099": 75, "296141": 71, "296171": 78, "29636264": 71, "296428": 68, "296523": 78, "296729": 89, "29675887": 94, "296759": 94, "29678199": [100, 127], "297": 159, "297102": 78, "297152": 68, "297204": 78, "297276": [111, 116], "297287": [78, 79], "297349": [82, 83], "29755975": 71, "297682": 94, "2977": 91, "29784405": 97, "297901": 68, "2979472333": 126, "298": [34, 64, 92], "298069": 78, "298088": 79, "298120": 80, "298132": 78, "298150": 88, "298158": 68, "298235e": 91, "298327": 75, "298336": 68, "298629": 109, "298995": 109, "299": [64, 95], "299372": 111, "299391": 71, "299537": 83, "299755": 88, "299865": 71, "2999": 75, "29999": 68, "2_": [36, 65, 99, 144, 145, 154], "2_x": [36, 65, 99], "2d": [13, 127, 136], "2dx_5": [80, 94], "2e": [59, 61, 62, 63, 64, 65, 110, 111, 127, 143, 157], "2f": [74, 84], "2m": [144, 150, 154], "2n_t": 67, "2x": 94, "2x_0": [32, 78, 79, 82, 83], "2x_4": 67, "3": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 44, 47, 48, 49, 52, 53, 54, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 116, 117, 126, 127, 143, 144, 150, 155, 156, 157, 158, 159], "30": [32, 59, 60, 62, 64, 65, 66, 68, 70, 71, 74, 75, 76, 77, 78, 79, 80, 87, 88, 89, 90, 91, 94, 95, 109, 110, 111, 126, 143, 157, 160], "300": [60, 77, 80, 91, 94, 101, 158], "3000": 75, "30000": 68, "30000000000000004": [80, 91, 94], "3002": 74, "30031116e": 126, "300431": 68, "300842": 68, "30093956": 97, "3009931": 126, "301": 64, "301366": [71, 143], "301371": 94, "301414": 68, "30154758": 126, "301579": 71, "3016": 90, "301629": 78, "30175": 91, "30186": 91, "302062": 78, "30227357": [111, 114], "30229388": 94, "302294": 94, "302552": 78, "302648": 89, "302777": 71, "303324": 89, "303835": 89, "303f00f0bd62": 64, "3041": 126, "304130": 94, "304159": 94, "304201": 67, "3044": 74, "30451227": 126, "304821": 71, "305095": 71, "305142": 88, "305272": 66, "30529": 91, "305341": 94, "305612": 89, "305687": 94, "305775": 94, "305811": 109, "305b": 64, "306023": 68, "306282e": 79, "30628774": 94, "306288": 94, "306416": 111, "30645": 91, "30672815": 62, "306915": 89, "306963": 94, "307098": 79, "307244e": 71, "307407": 94, "307444": 88, "307923": 16, "308": 91, "308239": 91, "30829018": 71, "308415": 16, "3086009": 126, "3087": 74, "308837": 79, "308939": 68, "309": 160, "30917769": [82, 83], "309464": 91, "309772": 89, "309952": 74, "31": [62, 63, 64, 65, 68, 70, 71, 74, 78, 79, 88, 89, 90, 109, 110, 111, 126, 143, 157, 160], "310097": 78, "311": 95, "311375": 79, "311458": 68, "311594": 78, "311740": 88, "311807": 78, "311869": 109, "311888": 68, "312002": 68, "312030": 91, "312172": 82, "312206": 71, "3125": 91, "312652": 95, "312769e": 79, "312873": 78, "313044": 143, "313154": 71, "313209": 80, "313494": 70, "313535": 94, "313657": 71, "31378": 64, "313796": 79, "313870": 85, "314": 126, "3141": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 64, 65, 72, 89, 100, 102, 107, 109, 110, 111, 127, 143, 157], "314247": 98, "314309": 78, "3146": 30, "314625": 79, "31473337": 68, "31476": [90, 91], "314781": 82, "315": 159, "315031": 98, "315155": 79, "315242": 68, "315290": [82, 83], "315403": 68, "316": [64, 159], "316193": 94, "316197": 74, "31633": 91, "316717": [78, 79], "317394": 67, "317487": 94, "317605": 68, "317607": 94, "317973": 68, "318": [64, 159], "318471e": 68, "318571": 143, "318753": [82, 83], "319": [64, 159], "319100": [82, 83], "31910229": [111, 116], "319348": 68, "319420": 85, "319466": 71, "319559": 74, "319579": 71, "319634": 91, "319750": 16, "319759": 94, "319850": 94, "319903": 66, "319965": 78, "32": [39, 62, 63, 64, 68, 70, 71, 78, 79, 88, 89, 90, 91, 99, 109, 110, 111, 113, 126, 127, 143, 157], "320": 91, "320000e": 91, "320314": 90, "320346": 69, "320415": 71, "320633": 80, "320941": 71, "321": 159, "321673": 143, "322": 92, "322404": 97, "322736": 37, "322764": 79, "322991": 78, "3231700783": 126, "3235": 91, "323636": 90, "32366503": [111, 114], "323679": 89, "324": [63, 159], "32405727": 71, "32458367": 62, "3245837": 94, "324863e": 71, "324910": 74, "325046": 91, "325056": 94, "325460533": 126, "325619": 71, "325657": 78, "325819": 91, "326": 95, "326105": 71, "326740": 94, "32674263": 70, "32682793": 68, "326832": 68, "326871": 98, "3268714482135149": 98, "327": [91, 159], "327121": 16, "327578": 79, "32789": 91, "327958": 75, "327997": 78, "328367": 91, "32875335": 70, "328884": 78, "32950022e": 126, "329671": 78, "329967": 79, "33": [62, 63, 64, 68, 71, 78, 79, 89, 90, 91, 109, 110, 111, 126, 143, 157, 158], "330": 159, "3300": [63, 90], "330000e": 91, "330285": [78, 79], "3304269": 62, "330615": 94, "33065771": 94, "330658": 94, "330731": 29, "330881": 71, "331": 159, "331161": 79, "331215": 88, "331521": 94, "331602": 91, "33168943": 91, "33175566": 94, "331756": 94, "331913": 82, "332": 159, "332782": 29, "332860": 71, "33287555": 71, "3329": 91, "332996": 89, "333000": 78, "333250": 91, "3333": [60, 62, 77, 109, 110, 111], "33331093": 71, "3333333": 64, "33333333": [68, 70, 71], "33335939e": 126, "3334": [74, 91], "333581": 90, "333882": 78, "333947": 79, "333973": 79, "334": 63, "3341": 74, "334750": 80, "33483549": 126, "335280": 68, "335446": 75, "335486": 71, "335609e": 94, "335846": 94, "335869": 109, "3359": 74, "336": 159, "33613": [111, 112], "336461": 91, "336498": 94, "336612": 67, "336716": 78, "336870": 69, "337": 159, "3371": 91, "3376": 61, "337986": 79, "338": [92, 97, 159], "33849": 91, "3386": 126, "338775": 80, "338855": 91, "338890": 79, "3389": 74, "338908": 80, "339136": 71, "339177": 74, "339268": 94, "339269": 97, "339553": 79, "339570": 94, "339762e": 91, "339856": 68, "339875": [82, 83], "34": [60, 61, 62, 63, 64, 65, 68, 70, 71, 78, 79, 89, 90, 96, 97, 109, 110, 111, 126, 143, 160], "340": [63, 91], "340041": 68, "340142": 111, "340217": 69, "340235": 85, "340274": 97, "340712": 91, "340844": 38, "340882": 68, "341105": 71, "341106": 78, "341122": 68, "341336": 27, "342117": 85, "3422": 91, "342214e": 79, "342362": 75, "342469": 71, "3426318": 71, "342669": 68, "342675": 62, "34287815": 97, "342992": 89, "343": [95, 111, 159], "343051": 71, "343087": 68, "343639": 109, "34375": 90, "343809": 68, "344": 160, "344212": 160, "344368": 78, "344440": 109, "344463": 79, "34450402": 70, "344505": [90, 91], "344579": [111, 114], "344640": 94, "344748": 93, "344787": [78, 79], "344834": 67, "344845": 79, "345": 159, "3454": 91, "345903": 94, "346107": 90, "346206": 94, "346238": 97, "346274": 78, "346678": 93, "346683": 80, "3466832975109777": 80, "346916": 92, "347213": 89, "347216": 71, "347310": 29, "347501": 87, "347574": 71, "347770e": 79, "347793": 79, "347840": 71, "348": 95, "348063": 68, "34827795": 69, "348338": 91, "348617": 94, "348622": 95, "348697": 80, "3486970271639334": 80, "34887111": 69, "349213": 70, "3492131": 61, "349260": 71, "34941962": 69, "349461": 79, "34965172": 69, "34967621": 62, "349772": 83, "34980811": 86, "349867": 79, "349900e": 91, "34m": 110, "34mglmnet": 110, "34mmlr3": 110, "34mmlr3learner": 110, "34mmlr3pipelin": 110, "34mranger": 110, "34mrpart": 110, "35": [63, 64, 68, 71, 78, 79, 80, 82, 89, 90, 91, 94, 109, 110, 111, 126, 143, 144, 150, 160], "3500000000000001": [80, 91, 94], "350103": 68, "350165": 110, "350323": 91, "350491": 78, "350518": 94, "350533": 94, "35053317": 94, "350606": 68, "350712": [82, 83], "35077502": [144, 150], "35155": 91, "352": [63, 89, 159], "352246": 94, "352250e": [90, 91], "3522697": 62, "352481": 71, "3525037508": 126, "35292": 91, "353748e": 94, "3538": 61, "354": [91, 159], "354188": 67, "354371": 94, "354529": 79, "355": 159, "355065": 75, "35554764": 126, "355564": 79, "355627": 109, "355736": 92, "35595272": 69, "35596048": 69, "35596539": 69, "35596563": 69, "356": 159, "356136e": 91, "356167": 83, "35620768e": 126, "356478": 71, "3566": 91, "356609": 68, "35674871": 71, "3568": 111, "356821": 109, "357": [91, 159], "357189e": 68, "35731523": [111, 113], "357450": 78, "357586986897548": 66, "357654": 91, "357700": 68, "358": 159, "358158": [90, 160], "358289": 89, "358395": 97, "3584387": 68, "358441": 70, "358690e": 91, "35871235": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "358787": 143, "359": [95, 159, 160], "359050": 79, "359229": 75, "3593": [86, 97], "359307": 79, "359622": 74, "3598": 91, "359806": 68, "35th": 158, "36": [63, 64, 68, 70, 71, 74, 78, 79, 89, 90, 109, 110, 111, 126, 143], "360": 159, "360004": 94, "360054": 143, "360249": 84, "360393": 71, "360475": [78, 79], "360531": 71, "360683": 80, "360801": 80, "360965": [103, 107], "361": [95, 159], "361220": 109, "361374": [111, 114], "361623": 85, "361764": 68, "3619201": 33, "36213": 91, "36231307e": 126, "362398": 25, "362760": 91, "363": 159, "363103": 80, "3631031251500065": 80, "363215": 78, "363276": 62, "363576": 88, "363750": 71, "363771": 78, "363990": 78, "364087": 68, "364276": 88, "3643": 143, "364340": [111, 114], "364595": 62, "36465191": 71, "3647": 64, "364800": 94, "365": 159, "365509e": 78, "365527": 91, "365540": 68, "36557195e": 126, "36566025e": 126, "365702": 78, "366188": [103, 107], "36621434": 71, "366285": 68, "366718627": 62, "366912": 78, "36696349": [111, 116], "367": [95, 159], "367017": 88, "367225": 78, "367323": 94, "367366": 85, "367571": 80, "367625": 94, "3676311": 126, "367697": 79, "368": [68, 70, 71, 91], "368152": 89, "3682": [63, 90, 91], "368324": 89, "368491": 71, "368577": 109, "36874147": 71, "368918": 68, "369": 159, "369212e": 79, "369556": 80, "3696": 97, "369796": 94, "369869": 90, "369981": 89, "36m": 110, "37": [63, 70, 74, 78, 79, 85, 86, 89, 90, 92, 109, 110, 111, 126, 143], "370": 159, "370000e": 91, "370165": 91, "370254e": 90, "37027632": 126, "3702770": 62, "370736": 89, "3707775": 62, "370779": 68, "371": 159, "3711": 74, "3711317415516624": 80, "371132": 80, "3712": 91, "371357": [90, 91], "371429": 80, "37191227": [111, 114], "371972": 26, "372": 158, "37200": [90, 91], "372020": 68, "372097": 80, "3722": 91, "3724": 91, "372555": 79, "3727679": 62, "373": 59, "373034e": 38, "373235": 91, "373451": 109, "373566": 68, "373783": 78, "373808": 68, "3738573": 62, "373864": 71, "373941e": 78, "374335": 94, "37433503": 94, "3745": 91, "374812e": 91, "374945": 68, "375081": 91, "375151": 78, "375327969": 126, "375403": 66, "375405": 68, "375465": 94, "375776": 78, "375995": 88, "376399": [111, 114], "376575": 70, "3766": 85, "376617": 85, "376681": 79, "376972": 79, "377147": 109, "377246": 91, "377311": 94, "377483": 68, "377572": 79, "377955": 94, "378161": 69, "378367": 94, "378471": 78, "378596": 89, "378834": 94, "3788859": 62, "379": [126, 158], "3790258": 71, "379256": 71, "379435": 71, "379614": 94, "379892": 78, "38": [64, 74, 78, 79, 90, 109, 110, 111, 126, 143], "3800694": 62, "380442": 78, "380538": 74, "380695": 74, "380837": [90, 91], "380878": 71, "381": 95, "381072": 94, "381247": 91, "381278": 91, "381684e": 90, "381685e": 91, "381689": 94, "382024": [103, 107], "382285": 91, "382872": 80, "382919": 16, "383132": 68, "383297": 94, "38348": 91, "384": 91, "384189": 88, "384208": 88, "384232": 74, "384677": 75, "38470495": [111, 116], "384717": 78, "384755": 79, "3848": 91, "38481985": 70, "38484114": 71, "384883": 91, "385093": 92, "385226": 143, "385604e": 92, "38564295": 71, "385917": 89, "386": [64, 91], "386102": 80, "386428e": 79, "386502": 91, "386616": 79, "386831": 75, "387": 64, "3871": 61, "387118": 71, "387187": 71, "387426": 94, "387673": 78, "387780": 94, "388005": 91, "388071": 94, "388144": 68, "388185": 75, "38818693": 126, "388216e": 110, "388735": 71, "38898864": 94, "388989": 94, "389": 64, "389083": 68, "389186": 111, "389489": [111, 112], "389603": 78, "38973512e": 126, "38983785": 66, "389891": 68, "38990574": 126, "39": [59, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 74, 75, 78, 79, 84, 86, 87, 89, 90, 91, 97, 98, 99, 109, 110, 111, 126, 143], "390": 59, "39007": 30, "39010121e": 126, "390155": 88, "390304": [111, 114], "390312": 79, "390379": 94, "390459": 71, "390495": 79, "3909762": 126, "391362": 68, "391377": 98, "391558": 71, "391617": 71, "39186467": 86, "392021": 68, "392242": 84, "392284": 68, "392378": 68, "392472": 91, "39255938": [111, 114], "392623": 79, "392801": 78, "392833": 97, "392864e": [90, 91], "393441": 79, "393604": 80, "393654": 75, "3937": 74, "39425708": 62, "394592": 68, "3946540": 126, "394757": 88, "3949027": 126, "395034": 71, "395076e": 91, "395195": 89, "395268": 109, "395558": 78, "395627": 71, "3958": 111, "396": 111, "3961": 91, "39611477": 63, "39621961e": 126, "396272": 88, "396531": 91, "396671": 78, "39668281": 70, "396835e": 79, "396985": 89, "396992": [78, 79], "397140": 80, "3972086": 126, "39727": 91, "397313": 61, "39738997": 70, "397578": 84, "3977": 74, "397811": 97, "3979": 70, "398": [102, 107, 157], "398000e": 91, "398166": 75, "398367": 74, "3985": 91, "39855494": 71, "398770": 94, "398999": 24, "399": 63, "399056": 94, "399223": 67, "399355": 67, "39963581": 71, "399679": 111, "399692": 94, "399858": 98, "39m": 110, "3cd0": 64, "3dx_1": [80, 94], "3e1c": 64, "3ec2": 64, "3f5d93": 92, "3x_": 94, "3x_4": [80, 94], "4": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 36, 37, 38, 39, 40, 41, 47, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 144, 150, 157, 159], "40": [62, 65, 66, 78, 79, 80, 90, 91, 94, 99, 102, 107, 109, 110, 111, 126, 143, 144, 150], "400": 89, "4000": [68, 71], "4000000000000001": 110, "40000000000000013": [80, 91, 94], "400113": 85, "400120": 68, "40029364": [144, 150], "400297": 79, "400587": 88, "400823": 94, "400905": 75, "400926": 68, "401": [10, 160], "401009": 88, "401041": 71, "4011211": 126, "401247": [39, 127, 143], "40127723e": 126, "401931": [82, 83], "402": [102, 107], "4020": 74, "402077": 91, "402100": 143, "402301e": 110, "402855": 71, "40291172": 126, "403": 95, "403113": 80, "403113188429014": 80, "403425": 94, "40359107": 66, "40361837": 70, "403626490670169": 100, "4036264906701690": 100, "403626491": 100, "403771948": 127, "4039": 61, "40418854": 70, "404300": 75, "404318": 61, "40448586": 70, "4044862": 126, "40452": 91, "404550": 93, "404703": 71, "404824": 91, "404834": 111, "405000": 91, "40513047": 126, "405203": 67, "405313": 78, "405516": 68, "40583": 61, "405890": 29, "405892": 78, "406085": 109, "4061618": 126, "406285": 94, "406446": 80, "4065173": 126, "406565": 78, "406756": 79, "40676": 61, "406785": 68, "407": [59, 95], "40732": 85, "40743212": 70, "407600": 71, "407758": 16, "407786e": 78, "408000": 79, "408135": 78, "40834": 126, "408476": [144, 150], "40847623": [144, 150], "408539": 94, "408565": 94, "408682": 78, "409154": 61, "4093": 97, "40938351": 70, "409390": 91, "409395": 94, "409532": 78, "409551": 79, "409746": 80, "409848": [78, 79], "41": [78, 79, 90, 91, 109, 110, 111, 126, 143], "410095e": 79, "4103": 74, "410393": 80, "410681": 67, "410795": 89, "41093655": 126, "411190": [78, 79], "411295": 94, "411304": [78, 79], "4115592390": 126, "411582": 94, "412": 91, "412127": 94, "412139": 70, "412143": 70, "412304": 98, "412406": 91, "41244486": 71, "412477": 67, "412653": 89, "412714": 80, "412726": 68, "412864": 82, "412919": 79, "412947": 78, "41295506": 71, "413": 91, "41336": 110, "413376": 111, "41341040": 62, "413784": 91, "413962": 68, "414": 95, "414088": 68, "41428195": [111, 114], "414379": 68, "415040": 79, "41525168e": 126, "415294": 94, "4153764": 126, "415556": 109, "41566": 111, "415812": 160, "41594139": 70, "416052": 75, "4166": 91, "4166667": 64, "41668361": 71, "416709": 79, "416737": 88, "416899": 78, "416e": 95, "417619": 79, "417669": 91, "4176787": 70, "417727": 90, "417767": [82, 83], "417834": 75, "417859": 78, "41798768e": 126, "417988": 94, "418052": 78, "418056": 94, "41805621": 94, "418400": 85, "418591": 91, "418632": 71, "418713": 126, "418741": 75, "418806e": 80, "418969": 109, "41918406e": 126, "41928800": 126, "419371": 94, "419454": 71, "419871": 75, "41989983e": 126, "4199952": 62, "41e5": 64, "42": [12, 15, 16, 35, 37, 65, 66, 67, 68, 70, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 90, 91, 93, 94, 97, 98, 99, 104, 107, 109, 110, 111, 112, 113, 114, 116, 126, 143, 158], "420059": 70, "42022599": 71, "420308e": 91, "42073312": 62, "42094064": [111, 116], "420967": 80, "421083": 61, "4211349413": 62, "421200": 98, "4213207": 70, "421357": [82, 83], "42143": 91, "421576e": 91, "421793": 97, "4218": 74, "421919": 91, "422007": 97, "42205144": 70, "422116": 91, "422119": 91, "422251": 79, "422325": 80, "42245598": 71, "42258461": 71, "422646e": 79, "42273206": 71, "422984": 16, "423115": 92, "4232": 74, "4234": 74, "4235": 74, "4235839": [82, 83], "423941": 70, "423951": 61, "42402765": 70, "424108": 80, "424127": 126, "42412729": 62, "4242": 87, "424328": 94, "424476": 71, "42450794": 70, "424510": 71, "424573": 88, "424651": 111, "424682": 78, "4247": 74, "424717": 80, "424748": 98, "424879": 91, "425": 89, "425000e": 91, "425103": 61, "42518562": 71, "425325": 85, "425414": [103, 107], "425493": 61, "42550": 91, "425636": 85, "4257457": 126, "426055": 61, "426258": 71, "426286": 68, "42631205": 70, "426540": 89, "426540301": 62, "4266481": 71, "426734": 88, "426736": 91, "427": 91, "427486": [78, 79], "42755087": 97, "427551": 97, "427573": 89, "4276": 74, "427654": 85, "427725": 94, "427800": 71, "427937": 68, "428": [143, 160], "428046": 93, "42811700": 160, "428126": 71, "428191": 78, "428228": 71, "428255": 94, "428343659": 126, "428411": [90, 91], "42845482": 70, "428467": 94, "4284675": 94, "42850302": 71, "4286": 74, "428766": 78, "428771": 29, "428779": 79, "428833": 71, "4290": 61, "42904446": 71, "429133": 88, "429141e": 78, "429298": 88, "429309": 91, "42934105": 99, "42ba": 64, "43": [63, 70, 75, 78, 79, 91, 109, 110, 111, 126, 143], "4302": 74, "430298e": [90, 91], "430345": 88, "430465": [111, 116], "4311947070055128": 110, "431306": 94, "4313371": 126, "431437": 97, "431605": 78, "431701914": 155, "431747": 79, "431929": 91, "431998": 75, "432130e": 90, "432300e": 94, "43231359e": 126, "432707": 95, "432718e": 68, "43294": 64, "432f": 64, "433": [64, 95], "4330": 91, "433221": 80, "433473": 91, "433509": 74, "43361991": 94, "433620": 94, "433630": [111, 114], "433684e": 78, "4339": 61, "434054e": 85, "434116": 78, "4344": 74, "434535": 94, "43453524": 94, "4346360": 126, "434698e": 78, "4348": 74, "43482": 91, "435": 64, "43503345": 111, "435401": 89, "4357": 91, "435927": 91, "435967": 89, "436": [64, 91], "436327": 91, "436719": 68, "436764": 95, "436806": 91, "437": 59, "437134": 91, "437601": 79, "437667": 90, "437924": 91, "438": 89, "438219": 94, "438569": 91, "438657": 71, "43883": 83, "438960": 89, "4391": 74, "439273": 71, "439294e": 68, "439541": [90, 91], "439561": 71, "439675": 95, "439699": 75, "43989": 24, "43f0": 64, "44": [70, 75, 78, 79, 109, 110, 111, 113, 126, 143], "4401": 74, "440170": 78, "440320": 91, "440450": 68, "440524": 78, "440605": 110, "440790": 74, "440a": 64, "441004": 91, "441153": 94, "441166": 71, "441209": 94, "44124313": 111, "441311": 82, "44147655": 70, "441528": 78, "4416552": 62, "441658": 71, "4417": 74, "441910": 68, "4420": 74, "442608": 68, "442755e": 71, "442847": [111, 112], "44297621": 71, "443016": 80, "443032": 90, "44312177": 63, "443264": 69, "44334102": 126, "443347": 71, "44349": 126, "443672": [111, 116], "443686": 94, "4438": 91, "4439612": 126, "443e": 95, "444046": 91, "44412537": 111, "4444": [60, 62, 77, 111], "444500": [90, 91], "444805": 88, "445": 95, "4450": 74, "44500831": 71, "4455": 74, "44563945e": 126, "445816": 78, "4460": 74, "446023": 93, "4462": 64, "44647451": 97, "446722": 68, "446745": 71, "44713577e": 126, "447624": [78, 79], "447706": 80, "447863": 80, "447863428811719": 80, "447897": 78, "448": 91, "448025": 71, "448070": 79, "4481": 74, "448587": 80, "4487": 91, "448745": 94, "44890536": 126, "448923": 84, "449150": 29, "449260": 68, "449345": 79, "449677": 85, "44fa97767be8": 64, "45": [66, 78, 79, 80, 84, 88, 91, 94, 109, 110, 111, 126, 143], "4500": 90, "45000000000000007": [80, 91, 94, 110], "450000e": 91, "450097": 79, "450152": 89, "4505648": [111, 114], "4506": 74, "450870601": 62, "450952": 70, "4512": 69, "4513": 74, "452": 64, "452091": 91, "452114": 109, "452488701": 62, "452489": 89, "452622": 78, "45278292": 126, "452974": 71, "453": 64, "4530506": 126, "453069": 71, "453243": 68, "4535": 91, "4536": 91, "453729": 68, "4539": 64, "454129": 16, "454406": 75, "45451": 91, "454525": 68, "45467447": 126, "454991": 74, "455": 64, "455078": 80, "455107": 80, "4552": 64, "455262": 79, "455293": 80, "4552b8af": 64, "455448": 97, "455564": 16, "455672": 91, "45594267": [111, 114], "455981": 144, "456225": 94, "456370": 89, "456453": 16, "4566031": 143, "45660310": 143, "456617": 94, "4567": 97, "4567272": 126, "4568": 74, "456837": 126, "456892": 80, "457088": 94, "458114": 91, "45814003": 70, "458196": [111, 114], "458307": 145, "4584": 91, "458420": 91, "4584447": 62, "458712": 70, "458814": 85, "458855": 63, "458862": 71, "459098": 78, "4592": 62, "459200": 89, "459383": 80, "45957837": 126, "459617": 16, "459760": 91, "459812": 80, "46": [70, 74, 78, 79, 84, 109, 110, 111, 126, 143], "4601": 91, "460207": [78, 79], "460218": 80, "460289": 94, "460385": 78, "460744": 90, "4608454": 126, "4610": 160, "4611": 74, "461162e": 79, "461396": 88, "461412": 109, "461469": 69, "461493": 88, "461629": 98, "461640": 71, "462319": 91, "462451": 80, "462473": 79, "463046": 79, "463149": 68, "463208": 91, "46328686": [111, 114], "463325": 94, "4634": 91, "463418": 98, "4637": 74, "46372631": 126, "463766": 83, "463803": 79, "463816": [111, 116], "463b": 64, "464076": 80, "464282": 89, "46448122": 71, "46448227": 126, "464537e": 78, "464668": 27, "4649": 74, "464936": 68, "465": [75, 111], "46507214": 97, "4652": 74, "465649": 98, "465730": 98, "4659651": 100, "465965114589023": 100, "4659651145890230": 100, "465981": 94, "46598119": 94, "466047": 94, "46618738": 126, "4663": 74, "466440": 80, "466684": 91, "466756": 94, "46709481": 126, "46722576e": 126, "467613": 89, "467613401": 62, "467681": [78, 79], "467770": 80, "468001": 91, "468035e": 78, "468072": 79, "468075": 94, "46807543": 94, "468086": 71, "4681318283245485060616480818992939596": 126, "468406": 91, "468449": [111, 116], "468869": 79, "468907": 75, "468d": 64, "469": 64, "469170": 91, "469379": 91, "469474": 98, "469676": 75, "469825": 80, "469935e": 68, "47": [63, 74, 75, 78, 79, 82, 90, 92, 97, 109, 110, 111, 126, 143, 159], "470": 59, "470023": 79, "470365": 91, "470801": 78, "470828": 78, "471": 75, "471370": 68, "471435": 95, "471454": [103, 107], "471666": [103, 107], "471752": 70, "472255": 91, "472315": 68, "472891": 94, "472952": 79, "472e": 64, "473036": 109, "473099": 80, "473143": 71, "47319": 111, "474058e": 68, "47419634": 157, "474214": [82, 83], "474297": 68, "474658": 71, "474699": 79, "474709": 111, "475147": 71, "475395": 91, "475551": 78, "475901": 68, "475e": 95, "476121": [111, 114], "476304": [111, 114], "4766": 74, "476808": 71, "476856": 80, "4770": 74, "477130": [78, 79], "477150": 94, "4772": 74, "477354": 91, "477357": 95, "477395": 91, "477474": 89, "47759584": 126, "477782": 71, "47782": 91, "477937": 68, "4782": 74, "478230": 71, "478399": 91, "47857478": 126, "478668": 79, "47878685": 71, "478794e": 79, "478896": 91, "479119": 79, "479288": 16, "479527": 79, "47966100e": 126, "479722": 79, "479876": [82, 83], "479882": 79, "479928": 94, "479959": [111, 116], "47be": 64, "48": [64, 75, 78, 79, 83, 90, 91, 109, 110, 111, 126, 143], "480": 75, "480000e": 91, "480133e": 94, "480199": 85, "48029755": 97, "480579": 79, "48069071": [100, 127, 143], "480691": [39, 127, 143], "480800e": 94, "4809": 70, "481172": 94, "481326": 71, "481399": [90, 91], "4814": 86, "481713": 85, "481761e": 91, "482": [64, 75], "482038": 80, "4822313": 71, "482349": 82, "482461": [144, 150], "48246134": [144, 150], "482483": 94, "482508": 88, "482790": 67, "48296": 97, "483": [95, 111], "48315": 97, "483165": 78, "483185": 68, "483186": 67, "483192": [90, 91], "48331": 97, "483357": 79, "483531": 94, "48353114": 94, "483642": 91, "483711": 94, "483717": 80, "483855": 91, "484": 91, "48404": 62, "484074": 79, "484111e": 78, "4842": 91, "484272e": 79, "484640": 94, "484865": 69, "48489102": [111, 114], "4849": 64, "484996": 71, "485": [64, 91], "485112951": 126, "485357": 71, "485368": 79, "485468": [103, 107], "48550": 98, "485617": [90, 91], "48583": [90, 91], "485871": 83, "486": [42, 91], "486202": 80, "486342": 91, "486532": 94, "487": [75, 91], "487204": 70, "487352": 69, "487467": 91, "487524": 85, "487641e": 94, "487872": 76, "487934": 16, "488455": 88, "488759": 78, "488811": 94, "488866": 78, "488909": [90, 91], "488982e": 80, "489054": 93, "489485": 68, "489488": [111, 116], "4895498": 94, "489550": 94, "489567": 96, "489699": 80, "49": [64, 75, 78, 79, 109, 110, 126, 143], "490070931": 62, "490488e": [90, 91], "490586": 71, "490689": 88, "490896": 75, "490898": 91, "490941": 91, "49098": 97, "491245": 89, "491249": 88, "4915707": [111, 113], "491919e": 71, "492": 91, "492075": 78, "4923156": 100, "49231564722955": 100, "492315647229550": 100, "492410": 94, "492417e": 111, "49245504": 71, "492626": 84, "49270769e": 126, "493": [95, 111, 158], "493024": 69, "4931": 91, "493144": 98, "493195": 85, "493219": 94, "493263": 71, "493426": 95, "493792": 88, "494": 95, "494253e": 78, "494324": 89, "494324401": 62, "494393": 71, "495": 93, "495028": 79, "49530782": 62, "4953902": 126, "495405": 94, "495657": 80, "495752": 94, "49596416e": 126, "496": 93, "496074": 78, "496300e": 91, "4965": 74, "49650883": 97, "496551": 94, "496591": [111, 116], "496714": 98, "496777": 160, "49693": 110, "497": 93, "4972272": 126, "497538": 92, "497915": 78, "497964": 109, "498": [91, 93], "498048": 74, "498122": 85, "498921": 94, "498950": 92, "498979": 91, "498f": 64, "499": [59, 93, 102, 107, 157], "499000e": [90, 91], "499110": 92, "499314": 92, "499416": 79, "4996": 74, "49d4": 64, "4a53": 64, "4b8f": 64, "4dba": 64, "4dd2": 64, "4e": [62, 63], "4ecd": 64, "4fee": 64, "4x": 94, "4x_0": [32, 78, 79, 82, 83], "4x_1": [32, 78, 79], "5": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 54, 55, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 116, 126, 127, 135, 143, 144, 150, 156, 157, 159], "50": [29, 62, 64, 65, 67, 71, 75, 80, 83, 86, 88, 90, 91, 92, 94, 109, 110, 126, 143], "500": [7, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 37, 38, 39, 40, 41, 44, 50, 60, 64, 68, 69, 70, 71, 72, 74, 77, 78, 79, 82, 83, 86, 90, 93, 95, 97, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 116, 127, 143, 144, 150, 157, 160], "5000": [47, 68, 69, 70, 71, 78, 79, 80, 94, 96], "50000": 89, "500000": [90, 91], "5000000000000001": [80, 91, 94], "500000e": 91, "500084": 94, "500242e": 68, "500267": 84, "5003517412": 62, "500635": 91, "500890": 92, "50093148e": 126, "501021": 91, "501050": 92, "5011": 74, "501203": 88, "501403": 94, "502005": 109, "502084": 111, "502268": 91, "502344": 68, "502494": 80, "5025850": 62, "502612": 94, "502880": 68, "502995": 94, "503089": 94, "503226": 79, "503504": 110, "503700": 75, "503768": 79, "503872": 71, "50398782e": 126, "504076": 79, "504286": 89, "5042861": 62, "5050973": 62, "505795": [111, 114], "506080": 74, "50672034": 62, "506736": 109, "506761": 68, "506833": 78, "506900e": 94, "506903": 80, "507": 95, "507285": 85, "507490": 68, "507683": 79, "50768b": 92, "507784": 71, "508068": 78, "508153": 93, "508406": 93, "508433": 78, "508459": 89, "508518": 71, "508756": [103, 107], "5089": 91, "508947": [15, 111, 113], "508988": 78, "509005": 78, "509059": 91, "509102": 91, "509196": 94, "509461": 94, "509651": 88, "50967": 98, "5097": 98, "5098": [72, 102, 107, 157], "509853": 94, "5099": [64, 72, 102, 107, 157], "509951": 80, "509958": 89, "51": [61, 63, 64, 70, 75, 82, 109, 110, 126, 143, 159], "510000e": [90, 91], "510058": 68, "510385": 89, "510555": 75, "510586": 94, "510771": 69, "51079110": 62, "510971": 88, "5115547": 100, "5115547181877": 100, "51155471818770": 100, "511668": 98, "5116683753999616": 98, "511722": 38, "512": 89, "512108": 94, "512149": 94, "51214922": 94, "51243406e": 126, "512519": 89, "512566": 79, "512572": 94, "512657e": 91, "512672": [111, 144, 150], "5131": 90, "513222": 85, "513658": 91, "513992": 94, "514": 64, "514024": 78, "514104": 70, "514179": 78, "514199": 74, "51494845": [111, 116], "514957": 79, "515208": 79, "515358": 80, "5154": 91, "5154782580830215": 89, "5155": 64, "516": 64, "516055": 68, "516125": 80, "516145": 91, "516222": 94, "516255": 94, "516256": 94, "516528": 94, "517": [64, 89], "5175": 91, "517510812139451": 66, "517798": 75, "518175": 89, "518478": 75, "518517": 30, "518610": 111, "518616": 79, "518846": 89, "518854": 75, "518978": 88, "5192627293034373944637683858788949899100": 126, "5192627293034373944637683858788949899100131016202223243840424649525766677084907111415253541515455565962717274777986914681318283245485060616480818992939596": 126, "519262729303437394463768385878894989910029121721313336434753586568697375788297131016202223243840424649525766677084904681318283245485060616480818992939596": 126, "51926272930343739446376838587889498991002912172131333643475358656869737578829713101620222324384042464952576667708490711141525354151545556596271727477798691": 126, "5192627293034373944637683858788949899100291217213133364347535865686973757882977111415253541515455565962717274777986914681318283245485060616480818992939596": 126, "519637": 78, "51966955": 62, "519710": 94, "519828": 79, "519898": 78, "52": [61, 64, 84, 88, 95, 109, 110, 126, 143], "520": 91, "520012": 78, "520096": 88, "520406": 78, "520641": 97, "520930": 80, "521002": 80, "521104": 91, "521233": 75, "521357": 79, "5214067": 126, "521608e": 78, "522398": 88, "522605": 91, "522835": 67, "523030": 98, "523140": 79, "523163": 80, "52343523e": 126, "523794e": 94, "5238": 74, "523977545": 62, "524215": 91, "52424539": 62, "5245763": 126, "524598": 71, "524657": 94, "524817": 88, "524934": [78, 79], "525064": 75, "52510803": 63, "5251546891842586": 98, "525324": 87, "52544332": 111, "5255": 64, "525643": 71, "52590": [63, 90], "526": 89, "526001": 71, "526125": 78, "526316": 71, "526411": 68, "526532": 91, "526582": 85, "526590": 71, "526769": [78, 79], "527141": 78, "52732": 110, "527339": 71, "527644e": 94, "528000e": 97, "528145": 79, "528272e": 79, "528381e": 99, "528725": 94, "528822": 68, "528937": [82, 83], "528996901": 62, "528997": 89, "529": 89, "529178": 78, "529405": 61, "529468": 109, "529692": 79, "529782": 61, "529879": 71, "529940": 71, "53": [61, 64, 75, 84, 89, 102, 107, 109, 110, 111, 113, 126, 143, 155, 158], "5301": 91, "530659": 84, "530830": 91, "530940": 94, "53094017": 94, "530983": 68, "531": 64, "531030": 79, "531223": 80, "531293": 71, "531515": 109, "531594": 91, "531999": 91, "532202e": 30, "532266": 80, "532332": 68, "532421": 82, "53257": 110, "532738": 94, "53273833": 94, "5328": 91, "532910": 71, "533": [95, 160], "533283": 111, "533316": 91, "533489": 67, "5337": 74, "533871": 91, "533900": 94, "534139": 75, "5346": 64, "534759": 78, "535179": 94, "535318": 94, "53606675": 94, "536067": 94, "5362361": 126, "536792": 91, "536798e": [90, 91], "537205": 71, "537438": [111, 112], "537681": 91, "53791422": [99, 111], "538": 64, "5382": 97, "538227": 79, "538282": 91, "53849791": 94, "538498": 94, "538513": [103, 107], "538937": [90, 91], "539455": 94, "539491": [82, 83], "539564": 71, "539767": 80, "539862": 71, "54": [61, 63, 64, 74, 88, 95, 101, 109, 110, 126, 143, 159], "540101": 94, "54010127": 94, "54014493": [111, 114], "540240": 91, "540375": 85, "5405": 85, "540549": 85, "540578": 88, "5408": 61, "541": 95, "541010": 94, "541060": 85, "541068": 71, "541159": 94, "54119805": 66, "54163": 97, "5416844": 100, "541684435562712": 100, "542": 95, "542268": [111, 112, 116], "542446": 83, "542451": 94, "542560": 98, "542647": 94, "542648": 109, "542671": 89, "542754": 79, "542769": 94, "542883": [144, 150], "5428834": [144, 150], "542919": 109, "542983": 71, "542989": 94, "543": [89, 91], "543014": 91, "543052": 75, "543075": 80, "543136": 80, "543380": 89, "5434231": 100, "543423145188043": 100, "54355648": 71, "5436005": 62, "5436876323961168": 80, "543688": 80, "543734": 91, "54373574": [111, 114], "543764": 83, "54378": 97, "544097": 94, "544276": 79, "544383": 98, "54439245": 126, "544555": 89, "544680": 79, "54483": [127, 143], "5448331": [127, 143], "54515": 68, "54517706e": 126, "545492": 75, "54550506": 95, "545605e": 94, "545672": 79, "54591931": 126, "545930": 109, "546223": 78, "546260": 88, "546438": 91, "546928": [111, 114], "546967": 78, "54716": 97, "547257": 79, "547306": 80, "5473064979425033": 80, "5475": 91, "547794": 16, "5479": 91, "547909": 91, "54838185": 71, "5494": 91, "549656": 70, "55": [63, 64, 80, 88, 90, 91, 94, 109, 110, 126, 143], "5500000000000002": [80, 91, 94], "550176": 88, "550472": 79, "550741": 68, "550888": 71, "551010e": 91, "551317": 79, "551594": 70, "551686": 80, "55176": 110, "552081": 88, "552714e": 71, "552727": 89, "552739": 68, "552776": 94, "553004": 75, "55307": 110, "553277": 68, "553537": 68, "553672": 91, "553754": 109, "553878": 28, "554076": 80, "554298": 71, "554348": 70, "554793e": 126, "5548": 74, "554937": [111, 114], "554986": 80, "554986206618521": 80, "555": 89, "5553": 69, "555445": 93, "555498": 94, "5555": [60, 77], "555544": 78, "55557885": 126, "555954": 91, "556191": [78, 79], "556207": 71, "55642562": [111, 114], "556460": 71, "556533e": 91, "556792": 94, "5572": 74, "557217": 68, "5574dcd4": 64, "557595": 89, "557741": 88, "557999": 89, "558134": [78, 79], "55820564": [111, 114], "55828259": 99, "558387": 79, "5584": 89, "5585": 89, "55855122": 66, "55863386": 111, "558655": 80, "5589": 89, "558996": 91, "559": [41, 158, 160], "5590": 89, "559144": 80, "559186": 80, "5592": 89, "559394": 94, "559522": 94, "559680": 91, "55969864": 71, "559712": [103, 107], "559844": 79, "55dc37e31fb1": 64, "55e": 63, "56": [64, 74, 101, 109, 110, 126, 143, 155, 158], "560135": [39, 127, 143], "56018481": 94, "560185": 94, "560250": 79, "560475": 71, "56063445": 71, "560689": 61, "560723": 84, "561309": 91, "5614534": [111, 114], "56148059": 126, "5616": 90, "561652": 79, "561785": [15, 111, 113], "562013": 94, "56223": 97, "562288": 98, "562317e": 78, "562320e": 78, "562390": 109, "562556": 70, "5625561": 61, "562712": [78, 79], "562866": 91, "563067": 95, "56329036": 71, "563374e": 80, "563456": 109, "563503": 94, "563563": 85, "563673": 91, "563690": 91, "563760": 91, "5638": 91, "56387280e": 126, "56390147e": 126, "564": 111, "56403823": [111, 114], "564045": 94, "564073": 91, "564232": [78, 79], "56425415": [111, 114], "564451": 79, "564577": 91, "564610": 78, "564800e": 91, "564829": 80, "565066": 80, "565915": 91, "566": [59, 98], "566024": 94, "56611407": [111, 114], "566600": 94, "56670073": 94, "566701": 94, "566964": 78, "567004": 97, "567491": 79, "567695": 78, "567945": [82, 83], "568071": 91, "568143e": 78, "568287": 85, "56876517": [111, 114], "568932": [111, 116], "569449": 109, "569911": 62, "5699994715": 62, "57": [64, 95, 109, 110, 126, 143, 160], "57015458": [111, 114], "570278": 78, "570283": 71, "570351": 74, "570408": 71, "570486": 61, "570562": 61, "570722": 157, "5708": 91, "570929": 68, "571040": 79, "571429": 80, "5714294154804167": 80, "57162199": 71, "57176619": 71, "571778": 61, "5718": 91, "5722": 90, "57245066": 94, "572451": 94, "572717": 96, "573349e": 79, "573679": 16, "573700": 67, "573983": 68, "574": 64, "574050": 78, "57409103": 71, "574160": 85, "5748": 110, "57496671": 62, "575": 11, "575381": 90, "575423": 16, "57572422": 97, "57585824": 97, "57592948e": 126, "57599221": 97, "575e": 95, "576": 64, "5763996": 62, "57643609": 97, "576879": 89, "577": 64, "5770": 90, "577071": 78, "57715074": 62, "577210e": 79, "577271": 89, "57751232": 91, "577634": 78, "5776971": 97, "57775704": 97, "577807": [78, 79], "577884": 91, "577e": 95, "578081": 91, "578307": 94, "57843836": [111, 114], "57847691": 71, "578557": 79, "578847e": 80, "57894895": 71, "579125": 90, "57914935": 63, "579197": 85, "579213": 98, "579238": 80, "579322e": 90, "579521": 93, "57e": 63, "58": [31, 63, 90, 98, 109, 110, 126, 143, 159], "5800": 91, "58000": 90, "580118": 71, "5804": [64, 91], "580414": 98, "580751": 90, "580831": 79, "580922": 82, "581011": 68, "581880": 88, "582031": 90, "582115": 70, "582290": 79, "582331": 91, "58241568": 126, "5824208": 126, "5825085": [111, 114], "582705e": 78, "582754": 96, "582761": 80, "582803": 78, "582980": 71, "582991": 90, "583076": 91, "583140": 79, "583195": [78, 79], "5833333": 64, "583534": 94, "583969": 78, "584742": 83, "584849": 80, "5852": 91, "585394": [111, 112], "585402": 74, "585412": 78, "585426": 126, "585479": 85, "585513": 79, "5856537": 71, "585793": 80, "586362": 94, "586388": 71, "5864": 61, "586423": 78, "5867": 91, "586734": 68, "5868472": 62, "586962e": 79, "586975e": 71, "587135": 79, "587594": 91, "588": 158, "58812": 110, "5882": 90, "588364": 24, "588396": 80, "5883964619856044": 80, "588481": 68, "588824": 78, "589016": 71, "589406": 71, "59": [68, 79, 109, 110, 111, 126, 143], "590": 91, "590098": 80, "590320": 67, "5905": 90, "590530": [111, 114], "590736": 94, "590738": 94, "590813": 94, "590880": 74, "590911": 80, "590991": 80, "591080": 67, "59163811": 71, "591652": 90, "591678": 90, "591679": 79, "59199423e": 126, "592093": 82, "592681e": 80, "59300411": [111, 114], "59307502e": 126, "5931003": [111, 114], "59334615": 71, "593637": 79, "593648": 110, "593981": 109, "594": 11, "594316e": 94, "595068": 71, "595353": 80, "59563003": [111, 116], "595987": 79, "596": 91, "596076e": 91, "596112e": 68, "596270": [82, 83], "596367": 78, "596588e": 79, "596753": 71, "59693274": 71, "597": 63, "597051": 109, "597923": 91, "59798": 91, "59827652": [111, 114], "598371": 91, "59838552": 71, "59854797": 111, "5985730": 63, "598753": 78, "599297": [109, 110, 111], "599334": [111, 116], "599586e": 88, "5cb31a99b9cc": 64, "5d": [80, 94], "5x_2": 67, "5x_3": 67, "5z_i": 94, "6": [11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 41, 42, 60, 61, 62, 63, 64, 65, 67, 68, 69, 70, 71, 72, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 102, 104, 106, 107, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 144, 150, 155, 157, 158, 159], "60": [62, 65, 66, 74, 80, 91, 92, 94, 99, 109, 110, 126, 143, 158], "600": 89, "6000": 91, "6000000000000002": [80, 91, 94], "600000e": 91, "600254": 93, "600354": 79, "600694": 111, "600776": 75, "601": 63, "60100": 71, "60101": 71, "601061": 80, "601109": 74, "60113395": [111, 114], "601598": 89, "601627": 71, "601652": 68, "601757": 91, "602079": 83, "602168": 80, "602322": 109, "602587": 94, "602628": 80, "602870": 78, "603803": 68, "604110": 91, "6043": 74, "604532": 91, "60458433": [111, 114], "604727": 68, "60481177": [111, 114], "604841": [90, 91], "60488374": [111, 114], "605": 91, "605195": 83, "606034": 94, "606118": 68, "606129": 94, "60622839": 69, "606342": 80, "606727": 78, "606733": 91, "6068": 62, "606800": 89, "60681061": 69, "606954": 80, "607": 92, "607422": 68, "607482": 71, "6075": 160, "60753412": [111, 114], "607618": 94, "60791012": 126, "60795263": [111, 114], "608": [81, 95], "60800174": 69, "608177": 74, "60857": 61, "608609": 71, "60876023": 69, "608818": 97, "6088517": 69, "60885843": 69, "60885893": 69, "60887382": 69, "609": 95, "609575": [12, 111, 113], "6098112": 71, "609947": 74, "61": [24, 68, 75, 109, 110, 126, 143, 159], "610053e": 79, "610093e": 78, "610318": 75, "611": 160, "611269": 89, "61170069": 95, "611859": 83, "611995": 91, "612": 95, "612073": 79, "612246": 85, "612328e": 78, "612792": 91, "61312417": 71, "61317195": 71, "6133": 63, "613391": 79, "613408": 94, "613498": 91, "61357": 111, "613574": 84, "613691": [12, 111, 113], "613950": 80, "614": 95, "614188": 89, "61458483": 111, "614716": 79, "615": 75, "615108": 71, "61554814": 71, "61583045": 71, "615863": [82, 83], "616086": 91, "616116": 91, "616346": 79, "616372": 90, "61669761": [144, 150], "616698": [144, 150], "616828": 91, "617": 59, "617059": 71, "617277": 109, "6173": 64, "61736113": 71, "617481": 91, "61771229": 95, "617780": 109, "6180": 74, "618069": 90, "61810738": 63, "618279": 71, "618377": 68, "618625": 78, "618722": 91, "618753": 91, "61881": 92, "618978": 71, "619": 95, "619294": 75, "619351": [78, 79], "619390": [78, 79], "619454": 67, "619613": 90, "619743": 68, "61e": [63, 160], "62": [75, 83, 84, 109, 110, 126, 143], "620021": 79, "620026": 94, "620135": 79, "620156": 94, "620206": 71, "620267": 79, "620874e": 111, "620995": 99, "621060": 74, "62108072": [111, 114], "621094": [90, 91], "621097e": 79, "621275": 93, "621359": 109, "621490": 94, "6215": 90, "621953": 94, "62195343": 94, "622": [91, 95], "622153": 91, "622272": 75, "6224": 62, "622502": 88, "623024": 80, "623107": 78, "6232": 74, "624": 89, "6240": 97, "6241": 91, "624206": 82, "624320": 16, "6243811": 62, "624535": 110, "624798": 90, "624894": 109, "624919": 91, "625": [62, 89], "625002": 91, "625159": 84, "625165": 93, "625342": 71, "625477": 94, "625888": 74, "625891": [82, 83], "626433": 94, "626530e": 78, "626765": 91, "626866": 79, "627319": 88, "627505": [82, 83], "627560": 94, "627564": 80, "627874": 79, "628": 95, "628069": 89, "628213": 16, "628483": 71, "628834": 91, "629319": 91, "629835": 91, "629e": 95, "63": [62, 75, 89, 109, 110, 126, 143, 158, 159], "630150e": 94, "630793": 71, "630880": 95, "630914": 84, "631113": 79, "63117311": 111, "631179": 79, "631333": 94, "631761": 71, "631762": 79, "6318": [90, 160], "632058": 89, "63245862e": 126, "632747e": 94, "6330631": 143, "633433": 89, "634": 95, "63407762": 160, "634078": [90, 160], "634493": 68, "634587": 143, "6349": 74, "63499": 91, "635000e": [90, 91], "635039": 71, "635199": [90, 91], "635346": 71, "635757": 91, "635900": 78, "63593298": [99, 111], "636048": 111, "636453": 27, "636766": 71, "637108": 80, "637240": 91, "637326": 94, "6379": 90, "63817859": 94, "638179": 94, "638264": 94, "638488": 84, "638794": 71, "639": 90, "639135": 89, "63916605": 63, "639437": 68, "639585": 79, "64": [75, 83, 90, 91, 95, 109, 110, 126, 143, 157], "640": 91, "640314": 79, "640432": 78, "641528": 94, "641938": 68, "641973": 71, "642": 95, "642016": 94, "642096": 93, "642114": 69, "642329": 75, "642648": [111, 112], "64269": 97, "6427": 91, "642768": 79, "643016": 71, "64340": 97, "64349465": 71, "643512": 80, "643623": 94, "64362337": 94, "643679": [111, 116], "643752": 94, "64385073": 71, "643885": 68, "643939": 75, "643988": 78, "644113": 109, "644609": 71, "644665": 80, "64476745e": 126, "644799": 67, "644963": 79, "645": 91, "645166": 71, "64533682": 71, "645583": 75, "64579": 61, "6458": 62, "645800": 89, "646419": 79, "646937": 67, "646963": 30, "647": 111, "64703482": 71, "64718797": 71, "647196": 67, "64723": 97, "647679": 79, "647689": 98, "647750": 91, "647864": 109, "647873": 94, "64797": 97, "648": 90, "648000": 71, "648000e": 91, "648094": 78, "648820e": 109, "649": [111, 158], "649158": 94, "649177": 74, "649335": 78, "649969": 91, "65": [75, 80, 84, 86, 91, 94, 95, 109, 110, 126, 143], "650": 81, "6500000000000001": [80, 91, 94], "650046": 111, "6502": 91, "650867": 80, "650893": 79, "651662": 91, "6519": 69, "651919": 94, "6522": 158, "652235": 70, "652324": 75, "652350": 89, "652450e": [90, 91], "652460": 70, "652526": 82, "6527": 81, "652778": 89, "652940": 79, "653008e": 90, "653094": 78, "653424": 82, "653829": 85, "653846": 80, "653900": 109, "653901": [78, 79], "654070e": 111, "654191": 74, "654755": 67, "654964": 88, "655284": 94, "6553": 160, "6554": 158, "655547": 78, "65557405e": 126, "655812": 68, "655959": 85, "656939": 79, "657": [64, 111], "657283": 97, "657638": 69, "658021": [111, 116], "658267": 94, "658345": 79, "6586": 61, "658612": [103, 107], "658769": 78, "659": 64, "659245": [78, 79], "659387": 70, "6593871": 61, "659423": [78, 79], "659473": 98, "659755": 95, "659799": [103, 107], "66": [75, 85, 92, 109, 110, 126, 143, 157, 159], "660": [59, 64, 111], "660128": 80, "660320": 83, "660360": 79, "660432": 68, "660479": [15, 111, 113], "660701": 68, "6607402": 95, "660776": 94, "660788": [111, 114], "661145": 91, "661166": 37, "66133": 111, "661338e": 91, "661521": 78, "6617520": 126, "66228034": 70, "662347": 79, "6625": 91, "66295923": 70, "663177": 75, "663182": 80, "66335493": 70, "663357": 79, "6634357241067574": 98, "663529": 94, "663533": 91, "663648": 71, "663672": 85, "663851": 71, "663859": 78, "66426329": 71, "66460138": 71, "664846": 89, "6651166": 71, "665264": 94, "665602": 88, "66564938": 68, "665653": [111, 116], "66573787": 68, "665868e": 91, "665974": 88, "66601815": [111, 113], "666104": 94, "666220": 78, "666307": 67, "666530": 78, "6666667": 64, "666742": 109, "666750": 91, "66687094": 68, "66688811": 71, "666959e": 85, "667": 89, "66703949": 71, "667285944503316": 80, "667286": 80, "667372": 68, "667536": 94, "66764304": 68, "667682": 66, "667985": 84, "668446": 84, "668584": 67, "669130": 88, "669562": 82, "66970003": 126, "669733": 79, "669836": 71, "669919": 79, "67": [59, 64, 74, 90, 92, 98, 109, 110, 126, 143, 157], "670051e": 78, "67022327": 68, "670867": 29, "67095958": [111, 114], "671168": 109, "671271": [78, 79], "671382": 78, "671420": 88, "671633": 91, "671717": 79, "671888": 92, "67199": 91, "67199985": 126, "6722": [64, 74], "672234": [78, 79], "672266": 78, "672384": [78, 79], "67245350": 62, "673092": [78, 79], "673302": 89, "6734628878523097": 80, "673463": 80, "673581815999206": 80, "673582": 80, "67410934": 62, "674181": 91, "674507e": 78, "6745349414": 62, "67456": 98, "674609": 80, "674782": 69, "675011": 80, "675011328023803": 80, "675489": 78, "675528": 79, "675653": [111, 112], "675892": 71, "676093e": 82, "67618978": 66, "676242": 79, "676405": 80, "6765": [63, 90], "676536": 143, "676714": 78, "67674909": 66, "676756": 94, "676807": 90, "677614": 94, "677779": 71, "677980": 80, "678": 95, "678104": 74, "678117": 91, "678540": 74, "678826": 80, "678830": 68, "678953": 74, "6792263": 70, "67936506": [111, 113], "679486": 74, "679539": 89, "67962965": 70, "679684": 69, "67ad635a": 64, "68": [64, 75, 92, 96, 97, 109, 110, 126, 143], "680": 91, "680366": 91, "680677": [111, 114], "6807": 74, "68093375": 70, "681": 89, "6810775": 97, "681176": 89, "681261": 78, "681817dcfcda": 64, "682315": 91, "6826": [38, 90], "682821": 68, "682830": 79, "683331": 80, "683487": 79, "683582": 111, "683637e": 85, "683785": 79, "683942": 94, "684": 160, "68410364": 63, "68411700": [63, 160], "684502": 94, "684657": 79, "684849e": 71, "685066": 78, "685107": 94, "685143": 88, "68554404e": 126, "68562150e": 126, "685807": 94, "685816": 78, "685970": 78, "685989": 111, "686684": 79, "686694": 79, "686698": 78, "686755": 69, "6870": 68, "687227e": 79, "687345": 94, "687647": 94, "687854": 67, "687871": 89, "6878711": 62, "688": 158, "688232": 71, "688537": 109, "688540": 109, "688886": 109, "688918": 91, "689072": [111, 114], "689088": [78, 79], "689188": 67, "689392": 94, "689542": [103, 107], "689967": 79, "69": [84, 109, 110, 126, 143, 159], "690068": 71, "69031384": 68, "69039393": 70, "690536": 91, "69081477": 70, "690968": 78, "69100983": 70, "69140475e": 126, "691511": 90, "691761": 91, "691938": 68, "6921": 74, "692478": 70, "692538": 79, "6925552878": 126, "692725": 94, "692768": 78, "692907": 91, "6930": 74, "693345": 91, "693359": 91, "6934117220290754": 80, "693412": 80, "693495e": 91, "693796": 89, "693837": 74, "693874": 78, "694154": 80, "694561": 95, "694755": 91, "694919": 89, "69505403": 111, "69507002": 126, "69520523": 66, "6955": 91, "695581": 84, "695582": 88, "69562150e": 126, "69572427": [111, 116], "695860": 78, "696011": 28, "696289": [82, 83], "696694": 68, "696826e": 79, "69684828": 111, "697": 89, "697000": 80, "697118": 94, "697420": [82, 83], "697591": 78, "697900": 79, "698223": 67, "698244": 67, "698302": 78, "69840389e": 126, "698450": 16, "69851736": 68, "698642": [111, 116], "698694": 89, "69887403": 68, "699035": 94, "699082": 80, "699097": 74, "69921": 64, "699259e": 94, "699333": 80, "6994544": 71, "6_design_1a": 81, "6_r2d_0": 81, "6_r2y_0": 81, "6b": 143, "6cea": 64, "7": [12, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 40, 41, 44, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 104, 106, 107, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 144, 150, 157, 158, 159], "70": [63, 65, 80, 90, 91, 94, 109, 110, 126, 143, 159], "700": [78, 79, 81, 89], "7000000000000002": [80, 91, 94], "700015": 94, "70007159": 94, "700072": 94, "700102": 94, "700314": 75, "700596": 94, "700643": [111, 114], "701088": 90, "701106": 84, "70122789": 68, "701413": 91, "701491": 82, "701672e": 80, "701841e": 83, "70211191": 68, "702119e": 79, "702500": 91, "702632": 79, "703049": 78, "70305686": [111, 116], "703148": 68, "70331796": 68, "703325": 85, "70344386": [111, 116], "703802": 71, "704": [59, 92], "7040": 91, "704039": 79, "704482": 85, "7045": 85, "704558": 85, "704881": 79, "704896": 85, "705170": 78, "70557077": [111, 116], "705581": 91, "7055958": 100, "705595810371231": 100, "7055958103712310": 100, "705620": 78, "705682": 78, "705798": 79, "705801": 79, "70583": 97, "705981": 79, "706173": 79, "706208": 91, "706385": 78, "706645": 80, "707101": 80, "70774361": [111, 116], "707868": 94, "707963e": 90, "708062": 79, "708154": 79, "708190": 89, "708459": 94, "708532": 78, "708762": 79, "708821": 75, "708837": 75, "708934": 78, "709": 92, "709026": 67, "709593": 78, "709606": 29, "71": [109, 110, 126, 143, 159], "710059": 75, "710295": 68, "71047894": 68, "711051": 78, "711141525354151545556596271727477798691": 126, "711284": 71, "711328": 91, "711518": 91, "712065": 82, "712072": 93, "712095": 75, "712209e": 79, "71228684": 68, "712503": 97, "712592": 90, "712631": 78, "712846": 109, "712921": 91, "712960": 80, "713": 91, "713137": 79, "7134": 68, "713582": 88, "713719": 78, "714240": 89, "714261": 79, "7142787": 94, "714279": 94, "714321": 90, "715013": 91, "71502339": 68, "715179e": 91, "715255": 69, "715407": 80, "7155": 91, "715596": 78, "7157": 91, "715764": 16, "7158581": 62, "716": 91, "716098": 75, "7161": 91, "716456": 94, "716615": 75, "716740": 109, "716762": 80, "716793": 80, "716799": 89, "7167991": 62, "717185": 94, "718294": 66, "718686": 98, "7193": 91, "719831": 79, "72": [65, 85, 109, 110, 126, 143, 159], "72017126": 68, "720208": 78, "720409": 78, "7204309": [111, 116], "720447": 74, "720571": 94, "720589": 95, "720664": 89, "721071": 94, "721097": 16, "721118": 88, "721243": 78, "7215093d9089": 64, "72155839e": 126, "721604": 79, "721609": 91, "722": 89, "72228103": [111, 114], "722316": 94, "7224": 91, "722634": 94, "72269685": [111, 116], "722848": 80, "723": 64, "723314": 94, "723342": 109, "723345e": 94, "723414e": 79, "723689": 91, "723846": 75, "7239": 91, "723958": 78, "7241399": 62, "724338": 94, "724375e": 79, "724498": 74, "724661": 69, "724767": [82, 83], "724918": 98, "725": 64, "725010": 75, "725031": 94, "72504367": 68, "725087": 91, "725142": 78, "725166": 94, "725232": 109, "725477": 78, "725687": 71, "725992": 78, "726": [64, 95], "726078": 74, "7268131": 62, "72745489": 71, "727501": 91, "727543": 67, "727693": 91, "727720": 71, "727780": 91, "727976": 80, "7282094": [111, 113], "728478": 79, "728612": 68, "728710": 94, "728718": 71, "72875815e": 126, "728799": 69, "728903": 68, "728e": 95, "729668": 109, "729807": 92, "72987186": [111, 116], "73": [63, 68, 75, 86, 109, 110, 126, 143], "730011e": 78, "730629": 88, "730773": 78, "7308": 61, "730823": 78, "731754": 80, "731928": 88, "732": 95, "732162": 79, "732307": 78, "7326": 91, "732845": 78, "73285": 27, "732983": 78, "734069": 68, "734332": 78, "7344": 74, "734948": 94, "735426": 78, "735587": 74, "735694": 94, "73569431": 94, "735710": 71, "735713": 79, "735848": 109, "735964": 67, "736082": [78, 79], "736770e": 78, "737210": 79, "737243": 68, "73750654": [111, 114], "7375615": 63, "73764317e": 126, "737796": 91, "737884": 25, "737951": [78, 79], "738147": 91, "738315": 91, "738422": 79, "738936": 79, "739": 91, "739089": [111, 116], "739125e": 78, "739182": 78, "739261": 79, "7393": 74, "739595": 95, "739660": 78, "739720": 91, "739817": 84, "74": [31, 63, 79, 90, 109, 110, 126, 143, 159], "740": 89, "740180e": 94, "740195": 68, "740200": 78, "740339": 74, "740417": 90, "7405": 91, "740505": 75, "7407627053044026": 80, "740763": 80, "740869": 80, "741043": 78, "74105858": 68, "741104": 80, "741380": 95, "741523": 75, "741702": 94, "7418": 61, "74189": 64, "742128": 94, "742365": 93, "7424": 69, "742407": 93, "742695": 78, "742907": 94, "743": 92, "7432": 61, "7437": 91, "74402577": 94, "744026": 94, "744125": 79, "744211": 91, "744236": 97, "74461783e": 126, "74475816": [111, 116], "745353": 78, "745638": 90, "746361": 94, "746843": 83, "747008": 78, "7470442": 126, "747057": 74, "747078": 70, "747164": 94, "747305": 68, "747945": 62, "747977": 79, "748284": 88, "748377": 90, "748513": 91, "748945": 91, "74938952": [111, 113], "749854893": 127, "75": [18, 29, 31, 64, 67, 75, 80, 85, 90, 91, 94, 109, 110, 126, 143, 159], "75000": 98, "7500000000000002": [80, 91, 94], "750275": 91, "750701": 75, "751013": 91, "751081": 91, "751194": 74, "751281": 79, "751372": 78, "751633": 91, "75171": 90, "751710": [80, 90], "751712655588833": 100, "7517126555888330": 100, "751712656": 100, "752522": 78, "752696": 75, "752871": [111, 114], "752909": 85, "753136": 78, "75329595": [111, 114], "7533": 90, "753516": 79, "753523": 94, "753728": 71, "753880": 79, "754503": 78, "754692": 109, "7548": 98, "754870": 89, "755885": 109, "7560824": 62, "756127": 92, "756200": 75, "756222": 69, "756337": 78, "756539": 74, "756585519864526": 80, "756586": 80, "756805": 89, "756867e": 91, "756969": 80, "757": [95, 158], "757136": 91, "757151": [78, 79], "757380": 68, "757559": 85, "757596": 80, "757663": 82, "757690": 94, "757819": 89, "758027": 91, "758340": 74, "758695": [111, 114], "758754": 79, "758852": 79, "75887": 64, "759373": 79, "76": [109, 110, 126, 143, 158, 159], "760104": 94, "760155": 91, "7603": 61, "760386": [12, 111, 113], "760403": 79, "760778": 89, "760915": 67, "761": [62, 89], "761224": 75, "761317": 74, "76169605": 71, "761714": 80, "762049": 74, "762284": 94, "76228406": 94, "762299": [111, 116], "7628744": [111, 114], "7631": 69, "763124": 69, "763327": 71, "764093": [78, 79], "76419024e": 126, "764315": 94, "76444177e": 126, "764859": 66, "764953": 90, "76535102": [111, 116], "765363": [78, 79], "765500e": [90, 91], "765557": 78, "7656": 91, "765710e": 99, "765772": 68, "765792": 94, "76591188": 62, "7660": 61, "766005": 94, "76608187": 69, "766418": 71, "766478": 92, "766499": 94, "766585": 91, "766693": 69, "7669": 91, "766940": 75, "766965": 70, "76702611e": 126, "767027": 74, "767188": [82, 83], "767435": 98, "767486": 79, "768273": [82, 83], "768319": 71, "768798": 75, "76883587": 71, "768874": 79, "769063": 91, "769361": 94, "769569": 71, "769805": 94, "77": [95, 109, 110, 126, 143], "770556": 91, "77056222": 68, "7706685": 126, "770944": [82, 83], "7710": 97, "771167": 143, "7712": 74, "771275": 91, "771383e": 91, "7714": 92, "771529": 88, "7716982": 63, "771876": 91, "771924": 68, "771938": 68, "771965": 91, "771986": 66, "772157": 85, "77227783e": 126, "772373": 78, "772444": 109, "772612": 94, "77261209": 94, "772791": 91, "77289874e": 126, "773": 64, "773160e": 71, "773177": 80, "77329414": [111, 114], "773339": 85, "773787": 71, "77401500e": 126, "774365": 79, "774683e": 89, "774915": 78, "775": [64, 91], "77515244": 71, "775191": [78, 79], "775969": 97, "776": 92, "7763": 90, "776601": 78, "776778": 69, "776887": 90, "777297": 26, "77746575": [111, 116], "7776071": 62, "777728": 24, "777867": 85, "777e": 95, "778305": 78, "7786": 61, "778952": 71, "779": [95, 111], "779068": 85, "779349": 78, "779517": [78, 79], "779682": 80, "779727e": 91, "78": [82, 95, 109, 110, 126, 143, 159], "780": 64, "780458": 94, "780697e": 71, "780856": 90, "780943": [79, 94], "781076": 91, "7811": 69, "781681": 94, "782": 64, "782159": 91, "7824": 68, "782447": 92, "782643": 94, "783": [64, 91], "783183": 79, "783276": 111, "7833": 61, "7838": 61, "78386025": [111, 116], "784": 143, "784057": 70, "784238": 89, "784405": 97, "784841": 91, "784872": 75, "784910": 79, "785": 64, "785107": 80, "785815": 75, "785911": 94, "785979": 88, "786": 64, "786191": 84, "786429": 78, "786744": 80, "786986": 75, "78711285e": 126, "787695": [111, 114], "78777": 97, "787795": 16, "787838e": 78, "788": 158, "78818": 64, "788267": 88, "788385": 109, "788896": 79, "788972": 78, "789": 111, "789355e": 97, "79": [75, 109, 110, 126, 159], "790": 90, "790000e": 91, "790115": 91, "790142": 88, "790261": 109, "790456": 92, "790723": [82, 83], "79081948": 71, "790885": 79, "7910908091075142": 80, "791091": 80, "791097": 90, "791241": 94, "791297": 29, "79164735": [111, 114], "7919965": [111, 114], "792396": 75, "792939": 80, "792972": 109, "793297e": 91, "79330022": [111, 116], "793315": 109, "79338596e": 126, "793570": 94, "793735": 94, "793818": [78, 79], "794": 111, "794119": 91, "794366": 91, "79458848e": 126, "794755": 91, "794805": 82, "794836": 68, "795": 95, "795008": 79, "795481": 70, "795647": 94, "7958": 69, "795828": 79, "795932": 110, "7963": 91, "796340": 91, "796954": 68, "796e": 95, "797189": [111, 116], "797280": 94, "797323": 89, "797410": 78, "797454": 111, "797617": 16, "797743": 143, "79792890e": 126, "797966": 74, "797971": 143, "797984": 79, "798308": 90, "798388": 79, "798685": 26, "798783": [82, 83], "798862e": 78, "799040": 70, "799403": 94, "79953099": [111, 116], "799698": 78, "799890": 78, "7b428990": 64, "7x": 94, "8": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 51, 54, 61, 62, 63, 64, 65, 67, 68, 69, 70, 71, 72, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 102, 107, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 144, 150, 157, 159, 160], "80": [65, 66, 74, 80, 91, 94, 99, 109, 110, 126, 159], "800": 89, "8000": [36, 65, 99], "8000000000000002": [80, 91, 94], "800000e": 91, "8001": 69, "80025934": 68, "800629": 70, "801130": 91, "801269": 111, "802": 95, "80207425": 71, "802149": 79, "802738": 109, "803300": 78, "803492e": 94, "803856": 68, "803889e": 91, "803908e": 79, "804219": 94, "804284": 97, "804316": 94, "804422e": 68, "804549": 91, "8048": 63, "804828": 94, "805007": 89, "805153e": [90, 91], "8055563": 62, "805714": 78, "805717": 92, "805720": 94, "805789": 78, "8059": 90, "806167": 91, "806220e": 91, "806356": 91, "806532e": 78, "806586": 71, "806732": 84, "806736e": 78, "80696592e": 126, "8070107": 68, "80714504e": 126, "807879": 94, "808": [63, 143], "808246": 90, "808347": 66, "808640": 91, "808860": 79, "809": 91, "809278": 66, "809392": 78, "8095": 92, "809913": [78, 79], "80a8": 64, "81": [62, 78, 81, 84, 109, 110, 126, 159], "810044": 90, "810134": 94, "8102": [61, 90], "810306": 75, "810382": [90, 91], "810564": 78, "810650": 94, "810833": 79, "810916": 91, "811155": 84, "811255": 79, "81128199": 94, "811282": 94, "811458": 90, "8116912": 143, "811763": 78, "811825": 89, "811901": 94, "81190107": 94, "812": 95, "812003": 78, "812028": [111, 112], "812658": 79, "8132463": 62, "813293": 94, "813342": 143, "813682": 91, "813728": 68, "814351": 80, "814433": 92, "814608": 68, "814637": 70, "814717": 80, "814913": 89, "815224": 143, "815226": [15, 111, 113], "815993": 94, "8160": 91, "816131": 78, "816176": 98, "816296e": 78, "816318": 89, "816417": 78, "8172": 68, "817967": [70, 111], "817977": 74, "818029": 91, "818113e": 68, "81827267": 94, "818273": 94, "818289": 94, "81828926": 94, "818380": [78, 79], "81856": 64, "818990e": 79, "819223": 109, "819843": 68, "82": [74, 98, 109, 110, 126, 159], "820": 59, "8202": 63, "820366": 89, "820523": 69, "820553": 69, "8208": 91, "8209": 63, "820963": 75, "820993": 91, "821": 158, "8210": 63, "821009": 91, "821021": 80, "821344": 68, "821566": 94, "821912": 68, "8221": 61, "822289": [90, 160], "82228913": 160, "822482": 80, "8226": 74, "8227": 91, "822822": 80, "823247": 94, "823273": [78, 79], "82329138": 86, "823666": 79, "823772": [111, 114], "824350": [78, 79], "824613": 71, "824701": 80, "824750": 80, "824889": 80, "824961e": 91, "8250": 61, "825617": 89, "825749": 79, "8260": 90, "826065": [78, 79], "826074e": 89, "826426": [15, 111, 113], "826492": 94, "826519": 29, "82666866e": 126, "82684324": 97, "826897": 94, "827093": 78, "827381": 94, "827414": 74, "827735": 94, "827855": 70, "827872e": 71, "827874854703913": 80, "827875": 80, "827938162750831": [82, 83], "828157": 75, "828418": 71, "828618": 85, "828915": [82, 83], "829038": 37, "829543": 80, "8295440": 126, "829764": 109, "829916": 71, "83": [109, 110, 126, 159], "830102": 78, "830120": 79, "830142": 87, "830204": 69, "830273": 75, "830301": 93, "830850": 78, "830854": 68, "830967": 78, "831": 95, "831019": 80, "831167e": 78, "831479": 78, "831740": 70, "831833": [111, 116], "832086": 94, "8325": 111, "8326928": 97, "832693": 97, "832875": 94, "83287529": 94, "832965": 91, "832991": 16, "833018": 91, "833024": 89, "833065": 75, "833096": 91, "833227e": 110, "833563": 88, "833907": 89, "834133": 85, "83433028": 68, "83436056": [111, 114], "834916": [111, 114], "835": 95, "8350": [74, 91], "835239": [111, 116], "835344": 75, "835662": 68, "835750": 85, "835839": 91, "836234": 111, "836813": 69, "837366e": 79, "837699": 79, "838114": 94, "838235": 92, "838322": 78, "83854057": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "838717": 69, "838883": 74, "839038": 68, "84": [64, 84, 95, 109, 110, 111, 126, 159], "840041": 91, "840303": 94, "84030318": 94, "840718": [12, 111, 113], "840836": 94, "840995e": 90, "841": [59, 62, 89], "841132": 90, "8415": 63, "841712": 91, "841847": 91, "841920": 79, "842205": 91, "842262": 111, "842405": 80, "842500": 71, "842625": 89, "842746": 94, "8428": 90, "842853": 94, "8429": 74, "843": 91, "843018": 109, "843296": 91, "843454": 78, "843730": 89, "844026": 109, "844308": 94, "8445": 86, "844549": [82, 83], "844573": 71, "844663": 75, "844673": 143, "844707": 94, "844893": 89, "8452": 74, "845241": 95, "845525": 78, "8461": 74, "846602": 91, "846707": 91, "847093": 71, "847136": 80, "847432": 71, "847595": 28, "847948": 80, "848578": 91, "848734": 78, "848757e": 90, "848868": 80, "848910": 71, "848997": 91, "84930915e": 126, "849427": 109, "849747": 97, "849766": 91, "8497f641": 64, "8498": 91, "85": [34, 80, 84, 86, 91, 94, 99, 109, 110, 126], "8500000000000002": [80, 91, 94], "850038": 75, "850101": 79, "850321": 89, "850575": [78, 79], "850656": 85, "851": 158, "851012": 88, "851100": 75, "8513": 64, "851366": 89, "852": 92, "852016": 94, "852592e": 88, "852916e": 79, "853206": 70, "853528": 68, "85397773": [111, 113], "85402594": 86, "855": 91, "855242": 78, "855780": 94, "855932": 68, "856037": 79, "856404": 83, "856552": 75, "856758": 109, "8571": 61, "857151": 71, "857161": 94, "857372": [111, 114], "857515": 109, "857544": 89, "858072": 68, "858577": 93, "858632": 71, "858635": 88, "858952": 75, "859": 91, "85911521e": 126, "85912862": 143, "859129": [39, 127, 143], "8597": 90, "85c5": 64, "85e": 63, "86": [109, 110, 126, 159], "860": 59, "860378": 91, "860663": 143, "860804": 94, "861019": 75, "861210": [111, 116], "861920": 71, "862043": [82, 83], "862274": 82, "862359": 80, "862559": 71, "863337": 78, "863452": 70, "86359358": 68, "863772": 90, "863786": 69, "863982270": 127, "864": 95, "86415573": 63, "86424193e": 126, "8644": 64, "864404": 91, "864664": 75, "8648": 74, "865101": 74, "865313": 91, "865442": 88, "865562": [78, 79], "865860": [90, 91], "865977": 71, "866102": [78, 79], "86610467": 94, "866105": 94, "866142": 79, "866179899731091": 100, "866179900": 100, "866403": 68, "866579": 91, "866798": 91, "867565": 94, "867775": 79, "8679": 91, "868": [64, 160], "8681": 86, "8686": 69, "86897905": [111, 116], "869": [64, 95], "869020": 80, "869132e": 71, "869301": 109, "869427": 38, "869586": 84, "869617": 109, "869778": 93, "869912e": 79, "87": [63, 78, 84, 89, 109, 110, 126, 159], "870": 70, "8700": 63, "870099": [82, 83], "870187": 79, "870260": 94, "870288": 78, "870332": 94, "870444": 90, "870870": [111, 114], "870943": 79, "871": [64, 92], "871080": 74, "87133195": 68, "871416": 71, "871972": 85, "872": 95, "872354": 94, "872643": 71, "872768": 94, "872852": 94, "87290240e": 126, "873039": 79, "87309461": [111, 116], "873198": 91, "873677": [82, 83], "873848": 70, "87384812361": 61, "87384812362": 61, "87430335": 143, "874303353": 143, "874702": [82, 83], "874721": 78, "8751": 91, "875724": 70, "8759": 91, "876": 95, "876233": 70, "87623301": 61, "876431e": 80, "87674597e": 126, "8768": 61, "8771": 91, "877153": 91, "877287": 16, "877446": 109, "877455": 93, "877833": [78, 79], "877903": 75, "878122": 88, "878281": 94, "8783048": 68, "878402": 78, "878661": 70, "878746": 75, "878779": 68, "878802": 91, "878895": 75, "878914": 78, "879103": 80, "879535": 68, "87e": 63, "88": [41, 63, 84, 95, 109, 126], "880106": 89, "880217": [111, 114], "880579": 94, "880591": 93, "880886": 90, "8810": 90, "881031": 68, "881201": 91, "88125046e": 126, "88140569": 68, "881465": 67, "881567": 79, "88173062": 62, "88173585": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "881937": 75, "882470": 16, "882641": 90, "882896": 94, "882928": 75, "883224": 79, "883233": 92, "883301": 80, "883622": 94, "883914": 80, "8843": 97, "884344e": 71, "8845": 61, "884821": 109, "884996": 80, "8850": 63, "885065": 94, "885513": 91, "885686e": 78, "885832": 95, "88592824": 68, "885978": [82, 83], "886": 59, "88600723": 68, "886074": 71, "886081": 68, "886086": [78, 79], "88629": 61, "886300": 78, "886611": 91, "88664": 64, "886764": 79, "88682216": 68, "887182": [111, 112], "887212": 91, "887270": 79, "887556": 80, "88755682": 68, "887731": 66, "887778": 91, "888146": 89, "8881461": 62, "888217": 91, "888348": 79, "888352": 75, "888423": 91, "888757": 94, "888775": 83, "889": 160, "88904395": 68, "889087": 68, "889261": 78, "889293": 94, "8893": 69, "889403": 71, "889566": 97, "889638": 75, "889733": 94, "88988263e": 126, "889913": [78, 79], "88ad": 64, "89": [63, 79, 109, 126, 158, 159], "890": [62, 89], "890204": [111, 116], "89027368": 143, "890273683": 143, "890342": 78, "89035917": 84, "890372": [72, 102, 107, 157], "8903720000100010000010": [64, 102, 107, 157], "8904": 59, "890454": 110, "890665": 85, "890855": 75, "8909": [62, 90, 160], "891102": 79, "891299": 74, "891527": [111, 116], "891593": 78, "891606": 90, "891649": 25, "891757": 79, "892": [64, 92], "892648": 94, "892796": [78, 79], "892828": 85, "893": 64, "8932105": 62, "893461": 75, "893521": 79, "893649": [78, 79], "893851": 94, "893884": 91, "894": 64, "894329e": 91, "8946549": 65, "894701": 79, "89472978": [111, 116], "895106": [78, 79], "895275": 16, "895308": 91, "895333": 94, "895385": 71, "895442": 90, "895690": [78, 79], "895768e": 80, "895853e": 78, "896023": 94, "896182e": 85, "896263": 85, "896283": 68, "896333": 79, "896505": 71, "896761": 69, "897115": 78, "897220": 94, "8974": 90, "8974226": [111, 114], "8979": 74, "898": 160, "898183": 75, "89851915": 68, "898722": 94, "899021": 109, "899250": 75, "899296": [111, 114], "899460": 94, "899568": 78, "8996": 86, "899654": 75, "8bdee1a1d83d": 64, "8da924c": 64, "8e3aa840": 64, "9": [12, 14, 15, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 102, 107, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 144, 150, 156, 157, 159, 160], "90": [42, 63, 65, 66, 80, 91, 94, 99, 109, 126, 159], "9000000000000002": [80, 91, 94], "900021": 110, "900127": [111, 116], "900829": 85, "901013": 79, "901148": 94, "90136": 90, "901360": 90, "90145324": [111, 116], "901526": 84, "901563": 16, "90172316": 68, "901998": 88, "902": 143, "902219": 78, "902494": 68, "902573": 80, "902920": 109, "903056e": 94, "903339": 80, "903366e": 80, "903418": 89, "903681": 94, "903731": 88, "903767": [78, 79], "903879": 79, "9045": 91, "904900": 78, "904976": 78, "905494": 80, "905612": 74, "905858": 98, "905951": 97, "906": 59, "906051": 80, "906051023766621": 80, "906195": [111, 114], "9067": 91, "906716732639898": [82, 83], "906757": 76, "907115": 94, "907176": 94, "907491": 80, "907801": 89, "907879": 75, "90794478": 143, "907944783": 143, "908024": 98, "908912": 79, "909304": [78, 79], "909571": 75, "90963122e": 126, "909942e": 24, "909997": [90, 160], "91": [96, 109, 126, 159], "91053356": [111, 114], "9109": 64, "91102953": 94, "911030": 94, "91113145": 68, "9112": 90, "911794e": 79, "911818": 79, "912033": 30, "912230": [78, 79], "9126": [63, 160], "9127": [63, 160], "912761": 69, "912801": 79, "912837": 78, "913": 64, "91315015": 62, "913280": 98, "913585": 85, "913774": 80, "914": 126, "9142": 91, "914245": 70, "91438767e": 126, "9145": 61, "914598": 75, "915": [63, 64, 90, 91], "915000e": [90, 91], "915488": [82, 83], "915502": 79, "915947": 69, "916171": 93, "9162": 74, "916236": 61, "916355": 79, "916359": 75, "9166667": 64, "91683613": [111, 114], "916913": 94, "916914": 94, "916921": 91, "916984": 94, "917": 64, "917038": 74, "917143": 68, "917497": 82, "91771387": 94, "917714": 94, "9179": 91, "918": 95, "918044": 78, "918199e": 78, "918227": 80, "918293": [111, 116], "918339": 70, "919879": 91, "91e": 63, "92": [68, 96, 109, 110, 126, 159], "920283": 91, "920337": 83, "920695": 91, "920827": 68, "9209": [61, 74], "9210": 91, "921048": 70, "921061": 98, "921198": 85, "921372": 80, "921577": 68, "921778": 85, "921913": 89, "921956": [78, 79], "921e4f0d": 64, "922232": 78, "922668": 85, "922996": 89, "923": 92, "923033": 79, "923084e": 80, "923517": 99, "923607": 94, "92369755": 62, "923943": 160, "924002": 94, "924061": 78, "924232e": 88, "9243": 91, "924347": 71, "924396": [82, 83], "924443": 75, "924540": 88, "924634": 67, "924724": 80, "924732": 91, "9248": 64, "924821": 80, "924956": 69, "9250": 69, "925248": [82, 83], "925278": 70, "92566": 111, "925736": 80, "925994": 90, "926260": 82, "926621": 80, "926685": 89, "927": 60, "927048": 71, "927074": 94, "927232": 91, "9274": 91, "927501": 79, "927563": 79, "928269": 91, "92827999": 111, "92881435e": 126, "929031": 78, "92905": 62, "929289": 68, "929347": 68, "929607": 91, "92964644": [111, 114], "929699": 79, "92972925e": 143, "929729e": [39, 127, 143], "93": [63, 85, 96, 109, 110, 126, 159], "9304028": 62, "93074943": [111, 116], "931": 101, "93134081": [111, 114], "931479": 94, "931646": 78, "931978": 157, "932027": 80, "932233": 74, "932327": 78, "932366": 70, "932404e": 91, "9325": 61, "932688": 70, "932692": 70, "9327": 61, "932906": 79, "932973": 94, "93300": 111, "933603": 79, "933996": 80, "934122": 79, "934270": 89, "934433": [78, 79], "9345": 64, "934511": 143, "93458": 97, "934634": 79, "934769": 71, "934992": 80, "935": 86, "935220": [111, 112], "935294137": 126, "935591": 94, "935730": 94, "935793": [111, 114], "935932": 68, "935989": 89, "9359891": 62, "935993": 79, "936094": 78, "936238": 79, "936425": 68, "93648": 99, "9365": 74, "936739": 94, "937586": 91, "937904": 71, "937967": 74, "938": 143, "938180": 71, "938263": [111, 116], "938460": 78, "9386744462704798": 88, "9388": 91, "938836": [111, 116], "939068": [82, 83], "939390": 78, "9395": 91, "93958082416": 160, "94": [86, 109, 126, 159, 160], "940893": 71, "941047": 16, "941364": 74, "941440": 78, "941709": 80, "9417090334740552": 80, "942": 92, "942139": 83, "942312": 94, "942460e": 94, "9425": 61, "942550": 91, "942629": 91, "942661": 89, "94309994e": 126, "943548": 85, "943693": [111, 112], "944045e": 94, "944253e": 94, "944266": [82, 83], "94427158": [111, 116], "944280": 91, "944317": 91, "94441007e": 126, "944630": 94, "94473": 65, "945058": 78, "946180": 85, "946227e": 71, "94629": 99, "946297": 80, "946406": 83, "946433": 94, "946968": 80, "947194": 78, "947254e": 71, "947290": 79, "947440": 93, "947466": 110, "947855": 75, "9480": 91, "948112": 95, "948158": 71, "948568": 68, "948828": 78, "948975": 84, "94906344": 62, "949241": 143, "949456": 94, "949459e": 78, "949866": 79, "95": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 50, 61, 63, 65, 66, 67, 68, 69, 70, 71, 74, 75, 78, 79, 80, 81, 82, 83, 84, 85, 90, 91, 94, 95, 97, 98, 99, 109, 111, 126, 143, 144, 150, 159, 160], "9500": 91, "950275": 70, "950280": 94, "950545": 76, "95062986e": 126, "950769": 71, "950833": 71, "951252": 68, "951532": 89, "951604": 79, "951920": 93, "952": [63, 95, 160], "952146": [111, 116], "952221": 68, "9523": 61, "952572": 74, "952738": 68, "952839": 94, "95305": 65, "95311164": [111, 116], "9532": 91, "953683": 89, "953706": 78, "95372559e": 126, "953884": 111, "954": 143, "95401167e": 126, "954738": 79, "9552": 61, "955438": 70, "955483": 68, "955525": 70, "955541": 29, "95559917": 110, "95560878": 68, "95567283": 68, "955689": 78, "955730": 68, "956": 92, "956047": 62, "9561": 61, "956272": 89, "956574": 91, "956588": 111, "95701514": 68, "95708782": 68, "957229": 83, "957277": 71, "957339": 89, "957375": 89, "957417": 91, "957479": 80, "9574793755564219": 80, "957735": 71, "957745": 80, "957820": 68, "9579": 63, "957996": 80, "958": 143, "9580": 63, "958636": 84, "958789": [111, 114], "959164": 71, "959440": 68, "959441": 79, "959613": 109, "95e": 63, "96": [63, 66, 78, 79, 92, 109, 126, 159], "960049": 75, "960236": 111, "960808": 80, "960846": 91, "9609": 61, "961018": 79, "96128545": 68, "961538": 91, "961962": 80, "962022": 79, "962041": 79, "962228": 79, "962364": 75, "962485": 68, "9625": 91, "962948": 74, "963403": 69, "964025e": 94, "964261e": 89, "964317": 91, "9647": 61, "964914": 16, "964940": 79, "965": 66, "965376": 78, "965531": 111, "96582": 110, "966015": 94, "966566": 79, "966610": 69, "966626": 71, "967467": 97, "967502": 71, "967533": 70, "96779417": 68, "968127": 75, "968134e": 94, "968288": 91, "968305e": 78, "969141": [109, 110, 111], "969405": 68, "969471": 69, "969702563412915": 80, "969703": 80, "9699": 90, "97": [12, 15, 16, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 157, 159, 160], "970058": 70, "970065": 94, "97049128": 68, "970652": 79, "971058": [82, 83], "971132": 91, "971876": 71, "971967": [111, 114], "972051": 91, "972405": 78, "97314470": 62, "973241": 94, "973293": 79, "973874": 80, "974023": 78, "97405109": 68, "974104": 79, "974202": 80, "974213": 79, "97441062": [82, 83], "974414": 80, "974416": 79, "974491": 74, "97470872": 97, "974797e": 68, "9748910611": 62, "974894": 71, "974925": 79, "97499195": 94, "974992": 94, "975": [78, 79, 82, 83, 85, 86, 92, 111, 114], "975252": 69, "975289": 75, "9753": 64, "975375": 68, "976088": 94, "97619643": [111, 116], "97622458": 68, "976398": 79, "976562": 94, "976791e": 79, "976935": 68, "977113": 79, "977280": [78, 79], "977295": 91, "9773": 91, "977489": 91, "978000e": 91, "978052": 79, "978410": 71, "978438e": 79, "978446": 78, "9787": 91, "978977": 94, "979": 95, "979135": 109, "979384": 85, "979409e": 79, "979966": 85, "98": [78, 79, 109, 126, 159], "98023747": [111, 114], "9802393": 62, "980407": 79, "980443": 71, "980574": 78, "980643e": 80, "980771": 78, "980838": 78, "981104": 93, "981339": 79, "981601": 71, "981622": 78, "981672": 80, "981703": 78, "982134": 70, "982353e": 91, "982417": 80, "982815": 91, "982948": 78, "983192": 94, "9833": 126, "983389": 78, "983482": 91, "983483": 75, "983759": 160, "98393441": 97, "984": 59, "984083": [82, 83], "984368": 74, "984485": 68, "984562": 94, "984698": 68, "984866": 143, "984872": [78, 79], "984937": 80, "98505871e": 126, "985207": [78, 79], "985279": 91, "985569": 91, "985686": 69, "985971": 79, "9862474": 68, "986249": 90, "986683": 78, "98673": 84, "9870004": 64, "987175": 74, "9875": 61, "98750578": 66, "988": 92, "9880384": 64, "988081": 78, "988421": [78, 79], "988463": 94, "988873e": 71, "989304": 92, "989656": 92, "989792": 70, "99": [54, 63, 75, 78, 79, 91, 92, 109, 126, 159], "990089": 70, "990096": 70, "990210": 91, "990567e": 91, "990838": 78, "991": 64, "99129667": 68, "9914": [90, 91, 97], "9915": [63, 90, 91, 97], "991512": 63, "99165734": 126, "991824": 78, "991963": [78, 79], "991977": 91, "992": 95, "992288": 78, "99232145": 97, "992359": 16, "992389": 68, "992582": [78, 79], "992986": 70, "993358": 71, "993416": 85, "993512": [111, 114], "994": 95, "994168239": 62, "994208": 75, "9943": 71, "994304": 91, "994332": 76, "9944": [15, 111, 113], "994615": 74, "9948104": 65, "994864": 91, "994937": 83, "995081e": 71, "9951": 61, "995248": 94, "99549118e": 126, "99571372e": 126, "996": 92, "9961": 90, "9961392": 62, "996413": 91, "996610": 71, "996642": 71, "996729": 78, "996934": 89, "9970": 91, "997034": 99, "997038": 79, "997045": 68, "9973": 71, "997494": 99, "997571": 89, "997621": 80, "997934": [82, 83], "998050": 93, "998063": 76, "9981": 74, "99864670889": 160, "998855": 91, "999": [54, 67, 74, 84, 87, 97, 160], "999120": 93, "999207": 94, "9995": [67, 78, 79], "999596": 37, "9996": [67, 78, 79], "9996553": 63, "9997": [67, 78, 79], "9998": [67, 78, 79], "9999": [67, 78, 79], "99c8": 64, "A": [10, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 34, 35, 37, 38, 39, 40, 45, 46, 48, 49, 50, 52, 53, 59, 60, 61, 63, 64, 68, 69, 70, 71, 74, 76, 77, 81, 86, 88, 92, 93, 95, 96, 97, 98, 101, 102, 107, 109, 110, 111, 112, 114, 116, 125, 143, 144, 145, 151, 152, 153, 154, 155, 157, 158, 160], "ATE": [26, 30, 31, 63, 72, 74, 75, 85, 90, 97, 98, 109, 111, 121, 125, 127, 135, 144, 152], "ATEs": [74, 75, 92], "And": [65, 92, 99, 144, 146, 147], "As": [60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 74, 75, 77, 78, 79, 80, 81, 84, 85, 86, 88, 89, 90, 91, 92, 94, 98, 99, 100, 110, 111, 114, 115, 116, 126, 127, 129, 131, 133, 143, 144, 145, 154, 160], "At": [18, 31, 40, 62, 66, 67, 69, 75, 84, 86, 89, 91, 94, 160], "Being": 160, "But": [85, 86], "By": [61, 62, 89, 95, 98, 110, 111, 114, 116, 144, 150], "For": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 25, 26, 37, 39, 49, 53, 59, 61, 62, 64, 66, 68, 69, 70, 71, 74, 75, 76, 84, 85, 86, 88, 89, 91, 93, 95, 97, 98, 100, 101, 102, 103, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 152, 154, 155, 156, 157, 160], "ITE": [35, 74, 75], "ITEs": [74, 75], "If": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 60, 62, 66, 68, 69, 70, 71, 74, 77, 78, 79, 85, 86, 89, 91, 95, 101, 102, 107, 109, 110, 111, 113, 119, 127, 128, 129, 130, 131, 132, 135, 143, 144, 145, 146, 147, 148, 149, 150, 153, 154, 155, 160], "In": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 52, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 126, 127, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160], "It": [13, 61, 62, 63, 74, 78, 79, 81, 82, 83, 89, 90, 91, 95, 96, 98, 103, 106, 107, 110, 111, 112, 126, 155, 159], "No": [33, 55, 59, 61, 63, 64, 65, 66, 68, 69, 70, 71, 72, 74, 75, 84, 87, 90, 91, 95, 97, 99, 102, 104, 105, 106, 107, 110, 111, 113, 114, 116, 127, 143, 157, 158], "Not": [111, 117], "Of": [86, 143, 160], "On": [60, 77, 88, 92, 96, 101, 158], "One": [41, 63, 68, 71, 90, 91, 98, 109, 143], "Or": 46, "Such": [98, 110], "That": [46, 160], "The": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 115, 117, 120, 122, 123, 124, 125, 126, 127, 132, 135, 140, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156, 158, 159, 160], "Then": [18, 80, 94, 96, 111, 143, 144, 154, 155, 156], "There": [63, 90, 98, 111, 117, 156, 160], "These": [25, 63, 64, 69, 73, 88, 90, 93, 95, 97, 109, 111, 160], "To": [19, 21, 59, 60, 62, 63, 64, 65, 66, 68, 69, 70, 71, 74, 75, 76, 77, 78, 79, 80, 82, 83, 85, 86, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 101, 102, 107, 109, 110, 111, 115, 126, 143, 144, 145, 150, 154, 155, 156, 157, 160], "Will": [14, 16, 22, 24, 25, 26, 27, 28, 29, 30], "With": [34, 78, 79, 110, 158], "_": [9, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 60, 62, 66, 67, 68, 70, 71, 77, 78, 79, 80, 81, 82, 83, 89, 90, 91, 93, 94, 95, 96, 100, 101, 106, 107, 109, 111, 114, 117, 126, 127, 129, 143, 144, 145, 150], "_0": [60, 62, 77, 81, 89, 100, 101, 126, 127, 141, 142, 143, 144, 154], "_1": [17, 18, 19, 31, 35, 40, 65, 92, 99, 127, 141, 142], "_2": [17, 18, 19, 31, 35, 40, 92], "_3": [17, 18, 19, 31, 35, 40], "_4": [17, 18, 19, 31, 35, 40], "_5": [31, 35], "__": [52, 53], "__init__": [88, 96], "__version__": 156, "_a": [127, 129, 131], "_all_coef": 126, "_all_s": 126, "_b": [127, 129, 131], "_compute_scor": 21, "_compute_score_deriv": 21, "_coordinate_desc": 89, "_d": [74, 95, 111], "_est_causal_pars_and_s": 159, "_estimator_typ": 88, "_h": [95, 111], "_i": [60, 65, 77, 94, 99, 101, 111, 117], "_id": 126, "_j": [17, 18, 19, 31, 35, 40, 43, 62, 89, 143], "_l": 110, "_lower_quantil": [68, 71], "_m": [110, 126], "_mean": [68, 71], "_n": [127, 128, 129, 130, 131, 135, 143, 144, 150, 153], "_n_folds_per_clust": 89, "_offset": 110, "_pred": 110, "_rmse": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "_upper_quantil": [68, 71], "_x": 47, "_y": [95, 111], "a0": 88, "a09a": 64, "a09b": 64, "a1": 88, "a3d9": 64, "a4a147": 92, "a5e6": 64, "a5e7": 64, "a6ba": 64, "a79359d2da46": 64, "a840": 64, "a_": [65, 99], "a_0": [41, 44, 127, 138], "a_1": 44, "a_j": [111, 118], "ab": [61, 96, 155], "ab71": 64, "abadi": [10, 66], "abb0fd28": 64, "abdt": [72, 102, 107, 157], "abl": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 60, 77, 86, 91, 92, 110, 144, 145, 154], "about": [14, 16, 63, 66, 67, 74, 86, 90, 96, 111, 155, 157, 160], "abov": [60, 63, 69, 75, 77, 78, 79, 82, 83, 86, 88, 90, 92, 93, 94, 95, 98, 101, 109, 110, 111, 115, 117, 156], "absolut": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 86, 110], "abstract": [21, 61, 62, 89, 127, 155, 159], "abus": [68, 71, 111, 114, 116, 117], "acc": [23, 61], "acceler": 74, "accept": [17, 19, 103, 107, 109, 110], "access": [48, 49, 61, 63, 68, 69, 70, 71, 74, 82, 83, 84, 86, 97, 105, 107, 110, 144, 150, 160], "accompani": 155, "accord": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 63, 65, 66, 69, 75, 77, 80, 90, 94, 95, 98, 99, 110, 111, 112, 117, 143, 144, 146, 147, 148, 149, 151, 152, 160], "accordingli": [65, 66, 86, 88, 90, 95, 99], "account": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 62, 63, 68, 71, 89, 90, 91, 97, 98, 144, 150, 154, 160], "accross": 87, "accumul": [63, 90, 91, 97], "accur": 74, "accuraci": [48, 52, 61, 111], "acemoglu": 158, "achiev": [62, 74, 85, 89, 93, 98, 111, 143], "acic_2024_post": 92, "acknowledg": [63, 64, 90], "acm": 158, "acov": 158, "across": [13, 17, 19, 63, 74, 90, 92, 160], "action": 159, "activ": [4, 5, 6, 7, 8, 9, 156, 159], "actual": [46, 68, 71, 84, 98], "acycl": [65, 99, 160], "ad": [5, 6, 7, 8, 9, 10, 11, 21, 48, 49, 52, 53, 84, 102, 107, 110, 111, 143, 144, 145, 159], "adapt": [25, 87, 90, 159], "add": [61, 62, 65, 66, 67, 72, 74, 75, 82, 83, 84, 92, 94, 95, 97, 98, 99, 110, 111, 158, 159], "add_trac": 98, "addit": [12, 13, 14, 15, 16, 17, 18, 19, 22, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 43, 44, 45, 47, 50, 68, 69, 70, 71, 81, 87, 98, 103, 104, 105, 106, 107, 110, 111, 112, 127, 136, 144, 151, 152, 154, 158, 159], "addition": [31, 40, 71, 75, 80, 91, 97, 110, 111, 126, 143, 144, 150, 157], "additional_inform": 13, "additional_paramet": 13, "address": 98, "adel": 158, "adj": [95, 98], "adj_coef_bench": 98, "adj_est": 98, "adj_vanderweelearah": 98, "adjust": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 51, 54, 62, 67, 85, 87, 89, 91, 97, 98, 109, 111, 117, 143, 144, 150, 158, 159, 160], "adjust_p": 54, "adopt": [66, 111, 113, 117], "advanc": [88, 108, 126, 158], "advantag": [60, 61, 63, 75, 77, 90, 91, 101, 156], "advers": [144, 145], "adversari": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 97, 144, 150, 154], "ae": [60, 62, 63, 65], "ae56": 64, "ae89": 64, "aesthet": 60, "aeturrel": 45, "afd9e4": 92, "affect": [74, 75, 81, 111, 159, 160], "after": [54, 55, 61, 63, 64, 65, 66, 81, 90, 91, 98, 99, 109, 110, 111, 117, 144, 146, 150, 156, 160], "after_stat": 60, "ag": [63, 90, 91, 93, 97, 160], "again": [60, 61, 62, 63, 65, 66, 68, 71, 75, 77, 84, 88, 89, 90, 95, 96, 97, 98, 99, 101, 144, 146, 147], "against": [66, 74, 84, 86, 93, 110], "agebra": 109, "agegt54": [64, 72, 102, 107, 157], "agelt35": [64, 72, 102, 107, 157], "agg": [61, 68, 71, 96], "agg_df": [68, 71], "agg_df_anticip": [68, 71], "agg_dict": [68, 71], "agg_dictionari": [68, 71], "agg_did_obj": [111, 112], "aggrag": [111, 112], "aggreg": [13, 16, 61, 100, 112, 126, 159], "aggregate_over_split": 46, "aggregated_eventstudi": [68, 69, 70, 71], "aggregated_framework": [68, 69, 70, 71, 111, 112], "aggregated_group": [68, 71], "aggregated_tim": [68, 70, 71], "aggregation_0": 13, "aggregation_1": 13, "aggregation_color_idx": 13, "aggregation_method_nam": 13, "aggregation_nam": 13, "aggregation_weight": [13, 68, 69, 70, 71], "aggt": 61, "ai": [70, 96, 158], "aim": 95, "aipw": 92, "aipw_est_1": 92, "aipw_est_2": 92, "aipw_obj_1": 92, "aipw_obj_2": 92, "air": [62, 89], "al": [10, 11, 32, 34, 41, 43, 44, 60, 62, 63, 64, 66, 77, 78, 79, 80, 81, 82, 83, 86, 87, 89, 90, 91, 94, 97, 101, 111, 113, 117, 126, 127, 133, 135, 136, 137, 138, 143, 144, 145, 154, 155, 157, 159], "alexandr": [81, 158], "algebra": 111, "algorithm": [59, 61, 62, 64, 65, 66, 68, 69, 70, 71, 75, 77, 80, 85, 86, 89, 91, 94, 97, 99, 108, 110, 111, 113, 114, 116, 126, 127, 143, 159, 160], "alia": [48, 49, 52, 53], "align": [41, 60, 62, 65, 67, 68, 71, 77, 80, 86, 87, 89, 90, 92, 93, 94, 99, 111, 114, 116, 117, 127, 129, 131, 159], "all": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 49, 52, 53, 56, 60, 61, 62, 63, 65, 66, 74, 75, 77, 84, 85, 86, 87, 88, 89, 90, 91, 93, 95, 98, 99, 101, 102, 104, 107, 109, 110, 111, 112, 113, 115, 117, 126, 127, 129, 131, 143, 144, 154, 155, 156, 159], "all_coef": 126, "all_dml1_coef": 100, "all_s": 126, "all_smpl": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 85], "all_smpls_clust": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "all_z_col": [62, 89], "allow": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 52, 53, 63, 68, 71, 74, 75, 90, 91, 95, 107, 109, 110, 111, 126, 127, 143, 155, 159, 160], "almqvist": 158, "along": [74, 110], "alongsid": [103, 105, 107], "alpha": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 42, 44, 60, 62, 63, 65, 68, 71, 72, 74, 75, 77, 78, 79, 80, 81, 85, 86, 87, 88, 89, 90, 91, 94, 100, 101, 109, 110, 111, 126, 127, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154], "alpha_": [43, 62, 89, 110], "alpha_0": [144, 154], "alpha_ml_l": 81, "alpha_ml_m": 81, "alpha_x": [25, 33, 111], "alreadi": [18, 65, 66, 68, 71, 96, 99, 110, 111, 113], "also": [12, 14, 15, 16, 22, 25, 26, 39, 59, 60, 61, 62, 63, 64, 66, 68, 69, 71, 74, 75, 76, 77, 78, 79, 82, 83, 84, 86, 88, 89, 90, 91, 93, 95, 97, 98, 101, 109, 110, 111, 126, 127, 143, 144, 145, 156, 157, 159, 160], "alter": [62, 89], "altern": [37, 61, 63, 64, 69, 90, 93, 108, 110, 143, 155, 157], "although": 98, "alwai": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 49, 53, 61, 87, 95, 159], "always_tak": [25, 63, 90], "alyssa": 158, "amamb": 89, "american": [42, 92], "amgrem": 89, "amhorn": 89, "amit": [98, 158], "amjavl": 89, "ammata": 89, "among": [63, 81, 90, 91, 97, 98], "amount": [16, 63, 88, 90, 91, 160], "amp": [59, 62, 64, 65, 66, 68, 69, 70, 71, 89, 91, 97, 99], "an": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 41, 48, 49, 52, 53, 60, 61, 62, 63, 64, 65, 68, 69, 70, 71, 75, 77, 78, 79, 81, 84, 86, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 105, 106, 107, 109, 110, 111, 112, 115, 117, 118, 126, 127, 143, 144, 145, 150, 155, 157, 158, 159, 160], "analog": [20, 21, 62, 68, 69, 71, 89, 91, 97, 109, 111, 113, 127, 128, 129, 130, 131, 143, 144, 150], "analys": [102, 107, 160], "analysi": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 47, 60, 62, 63, 77, 89, 90, 91, 96, 101, 108, 109, 111, 116, 145, 150, 154, 155, 159], "analyst": 96, "analyt": [92, 94], "analyz": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 63, 90, 91, 97, 160], "ancillari": 98, "andrea": 158, "angl": 63, "angrist": 92, "ani": [59, 60, 61, 64, 65, 66, 74, 76, 77, 96, 98, 99, 101, 111, 156, 160], "anna": [12, 14, 15, 17, 18, 19, 31, 35, 40, 61, 66, 68, 69, 70, 71, 111, 112, 113, 115, 117, 158], "annal": [143, 158], "anneal": 110, "annot": 60, "annual": 158, "anoth": [60, 61, 62, 63, 77, 86, 88, 89, 96, 101, 110, 111, 119], "anticip": [14, 16, 17, 19, 61, 69, 70, 111, 112, 114, 115, 116, 117], "anticipation_period": [14, 16, 17, 19, 68, 71], "anymor": [62, 89], "aos1161": 143, "aos1230": 143, "aos1671": 143, "ap": [63, 90], "ape_e401_uncond": 63, "ape_p401_uncond": 63, "api": [74, 87, 102, 107, 155, 159], "apo": [22, 23, 74, 118, 132, 151], "apo_result": 74, "apoorva": 159, "apoorva__l": 92, "apoorval": 92, "app": 159, "appeal": 98, "append": [74, 77, 86, 96, 101], "appendix": [34, 36, 65, 68, 71, 97, 99, 144, 145], "appli": [11, 12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 54, 55, 59, 60, 62, 63, 64, 65, 67, 68, 70, 71, 77, 85, 86, 89, 90, 91, 95, 96, 98, 99, 101, 111, 126, 127, 143, 155, 157, 158, 159, 160], "applic": [41, 60, 66, 74, 77, 92, 98, 101, 103, 107, 109, 126, 158, 160], "applicatoin": 69, "apply_along_axi": 93, "apply_cross_fit": [60, 126], "apply_crossfit": 159, "appreci": 155, "approach": [14, 16, 22, 24, 25, 26, 27, 28, 29, 30, 61, 62, 74, 75, 89, 95, 97, 98, 108, 110, 111, 126, 143, 144, 145, 156, 158, 160], "appropri": [63, 81, 90, 111, 126, 160], "approx": [109, 111, 114, 116], "approxim": [60, 71, 77, 78, 79, 80, 86, 94, 98, 101, 109, 111, 143, 159, 160], "april": 68, "apt": 156, "ar": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 37, 38, 39, 40, 41, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 75, 77, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 139, 140, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160], "arang": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 67, 77, 80, 87, 91, 93, 94, 97, 98, 102, 107, 110], "arbitrarili": [49, 53], "architectur": [74, 127, 158], "arellano": 158, "arg": [88, 95, 109, 111], "argmax": 96, "argmin": 86, "argu": [60, 63, 77, 90, 91, 97, 101, 160], "argument": [12, 14, 15, 17, 18, 19, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 43, 44, 45, 46, 47, 50, 63, 66, 68, 69, 70, 71, 78, 79, 84, 86, 89, 90, 91, 96, 100, 102, 109, 110, 111, 112, 159, 160], "aris": [60, 61, 62, 69, 77, 89, 98, 101, 160], "aronow": 92, "around": [61, 63, 74, 90, 91, 95, 111, 127], "arr": 93, "arrai": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 48, 49, 50, 51, 52, 53, 54, 65, 66, 68, 69, 70, 71, 74, 75, 77, 78, 79, 80, 86, 89, 92, 93, 96, 97, 98, 99, 100, 101, 103, 105, 106, 109, 110, 126, 143, 144, 150, 157, 159, 160], "arrang": 62, "array_lik": 29, "articl": [45, 155], "arxiv": [43, 61, 62, 89, 96, 98, 155, 158, 159], "as_learn": [64, 110], "asarrai": [78, 79], "aspect": [63, 90, 91], "assert": 110, "assess": 61, "asset": [91, 97, 160], "assign": [4, 5, 6, 7, 8, 9, 17, 19, 54, 63, 68, 71, 74, 87, 90, 95, 102, 107, 109, 110, 111, 112, 125, 144, 160], "assmput": [111, 125], "associ": [63, 81, 90, 111, 143, 158], "assum": [59, 62, 66, 76, 89, 92, 93, 96, 98, 111, 112, 113, 115, 117, 127, 128, 129, 130, 131, 143, 144, 154, 160], "assumpt": [61, 62, 63, 65, 66, 67, 68, 69, 71, 86, 89, 90, 92, 95, 99, 111, 112, 113, 115, 117, 125, 143, 160], "assur": 159, "astyp": [68, 70, 76, 95, 98], "asymptot": [20, 21, 60, 62, 74, 77, 89, 101, 126, 143, 158], "ate": [74, 75], "ate_estim": [65, 99], "ates": [74, 75], "athei": 158, "att": [16, 17, 19, 26, 31, 61, 67, 84, 85, 93, 98, 109, 111, 112, 113, 114, 115, 116, 117, 121, 127, 135, 144, 152, 159], "att_": [111, 115], "att_gt": [61, 69, 70], "attach": 61, "atte_estim": 66, "attempt": [48, 49], "attenu": [63, 90], "attr": 63, "attribut": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 50, 51, 52, 53, 54, 55, 86, 88, 100, 104, 107, 110, 126, 127, 143], "attributeerror": [48, 49], "attrict": 111, "attrit": [30, 65, 99, 111, 125], "au": [64, 110, 155, 157], "auc": 61, "author": [61, 98, 155], "auto_ml": 88, "autodoubleml": 88, "autom": 88, "automat": [13, 16, 60, 68, 70, 71, 77, 84, 101, 109, 144, 150], "automl": 159, "automl_l": 88, "automl_l_lesstim": 88, "automl_m": 88, "automl_m_lesstim": 88, "automobil": [62, 89], "autos": 81, "autosklearn": 88, "auxiliari": [37, 60, 77, 101], "avail": [33, 61, 63, 64, 66, 68, 69, 71, 74, 75, 81, 86, 90, 91, 92, 93, 95, 98, 101, 102, 107, 109, 110, 111, 112, 113, 114, 116, 144, 154, 155, 156, 159, 160], "avaiv": 51, "aver": 75, "averag": [17, 18, 19, 22, 23, 25, 26, 31, 39, 40, 59, 61, 64, 65, 66, 67, 68, 70, 71, 76, 84, 91, 92, 93, 95, 96, 97, 98, 99, 108, 112, 113, 117, 118, 119, 120, 121, 125, 132, 135, 143, 151, 152, 158, 159, 160], "average_it": [74, 75], "avoid": [55, 60, 61, 77, 95, 111, 126, 156, 159], "awai": 97, "ax": [13, 16, 67, 68, 69, 70, 71, 74, 75, 77, 78, 79, 80, 82, 83, 86, 88, 89, 90, 91, 92, 94, 95], "ax1": [74, 75, 80, 85, 91, 94], "ax2": [74, 75, 80, 85, 91, 94], "axhlin": [67, 74, 88, 95], "axi": [13, 16, 62, 63, 74, 75, 81, 85, 86, 89, 90, 92, 93, 95], "axvlin": [74, 75, 77], "b": [12, 14, 15, 16, 17, 19, 45, 60, 62, 64, 77, 78, 79, 89, 92, 94, 95, 98, 101, 109, 110, 111, 117, 143, 144, 154, 155, 157, 158], "b208": 64, "b371": 64, "b5d34a6f42b": 64, "b5d7": 64, "b_": 111, "b_0": 44, "b_1": 44, "b_j": 45, "bach": [81, 86, 88, 98, 155, 158, 159], "back": 74, "backbon": 86, "backend": [5, 6, 7, 8, 9, 61, 91, 97, 98, 102, 104, 106, 108, 159], "backward": [4, 102, 107, 159], "bad": 92, "balanc": [41, 63, 68, 69, 71, 87, 90, 91], "balanced_r0": 41, "band": [61, 108, 160], "bandwidth": [27, 28, 29, 46, 95, 111, 159], "bar": [84, 88, 90, 109, 111, 127, 132, 135, 144, 151], "base": [12, 14, 15, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 37, 38, 39, 47, 51, 55, 60, 61, 62, 63, 65, 66, 67, 69, 70, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 105, 107, 109, 110, 111, 112, 115, 117, 125, 126, 127, 143, 144, 145, 150, 155, 157, 158, 159, 160], "base_estim": [52, 53, 95], "baseestim": 96, "baselin": [14, 35, 63, 88, 90], "basi": [22, 26, 39, 50, 71, 78, 79, 85, 109], "basic": [61, 62, 63, 66, 70, 89, 90, 91, 92, 95, 97, 98, 108, 110], "basis_df": 85, "basis_matrix": 85, "batch": 64, "battocchi": 158, "batuhan": 159, "bay": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 143], "bb2913dc": 64, "bbotk": [64, 110, 159], "bbox_inch": 77, "bbox_to_anchor": [77, 87], "bcallaway11": [61, 69], "bd929a9e": 64, "bde4": 64, "becam": [63, 90, 91], "becaus": [49, 53, 59, 60, 61, 62, 76, 77, 84, 89, 92, 96, 98, 101, 160], "becker": [64, 110], "becom": [62, 88, 89, 109, 126], "bee": 67, "been": [13, 16, 62, 63, 68, 70, 71, 88, 89, 90, 91, 97, 98, 109, 110, 111, 117, 155, 159], "befor": [17, 19, 54, 55, 61, 63, 67, 68, 70, 71, 75, 84, 90, 94, 98, 111, 113, 160], "begin": [33, 41, 42, 43, 60, 62, 63, 64, 65, 67, 68, 71, 77, 80, 86, 89, 90, 92, 93, 94, 99, 100, 102, 107, 110, 111, 114, 116, 117, 126, 127, 129, 131, 143, 157, 160], "behav": [68, 70, 71, 96], "behavior": [63, 92, 110], "behind": [69, 111], "being": [17, 19, 20, 21, 35, 36, 47, 52, 53, 62, 71, 89, 95, 98, 111, 116, 117, 126, 127, 133, 143, 144, 150, 155], "belloni": [34, 81, 143, 158], "below": [54, 55, 59, 63, 69, 76, 90, 92, 96, 111, 156, 157], "bench_x1": 98, "bench_x2": 98, "benchmark": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 56, 74, 75, 84, 145, 159], "benchmark_dict": [56, 97], "benchmark_inc": 97, "benchmark_pira": 97, "benchmark_result": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39], "benchmark_twoearn": 97, "benchmarking_set": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 75, 84, 97, 98, 144, 145], "benchmarking_vari": 84, "benefit": [60, 63, 77, 90, 101], "bernoulli": [33, 41], "berri": [62, 89], "besid": 157, "best": [22, 26, 39, 49, 50, 53, 68, 71, 74, 78, 79, 82, 83, 88, 156], "best_loss": 88, "beta": [30, 33, 34, 36, 42, 63, 65, 90, 93, 95, 99, 111, 127, 138], "beta_": [65, 99], "beta_0": [32, 65, 87, 93, 99, 109, 111, 122, 127, 138], "beta_a": [31, 40, 98], "beta_j": [33, 34, 36, 42], "better": [61, 68, 71, 74, 75, 86, 98, 111, 117], "between": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 55, 59, 65, 67, 68, 71, 74, 75, 76, 80, 81, 88, 92, 94, 96, 97, 98, 99, 111, 119, 127, 128, 129, 130, 131, 135, 138, 139, 140, 143, 144, 154, 157, 159], "betwen": [59, 76], "beyond": 158, "bia": [36, 59, 65, 76, 81, 95, 98, 99, 108, 111, 125, 126, 127, 141, 142, 144, 154, 158, 159], "bias": [59, 63, 68, 71, 76, 90, 91, 97, 160], "bias_bench": 98, "bibtex": 155, "big": [81, 100, 126, 127, 128, 129, 136, 143, 144, 146, 147, 148, 149, 152, 153, 154], "bigg": [62, 89, 127, 134, 135, 144, 147, 152], "bilia": 11, "bilinski": 158, "bin": [60, 74, 75, 77, 156], "binari": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 32, 37, 39, 41, 47, 59, 61, 63, 64, 66, 76, 84, 85, 86, 87, 90, 92, 93, 98, 109, 110, 113, 117, 120, 121, 122, 125, 144, 151, 152, 159, 160], "binary_outcom": 47, "binary_treat": [32, 78, 82, 84], "binary_unbalanc": 41, "bind": 159, "binder": [64, 110, 155, 157, 159], "binomi": [76, 92, 93, 94, 96], "bischl": [64, 110, 155, 157], "black": [60, 64, 72, 102, 107, 157], "blob": 61, "blog": 45, "blondel": [155, 157], "blp": [50, 62, 89], "blp_data": [62, 89], "blp_model": [82, 83], "blue": [60, 62, 65, 89], "bodori": 158, "bond": [63, 90, 91], "bonferroni": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 143], "bonu": [11, 64, 102, 107, 157], "book": [64, 96, 98, 110, 158], "bool": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 41, 46, 47, 48, 49, 50, 52, 53, 54, 55, 84, 95], "boolean": [36, 82, 83, 102, 107, 126], "boost": [59, 63, 66, 68, 71, 74, 76, 86, 90], "boost_class": [63, 90], "boost_summari": 90, "boostrap": [80, 159], "bootstrap": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 68, 69, 70, 71, 75, 78, 79, 80, 82, 83, 91, 94, 108, 109, 111, 112, 126, 127, 155, 157, 159, 160], "both": [12, 13, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 40, 61, 63, 64, 66, 67, 74, 85, 86, 87, 88, 90, 91, 93, 95, 96, 97, 98, 102, 103, 107, 110, 111, 114, 115, 116, 143, 144, 145, 150, 153, 154, 159, 160], "bottom": [62, 63, 86, 89, 90, 91], "bound": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 35, 37, 38, 39, 54, 55, 63, 68, 71, 74, 75, 84, 85, 90, 97, 98, 144, 145, 150, 154, 159, 160], "branch": 64, "brantli": [61, 158], "break": [60, 159], "breve": [127, 138], "breviti": 160, "brew": 156, "brewer": 62, "bridg": 98, "brief": 101, "briefli": 69, "bring": [59, 76], "brucher": [155, 157], "bsd": [155, 159], "bst": 90, "budget": [88, 110], "bug": [155, 159], "build": [62, 86, 89, 93], "build_design_matric": [78, 79], "build_sim_dataset": 61, "built": [51, 88, 110, 155], "bureau": [98, 126, 158], "busi": [36, 43, 62, 89, 98, 158], "b\u00fchlmann": 158, "c": [10, 11, 17, 18, 19, 34, 40, 42, 44, 59, 60, 61, 62, 63, 64, 67, 72, 76, 77, 81, 82, 83, 89, 90, 92, 95, 96, 101, 102, 107, 110, 111, 117, 127, 129, 131, 144, 147, 149, 155, 156, 157, 158, 160], "c1": [10, 11, 44, 62, 81, 89, 96, 101, 155, 158], "c68": [10, 11, 44, 62, 81, 89, 96, 101, 155, 158], "c895": 64, "c_": [16, 111, 114, 116, 117, 127, 129, 131, 143], "c_d": [34, 144, 152, 153, 154], "c_i": [111, 117], "c_y": [34, 144, 154], "ca1af7be64b2": 64, "caac5a95": 64, "calcualt": 93, "calcul": [22, 26, 39, 61, 63, 68, 71, 74, 75, 78, 79, 80, 82, 83, 86, 88, 90, 94, 97, 144, 150, 154], "calendar": [68, 69, 70, 71], "calibr": [14, 16, 22, 24, 25, 26, 27, 28, 29, 30, 54, 55, 86, 88, 98], "calibration_method": [54, 55], "call": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 49, 52, 53, 59, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 74, 76, 78, 79, 80, 82, 83, 90, 91, 93, 94, 95, 97, 98, 99, 102, 107, 110, 126, 127, 143, 144, 150, 154, 157, 159, 160], "callabl": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 77, 78, 79, 86, 108, 110, 155], "callawai": [17, 19, 61, 68, 69, 70, 71, 111, 112, 115, 117, 158], "camera": 81, "cameron": [62, 89], "can": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 46, 49, 51, 53, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 107, 109, 110, 111, 112, 113, 115, 117, 118, 125, 126, 127, 128, 129, 130, 131, 132, 135, 138, 139, 140, 143, 144, 145, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160], "candid": 98, "cannot": [86, 95, 98, 111, 160], "capabl": [5, 6, 7, 8, 9, 59, 76], "capo": [22, 85], "capo0": 85, "capo1": 85, "capsiz": [74, 75, 88, 92, 95], "capthick": [74, 75, 95], "cardin": [62, 89], "care": [69, 96, 110], "carlo": [31, 32, 35, 40, 78, 79, 82, 83, 98, 158], "casalicchio": [64, 110, 155, 157], "case": [4, 5, 6, 7, 8, 9, 11, 14, 22, 25, 26, 32, 46, 59, 62, 63, 68, 69, 71, 76, 78, 79, 80, 81, 84, 85, 87, 88, 89, 93, 94, 95, 96, 97, 98, 102, 107, 109, 110, 111, 113, 117, 125, 126, 127, 129, 131, 143, 144, 150, 157, 159, 160], "cat": [60, 159], "catboost": 86, "cate": [26, 39, 50, 85, 108, 159], "cate_obj": 109, "categori": 74, "cattaneo": [111, 158], "caus": [60, 77, 95, 101], "causal": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 59, 60, 62, 63, 64, 65, 76, 77, 85, 86, 88, 89, 90, 92, 96, 97, 99, 100, 101, 102, 105, 107, 108, 111, 117, 126, 143, 144, 150, 158], "causal_contrast": [23, 74, 75, 85, 111], "causal_contrast_att": 85, "causal_contrast_c": 85, "causal_contrast_model": [74, 75, 111], "causal_contrast_result": 74, "causaldml": 158, "causalml": [96, 158], "causalweight": 158, "caution": 143, "caveat": 98, "cbind": 62, "cbook": [68, 69, 70, 71], "cc": 90, "ccp_alpha": [26, 51, 90], "cd": 156, "cd_fast": 89, "cda85647": 64, "cdf": 109, "cdid": [62, 89], "cdot": [17, 18, 19, 31, 35, 40, 41, 47, 62, 67, 68, 71, 80, 84, 89, 92, 94, 95, 98, 109, 111, 112, 114, 116, 117, 126, 127, 129, 131, 132, 135, 136, 138, 141, 142, 143, 144, 147, 149, 151], "cdot1": 84, "cell": 88, "center": [68, 69, 71, 74, 81, 87], "central": [126, 159], "certain": [111, 127, 129, 131], "cexcol": 62, "cexrow": 62, "cf_d": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 56, 68, 71, 75, 84, 85, 97, 98, 144, 145, 150, 151, 152, 153, 154, 160], "cf_y": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 56, 68, 71, 75, 84, 85, 97, 98, 144, 145, 150, 151, 152, 153, 154, 160], "cff": 159, "chad": 98, "challeng": [62, 89, 144, 145], "chanc": 71, "chang": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 52, 53, 63, 65, 66, 70, 87, 91, 97, 98, 99, 111, 116, 117, 127, 131, 135, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 156, 158, 159], "channel": 160, "chapter": [20, 21, 64, 96, 110, 144, 154], "charact": [63, 64, 110, 159], "characterist": [97, 160], "chart": 88, "check": [48, 49, 52, 53, 60, 63, 66, 67, 68, 71, 77, 86, 88, 90, 91, 96, 100, 101, 111, 112, 155, 156, 159], "check_data": 159, "check_scor": 159, "checkmat": 159, "chernozhukov": [10, 11, 34, 42, 44, 60, 62, 63, 77, 81, 86, 88, 89, 90, 91, 96, 97, 101, 126, 127, 135, 143, 144, 145, 154, 155, 158, 159], "chetverikov": [10, 11, 44, 62, 81, 89, 96, 101, 143, 155, 158], "chiang": [43, 62, 89, 158], "chieh": 158, "choic": [5, 6, 7, 8, 9, 14, 16, 22, 24, 25, 26, 27, 28, 29, 30, 63, 68, 69, 70, 71, 81, 90, 93, 109, 110, 111, 115, 127, 129, 131, 144, 145, 150, 154, 159], "cholecyst": 96, "choos": [59, 63, 69, 76, 77, 81, 86, 90, 91, 100, 111, 115, 126, 127, 128, 129, 130, 131, 135, 138, 139, 140, 143, 157, 160], "chosen": [22, 35, 40, 86, 110, 111], "chou": 92, "chr": 63, "christian": [81, 158], "christoph": 158, "chunk": 110, "ci": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 66, 67, 68, 69, 70, 71, 74, 75, 78, 79, 80, 82, 83, 84, 85, 88, 90, 91, 94, 95, 97, 98, 109, 111, 144, 150, 159, 160], "ci_at": [74, 75], "ci_bound": 16, "ci_cvar": [80, 91], "ci_cvar_0": 80, "ci_cvar_1": 80, "ci_joint": [68, 69, 70, 71, 75], "ci_joint_cvar": 80, "ci_joint_lqt": 94, "ci_joint_qt": 94, "ci_length": 66, "ci_low": [74, 75], "ci_lpq_0": 94, "ci_lpq_1": 94, "ci_lqt": [91, 94], "ci_pointwis": [74, 75], "ci_pq_0": [91, 94], "ci_pq_1": [91, 94], "ci_qt": [91, 94], "ci_upp": [74, 75], "cinelli": [98, 144, 145, 158], "circumv": 160, "citat": 159, "cite": 155, "cl": 67, "claim": 64, "clarifi": [68, 69, 70, 71], "clash": 61, "class": [0, 4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 50, 51, 52, 53, 54, 55, 63, 64, 65, 66, 68, 69, 70, 71, 72, 74, 75, 84, 85, 87, 88, 90, 91, 96, 97, 99, 100, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 126, 127, 143, 155, 157, 159], "class_estim": 95, "class_learn": 91, "class_learner_1": 86, "class_learner_2": 86, "classes_": [88, 96], "classic": [61, 62, 69, 89, 160], "classif": [26, 48, 52, 59, 61, 63, 64, 65, 66, 68, 69, 70, 71, 86, 88, 91, 93, 97, 99, 109, 110, 111, 113, 114, 116, 160], "classifavg": 64, "classifi": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 39, 46, 48, 52, 64, 74, 75, 87, 88, 95, 96, 110, 159], "classifiermixin": 96, "classmethod": [4, 5, 6, 7, 8, 9, 54], "claudia": [158, 159], "claus": [155, 159], "clean": 159, "cleaner": 86, "cleanup": 159, "clear": [62, 74, 89], "clearli": [68, 71, 95], "clever": 86, "client": 74, "clip": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 54, 55], "clipping_threshold": [12, 15, 54, 55, 78], "clone": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 59, 60, 64, 77, 86, 89, 91, 100, 110, 111, 126, 127, 143, 144, 150, 156, 157], "close": [55, 61, 63, 90, 96, 98, 144, 145], "cluster": [5, 6, 7, 8, 9, 12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 43, 70, 102, 103, 105, 106, 107, 158, 159], "cluster_col": [4, 5, 6, 8, 9, 62, 89, 102, 103, 105, 106, 107], "cluster_var": [4, 5, 6, 7, 8, 9, 43, 102, 103, 107], "cluster_var_i": [62, 89], "cluster_var_j": [62, 89], "cmap": 89, "cmd": 159, "co": [41, 45, 67], "codaci": 159, "code": [22, 26, 39, 45, 59, 61, 62, 63, 64, 65, 76, 81, 90, 101, 109, 110, 111, 126, 127, 143, 156, 157, 159, 160], "codecov": 159, "coef": [12, 15, 16, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 97, 98, 99, 100, 101, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 157, 160], "coef_": 98, "coef_df": 62, "coef_valu": 88, "coeffici": [16, 31, 32, 40, 49, 50, 53, 63, 65, 82, 83, 86, 87, 90, 92, 93, 95, 98, 99, 109, 143, 144, 150, 160], "coefs_t": 93, "coefs_w": 93, "coffici": [144, 150], "cofid": 50, "coincid": [67, 85, 91], "col": [60, 62, 90], "col_nam": [68, 71], "collect": [64, 65, 66, 74, 89, 99], "colnam": [62, 86], "color": [13, 16, 63, 65, 67, 71, 74, 75, 77, 78, 79, 80, 88, 89, 90, 91, 92, 94, 95, 98], "color_palett": [13, 16, 68, 71, 74, 75, 77, 89, 90, 91], "colorbar": 89, "colorblind": [13, 16, 68, 71, 74, 75], "colorramppalett": 62, "colorscal": [78, 79], "colour": [60, 62], "column": [5, 6, 7, 8, 9, 16, 66, 67, 68, 69, 70, 71, 72, 74, 75, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 95, 97, 98, 99, 102, 103, 104, 105, 106, 107, 109, 110, 111, 114, 116, 126, 157, 159, 160], "column_stack": [8, 67, 74, 75, 82, 83, 95, 97, 98, 111], "colv": 62, "com": [45, 61, 63, 64, 69, 70, 81, 92, 98, 110, 155, 156], "comb": 81, "combin": [14, 16, 61, 62, 64, 66, 69, 70, 74, 75, 85, 86, 88, 89, 98, 110, 111, 115, 126, 144, 150, 159], "combind": 91, "combined_loss": 81, "come": [100, 110, 127, 144, 145, 155, 160], "command": [156, 159], "comment": [102, 107], "commit": 159, "common": [86, 97, 98, 109, 111, 158], "commonli": 74, "companion": 158, "compar": [60, 62, 67, 68, 71, 74, 77, 78, 79, 80, 82, 83, 85, 87, 92, 94, 95, 96, 98, 101, 110, 111, 144, 145, 159], "comparevers": 63, "comparison": [68, 71, 75, 86, 92], "compat": [4, 59, 61, 76, 102, 107, 159], "complement": 98, "complet": [88, 101, 144, 150, 156], "complex": [26, 61, 88], "compli": [95, 111], "complianc": [94, 95, 111, 127, 136], "complic": [64, 160], "complier": [63, 90, 91, 94, 95, 109, 111], "compon": [17, 19, 52, 53, 61, 63, 69, 74, 81, 86, 88, 90, 93, 109, 110, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 139, 140, 159], "compont": 61, "composit": 158, "comprehens": 74, "compris": 143, "comput": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 56, 60, 61, 63, 64, 68, 69, 70, 71, 74, 77, 90, 91, 96, 97, 98, 126, 127, 138, 144, 145, 146, 147, 148, 149, 150, 151, 152, 155, 158, 159, 160], "computation": [144, 145], "concat": [74, 88, 89, 90, 93, 143], "concaten": [67, 90, 143], "concentr": 143, "concern": 98, "conclud": [95, 98, 160], "cond": [111, 113, 125], "conda": [158, 159], "condit": [20, 21, 22, 24, 26, 31, 32, 39, 40, 60, 62, 63, 65, 66, 67, 68, 71, 75, 77, 84, 85, 89, 90, 93, 95, 98, 99, 101, 108, 111, 114, 115, 116, 117, 143, 144, 151, 152, 154, 157, 158, 159, 160], "conduct": [109, 111, 113, 114, 116, 160], "conf": [61, 94], "confer": 158, "confid": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 50, 61, 62, 63, 65, 66, 68, 69, 70, 71, 74, 75, 78, 79, 80, 82, 83, 85, 89, 91, 94, 95, 97, 99, 108, 109, 111, 126, 127, 144, 150, 157, 158, 159, 160], "confidenceband": 80, "confidenti": 98, "config": [54, 55, 92], "configur": [14, 16, 22, 24, 25, 26, 27, 28, 29, 30, 55, 64, 88], "confint": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 50, 63, 65, 66, 67, 68, 69, 70, 71, 74, 75, 78, 79, 80, 82, 83, 85, 86, 91, 93, 94, 95, 96, 97, 99, 109, 126, 143, 155, 157, 160], "conflict": 156, "confound": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 37, 38, 39, 40, 56, 59, 63, 68, 69, 71, 76, 84, 87, 90, 94, 97, 98, 102, 107, 111, 122, 123, 124, 143, 144, 145, 150, 153, 154, 157, 158, 159, 160], "congress": 158, "connect": [63, 90, 91], "consequ": [31, 40, 62, 84, 89, 97, 109, 111, 113, 144, 145, 151, 152, 154], "conserv": [97, 98, 144, 154], "consid": [24, 25, 26, 27, 28, 47, 54, 55, 60, 62, 63, 65, 66, 67, 68, 69, 70, 71, 77, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 109, 110, 111, 112, 117, 120, 121, 126, 127, 143, 144, 145, 155, 160], "consider": [98, 111], "consist": [37, 38, 39, 49, 53, 63, 66, 87, 88, 90, 91, 92, 98, 101, 102, 107, 111, 117, 122, 123, 124, 157, 159], "consol": [60, 159], "constant": [17, 19, 34, 49, 53, 68, 71, 74, 81, 87, 93, 109, 111, 143], "constrained_layout": 77, "construct": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 50, 64, 67, 68, 71, 78, 79, 80, 85, 91, 96, 97, 100, 109, 127, 133, 142, 143, 159, 160], "construct_framework": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "construct_iv": 89, "constructiv": 62, "constructor": [64, 103, 107], "consum": [62, 89], "cont": 35, "cont_d": [74, 75], "contain": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 47, 48, 49, 52, 53, 60, 62, 63, 68, 69, 71, 75, 77, 78, 79, 82, 83, 86, 89, 90, 96, 101, 103, 104, 106, 107, 109, 110, 111, 112, 114, 116, 143, 144, 145, 150, 159], "context": [98, 111, 125, 160], "contin": [35, 88], "continu": [35, 37, 41, 59, 64, 74, 75, 76, 81, 87, 92, 95, 111, 144, 154, 159, 160], "contour": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 81, 84, 97, 98, 144, 150], "contour_plot": 98, "contours_z": [78, 79], "contrast": [23, 66, 74, 80, 85, 111, 119], "contribut": [156, 159], "contributor": 159, "control": [14, 16, 17, 19, 42, 47, 54, 61, 69, 70, 81, 85, 91, 93, 95, 96, 98, 102, 103, 107, 111, 112, 114, 115, 116, 117, 127, 129, 131, 144, 147, 149, 160], "control_group": [14, 16, 68, 69, 70, 71, 111, 112, 114, 116], "conveni": [102, 105, 107], "convent": [63, 69, 90, 91, 95, 111, 117], "converg": [37, 60, 77, 86, 89, 101], "convergencewarn": 89, "convers": 89, "convert": [74, 80, 89, 94], "convex": 92, "cooper": 159, "coor": [64, 110, 155, 157], "coordin": [74, 98], "copi": [71, 88, 90, 93, 98], "cor": [144, 154], "core": [17, 19, 68, 69, 70, 71, 72, 74, 75, 80, 84, 87, 89, 90, 91, 94, 97, 102, 104, 107, 110, 157, 159], "cores_us": [80, 91, 94], "correct": [84, 85, 87, 98, 109, 143, 159], "correctli": [48, 52, 66, 92, 97, 144, 154], "correl": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 62, 65, 81, 89, 96, 97, 99, 111, 144, 145, 154], "correpond": [68, 71, 111], "correspond": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 60, 62, 63, 64, 66, 67, 68, 69, 70, 71, 75, 77, 78, 79, 81, 85, 86, 89, 90, 91, 93, 94, 97, 98, 101, 109, 110, 111, 113, 115, 117, 118, 125, 126, 127, 129, 131, 143, 144, 145, 147, 149, 150, 152, 154, 159, 160], "correspondingli": [68, 71], "cosh": 45, "coul": 62, "could": [59, 64, 68, 69, 71, 76, 78, 79, 88, 98, 159, 160], "counfound": [31, 40, 94, 97, 109, 144, 154], "count": [74, 75, 90, 91], "counti": 69, "countour": [144, 150], "countyr": 69, "coupl": [63, 90, 91], "cournapeau": [155, 157], "cours": [63, 86, 90, 98, 143, 160], "cov": [30, 31, 47, 95], "cov_nam": [95, 111], "cov_typ": [22, 26, 39, 50, 159], "covari": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 19, 26, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 51, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 72, 74, 75, 77, 78, 79, 81, 82, 83, 84, 87, 88, 89, 90, 91, 92, 93, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 113, 114, 116, 122, 123, 124, 125, 127, 128, 129, 130, 131, 143, 144, 145, 157, 158, 159], "cover": [61, 81, 97, 106, 107], "coverag": [68, 71, 86, 95, 96, 109, 159], "cp": [63, 64, 110], "cpu": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 74], "cpu_count": [80, 91, 94], "cran": [64, 158, 159], "creat": [13, 16, 17, 19, 32, 54, 59, 62, 64, 68, 71, 75, 76, 77, 78, 79, 80, 82, 83, 87, 89, 91, 93, 94, 98, 102, 107, 110, 144, 145, 150, 154, 156, 159], "create_default_for_vers": 74, "create_synthetic_group_data": 93, "crictial": 126, "critic": [98, 160], "cross": [12, 14, 15, 16, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 54, 55, 59, 60, 61, 63, 64, 65, 69, 74, 77, 86, 88, 90, 91, 95, 98, 101, 103, 107, 108, 110, 114, 115, 117, 130, 142, 143, 146, 147, 150, 159, 160], "cross_sectional_data": [15, 18, 66, 111, 113], "crossfit": [86, 111], "crosstab": 92, "crucial": [81, 111, 160], "csail": [155, 157], "csdid": 70, "csv": [70, 81], "cuda": 74, "cumul": 111, "current": [51, 55, 61, 66, 67, 68, 71, 85, 127, 144, 154, 155, 156, 160], "custom": [13, 16, 60, 61, 69, 77, 98, 110], "custom_measur": 61, "cut": 93, "cutoff": [46, 47, 95, 111], "cv": [54, 64, 90, 110, 126], "cv_calibr": [54, 55], "cv_glmnet": [62, 63, 64, 65, 110, 111, 143, 157], "cvar": [24, 29, 108, 133, 159], "cvar_0": 80, "cvar_1": 80, "d": [4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 51, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 116, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 148, 150, 151, 152, 153, 154, 155, 157, 158, 160], "d0": [80, 94, 143], "d0_true": 94, "d0cdb0ea4795": 64, "d1": [80, 92, 94, 143], "d10": 143, "d1_true": 94, "d2": [92, 143], "d21ee5775b5f": 64, "d2cml": 70, "d3": 143, "d4": 143, "d5": 143, "d5a0c70f1d98": 64, "d6": 143, "d7": 143, "d8": 143, "d9": 143, "d_": [35, 43, 62, 67, 75, 89, 111, 113, 117, 143], "d_0": [111, 118], "d_1": [92, 143], "d_2": 92, "d_col": [4, 5, 6, 7, 8, 9, 16, 59, 60, 62, 63, 64, 65, 68, 69, 70, 71, 76, 78, 79, 82, 83, 85, 89, 90, 91, 93, 95, 96, 97, 100, 101, 102, 103, 104, 106, 107, 110, 111, 112, 114, 116, 126, 127, 157, 159, 160], "d_i": [32, 33, 34, 36, 41, 42, 44, 45, 60, 65, 66, 75, 77, 80, 92, 94, 95, 99, 101, 111, 113, 125], "d_j": [75, 111, 118, 119, 143], "d_k": [111, 119, 143], "d_l": [111, 118], "d_w": 93, "da1440": 92, "dag": [65, 98, 99, 160], "dai": 68, "dark": [60, 77], "darkblu": 62, "darkr": 62, "dash": [65, 68, 71, 74, 75], "dat": [102, 107], "data": [0, 10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 52, 53, 61, 67, 81, 86, 92, 96, 100, 102, 104, 105, 106, 108, 109, 110, 112, 114, 115, 116, 117, 126, 143, 147, 148, 149, 150, 158, 159], "data_apo": [74, 75], "data_cvar": 91, "data_dict": [46, 78, 79, 82, 83, 84, 95, 111], "data_dml": 97, "data_dml_bas": [63, 78, 79, 82, 83, 90, 91, 93], "data_dml_base_iv": [63, 90, 91], "data_dml_flex": [63, 90], "data_dml_flex_iv": 63, "data_dml_iv_flex": 90, "data_dml_new": 93, "data_fram": 160, "data_lqt": 91, "data_pq": 91, "data_qt": 91, "data_transf": [62, 89, 90], "dataclass": 55, "datafram": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 50, 51, 62, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 103, 104, 105, 106, 109, 110, 111, 113, 127, 143, 144, 145, 150, 157, 160], "dataset": [0, 5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 60, 61, 65, 66, 68, 71, 74, 75, 77, 78, 79, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 144, 150, 157, 159, 160], "datatyp": [70, 159], "date": [16, 17, 19, 110], "date_format": 16, "datetim": [7, 16, 17, 19, 68, 69, 71, 104, 107], "datetime64": [68, 104, 107], "datetime_unit": [7, 16, 68, 71, 104, 107, 111, 112, 114, 116], "david": 159, "db": [63, 90, 91, 97, 160], "dbl": [61, 62, 63, 64, 102, 107, 143, 157, 160], "dc13a11076b3": 64, "ddc9": 64, "de": [59, 76, 158], "deal": [59, 76], "debias": [10, 11, 41, 43, 44, 62, 81, 89, 96, 108, 110, 126, 155, 158, 159], "debt": [63, 90, 91], "decai": [65, 99], "decid": [63, 90, 96], "decis": [26, 59, 63, 76, 90, 91, 109, 111, 158, 160], "decision_effect": 59, "decision_impact": [59, 76], "decisiontreeclassifi": [26, 51, 90], "decisiontreeregressor": 90, "declar": 160, "decomposit": [111, 112], "decreas": 95, "dedic": [102, 106, 107], "deep": [48, 49, 52, 53, 88], "deeper": 26, "def": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 68, 71, 77, 80, 86, 88, 89, 92, 93, 94, 96, 98, 110, 127], "default": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 61, 62, 65, 66, 68, 69, 70, 71, 82, 83, 86, 89, 93, 95, 97, 98, 99, 100, 103, 107, 109, 110, 111, 126, 143, 144, 150, 151, 157, 160], "default_arg": [68, 71], "default_convert": 89, "default_jitt": 16, "defier": [95, 111], "defin": [17, 18, 19, 31, 32, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 46, 47, 49, 53, 60, 63, 64, 66, 69, 74, 75, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 90, 91, 93, 94, 95, 96, 97, 98, 109, 110, 111, 113, 114, 116, 117, 125, 127, 128, 130, 131, 144, 145, 150, 154, 159], "definit": [45, 68, 71, 82, 83, 85, 111, 115, 144, 151, 152], "degre": [47, 63, 78, 79, 85, 89, 90, 95, 109, 144, 145], "dekel": 158, "delete_origin": 64, "deliber": 92, "delta": [16, 42, 61, 66, 68, 71, 98, 111, 113, 114, 115, 116, 117, 127, 129, 131], "delta_bench": 98, "delta_i": 61, "delta_j": 42, "delta_t": [17, 19, 68, 71], "delta_theta": [56, 75, 84, 97, 98, 144, 145], "delta_v": 98, "demand": [62, 89, 144, 145], "demir": [10, 11, 44, 62, 81, 89, 96, 101, 126, 155, 158], "demo": [69, 98], "demonstr": [60, 61, 62, 69, 74, 77, 89, 95, 98, 102, 107, 111, 143, 155, 157], "deni": 158, "denomin": [144, 145, 151, 152], "denot": [19, 38, 62, 63, 65, 66, 67, 71, 89, 90, 95, 98, 99, 109, 111, 113, 114, 116, 117, 118, 123, 127, 144, 145, 150, 152, 154], "dens_net_tfa": 63, "densiti": [27, 28, 29, 60, 65, 74, 75, 77], "dep": 72, "dep1": [64, 72, 102, 107, 157], "dep2": [64, 72, 102, 107, 157], "depend": [17, 19, 22, 24, 26, 27, 29, 32, 41, 64, 66, 68, 71, 78, 79, 82, 83, 84, 86, 87, 88, 93, 95, 100, 109, 110, 111, 114, 115, 116, 127, 136, 137, 144, 145, 151, 154, 157, 158], "deprec": [4, 14, 16, 22, 24, 25, 26, 27, 28, 29, 30, 66, 67, 68, 69, 70, 71, 78, 100, 102, 107, 111, 126, 127, 144], "deprecationwarn": [67, 78], "depreci": 159, "depth": [26, 51, 63, 64, 93, 100, 109, 110, 111, 126, 127, 143, 157, 160], "deriv": [12, 14, 15, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 111, 143], "describ": [13, 61, 62, 68, 71, 89, 90, 91, 98, 110, 126, 127, 138, 156, 159], "descript": [19, 63, 70, 72, 97, 110, 126, 127, 129, 131, 144, 145, 147, 149], "deserv": 111, "design": [8, 17, 19, 41, 46, 47, 74, 75, 88, 105, 107, 108, 158, 159], "design_info": [78, 79], "design_matrix": [78, 79, 109], "desir": [40, 64, 93, 111, 156], "detail": [12, 14, 15, 20, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 60, 63, 64, 66, 67, 74, 75, 77, 81, 88, 91, 95, 97, 98, 101, 102, 107, 109, 110, 112, 113, 114, 115, 116, 117, 125, 127, 133, 135, 136, 137, 141, 142, 143, 144, 145, 147, 149, 154, 155, 156, 157, 159, 160], "determin": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 41, 49, 53, 63, 71, 80, 90, 91, 94, 95, 97, 111, 143, 144, 154], "determinist": [93, 95, 109, 111], "deutsch": 155, "dev": [156, 159], "develop": [61, 62, 64, 89, 98, 111, 113, 117, 159], "deviat": [86, 111, 144, 154], "devic": 74, "dezeur": 158, "df": [5, 6, 7, 8, 9, 16, 59, 60, 62, 65, 67, 68, 71, 74, 75, 76, 78, 79, 80, 85, 89, 92, 94, 95, 97, 98, 99, 101, 103, 104, 106, 107, 109, 111, 112, 114, 116], "df_agg": 81, "df_all_apo": 74, "df_all_at": 74, "df_anticip": [68, 71], "df_apo": [74, 75], "df_apo_ci": 75, "df_apos_ci": 75, "df_ate": [74, 75], "df_bench": 98, "df_binari": 98, "df_bonu": [64, 102, 107, 157], "df_capo0": 85, "df_capo1": 85, "df_cate": [78, 79, 85], "df_causal_contrast_c": 85, "df_ci": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 50], "df_coef": 86, "df_cvar": 91, "df_fuzzi": 95, "df_lqte": 91, "df_ml_g0": 86, "df_ml_g1": 86, "df_ml_m": 86, "df_pa": [66, 99], "df_perform": 74, "df_plot": 62, "df_post_treat": [68, 71], "df_pq": 91, "df_qte": 91, "df_result": 81, "df_sharp": 95, "df_sort": [74, 75], "df_summari": 90, "df_treat": 71, "df_wide": 89, "dfg": 155, "dgp": [17, 18, 19, 62, 65, 67, 68, 71, 80, 81, 89, 92, 93, 94, 98, 99], "dgp1": [17, 18, 19], "dgp2": [17, 18, 19], "dgp3": [17, 18, 19], "dgp4": [17, 18, 19], "dgp5": [17, 18, 19], "dgp6": [17, 18, 19], "dgp_confounded_irm_data": 98, "dgp_dict": 98, "dgp_tpye": 66, "dgp_type": [17, 18, 19, 66, 68, 71], "diagnost": 69, "diagon": 98, "diagram": [59, 76, 111], "dichotom": [59, 76], "dict": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 50, 51, 52, 53, 56, 68, 71, 78, 79, 81, 88, 98, 110], "dict_kei": [144, 150], "dict_rdd": [105, 107], "dictionari": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 35, 37, 38, 39, 40, 47, 56, 68, 71, 78, 79, 82, 83, 97, 109, 110, 111, 112, 144, 150], "dictonari": [63, 90], "did": [0, 5, 7, 60, 66, 67, 68, 69, 70, 71, 74, 89, 103, 104, 107, 108, 112, 113, 114, 116, 117, 127, 129, 131, 159, 160], "did_aggreg": [68, 70, 71], "did_data": 67, "did_multi": [111, 112], "diff": 90, "differ": [5, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 37, 38, 39, 40, 59, 60, 62, 63, 64, 65, 68, 70, 71, 74, 75, 76, 77, 80, 82, 83, 84, 85, 88, 89, 90, 91, 92, 94, 95, 97, 98, 99, 100, 103, 107, 108, 109, 110, 112, 113, 114, 116, 117, 126, 128, 129, 130, 131, 156, 157, 158, 159, 160], "differenti": 111, "difficult": 98, "dillon": 158, "dim": [47, 63], "dim_x": [12, 14, 15, 17, 19, 22, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 47, 60, 62, 64, 77, 85, 86, 87, 88, 89, 101, 109, 110, 111, 144, 150], "dim_z": [38, 42, 111], "dimens": [17, 19, 32, 43, 62, 89, 93, 126], "dimension": [13, 32, 34, 37, 38, 39, 81, 96, 109, 111, 122, 123, 124, 126, 143, 144, 150, 157, 158], "direct": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 60, 65, 67, 77, 85, 99, 101, 111, 160], "directli": [46, 60, 61, 63, 75, 77, 86, 97, 101, 102, 107, 144, 150, 157, 160], "discontinu": [8, 46, 47, 105, 107, 108, 158, 159], "discret": [23, 35, 74, 75, 89, 111, 118, 159], "discretis": 91, "discuss": [33, 62, 63, 89, 90, 111, 112, 117, 158, 159, 160], "disjoint": [62, 82, 83, 89], "displai": [13, 62, 68, 69, 70, 71, 74, 75, 89, 98, 109, 110, 144, 150], "displot": 90, "disproportion": [63, 90], "disregard": [49, 53], "dist": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "distinct": [102, 107], "distinguish": [16, 68, 71], "distr": 110, "distribut": [47, 60, 66, 74, 75, 77, 86, 98, 101, 111, 113, 117, 144, 152, 156, 158, 159], "diverg": [46, 60, 77, 101], "divid": [68, 71, 111], "dmatrix": [78, 79, 109], "dml": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 59, 60, 64, 65, 69, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 127, 143, 144, 150, 156], "dml1": [108, 157, 159, 160], "dml2": [59, 62, 64, 65, 72, 89, 108, 111, 127, 143, 157, 159, 160], "dml_apo": 85, "dml_apo_obj": 111, "dml_apos_att": 85, "dml_apos_obj": 111, "dml_combin": 143, "dml_cover": 96, "dml_cv_predict": 159, "dml_cvar": [80, 91], "dml_cvar_0": 80, "dml_cvar_1": 80, "dml_cvar_obj": [24, 109], "dml_data": [7, 16, 61, 62, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 84, 85, 86, 89, 92, 96, 97, 98, 99, 103, 104, 105, 106, 107, 109, 110, 111, 112, 114, 116, 143, 160], "dml_data_anticip": [68, 71], "dml_data_arrai": [106, 107], "dml_data_bench": 98, "dml_data_bonu": [64, 157], "dml_data_df": 160, "dml_data_fuzzi": 95, "dml_data_lasso": 72, "dml_data_sharp": 95, "dml_data_sim": [64, 157], "dml_df": [62, 89], "dml_did": [66, 67], "dml_did_obj": [12, 15, 16, 111, 112, 113, 114, 116], "dml_iivm": 96, "dml_iivm_boost": [63, 90], "dml_iivm_forest": [63, 90], "dml_iivm_lasso": [63, 90], "dml_iivm_obj": [25, 76, 111], "dml_iivm_tre": [63, 90], "dml_irm": [78, 82, 85, 86, 93], "dml_irm_at": 84, "dml_irm_att": 85, "dml_irm_boost": [63, 90], "dml_irm_forest": [63, 90], "dml_irm_gat": 84, "dml_irm_gatet": 84, "dml_irm_lasso": [63, 72, 90], "dml_irm_new": 93, "dml_irm_obj": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 97, 109, 110, 111], "dml_irm_obj_ext": 110, "dml_irm_rf": 72, "dml_irm_tre": [63, 90], "dml_irm_weighted_att": 85, "dml_kwarg": 85, "dml_length": 96, "dml_long": 56, "dml_lplr": 87, "dml_lplr_obj": [37, 111], "dml_lpq_0": 94, "dml_lpq_1": 94, "dml_lpq_obj": [27, 109], "dml_lqte": [91, 94], "dml_obj": [61, 68, 69, 70, 71, 74, 75, 97, 98], "dml_obj_al": [68, 71], "dml_obj_anticip": [68, 71], "dml_obj_bench": 98, "dml_obj_lasso": 69, "dml_obj_linear": [68, 71], "dml_obj_linear_logist": 69, "dml_obj_nyt": [68, 71], "dml_obj_univers": [68, 71], "dml_pliv": [62, 89], "dml_pliv_obj": [38, 62, 89, 111], "dml_plr": [79, 83, 143], "dml_plr_1": 143, "dml_plr_2": 143, "dml_plr_boost": [63, 90], "dml_plr_forest": [63, 90, 160], "dml_plr_lasso": [63, 72, 90], "dml_plr_no_split": 126, "dml_plr_obj": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 97, 100, 109, 110, 111, 126, 127, 143, 144, 145, 150], "dml_plr_obj_extern": 126, "dml_plr_obj_intern": 126, "dml_plr_obj_onfold": 88, "dml_plr_obj_untun": 88, "dml_plr_rf": 72, "dml_plr_tree": [63, 90, 160], "dml_pq_0": [91, 94], "dml_pq_1": [91, 94], "dml_pq_obj": [28, 109], "dml_procedur": [72, 100, 157, 159, 160], "dml_qte": [91, 94], "dml_qte_obj": [29, 109], "dml_robust_confset": 96, "dml_robust_length": 96, "dml_short": 56, "dml_ssm": [65, 99, 111], "dml_standard_ci": 96, "dml_tune": 159, "dmldummyclassifi": 110, "dmldummyregressor": 110, "dmlmt": 158, "dnorm": 60, "do": [61, 62, 63, 64, 68, 69, 71, 85, 86, 89, 90, 91, 92, 98, 109, 110, 126, 144, 154, 157, 160], "doabl": 127, "doc": [59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 155, 159], "docu": 159, "document": [67, 68, 69, 70, 71, 73, 74, 78, 79, 82, 83, 85, 87, 88, 98, 111, 117, 127, 144, 155, 159], "doe": [16, 23, 29, 61, 62, 63, 67, 68, 71, 74, 75, 85, 89, 90, 92, 95, 97, 98, 111, 127, 131, 144, 154, 160], "doesn": [59, 76], "doi": [10, 11, 17, 18, 19, 31, 33, 36, 40, 41, 43, 44, 61, 62, 64, 81, 89, 98, 101, 110, 126, 143, 155, 157, 159], "domain": 93, "don": [61, 88], "done": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 69, 88, 91, 110, 126, 144, 145], "dosag": [74, 75], "dot": [30, 67, 68, 71, 93, 102, 107, 109, 110, 111, 115, 117, 118, 143, 157], "doubl": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 41, 43, 44, 45, 46, 63, 69, 81, 86, 88, 90, 92, 96, 108, 110, 126, 127, 143, 144, 145, 159], "double_ml": [68, 96], "double_ml_bonus_data": 72, "double_ml_data_from_data_fram": [60, 101, 102, 107, 160], "double_ml_data_from_matrix": [61, 64, 102, 107, 110, 143, 157], "double_ml_framework": [111, 112], "double_ml_irm": 72, "double_ml_score_mixin": 0, "doubleiivm": 155, "doubleml": [0, 60, 62, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 91, 92, 93, 94, 95, 96, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 126, 127, 144, 150, 157, 158, 159], "doubleml2022python": 155, "doubleml2024r": 155, "doubleml_did_eval_linear": 61, "doubleml_did_eval_rf": 61, "doubleml_did_linear": 61, "doubleml_did_rf": 61, "doubleml_framework": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "doublemlapo": [74, 75, 85, 111, 127, 132, 159], "doublemlblp": [22, 26, 39, 78, 79, 85, 109, 159], "doublemlclusterdata": [0, 102, 107], "doublemlcvar": [80, 109, 127, 133, 159], "doublemldata": [0, 4, 7, 10, 11, 12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 59, 62, 64, 65, 66, 67, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 108, 109, 110, 111, 113, 126, 127, 143, 144, 150, 159, 160], "doublemldid": [66, 67, 111, 113, 127, 130, 159], "doublemldidaggreg": [68, 69, 70, 71, 111, 112], "doublemldidbinari": 67, "doublemldidc": [66, 111, 113, 127, 128, 159], "doublemldiddata": [0, 12, 15, 18, 66, 67, 103, 111, 113], "doublemldidmulti": [68, 69, 70, 71, 111, 112, 114, 115, 116, 127, 129, 131, 159], "doublemlframework": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 68, 69, 70, 71, 111, 112, 126, 143, 159], "doublemlframwork": 23, "doublemlidid": [111, 113], "doublemlididc": [111, 113], "doublemliivm": [59, 63, 76, 90, 96, 110, 111, 126, 127, 134, 159], "doublemlirm": [12, 14, 15, 22, 24, 25, 27, 28, 30, 37, 38, 39, 61, 63, 72, 75, 78, 82, 84, 85, 86, 90, 92, 93, 97, 98, 109, 110, 111, 126, 127, 135, 155, 159], "doublemllplr": [87, 111, 127, 138, 159], "doublemllpq": [94, 109, 127, 136, 159], "doublemlpaneldata": [0, 14, 16, 67, 69, 70, 104, 111, 112, 114, 115, 116, 159], "doublemlpliv": [110, 111, 126, 127, 139, 155, 159], "doublemlplr": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 60, 63, 64, 72, 77, 79, 83, 88, 90, 92, 97, 100, 101, 109, 110, 111, 126, 127, 140, 143, 144, 150, 155, 157, 159, 160], "doublemlpolicytre": [26, 109], "doublemlpq": [91, 94, 109, 127, 137, 159], "doublemlqt": [80, 91, 94, 109, 143, 159], "doublemlrdddata": [0, 46, 95, 105, 111, 159], "doublemlresampl": [85, 86], "doublemlsmm": 159, "doublemlssm": [65, 99, 111, 127, 141, 142, 159], "doublemlssmdata": [0, 30, 36, 99, 106, 111, 159], "doubli": [18, 31, 40, 61, 69, 96, 158], "doudou": [41, 158], "down": [87, 98], "download": [59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 102, 107, 156, 157], "download_fil": 69, "downward": 98, "dpg_dict": 97, "dpi": [60, 77, 92], "dr": [111, 115], "dramat": 61, "draw": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 98, 126, 159], "draw_sample_split": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 85, 86, 126], "drawn": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 47, 63, 68, 71, 90, 91, 93, 126], "drive": [60, 77, 101], "driven": [98, 160], "drop": [61, 88, 89, 92, 102, 107, 110, 127, 128, 129, 130, 131, 143], "dropna": [68, 71], "dt": [68, 127, 128, 144, 146], "dt_bonu": [102, 107], "dta": [61, 70], "dtrain": 90, "dtype": [68, 69, 70, 71, 72, 74, 75, 82, 83, 84, 87, 89, 90, 91, 96, 97, 102, 104, 107, 109, 157], "dualiti": 89, "dubourg": [155, 157], "duchesnai": [155, 157], "due": [60, 61, 69, 77, 78, 79, 84, 87, 97, 98, 101, 111, 125, 144, 145, 159, 160], "duflo": [10, 11, 44, 62, 81, 89, 96, 101, 126, 155, 158], "dummi": [22, 26, 39, 48, 49, 50, 69, 88, 98, 109, 110, 111, 113, 159], "dummyclassifi": [48, 69], "dummyregressor": [49, 69], "duplic": 159, "durabl": [64, 72, 102, 107, 157], "durat": 11, "dure": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 62, 63, 64, 65, 88, 89, 90, 110, 126, 157, 159, 160], "dx": 33, "dynam": [61, 158], "e": [6, 7, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 35, 36, 37, 38, 39, 40, 44, 46, 48, 49, 52, 53, 60, 61, 62, 63, 65, 66, 68, 69, 70, 71, 74, 75, 77, 78, 79, 81, 84, 86, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 101, 104, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160], "e20ea26": 64, "e401": [63, 90, 91, 97, 160], "e4016553": 160, "e45228": 92, "e57c": 64, "each": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 48, 49, 52, 53, 62, 64, 67, 68, 69, 70, 71, 74, 75, 82, 83, 86, 87, 88, 89, 91, 92, 93, 96, 97, 98, 100, 102, 104, 107, 110, 111, 117, 126, 143, 144, 150, 160], "earlier": [68, 71, 160], "earn": [63, 90, 91], "earner": [63, 90, 97], "easi": [64, 96, 127], "easier": 88, "easili": [64, 86, 88, 91, 159], "ec973f": 92, "ecolor": [67, 74, 75, 90, 92], "econ": 158, "econml": 158, "econom": [36, 42, 43, 45, 62, 81, 89, 92, 98, 126, 158], "econometr": [10, 11, 17, 18, 19, 31, 40, 41, 44, 45, 61, 62, 81, 89, 96, 101, 155, 158], "econometrica": [34, 62, 89, 92, 96, 101, 158], "ecosystem": [155, 160], "ectj": [10, 11, 41, 44, 62, 81, 89, 101, 155], "ed": 158, "edge_color": 77, "edgecolor": 77, "edit": [156, 158], "edu": [155, 157], "educ": [63, 90, 91, 97, 160], "ee97bda7": 64, "effect": [12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 47, 48, 49, 52, 53, 59, 60, 61, 62, 64, 65, 66, 67, 75, 76, 77, 81, 84, 89, 93, 95, 96, 99, 101, 108, 110, 112, 113, 115, 117, 119, 120, 121, 125, 126, 127, 135, 143, 144, 145, 157, 158, 159, 160], "effici": [74, 111, 158], "effort": 127, "eight": [62, 89], "either": [12, 14, 15, 16, 17, 19, 22, 24, 25, 26, 27, 28, 30, 32, 37, 38, 39, 64, 67, 68, 71, 81, 93, 95, 109, 110, 111, 114, 117, 160], "elapsed_tim": 74, "eleanor": 158, "element": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 65, 66, 68, 69, 70, 71, 78, 79, 80, 86, 89, 91, 94, 97, 99, 111, 114, 115, 116, 127, 128, 129, 130, 131, 132, 138, 144, 147, 149, 150, 153, 154, 159], "element_text": [62, 63], "elementari": 158, "elif": [82, 83, 93], "elig": [91, 97, 160], "eligibl": [63, 90, 97], "ell": [60, 62, 77, 81, 89, 101, 127, 139, 140, 157], "ell_0": [25, 38, 39, 60, 77, 81, 88, 101, 111, 120, 127, 140], "ell_1": 69, "ell_2": 86, "els": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 50, 61, 62, 63, 67, 74, 82, 83, 89, 93, 98], "em": 158, "emphas": [62, 89], "empir": [20, 21, 60, 62, 69, 77, 89, 92, 98, 101, 111, 114, 115, 116, 126, 127, 143], "emploi": [62, 81, 89, 98, 127, 134], "employ": [63, 69, 90, 91], "employe": 160, "empti": 89, "emul": [144, 145], "enabl": [13, 74, 75, 93, 97, 109, 144, 145, 159], "enable_metadata_rout": [48, 49, 52, 53], "encapsul": [48, 49, 52, 53, 110], "encod": 92, "encount": [71, 74], "end": [33, 41, 42, 43, 60, 61, 62, 63, 65, 67, 68, 71, 77, 80, 81, 86, 89, 90, 92, 93, 94, 99, 100, 102, 107, 110, 111, 114, 116, 117, 126, 127, 129, 131, 143, 157, 160], "end_tim": 74, "endogen": [63, 90, 91, 160], "enet_coordinate_descent_gram": 89, "enforc": 69, "engin": [64, 158], "enrol": [63, 90, 91], "ensembl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 72, 74, 75, 77, 78, 79, 82, 83, 84, 86, 87, 90, 93, 97, 98, 100, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 144, 150, 157, 160], "ensemble_learner_pipelin": 110, "ensemble_pipe_classif": 64, "ensemble_pipe_regr": 64, "ensur": [52, 53, 62, 74, 85, 88, 89, 93, 96, 105, 107], "enter": 111, "entir": [16, 60, 63, 77, 90, 101, 144, 145], "entri": [12, 14, 15, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 47, 60, 62, 68, 69, 70, 71, 72, 74, 75, 77, 84, 87, 89, 90, 91, 97, 101, 102, 104, 107, 110, 155, 157, 159], "enumer": [67, 74, 75, 80, 82, 83, 86, 89, 90, 91, 94, 100, 110, 126], "env": 156, "environ": 156, "ep": 92, "epanechnikov": 46, "epsilon": [63, 66, 67, 80, 90, 94, 109, 111, 113, 117], "epsilon_": [62, 67, 68, 71, 89], "epsilon_0": 47, "epsilon_1": 47, "epsilon_i": [32, 80, 92, 93, 94], "epsilon_sampl": 93, "epsilon_tru": [80, 94], "eqnarrai": 63, "equal": [13, 17, 19, 22, 26, 62, 65, 68, 71, 89, 92, 96, 99, 109, 110, 111, 144, 152], "equat": [47, 62, 63, 89, 90, 98, 100, 143, 160], "equilibrium": [62, 89], "equiv": [111, 117, 127, 129, 131], "equival": [81, 85, 126], "err": [12, 15, 16, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 66, 68, 69, 70, 71, 72, 74, 75, 76, 78, 79, 80, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 94, 97, 98, 99, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 157, 160], "error": [5, 6, 7, 8, 9, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 52, 53, 59, 60, 61, 63, 64, 65, 67, 74, 77, 81, 82, 83, 86, 88, 90, 95, 98, 101, 110, 111, 123, 124, 126, 127, 143, 144, 150, 157, 159, 160], "error_on_convergence_failur": 37, "errorbar": [67, 74, 75, 82, 83, 88, 90, 92, 95], "erstellt": [62, 63, 64], "es_linear_logist": 69, "es_rf": 69, "escap": 89, "esim": 95, "especi": [86, 88, 102, 107], "essenti": 98, "est": 95, "est_bound": 16, "est_method": 61, "esther": [126, 158], "estim": [10, 12, 13, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 46, 48, 49, 50, 51, 52, 53, 60, 61, 62, 64, 67, 75, 77, 78, 79, 80, 82, 83, 85, 86, 87, 89, 93, 95, 96, 100, 101, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 121, 128, 129, 130, 133, 136, 137, 138, 142, 144, 145, 150, 155, 158, 159], "estimand": 96, "estimator_list": 88, "et": [10, 11, 32, 34, 41, 43, 44, 60, 62, 63, 64, 66, 77, 78, 79, 80, 81, 82, 83, 86, 87, 89, 90, 91, 94, 97, 101, 111, 113, 117, 126, 127, 133, 135, 136, 137, 138, 143, 144, 145, 154, 155, 157, 159], "eta": [20, 21, 60, 62, 63, 67, 88, 89, 90, 94, 95, 100, 109, 111, 114, 115, 116, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 154, 157, 160], "eta1": 92, "eta2": 92, "eta_": [143, 144, 154], "eta_0": [46, 100, 111, 114, 115, 116, 127, 143], "eta_d": [95, 111], "eta_i": [17, 19, 32, 67, 68, 71, 93, 94, 95, 111], "eta_sampl": 93, "eta_tru": 94, "etc": [12, 13, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 86, 88, 89, 159], "ev": [60, 77, 101], "eval": [16, 64, 68, 69, 70, 71, 110, 111, 114, 115, 116, 127, 129, 131], "eval_metr": [63, 90, 160], "eval_pr": 61, "eval_predict": 61, "evalu": [12, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 34, 37, 38, 39, 61, 64, 67, 68, 69, 70, 71, 78, 79, 80, 84, 85, 91, 94, 97, 100, 111, 115, 117, 158, 159], "evaluate_learn": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 86, 88, 110, 159], "evalut": 110, "even": [63, 64, 68, 71, 90, 92, 95, 110, 111, 160], "event": [111, 112], "eventstudi": [16, 68, 69, 70, 71, 111, 112], "eventu": [62, 89], "everi": [62, 69, 89], "everyth": 155, "evid": [84, 88], "exact": [85, 98], "exactli": [95, 98, 111], "examin": 74, "exampl": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 46, 54, 55, 59, 60, 63, 64, 65, 66, 67, 68, 70, 71, 72, 74, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 109, 110, 111, 112, 114, 115, 116, 125, 126, 127, 143, 144, 150, 155, 157, 159, 160], "example_attgt": 61, "example_attgt_dml_eval_linear": 61, "example_attgt_dml_eval_rf": 61, "example_attgt_dml_linear": 61, "example_attgt_dml_rf": 61, "except": [49, 53, 81, 98, 159], "excess": 86, "exclud": 56, "exclus": [22, 26, 39, 82, 83, 109], "execut": [64, 160], "exemplarili": 157, "exemplatori": 93, "exhaust": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 110], "exhibit": [62, 89], "exist": [48, 49, 52, 53, 85, 111, 113, 117, 144, 154], "exogen": [63, 90, 91, 111, 160], "exp": [17, 18, 19, 31, 32, 34, 35, 40, 41, 44, 60, 67, 77, 78, 79, 82, 83, 92, 93, 101], "expect": [31, 37, 40, 49, 53, 61, 65, 66, 68, 69, 71, 75, 84, 86, 87, 88, 95, 98, 99, 105, 106, 107, 109, 111, 126, 143, 144, 151, 157], "experi": [11, 33, 34, 60, 63, 77, 90, 96, 98, 101, 102, 107, 126, 157, 158], "experiment": [12, 14, 15, 16, 17, 18, 19, 127, 128, 129, 130, 131, 144, 146, 147, 148, 149], "expertis": 98, "expit": [37, 87, 111, 122, 127, 138], "explain": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 97, 144, 145, 153, 154], "explan": [62, 66, 89, 97, 144, 153, 155, 160], "explanatori": [98, 143], "explicit": 98, "explicitli": [68, 70, 71, 84, 160], "exploit": [60, 77, 101, 111, 160], "explor": 88, "exponenti": 143, "export": [88, 159], "expos": [102, 103, 105, 106, 107, 111, 117], "exposur": [7, 17, 19, 67, 68, 69, 70, 71], "express": [62, 81, 95, 144, 154], "ext": 16, "extend": [87, 98, 103, 106, 107, 110, 155, 159], "extendend": [144, 154], "extens": [110, 127, 155, 158, 159], "extent": 81, "extern": [60, 77, 85, 88, 108, 144, 145, 159], "external_predict": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 77, 110], "externalptr": 63, "extra": 64, "extract": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 74, 88], "extralearn": 64, "extrem": [54, 55, 63, 90], "extreme_threshold": [54, 55], "ey": 81, "ezequiel": 159, "f": [63, 64, 66, 67, 68, 71, 74, 75, 77, 80, 81, 86, 89, 90, 91, 93, 94, 96, 97, 98, 99, 110, 144, 154, 155, 157], "f00584a57972": 64, "f1718fdeb9b0": 64, "f2e7": 64, "f3d24993": 64, "f6ebc": 92, "f_": [17, 18, 19, 31, 67, 109], "f_loc": [80, 94], "f_p": 67, "f_scale": [80, 94], "f_t": [68, 71], "f_x": 111, "face_color": 77, "facet_wrap": 63, "facilit": 88, "fact": [63, 90, 91], "factor": [47, 60, 61, 62, 63, 64, 77, 86, 101, 110, 144, 147, 149, 160], "faculti": 158, "fail": 159, "failur": 37, "fair": 86, "fake": [59, 76, 96], "fall": 74, "fallback": 110, "fals": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 41, 46, 47, 48, 49, 50, 52, 53, 54, 55, 60, 63, 64, 65, 66, 68, 71, 74, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 93, 94, 95, 96, 98, 99, 102, 107, 110, 111, 114, 126, 127, 128, 129, 130, 131, 143, 144, 146, 147, 148, 149, 160], "famili": [63, 90, 110], "familiar": 96, "fanci": 61, "far": [63, 90], "farbmach": 33, "fast": [86, 93, 110], "faster": 81, "fb5c25fa": 64, "fc9e": 64, "fd8a": 64, "featur": [10, 11, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 49, 51, 53, 61, 67, 68, 71, 72, 74, 84, 85, 86, 90, 93, 95, 98, 105, 107, 109, 110, 111], "featureless": [64, 110], "features_bas": [63, 90, 91, 97], "features_flex": 63, "featureunion": 64, "februari": [68, 98], "feder": 69, "femal": [64, 72, 102, 107, 157], "fern\u00e1ndez": [34, 126, 158], "fetch": [63, 89, 90, 91, 102, 107], "fetch_401k": [63, 90, 91, 97, 160], "fetch_bonu": [64, 72, 102, 107, 157], "few": [63, 90, 91], "ff7f0e": 67, "field": [62, 89, 110, 160], "fifteenth": 158, "fifth": [62, 68, 71], "fig": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 67, 68, 69, 70, 71, 74, 75, 78, 79, 80, 81, 85, 86, 88, 91, 92, 94, 95, 98], "fig_al": 77, "fig_dml": 77, "fig_non_orth": 77, "fig_orth_nosplit": 77, "fig_po_al": 77, "fig_po_dml": 77, "fig_po_nosplit": 77, "figsiz": [13, 16, 67, 68, 71, 72, 74, 75, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95], "figur": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 44, 60, 62, 67, 68, 69, 71, 72, 74, 75, 77, 78, 79, 80, 81, 82, 83, 85, 87, 88, 89, 90, 91, 94, 98, 101], "figure_format": 92, "file": [10, 11, 69, 81, 92, 158, 159], "filenam": 60, "fill": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 60, 62, 63, 65, 66, 86, 90, 99], "fill_between": [78, 79, 80, 85, 91, 94], "fill_valu": 86, "fillna": 68, "filter": 64, "filterwarn": [74, 77, 86], "final": [60, 64, 65, 67, 68, 70, 71, 75, 77, 78, 79, 80, 82, 83, 84, 91, 94, 95, 99, 101, 111, 125, 127, 129, 131, 160], "final_estim": 95, "financi": [10, 97, 160], "find": [63, 66, 67, 74, 90, 98, 109, 110, 160], "finish": 64, "finit": [60, 63], "firm": [62, 89, 97], "firmid": 89, "first": [7, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 43, 46, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 75, 76, 77, 78, 79, 82, 83, 84, 85, 86, 87, 89, 90, 91, 93, 94, 95, 98, 99, 101, 109, 111, 112, 115, 117, 126, 143, 144, 150, 156, 157, 159, 160], "fit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 50, 51, 52, 53, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 108, 109, 110, 111, 112, 113, 114, 116, 117, 127, 130, 142, 143, 144, 145, 150, 155, 159, 160], "fit_arg": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39], "fit_transform": [85, 89, 90], "five": 89, "fix": [67, 68, 71, 86, 159], "flag": [18, 102, 107, 126, 156], "flake8": 159, "flamlclassifierdoubleml": 88, "flamlregressordoubleml": 88, "flatten": [88, 92], "flexibl": [46, 59, 61, 63, 64, 66, 76, 90, 111, 155, 158, 159, 160], "flexibli": [63, 69, 90, 97], "float": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 41, 46, 47, 48, 49, 50, 52, 53, 54, 55, 68, 69, 70, 71, 104, 107], "float32": [90, 91, 97], "float64": [68, 69, 70, 71, 72, 74, 75, 80, 82, 83, 84, 87, 89, 90, 97, 102, 104, 107, 110, 157], "floor": 64, "floor_divid": 89, "flt": 64, "flush": 60, "fmt": [67, 74, 75, 82, 83, 88, 90, 92, 95], "fobj": 90, "focu": [62, 63, 69, 85, 89, 90, 91, 98, 109, 111, 113, 117, 125, 160], "focus": [91, 97, 98, 160], "fold": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 59, 62, 63, 64, 65, 66, 68, 69, 70, 71, 86, 89, 90, 91, 97, 99, 100, 108, 110, 111, 113, 114, 116, 127, 130, 143, 157, 160], "follow": [17, 18, 19, 31, 32, 35, 40, 41, 60, 62, 63, 65, 66, 67, 68, 69, 70, 71, 77, 78, 79, 80, 82, 83, 87, 88, 89, 90, 91, 94, 95, 96, 97, 98, 99, 101, 102, 107, 109, 110, 111, 112, 114, 115, 116, 117, 126, 127, 129, 131, 138, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 160], "font_scal": [89, 90, 91], "fontsiz": [68, 71, 80, 87, 91, 94], "force_all_d_finit": [5, 6, 7, 8, 9, 102, 103, 107], "force_all_x_finit": [4, 5, 6, 7, 8, 9, 102, 107], "forest": [33, 59, 60, 61, 63, 64, 66, 74, 76, 77, 84, 86, 90, 97, 101, 110, 157, 160], "forest_summari": 90, "forg": [156, 158, 159], "form": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 40, 50, 52, 53, 63, 65, 66, 67, 78, 79, 80, 82, 83, 84, 86, 90, 94, 95, 97, 99, 109, 111, 112, 113, 117, 120, 121, 122, 123, 124, 127, 129, 131, 132, 135, 144, 145, 150, 151, 152, 153, 154, 156, 157], "format": [7, 16, 41, 69, 74, 77, 84, 144, 150, 159], "former": [69, 96], "formula": [62, 63, 89, 90, 95, 98, 159], "formula_flex": 63, "forschungsgemeinschaft": 155, "forthcom": [98, 158], "forum": 159, "forward": [26, 51], "found": [70, 78, 79, 81, 82, 83, 87, 88, 101, 102, 107, 110, 111, 125, 157], "foundat": [74, 155, 158], "four": [63, 74, 86, 90, 111, 114, 159], "fourth": [62, 89], "frac": [17, 18, 19, 21, 25, 31, 33, 34, 36, 40, 41, 42, 44, 45, 49, 53, 60, 62, 64, 67, 71, 77, 81, 84, 87, 89, 92, 95, 100, 101, 109, 111, 114, 115, 116, 120, 122, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 141, 142, 143, 144, 145, 146, 147, 148, 149, 151, 152, 153, 154], "fraction": [19, 64], "frame": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 50, 59, 60, 62, 63, 65, 68, 69, 70, 71, 72, 74, 75, 78, 79, 82, 83, 84, 87, 89, 90, 91, 92, 93, 97, 101, 102, 104, 107, 157, 160], "framealpha": [68, 71], "frameon": [68, 71, 87], "framework": [13, 16, 21, 60, 62, 64, 74, 77, 86, 88, 89, 92, 98, 101, 110, 143, 155, 157, 159, 160], "freez": 156, "fribourg": 158, "friendli": [68, 71, 74, 75], "from": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 50, 53, 54, 55, 59, 60, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 109, 110, 111, 112, 113, 114, 115, 116, 117, 126, 127, 143, 144, 150, 157, 159, 160], "from_arrai": [4, 5, 6, 7, 8, 9, 30, 46, 66, 67, 77, 80, 94, 101, 102, 103, 105, 106, 107, 110, 143, 157], "from_config": [54, 55], "from_product": 89, "front": 75, "fr\u00e9chet": [144, 154], "fs_kernel": [46, 111], "fs_specif": [46, 111], "fsize": [63, 90, 91, 97, 160], "full": [66, 67, 74, 75, 77, 80, 82, 83, 86, 90, 91, 94, 95, 96, 99, 101, 111], "fulli": [26, 63, 73, 88, 90, 96, 111, 121], "fun": 60, "func": 61, "function": [0, 4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 44, 45, 46, 59, 60, 63, 64, 65, 66, 68, 69, 70, 71, 74, 76, 77, 78, 79, 80, 85, 86, 87, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 108, 110, 111, 112, 113, 114, 115, 116, 117, 120, 122, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 146, 147, 148, 149, 154, 155, 158, 159, 160], "fund": [63, 90, 91, 155], "further": [13, 16, 17, 18, 19, 31, 32, 35, 40, 43, 62, 64, 65, 66, 67, 68, 69, 70, 71, 75, 78, 79, 80, 84, 85, 86, 87, 89, 91, 93, 94, 95, 97, 98, 99, 110, 111, 113, 116, 125, 127, 133, 136, 137, 138, 141, 142, 143, 144, 145, 150, 153, 154, 155, 157, 159, 160], "furthermor": [77, 104, 107, 127, 132, 135], "futur": [4, 14, 16, 22, 24, 25, 26, 27, 28, 29, 30, 68, 69, 70, 71, 78, 111, 127, 144], "futurewarn": [67, 68, 69, 70, 71], "fuzzi": [46, 47], "g": [6, 7, 12, 14, 15, 16, 17, 19, 22, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 46, 48, 49, 52, 53, 60, 61, 64, 66, 67, 68, 69, 70, 71, 72, 77, 78, 79, 81, 84, 86, 91, 92, 93, 96, 97, 99, 101, 104, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 127, 128, 129, 130, 131, 132, 134, 135, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160], "g0": 74, "g1": 74, "g_": [47, 75, 111, 116, 127, 128, 130, 131, 133, 136, 137, 143], "g_0": [12, 14, 15, 16, 22, 25, 26, 28, 38, 39, 44, 45, 46, 47, 60, 62, 63, 74, 77, 86, 89, 90, 101, 109, 110, 111, 118, 119, 120, 121, 123, 124, 127, 129, 131, 132, 140, 141, 142, 144, 151, 152, 154, 157, 160], "g_1": [47, 86], "g_all": [60, 63], "g_all_po": 60, "g_ci": 63, "g_d": [127, 133, 137], "g_dml": 60, "g_dml_po": 60, "g_hat": [38, 39, 60, 77, 127], "g_hat0": [25, 26], "g_hat1": [25, 26], "g_i": [17, 19, 111, 114, 116, 117, 127, 129, 131], "g_k": 109, "g_nonorth": 60, "g_nosplit": 60, "g_nosplit_po": 60, "g_valu": 14, "g_x": 67, "gain": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 56, 86, 144, 145, 152, 159], "gain_statist": 159, "galleri": [101, 109, 110, 111, 112, 114, 116, 125, 155, 159], "gama": 88, "gamma": [36, 42, 45, 62, 89, 92, 93, 95, 98, 111, 127, 133, 136], "gamma_0": [32, 65, 93, 99, 127, 133, 136], "gamma_a": [31, 40, 98], "gamma_bench": 98, "gamma_v": 98, "gap": [89, 98], "gapo": 22, "gate": [22, 26, 39, 50, 92, 93, 108, 159], "gate_obj": 109, "gatet": 109, "gaussian": [27, 28, 29, 60, 77, 101, 109, 110, 143, 158], "ge": [18, 31, 32, 84, 93, 109, 111, 115, 117], "geer": 158, "gelbach": [62, 89], "gener": [0, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 49, 53, 54, 59, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 102, 107, 108, 109, 110, 111, 114, 116, 117, 118, 126, 127, 129, 131, 132, 135, 143, 145, 146, 147, 148, 149, 151, 152, 154, 158, 159, 160], "generate_treat": 94, "generate_weakiv_data": 96, "geom_bar": 63, "geom_dens": [63, 65], "geom_errorbar": 63, "geom_funct": 60, "geom_histogram": 60, "geom_hlin": 63, "geom_point": 63, "geom_til": 62, "geom_vlin": [60, 65], "geq": [17, 19, 95, 111], "german": 155, "get": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 52, 53, 64, 68, 70, 71, 74, 75, 86, 92, 97, 98, 144, 145, 155, 156], "get_dummi": 92, "get_feature_names_out": [85, 89, 90], "get_legend_handles_label": [74, 75], "get_level_valu": 88, "get_logg": [60, 61, 62, 63, 64, 65, 100, 110, 111, 126, 127, 143, 157], "get_metadata_rout": [48, 49, 52, 53], "get_param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 52, 53, 88, 110], "get_ylim": 85, "ggdid": 61, "ggplot": [60, 62, 63, 65], "ggplot2": [60, 62, 63, 65], "ggsave": 60, "ggtitl": 63, "gh": 159, "git": 156, "github": [61, 63, 69, 81, 88, 92, 155, 158, 159], "githubusercont": [70, 81], "give": [63, 85, 90], "given": [12, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 34, 37, 38, 39, 40, 44, 45, 46, 47, 48, 49, 52, 53, 60, 62, 65, 67, 69, 70, 75, 77, 82, 83, 89, 91, 92, 95, 98, 99, 101, 109, 111, 114, 115, 116, 127, 132, 143, 144, 150, 151, 152, 153, 154, 157, 159], "glmnet": [63, 64, 110, 159], "global": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 52, 53, 110, 111], "globalclassifi": 95, "globallearn": 95, "globalregressor": 95, "glrn": 64, "glrn_lasso": 64, "gmm": 96, "gname": 61, "go": [78, 79, 81, 85, 88, 95, 98], "goal": [75, 82, 83, 111], "goe": 111, "goldman": 158, "good": [81, 87, 144, 145, 160], "gpu": 74, "gradient": [63, 90], "gradientboostingclassifi": 86, "gradientboostingregressor": 86, "gradual": 98, "gramfort": [155, 157], "graph": [64, 65, 99, 160], "graph_ensemble_classif": 64, "graph_ensemble_regr": 64, "graph_obj": 95, "graph_object": [78, 79, 81, 98], "graphlearn": [64, 110], "grasp": [75, 144, 145], "great": [67, 160], "greater": 160, "green": [60, 78, 79, 80, 94], "greg": 158, "grei": [63, 74, 75], "grenand": 158, "grey50": 62, "grid": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 62, 64, 68, 71, 74, 75, 78, 79, 80, 81, 85, 87, 91, 92, 94, 98, 110, 144, 150], "grid_arrai": [78, 79], "grid_basi": 85, "grid_bound": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 98], "grid_search": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 64, 110], "grid_siz": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 78, 79], "gridextra": 62, "gridsearchcv": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 110], "grisel": [155, 157], "grob": 62, "group": [7, 14, 16, 17, 19, 22, 26, 39, 59, 61, 69, 74, 75, 76, 84, 85, 91, 92, 93, 98, 108, 111, 112, 114, 115, 116, 117, 127, 129, 131, 144, 147, 149], "group_0": 109, "group_1": [82, 83, 109], "group_2": [82, 83, 109], "group_3": [82, 83], "group_effect": 93, "group_ind": 84, "group_treat": 84, "groupbi": [68, 71, 74, 81, 90, 96], "gruber": 33, "gt": [59, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 74, 75, 76, 78, 79, 80, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 102, 107, 157], "gt_combin": [16, 68, 69, 71, 111, 112, 114, 115, 116], "gt_dict": [68, 71], "guarante": [62, 89], "guber": 33, "guess": [97, 144, 145], "guid": [20, 21, 48, 49, 52, 53, 60, 61, 62, 64, 67, 68, 69, 70, 71, 75, 77, 84, 85, 89, 95, 97, 110, 155, 157, 159], "guidelin": 159, "gunion": [64, 110], "gxidclusterperiodytreat": 61, "h": [17, 18, 19, 31, 33, 40, 43, 61, 62, 89, 95, 96, 104, 107, 111, 158], "h20": 88, "h_0": [68, 71, 75, 84, 85, 97, 98, 144, 150, 160], "h_f": [46, 111], "ha": [12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 50, 51, 52, 53, 60, 61, 62, 63, 69, 71, 77, 81, 86, 88, 89, 90, 91, 92, 95, 96, 97, 98, 102, 107, 109, 110, 111, 113, 117, 144, 145, 150, 151, 152, 153, 154, 155, 160], "had": 69, "half": [60, 77, 92, 101, 126], "hand": [46, 86, 88, 92, 96, 160], "handbook": 92, "handl": [61, 69, 74, 75, 86, 103, 104, 107, 110, 159], "hansen": [10, 11, 34, 42, 44, 62, 81, 89, 96, 101, 155, 158], "happend": 86, "hard": [97, 144, 145], "harold": 158, "harsh": [48, 52], "hasn": [13, 16, 68, 70, 71], "hat": [60, 62, 77, 81, 84, 89, 92, 95, 100, 101, 109, 111, 126, 127, 143, 144, 145, 150, 153], "have": [16, 22, 23, 26, 29, 32, 35, 39, 50, 51, 52, 53, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 74, 75, 76, 78, 79, 84, 85, 86, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 102, 107, 109, 110, 111, 115, 127, 129, 131, 143, 144, 145, 151, 154, 156, 157, 159, 160], "hazlett": [98, 144, 145], "hc": [61, 158], "hc0": [50, 159], "hdm": [62, 89], "he": [65, 99], "head": [61, 62, 64, 68, 69, 70, 71, 72, 78, 79, 82, 83, 85, 88, 89, 90, 92, 95, 98, 102, 103, 107, 109, 157], "heat": [62, 89], "heatmap": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 62, 89, 98], "heavili": 86, "hei": 158, "height": [13, 16, 60, 62, 81, 88], "help": [61, 63, 69, 80, 86, 91, 93, 98, 111, 112, 126, 160], "helper": [103, 107, 159], "henc": [61, 63, 64, 90, 98, 110, 127, 160], "here": [27, 28, 29, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 78, 79, 80, 82, 83, 84, 86, 87, 89, 90, 91, 93, 94, 95, 98, 99, 102, 107, 110, 111, 114, 156], "herzig": 159, "heterogen": [17, 19, 26, 32, 41, 63, 84, 90, 91, 93, 108, 111, 121, 126, 158, 159, 160], "heteroskedast": [82, 83], "heurist": [60, 77, 101], "high": [34, 37, 38, 39, 63, 67, 81, 90, 91, 96, 100, 111, 122, 123, 124, 143, 155, 157, 158], "higher": [61, 63, 81, 90, 91, 92, 95, 96, 159, 160], "highli": [63, 90, 155], "highlight": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 66, 85, 88, 98, 159], "highlightcolor": [78, 79], "hint": 88, "hispan": 72, "hist": [74, 75], "hist_e401": 63, "hist_p401": 63, "histogram": [74, 75], "histplot": 77, "hjust": 63, "hline": [102, 107, 143, 157, 160], "hold": [36, 55, 62, 63, 65, 87, 88, 89, 90, 99, 109, 110, 111, 115], "holdout": [110, 126], "holm": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "home": [63, 90], "homogen": 111, "hook": 159, "hopefulli": 91, "horizont": [62, 67, 74, 89], "hostedtoolcach": [67, 68, 69, 70, 71, 89, 90, 95, 96, 98], "hot": 92, "hotstart_backward": [64, 110], "hotstart_forward": [64, 110], "household": [63, 90, 91, 97], "how": [13, 17, 19, 41, 48, 49, 52, 53, 59, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 101, 103, 107, 110, 111, 155, 156], "howev": [60, 63, 65, 77, 87, 88, 90, 95, 98, 99, 101, 111, 160], "hown": [63, 90, 91, 97, 160], "hpwt": [62, 89], "hpwt0": 62, "hpwtairmpdspac": 62, "href": 155, "hspace": 86, "hstack": [30, 67], "html": [64, 155, 157, 159], "http": [33, 45, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 110, 155, 156, 157, 159], "huber": [36, 65, 99, 111, 125, 127, 141, 142, 158], "hue": [68, 71, 90], "huge": 86, "hugo": 158, "husd": [64, 72, 102, 107, 157], "hyperparamet": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 63, 64, 72, 74, 81, 86, 88, 90, 108, 157], "hypothes": [143, 158], "hypothesi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 63, 90, 97, 144, 150, 158], "hypothet": 98, "i": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 142, 143, 144, 145, 147, 149, 150, 151, 152, 154, 155, 156, 157, 159, 160], "i0": [66, 67, 111, 113], "i03": 155, "i1": [66, 111, 113], "i_": [42, 89, 93], "i_1": [62, 89], "i_2": [62, 89], "i_3": [62, 89], "i_4": 67, "i_est": 77, "i_fold": 62, "i_k": [62, 89, 100, 126, 143], "i_learn": 86, "i_level": 75, "i_rep": [60, 65, 66, 77, 86, 99, 101], "i_split": 89, "i_train": 77, "icp": 158, "id": [7, 16, 61, 62, 64, 68, 69, 70, 71, 89, 104, 107, 111, 112, 114, 116, 144, 149], "id_col": [7, 16, 68, 69, 70, 71, 104, 107, 111, 112, 114, 116], "id_var": 89, "idea": [63, 64, 90, 91, 98, 110, 111, 144, 145, 160], "ident": [17, 18, 19, 31, 32, 35, 40, 42, 51, 64, 69, 75, 85, 88, 95, 110, 111, 117, 127, 135, 144, 150], "identfi": 98, "identif": [68, 69, 70, 71, 95, 96, 111, 160], "identifi": [7, 62, 63, 66, 69, 74, 84, 89, 90, 91, 95, 98, 103, 104, 105, 106, 107, 109, 111, 113, 115, 117, 125, 144, 154, 159], "identifii": 109, "idnam": 61, "idx_gt_att": 16, "idx_learn": 74, "idx_tau": [80, 91, 94], "idx_treat": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 74, 75, 144, 150], "ieee": 158, "ifels": 61, "ignor": [48, 49, 52, 53, 69, 74, 77, 86, 95], "ignore_index": 74, "ii": [62, 89], "iid": [68, 71, 111, 113], "iivm": [20, 21, 25, 33, 91, 100, 109, 120, 134, 155, 159], "iivm_summari": 90, "iivmglmnet": 63, "iivmrang": 63, "iivmrpart": 63, "iivmxgboost11861": 63, "ij": [43, 62, 65, 75, 89, 99], "ilia": 158, "illustr": [60, 62, 63, 64, 65, 66, 67, 69, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 89, 90, 91, 93, 94, 97, 98, 99, 101, 110, 160], "iloc": [66, 67, 68, 69, 70, 71, 74, 75, 86, 89, 92, 96], "immedi": 156, "immun": [126, 158], "impact": [59, 76, 86, 92, 97], "implement": [12, 14, 15, 16, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 52, 53, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 77, 81, 85, 86, 89, 90, 92, 95, 97, 98, 99, 101, 108, 109, 110, 112, 113, 115, 117, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 155, 157, 158, 159, 160], "impli": [31, 40, 62, 63, 68, 71, 89, 90, 91, 95, 109, 111, 117, 144, 146, 147, 148, 149, 151, 152], "implicitli": [111, 115], "implment": [67, 111, 112], "import": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 54, 55, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 144, 150, 156, 157, 159, 160], "importlib": 81, "impos": 98, "improv": [66, 68, 71, 86, 93, 111, 159], "in_sample_norm": [12, 14, 15, 16, 66, 127, 128, 129, 130, 131, 144, 146, 147, 148, 149], "inbuild": 86, "inbuilt": 86, "inc": [63, 90, 91, 97, 160], "includ": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 56, 61, 63, 67, 68, 69, 70, 71, 74, 75, 82, 83, 85, 90, 95, 97, 98, 109, 111, 143, 144, 150, 151, 152, 154, 156, 159, 160], "include_bia": [85, 89, 90], "include_never_tr": [17, 19, 71], "include_scenario": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 98], "incom": [63, 90, 91, 93, 97, 160], "incorpor": [64, 97, 144, 150], "increas": [19, 68, 71, 84, 86, 87, 89, 98, 160], "increment": 159, "ind": 90, "independ": [12, 14, 15, 16, 18, 31, 32, 40, 47, 62, 64, 67, 84, 87, 89, 93, 111, 117, 125, 127, 128, 129, 130, 131, 159], "index": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 60, 62, 67, 72, 74, 77, 81, 82, 83, 88, 89, 90, 92, 93, 101, 102, 107, 126, 127, 128, 129, 130, 131, 155, 157], "index_col": 81, "india": [126, 158], "indic": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 36, 37, 38, 39, 46, 50, 62, 63, 65, 67, 68, 69, 70, 71, 84, 89, 90, 91, 95, 96, 98, 99, 100, 102, 103, 106, 107, 109, 111, 113, 117, 118, 125, 126], "individu": [17, 19, 22, 26, 52, 53, 61, 63, 67, 68, 71, 74, 75, 82, 83, 84, 88, 90, 91, 95, 97, 109, 111, 160], "individual_df": 67, "induc": [108, 126], "industri": [62, 89], "inf": [5, 6, 7, 8, 9, 61, 68, 69, 70, 71, 96], "inf_model": 127, "infer": [34, 42, 59, 60, 62, 69, 74, 76, 77, 81, 86, 88, 89, 96, 101, 108, 111, 126, 155, 157, 158, 159], "inferenti": 160, "infinit": [5, 6, 7, 8, 9, 96, 102, 103, 107, 159], "influenc": [25, 49, 53, 74, 111], "info": [59, 64, 68, 69, 70, 71, 72, 74, 75, 84, 87, 88, 89, 90, 91, 97, 102, 104, 107, 157, 159, 160], "inform": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 50, 52, 53, 59, 64, 68, 69, 70, 71, 76, 78, 79, 86, 95, 96, 97, 98, 111, 112, 144, 145, 158], "infti": [60, 77, 101, 111, 117], "inher": 98, "inherit": [92, 103, 104, 105, 106, 107, 159], "initi": [4, 5, 6, 7, 8, 9, 12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 63, 64, 65, 66, 68, 69, 70, 71, 74, 80, 90, 91, 94, 95, 97, 98, 99, 102, 104, 107, 109, 110, 111, 126, 157, 159, 160], "inlin": [72, 92], "inlinebackend": 92, "inner": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 110], "innermost": 110, "input": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 49, 53, 64, 69, 97, 100, 110, 111, 115, 143, 144, 145, 150], "insensit": 111, "insid": [48, 49, 52, 53], "insight": [81, 98], "insignific": 97, "inspect": 157, "inspir": [31, 33, 34, 36, 68, 71, 98], "instabl": 55, "instal": [63, 74, 88, 95, 111, 159], "install_github": 156, "instanc": [52, 53, 63, 64, 74, 90, 110], "instanti": [62, 63, 89, 90, 110, 126], "instead": [4, 12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 52, 53, 59, 61, 63, 67, 68, 69, 70, 71, 74, 75, 76, 78, 84, 88, 90, 91, 102, 107, 109, 110, 111, 127, 131, 144, 146, 147, 148, 149, 152, 153, 159], "instruct": [156, 159], "instrument": [5, 6, 7, 8, 9, 10, 25, 33, 37, 38, 42, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 74, 75, 84, 87, 89, 90, 91, 94, 97, 99, 102, 104, 105, 106, 107, 110, 111, 113, 114, 116, 120, 123, 127, 136, 138, 143, 157, 160], "instrument_effect": 59, "instrument_impact": 76, "instrument_strength": 96, "insuffienct": 88, "int": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 41, 46, 47, 50, 51, 54, 61, 62, 63, 66, 69, 74, 76, 80, 93, 94, 96, 98, 99, 104, 107], "int32": 69, "int64": [68, 70, 71, 72, 89, 102, 104, 107, 157], "int8": [90, 91, 97], "integ": [18, 64, 110], "integr": [74, 88, 98, 144, 154, 159], "intend": [46, 64, 98, 160], "intent": [111, 160], "inter": 110, "interact": [22, 23, 25, 26, 31, 33, 34, 35, 46, 47, 75, 98, 108, 110, 120, 121, 151, 152, 155, 159, 160], "interchang": 143, "interest": [25, 26, 31, 37, 38, 39, 40, 60, 63, 65, 66, 77, 81, 87, 90, 91, 95, 96, 99, 101, 109, 111, 113, 115, 118, 119, 120, 121, 122, 123, 124, 125, 127, 143, 157, 160], "interfac": [37, 61, 63, 64, 74, 102, 107, 110, 126, 157], "intermedi": 98, "intern": [19, 37, 61, 63, 64, 75, 88, 91, 110, 111, 112, 158], "internet": [63, 90, 91], "interpret": [68, 69, 70, 71, 82, 83, 87, 96, 98, 109, 144, 145, 151, 152, 153, 154, 156, 160], "intersect": [98, 144, 150, 159], "interv": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 50, 61, 62, 63, 65, 66, 68, 69, 70, 71, 74, 75, 78, 79, 80, 82, 83, 85, 89, 91, 94, 95, 97, 99, 108, 109, 126, 127, 144, 150, 157, 158, 159, 160], "intial": 95, "introduc": [60, 77, 101, 102, 107, 143, 159, 160], "introduct": [60, 62, 64, 77, 89, 91, 97, 110, 111, 113, 117, 144, 145], "introductori": [61, 98], "intrument": [65, 99], "intspecifi": 46, "intuit": 98, "inuidur1": [64, 72, 102, 107, 157], "inuidur1femaleblackothracedep1dep2q2q3q4q5q6agelt35agegt54durablelusdhusdtg": [64, 102, 107, 157], "inuidur2": [72, 102, 107, 157], "inv_sigmoid": 92, "invalid": [60, 71, 77, 89, 96, 101], "invari": [111, 113, 117], "invers": [22, 24, 25, 26, 27, 28, 29, 30, 65, 99, 144, 151, 152], "invert": [25, 96], "invert_yaxi": 89, "investig": [81, 88, 98], "involv": [109, 110, 127, 160], "io": [92, 159], "ipw_norm": 159, "ipykernel_48215": 67, "ipykernel_50449": 78, "ipykernel_52203": 89, "ipynb": [59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], "ira": [63, 90, 91], "irm": [0, 9, 12, 14, 15, 20, 21, 37, 38, 39, 50, 51, 74, 75, 79, 83, 84, 86, 98, 99, 100, 106, 107, 108, 110, 114, 116, 121, 135, 151, 152, 155, 159, 160], "irm_summari": 90, "irmglmnet": 63, "irmrang": 63, "irmrpart": 63, "irmxgboost8047": 63, "irrespect": 98, "irrevers": [111, 117], "is_classifi": [12, 14, 15, 22, 25, 26, 39], "is_gat": [22, 26, 39, 50], "isfinit": [68, 69, 70, 71], "isnan": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 86, 110], "isoton": [54, 55, 98], "isotonicregress": 98, "issn": 81, "issu": [13, 16, 98, 155, 158, 159], "ite": [68, 71, 74, 75, 82, 83, 84], "ite_lower_quantil": [68, 71], "ite_mean": [68, 71], "ite_upper_quantil": [68, 71], "item": [25, 74, 90, 100, 110, 126], "iter": [46, 59, 65, 66, 89, 90, 95, 99, 110, 143, 160], "itertool": 81, "its": [48, 49, 74, 98, 100, 109, 110, 111, 113, 114, 116, 126, 127, 143], "iv": [25, 33, 38, 39, 42, 43, 60, 62, 77, 89, 101, 102, 107, 120, 123, 139, 140, 144, 153, 155, 159, 160], "iv_2": 59, "iv_var": [62, 89], "iv\u00e1n": [126, 158], "j": [10, 11, 17, 18, 19, 31, 33, 34, 36, 40, 41, 42, 43, 44, 45, 60, 61, 62, 64, 65, 75, 77, 81, 89, 92, 96, 99, 101, 110, 111, 118, 143, 155, 157], "j_": [62, 89], "j_0": 143, "j_1": [62, 89], "j_2": [62, 89], "j_3": [62, 89], "j_k": [62, 89], "jame": 158, "jan": 159, "janari": [68, 71], "janni": [63, 90], "januari": 68, "jasenakova": 159, "javanmard": 158, "jbe": [62, 89], "jeconom": [17, 18, 19, 31, 40, 61], "jerzi": 158, "jia": 98, "jitter": [16, 74], "jitter_strength": 74, "jitter_valu": 16, "jk": [111, 119], "jmlr": [64, 155, 157, 159], "job": [63, 90, 91], "john": 158, "joint": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 68, 69, 70, 71, 75, 78, 79, 80, 82, 83, 85, 91, 94, 96, 111, 113, 143, 159, 160], "joint_prob": 87, "jointli": [94, 109], "jonathan": 158, "joss": [64, 110, 155, 157], "journal": [10, 11, 17, 18, 19, 31, 36, 40, 41, 43, 44, 61, 62, 64, 81, 89, 92, 96, 98, 101, 110, 155, 157, 158, 159], "jss": 155, "juliu": 159, "jump": [93, 95, 111], "jun": [61, 158], "jupyt": [59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], "juraj": 158, "just": [61, 64, 66, 67, 68, 71, 75, 80, 82, 83, 84, 85, 93, 94, 111, 127, 128, 129, 130, 131, 144, 145], "justif": [126, 144, 145], "k": [10, 17, 18, 19, 33, 34, 36, 40, 41, 42, 43, 44, 60, 62, 64, 77, 86, 88, 89, 95, 96, 100, 101, 108, 109, 111, 143, 160], "k_h": [95, 111], "kaggl": [63, 90], "kallu": [80, 91, 94, 96, 97, 127, 133, 136, 137, 158], "kappa": 111, "kato": [43, 62, 89, 143, 158], "kb": [69, 70, 74, 75, 84, 87, 89, 90, 91, 97, 102, 104, 107, 157], "kde": [27, 28, 29, 90], "kdeplot": [66, 86, 99], "kdeunivari": [27, 28, 29], "kecsk\u00e9sov\u00e1": 159, "keel": 96, "keep": [49, 53, 61, 85, 98, 106, 107, 160], "kei": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 46, 51, 62, 63, 78, 79, 82, 83, 88, 89, 90, 91, 95, 98, 110, 111, 127, 144, 150, 159], "keith": 158, "kelz": 96, "kengo": 158, "kennedi": 96, "kept": [105, 107], "kernel": [27, 28, 29, 46, 49, 53, 95, 111], "kernel_regress": 95, "kernelreg": 95, "keyword": [17, 18, 19, 22, 26, 39, 43, 44, 45, 47, 50], "kf": 126, "kfold": [89, 126], "kind": [59, 76, 90], "kj": [17, 18, 19, 33, 34, 36, 40, 41, 42, 43, 44, 60, 62, 77, 89, 101], "klaassen": [33, 81, 86, 88, 98, 155, 158], "klaa\u00dfen": 33, "kluge": 159, "knau": 158, "know": [66, 93, 96], "knowledg": [59, 76, 86, 92, 93], "known": [84, 86, 95, 96, 98, 110, 111, 117], "kohei": 158, "kotthof": 64, "kotthoff": [64, 110, 155, 157], "krueger": 92, "kueck": [63, 90], "kurz": [155, 158, 159], "kwarg": [17, 18, 19, 22, 26, 31, 35, 39, 40, 43, 44, 45, 46, 47, 48, 50, 88, 111], "l": [62, 64, 65, 72, 78, 79, 89, 96, 98, 99, 110, 144, 153, 155, 157], "l1": [90, 99, 111], "l_hat": [38, 39, 60, 77, 127], "lab": 65, "label": [13, 16, 48, 52, 67, 74, 75, 77, 78, 79, 80, 82, 83, 85, 87, 88, 91, 92, 94, 95], "labor": 92, "laffer": 158, "laff\u00e9r": [36, 65, 99, 111, 125, 127, 141, 142], "lal": [92, 159], "lambda": [62, 63, 64, 65, 68, 71, 90, 92, 93, 110, 111, 127, 128, 129, 143, 157], "lambda_": 81, "lambda_0": [127, 128, 129], "lambda_l1": 74, "lambda_l2": 74, "lambda_t": [18, 19, 71], "land": 93, "lang": [64, 110, 155, 157], "langl": [32, 93], "lanni": 96, "lappli": 126, "larg": [60, 77, 84, 86, 88, 92, 98, 111], "larger": [26, 41, 61, 95, 98, 144, 150], "largest": 86, "largli": 86, "lasso": [62, 63, 64, 65, 69, 90, 99, 110, 157, 158], "lasso_class": [63, 90], "lasso_pip": [64, 110], "lasso_summari": 90, "lassocv": [30, 69, 81, 89, 90, 99, 110, 111, 143, 157], "last": [18, 64, 156], "late": [25, 59, 63, 90, 96, 111, 120, 127, 134], "latent": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 97, 144, 153, 154], "later": [63, 64, 95, 98, 110, 160], "latest": 155, "latter": [52, 53, 96, 111], "layout": 81, "lbrace": [25, 26, 33, 34, 36, 62, 89, 100, 111, 118, 120, 121, 126, 127, 132, 143, 144, 151], "ldot": [37, 38, 39, 62, 65, 87, 89, 99, 100, 111, 122, 123, 124, 126, 143, 157], "le": [18, 66, 93, 109, 111, 113, 127, 136, 137], "lead": [61, 98, 111], "leadsto": 143, "lear": [64, 110, 155, 157], "learn": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 41, 43, 44, 45, 46, 59, 63, 64, 69, 72, 75, 76, 80, 81, 85, 86, 88, 90, 91, 92, 94, 95, 96, 98, 102, 107, 108, 110, 126, 127, 143, 144, 145, 159, 160], "learner": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 54, 59, 60, 61, 62, 63, 65, 66, 68, 69, 70, 71, 74, 77, 78, 79, 81, 87, 89, 90, 91, 96, 97, 98, 99, 100, 101, 108, 111, 113, 114, 116, 126, 127, 143, 144, 150, 159, 160], "learner_class": [30, 159], "learner_cv": 64, "learner_dict": 74, "learner_forest_classif": 64, "learner_forest_regr": 64, "learner_l": 97, "learner_lasso": 64, "learner_list": 86, "learner_m": 97, "learner_nam": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 54, 74, 110], "learner_pair": 74, "learner_param_v": 64, "learner_rf": 143, "learnerclassif": 64, "learnerregr": 64, "learnerregrcvglmnet": 64, "learnerregrrang": [64, 110], "learning_r": [68, 71, 74, 77, 80, 91, 94, 95, 98, 101], "least": [59, 63, 76, 90, 91, 97, 111, 115, 126], "leav": [65, 98, 99], "left": [17, 19, 33, 34, 36, 42, 43, 60, 62, 74, 75, 77, 86, 89, 90, 91, 92, 94, 95, 101, 111, 127, 128, 129, 130, 131, 143, 144, 146, 147, 148, 149, 151, 152], "legend": [63, 67, 68, 71, 74, 75, 77, 78, 79, 80, 82, 83, 85, 86, 87, 91, 92, 94], "lemp": 69, "len": [74, 75, 80, 86, 88, 89, 91, 94, 96], "length": [12, 13, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 64, 66, 68, 71, 96, 110], "leq": [62, 89], "less": [61, 63, 90, 91, 95, 98], "lester": 158, "let": [17, 18, 19, 31, 35, 40, 60, 61, 63, 64, 65, 66, 68, 71, 74, 75, 77, 80, 82, 83, 85, 86, 90, 91, 94, 98, 99, 100, 101, 110, 111, 113, 117, 125, 144, 145, 154, 160], "level": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 46, 50, 62, 63, 65, 66, 67, 68, 69, 70, 71, 74, 75, 78, 79, 80, 82, 83, 84, 85, 89, 90, 91, 94, 96, 97, 98, 99, 110, 118, 119, 127, 132, 144, 150, 151, 160], "level_0": [64, 89], "level_1": 89, "level_bound": [74, 75], "leverag": 74, "levi": 96, "levinsohn": [62, 89], "lewi": 158, "lgbm": 74, "lgbm_argument": 74, "lgbmclassifi": [66, 67, 68, 71, 74, 80, 86, 91, 94, 95, 98], "lgbmregressor": [66, 67, 68, 71, 74, 77, 80, 86, 91, 95, 98, 101], "lgr": [60, 61, 62, 63, 64, 65, 100, 110, 111, 126, 127, 143, 157], "lib": [67, 68, 69, 70, 71, 89, 90, 95, 96, 98], "liblinear": [90, 99, 111], "librari": [59, 60, 61, 62, 63, 64, 65, 74, 100, 101, 102, 107, 110, 111, 126, 127, 143, 156, 157, 160], "licens": [155, 159], "lie": 158, "lightgbm": [66, 67, 68, 71, 74, 77, 80, 86, 91, 94, 95, 98], "like": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 52, 53, 61, 63, 64, 68, 69, 70, 71, 81, 90, 91, 98, 110, 111, 112, 126, 157, 160], "lim": 92, "lim_": [95, 111], "limegreen": [78, 79], "limit": [92, 111, 117, 158], "limits_": 109, "lin": [95, 98, 111], "line": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 67, 69, 74, 75, 87, 98], "line_width": 74, "linear": [17, 19, 20, 21, 22, 26, 31, 35, 38, 39, 40, 41, 42, 43, 44, 45, 50, 59, 60, 61, 62, 64, 66, 67, 69, 74, 75, 76, 77, 78, 79, 81, 82, 85, 86, 87, 88, 89, 96, 97, 98, 100, 101, 108, 109, 110, 114, 115, 116, 122, 123, 124, 126, 128, 129, 130, 131, 132, 134, 135, 139, 140, 143, 150, 152, 153, 154, 155, 157, 158, 159, 160], "linear_learn": [68, 71], "linear_model": [22, 26, 30, 39, 50, 68, 69, 70, 71, 72, 74, 75, 76, 81, 85, 86, 89, 90, 95, 96, 98, 99, 110, 111, 143, 157], "linearli": [95, 111], "linearregress": [59, 68, 69, 70, 71, 74, 75, 76, 85, 86, 95, 96, 98], "linearscoremixin": [0, 127], "lineplot": [68, 71, 74, 75], "linestyl": [67, 68, 71, 74, 75, 87, 88, 95], "linetyp": 65, "linewidth": [67, 68, 71, 74], "link": [87, 98, 111, 122, 159], "linspac": [78, 79, 85, 98], "lint": 159, "linux": 156, "list": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 49, 53, 54, 60, 61, 62, 63, 64, 68, 69, 71, 77, 78, 79, 89, 91, 93, 101, 110, 126, 127, 156, 159], "list_confset": 25, "listedcolormap": 89, "literatur": [98, 111, 113, 117], "littl": [84, 96], "liu": [41, 87, 127, 138, 158], "ll": [64, 143, 160], "lllllllllllllllll": [102, 107, 157], "lm": [59, 61, 98], "ln_alpha_ml_l": 81, "ln_alpha_ml_m": 81, "load": [59, 61, 63, 64, 81, 90, 91, 102, 107, 156, 157], "loader": 0, "loc": [67, 68, 69, 70, 71, 74, 75, 77, 80, 81, 82, 83, 87, 89, 92, 94, 97, 98], "local": [25, 27, 96, 109, 111, 120, 158, 159], "localconvert": 89, "locat": [80, 94, 111], "log": [37, 62, 66, 68, 69, 70, 71, 74, 81, 86, 89, 91, 92, 97, 99, 110, 111, 113, 114, 116], "log_odd": 93, "log_p": [62, 89], "log_reg": [59, 61], "logarithm": [74, 81], "logic": [25, 64, 110], "logical_not": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 86, 110], "logist": [31, 37, 41, 47, 59, 61, 63, 65, 69, 75, 76, 90, 96, 98, 99, 122, 158, 159, 160], "logisticregress": [59, 68, 69, 70, 71, 72, 74, 75, 76, 85, 95, 96, 98], "logisticregressioncv": [30, 69, 86, 90, 99, 111], "logit": [37, 86, 92, 127, 138], "loglik": 64, "logloss": [63, 74, 90, 160], "logloss_m": 74, "logo": 159, "logspac": 90, "long": [7, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 60, 69, 77, 86, 97, 98, 144, 145, 154, 158], "longer": [68, 71, 102, 107, 111, 115], "look": [61, 63, 64, 66, 67, 68, 69, 70, 71, 80, 86, 87, 90, 91, 94, 95, 97], "loop": [75, 96], "loss": [66, 68, 69, 70, 71, 74, 86, 88, 91, 95, 97, 99, 110, 111, 113, 114, 116], "loss_ml_g0": 86, "loss_ml_g1": 86, "loss_ml_m": 86, "low": [67, 84, 96, 109, 158], "lower": [25, 63, 64, 67, 68, 71, 74, 75, 80, 81, 84, 85, 91, 92, 94, 95, 96, 97, 98, 110, 144, 150, 154, 160], "lower_bound": [78, 79], "lplr": [87, 122, 138], "lpop": 69, "lpq": [27, 29, 91, 109, 136, 159], "lpq_0": 94, "lpq_1": 94, "lqte": 109, "lr": 95, "lrn": [59, 60, 61, 62, 63, 64, 65, 100, 110, 111, 126, 127, 143, 157, 160], "lrn_0": 64, "lt": [59, 61, 62, 63, 64, 65, 68, 69, 70, 71, 72, 74, 75, 84, 87, 89, 90, 91, 93, 97, 98, 102, 107, 157], "lucien": 159, "luka": 158, "luk\u00e1\u0161": 36, "lusd": [64, 72, 102, 107, 157], "lvert": 81, "m": [7, 10, 11, 16, 30, 31, 37, 42, 43, 44, 60, 62, 64, 68, 71, 72, 74, 77, 81, 84, 86, 88, 89, 92, 96, 101, 104, 107, 108, 109, 110, 111, 112, 114, 116, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 146, 147, 148, 149, 151, 152, 153, 154, 155, 156, 157, 158, 159], "m_": [74, 75, 111, 114, 116, 118, 127, 129, 131, 132, 136, 143], "m_0": [12, 14, 15, 16, 22, 24, 25, 26, 28, 37, 38, 39, 44, 45, 46, 60, 62, 63, 77, 81, 84, 88, 89, 90, 101, 109, 110, 111, 120, 121, 123, 124, 127, 128, 129, 130, 131, 133, 136, 137, 138, 140, 141, 142, 157, 160], "m_hat": [25, 26, 38, 39, 60, 77, 85, 127], "m_i": [95, 111], "ma": [43, 62, 89, 96, 111, 112, 158], "mac": 156, "machin": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 36, 37, 38, 39, 41, 43, 44, 45, 46, 59, 63, 64, 65, 66, 68, 69, 70, 71, 72, 75, 76, 80, 81, 85, 86, 88, 90, 91, 92, 94, 95, 96, 97, 98, 99, 108, 110, 111, 113, 114, 116, 126, 127, 143, 144, 145, 159, 160], "machineri": [81, 158], "mackei": 158, "maco": 156, "made": [111, 125, 160], "mae": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 86, 110], "maggi": 158, "magnitud": [41, 144, 145], "mai": [49, 53, 65, 66, 99], "main": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 70, 81, 91, 98, 111, 143, 144, 145, 158, 160], "mainli": 98, "maintain": [61, 155, 159], "mainten": 159, "major": [64, 98, 159], "make": [59, 74, 75, 76, 86, 88, 98, 109, 110, 159, 160], "make_confounded_irm_data": [98, 159], "make_confounded_plr_data": 97, "make_did_cs2021": [7, 16, 68, 104, 107, 111, 112, 116], "make_did_cs_cs2021": [71, 111, 114], "make_did_sz2020": [5, 12, 15, 66, 103, 107, 111, 113], "make_heterogeneous_data": [78, 79, 82, 83, 84], "make_iivm_data": [25, 27, 109, 111], "make_irm_data": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 85, 86, 109, 110, 111], "make_irm_data_discrete_treat": [74, 75], "make_lplr_lzz2020": [37, 87, 111], "make_pipelin": 90, "make_pliv_chs2015": [38, 111], "make_pliv_multiway_cluster_ckms2021": [62, 89], "make_plr_ccddhnr2018": [6, 7, 12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 77, 88, 100, 101, 109, 110, 111, 126, 127, 143, 144, 150, 159], "make_simple_rdd_data": [8, 46, 95, 105, 107, 111], "make_spd_matrix": 45, "make_ssm_data": [9, 65, 99, 106, 107, 111], "malt": [155, 158], "man": [59, 76], "manag": [110, 156], "mandatori": [105, 107], "mani": [20, 21, 42, 60, 61, 62, 64, 66, 77, 88, 89, 101, 127, 143, 160], "manili": 50, "manipul": [63, 64, 95, 111], "manual": [63, 85, 88, 97, 160], "mao": 158, "map": [25, 48, 49, 52, 53, 61, 62, 89, 109, 111, 120], "mapsto": [100, 109], "mar": [36, 111], "march": [81, 86, 88], "margin": [78, 79, 98], "marit": [63, 90], "marker": [68, 71, 75, 98], "markers": 92, "market": 92, "markettwo": 62, "markov": [45, 158], "marr": [63, 90, 91, 97, 160], "marshal": 110, "martin": [36, 98, 155, 158, 159], "masatoshi": 158, "masip": [96, 159], "mask": 16, "maskedarrai": [111, 112], "master": [61, 69], "mat": 62, "match": [110, 144, 153], "math": [30, 68, 69, 70, 71], "mathbb": [17, 18, 19, 20, 21, 25, 26, 31, 35, 38, 39, 40, 41, 62, 65, 66, 67, 68, 71, 74, 75, 84, 86, 87, 88, 89, 92, 95, 99, 109, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 140, 141, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 157, 160], "mathcal": [17, 18, 19, 31, 32, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 47, 60, 62, 65, 67, 77, 80, 89, 93, 94, 99, 101, 111, 112, 115, 117], "mathop": 109, "mathrm": [31, 40, 68, 69, 70, 71, 95, 111, 112, 114, 115, 116, 117, 127, 129, 131, 144, 147, 149], "matia": 158, "matplotlib": [13, 16, 66, 67, 68, 69, 70, 71, 72, 74, 75, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 98, 99], "matric": [93, 159], "matrix": [17, 18, 19, 31, 33, 34, 35, 36, 40, 42, 43, 44, 45, 49, 53, 60, 62, 63, 64, 65, 77, 89, 99, 101, 102, 107, 110, 143, 157, 159, 160], "matt": 158, "matter": [86, 92], "max": [17, 19, 63, 64, 74, 85, 90, 91, 96, 100, 109, 110, 111, 126, 127, 129, 131, 133, 143, 144, 147, 149, 157, 160], "max_depth": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 72, 74, 90, 97, 100, 109, 110, 111, 113, 126, 127, 143, 144, 150, 157, 160], "max_featur": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 72, 90, 97, 100, 109, 110, 111, 126, 127, 143, 144, 150, 157, 160], "max_it": [74, 89, 90, 98], "maxim": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 93, 109, 111], "maxima": 143, "maximum": [54, 55, 109, 110], "mb": [68, 71, 72, 102, 107, 157], "mb706": 159, "mea": 33, "mean": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 48, 49, 52, 53, 59, 60, 62, 63, 66, 68, 71, 74, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 94, 96, 97, 98, 101, 110, 111, 143, 160], "mean_absolute_error": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 86, 110], "meant": [69, 109, 159], "measir": 97, "measur": [61, 64, 69, 81, 88, 96, 97, 98, 110, 111, 112, 125, 144, 145, 151, 152, 153, 154], "measure_col": 81, "measure_func": 61, "measure_pr": 61, "measures_r": 61, "mechan": [48, 49, 52, 53, 98, 111, 117], "median": [96, 98, 126], "medium": 74, "melt": 62, "membership": 98, "memori": [68, 69, 70, 71, 72, 74, 75, 84, 87, 89, 90, 91, 97, 102, 104, 107, 157], "mention": [84, 109], "merg": [63, 90], "mert": [126, 158], "meshgrid": [78, 79, 98], "messag": [60, 61, 62, 63, 64, 65, 74, 86, 157, 159], "meta": [48, 49, 52, 53, 110, 157], "metadata": [48, 49, 52, 53], "metadata_rout": [48, 49, 52, 53], "metadatarequest": [48, 49, 52, 53], "method": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 82, 83, 85, 86, 89, 90, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 107, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 144, 145, 150, 155, 157, 159], "methodolog": 158, "methodologi": 98, "metric": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 52, 74, 110], "michael": 158, "michaela": 159, "michel": [155, 157], "michela": [36, 158], "mid": [63, 90, 92, 95, 111, 127, 140], "mid_point": [74, 75], "might": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 62, 69, 80, 85, 86, 89, 93, 95, 97, 98, 110, 111], "mild": [60, 77, 101], "militari": 92, "miller": [62, 89], "mimic": 98, "min": [62, 63, 64, 65, 68, 69, 70, 71, 74, 80, 85, 89, 90, 91, 94, 95, 100, 110, 111, 115, 126, 127, 143, 157, 160], "min_": 109, "min_data_in_leaf": 74, "min_samples_leaf": [12, 14, 15, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 51, 74, 84, 87, 90, 97, 100, 109, 110, 111, 113, 126, 127, 143, 144, 150, 160], "min_samples_split": [90, 111, 112, 114, 116], "minim": [26, 51, 63, 86, 90, 95, 111], "minimum": [54, 55, 69], "minor": [60, 69, 77, 101, 127, 159], "minsplit": 63, "minut": 88, "mirror": [102, 107], "miruna": 158, "mislead": 159, "miss": [5, 6, 7, 8, 9, 30, 64, 102, 103, 107, 110, 111, 127, 141, 159], "missing": [36, 65, 99], "misspecif": 66, "misspecifi": 66, "mit": [155, 157], "mixin": [0, 20, 21, 127], "ml": [45, 62, 63, 64, 69, 81, 88, 89, 90, 95, 96, 100, 108, 110, 111, 126, 155, 158, 159], "ml_a": 37, "ml_g": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 59, 60, 61, 63, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 80, 82, 84, 85, 86, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 109, 110, 111, 112, 113, 114, 116, 159], "ml_g0": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 61, 63, 66, 68, 69, 70, 72, 86, 90, 97, 110, 111, 113, 116], "ml_g1": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 61, 63, 66, 68, 69, 70, 72, 86, 90, 97, 110, 111, 113, 116], "ml_g_d0": [99, 111], "ml_g_d0_t0": [66, 71, 111, 113, 114], "ml_g_d0_t1": [66, 71, 111, 113, 114], "ml_g_d1": [99, 111], "ml_g_d1_t0": [66, 71, 111, 113, 114], "ml_g_d1_t1": [66, 71, 111, 113, 114], "ml_g_d_lvl0": [74, 111], "ml_g_d_lvl1": [74, 111], "ml_g_sim": 30, "ml_l": [38, 39, 60, 62, 63, 64, 72, 77, 79, 83, 88, 89, 90, 92, 97, 100, 101, 110, 111, 126, 127, 143, 144, 150, 157, 159, 160], "ml_l_bonu": 157, "ml_l_forest": 64, "ml_l_forest_pip": 64, "ml_l_lasso": 64, "ml_l_lasso_pip": 64, "ml_l_rf": 160, "ml_l_sim": 157, "ml_l_tune": 110, "ml_l_xgb": 160, "ml_m": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 144, 150, 157, 159, 160], "ml_m_bench_control": 98, "ml_m_bench_treat": 98, "ml_m_bonu": 157, "ml_m_forest": 64, "ml_m_forest_pip": 64, "ml_m_lasso": 64, "ml_m_lasso_pip": 64, "ml_m_rf": 160, "ml_m_sim": [30, 157], "ml_m_tune": 110, "ml_m_xgb": 160, "ml_pi": [30, 65, 99, 111], "ml_pi_sim": 30, "ml_r": [25, 38, 59, 62, 63, 76, 89, 90, 96, 111, 159], "ml_r0": 111, "ml_r1": [63, 90, 111], "ml_t": [37, 87, 111], "mlr": [64, 110], "mlr3": [59, 60, 61, 62, 63, 65, 100, 110, 111, 126, 127, 143, 155, 157, 159, 160], "mlr3book": [64, 110], "mlr3extralearn": [63, 110], "mlr3filter": 64, "mlr3learner": [59, 60, 61, 62, 63, 100, 110, 111, 126, 127, 143, 157, 160], "mlr3measur": 61, "mlr3pipelin": [110, 159], "mlr3tune": [64, 110, 159], "mlr3vers": 63, "mlrmeasur": 61, "mode": [98, 156], "model": [0, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 49, 50, 51, 53, 56, 59, 60, 61, 62, 64, 66, 67, 68, 69, 70, 71, 76, 77, 80, 81, 84, 86, 89, 91, 94, 97, 100, 101, 102, 104, 106, 107, 108, 110, 113, 114, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 128, 129, 130, 131, 132, 134, 135, 138, 139, 140, 145, 150, 151, 152, 153, 154, 155, 158, 159], "model_data": [63, 90], "model_label": 88, "model_list": 74, "model_select": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 77, 89, 110, 126], "modellist": [74, 85], "modelmlestimatelowerupp": 63, "modelvers": 74, "modern": [64, 110, 155, 157], "modul": [74, 95, 107, 111, 156], "molei": [41, 158], "moment": [20, 21, 62, 89, 111, 114, 115, 116, 127, 143, 144, 145, 154, 157], "monoton": 111, "mont": [31, 32, 35, 40, 78, 79, 82, 83], "montanari": 158, "month": [68, 71], "more": [26, 41, 50, 59, 61, 63, 68, 69, 70, 71, 74, 75, 76, 78, 79, 81, 85, 86, 88, 90, 91, 95, 97, 98, 100, 109, 110, 111, 112, 113, 114, 115, 116, 117, 125, 127, 135, 143, 144, 145, 150, 154, 157, 160], "moreov": [63, 64, 69, 81, 110, 143, 160], "mortgag": [63, 90, 91], "most": [63, 80, 86, 90, 91, 94, 98, 109, 110, 111, 144, 150, 156], "motiv": [98, 101], "motivation_example_bch": 81, "mp": 61, "mpd": [62, 89], "mpdta": 69, "mpg": 89, "mse": [64, 81, 110], "mserd": 95, "msg": 68, "msr": [64, 110], "mtry": [63, 64, 100, 110, 111, 126, 127, 143, 160], "mu": 67, "mu_": 67, "mu_0": 111, "mu_mean": 67, "much": [63, 64, 74, 90, 95, 96, 98, 160], "muld": [72, 102, 107, 157], "multi": [16, 48, 52, 61, 62, 78, 79, 89, 127, 129, 131, 159], "multiclass": [64, 88], "multiindex": 89, "multioutput": [49, 53], "multioutputregressor": [49, 53], "multipl": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 61, 62, 63, 65, 66, 69, 70, 74, 85, 89, 90, 97, 98, 99, 102, 107, 110, 115, 119, 123, 126, 143, 144, 145, 158, 159, 160], "multipletest": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "multipli": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 77, 108, 109, 127, 160], "multiprocess": [80, 91, 94], "multitest": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "multivariate_norm": 30, "multiwai": [43, 62, 89, 158], "music": 158, "must": [12, 13, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 55, 87, 102, 103, 107, 110, 111], "mutat": 64, "mutual": [22, 26, 39, 63, 82, 83, 90, 91, 109], "my_sampl": 126, "my_task": 126, "n": [12, 14, 15, 17, 18, 19, 22, 24, 25, 26, 27, 28, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 59, 60, 62, 64, 65, 67, 68, 71, 74, 75, 76, 77, 80, 81, 84, 89, 92, 93, 94, 95, 96, 99, 100, 101, 109, 110, 111, 117, 126, 143, 155, 156], "n_": [35, 67, 71, 144, 147, 149], "n_aggreg": 13, "n_coef": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 144, 150], "n_color": [68, 71], "n_complier": 94, "n_core": [80, 91, 94], "n_estim": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 66, 67, 68, 71, 72, 74, 77, 78, 79, 80, 82, 83, 84, 90, 91, 93, 94, 95, 97, 98, 100, 101, 109, 110, 111, 113, 126, 127, 143, 144, 150, 157, 160], "n_eval": [64, 110], "n_featur": [48, 49, 52, 53], "n_fold": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 60, 61, 62, 63, 66, 68, 69, 71, 72, 77, 78, 79, 80, 82, 83, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 97, 98, 101, 110, 126, 157, 160], "n_folds_inn": 37, "n_folds_per_clust": [62, 89], "n_folds_tun": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "n_framework": 13, "n_iter": [46, 95, 111], "n_iter_randomized_search": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 110], "n_job": [80, 90, 91, 94], "n_jobs_cv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 86], "n_jobs_model": [16, 23, 29, 80, 91, 94], "n_learner": 74, "n_level": [35, 74, 75], "n_ob": [7, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 47, 50, 51, 60, 64, 65, 66, 67, 68, 71, 74, 75, 77, 78, 79, 82, 83, 84, 85, 86, 87, 88, 95, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 116, 126, 143, 144, 150, 157], "n_output": [48, 49, 52, 53], "n_period": [17, 19, 68, 71], "n_pre_treat_period": [17, 19, 68, 71], "n_rep": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 60, 61, 62, 65, 66, 68, 71, 72, 74, 75, 77, 84, 85, 86, 89, 95, 97, 98, 99, 101, 110, 126, 144, 150, 157, 160], "n_rep_boot": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 68, 69, 70, 71, 75, 78, 79, 80, 82, 83, 85, 91, 94, 143], "n_sampl": [48, 49, 52, 53, 93, 96], "n_samples_fit": [49, 53], "n_split": 126, "n_t": 67, "n_target": [52, 53], "n_theta": 13, "n_time_period": 67, "n_true": [80, 94], "n_var": [60, 64, 77, 101, 102, 107, 110, 143, 157], "n_w": 93, "n_x": [32, 78, 79, 82, 83, 84], "na": [5, 6, 7, 8, 9, 60, 62, 65, 101, 159], "na_real_": [62, 159], "naiv": [60, 77, 101], "name": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 52, 53, 54, 60, 61, 62, 67, 68, 69, 71, 74, 82, 83, 84, 88, 89, 95, 97, 98, 110, 156, 159], "namespac": 61, "nan": [5, 6, 7, 8, 9, 12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 66, 67, 74, 75, 77, 80, 82, 83, 86, 88, 90, 91, 94, 99, 101, 110], "nanmean": 77, "narita": 158, "nat": [68, 71], "nathan": 158, "nation": [98, 126, 158], "nativ": 61, "natt": 93, "natur": 98, "nbest": 74, "ncol": [62, 63, 64, 87, 95, 102, 107, 110, 143, 157], "ncoverag": 86, "ndarrai": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 54, 102, 107], "nearli": 86, "necess": [62, 89], "necessari": [61, 62, 74, 88, 89, 95, 111, 156], "need": [12, 14, 15, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 59, 60, 61, 63, 65, 74, 76, 77, 88, 91, 96, 99, 110, 126, 127, 138, 144, 154, 159, 160], "neg": [49, 53], "neighborhood": [95, 143], "neither": [5, 6, 7, 8, 9, 62, 89, 96, 102, 107], "neng": 158, "neq": [74, 95, 111], "nest": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 52, 53, 110, 127, 142, 144, 150], "net": [91, 97, 160], "net_tfa": [63, 90, 91, 97, 160], "network": 74, "nev": [111, 114, 116, 117], "never": [17, 19, 25, 61, 62, 68, 69, 70, 71, 89, 111, 117, 159], "never_tak": [25, 63, 90], "never_tr": [14, 16, 68, 69, 70, 71, 111, 112, 114, 116], "nevertheless": 85, "new": [59, 60, 61, 62, 63, 64, 65, 78, 79, 88, 90, 93, 100, 101, 102, 107, 109, 110, 111, 126, 127, 143, 155, 157, 158, 159, 160], "new_data": [78, 79, 93], "newei": [10, 11, 44, 62, 81, 89, 98, 101, 155, 158], "newest": 159, "next": [61, 63, 64, 78, 79, 80, 84, 86, 87, 90, 91, 93, 94, 96, 98, 159], "neyman": [62, 89, 100, 108, 144, 154, 155, 158], "nfold": [62, 63, 65, 111], "nh": 111, "nice": [61, 74], "nifa": [90, 91, 97], "nil": 98, "nine": [62, 89], "nlogloss": 74, "nn": 95, "noack": [95, 111, 158, 159], "node": [63, 64, 100, 111, 126, 127, 143, 157, 160], "nois": [47, 92, 93], "nomin": 96, "non": [14, 17, 18, 19, 25, 43, 44, 45, 46, 59, 60, 63, 67, 68, 71, 76, 77, 90, 91, 93, 95, 110, 126, 127, 129, 131, 143, 144, 147, 149], "non_orth_scor": [60, 77, 127], "nondur": 72, "none": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 46, 48, 49, 50, 52, 53, 54, 55, 62, 63, 66, 68, 69, 70, 71, 72, 74, 75, 76, 84, 87, 90, 91, 96, 97, 98, 99, 102, 104, 105, 106, 107, 110, 111, 113, 114, 116, 127, 143, 156, 157], "nonignor": [30, 142], "nonlinear": [17, 19, 21, 63, 68, 71, 90, 95, 111, 127, 136, 137, 159], "nonlinearscoremixin": [0, 127], "nonparametr": [27, 28, 29, 95, 98, 127, 144, 145, 151, 152, 153, 154, 158], "nop": 64, "nor": [5, 6, 7, 8, 9, 62, 89, 96, 102, 107], "norm": 77, "normal": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 65, 66, 67, 68, 70, 71, 76, 77, 80, 84, 91, 92, 93, 94, 95, 96, 99, 101, 102, 107, 110, 111, 127, 128, 129, 130, 131, 143, 157], "normalize_ipw": [22, 23, 24, 25, 26, 27, 28, 29, 30, 65, 85, 91, 99], "not_yet_tr": [14, 16, 68, 71], "notat": [62, 65, 66, 89, 99, 111, 113, 114, 115, 116, 117, 125, 127, 129, 131], "note": [5, 6, 7, 8, 9, 13, 16, 19, 20, 21, 25, 26, 30, 37, 38, 39, 48, 49, 51, 52, 53, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 126, 127, 155, 157], "notebook": [59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 109, 110, 111, 159, 160], "notic": [59, 76, 96], "now": [61, 62, 63, 65, 69, 70, 74, 78, 79, 86, 89, 90, 93, 96, 98, 99, 107, 157, 159], "np": [5, 6, 7, 8, 9, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 54, 66, 67, 68, 69, 70, 71, 72, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 107, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 144, 150, 157, 160], "nrmse": 74, "nround": [60, 63, 160], "nrow": [61, 62, 64, 95, 102, 107, 110, 143, 157], "nu": [18, 25, 45, 65, 99, 111, 120, 144, 145, 147, 149, 150, 153, 154], "nu2": [68, 86, 144, 150], "nu_0": [144, 154], "nu_i": [65, 99], "nuis_g0": 59, "nuis_g1": 59, "nuis_l": 160, "nuis_m": [59, 160], "nuis_r0": 59, "nuis_r1": 59, "nuis_rmse_ml_l": 81, "nuis_rmse_ml_m": 81, "nuisanc": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 44, 45, 46, 60, 61, 62, 63, 64, 65, 66, 69, 74, 77, 78, 79, 80, 81, 84, 86, 87, 89, 90, 91, 94, 96, 97, 98, 99, 100, 101, 110, 111, 114, 115, 116, 126, 127, 128, 129, 130, 131, 132, 136, 138, 143, 144, 147, 149, 154, 155, 159, 160], "nuisance_el": [144, 146, 147, 148, 149, 151, 152, 153], "nuisance_loss": [74, 86, 110, 159], "nuisance_spac": [37, 127, 138], "nuisance_target": 86, "null": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 61, 97, 110, 144, 150, 159], "null_hypothesi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 97, 144, 150], "num": [63, 64, 100, 110, 111, 126, 127, 143, 157], "num_leav": [67, 80, 91, 94], "number": [12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 50, 51, 53, 60, 62, 67, 68, 69, 71, 77, 78, 79, 80, 81, 82, 83, 86, 89, 91, 93, 94, 95, 98, 111, 115, 126, 143, 155, 157, 160], "numer": [21, 55, 59, 64, 85, 92, 110, 127, 144, 151, 152, 159], "numeric_onli": 81, "numpi": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 46, 50, 51, 54, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 104, 107, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 144, 150, 157], "nuniqu": [68, 71], "ny": 158, "nyt": [111, 114, 116, 117], "o": [67, 74, 75, 81, 82, 83, 86, 88, 90, 92, 95, 155, 157], "ob": [61, 63, 67, 71, 95, 144, 147], "obei": 127, "obj": 90, "obj_dml_data": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 59, 60, 62, 69, 70, 76, 77, 80, 85, 88, 89, 94, 100, 101, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 144, 150, 159], "obj_dml_data_bonu": [102, 107], "obj_dml_data_bonus_df": [102, 107], "obj_dml_data_from_arrai": [5, 6, 7, 8, 9], "obj_dml_data_from_df": [5, 6, 8, 9], "obj_dml_data_sim": [102, 107], "obj_dml_data_sim_clust": [102, 107], "obj_dml_plr": [60, 77, 101], "obj_dml_plr_bonu": [64, 157], "obj_dml_plr_bonus_pip": 64, "obj_dml_plr_bonus_pipe2": 64, "obj_dml_plr_bonus_pipe3": 64, "obj_dml_plr_bonus_pipe_ensembl": 64, "obj_dml_plr_fullsampl": 88, "obj_dml_plr_lesstim": 88, "obj_dml_plr_nonorth": [60, 77], "obj_dml_plr_orth_nosplit": [60, 77], "obj_dml_plr_sim": [64, 157], "obj_dml_plr_sim_pip": 64, "obj_dml_plr_sim_pipe_ensembl": 64, "obj_dml_plr_sim_pipe_tun": 64, "obj_dml_sim": 30, "object": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 59, 63, 64, 65, 66, 68, 69, 70, 71, 72, 75, 78, 79, 80, 84, 85, 87, 88, 90, 91, 94, 95, 99, 102, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 155, 157, 158, 159, 160], "obs_confound": [59, 76], "observ": [12, 14, 15, 16, 17, 18, 19, 22, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 50, 51, 56, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 84, 86, 87, 88, 89, 90, 91, 94, 95, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 116, 117, 125, 126, 127, 128, 129, 130, 131, 143, 144, 145, 146, 147, 148, 149, 157, 158, 160], "obtain": [19, 25, 40, 59, 60, 61, 62, 65, 66, 68, 69, 70, 71, 76, 77, 78, 79, 80, 81, 86, 89, 94, 96, 98, 99, 100, 101, 109, 110, 126, 127, 138, 143, 144, 145, 150, 156, 157], "obvious": [68, 71], "occur": [17, 19, 68, 71, 88, 159], "odd": 37, "off": [93, 158], "offer": [17, 19, 61, 63, 90, 91, 98, 160], "offici": 156, "offset": 110, "often": 94, "oka": 158, "ol": [22, 26, 39, 50], "olma": [95, 111, 158, 159], "omega": [84, 109, 111, 112, 127, 132, 135, 144, 151, 152], "omega_": [43, 62, 89], "omega_1": [43, 62, 89], "omega_2": [43, 62, 89], "omega_epsilon": [62, 89], "omega_v": [43, 62, 89], "omega_x": [43, 62, 89], "omit": [68, 71, 97, 98, 111, 115, 127, 129, 131, 144, 145, 154, 158, 159, 160], "ommit": 98, "onc": [61, 88, 98, 111, 117, 160], "one": [13, 16, 38, 56, 59, 60, 61, 62, 63, 64, 68, 69, 70, 71, 75, 76, 77, 78, 79, 86, 87, 89, 91, 92, 95, 96, 97, 98, 101, 102, 107, 109, 110, 111, 112, 115, 117, 123, 126, 127, 128, 129, 130, 131, 135, 138, 139, 140, 143, 144, 145, 150, 151, 152, 153, 157, 159], "ones": [64, 67, 80, 88, 94, 97, 109], "ones_lik": [74, 75, 94], "onli": [5, 6, 7, 8, 9, 12, 14, 15, 16, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 50, 52, 53, 54, 55, 61, 62, 63, 68, 71, 78, 79, 82, 83, 84, 86, 87, 88, 89, 90, 91, 95, 100, 109, 110, 111, 114, 116, 125, 127, 129, 131, 133, 136, 137, 143, 144, 145, 147, 149, 151, 152, 154, 159], "onlin": 160, "onto": 86, "oo": 88, "oob_error": [64, 110], "oop": 159, "opac": [78, 79], "open": [64, 110, 155, 157], "oper": [64, 159], "opposit": [93, 95, 111], "oprescu": [32, 78, 79, 82, 83, 158], "opt": [67, 68, 69, 70, 71, 89, 90, 95, 96, 98], "optim": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 46, 64, 78, 79, 88, 93, 109, 110, 158], "option": [12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 52, 53, 54, 55, 59, 60, 62, 63, 65, 68, 69, 71, 75, 78, 79, 82, 83, 84, 86, 89, 90, 91, 99, 102, 103, 105, 106, 107, 110, 111, 126, 127, 133, 136, 137, 143, 159], "oracl": [35, 47, 68, 71, 74, 75], "oracle_valu": [31, 35, 40, 47, 74, 75], "orang": 60, "orcal": [31, 40], "order": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 52, 61, 62, 63, 64, 69, 85, 89, 90, 95, 110, 111, 126, 127], "org": [33, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 110, 155, 156, 158, 159], "orient": [64, 110, 127, 155, 157, 158, 159], "origin": [48, 49, 51, 52, 53, 61, 64, 68, 69, 70, 71, 93, 97, 98, 109, 127, 135], "orign": [63, 90], "orth_sign": [50, 51, 85], "orthogon": [50, 51, 62, 63, 68, 89, 90, 100, 108, 111, 143, 144, 154, 155, 158], "orthongon": [144, 154], "osx": 156, "other": [5, 6, 7, 8, 9, 37, 38, 39, 48, 49, 52, 53, 60, 62, 63, 64, 65, 66, 68, 69, 70, 71, 75, 77, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 101, 102, 105, 107, 109, 110, 111, 123, 124, 126, 127, 135, 143, 144, 154, 155, 156, 157, 158, 159, 160], "other_ind": 89, "otherwis": [12, 14, 15, 22, 25, 26, 39, 48, 49, 52, 53, 63, 90, 91, 93, 111, 113, 127, 129, 131], "othrac": [64, 72, 102, 107, 157], "our": [60, 61, 63, 64, 66, 68, 71, 74, 77, 78, 79, 80, 86, 88, 90, 91, 94, 95, 97, 98, 101, 111, 155, 157, 159, 160], "ourselv": 86, "out": [38, 39, 62, 64, 66, 67, 68, 69, 70, 71, 72, 81, 86, 88, 89, 91, 97, 98, 99, 100, 102, 107, 108, 109, 110, 111, 112, 113, 114, 116, 127, 139, 140, 143, 144, 145, 150, 153, 155, 157, 159, 160], "outcom": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 35, 37, 38, 39, 40, 41, 47, 59, 61, 62, 63, 64, 67, 68, 69, 70, 71, 72, 76, 81, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 102, 104, 105, 106, 107, 110, 113, 114, 116, 117, 118, 122, 123, 124, 125, 129, 131, 132, 143, 145, 150, 151, 153, 154, 157, 159, 160], "outcome_0": 76, "outcome_1": 76, "outer": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 110], "outperform": 74, "output": [61, 68, 70, 71, 86, 96, 100, 110, 111, 114, 116, 143, 160], "output_list": 96, "outshr": 89, "outsid": 60, "over": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 60, 68, 69, 71, 75, 77, 81, 86, 101, 108, 110, 111, 112, 144, 150, 159], "overal": [13, 68, 69, 70, 71, 87, 93, 98, 111, 112], "overall_aggregation_weight": [13, 68, 69, 70, 71], "overcom": [108, 127], "overfit": [88, 108, 126], "overlap": [66, 87, 98, 111, 113, 117], "overrid": [110, 159], "overridden": 111, "overst": [63, 90, 91], "overview": [86, 143, 144, 150, 158], "overwrit": 159, "ownership": [63, 90], "p": [12, 14, 15, 16, 17, 18, 19, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 37, 38, 39, 40, 41, 47, 54, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 101, 109, 110, 111, 112, 113, 114, 116, 117, 122, 126, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 141, 142, 143, 144, 151, 152, 155, 156, 157, 159], "p401": [63, 90, 91], "p_0": [127, 128, 129, 130], "p_1": 143, "p_adjust": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 126, 143, 155, 157], "p_dbl": [64, 110], "p_hat": 85, "p_i": 41, "p_int": 110, "p_n": 42, "p_val": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "p_x": [43, 62, 89], "p_x0": 92, "p_x1": 92, "packag": [59, 60, 62, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 87, 88, 89, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 109, 110, 111, 113, 117, 125, 126, 127, 143, 144, 145, 155, 157, 158, 159, 160], "packagedata": 89, "packagevers": 63, "pad": 87, "page": [98, 155, 158], "pair": [59, 76], "pake": [62, 89], "paket": [62, 63, 64], "pal": 62, "palett": [13, 16, 68, 71, 74, 75], "pand": [68, 71], "panda": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 41, 50, 51, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 102, 104, 107, 109, 111, 144, 145, 157], "pandas2ri": 89, "panel": [6, 7, 12, 14, 16, 17, 18, 19, 69, 71, 102, 103, 104, 107, 114, 115, 117, 148, 149, 158, 159], "paper": [33, 42, 64, 88, 92, 95, 97, 98, 144, 154, 155, 157, 158, 159], "par": 72, "par_grid": [64, 110], "paradox": [64, 110, 159], "parallel": [61, 66, 67, 68, 71, 75, 80, 86, 94, 111, 113, 115, 117], "param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 52, 53, 88, 110], "param_grid": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 110], "param_nam": 61, "param_set": [64, 110], "param_v": 64, "paramet": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 60, 61, 62, 63, 65, 66, 68, 69, 71, 74, 75, 77, 78, 79, 80, 81, 84, 85, 86, 89, 90, 93, 94, 95, 96, 97, 98, 99, 100, 101, 107, 108, 109, 110, 113, 114, 115, 116, 117, 118, 119, 120, 121, 125, 126, 136, 137, 143, 144, 145, 150, 152, 154, 155, 157, 158, 159, 160], "parametr": [25, 61, 98, 101, 110, 160], "params_exact": 110, "params_nam": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 61], "parenttoc": 155, "part": [45, 60, 62, 63, 64, 70, 77, 86, 88, 89, 90, 101, 110, 126, 144, 154, 159, 160], "parti": 45, "partial": [21, 37, 38, 39, 40, 41, 42, 43, 44, 45, 62, 64, 72, 81, 87, 88, 89, 97, 100, 108, 110, 122, 123, 124, 126, 139, 140, 143, 150, 151, 152, 153, 154, 155, 157, 158, 159, 160], "partial_": [127, 143], "partiallli": 97, "particip": [10, 91, 97, 160], "particular": [111, 155], "particularli": [74, 88], "partion": [62, 89], "partit": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 89, 100, 108], "partli": 160, "pass": [22, 26, 39, 48, 49, 50, 52, 53, 61, 64, 69, 88, 110, 160], "passo": [155, 157], "past": 62, "paste0": [62, 65], "pastel": 77, "path": [110, 111], "path_to_r": 81, "patsi": [78, 79, 109], "pattern": 98, "paul": 158, "pd": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 50, 66, 67, 68, 69, 70, 71, 74, 75, 78, 79, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 109, 111], "pdf": [77, 92], "pedregosa": [155, 157], "pedregosa11a": [155, 157], "pedro": [61, 158], "penal": [65, 69, 99], "penalti": [63, 64, 69, 76, 90, 96, 98, 99, 110, 111], "pennsylvania": [11, 102, 107, 157], "pension": [63, 90, 91, 160], "peopl": [63, 90, 91], "pep8": 159, "per": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 62, 68, 69, 70, 71, 89], "percent": 110, "percentag": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40], "perf_count": 86, "perfectli": [87, 95, 111], "perform": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 51, 60, 62, 64, 66, 68, 69, 70, 71, 77, 81, 84, 86, 88, 89, 91, 97, 98, 99, 101, 110, 111, 113, 114, 116, 126, 127, 143, 155, 157, 158, 160], "performance_result": 74, "perfrom": 84, "perhap": 160, "period": [12, 14, 16, 17, 19, 61, 66, 67, 70, 104, 107, 112, 113, 114, 115, 116, 117, 129, 131, 158, 159], "perp": [111, 125], "perrot": [155, 157], "person": 160, "pessimist": 98, "peter": 158, "petra": 159, "petronelaj": 159, "pfister": [64, 110, 155, 157], "phi": [62, 89, 109, 143], "philipp": [98, 155, 158], "philippbach": [155, 159], "pi": [30, 34, 42, 45, 109, 111, 127, 141, 142], "pi_": [43, 62, 89], "pi_0": [127, 141, 142], "pi_i": [65, 99, 111], "pick": [95, 160], "pip": [95, 111], "pip3": 156, "pipe": 64, "pipe_forest_classif": 64, "pipe_forest_regr": 64, "pipe_lasso": 64, "pipelin": [48, 49, 52, 53, 64, 90, 159], "pipeop": 64, "pira": [63, 90, 91, 97, 160], "pivot": [74, 81, 89, 158], "pivot_logloss": 74, "pivot_rmse_g0": 74, "pivot_rmse_g1": 74, "place": 159, "plai": [88, 160], "plan": [10, 63, 90, 91, 160], "plausibl": [98, 144], "pleas": [48, 49, 52, 53, 61, 66, 67, 69, 75, 88, 98, 126, 155, 156], "plim": 92, "pliv": [20, 21, 38, 62, 89, 100, 109, 123, 139, 155, 159], "plm": [0, 6, 7, 12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 77, 87, 88, 89, 97, 100, 101, 108, 109, 110, 126, 143, 150, 159, 160], "plot": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 51, 60, 61, 63, 64, 65, 67, 68, 69, 70, 71, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 90, 91, 92, 94, 95, 97, 98, 99, 109, 144, 150], "plot_data": [68, 71], "plot_effect": [13, 16, 68, 69, 70, 71], "plot_tre": [51, 93, 109], "plotli": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 78, 79, 81, 95, 98], "plr": [12, 14, 15, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 64, 88, 92, 97, 100, 110, 124, 126, 140, 143, 150, 152, 153, 154, 155, 157, 159, 160], "plr_est": 92, "plr_est1": 92, "plr_est2": 92, "plr_obj": 92, "plr_obj_1": 92, "plr_obj_2": 92, "plr_summari": 90, "plrglmnet": 63, "plrranger": 63, "plrrpart": 63, "plrxgboost8700": 63, "plt": [66, 67, 68, 71, 72, 74, 75, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 98, 99], "plt_smpl": [62, 89], "plt_smpls_cluster": [62, 89], "plug": [84, 144, 146, 147, 148, 149, 150, 151, 152], "pm": [46, 62, 89, 143, 144, 150, 154], "pmatrix": [65, 99], "pmlr": [81, 86, 88], "po": [64, 110], "poe": 158, "point": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 61, 62, 74, 82, 83, 89, 98, 109, 111, 160], "pointwis": [50, 80, 82, 83, 94], "poli": [63, 85, 89, 90], "polici": [26, 37, 38, 39, 51, 87, 108, 111, 122, 123, 124, 157, 158, 159], "policy_tre": [26, 93, 109], "policy_tree_2": 93, "policy_tree_obj": 109, "policytre": 93, "polit": 92, "poly_dict": 90, "polynomi": [10, 11, 47, 63, 72, 85, 90, 95], "polynomial_featur": [10, 11, 63, 72], "polynomialfeatur": [85, 89, 90], "poor": 96, "pop": [127, 129], "popul": [98, 111, 114, 116, 127, 131], "popular": [86, 111, 144, 145], "porport": 97, "posit": [45, 63, 68, 70, 71, 74, 86, 92, 98, 160], "posixct": [64, 110], "possibl": [5, 6, 7, 8, 9, 49, 52, 53, 61, 64, 68, 70, 71, 78, 79, 82, 83, 84, 85, 86, 88, 93, 95, 96, 97, 98, 110, 111, 115, 118, 119, 143, 144, 145, 159, 160], "possibli": [96, 144, 145], "post": [16, 42, 45, 111, 113, 115, 117, 143, 158], "postdoubl": 158, "poster": 92, "potenti": [17, 19, 22, 23, 24, 27, 28, 30, 31, 35, 37, 47, 65, 66, 68, 71, 85, 92, 95, 99, 113, 117, 118, 125, 132, 133, 143, 151, 156, 159, 160], "potential_level": [74, 75], "power": [64, 88, 96, 98, 110, 158], "pp": [61, 81, 86, 88], "pq": [27, 28, 29, 91, 137, 159], "pq_0": [91, 94], "pq_1": [91, 94], "pr": [30, 59, 62, 63, 64, 65, 110, 111, 126, 127, 143, 157, 160], "practic": [86, 98, 158], "pre": [14, 16, 17, 19, 61, 65, 66, 68, 69, 70, 71, 99, 110, 111, 113, 114, 115, 116, 127, 129, 131, 159], "precis": [61, 111, 144, 152, 160], "precomput": [49, 53], "pred": [61, 88], "pred_df": 93, "pred_dict": 110, "pred_treat": 93, "predict": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 50, 51, 52, 53, 54, 60, 62, 63, 64, 69, 74, 77, 80, 81, 85, 86, 88, 89, 90, 93, 96, 98, 101, 109, 126, 144, 145, 150, 152, 159, 160], "predict_proba": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 39, 46, 48, 52, 88, 96, 110], "predictor": [22, 26, 39, 50, 51, 78, 79, 82, 83, 98, 100], "prefer": [63, 90, 91, 160], "preliminari": [24, 60, 77, 95, 127, 133, 136, 137, 138, 142], "prepar": [61, 62, 89, 159], "preprint": [96, 158], "preprocess": [63, 69, 85, 89, 90, 91, 110], "presenc": [63, 90, 91], "present": [61, 69, 98, 110, 127, 135, 160], "prespecifi": 97, "pretreat": [12, 14, 15, 16, 61, 66], "prettenhof": [155, 157], "preval": 98, "prevent": [126, 159], "previou": [67, 84, 85, 92, 156, 160], "previous": [69, 96, 110, 160], "price": [62, 89], "priliminari": [27, 29], "primari": [74, 75], "principl": [144, 145], "print": [14, 16, 46, 54, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 144, 150, 156, 157, 159, 160], "print_detail": 61, "print_period": [14, 16], "prior": [74, 86, 111, 125], "privat": 159, "prob": 64, "prob_dist": 96, "prob_dist_": 96, "probabilit": [84, 87], "probability_from_treat": 87, "probabl": [17, 18, 19, 22, 24, 25, 26, 27, 28, 29, 30, 35, 52, 60, 61, 65, 66, 68, 71, 75, 77, 84, 87, 92, 94, 95, 96, 98, 99, 101, 111, 127, 128, 129, 130, 131, 136, 158], "problem": [63, 69, 90, 91, 109, 110], "procedur": [60, 62, 63, 77, 86, 89, 90, 97, 98, 110, 143, 156, 159], "proceed": [42, 158], "process": [14, 16, 17, 18, 19, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 47, 55, 61, 65, 66, 67, 70, 71, 78, 79, 80, 81, 82, 83, 86, 87, 88, 93, 94, 98, 99, 108, 143, 144, 145, 158, 159], "processor": [54, 55], "produc": 92, "product": [78, 79, 81, 86, 98, 144, 154], "producton": 62, "program": [34, 63, 90, 91, 158, 160], "progress": 73, "project": [64, 78, 79, 109, 155, 159], "project_z": [78, 79], "prone": 127, "pronounc": 95, "propens": [14, 16, 17, 19, 22, 24, 25, 26, 27, 28, 29, 30, 31, 40, 41, 54, 55, 63, 65, 66, 68, 69, 71, 74, 84, 85, 86, 90, 91, 98, 99, 109, 111, 114, 116, 118, 127, 129, 131, 144, 151, 159], "propensity_scor": 54, "proper": 74, "properli": [74, 88, 160], "properti": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 63, 64, 68, 69, 70, 71, 86, 90, 91, 92, 96, 97, 102, 103, 105, 106, 107, 110, 111, 144, 150, 157, 159], "proport": [97, 144, 145, 153, 154], "propos": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 62, 64, 89, 95, 144, 145, 158, 159], "provid": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 52, 53, 54, 55, 61, 62, 63, 64, 69, 74, 78, 79, 82, 83, 85, 88, 89, 90, 95, 96, 98, 100, 101, 102, 104, 107, 108, 110, 143, 155, 157, 159, 160], "prune": [26, 51], "ps911c": 89, "ps944": 89, "ps_processor_config": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 78], "pscore1": 92, "pscore2": 92, "psi": [20, 21, 60, 61, 62, 89, 100, 111, 114, 115, 116, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 154, 157], "psi_": [143, 144, 147, 149, 150, 153, 154], "psi_a": [20, 25, 26, 38, 39, 60, 62, 77, 89, 111, 114, 115, 116, 126, 127, 128, 129, 130, 131, 132, 134, 135, 139, 140, 143], "psi_b": [20, 25, 26, 38, 39, 60, 77, 109, 111, 114, 115, 116, 126, 127, 128, 129, 130, 131, 132, 134, 135, 139, 140], "psi_el": [87, 126, 127], "psi_j": 143, "psi_nu2": [144, 150], "psi_sigma2": [144, 150], "psprocessor": [55, 159], "psprocessorconfig": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 54], "public": [59, 76, 159], "publish": [98, 155, 159], "pull": [63, 159], "purchas": 98, "pure": 98, "purp": [78, 79], "purpos": [60, 77, 84, 97, 98, 127, 129, 131, 144, 145, 157], "pval": 143, "px": [81, 95], "py": [67, 68, 69, 70, 71, 78, 89, 90, 95, 96, 98, 155, 156, 159], "py3": 156, "py_al": 77, "py_did": 66, "py_did_pretest": 67, "py_dml": 77, "py_dml_nosplit": 77, "py_dml_po": 77, "py_dml_po_nosplit": 77, "py_double_ml_apo": 75, "py_double_ml_bas": 77, "py_double_ml_basic_iv": 76, "py_double_ml_c": 78, "py_double_ml_cate_plr": 79, "py_double_ml_cvar": 80, "py_double_ml_firststag": 81, "py_double_ml_g": 82, "py_double_ml_gate_plr": 83, "py_double_ml_gate_sensit": 84, "py_double_ml_irm_vs_apo": 85, "py_double_ml_learn": 86, "py_double_ml_lplr": 87, "py_double_ml_meets_flaml": 88, "py_double_ml_multiway_clust": 89, "py_double_ml_pens": 90, "py_double_ml_pension_qt": 91, "py_double_ml_plm_irm_hetfx": 92, "py_double_ml_policy_tre": 93, "py_double_ml_pq": 94, "py_double_ml_rdflex": 95, "py_double_ml_robust_iv": 96, "py_double_ml_sensit": 97, "py_double_ml_sensitivity_book": 98, "py_double_ml_ssm": 99, "py_non_orthogon": 77, "py_panel": 68, "py_panel_data_exampl": 69, "py_panel_simpl": 70, "py_po_al": 77, "py_rep_c": 71, "py_tabpfn": 74, "pypi": [158, 159], "pyplot": [66, 67, 68, 71, 72, 74, 75, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 98, 99], "pyproject": 159, "pyreadr": 69, "python": [45, 61, 88, 95, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 126, 127, 143, 144, 145, 150, 155, 157, 158, 159, 160], "python3": [67, 68, 69, 70, 71, 89, 90, 95, 96, 98, 156], "pytorch": 74, "q": [64, 80, 94, 95, 110, 155, 157], "q2": [64, 72, 102, 107, 157], "q3": [64, 72, 102, 107, 157], "q4": [64, 72, 102, 107, 157], "q5": [64, 72, 102, 107, 157], "q6": [64, 72, 102, 107, 157], "q_i": [95, 111], "qquad": 34, "qte": [80, 91, 159], "quad": [17, 18, 19, 41, 63, 65, 66, 90, 93, 95, 99, 109, 111, 113, 117, 125, 127, 136, 143, 144, 146, 147], "quadrat": [65, 99], "qualiti": [97, 100, 159], "quanitl": 91, "quant": 80, "quantifi": [98, 111, 117], "quantil": [16, 23, 24, 27, 28, 29, 35, 68, 71, 75, 80, 85, 97, 108, 110, 133, 136, 137, 158, 159], "quantiti": [59, 76, 98], "queri": 90, "question": [98, 160], "quick": 91, "quit": [74, 86, 93, 97, 144, 145], "r": [25, 33, 48, 49, 52, 53, 67, 68, 69, 70, 71, 77, 78, 79, 81, 89, 92, 95, 96, 98, 100, 101, 102, 107, 108, 111, 120, 126, 127, 134, 138, 139, 143, 144, 145, 151, 152, 153, 154, 155, 157, 158, 159, 160], "r2_d": [34, 86], "r2_score": [49, 53], "r2_y": [34, 86], "r6": [64, 159], "r_0": [25, 37, 38, 41, 63, 87, 90, 111, 120, 122, 127, 138], "r_all": 60, "r_d": 34, "r_df": 89, "r_dml": 60, "r_dml_nosplit": 60, "r_dml_po": 60, "r_dml_po_nosplit": 60, "r_double_ml_bas": 60, "r_double_ml_basic_iv": 59, "r_double_ml_did": 61, "r_double_ml_multiway_clust": 62, "r_double_ml_pens": 63, "r_double_ml_pipelin": 64, "r_double_ml_ssm": 65, "r_hat": [38, 87], "r_hat0": 25, "r_hat1": 25, "r_non_orthogon": 60, "r_po_al": 60, "r_y": 34, "rais": [5, 6, 7, 8, 9, 37, 48, 49, 52, 53, 68, 69, 70, 71, 110], "randint": 92, "randn": 30, "random": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 36, 37, 38, 39, 45, 46, 47, 59, 60, 61, 63, 64, 66, 67, 68, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 104, 107, 109, 110, 112, 113, 114, 116, 117, 126, 141, 143, 144, 150, 154, 157, 158, 160], "random_search": 110, "random_st": [35, 68, 71, 74, 77, 84, 85, 93], "randomforest": [63, 74, 86, 90], "randomforest_class": [63, 78, 87, 90, 93], "randomforest_reg": [78, 87, 93], "randomforestclassifi": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 72, 74, 75, 78, 79, 82, 83, 84, 86, 87, 90, 93, 95, 97, 98, 109, 110, 111, 112, 113, 114, 116, 160], "randomforestregressor": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 72, 74, 75, 77, 78, 79, 82, 83, 84, 86, 87, 90, 93, 95, 97, 98, 100, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 144, 150, 157, 160], "randomized_search": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 110], "randomizedsearchcv": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 110], "randomli": [60, 62, 77, 89, 101, 126, 160], "rang": [8, 60, 66, 67, 74, 75, 77, 80, 82, 83, 86, 87, 88, 89, 91, 93, 94, 95, 96, 98, 99, 101, 110, 111], "rangeindex": [68, 69, 70, 71, 72, 74, 75, 84, 87, 89, 90, 91, 97, 102, 104, 107, 157], "ranger": [61, 63, 64, 100, 110, 111, 126, 127, 143, 157, 160], "rangl": [32, 93], "rank": 159, "rate": [81, 86, 111], "rather": [95, 98, 111], "ratio": [110, 126, 144, 145], "rational": 69, "ravel": [78, 79], "raw": [54, 63, 69, 70, 81, 90], "raw_res_manual_lasso_r_100_n_100_p_200_rho_0": 81, "rbind": 63, "rbindlist": 63, "rbinom": 59, "rbrace": [25, 26, 33, 34, 36, 62, 89, 100, 111, 118, 120, 121, 126, 127, 132, 143, 144, 151], "rcolorbrew": 62, "rcparam": [67, 72, 78, 79, 80, 82, 83, 85, 89, 90, 91, 94], "rd": [111, 159], "rda": 69, "rdbu": 62, "rdbu_r": 89, "rdbwselect": 111, "rdd": [0, 8, 105, 107, 108, 156], "rdflex": [95, 111, 159], "rdflex_fuzzi": 95, "rdflex_fuzzy_stack": 95, "rdflex_obj": [46, 111], "rdflex_sharp": 95, "rdflex_sharp_stack": 95, "rdrobust": [46, 95, 111, 156, 159], "rdrobust_fuzzi": 95, "rdrobust_fuzzy_noadj": 95, "rdrobust_sharp": 95, "rdrobust_sharp_noadj": 95, "rdt044": 81, "re": [68, 89, 98, 156], "read": [69, 156], "read_csv": [70, 81], "read_r": 69, "readabl": [74, 159], "reader": [69, 96], "readili": 155, "real": [63, 90, 91, 97, 144, 145], "realat": 111, "realiz": [95, 111, 125], "reason": [5, 6, 7, 8, 9, 59, 74, 76, 81, 86, 88, 97, 98, 144, 145, 160], "recal": [72, 144, 154], "receiv": [17, 19, 68, 71, 75, 95, 111, 113, 115], "recent": [88, 111, 113, 117, 158], "recogn": [63, 90, 91], "recommend": [64, 68, 71, 74, 86, 95, 98, 100, 111, 126, 144, 156, 158, 159], "recov": [59, 61, 76, 92], "recsi": 158, "red": [62, 65, 74, 82, 83, 88, 89], "reduc": [63, 84, 88, 90, 95, 97, 98, 111, 159], "redund": 159, "reemploy": [11, 102, 107, 157], "ref": 69, "refactor": 159, "refer": [10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 63, 67, 68, 71, 74, 75, 84, 87, 90, 91, 95, 97, 102, 107, 108, 109, 111, 112, 114, 116, 117, 127, 144, 145, 150, 158, 159], "reference_level": [23, 74, 75, 85, 111], "refin": 159, "refit": [144, 145], "reflect": [93, 98, 109], "reg": [17, 18, 19, 63, 90, 160], "reg_estim": 95, "reg_learn": 91, "reg_learner_1": 86, "reg_learner_2": 86, "regard": [98, 155], "regener": 159, "region": [62, 80, 89, 143, 158], "regr": [59, 60, 61, 62, 63, 64, 65, 100, 110, 111, 126, 127, 129, 143, 157, 160], "regravg": [64, 110], "regress": [8, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 55, 59, 61, 62, 64, 65, 66, 68, 69, 70, 71, 75, 76, 81, 87, 88, 89, 92, 96, 97, 98, 99, 100, 101, 105, 107, 108, 109, 110, 113, 114, 116, 120, 121, 122, 123, 124, 126, 131, 143, 145, 150, 151, 152, 153, 154, 155, 157, 158, 159, 160], "regressor": [37, 49, 53, 60, 63, 74, 75, 77, 80, 86, 87, 88, 90, 96, 101], "regular": [42, 108, 110, 127, 143, 158], "reich": [64, 110], "reinforc": 158, "reject": [63, 90], "rel": [63, 69, 74, 90, 96, 111, 112, 144, 145, 151, 152], "relat": [85, 98, 160], "relationship": [59, 74, 76, 81, 98, 143], "relev": [12, 14, 15, 16, 32, 48, 49, 50, 52, 53, 68, 71, 80, 93, 94, 102, 107, 111, 144, 160], "reli": [16, 66, 67, 68, 71, 78, 79, 84, 85, 109, 110, 111, 113, 127, 131, 144, 145, 160], "reload": 63, "remain": [61, 102, 107, 143, 160], "remark": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 60, 66, 67, 68, 69, 70, 71, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 86, 91, 97, 109, 110, 111, 112, 113, 114, 116, 126, 127, 128, 129, 130, 131, 136, 137, 143, 144, 147, 149, 152], "remot": 156, "remov": [4, 14, 16, 22, 24, 25, 26, 27, 28, 29, 30, 63, 67, 78, 85, 98, 102, 107, 108, 111, 126, 127, 144, 159], "renam": [68, 71, 90, 159], "render": [97, 98], "reorgan": 159, "rep": [60, 65, 101, 110, 143], "repeat": [12, 14, 15, 16, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 59, 60, 62, 63, 64, 65, 68, 69, 70, 77, 84, 89, 90, 91, 92, 95, 97, 99, 101, 103, 107, 108, 110, 114, 115, 116, 117, 143, 146, 147, 157, 159, 160], "repeatedkfold": 89, "repet": 97, "repetit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 50, 69, 78, 79, 81, 82, 83, 84, 86, 108, 110, 143, 157, 159, 160], "replac": [93, 98, 159], "replic": [10, 11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 63, 69, 70, 77, 81, 96, 98], "repo": 159, "report": [63, 88, 90, 155, 159], "repositori": [68, 71, 81, 95, 159], "repr": [60, 62], "repres": [17, 19, 74, 92, 98, 111], "represent": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 68, 97, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 157, 159], "reproduc": 35, "request": [48, 49, 52, 53, 159], "requir": [38, 39, 48, 52, 55, 59, 63, 64, 68, 69, 70, 71, 74, 75, 84, 90, 91, 97, 102, 107, 111, 112, 114, 115, 116, 127, 129, 131, 143, 144, 145, 150, 156, 159, 160], "requirenamespac": 61, "rerun": 69, "res_df": 89, "res_dict": [31, 32, 35, 40, 47], "resampl": [37, 59, 62, 64, 65, 66, 68, 69, 70, 71, 89, 91, 97, 99, 110, 111, 113, 114, 116, 126, 127, 143, 155, 157, 160], "resdat": 71, "research": [62, 64, 89, 92, 98, 126, 155, 157, 158, 160], "resembl": [65, 99], "reset": 61, "reset_index": [68, 71, 81, 89, 90], "reshap": [67, 77, 78, 79, 85, 102, 107], "reshape2": 62, "residu": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 49, 53, 97, 144, 145, 153, 154], "resolut": [64, 110], "resourc": 86, "resourcewis": 86, "respect": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 63, 68, 69, 70, 71, 75, 90, 91, 95, 109, 111, 115, 120, 125, 126, 144, 154, 160], "respons": [10, 64, 110], "rest": 111, "restart": 156, "restrict": 86, "restructur": 159, "restud": 81, "result": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 74, 75, 77, 78, 79, 81, 84, 85, 86, 93, 95, 96, 97, 98, 99, 101, 110, 126, 127, 128, 129, 130, 131, 144, 145, 150, 157, 159], "result_iivm": 63, "result_irm": 63, "result_plr": 63, "result_typ": 16, "results_df": 96, "retain": [48, 49, 52, 53], "retina": 92, "retir": [63, 90, 91, 97], "return": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 60, 61, 62, 64, 65, 67, 68, 69, 70, 71, 77, 80, 86, 88, 89, 92, 93, 94, 96, 97, 98, 99, 100, 110, 127, 144, 145, 159], "return_count": [74, 75, 86], "return_tune_r": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "return_typ": [5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 18, 22, 24, 25, 26, 27, 28, 29, 30, 33, 34, 36, 37, 38, 39, 41, 42, 43, 44, 45, 60, 63, 64, 65, 66, 77, 85, 86, 88, 90, 91, 97, 99, 100, 101, 102, 103, 105, 106, 107, 109, 110, 111, 113, 126, 127, 143, 144, 150, 157, 160], "rev": 62, "reveal": 84, "review": [42, 81, 158], "revist": [62, 89], "reweight": [111, 112], "rf": 95, "rf_argument": 74, "rho": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 56, 68, 71, 75, 84, 85, 95, 97, 98, 144, 145, 150, 154, 160], "rho_val": 98, "richter": [64, 110, 155, 157], "riesz": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 68, 97, 144, 145, 146, 147, 148, 149, 150, 153, 154], "riesz_rep": [144, 150], "right": [17, 19, 33, 34, 36, 42, 43, 60, 62, 77, 86, 89, 90, 91, 92, 94, 95, 98, 101, 111, 127, 128, 129, 130, 131, 143, 144, 146, 147, 148, 149, 151, 152], "rightarrow_": [60, 77, 101], "risk": [24, 108, 159], "ritov": 158, "rival": 89, "rival_ind": 89, "rmd": 61, "rmse": [61, 66, 68, 69, 70, 71, 74, 86, 88, 97, 99, 110, 111, 113, 114, 116, 127, 143, 157, 159], "rmse_dml_ml_l_fullsampl": 88, "rmse_dml_ml_l_lesstim": 88, "rmse_dml_ml_l_onfold": 88, "rmse_dml_ml_l_untun": 88, "rmse_dml_ml_m_fullsampl": 88, "rmse_dml_ml_m_lesstim": 88, "rmse_dml_ml_m_onfold": 88, "rmse_dml_ml_m_untun": 88, "rmse_g0": 74, "rmse_g1": 74, "rmse_oos_ml_l": 88, "rmse_oos_ml_m": 88, "rmse_oos_onfolds_ml_l": 88, "rmse_oos_onfolds_ml_m": 88, "rnorm": [59, 64, 102, 107, 110, 143, 157], "robin": [10, 11, 44, 62, 81, 89, 101, 155, 158], "robinson": [60, 77, 101], "robject": 89, "robu": [82, 83], "robust": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 43, 61, 68, 69, 71, 75, 84, 85, 95, 97, 98, 102, 107, 111, 144, 150, 158, 159, 160], "robust_confset": [25, 96, 159], "robust_cov": 96, "robust_length": 96, "roc\u00edo": 158, "role": [4, 5, 6, 7, 8, 9, 60, 77, 88, 101, 105, 107, 160], "romano": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 143], "root": [12, 14, 15, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 74, 81, 101, 110, 127, 158], "rotat": [88, 95], "roth": [95, 111, 113, 117, 158], "rough": [98, 160], "roughli": [68, 71, 98], "round": [54, 63, 69, 74, 75, 85, 86, 92, 98], "rout": [48, 49, 52, 53], "row": [13, 16, 60, 63, 67, 69, 72, 78, 79, 88, 89, 93, 102, 107, 111, 114, 116, 126, 157, 160], "rownam": 62, "rowv": 62, "roxygen2": 159, "royal": [98, 158], "rpart": [63, 64, 110], "rpart_cv": 64, "rprocess": 86, "rpy2": 89, "rpy2pi": 89, "rskf": 85, "rsmp": [64, 110, 126], "rsmp_tune": [64, 110], "rssb": 98, "rtype": 23, "ruben": 158, "ruiz": [59, 76], "rule": [61, 109], "run": [8, 61, 69, 74, 95, 105, 107, 111, 156, 159], "runif": 59, "runtime_learn": 64, "runtimewarn": 71, "rv": [16, 68, 71, 75, 84, 85, 97, 98, 144, 150, 160], "rva": [68, 71, 75, 84, 85, 97, 98, 144, 150, 160], "rvert": 81, "rvert_": 81, "s1": 88, "s2": 88, "s_": [43, 62, 89, 111, 125], "s_1": 44, "s_2": 44, "s_col": [4, 9, 65, 95, 99, 106, 107, 111], "s_i": [36, 65, 95, 99, 111], "s_x": [43, 62, 89], "safeguard": [66, 110], "sake": [63, 90, 98, 160], "same": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 35, 37, 38, 39, 50, 60, 62, 65, 68, 69, 71, 77, 78, 79, 84, 85, 86, 89, 91, 93, 95, 96, 97, 98, 99, 110, 111, 114, 116, 127, 130, 131, 143, 144, 152, 159], "samii": 92, "sampl": [9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 36, 37, 38, 39, 40, 43, 46, 48, 49, 52, 53, 59, 61, 62, 64, 66, 68, 69, 70, 71, 76, 82, 83, 85, 86, 89, 91, 93, 96, 97, 106, 107, 108, 110, 113, 114, 116, 117, 125, 128, 143, 144, 147, 149, 157, 158, 159], "sample_weight": [46, 48, 49, 52, 53, 95], "sant": [12, 14, 15, 17, 18, 19, 31, 35, 40, 61, 66, 68, 69, 70, 71, 111, 112, 113, 115, 117, 158], "sara": 158, "sasaki": [43, 62, 89, 158], "satisfi": [65, 69, 99, 110, 127, 143], "save": [60, 63, 69, 77, 82, 83, 86, 88, 90, 91, 110, 144, 150, 160], "savefig": 77, "saveguard": 86, "saver": [63, 90, 91], "sc": [68, 71], "scalar": 111, "scale": [17, 19, 60, 62, 67, 80, 85, 92, 94, 98, 143, 144, 147, 149, 154], "scale_color_manu": 60, "scale_fill_manu": [60, 62], "scaled_psi": 85, "scatter": [67, 74, 75, 82, 83, 87, 92, 95, 98], "scatterplot": [74, 75], "scenario": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 68, 71, 75, 84, 85, 97, 98, 111, 144, 150, 160], "scene": [78, 79, 81], "scene_camera": 81, "schacht": [81, 86, 88], "schaefer": 92, "schedul": [102, 107, 159], "scheme": [62, 89, 110, 111, 112, 126, 155], "schneider": 64, "schratz": [64, 110, 155, 157], "scienc": [45, 59, 76, 92, 158], "scikit": [69, 86, 90, 110, 155, 157, 159, 160], "scipi": [77, 87], "score": [0, 8, 12, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 41, 46, 47, 48, 49, 52, 53, 54, 55, 59, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 74, 78, 79, 80, 81, 84, 85, 86, 88, 89, 90, 91, 94, 95, 96, 97, 98, 99, 100, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 146, 147, 148, 149, 150, 152, 153, 154, 155, 159, 160], "score_col": [8, 95, 105, 107, 111], "scoring_method": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "script": 156, "sd": 59, "se": [60, 62, 77, 97, 101, 110, 126, 143, 144, 150, 158, 160], "se_aggr": 110, "se_df": 62, "se_dml": [60, 77, 101], "se_dml_po": [60, 77, 101], "se_nonorth": [60, 77], "se_orth_nosplit": [60, 77], "se_orth_po_nosplit": [60, 77], "seaborn": [13, 16, 66, 68, 71, 72, 74, 75, 77, 86, 89, 90, 91, 98, 99], "seamlessli": 74, "search": [12, 14, 15, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 110, 127], "search_mod": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 110], "searchabl": 63, "second": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 43, 60, 62, 64, 74, 77, 86, 88, 89, 100, 101, 126, 143, 144, 145, 154, 157], "secondari": [74, 75], "section": [15, 16, 18, 19, 61, 62, 63, 64, 67, 84, 88, 89, 91, 98, 103, 107, 112, 114, 115, 116, 117, 135, 146, 147, 159], "secur": 92, "see": [10, 11, 12, 14, 15, 19, 20, 21, 22, 24, 25, 26, 27, 28, 30, 34, 36, 37, 38, 39, 48, 49, 50, 52, 53, 59, 61, 62, 63, 64, 66, 68, 69, 70, 71, 74, 75, 76, 78, 79, 85, 87, 88, 89, 91, 92, 93, 95, 96, 97, 98, 110, 111, 113, 115, 117, 126, 127, 133, 135, 136, 137, 141, 142, 144, 145, 147, 149, 150, 154, 156, 157, 159], "seed": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 37, 38, 39, 46, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 107, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 144, 150, 157, 160], "seek": 92, "seem": [61, 63, 84, 90, 91, 144, 160], "seen": [82, 83, 85], "sel_cols_chiang": 89, "select": [9, 17, 19, 30, 35, 36, 42, 59, 81, 86, 95, 98, 100, 102, 106, 107, 108, 110, 125, 143, 157, 158, 159, 160], "selected_coef": 86, "selected_featur": [64, 110], "selected_learn": 86, "self": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 48, 49, 50, 51, 52, 53, 86, 88, 96, 160], "selfref": 63, "semenova": [78, 79, 158], "semi": 101, "semiparametr": 10, "sens": [97, 98], "sensemakr": [144, 145], "sensit": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 56, 108, 109, 111, 116, 145, 150, 154, 159], "sensitivity_analysi": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 68, 71, 75, 84, 85, 97, 98, 144, 150, 160], "sensitivity_benchmark": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 75, 84, 97, 98, 144, 145], "sensitivity_el": [144, 150], "sensitivity_param": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 97, 98, 144, 145, 150], "sensitivity_plot": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 75, 84, 97, 98, 144, 150], "sensitivity_summari": [68, 71, 75, 84, 85, 97, 98, 144, 150, 160], "sensitv": 85, "sensitvity_benchmark": 75, "sensiv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39], "senstiv": [144, 153], "sep": 60, "separ": [13, 74, 92, 97, 105, 106, 107, 110, 111, 126, 159], "seper": [88, 95, 97, 143, 144, 145], "seq_len": [60, 65, 101], "sequenc": 89, "sequenti": [11, 110], "ser": [68, 69, 70, 71], "seri": [68, 69, 70, 71, 98, 158], "serv": [17, 19, 102, 104, 107, 157, 159], "serverless": [158, 159], "servic": 92, "set": [4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 43, 44, 45, 48, 51, 52, 53, 55, 59, 60, 61, 62, 63, 65, 66, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 100, 101, 102, 103, 105, 107, 109, 111, 112, 114, 115, 117, 126, 127, 128, 129, 130, 131, 132, 135, 143, 144, 145, 151, 152, 153, 156, 157, 159, 160], "set_as_param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "set_config": [48, 49, 52, 53], "set_fit_request": [52, 53], "set_fold_specif": 110, "set_index": 90, "set_ml_nuisance_param": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 63, 72, 90, 110, 159], "set_param": [48, 49, 52, 53, 88, 110], "set_sample_split": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 85, 86, 126, 159], "set_score_request": [48, 49, 52, 53], "set_styl": [90, 91], "set_text": 86, "set_threshold": [60, 61, 62, 63, 64, 65, 100, 110, 111, 126, 127, 143, 157], "set_tick": 89, "set_ticklabel": 89, "set_titl": [74, 75, 85, 88, 89, 95], "set_x_d": [4, 5, 6, 7, 8, 9], "set_xlabel": [74, 75, 77, 85, 88, 89, 95], "set_xlim": 77, "set_xtick": 92, "set_xticklabel": 92, "set_ylabel": [74, 75, 85, 88, 89, 92, 95], "set_ylim": [80, 85, 88, 89, 94], "setdiff": 159, "setdiff1d": 89, "setminu": [62, 89, 143], "settings_l": 88, "settings_m": 88, "setup": [156, 159], "seven": [62, 89], "sever": [56, 63, 64, 68, 69, 70, 71, 74, 86, 88, 90, 91, 97, 98, 101, 110, 160], "shadow": 87, "shape": [8, 12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 50, 51, 52, 53, 67, 68, 71, 74, 75, 78, 79, 82, 83, 86, 89, 90, 93, 95, 97, 98, 110, 111], "share": [41, 62, 63, 89, 90], "sharma": [98, 158], "sharp": 46, "shift": [68, 71, 87], "shock": [62, 89], "short": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 97, 98, 111, 115, 144, 145, 158, 159, 160], "shortcut": 63, "shortli": [62, 64, 89, 110], "shota": 158, "should": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 48, 49, 52, 53, 63, 65, 68, 69, 71, 75, 82, 83, 86, 90, 95, 96, 97, 99, 102, 107, 109, 110, 111, 112, 119, 143, 144, 145, 155], "show": [59, 60, 62, 65, 66, 68, 71, 72, 74, 75, 76, 77, 78, 79, 81, 84, 85, 86, 88, 89, 92, 95, 96, 98, 99, 101, 144, 153, 156], "showcas": 93, "showlabel": 98, "showlegend": 98, "shown": [59, 76, 92, 157], "showscal": [78, 79, 81], "shrink": 95, "shuffl": 126, "side": [95, 111, 144, 150], "sigma": [17, 18, 19, 30, 31, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 60, 62, 65, 77, 89, 99, 101, 109, 126, 143, 144, 145, 147, 149, 150, 153, 154], "sigma2": [110, 144, 150], "sigma_": [17, 18, 19, 33, 34, 36, 40, 41, 42, 43, 44, 60, 62, 77, 89, 101], "sigma_0": [144, 154], "sigma_j": 143, "sigmoid": [41, 92], "sign": [96, 98], "signal": [50, 51], "signatur": [25, 26, 27, 28, 29, 38, 39, 127], "signif": [59, 61, 62, 63, 64, 65, 110, 111, 126, 127, 143, 157, 160], "signific": [59, 62, 63, 64, 65, 68, 71, 75, 84, 85, 90, 93, 95, 97, 98, 110, 111, 126, 127, 143, 144, 150, 157, 160], "significantli": 74, "silverman": [27, 28, 29], "sim": [17, 18, 19, 31, 32, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 47, 60, 61, 62, 65, 67, 77, 80, 89, 93, 94, 99, 101, 111], "sim_data": 70, "similar": [35, 40, 61, 64, 69, 78, 79, 84, 88, 91, 95, 96, 97, 98, 111, 127, 128, 129], "similarli": [69, 88, 96], "simpl": [32, 52, 53, 61, 64, 78, 79, 82, 83, 84, 85, 87, 93, 98, 108, 111, 144, 145], "simplest": 109, "simpli": [64, 66, 160], "simplic": [63, 86, 90, 93, 98], "simplif": [144, 146], "simplifi": [68, 71, 85, 92, 98, 109, 144, 153], "simul": [17, 18, 19, 31, 32, 33, 34, 35, 36, 40, 41, 42, 44, 45, 60, 64, 65, 68, 70, 71, 77, 78, 79, 80, 81, 82, 83, 86, 88, 94, 95, 98, 99, 101, 110, 143, 157], "simul_data": 30, "simulaten": [111, 119], "simulation_run": 81, "simult": 61, "simultan": [69, 108, 160], "sin": [32, 35, 41, 45, 67, 78, 79, 82, 83], "sinc": [31, 40, 48, 52, 63, 65, 66, 67, 75, 82, 83, 84, 86, 87, 88, 90, 92, 99, 110, 111, 113, 144, 150, 152, 156, 159], "singl": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 66, 68, 69, 70, 71, 82, 83, 91, 92, 110, 143], "single_learner_pipelin": 110, "singleton": 126, "sinh": 45, "sipp": [63, 90, 91], "site": [67, 68, 69, 70, 71, 89, 90, 95, 96, 98], "situat": [62, 89], "six": [17, 19, 62], "sixth": 89, "size": [13, 16, 30, 60, 62, 63, 64, 67, 68, 69, 70, 71, 74, 77, 80, 81, 84, 86, 88, 90, 92, 93, 94, 96, 98, 100, 102, 107, 110, 111, 112, 126, 127, 143, 144, 147, 149, 157, 160], "sizeabl": 98, "skill": 158, "sklearn": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 45, 46, 48, 49, 51, 52, 53, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 95, 96, 97, 98, 99, 100, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 144, 150, 157, 160], "skotara": 98, "slide": 92, "slight": [111, 114, 116], "slightli": [12, 14, 15, 67, 68, 71, 82, 83, 84, 86, 109, 111, 115, 117, 127, 128, 129, 130, 131, 144, 145], "slow": [60, 77, 101], "slower": [60, 77, 101], "small": [32, 65, 66, 67, 74, 85, 93, 99, 111, 144, 145, 152], "smaller": [41, 63, 66, 82, 83, 84, 88, 90, 95, 98, 111, 160], "smallest": [14, 74, 86], "smpl": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 62, 77, 86, 89, 126, 127], "smpls_cluster": [62, 89], "smucler": [96, 159], "sn": [66, 68, 71, 72, 74, 75, 77, 86, 89, 90, 91, 98, 99], "so": [52, 53, 59, 63, 64, 65, 66, 68, 69, 71, 76, 88, 90, 92, 98, 99, 110, 143, 160], "social": [92, 158], "societi": [62, 89, 98, 158], "softwar": [64, 110, 155, 157, 158, 159], "solari": 159, "sole": [69, 98], "solut": [100, 109, 127], "solv": [20, 62, 89, 109, 110, 111, 114, 115, 116, 143], "solver": [90, 99, 111], "some": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 52, 53, 63, 64, 65, 66, 67, 69, 72, 86, 88, 90, 91, 95, 96, 97, 99, 109, 110, 111, 120, 156, 159], "sometim": [86, 111, 117], "sonabend": [64, 110], "sophist": 110, "sort": [13, 74, 90, 96, 111], "sort_bi": 13, "sort_valu": [74, 75], "sourc": [64, 110, 157, 159], "sourcefileload": 81, "sp": 61, "space": [62, 74, 89, 110], "spars": [81, 110, 143, 157, 158], "sparsiti": 158, "spec": 158, "special": [62, 87, 89, 108, 111], "specialis": [105, 107], "specif": [12, 14, 15, 17, 19, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 41, 46, 62, 63, 68, 69, 70, 71, 74, 75, 85, 86, 89, 90, 98, 102, 107, 108, 109, 110, 111, 114, 116, 126, 127, 135, 143, 150, 154, 155, 157, 159], "specifi": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 46, 54, 55, 59, 62, 63, 64, 65, 66, 68, 69, 70, 71, 74, 75, 76, 78, 79, 80, 82, 83, 85, 87, 88, 89, 90, 91, 93, 94, 95, 97, 98, 99, 100, 102, 104, 107, 108, 109, 111, 132, 135, 156, 157, 159, 160], "specifii": 91, "speed": [16, 23, 29, 86], "speedup": 86, "spefici": 25, "spindler": [42, 81, 86, 88, 96, 98, 155, 158, 159], "spine": [90, 91], "spline": [78, 79, 109], "spline_basi": [78, 79, 109], "spline_grid": [78, 79], "split": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 46, 59, 62, 64, 65, 66, 68, 69, 70, 71, 85, 86, 89, 91, 93, 97, 99, 108, 109, 110, 111, 113, 114, 116, 127, 143, 157, 159], "split_sampl": [85, 86], "sponsor": [63, 90, 91], "sprintf": 60, "sq_error": 81, "sqrt": [17, 18, 19, 31, 34, 35, 40, 60, 62, 64, 72, 77, 80, 89, 94, 101, 126, 143, 144, 145, 157], "squar": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 49, 53, 63, 74, 81, 90, 110, 111, 144, 154, 158], "squarederror": [63, 90, 160], "squeez": [66, 80, 87, 94, 99], "src": 90, "ssm": [9, 36, 108, 125], "ssrn": 33, "stabil": 84, "stabl": [59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 111, 112, 155], "stack": [64, 110], "stackingclassifi": 95, "stackingregressor": 95, "stacklrn": 64, "stackrel": 111, "stage": [46, 69, 78, 79, 82, 83, 87, 93, 95, 110, 111, 159, 160], "stagger": [111, 117], "stai": [111, 117], "standard": [16, 18, 61, 64, 68, 69, 70, 71, 80, 82, 83, 95, 96, 103, 105, 107, 111, 112, 114, 115, 116, 126, 127, 143, 144, 150, 154, 159, 160], "standard_norm": [102, 107, 110, 143, 157], "standardscal": 90, "star": 111, "start": [17, 19, 61, 63, 64, 68, 69, 70, 71, 74, 78, 79, 81, 84, 86, 88, 89, 90, 94, 98, 111, 113, 155, 160], "start_dat": [17, 19], "start_tim": 74, "stat": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 77, 95, 102, 107, 110, 111, 143, 155, 158], "stat_bin": 60, "stat_dens": 63, "state": 160, "stationar": 66, "stationari": [111, 113], "statist": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 36, 37, 38, 39, 43, 56, 62, 89, 96, 97, 98, 143, 144, 150, 155, 157, 158, 159, 160], "statsmodel": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 50, 95], "statu": [61, 63, 65, 66, 90, 92, 95, 99, 111, 117], "std": [12, 15, 16, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 59, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 74, 75, 76, 78, 79, 80, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 94, 97, 98, 99, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 157, 160], "stefan": 158, "step": [19, 60, 63, 64, 77, 82, 83, 84, 90, 93, 101, 110, 111, 143, 155, 160], "stepdown": 143, "stick": [63, 90], "still": [65, 66, 78, 79, 82, 83, 84, 91, 95, 97, 99, 110, 111, 115], "stochast": [38, 39, 111, 123, 124, 157], "stock": [63, 90, 91, 96], "store": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 74, 96, 100, 110, 126, 127, 143, 144, 150, 159], "store_model": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 88], "store_predict": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 61, 90, 93], "stori": [98, 158], "str": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 46, 48, 49, 50, 52, 53, 54, 55, 63, 68, 71, 74, 75, 82, 83, 94, 95, 96, 109, 111, 159], "straightforward": [68, 71, 82, 83, 86, 109], "strategi": [54, 92, 98, 111, 160], "stratifi": [85, 86], "stratum": 92, "strength": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 96, 97, 98, 144, 145, 150, 153], "strftime": 68, "strictli": 111, "string": [12, 13, 14, 15, 16, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 109, 143, 144, 150, 157, 159], "string_label": 92, "strong": [65, 96, 99, 144, 145], "stronger": [96, 111, 115, 143, 160], "structur": [10, 11, 16, 17, 19, 44, 62, 63, 65, 69, 81, 89, 90, 96, 99, 101, 110, 155, 158, 160], "student": 158, "studi": [36, 62, 63, 81, 86, 88, 89, 90, 91, 96, 97, 111, 112, 157, 160], "style": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 68, 71, 88, 159], "styler": 159, "styliz": 98, "sub": [48, 49, 52, 53, 62, 89], "subclass": [104, 107, 159], "subfold": 110, "subgroup": [25, 63, 90, 159], "subject": [62, 89], "submiss": 159, "submit": [68, 71], "submodul": 159, "subobject": [48, 49, 52, 53], "subplot": [62, 67, 74, 75, 77, 78, 79, 80, 82, 83, 85, 86, 88, 89, 90, 91, 92, 94, 95], "subplots_adjust": 86, "subpopul": [111, 125], "subsampl": [37, 64, 86, 127, 129], "subscript": [111, 115, 127, 129, 131, 144, 145], "subsequ": [62, 74, 89], "subset": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 52, 62, 86, 89, 93, 100, 109, 110, 144, 145, 147, 149], "subseteq": 109, "substanti": [63, 90, 92], "substract": 143, "subtract": 143, "sudo": 156, "suffic": 98, "suffici": [86, 88, 98], "suggest": [62, 63, 89, 90, 98, 159], "suitabl": [65, 78, 79, 99, 111, 115], "sum": [49, 53, 62, 63, 89, 90, 91, 94, 95, 109, 143], "sum_": [17, 19, 47, 60, 62, 77, 89, 95, 100, 101, 109, 111, 112, 117, 143], "sum_i": 92, "sum_oth": 89, "sum_riv": 89, "summar": [13, 61, 68, 69, 70, 71, 74, 75, 92, 98, 100, 144, 150], "summari": [12, 15, 16, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 59, 61, 62, 64, 65, 66, 68, 69, 70, 71, 72, 75, 76, 78, 79, 80, 82, 83, 84, 85, 87, 89, 91, 94, 96, 97, 98, 99, 101, 102, 104, 107, 109, 110, 111, 112, 113, 114, 116, 126, 127, 143, 144, 157, 159, 160], "summary_df": 96, "summary_result": 63, "summary_stat": 74, "superior": 74, "suppli": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 78, 79, 82, 83, 84, 93, 102, 107, 109, 144, 145, 150, 151], "support": [25, 32, 37, 46, 55, 61, 62, 68, 69, 70, 71, 86, 87, 89, 93, 95, 106, 107, 110, 111, 120, 160], "support_s": [32, 78, 79, 82, 83, 93], "support_t": 93, "support_w": 93, "suppos": 98, "suppress": [61, 63, 64, 65], "suppresswarn": 60, "suprema": 143, "suptitl": [80, 86, 88, 91, 94], "supxlabel": [80, 91, 94], "supylabel": [80, 91, 94], "sure": [74, 75, 110, 159], "surfac": [78, 79, 81], "surgic": 96, "surpress": [62, 157], "survei": [63, 90, 91, 160], "susan": 158, "sven": [98, 155, 158], "svenklaassen": [155, 159], "svg": [60, 77], "switch": [60, 77, 98, 101], "symbol": 98, "symmetr": 45, "syntax": [95, 111], "syntaxwarn": 89, "synthesi": 158, "synthet": [17, 19, 32, 41, 47, 59, 74, 76, 78, 79, 80, 82, 83, 87, 88, 93, 94, 96], "syrgkani": [96, 98, 158], "system": 158, "szita": 158, "t": [4, 5, 7, 12, 13, 15, 16, 17, 18, 19, 24, 25, 26, 27, 28, 29, 30, 31, 35, 37, 38, 39, 40, 48, 49, 52, 53, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 78, 79, 80, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 103, 104, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 126, 127, 128, 129, 143, 144, 146, 147, 157, 160], "t_": [16, 68, 69, 70, 71, 111, 114, 115, 116, 127, 129, 131], "t_0": [37, 127, 138], "t_1_start": 86, "t_1_stop": 86, "t_2_start": 86, "t_2_stop": 86, "t_3_start": 86, "t_3_stop": 86, "t_col": [4, 5, 7, 15, 16, 68, 69, 70, 71, 103, 104, 107, 111, 112, 113, 114, 116], "t_df": 93, "t_diff": 67, "t_dml": 60, "t_g": [17, 19], "t_i": [66, 93, 95, 111, 113, 114, 117, 127, 129], "t_idx": 67, "t_nonorth": 60, "t_orth_nosplit": 60, "t_sigmoid": 93, "t_stat": 143, "t_value_ev": 14, "t_value_pr": 14, "tabl": [60, 62, 63, 64, 65, 74, 75, 96, 100, 102, 107, 110, 111, 126, 127, 143, 157, 160], "tabpfn": 159, "tabpfnclassifi": 74, "tabpfnregressor": 74, "tabular": [74, 86, 102, 107, 143, 157, 160], "taddi": 158, "tailor": [103, 107], "takatsu": 96, "take": [25, 26, 31, 32, 37, 38, 39, 40, 65, 66, 67, 68, 69, 70, 71, 78, 79, 80, 81, 82, 83, 86, 91, 94, 95, 96, 97, 99, 100, 109, 110, 111, 112, 117, 120, 121, 122, 123, 124, 127, 132, 135, 144, 151, 152, 153, 157], "taken": [63, 90, 91, 160], "taker": [25, 159], "talk": 160, "target": [12, 14, 15, 20, 21, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 52, 53, 59, 62, 63, 64, 65, 78, 79, 86, 89, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 125, 126, 127, 136, 137, 143, 144, 152, 154, 155, 157, 159, 160], "task": [59, 74, 88, 102, 107, 126, 160], "task_typ": 159, "tau": [47, 67, 80, 91, 92, 94, 95, 109, 111, 127, 133, 136, 137], "tau_": [92, 95, 111], "tau_0": [95, 111], "tau_1": 92, "tau_2": 92, "tau_vec": [80, 91, 94], "tax": [63, 90, 91], "te": [61, 78, 79, 93], "techniqu": [60, 77, 101, 126, 160], "teen": 69, "teichert": 159, "templat": 159, "ten": 88, "tend": [63, 90, 91, 111], "tensor": [78, 79], "tenth": 158, "term": [7, 14, 60, 62, 63, 64, 67, 77, 81, 89, 90, 92, 98, 101, 111, 117, 155, 160], "termin": [64, 110], "terminatorev": 64, "test": [11, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 37, 38, 39, 48, 49, 52, 53, 59, 60, 61, 62, 63, 64, 65, 68, 69, 70, 71, 77, 84, 89, 96, 98, 101, 110, 111, 126, 127, 143, 157, 158, 159, 160], "test_id": [62, 126], "test_ind": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "test_set": 126, "test_siz": 77, "text": [16, 17, 18, 19, 31, 33, 35, 37, 40, 41, 46, 47, 62, 63, 68, 69, 70, 71, 74, 80, 81, 87, 92, 93, 94, 95, 98, 109, 111, 114, 115, 116, 117, 122, 126, 127, 129, 131, 138, 144, 147, 149], "textbf": [100, 110, 160], "textposit": 98, "textrm": [144, 145, 151, 152, 153, 154], "tg": [64, 72, 102, 107, 157], "th": [62, 89], "than": [26, 60, 61, 63, 77, 81, 85, 86, 90, 91, 92, 95, 96, 97, 98, 101, 111, 115, 144, 150, 160], "thank": [61, 63, 64, 90, 159], "thatw": 67, "thei": [61, 63, 67, 82, 83, 90, 92, 96, 102, 107, 111, 144, 154], "them": [13, 63, 64, 78, 79, 80, 84, 88, 90, 94, 111], "theme": [62, 63], "theme_minim": [60, 63, 65], "theorem": [111, 117, 144, 154], "theoret": [86, 98, 126, 158], "theori": [109, 158], "therebi": [62, 64, 89, 160], "therefor": [68, 70, 71, 75, 92, 95, 97, 126, 127, 144, 153], "thereof": 41, "theta": [12, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 43, 45, 60, 62, 64, 65, 66, 67, 68, 71, 74, 75, 77, 81, 84, 85, 86, 89, 95, 97, 98, 99, 100, 101, 102, 107, 109, 110, 111, 112, 114, 115, 116, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 150, 153, 154, 157, 160], "theta_": [17, 19, 68, 71, 75, 95, 98, 109, 111, 118, 119, 143, 144, 154], "theta_0": [25, 26, 32, 37, 38, 39, 60, 62, 63, 65, 75, 77, 78, 79, 81, 82, 83, 89, 90, 98, 99, 101, 109, 111, 113, 120, 121, 123, 124, 125, 127, 136, 137, 140, 143, 144, 151, 152, 154, 157], "theta_d": 74, "theta_dml": [60, 77, 101], "theta_dml_po": [60, 77, 101], "theta_initi": 77, "theta_nonorth": [60, 77], "theta_orth_nosplit": [60, 77], "theta_orth_po_nosplit": [60, 77], "theta_resc": 60, "theta_t": 67, "thi": [4, 12, 13, 14, 15, 16, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 48, 49, 51, 52, 53, 55, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 109, 110, 111, 115, 117, 118, 119, 120, 121, 126, 127, 128, 129, 130, 131, 133, 135, 136, 137, 143, 144, 145, 150, 151, 152, 155, 156, 157, 158, 159, 160], "think": 64, "third": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 60, 69, 70, 77, 89, 101, 126], "thirion": [155, 157], "this_df": [81, 90], "this_split_ind": 89, "those": [61, 63, 69, 90, 91, 96], "though": [59, 76, 92], "thread": [92, 110], "three": [62, 64, 82, 83, 156, 159], "threshold": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 54, 55, 95, 98, 110, 111], "through": [61, 69, 80, 82, 83, 94, 95, 102, 107, 110, 111], "throughout": [84, 96], "thu": [88, 95, 109, 111], "tibbl": 61, "tick": 16, "tick_param": 95, "tight": 77, "tight_layout": [68, 71, 87, 88, 89, 95], "tighter": 95, "tild": [17, 18, 19, 31, 35, 40, 62, 89, 92, 100, 109, 126, 127, 129, 131, 136, 137, 141, 142, 143, 144, 153, 154], "tile": 96, "time": [5, 7, 12, 14, 16, 17, 19, 42, 43, 60, 61, 62, 63, 65, 66, 67, 69, 74, 77, 81, 82, 83, 89, 90, 91, 95, 97, 98, 99, 103, 104, 107, 111, 112, 113, 114, 115, 116, 117, 127, 144, 158, 159, 160], "time_budget": 88, "time_df": 67, "time_period": 67, "time_typ": [17, 19, 68, 71], "titiunik": [111, 158], "titl": [13, 16, 62, 63, 65, 68, 69, 71, 74, 75, 78, 79, 80, 81, 82, 83, 86, 87, 88, 89, 90, 91, 92, 94, 95, 98, 155], "title_fonts": [68, 71], "tmp": [67, 78, 89], "tname": 61, "tnr": [64, 110], "to_datetim": 68, "to_fram": 93, "to_numpi": [80, 84, 91, 94], "to_str": 74, "todo": [62, 72], "toeplitz": 81, "togeth": [82, 83, 106, 107, 143], "toler": 89, "tomasz": [158, 159], "toml": 159, "tongarlak": 159, "too": [55, 86], "tool": [61, 64, 97, 160], "top": [62, 86, 89, 90, 91, 95, 98, 111, 155], "total": [49, 53, 63, 88, 90, 111, 112], "total_width": 74, "tpot": 88, "track": [103, 105, 107], "tracker": 155, "tradit": [74, 143], "train": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 52, 53, 60, 62, 64, 74, 77, 78, 79, 80, 82, 83, 85, 86, 87, 89, 90, 93, 94, 100, 101, 126], "train_id": [62, 126], "train_ind": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "train_set": 126, "train_test_split": 77, "transact": 158, "transform": [31, 40, 41, 47, 74, 85, 92, 98, 160], "translat": [74, 81], "transpos": 67, "treament": 93, "treat": [16, 17, 18, 19, 26, 54, 61, 66, 67, 68, 69, 70, 71, 75, 84, 87, 93, 95, 98, 109, 111, 113, 114, 116, 117, 121, 127, 129, 131, 143, 160], "treat1_param": 92, "treat2_param": 92, "treat_var": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 110], "treatment": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 41, 44, 46, 54, 59, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 81, 84, 86, 87, 88, 89, 93, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 125, 126, 128, 129, 130, 131, 132, 133, 135, 136, 137, 143, 147, 149, 150, 151, 153, 155, 157, 158, 159, 160], "treatment_df": 67, "treatment_effect": [32, 78, 79], "treatment_level": [22, 23, 74, 75, 85, 111], "treatment_levels_plot": 74, "treatment_var": [4, 5, 6, 7, 8, 9], "tree": [26, 51, 63, 64, 66, 67, 68, 71, 74, 86, 90, 100, 108, 110, 111, 126, 127, 143, 157, 159], "tree_param": [26, 51], "tree_summari": 90, "trees_class": [63, 90], "trend": [61, 66, 67, 68, 71, 89, 111, 113, 115, 117, 158], "tri": [81, 144, 145], "triangular": [46, 95, 111], "trim": [14, 16, 22, 24, 25, 26, 27, 28, 29, 30, 63, 90, 91, 98], "trimming_rul": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 91], "trimming_threshold": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 63, 78, 85, 90, 91, 93, 94, 98], "trm": [64, 110], "true": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 35, 36, 37, 38, 39, 41, 46, 47, 48, 49, 52, 53, 55, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 107, 110, 111, 113, 126, 127, 132, 133, 136, 137, 141, 142, 143, 144, 146, 147, 148, 149, 154, 157, 160], "true_effect": [67, 78, 79, 82, 83, 96], "true_gatet_effect": 84, "true_group_effect": 84, "true_tau": 95, "truemfunct": 96, "truncat": [14, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 91], "try": [86, 97], "tune": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 74, 81, 86, 95, 108, 111, 155, 157, 159], "tune_on_fold": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 110], "tune_r": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39], "tune_set": [64, 110], "tuned_model": 88, "tuner": 110, "tunergridsearch": 64, "tupl": [12, 13, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 41, 68, 71, 111, 114, 115, 116], "turn": 98, "turrel": 45, "tutori": 63, "tw": [90, 91], "twice": 111, "twinx": [74, 75], "two": [12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 37, 38, 39, 59, 60, 63, 64, 66, 68, 69, 70, 71, 76, 77, 80, 85, 86, 88, 90, 91, 92, 93, 94, 96, 97, 98, 100, 101, 109, 110, 113, 116, 126, 136, 143, 160], "twoclass": 64, "twoearn": [63, 90, 91, 97, 160], "type": [7, 12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 35, 37, 38, 39, 40, 41, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 60, 61, 62, 63, 64, 68, 71, 77, 86, 87, 88, 89, 95, 98, 101, 108, 110, 111, 127, 139, 140, 143, 144, 153, 159, 160], "typeerror": [68, 69, 70, 71], "typic": [74, 111, 117, 155], "u": [18, 25, 26, 27, 28, 29, 31, 32, 34, 36, 40, 49, 53, 60, 61, 62, 63, 66, 67, 68, 69, 71, 74, 75, 77, 80, 82, 83, 85, 86, 89, 90, 91, 93, 94, 96, 97, 98, 101, 111, 118, 120, 121, 144, 145, 156, 160], "u_hat": [60, 77, 127], "u_i": [33, 36, 42, 45], "u_t": 18, "uehara": 158, "uhash": 64, "ulf": 158, "unambigu": 98, "unbalanc": 41, "uncertainti": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 82, 83, 85, 95, 97, 144, 150, 160], "unchang": [48, 49, 52, 53], "uncondit": [63, 68, 71, 90, 160], "unconfounded": [98, 158], "under": [25, 30, 60, 63, 66, 69, 77, 90, 93, 95, 98, 101, 111, 115, 117, 125, 143, 158], "underbrac": [60, 67, 77, 101, 109], "underfit": 88, "underli": [31, 35, 63, 64, 68, 71, 74, 75, 82, 83, 92, 93, 111, 117, 144, 145, 160], "underlin": [62, 89], "underset": [95, 111], "understand": [68, 69, 71, 74, 98], "undesir": 110, "unevenli": 126, "unifi": 107, "uniform": [18, 46, 47, 67, 76, 78, 79, 80, 93, 94, 143], "uniform_averag": [49, 53], "uniformli": [25, 68, 71, 80, 91, 143], "union": [25, 41], "uniqu": [7, 59, 68, 69, 70, 71, 74, 75, 76, 86, 95, 103, 104, 107, 111, 114, 116, 127, 144, 154], "unique_label": 88, "unit": [7, 17, 19, 60, 61, 65, 66, 67, 68, 69, 70, 71, 84, 87, 95, 99, 104, 107, 111, 113, 114, 115, 116, 117, 127, 128, 129, 130, 131, 144, 147, 149, 159], "univari": [32, 78, 79], "univers": [16, 158], "unknown": 111, "unlik": [63, 90, 91, 98], "unobserv": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 59, 63, 68, 71, 76, 90, 91, 97, 98, 111, 144, 145, 154, 160], "unpen": 61, "unstabl": [144, 145], "unter": [62, 63, 64], "untest": 98, "until": [111, 113, 159], "untreat": [87, 98, 111, 113], "untun": 74, "up": [16, 23, 29, 63, 81, 86, 87, 88, 90, 91, 97, 98, 110, 111, 113, 126, 144, 145, 156, 159, 160], "upcom": 159, "updat": [48, 49, 52, 53, 62, 89, 90, 158, 159], "update_layout": [78, 79, 81, 95, 98], "update_trac": [78, 79], "upload": 159, "upon": [127, 159], "upper": [25, 63, 64, 67, 68, 71, 74, 75, 77, 80, 84, 85, 87, 91, 94, 95, 97, 98, 110, 144, 150, 154, 160], "upper_bound": [78, 79], "upsilon": [65, 99], "upsilon_i": [65, 99], "upward": [63, 90, 91, 98], "upweight": 92, "url": [69, 81, 155, 158], "us": [4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 52, 53, 54, 55, 60, 62, 63, 65, 66, 67, 68, 69, 70, 71, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 107, 109, 111, 112, 114, 116, 126, 127, 128, 129, 130, 131, 143, 144, 145, 150, 152, 153, 154, 155, 156, 157, 159, 160], "usa": 158, "usabl": 86, "usag": [61, 68, 69, 70, 71, 72, 74, 75, 84, 87, 89, 90, 91, 97, 102, 157, 159], "use_label_encod": [90, 160], "use_other_treat_as_covari": [4, 5, 6, 7, 8, 9, 102, 107], "use_pred_offset": 110, "use_weight": 110, "usecolormap": [78, 79], "user": [20, 21, 48, 49, 52, 53, 60, 61, 62, 63, 64, 69, 75, 77, 84, 85, 86, 89, 90, 95, 97, 109, 110, 111, 127, 143, 155, 156, 157, 159, 160], "userwarn": [67, 68, 70, 71, 74, 90, 95, 96, 98], "usual": [62, 66, 68, 69, 70, 71, 74, 78, 79, 86, 89, 95, 97, 98, 109, 110, 126, 144, 154], "utab019": 41, "util": [0, 21, 67, 68, 71, 85, 86, 88, 92, 95, 98, 110, 111, 159], "v": [10, 11, 25, 26, 34, 36, 38, 39, 42, 43, 44, 49, 53, 60, 62, 63, 68, 71, 74, 75, 77, 84, 85, 86, 88, 89, 90, 92, 95, 96, 100, 101, 109, 111, 118, 120, 121, 123, 124, 143, 155, 157, 158, 159, 160], "v108": 155, "v12": [155, 157], "v2": 74, "v22": 64, "v23": 155, "v_": [43, 62, 89, 111], "v_i": [33, 34, 36, 44, 45, 60, 77, 101, 111], "v_j": 143, "val": [34, 68, 69, 70, 71, 126, 158], "val_list": 81, "valid": [5, 6, 7, 8, 9, 12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 30, 33, 37, 38, 39, 54, 55, 60, 61, 62, 63, 66, 67, 68, 69, 71, 74, 77, 80, 86, 88, 89, 90, 91, 94, 95, 98, 101, 108, 109, 110, 126, 127, 133, 136, 137, 144, 145, 158, 160], "valu": [5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 52, 53, 55, 56, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 74, 75, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 95, 96, 97, 98, 100, 102, 103, 104, 107, 108, 110, 111, 112, 117, 118, 125, 126, 133, 136, 137, 141, 142, 143, 144, 145, 150, 154, 157, 159, 160], "value_count": 90, "van": 158, "vanderpla": [155, 157], "vanish": [60, 77, 101], "var": [17, 18, 19, 31, 35, 40, 62, 89, 92, 95, 144, 145, 151, 152, 153, 154], "var_ep": 98, "varepsilon": [25, 31, 40, 43, 62, 65, 89, 99, 109, 111, 120], "varepsilon_": [17, 19, 43, 62, 68, 71, 89], "varepsilon_0": 18, "varepsilon_1": 18, "varepsilon_d": [35, 40], "varepsilon_i": [35, 42, 65, 80, 94, 99], "vari": [17, 19, 63, 67, 68, 71, 86, 90, 92, 98], "variabl": [4, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 40, 41, 46, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 74, 75, 81, 84, 87, 88, 89, 90, 91, 95, 97, 98, 99, 102, 103, 104, 105, 106, 107, 109, 110, 111, 113, 114, 116, 117, 118, 120, 121, 122, 123, 124, 126, 127, 143, 144, 145, 150, 154, 157, 158, 159, 160], "varianc": [20, 21, 62, 64, 89, 95, 97, 98, 108, 111, 126, 144, 145, 150, 152, 153, 154, 157, 159], "variant": [61, 69, 85], "variat": [12, 14, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 30, 37, 38, 39, 40, 85, 97, 144, 145, 154], "variou": [61, 88, 98, 110, 160], "varoquaux": [155, 157], "vasili": [98, 158], "vast": 74, "vector": [12, 14, 15, 17, 19, 22, 24, 25, 26, 27, 28, 30, 32, 33, 34, 36, 37, 38, 39, 42, 43, 45, 59, 62, 63, 65, 66, 76, 82, 83, 84, 87, 89, 90, 93, 96, 99, 111, 113, 122, 123, 124, 125, 143, 157, 159], "vee": [127, 129, 131], "venv": 156, "verbos": [63, 66, 67, 68, 71, 74, 77, 80, 86, 88, 91, 94, 95, 98], "veri": [61, 62, 64, 69, 84, 86, 89, 96, 98, 127, 155], "verifi": 92, "versa": [86, 92, 144, 150], "version": [4, 14, 16, 22, 24, 25, 26, 27, 28, 29, 30, 31, 48, 49, 52, 53, 62, 63, 64, 66, 67, 78, 98, 100, 102, 107, 109, 111, 114, 115, 116, 127, 143, 144, 146, 147, 148, 149, 151, 152, 155, 159], "versoin": 98, "versu": 87, "vertic": [62, 74, 75, 89], "via": [12, 14, 15, 18, 21, 22, 24, 25, 26, 27, 28, 30, 31, 37, 38, 39, 40, 54, 61, 65, 66, 67, 68, 69, 70, 71, 80, 81, 82, 83, 84, 85, 86, 89, 95, 97, 99, 100, 102, 107, 108, 109, 110, 111, 112, 113, 114, 116, 126, 133, 142, 143, 144, 145, 150, 154, 155, 156, 157, 158, 159, 160], "viabl": [12, 14, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39], "vice": [86, 92, 144, 150], "victor": [81, 98, 126, 155, 158], "vignett": [61, 159], "villa": [59, 76], "violat": [68, 71], "violet": [80, 91, 94], "vira": 158, "virtual": [69, 156], "virtualenv": 156, "visibl": [91, 95, 98], "visit": [155, 160], "visual": [13, 62, 68, 69, 70, 71, 84, 85, 88, 89, 95], "vmax": 71, "vmin": 71, "vol": 61, "volum": [98, 155], "voluntari": 92, "vv740": 89, "vv760g": 89, "w": [10, 11, 17, 18, 19, 20, 21, 31, 37, 40, 44, 48, 49, 52, 53, 62, 81, 89, 92, 93, 96, 100, 101, 111, 114, 115, 116, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 157], "w24678": 126, "w30302": 158, "w_": [17, 18, 19, 62, 89, 93, 111], "w_1": [17, 18, 19, 93], "w_2": [17, 18, 19, 93], "w_3": [17, 18, 19], "w_4": [17, 18, 19], "w_df": 93, "w_i": [36, 66, 93, 95, 100, 109, 111, 126, 127, 129, 131, 143], "wa": [62, 67, 68, 71, 88, 89, 95, 98, 159], "wage": 69, "wager": 158, "wai": [63, 86, 88, 90, 96, 98, 110, 127, 156], "wander": 45, "wang": 158, "want": [59, 62, 63, 64, 66, 76, 80, 86, 89, 94, 95, 110, 111, 155, 156, 158], "warn": [13, 16, 54, 59, 60, 61, 62, 63, 64, 65, 67, 68, 70, 71, 74, 77, 86, 90, 96, 98, 100, 110, 111, 126, 127, 143, 157, 159], "warn_msg_prefix": 96, "wayon": 62, "we": [26, 51, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 107, 109, 110, 111, 112, 114, 115, 116, 117, 121, 126, 127, 129, 131, 134, 138, 143, 144, 145, 154, 156, 157, 159, 160], "weak": [25, 144, 145, 158, 159], "weakest": [68, 71], "wealth": [10, 97], "websit": [63, 64, 69, 110, 155], "wedg": [62, 89], "week": 159, "wei": 143, "weight": [13, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 48, 49, 52, 53, 62, 63, 64, 65, 68, 69, 70, 71, 74, 75, 84, 85, 89, 90, 95, 99, 108, 110, 111, 112, 127, 132, 135, 143, 144, 151, 152, 159], "weights_bar": [22, 26, 85], "weights_dict": 85, "weiss": [155, 157], "well": [5, 6, 7, 8, 9, 52, 53, 60, 62, 77, 81, 86, 87, 88, 89, 96, 100, 101, 126, 156, 157], "were": [63, 65, 90, 91, 99, 160], "what": [61, 81, 86, 158], "when": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 48, 49, 52, 53, 55, 63, 66, 69, 74, 85, 90, 92, 96, 111, 121, 125, 127, 143, 155, 156, 157, 159], "whenev": [63, 90], "whera": [144, 152], "where": [12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 49, 50, 51, 53, 59, 60, 62, 63, 65, 66, 67, 68, 69, 70, 71, 75, 76, 77, 80, 84, 87, 89, 90, 92, 93, 94, 95, 96, 98, 99, 100, 101, 109, 110, 111, 112, 113, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 150, 151, 152, 154, 156, 157, 159, 160], "wherea": [32, 65, 66, 68, 71, 75, 96, 98, 99, 111, 114, 116, 127, 135, 144, 151, 160], "whether": [5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 36, 37, 38, 39, 46, 50, 54, 55, 63, 67, 86, 90, 91, 95, 96, 98, 102, 107, 110, 111, 144, 145, 159], "which": [5, 6, 7, 8, 9, 12, 14, 15, 16, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 48, 52, 54, 55, 59, 60, 61, 63, 64, 65, 66, 68, 69, 70, 71, 73, 74, 75, 76, 77, 81, 84, 86, 88, 90, 91, 93, 95, 97, 98, 99, 101, 102, 107, 109, 110, 111, 115, 127, 143, 144, 145, 150, 151, 152, 154, 156, 159, 160], "while": [59, 76, 111], "white": [62, 82, 83, 89, 98], "whitegrid": [90, 91], "whitnei": [98, 158], "who": [61, 63, 90, 98], "whole": [60, 66, 77, 95, 101, 110, 127, 128, 144, 145], "whom": 111, "why": [69, 74], "widehat": [68, 69, 70, 71, 111], "width": [13, 16, 60, 62, 74, 78, 79, 81], "wiki": 159, "wiksel": 158, "wild": [12, 14, 15, 16, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 143], "window": 156, "wise": [82, 83], "wish": 156, "within": [46, 62, 68, 69, 70, 71, 74, 82, 83, 89, 93, 95], "without": [25, 35, 46, 59, 60, 68, 69, 70, 71, 74, 76, 77, 86, 88, 98, 101, 108, 110, 111, 144, 145, 156, 159], "wolf": [12, 14, 15, 16, 22, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39, 143], "won": 98, "word": [46, 95, 111, 159, 160], "work": [48, 49, 52, 53, 69, 70, 73, 74, 75, 84, 86, 92, 97, 110, 111, 143, 156, 158], "workflow": [155, 159], "workspac": 90, "world": 158, "worri": 98, "wors": [49, 53], "would": [49, 53, 61, 63, 64, 68, 69, 70, 71, 78, 79, 81, 86, 90, 91, 95, 97, 98, 109, 110, 144, 154, 160], "wrapper": [4, 61, 95, 102, 107, 110], "wright": 96, "write": [60, 61, 65, 66, 77, 99, 101, 144, 154], "written": [111, 127, 144, 151, 152], "wrong": [86, 92], "wspace": 86, "wurd": [62, 63, 64], "www": [155, 156], "x": [4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 19, 22, 24, 25, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 52, 53, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 105, 106, 107, 109, 110, 111, 113, 114, 116, 117, 118, 119, 120, 121, 122, 123, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 146, 147, 148, 149, 151, 152, 153, 154, 157, 160], "x0": [74, 75, 92, 95], "x1": [62, 64, 65, 66, 74, 75, 85, 87, 88, 89, 92, 95, 97, 98, 99, 102, 105, 106, 107, 109, 110, 111, 127, 143, 144, 145, 157], "x10": [62, 64, 65, 85, 87, 88, 89, 99, 102, 106, 107, 110, 111, 127, 143, 157], "x100": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x11": [62, 64, 65, 87, 89, 99, 102, 106, 107, 110, 111, 127, 143, 157], "x12": [62, 64, 65, 87, 89, 99, 102, 106, 107, 110, 111, 127, 143, 157], "x13": [62, 64, 65, 87, 89, 99, 102, 106, 107, 110, 111, 127, 143, 157], "x14": [62, 64, 65, 87, 89, 99, 102, 106, 107, 110, 111, 127, 143, 157], "x15": [62, 64, 65, 87, 89, 99, 102, 106, 107, 110, 111, 127, 143, 157], "x16": [62, 64, 65, 87, 89, 99, 102, 106, 107, 110, 111, 127, 143, 157], "x17": [62, 64, 65, 87, 89, 99, 102, 106, 107, 110, 111, 127, 143, 157], "x18": [62, 64, 65, 87, 89, 99, 102, 106, 107, 110, 111, 127, 143, 157], "x19": [62, 64, 65, 87, 89, 99, 102, 106, 107, 110, 111, 127, 143, 157], "x1x2x3x4x5x6x7x8x9x10": 62, "x2": [62, 64, 65, 66, 74, 75, 85, 87, 88, 89, 95, 97, 98, 99, 102, 105, 106, 107, 109, 110, 111, 127, 143, 157], "x20": [62, 64, 65, 87, 89, 99, 102, 106, 107, 110, 111, 127, 143, 157], "x21": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x22": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x23": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x24": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x25": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x26": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x27": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x28": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x29": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x2_dummi": 98, "x2_preds_control": 98, "x2_preds_treat": 98, "x3": [62, 64, 65, 66, 74, 75, 85, 87, 88, 89, 97, 98, 99, 102, 105, 106, 107, 109, 110, 111, 127, 143, 157], "x30": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x31": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x32": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x33": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x34": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x35": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x36": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x37": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x38": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x39": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x4": [62, 64, 65, 66, 74, 75, 85, 87, 88, 89, 97, 98, 99, 102, 106, 107, 110, 111, 127, 143, 157], "x40": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x41": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x42": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x43": [62, 64, 65, 88, 89, 99, 102, 106, 107, 111, 157], "x44": [62, 64, 65, 88, 89, 99, 102, 106, 107, 111, 157], "x45": [62, 64, 65, 88, 89, 99, 102, 106, 107, 111, 157], "x46": [62, 64, 65, 88, 89, 99, 102, 106, 107, 111, 157], "x47": [62, 64, 65, 88, 89, 99, 102, 106, 107, 111, 157], "x48": [62, 64, 65, 88, 89, 99, 102, 106, 107, 111, 157], "x49": [62, 64, 65, 88, 89, 99, 102, 106, 107, 111, 157], "x5": [62, 64, 65, 85, 87, 88, 89, 98, 99, 102, 106, 107, 110, 111, 127, 143, 157], "x50": [62, 64, 65, 88, 89, 99, 102, 106, 107, 111, 157], "x51": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x52": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x53": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x54": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x55": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x56": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x57": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x58": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x59": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x6": [62, 64, 65, 85, 87, 88, 89, 99, 102, 106, 107, 110, 111, 127, 143, 157], "x60": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x61": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x62": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x63": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x64": [62, 64, 65, 67, 68, 69, 70, 71, 89, 90, 95, 96, 98, 99, 102, 106, 107, 111, 157], "x65": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x66": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x67": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x68": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x69": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x7": [62, 64, 65, 85, 87, 88, 89, 99, 102, 106, 107, 110, 111, 127, 143, 157], "x70": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x71": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x72": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x73": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x74": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x75": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x76": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x77": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x78": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x79": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x8": [62, 64, 65, 85, 87, 88, 89, 99, 102, 106, 107, 110, 111, 127, 143, 157], "x80": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x81": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x82": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x83": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x84": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x85": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x86": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x87": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x88": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x89": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x9": [62, 64, 65, 85, 87, 88, 89, 99, 102, 106, 107, 110, 111, 127, 143, 157], "x90": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x91": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x92": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x93": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x94": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x95": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x96": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x96x97x98x99x100ydcluster_var_icluster_var_jz": 62, "x97": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x98": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x99": [62, 64, 65, 89, 99, 102, 106, 107, 111, 157], "x_": [17, 19, 41, 43, 44, 60, 62, 67, 77, 89, 98, 101], "x_0": [67, 78, 79, 82, 83, 84], "x_1": [17, 18, 19, 31, 35, 37, 38, 39, 40, 67, 78, 79, 80, 82, 83, 84, 87, 94, 98, 111, 122, 123, 124, 144, 145, 157], "x_1x_3": [80, 94], "x_2": [17, 18, 19, 31, 35, 40, 67, 78, 79, 80, 82, 83, 84, 94, 98, 144, 145], "x_3": [17, 18, 19, 31, 35, 40, 67, 78, 79, 82, 83, 84, 144, 145], "x_4": [17, 18, 19, 31, 35, 40, 78, 79, 80, 82, 83, 84, 94], "x_5": [31, 35, 40, 78, 79, 82, 83], "x_6": [78, 79, 82, 83], "x_7": [78, 79, 82, 83], "x_8": [78, 79, 82, 83], "x_9": [78, 79, 82, 83], "x_binary_control": 98, "x_binary_tr": 98, "x_center": 74, "x_col": [4, 5, 6, 7, 8, 9, 16, 59, 62, 63, 64, 68, 69, 70, 71, 76, 81, 89, 90, 91, 93, 95, 96, 97, 98, 102, 103, 104, 107, 110, 111, 112, 114, 116, 157, 159, 160], "x_cols_bench": 98, "x_cols_binari": 98, "x_cols_poli": 89, "x_conf": 94, "x_conf_tru": 94, "x_df": 67, "x_domain": 64, "x_end": 74, "x_i": [32, 33, 34, 36, 41, 42, 44, 45, 47, 60, 65, 66, 77, 80, 82, 83, 92, 94, 95, 99, 101, 109, 111, 113, 114, 116, 117, 125, 127, 129, 131], "x_jitter": 74, "x_p": [37, 38, 39, 87, 111, 122, 123, 124, 157], "x_rang": 74, "x_start": 74, "x_train": 88, "x_true": [80, 94], "x_var": 64, "xaxis_titl": [78, 79, 81, 95, 98], "xformla": 61, "xgb": 88, "xgb_untuned_l": 88, "xgb_untuned_m": 88, "xgbclassifi": [86, 90, 92, 160], "xgboost": [60, 63, 86, 90, 92, 160], "xgbregressor": [86, 88, 90, 92, 160], "xi": [17, 18, 19, 35, 111], "xi_": 143, "xi_0": [43, 62, 89], "xi_i": [65, 99], "xiaoji": 158, "xintercept": [60, 65], "xlab": [60, 62, 63], "xlabel": [67, 68, 69, 71, 74, 75, 78, 79, 80, 82, 83, 87, 88, 90, 91, 94], "xlim": [60, 63, 74], "xmax": 74, "xmax_rel": 74, "xmin": 74, "xmin_rel": 74, "xtick": [74, 75, 88], "xval": [64, 110], "xx": 77, "y": [4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 18, 22, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 52, 53, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 116, 118, 120, 121, 122, 123, 124, 126, 127, 128, 129, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 150, 151, 152, 153, 154, 157, 160], "y0": [61, 68, 71, 74, 75, 80, 94], "y0_cvar": 80, "y0_quant": [80, 94], "y1": [61, 68, 71, 80, 94], "y1_cvar": 80, "y1_quant": [80, 94], "y_": [16, 17, 19, 43, 62, 65, 66, 67, 68, 71, 89, 99, 111, 113, 114, 116, 117, 125, 127, 129, 131], "y_0": [12, 14, 18, 47, 127, 130], "y_1": [12, 14, 18, 47, 127, 130], "y_col": [4, 5, 6, 7, 8, 9, 16, 59, 60, 62, 63, 64, 65, 68, 69, 70, 71, 76, 78, 79, 81, 82, 83, 85, 89, 90, 91, 93, 95, 96, 97, 100, 101, 102, 103, 104, 106, 107, 110, 111, 112, 114, 116, 126, 127, 157, 159, 160], "y_df": [67, 93], "y_diff": 67, "y_i": [32, 33, 34, 36, 41, 42, 44, 45, 60, 65, 66, 77, 80, 92, 93, 94, 95, 99, 101, 111, 113, 125], "y_label": [13, 16], "y_lower_quantil": [68, 71], "y_mean": [68, 71], "y_pred": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 49, 53, 86, 110], "y_train": 88, "y_true": [12, 14, 15, 22, 24, 25, 26, 27, 28, 30, 37, 38, 39, 49, 53, 86, 110], "y_upper_quantil": [68, 71], "ya": 158, "yasui": 158, "yata": 158, "yaxis_titl": [78, 79, 81, 95, 98], "year": [69, 155], "yerr": [67, 74, 75, 82, 83, 88, 90, 92, 95], "yet": [62, 68, 70, 71, 73, 111, 114, 116, 117], "yggvpl": 89, "yi": [41, 158], "yield": 111, "yintercept": 63, "ylab": [60, 62, 63], "ylabel": [67, 68, 69, 71, 74, 75, 78, 79, 80, 82, 83, 87, 88, 90, 91, 94], "ylim": 90, "ymax": 63, "ymin": 63, "yname": 61, "york": 158, "you": [48, 49, 52, 53, 59, 60, 67, 68, 69, 70, 71, 74, 76, 89, 97, 111, 155, 156, 160], "your": [86, 156], "ython": 155, "yukun": 158, "yusuk": 158, "yuya": 158, "yy": 77, "z": [4, 5, 6, 7, 8, 9, 17, 18, 19, 25, 27, 30, 31, 33, 35, 36, 38, 40, 42, 43, 59, 62, 63, 65, 68, 71, 76, 78, 79, 81, 89, 90, 94, 96, 98, 99, 109, 111, 120, 123, 127, 134, 136, 139, 142, 143, 159], "z1": [7, 16, 38, 68, 71, 103, 104, 107, 111, 112, 113, 114, 116], "z2": [7, 16, 68, 71, 103, 104, 107, 111, 112, 113, 114, 116], "z3": [7, 16, 68, 71, 103, 104, 107, 111, 112, 113, 114, 116], "z4": [7, 16, 68, 71, 103, 104, 107, 111, 112, 113, 114, 116], "z_": [43, 62, 89], "z_0": [127, 138], "z_1": [31, 35, 40, 68, 71], "z_2": [31, 35, 40], "z_3": [31, 35, 40], "z_4": [31, 35, 40, 68, 71], "z_5": 31, "z_col": [4, 5, 6, 7, 8, 9, 25, 27, 38, 59, 62, 63, 65, 76, 89, 90, 91, 96, 99, 102, 103, 107, 109, 111, 159], "z_i": [36, 42, 65, 94, 99, 111], "z_j": [17, 18, 19, 31, 35, 40], "z_true": 94, "zadik": 158, "zaxis_titl": [78, 79, 81], "zero": [14, 18, 47, 66, 67, 68, 71, 80, 85, 86, 93, 94, 97, 98, 111, 127, 129, 131, 143, 144, 147, 149], "zeros_lik": 94, "zeta": [25, 38, 39, 63, 90, 109, 111, 120, 123, 124, 157], "zeta_": [43, 62, 89], "zeta_0": [43, 62, 89], "zeta_i": [34, 42, 44, 60, 77, 101], "zeta_j": 143, "zhang": [41, 158], "zhao": [12, 14, 15, 18, 31, 35, 40, 61, 66, 68, 71, 111, 113, 117, 158], "zhou": [41, 158], "zimmert": [66, 111, 117, 158], "zip": [78, 79], "zorder": [74, 75], "\u03c4_x0": 92, "\u03c4_x1": 92, "\u2139": 60}, "titles": ["API Reference", "<span class=\"section-number\">1. </span>DoubleML Data Class", "<span class=\"section-number\">3. </span>Datasets", "<span class=\"section-number\">2. </span>DoubleML Models", "<span class=\"section-number\">1.2. </span>doubleml.data.DoubleMLClusterData", "<span class=\"section-number\">1.6. </span>doubleml.data.DoubleMLDIDData", "<span class=\"section-number\">1.1. </span>doubleml.data.DoubleMLData", "<span class=\"section-number\">1.3. </span>doubleml.data.DoubleMLPanelData", "<span class=\"section-number\">1.5. </span>doubleml.data.DoubleMLRDDData", "<span class=\"section-number\">1.4. </span>doubleml.data.DoubleMLSSMData", "<span class=\"section-number\">3.1.1. </span>doubleml.datasets.fetch_401K", "<span class=\"section-number\">3.1.2. </span>doubleml.datasets.fetch_bonus", "<span class=\"section-number\">2.3.4. </span>doubleml.did.DoubleMLDID", "<span class=\"section-number\">2.3.2. </span>doubleml.did.DoubleMLDIDAggregation", "<span class=\"section-number\">2.3.3. </span>doubleml.did.DoubleMLDIDBinary", "<span class=\"section-number\">2.3.5. </span>doubleml.did.DoubleMLDIDCS", "<span class=\"section-number\">2.3.1. </span>doubleml.did.DoubleMLDIDMulti", "<span class=\"section-number\">3.2.14. </span>doubleml.did.datasets.make_did_CS2021", "<span class=\"section-number\">3.2.13. </span>doubleml.did.datasets.make_did_SZ2020", "<span class=\"section-number\">3.2.15. </span>doubleml.did.datasets.make_did_cs_CS2021", "<span class=\"section-number\">5.1. </span>doubleml.double_ml_score_mixins.LinearScoreMixin", "<span class=\"section-number\">5.2. </span>doubleml.double_ml_score_mixins.NonLinearScoreMixin", "<span class=\"section-number\">2.2.2. </span>doubleml.irm.DoubleMLAPO", "<span class=\"section-number\">2.2.3. </span>doubleml.irm.DoubleMLAPOS", "<span class=\"section-number\">2.2.7. </span>doubleml.irm.DoubleMLCVAR", "<span class=\"section-number\">2.2.4. </span>doubleml.irm.DoubleMLIIVM", "<span class=\"section-number\">2.2.1. </span>doubleml.irm.DoubleMLIRM", "<span class=\"section-number\">2.2.6. </span>doubleml.irm.DoubleMLLPQ", "<span class=\"section-number\">2.2.5. </span>doubleml.irm.DoubleMLPQ", "<span class=\"section-number\">2.2.8. </span>doubleml.irm.DoubleMLQTE", "<span class=\"section-number\">2.2.9. </span>doubleml.irm.DoubleMLSSM", "<span class=\"section-number\">3.2.5. </span>doubleml.irm.datasets.make_confounded_irm_data", "<span class=\"section-number\">3.2.3. </span>doubleml.irm.datasets.make_heterogeneous_data", "<span class=\"section-number\">3.2.2. </span>doubleml.irm.datasets.make_iivm_data", "<span class=\"section-number\">3.2.1. </span>doubleml.irm.datasets.make_irm_data", "<span class=\"section-number\">3.2.4. </span>doubleml.irm.datasets.make_irm_data_discrete_treatments", "<span class=\"section-number\">3.2.6. </span>doubleml.irm.datasets.make_ssm_data", "<span class=\"section-number\">2.1.2. </span>doubleml.plm.DoubleMLLPLR", "<span class=\"section-number\">2.1.3. </span>doubleml.plm.DoubleMLPLIV", "<span class=\"section-number\">2.1.1. </span>doubleml.plm.DoubleMLPLR", "<span class=\"section-number\">3.2.12. </span>doubleml.plm.datasets.make_confounded_plr_data", "<span class=\"section-number\">3.2.9. </span>doubleml.plm.datasets.make_lplr_LZZ2020", "<span class=\"section-number\">3.2.10. </span>doubleml.plm.datasets.make_pliv_CHS2015", "<span class=\"section-number\">3.2.11. </span>doubleml.plm.datasets.make_pliv_multiway_cluster_CKMS2021", "<span class=\"section-number\">3.2.7. </span>doubleml.plm.datasets.make_plr_CCDDHNR2018", "<span class=\"section-number\">3.2.8. </span>doubleml.plm.datasets.make_plr_turrell2018", "<span class=\"section-number\">2.4.1. </span>doubleml.rdd.RDFlex", "<span class=\"section-number\">3.2.16. </span>doubleml.rdd.datasets.make_simple_rdd_data", "<span class=\"section-number\">4.1.2. </span>doubleml.utils.DMLDummyClassifier", "<span class=\"section-number\">4.1.1. </span>doubleml.utils.DMLDummyRegressor", "<span class=\"section-number\">4.1.3. </span>doubleml.utils.DoubleMLBLP", "<span class=\"section-number\">4.1.4. </span>doubleml.utils.DoubleMLPolicyTree", "<span class=\"section-number\">4.1.6. </span>doubleml.utils.GlobalClassifier", "<span class=\"section-number\">4.1.5. </span>doubleml.utils.GlobalRegressor", "<span class=\"section-number\">4.1.8. </span>doubleml.utils.PSProcessor", "<span class=\"section-number\">4.1.7. </span>doubleml.utils.PSProcessorConfig", "<span class=\"section-number\">4.2.1. </span>doubleml.utils.gain_statistics", "<span class=\"section-number\">5. </span>Score Mixin Classes for DoubleML Models", "<span class=\"section-number\">4. </span>Utility Classes and Functions", "R: Basic Instrumental Variables Calculation", "R: Basics of Double Machine Learning", "R: DoubleML for Difference-in-Differences", "R: Cluster Robust Double Machine Learning", "R: Impact of 401(k) on Financial Wealth", "R: Ensemble Learners and More with <code class=\"docutils literal notranslate\"><span class=\"pre\">mlr3pipelines</span></code>", "R: Sample Selection Models", "Python: Difference-in-Differences", "Python: Difference-in-Differences Pre-Testing", "Python: Panel Data with Multiple Time Periods", "Python: Real-Data Example for Multi-Period Difference-in-Differences", "Python: Panel Data Introduction", "Python: Repeated Cross-Sectional Data with Multiple Time Periods", "DML: Bonus Data", "Examples", "Python: Causal Machine Learning with TabPFN", "Python: Average Potential Outcome (APO) Models", "Python: Basic Instrumental Variables calculation", "Python: Basics of Double Machine Learning", "Python: Conditional Average Treatment Effects (CATEs) for IRM models", "Python: Conditional Average Treatment Effects (CATEs) for PLR models", "Python: Conditional Value at Risk of potential outcomes", "Python: First Stage and Causal Estimation", "Python: Group Average Treatment Effects (GATEs) for IRM models", "Python: Group Average Treatment Effects (GATEs) for PLR models", "Python: GATE Sensitivity Analysis", "Python: IRM and APO Model Comparison", "Python: Choice of learners", "Python: Log-Odds Effects for Logistic PLR models", "DoubleML meets FLAML - How to tune learners automatically within <code class=\"docutils literal notranslate\"><span class=\"pre\">DoubleML</span></code>", "Python: Cluster Robust Double Machine Learning", "Python: Impact of 401(k) on Financial Wealth", "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)", "Python: PLM and IRM for Multiple Treatments", "Python: Policy Learning with Trees", "Python: Potential Quantiles and Quantile Treatment Effects", "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)", "Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments", "Python: Sensitivity Analysis", "Python: Sensitivity Analysis for Causal ML", "Python: Sample Selection Models", "<span class=\"section-number\">6. </span>Double machine learning algorithms", "<span class=\"section-number\">1. </span>The basics of double/debiased machine learning", "DoubleMLData from dataframes", "Key arguments", "Key arguments", "Key arguments", "Key arguments", "<span class=\"section-number\">2. </span>Data Backend", "User Guide", "<span class=\"section-number\">4. </span>Heterogeneous treatment effects", "<span class=\"section-number\">7. </span>Learners, hyperparameters and hyperparameter tuning", "<span class=\"section-number\">3. </span>Models", "&lt;no title&gt;", "Panel Data", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">9. </span>Sample-splitting, cross-fitting and repeated cross-fitting", "<span class=\"section-number\">5. </span>Score functions", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">8. </span>Variance estimation and confidence intervals", "<span class=\"section-number\">10. </span>Sensitivity analysis", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "DoubleML", "Installing DoubleML", "Getting Started", "Double Machine Learning Literature", "Release Notes", "DoubleML Workflow"], "titleterms": {"": 88, "0": 160, "1": [88, 98, 160], "2": [88, 98, 160], "2011": 98, "2023": 98, "3": [88, 98, 160], "4": [98, 160], "401": [63, 90, 91, 97], "5": [98, 160], "6": 160, "7": 160, "95": 88, "A": [62, 89], "ATE": [65, 84, 92, 99], "No": [62, 89], "One": [62, 78, 79, 89], "That": 96, "The": [63, 90, 92, 101, 157], "acknowledg": [61, 155], "acycl": [59, 76], "addit": 92, "adjust": [68, 71, 95], "advanc": [95, 110, 143], "aggreg": [68, 69, 70, 71, 111], "al": 98, "algorithm": [100, 144, 155, 157], "all": [68, 71], "altern": 127, "analysi": [68, 71, 75, 84, 85, 97, 98, 144, 160], "anticip": [68, 71], "api": [0, 88], "apo": [75, 85, 111, 127, 144], "applic": [62, 89, 97], "approach": [60, 77, 86, 101], "ar": 96, "arah": 98, "arbitrari": 92, "archiv": 73, "argument": [103, 104, 105, 106, 107], "arrai": [102, 107], "asset": [63, 90], "assumpt": 98, "att": [66, 68, 69, 70, 71], "augment": 92, "automat": 88, "automl": 88, "averag": [63, 74, 75, 78, 79, 82, 83, 85, 90, 109, 111, 127, 144], "backend": [62, 63, 89, 90, 107, 157, 160], "band": 143, "base": [64, 68, 71], "basic": [59, 60, 68, 71, 76, 77, 101], "benchmark": [97, 98, 144], "bia": [60, 77, 101], "binari": [111, 127], "bonu": 72, "bootstrap": 143, "build": 156, "calcul": [59, 76], "call": 88, "callabl": 127, "case": 73, "cate": [78, 79, 92, 109], "causal": [69, 72, 74, 75, 81, 98, 127, 157, 160], "chernozhukov": 98, "choic": 86, "citat": 155, "class": [1, 57, 58, 62, 89], "cluster": [62, 89], "code": 155, "coeffici": 88, "combin": [68, 71, 81], "compar": [86, 88], "comparison": [61, 74, 85, 88], "comput": [86, 88], "conclus": [88, 98], "conda": 156, "condit": [69, 78, 79, 80, 91, 109, 127], "confid": [88, 96, 143], "construct": 110, "contrast": 75, "control": [68, 71], "covari": [68, 71, 95], "coverag": [66, 81], "cran": 156, "creat": [74, 88], "cross": [62, 66, 71, 89, 111, 113, 126, 127, 144, 157], "custom": [86, 88], "cvar": [80, 91, 109, 127], "dag": [59, 76], "data": [1, 4, 5, 6, 7, 8, 9, 59, 60, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 87, 88, 89, 90, 91, 93, 94, 95, 97, 98, 99, 101, 107, 111, 113, 127, 144, 157, 160], "datafram": [102, 107], "dataset": [2, 10, 11, 17, 18, 19, 31, 32, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 47, 72], "debias": [60, 77, 101, 157], "default": 88, "defin": [62, 89], "demo": 61, "depend": 156, "descript": [68, 71], "design": [95, 111], "detail": [61, 68, 69, 70, 71, 111], "develop": 156, "dgp": [60, 74, 75, 77], "did": [3, 12, 13, 14, 15, 16, 17, 18, 19, 61, 111], "differ": [61, 66, 67, 69, 73, 86, 111, 127, 143, 144], "dimension": [78, 79], "direct": [59, 76], "disclaim": 98, "discontinu": [95, 111], "distribut": [65, 99], "dml": [62, 72, 89, 126, 157, 160], "dml1": 100, "dml2": 100, "dmldummyclassifi": 48, "dmldummyregressor": 49, "doubl": [60, 62, 77, 89, 100, 101, 155, 157, 158], "double_ml_score_mixin": [20, 21], "doubleml": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 61, 63, 64, 74, 76, 88, 90, 97, 98, 143, 155, 156, 160], "doublemlapo": [22, 23], "doublemlblp": 50, "doublemlclusterdata": [4, 62], "doublemlcvar": 24, "doublemldata": [6, 63, 74, 89, 90, 102, 107, 157], "doublemldid": 12, "doublemldidaggreg": 13, "doublemldidbinari": 14, "doublemldidc": 15, "doublemldiddata": [5, 107], "doublemldidmulti": 16, "doublemliivm": 25, "doublemlirm": 26, "doublemllplr": 37, "doublemllpq": 27, "doublemlpaneldata": [7, 68, 71, 107], "doublemlpliv": [38, 62, 89], "doublemlplr": 39, "doublemlpolicytre": 51, "doublemlpq": 28, "doublemlqt": 29, "doublemlrdddata": [8, 107], "doublemlssm": 30, "doublemlssmdata": [9, 107], "effect": [63, 68, 69, 70, 71, 73, 74, 78, 79, 80, 82, 83, 85, 87, 90, 91, 92, 94, 97, 98, 109, 111], "elig": [63, 90], "empir": 81, "ensembl": [64, 95], "error": [62, 89], "estim": [59, 63, 65, 66, 68, 69, 70, 71, 72, 74, 76, 81, 84, 88, 90, 91, 92, 94, 97, 98, 99, 126, 127, 143, 157, 160], "et": 98, "evalu": [74, 86, 88, 110], "event": [68, 69, 70, 71], "exampl": [61, 62, 69, 73, 78, 79, 89, 97, 103, 104, 105, 106, 107], "exploit": [61, 64], "extern": [110, 126], "featur": [64, 155], "fetch_401k": 10, "fetch_bonu": 11, "figur": 92, "file": 156, "final": 61, "financi": [63, 90, 91], "first": 81, "fit": [62, 88, 89, 126, 157], "flaml": 88, "flexibl": 95, "fold": [88, 126], "forest": 72, "formul": [98, 160], "from": [61, 64, 102, 107, 156], "full": 88, "function": [58, 61, 62, 89, 127, 157], "fuzzi": [95, 111], "gain_statist": 56, "gate": [82, 83, 84, 109], "gatet": 84, "gener": [2, 60, 73, 74, 75, 77, 88, 95, 101, 144], "get": 157, "github": 156, "global": 95, "globalclassifi": 52, "globalregressor": 53, "graph": [59, 76], "group": [68, 70, 71, 82, 83, 109], "guid": 108, "helper": [62, 89], "heterogen": [73, 85, 92, 109], "how": [64, 88], "hyperparamet": [85, 110], "identif": 98, "iivm": [63, 90, 111, 127], "impact": [63, 90, 91], "implement": [100, 111, 127, 144], "import": 74, "induc": [60, 77, 101], "infer": [143, 160], "initi": [62, 88, 89], "insight": 74, "instal": 156, "instrument": [59, 76, 96], "integr": 61, "interact": [63, 82, 90, 93, 111, 127, 144], "interv": [88, 96, 143], "introduct": 70, "invers": 92, "irm": [3, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 63, 72, 78, 82, 85, 90, 92, 93, 97, 109, 111, 127, 144], "iv": [59, 63, 76, 90, 111, 127], "k": [63, 90, 91, 97, 126], "kei": [74, 103, 104, 105, 106, 107], "lambda": 81, "lasso": [72, 81], "latest": 156, "lear": [62, 89], "learn": [60, 62, 74, 77, 89, 93, 100, 101, 109, 155, 157, 158], "learner": [64, 72, 85, 86, 88, 95, 110, 157], "less": 88, "level": 111, "linear": [63, 68, 71, 83, 90, 92, 95, 111, 127, 144], "linearscoremixin": 20, "literatur": 158, "load": [62, 72, 89, 98], "loader": 2, "local": [63, 90, 91, 94, 95, 127], "log": 87, "logist": [87, 111, 127], "loss": 81, "lplr": [111, 127], "lpq": [94, 127], "lqte": [91, 94], "m": 126, "machin": [60, 62, 74, 77, 89, 100, 101, 155, 157, 158], "main": 155, "mainten": 155, "make_confounded_irm_data": 31, "make_confounded_plr_data": 40, "make_did_cs2021": 17, "make_did_cs_cs2021": 19, "make_did_sz2020": 18, "make_heterogeneous_data": 32, "make_iivm_data": 33, "make_irm_data": 34, "make_irm_data_discrete_treat": 35, "make_lplr_lzz2020": 41, "make_pliv_chs2015": 42, "make_pliv_multiway_cluster_ckms2021": 43, "make_plr_ccddhnr2018": 44, "make_plr_turrell2018": 45, "make_simple_rdd_data": 47, "make_ssm_data": 36, "mar": [65, 99], "market": [62, 89], "matric": [102, 107], "meet": 88, "method": [74, 88, 160], "metric": [86, 88], "minimum": 110, "miss": [65, 99], "missing": [111, 127], "mixin": 57, "ml": [60, 61, 77, 98, 101, 160], "mlr3": 64, "mlr3extralearn": 64, "mlr3learner": 64, "mlr3pipelin": 64, "model": [3, 57, 63, 65, 72, 74, 75, 78, 79, 82, 83, 85, 87, 88, 90, 92, 93, 96, 98, 99, 109, 111, 126, 127, 143, 144, 157, 160], "modul": 72, "more": 64, "motiv": [62, 89], "multi": 69, "multipl": [68, 71, 75, 92, 111], "multipli": 143, "naiv": [59, 76], "net": [63, 90], "neyman": [127, 157], "nonignor": [65, 99, 111, 127], "nonlinearscoremixin": 21, "nonrespons": [65, 99, 111, 127], "note": 159, "nuisanc": [88, 157], "object": [62, 74, 89, 97], "odd": 87, "option": 156, "orthogon": [60, 77, 101, 127, 157], "out": [60, 77, 101], "outcom": [65, 66, 74, 75, 80, 99, 109, 111, 127, 144], "over": 143, "overcom": [60, 77, 101], "overfit": [60, 77, 101], "overlap": 92, "packag": [61, 63, 90, 156], "panel": [66, 68, 70, 111, 113, 127, 144], "parallel": 69, "paramet": [64, 72, 88, 111, 127], "partial": [60, 63, 77, 83, 90, 92, 101, 111, 127, 144], "particip": [63, 90], "partit": 126, "penalti": 81, "perform": [61, 74, 92], "period": [68, 69, 71, 111, 127, 144], "pip": 156, "pipelin": 110, "pliv": [111, 127], "plm": [3, 37, 38, 39, 40, 41, 42, 43, 44, 45, 92, 111, 127, 144], "plot": [62, 88, 89], "plr": [63, 72, 79, 83, 87, 90, 109, 111, 127, 144], "polici": [93, 109], "potenti": [74, 75, 80, 91, 94, 109, 111, 127, 144], "pq": [94, 109, 127], "pre": 67, "predict": [61, 110], "preprocess": 64, "problem": 160, "process": [60, 62, 74, 75, 77, 89, 101], "product": [62, 89], "propens": 92, "provid": 126, "psprocessor": 54, "psprocessorconfig": 55, "python": [66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 110, 156], "qte": [94, 109], "qualiti": 81, "quantil": [91, 94, 109, 127], "question": 69, "r": [59, 60, 61, 62, 63, 64, 65, 73, 110, 156], "random": [65, 72, 99, 111, 127], "rank": 92, "rdd": [3, 46, 47, 95, 111], "rdflex": 46, "real": [62, 69, 89], "refer": [0, 59, 61, 62, 64, 76, 81, 86, 88, 89, 92, 96, 98, 101, 110, 126, 143, 155, 157], "regress": [63, 82, 83, 90, 93, 95, 111, 127, 144], "regular": [60, 77, 101], "releas": [156, 159], "remark": 61, "remov": [60, 77, 101], "repeat": [66, 71, 111, 113, 126, 127, 144], "repetit": 126, "requir": 110, "research": 69, "respect": [62, 89], "result": [62, 63, 89, 90, 92], "risk": [80, 91, 109, 127], "robust": [62, 89, 96], "run": 96, "sampl": [60, 65, 77, 88, 99, 101, 111, 126, 127], "sandbox": 73, "score": [57, 60, 77, 92, 101, 127, 157], "section": [66, 71, 111, 113, 127, 144], "select": [65, 68, 71, 99, 111, 127], "sensit": [68, 71, 75, 84, 85, 97, 98, 144, 160], "set": [64, 110], "setup": 74, "sharp": [95, 111], "simpl": [60, 77, 101], "simul": [59, 62, 66, 76, 89, 96, 97], "simultan": 143, "singl": 75, "small": 96, "sourc": [155, 156], "special": 107, "specif": [144, 160], "specifi": [72, 110, 127], "split": [60, 77, 101, 126], "ssm": 111, "stack": 95, "stage": 81, "standard": [62, 86, 89], "start": 157, "step": 88, "structur": 74, "studi": [68, 69, 70, 71, 73], "summari": [63, 74, 88, 90, 92], "tabpfn": 74, "takeawai": 74, "test": 67, "theori": 144, "time": [68, 70, 71, 86, 88], "train": 88, "treat": 85, "treatment": [63, 74, 78, 79, 80, 82, 83, 85, 90, 91, 92, 94, 109, 111, 127, 144], "tree": [93, 109], "trend": 69, "tune": [64, 88, 110], "two": [62, 78, 79, 89, 111, 127, 144], "type": 107, "uncondit": 69, "under": [65, 92, 99], "univers": [68, 71], "untun": 88, "up": 64, "us": [59, 61, 64, 72, 76, 88, 110], "usag": [103, 104, 105, 106, 107], "user": 108, "util": [48, 49, 50, 51, 52, 53, 54, 55, 56, 58], "v": 81, "valid": 143, "valu": [80, 91, 109, 127], "vanderweel": 98, "variabl": [59, 76, 96], "varianc": 143, "version": 156, "via": 127, "visual": [74, 87], "wai": [62, 89], "weak": 96, "wealth": [63, 90, 91], "weight": [92, 109], "when": 88, "whl": 156, "within": 88, "without": [95, 126], "workflow": 160, "xgboost": 88, "zero": [62, 89]}})