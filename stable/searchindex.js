Search.setIndex({"alltitles": {"(One-Way) Cluster Robust Double Machine Learing": [[54, "(One-Way)-Cluster-Robust-Double-Machine-Learing"], [80, "(One-Way)-Cluster-Robust-Double-Machine-Learing"]], "0. Problem Formulation": [[146, "problem-formulation"]], "1. Data Backend": [[146, "data-backend"]], "1. Formulation of Causal Model & Identification Assumptions": [[89, "1.-Formulation-of-Causal-Model-&-Identification-Assumptions"]], "2. Causal Model": [[146, "causal-model"]], "2. Estimation of Causal Effect": [[89, "2.-Estimation-of-Causal-Effect"]], "3. ML Methods": [[146, "ml-methods"]], "3. Sensitivity Analysis": [[89, "3.-Sensitivity-Analysis"]], "4. Benchmarking Analysis": [[89, "4.-Benchmarking-Analysis"]], "4. DML Specifications": [[146, "dml-specifications"]], "5. Conclusion": [[89, "5.-Conclusion"]], "5. Estimation": [[146, "estimation"]], "6. Inference": [[146, "inference"]], "7. Sensitivity Analysis": [[146, "sensitivity-analysis"]], "A Motivating Example: Two-Way Cluster Robust DML": [[54, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"], [80, "A-Motivating-Example:-Two-Way-Cluster-Robust-DML"]], "API Reference": [[0, null]], "ATE Estimation and Sensitivity": [[76, "ATE-Estimation-and-Sensitivity"]], "ATE estimates distribution": [[57, "ATE-estimates-distribution"], [57, "id3"], [90, "ATE-estimates-distribution"], [90, "id3"]], "ATT Estimation": [[60, "ATT-Estimation"], [60, "id1"], [62, "ATT-Estimation"], [63, "ATT-Estimation"], [63, "id1"]], "ATT Estimation: Conditional Parallel Trends": [[61, "ATT-Estimation:-Conditional-Parallel-Trends"]], "ATT Estimation: Unconditional Parallel Trends": [[61, "ATT-Estimation:-Unconditional-Parallel-Trends"]], "ATTE Estimation": [[58, "ATTE-Estimation"], [58, "id2"]], "Acknowledgements": [[141, "acknowledgements"]], "Acknowledgements and Final Remarks": [[53, "Acknowledgements-and-Final-Remarks"]], "Additional Results: CATE estimates": [[83, "Additional-Results:-CATE-estimates"]], "Advanced: External Predictions": [[98, "advanced-external-predictions"]], "Advanced: Global and Local Learners, Stacked Ensembles": [[86, "Advanced:-Global-and-Local-Learners,-Stacked-Ensembles"]], "Aggregated Effects": [[60, "Aggregated-Effects"], [63, "Aggregated-Effects"]], "Aggregation Details": [[60, "Aggregation-Details"], [61, "Aggregation-Details"], [62, "Aggregation-Details"], [63, "Aggregation-Details"]], "Algorithm DML1": [[91, "algorithm-dml1"]], "Algorithm DML2": [[91, "algorithm-dml2"]], "All Combinations": [[60, "All-Combinations"]], "All combinations": [[63, "All-combinations"]], "Anticipation": [[60, "Anticipation"], [63, "Anticipation"]], "Application Results": [[54, "Application-Results"], [80, "Application-Results"]], "Application: 401(k)": [[88, "Application:-401(k)"]], "AutoML with less Computation time": [[79, "AutoML-with-less-Computation-time"]], "Average Potential Outcome (APOs)": [[67, "Average-Potential-Outcome-(APOs)"]], "Average Potential Outcomes (APOs)": [[99, "average-potential-outcomes-apos"], [114, "average-potential-outcomes-apos"], [130, "average-potential-outcomes-apos"]], "Average Potential Outcomes (APOs) for Multiple Treatment Levels": [[99, "average-potential-outcomes-apos-for-multiple-treatment-levels"]], "Average Treatment Effect": [[77, "Average-Treatment-Effect"]], "Average Treatment Effect on the Treated": [[77, "Average-Treatment-Effect-on-the-Treated"]], "Basics": [[60, "Basics"], [63, "Basics"]], "Benchmarking": [[130, "benchmarking"]], "Benchmarking Analysis": [[88, "Benchmarking-Analysis"]], "Binary Interactive Regression Model (IRM)": [[99, "binary-interactive-regression-model-irm"], [114, "binary-interactive-regression-model-irm"]], "CATEs for IRM models": [[97, "cates-for-irm-models"]], "CATEs for PLR models": [[97, "cates-for-plr-models"]], "CVaR Treatment Effects": [[72, "CVaR-Treatment-Effects"]], "CVaR of potential outcomes": [[97, "cvar-of-potential-outcomes"]], "CVaR treatment effects": [[97, "cvar-treatment-effects"]], "Causal Analysis with DoubleML": [[89, "Causal-Analysis-with-DoubleML"]], "Causal Contrasts": [[67, "Causal-Contrasts"]], "Causal Research Question": [[61, "Causal-Research-Question"]], "Causal estimation vs. lasso penalty \\lambda": [[73, "Causal-estimation-vs.-lasso-penalty-\\lambda"]], "Chernozhukov et al. (2023): Benchmarking": [[89, "Chernozhukov-et-al.-(2023):-Benchmarking"]], "Citation": [[141, "citation"]], "Cluster Robust Cross Fitting": [[54, "Cluster-Robust-Cross-Fitting"], [80, "Cluster-Robust-Cross-Fitting"]], "Cluster Robust Standard Errors": [[54, "Cluster-Robust-Standard-Errors"], [80, "Cluster-Robust-Standard-Errors"]], "Clustering and double machine learning": [[54, "Clustering-and-double-machine-learning"], [80, "Clustering-and-double-machine-learning"]], "Combined loss vs. lasso penalty \\lambda": [[73, "Combined-loss-vs.-lasso-penalty-\\lambda"]], "Compare Metrics for Nuisance Estimation": [[79, "Compare-Metrics-for-Nuisance-Estimation"]], "Comparing different learners": [[78, "Comparing-different-learners"]], "Comparison and summary": [[79, "Comparison-and-summary"]], "Comparison to AutoML with less Computation time and Untuned XGBoost Learners": [[79, "Comparison-to-AutoML-with-less-Computation-time-and-Untuned-XGBoost-Learners"]], "Comparison to did package": [[53, "Comparison-to-did-package"]], "Computation time": [[78, "Computation-time"]], "Conclusion": [[79, "Conclusion"]], "Conditional Value at Risk (CVaR)": [[72, "Conditional-Value-at-Risk-(CVaR)"]], "Conditional average treatment effects (CATEs)": [[97, "conditional-average-treatment-effects-cates"]], "Conditional value at risk (CVaR)": [[97, "conditional-value-at-risk-cvar"], [114, "conditional-value-at-risk-cvar"]], "Confidence bands and multiplier bootstrap for valid simultaneous inference": [[129, "confidence-bands-and-multiplier-bootstrap-for-valid-simultaneous-inference"]], "Control Groups": [[60, "Control-Groups"], [63, "Control-Groups"]], "Coverage Simulation": [[58, "Coverage-Simulation"], [58, "id3"]], "Creating the DoubleMLData Object": [[66, "Creating-the-DoubleMLData-Object"]], "Cross-fitting with K folds": [[113, "cross-fitting-with-k-folds"]], "Cross-fitting, DML algorithms and Neyman-orthogonal score functions": [[143, "cross-fitting-dml-algorithms-and-neyman-orthogonal-score-functions"]], "Custom evaluation metrics": [[78, "Custom-evaluation-metrics"]], "DML: Bonus Data": [[64, null]], "Data": [[55, "Data"], [57, "Data"], [57, "id1"], [58, "Data"], [58, "id1"], [60, "Data"], [61, "Data"], [62, "Data"], [63, "Data"], [70, "Data"], [71, "Data"], [72, "Data"], [74, "Data"], [75, "Data"], [76, "Data"], [77, "Data"], [81, "Data"], [82, "Data"], [84, "Data"], [85, "Data"], [85, "id1"], [88, "Data"], [90, "Data"], [90, "id1"], [143, "data"]], "Data Backend": [[95, null]], "Data Description": [[60, "Data-Description"], [63, "Data-Description"]], "Data Details": [[60, "Data-Details"], [63, "Data-Details"]], "Data Generating Process (DGP)": [[52, "Data-Generating-Process-(DGP)"], [66, "Data-Generating-Process-(DGP)"], [67, "Data-Generating-Process-(DGP)"], [69, "Data-Generating-Process-(DGP)"]], "Data Generation": [[79, "Data-Generation"]], "Data Simulation": [[51, "Data-Simulation"], [68, "Data-Simulation"]], "Data and Effect Estimation": [[88, "Data-and-Effect-Estimation"]], "Data generating process": [[92, "data-generating-process"]], "Data preprocessing": [[56, "Data-preprocessing"]], "Data with Anticipation": [[60, "Data-with-Anticipation"], [63, "Data-with-Anticipation"]], "Data-Backend for Cluster Data": [[54, "Data-Backend-for-Cluster-Data"], [80, "Data-Backend-for-Cluster-Data"]], "Dataset Generators": [[2, "dataset-generators"]], "Dataset Loaders": [[2, "dataset-loaders"]], "Datasets": [[2, null]], "Define Helper Functions for Plotting": [[54, "Define-Helper-Functions-for-Plotting"], [80, "Define-Helper-Functions-for-Plotting"]], "Demo Example from did": [[53, "Demo-Example-from-did"]], "Details on Predictive Performance": [[53, "Details-on-Predictive-Performance"]], "Difference-in-Differences": [[65, "difference-in-differences"]], "Difference-in-Differences Models": [[114, "difference-in-differences-models"], [130, "difference-in-differences-models"]], "Difference-in-Differences Models (DID)": [[99, "difference-in-differences-models-did"]], "Difference-in-Differences for Panel Data": [[130, "difference-in-differences-for-panel-data"]], "Difference-in-Differences for repeated cross-sections": [[130, "difference-in-differences-for-repeated-cross-sections"]], "Disclaimer": [[89, "Disclaimer"]], "Double Machine Learning Algorithm": [[141, "double-machine-learning-algorithm"]], "Double Machine Learning Literature": [[144, null]], "Double machine learning algorithms": [[91, null]], "Double/debiased machine learning": [[52, "Double/debiased-machine-learning"], [69, "Double/debiased-machine-learning"], [92, "double-debiased-machine-learning"]], "DoubleML": [[141, null]], "DoubleML Data Class": [[1, null]], "DoubleML Models": [[3, null]], "DoubleML Object": [[88, "DoubleML-Object"]], "DoubleML Workflow": [[146, null]], "DoubleML meets FLAML - How to tune learners automatically within DoubleML": [[79, null]], "DoubleML with TabPFN": [[66, "DoubleML-with-TabPFN"]], "DoubleMLData": [[95, "doublemldata"]], "DoubleMLData from arrays and matrices": [[93, "doublemldata-from-arrays-and-matrices"], [95, "doublemldata-from-arrays-and-matrices"]], "DoubleMLData from dataframes": [[93, null], [95, "doublemldata-from-dataframes"]], "DoubleMLPanelData": [[60, "DoubleMLPanelData"], [63, "DoubleMLPanelData"], [95, "doublemlpaneldata"]], "Effect Aggregation": [[61, "Effect-Aggregation"], [62, "Effect-Aggregation"], [99, "effect-aggregation"]], "Effect Heterogeneity": [[65, "effect-heterogeneity"], [77, "Effect-Heterogeneity"]], "Empirical coverage vs. lasso penalty \\lambda": [[73, "Empirical-coverage-vs.-lasso-penalty-\\lambda"]], "Estimate DML models without sample-splitting": [[113, "estimate-dml-models-without-sample-splitting"]], "Estimate double/debiased machine learning models": [[143, "estimate-double-debiased-machine-learning-models"]], "Estimating Potential Quantiles and Quantile Treatment Effects": [[82, "Estimating-Potential-Quantiles-and-Quantile-Treatment-Effects"]], "Estimating local quantile treatment effects (LQTEs)": [[82, "Estimating-local-quantile-treatment-effects-(LQTEs)"]], "Estimating the Average Treatment Effect of 401(k) Eligibility on Net Financial Assets": [[55, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"], [81, "Estimating-the-Average-Treatment-Effect-of-401(k)-Eligibility-on-Net-Financial-Assets"]], "Estimating the treatment effect on the Conditional Value a Risk (CVaR)": [[82, "Estimating-the-treatment-effect-on-the-Conditional-Value-a-Risk-(CVaR)"]], "Estimation": [[57, "Estimation"], [57, "id2"], [90, "Estimation"], [90, "id2"]], "Estimation of Average Potential Outcomes": [[66, "Estimation-of-Average-Potential-Outcomes"]], "Estimation quality vs. \\lambda": [[73, "Estimation-quality-vs.-\\lambda"]], "Evaluate learners": [[98, "evaluate-learners"]], "Event Study Aggregation": [[60, "Event-Study-Aggregation"], [61, "Event-Study-Aggregation"], [62, "Event-Study-Aggregation"], [63, "Event-Study-Aggregation"]], "Example: Sensitivity Analysis for Causal ML": [[89, null]], "Examples": [[65, null]], "Exploiting the Functionalities of did": [[53, "Exploiting-the-Functionalities-of-did"]], "Externally provide a sample splitting / partition": [[113, "externally-provide-a-sample-splitting-partition"]], "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)": [[86, null]], "Fuzzy RDD": [[86, "Fuzzy-RDD"]], "Fuzzy RDD Without Adjustment": [[86, "Fuzzy-RDD-Without-Adjustment"]], "Fuzzy RDD with Flexible Adjustment": [[86, "Fuzzy-RDD-with-Flexible-Adjustment"]], "Fuzzy RDD with Linear Adjustment": [[86, "Fuzzy-RDD-with-Linear-Adjustment"]], "Fuzzy Regression Discontinuity Design": [[99, "fuzzy-regression-discontinuity-design"]], "GATE Estimation and Sensitivity": [[76, "GATE-Estimation-and-Sensitivity"]], "GATET Estimation and Sensitivity": [[76, "GATET-Estimation-and-Sensitivity"]], "GATEs for IRM models": [[97, "gates-for-irm-models"]], "GATEs for PLR models": [[97, "gates-for-plr-models"]], "General Examples": [[65, "general-examples"]], "General algorithm": [[130, "general-algorithm"]], "Generate Fuzzy Data": [[86, "Generate-Fuzzy-Data"]], "Generate Sharp Data": [[86, "Generate-Sharp-Data"]], "Getting Started": [[143, null]], "Group Aggregation": [[60, "Group-Aggregation"], [62, "Group-Aggregation"], [63, "Group-Aggregation"]], "Group Average Treatment Effects (GATEs)": [[74, "Group-Average-Treatment-Effects-(GATEs)"], [75, "Group-Average-Treatment-Effects-(GATEs)"]], "Group average treatment effects (GATEs)": [[97, "group-average-treatment-effects-gates"]], "Group-Time Combinations": [[60, "Group-Time-Combinations"], [63, "Group-Time-Combinations"]], "Heterogeneous treatment effects": [[97, null]], "How to exploit more features of mlr3pipelines in DoubleML": [[56, "How-to-exploit-more-features-of-mlr3pipelines-in-DoubleML"]], "Hyperparameter tuning": [[98, "hyperparameter-tuning"], [98, "id16"]], "Hyperparameter tuning with pipelines": [[98, "hyperparameter-tuning-with-pipelines"]], "Implementation": [[130, "implementation"]], "Implementation Details": [[99, "implementation-details"]], "Implementation of the double machine learning algorithms": [[91, "implementation-of-the-double-machine-learning-algorithms"]], "Implementation of the score function and the estimate of the causal parameter": [[114, "implementation-of-the-score-function-and-the-estimate-of-the-causal-parameter"]], "Implemented Neyman orthogonal score functions": [[114, "implemented-neyman-orthogonal-score-functions"]], "Imports and Setup": [[66, "Imports-and-Setup"]], "Initialize DoubleMLClusterData object": [[54, "Initialize-DoubleMLClusterData-object"], [80, "Initialize-DoubleMLClusterData-object"]], "Initialize the objects of class DoubleMLPLIV": [[54, "Initialize-the-objects-of-class-DoubleMLPLIV"], [80, "Initialize-the-objects-of-class-DoubleMLPLIV"]], "Installing DoubleML": [[142, null]], "Instrumental Variables Directed Acyclic Graph (IV - DAG)": [[51, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"], [68, "Instrumental-Variables-Directed-Acyclic-Graph-(IV---DAG)"]], "Interactive IV Model (IIVM)": [[55, "Interactive-IV-Model-(IIVM)"], [81, "Interactive-IV-Model-(IIVM)"]], "Interactive IV model (IIVM)": [[99, "interactive-iv-model-iivm"], [114, "interactive-iv-model-iivm"]], "Interactive Regression Model (IRM)": [[55, "Interactive-Regression-Model-(IRM)"], [74, "Interactive-Regression-Model-(IRM)"], [81, "Interactive-Regression-Model-(IRM)"], [84, "Interactive-Regression-Model-(IRM)"]], "Interactive regression model (IRM)": [[130, "interactive-regression-model-irm"]], "Interactive regression models (IRM)": [[99, "interactive-regression-models-irm"], [114, "interactive-regression-models-irm"], [130, "interactive-regression-models-irm"]], "Key Takeaways": [[66, "Key-Takeaways"]], "Learners and Hyperparameters": [[77, "Learners-and-Hyperparameters"]], "Learners to estimate the nuisance models": [[143, "learners-to-estimate-the-nuisance-models"]], "Learners, hyperparameters and hyperparameter tuning": [[98, null]], "Linear Covariate Adjustment": [[60, "Linear-Covariate-Adjustment"], [63, "Linear-Covariate-Adjustment"]], "Load Data": [[89, "Load-Data"]], "Load and Process Data": [[54, "Load-and-Process-Data"], [80, "Load-and-Process-Data"]], "Load bonus data using the dml datasets module": [[64, "Load-bonus-data-using-the-dml-datasets-module"]], "Local Average Treatment Effects of 401(k) Participation on Net Financial Assets": [[55, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"], [81, "Local-Average-Treatment-Effects-of-401(k)-Participation-on-Net-Financial-Assets"]], "Local Potential Quantile Estimation": [[85, "Local-Potential-Quantile-Estimation"]], "Local Potential Quantiles (LPQs)": [[85, "Local-Potential-Quantiles-(LPQs)"]], "Local Quantile Treatment Effects (LQTEs)": [[85, "Local-Quantile-Treatment-Effects-(LQTEs)"]], "Local potential quantiles (LPQs)": [[114, "local-potential-quantiles-lpqs"]], "Machine Learning Methods Comparison": [[66, "Machine-Learning-Methods-Comparison"]], "Main Features": [[141, "main-features"]], "Minimum requirements for learners": [[98, "minimum-requirements-for-learners"], [98, "id2"]], "Missingness at Random": [[99, "missingness-at-random"], [114, "missingness-at-random"]], "Model Performance Evaluation": [[66, "Model-Performance-Evaluation"]], "Model-specific implementations": [[130, "model-specific-implementations"]], "Models": [[99, null]], "Motivation": [[54, "Motivation"], [80, "Motivation"]], "Multiple Average Potential Outcome Models (APOS)": [[67, "Multiple-Average-Potential-Outcome-Models-(APOS)"]], "Naive estimation": [[51, "Naive-estimation"], [68, "Naive-estimation"]], "No Clustering / Zero-Way Clustering": [[54, "No-Clustering-/-Zero-Way-Clustering"], [80, "No-Clustering-/-Zero-Way-Clustering"]], "Nonignorable Nonresponse": [[99, "nonignorable-nonresponse"], [114, "nonignorable-nonresponse"]], "One-Way Clustering with Respect to the Market": [[54, "One-Way-Clustering-with-Respect-to-the-Market"], [80, "One-Way-Clustering-with-Respect-to-the-Market"]], "One-Way Clustering with Respect to the Product": [[54, "One-Way-Clustering-with-Respect-to-the-Product"], [80, "One-Way-Clustering-with-Respect-to-the-Product"]], "One-dimensional Example": [[70, "One-dimensional-Example"], [71, "One-dimensional-Example"]], "Outcome missing at random (MAR)": [[57, "Outcome-missing-at-random-(MAR)"], [90, "Outcome-missing-at-random-(MAR)"]], "Outcome missing under nonignorable nonresponse": [[57, "Outcome-missing-under-nonignorable-nonresponse"], [90, "Outcome-missing-under-nonignorable-nonresponse"]], "Overcoming regularization bias by orthogonalization": [[52, "Overcoming-regularization-bias-by-orthogonalization"], [69, "Overcoming-regularization-bias-by-orthogonalization"], [92, "overcoming-regularization-bias-by-orthogonalization"]], "Panel Data": [[99, "id9"], [101, null], [114, "panel-data"], [114, "id3"], [130, "panel-data"]], "Panel Data (Repeated Outcomes)": [[58, "Panel-Data-(Repeated-Outcomes)"]], "Panel data": [[99, "panel-data"]], "Parameter tuning": [[56, "Parameter-tuning"]], "Parameters & Implementation": [[99, "parameters-implementation"]], "Partialling out score": [[52, "Partialling-out-score"], [69, "Partialling-out-score"], [92, "partialling-out-score"]], "Partially Linear Regression Model (PLR)": [[55, "Partially-Linear-Regression-Model-(PLR)"], [75, "Partially-Linear-Regression-Model-(PLR)"], [81, "Partially-Linear-Regression-Model-(PLR)"]], "Partially linear IV regression model (PLIV)": [[99, "partially-linear-iv-regression-model-pliv"], [114, "partially-linear-iv-regression-model-pliv"]], "Partially linear models (PLM)": [[99, "partially-linear-models-plm"], [114, "partially-linear-models-plm"], [130, "partially-linear-models-plm"]], "Partially linear regression model (PLR)": [[99, "partially-linear-regression-model-plr"], [114, "partially-linear-regression-model-plr"], [130, "partially-linear-regression-model-plr"]], "Performance Summary and Insights": [[66, "Performance-Summary-and-Insights"]], "Plot Coefficients and 95% Confidence Intervals": [[79, "Plot-Coefficients-and-95%-Confidence-Intervals"]], "Policy Learning with Trees": [[84, "Policy-Learning-with-Trees"], [97, "policy-learning-with-trees"]], "Potential Quantile Estimation": [[85, "Potential-Quantile-Estimation"]], "Potential Quantiles (PQs)": [[85, "Potential-Quantiles-(PQs)"]], "Potential quantiles (PQs)": [[97, "potential-quantiles-pqs"], [114, "potential-quantiles-pqs"]], "Python: Average Potential Outcome (APO) Models": [[67, null]], "Python: Basic Instrumental Variables calculation": [[68, null]], "Python: Basics of Double Machine Learning": [[69, null]], "Python: Building the package from source": [[142, "python-building-the-package-from-source"]], "Python: Case studies": [[65, "python-case-studies"]], "Python: Causal Machine Learning with TabPFN": [[66, null]], "Python: Choice of learners": [[78, null]], "Python: Cluster Robust Double Machine Learning": [[80, null]], "Python: Conditional Average Treatment Effects (CATEs) for IRM models": [[70, null]], "Python: Conditional Average Treatment Effects (CATEs) for PLR models": [[71, null]], "Python: Conditional Value at Risk of potential outcomes": [[72, null]], "Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments": [[87, null]], "Python: Difference-in-Differences": [[58, null]], "Python: Difference-in-Differences Pre-Testing": [[59, null]], "Python: First Stage and Causal Estimation": [[73, null]], "Python: GATE Sensitivity Analysis": [[76, null]], "Python: Group Average Treatment Effects (GATEs) for IRM models": [[74, null]], "Python: Group Average Treatment Effects (GATEs) for PLR models": [[75, null]], "Python: IRM and APO Model Comparison": [[77, null]], "Python: Impact of 401(k) on Financial Wealth": [[81, null]], "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)": [[82, null]], "Python: Installing DoubleML": [[142, "python-installing-doubleml"]], "Python: Installing a released version from a .whl file": [[142, "python-installing-a-released-version-from-a-whl-file"]], "Python: Installing the latest release from pip or conda": [[142, "python-installing-the-latest-release-from-pip-or-conda"]], "Python: Learners and hyperparameters": [[98, "python-learners-and-hyperparameters"]], "Python: Optional Dependencies": [[142, "python-optional-dependencies"]], "Python: PLM and IRM for Multiple Treatments": [[83, null]], "Python: Panel Data Introduction": [[62, null]], "Python: Panel Data with Multiple Time Periods": [[60, null]], "Python: Policy Learning with Trees": [[84, null]], "Python: Potential Quantiles and Quantile Treatment Effects": [[85, null]], "Python: Real-Data Example for Multi-Period Difference-in-Differences": [[61, null]], "Python: Repeated Cross-Sectional Data with Multiple Time Periods": [[63, null]], "Python: Sample Selection Models": [[90, null]], "Python: Sensitivity Analysis": [[88, null]], "Quantile Treatment Effects (QTEs)": [[85, "Quantile-Treatment-Effects-(QTEs)"]], "Quantile treatment effects (QTEs)": [[97, "quantile-treatment-effects-qtes"]], "Quantiles": [[97, "quantiles"]], "R: Basic Instrumental Variables Calculation": [[51, null]], "R: Basics of Double Machine Learning": [[52, null]], "R: Case studies": [[65, "r-case-studies"]], "R: Cluster Robust Double Machine Learning": [[54, null]], "R: DoubleML for Difference-in-Differences": [[53, null]], "R: Ensemble Learners and More with mlr3pipelines": [[56, null]], "R: Impact of 401(k) on Financial Wealth": [[55, null]], "R: Installing DoubleML": [[142, "r-installing-doubleml"]], "R: Installing the development version from GitHub": [[142, "r-installing-the-development-version-from-github"]], "R: Installing the latest release from CRAN": [[142, "r-installing-the-latest-release-from-cran"]], "R: Learners and hyperparameters": [[98, "r-learners-and-hyperparameters"]], "R: Sample Selection Models": [[57, null]], "Ranking Treatment Effects under Treatment Propensity and Treatment Effect Heterogeneity": [[83, "Ranking-Treatment-Effects-under-Treatment-Propensity-and-Treatment-Effect-Heterogeneity"]], "Real-Data Application": [[54, "Real-Data-Application"], [80, "Real-Data-Application"]], "References": [[51, "References"], [53, "References"], [54, "References"], [56, "References"], [68, "References"], [73, "References"], [78, "References"], [79, "References"], [80, "References"], [83, "References"], [87, "References"], [89, "References"], [92, "references"], [98, "references"], [113, "references"], [129, "references"], [141, "references"], [143, "references"]], "Regression Discontinuity Designs (RDD)": [[99, "regression-discontinuity-designs-rdd"]], "Regularization Bias in Simple ML-Approaches": [[52, "Regularization-Bias-in-Simple-ML-Approaches"], [69, "Regularization-Bias-in-Simple-ML-Approaches"]], "Regularization bias in simple ML-approaches": [[92, "regularization-bias-in-simple-ml-approaches"]], "Release Notes": [[145, null]], "Repeated Cross-Sectional Data": [[58, "Repeated-Cross-Sectional-Data"], [114, "repeated-cross-sectional-data"], [114, "id4"], [130, "repeated-cross-sectional-data"]], "Repeated cross-fitting with K folds and M repetitions": [[113, "repeated-cross-fitting-with-k-folds-and-m-repetitions"]], "Repeated cross-sections": [[99, "repeated-cross-sections"], [99, "id10"], [101, "repeated-cross-sections"]], "Running a small simulation": [[87, "Running-a-small-simulation"]], "Sample Selection Models": [[114, "sample-selection-models"]], "Sample Selection Models (SSM)": [[99, "sample-selection-models-ssm"]], "Sample splitting to remove bias induced by overfitting": [[52, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [69, "Sample-splitting-to-remove-bias-induced-by-overfitting"], [92, "sample-splitting-to-remove-bias-induced-by-overfitting"]], "Sample-splitting without cross-fitting": [[113, "sample-splitting-without-cross-fitting"]], "Sample-splitting, cross-fitting and repeated cross-fitting": [[113, null]], "Sandbox/Archive": [[65, "sandbox-archive"]], "Score Mixin Classes for DoubleML Models": [[49, null]], "Score functions": [[114, null]], "Selected Combinations": [[60, "Selected-Combinations"], [63, "Selected-Combinations"]], "Sensitivity Analysis": [[60, "Sensitivity-Analysis"], [63, "Sensitivity-Analysis"], [67, "Sensitivity-Analysis"], [77, "Sensitivity-Analysis"], [88, "Sensitivity-Analysis"], [88, "id1"]], "Sensitivity Analysis with IRM": [[88, "Sensitivity-Analysis-with-IRM"]], "Sensitivity analysis": [[130, null]], "Set up learners based on mlr3pipelines": [[56, "Set-up-learners-based-on-mlr3pipelines"]], "Sharp RDD": [[86, "Sharp-RDD"]], "Sharp RDD Without Adjustment": [[86, "Sharp-RDD-Without-Adjustment"]], "Sharp RDD with Flexible Adjustment": [[86, "Sharp-RDD-with-Flexible-Adjustment"]], "Sharp RDD with Linear Adjustment": [[86, "Sharp-RDD-with-Linear-Adjustment"]], "Sharp Regression Discontinuity Design": [[99, "sharp-regression-discontinuity-design"]], "Simulate two-way cluster data": [[54, "Simulate-two-way-cluster-data"], [80, "Simulate-two-way-cluster-data"]], "Simulation Example": [[88, "Simulation-Example"]], "Simultaneous inference over different DoubleML models (advanced)": [[129, "simultaneous-inference-over-different-doubleml-models-advanced"]], "Single Average Potential Outcome Models (APO)": [[67, "Single-Average-Potential-Outcome-Models-(APO)"]], "Source code and maintenance": [[141, "source-code-and-maintenance"]], "Special Data Types": [[95, "special-data-types"]], "Specify learner and estimate causal parameter: IRM model with Lasso as learner": [[64, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: IRM model with random forest as learner": [[64, "Specify-learner-and-estimate-causal-parameter:-IRM-model-with-random-forest-as-learner"]], "Specify learner and estimate causal parameter: PLR model with Lasso as learner": [[64, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-Lasso-as-learner"]], "Specify learner and estimate causal parameter: PLR model with random forest as learner": [[64, "Specify-learner-and-estimate-causal-parameter:-PLR-model-with-random-forest-as-learner"]], "Specifying alternative score functions via callables": [[114, "specifying-alternative-score-functions-via-callables"]], "Specifying learners and set hyperparameters": [[98, "specifying-learners-and-set-hyperparameters"], [98, "id9"]], "Standard approach": [[78, "Standard-approach"]], "Step 1: Custom API for FLAML Models within DoubleML": [[79, "Step-1:-Custom-API-for-FLAML-Models-within-DoubleML"]], "Step 1: Initialize and Train the AutoML Models:": [[79, "Step-1:-Initialize-and-Train-the-AutoML-Models:"]], "Step 2: Evaluate the Tuned Models": [[79, "Step-2:-Evaluate-the-Tuned-Models"]], "Step 2: Using the API when calling DoubleML\u2019s .fit() Method": [[79, "Step-2:-Using-the-API-when-calling-DoubleML's-.fit()-Method"]], "Step 3: Create and Fit DoubleML Model": [[79, "Step-3:-Create-and-Fit-DoubleML-Model"]], "Summary Figure": [[83, "Summary-Figure"]], "Summary of Results": [[55, "Summary-of-Results"], [81, "Summary-of-Results"]], "The Augmented Inverse Propensity Weighting Model estimates the ATE under arbitrary effect and propensity score heterogeneity": [[83, "The-Augmented-Inverse-Propensity-Weighting-Model-estimates-the-ATE-under-arbitrary-effect-and-propensity-score-heterogeneity"]], "The Data Backend: DoubleMLData": [[55, "The-Data-Backend:-DoubleMLData"], [81, "The-Data-Backend:-DoubleMLData"]], "The DoubleML package": [[55, "The-DoubleML-package"], [81, "The-DoubleML-package"]], "The Partially Linear Model performs overlap weighting": [[83, "The-Partially-Linear-Model-performs-overlap-weighting"]], "The basics of double/debiased machine learning": [[92, null]], "The causal model": [[143, "the-causal-model"]], "The data-backend DoubleMLData": [[143, "the-data-backend-doublemldata"]], "Theory": [[130, "theory"]], "Time Aggregation": [[60, "Time-Aggregation"], [62, "Time-Aggregation"], [63, "Time-Aggregation"]], "Tuning on the Folds": [[79, "Tuning-on-the-Folds"]], "Tuning on the full Sample": [[79, "Tuning-on-the-full-Sample"]], "Two treatment periods": [[99, "two-treatment-periods"], [114, "two-treatment-periods"], [130, "two-treatment-periods"]], "Two-Dimensional Example": [[70, "Two-Dimensional-Example"], [71, "Two-Dimensional-Example"]], "Two-Way Clustering with Respect to Product and Market": [[54, "Two-Way-Clustering-with-Respect-to-Product-and-Market"], [80, "Two-Way-Clustering-with-Respect-to-Product-and-Market"]], "Universal Base Period": [[60, "Universal-Base-Period"], [63, "Universal-Base-Period"]], "Untuned (default parameter) XGBoost": [[79, "Untuned-(default-parameter)-XGBoost"]], "Use ensemble learners based on mlr3pipelines": [[56, "Use-ensemble-learners-based-on-mlr3pipelines"]], "User Guide": [[96, null]], "Using DoubleML": [[51, "Using-DoubleML"], [68, "Using-DoubleML"]], "Using ML for DiD: Integrating DoubleML in did": [[53, "Using-ML-for-DiD:-Integrating-DoubleML-in-did"]], "Using learners from mlr3, mlr3learners and mlr3extralearners": [[56, "Using-learners-from-mlr3,-mlr3learners-and-mlr3extralearners"]], "Using pipelines to construct learners": [[98, "using-pipelines-to-construct-learners"]], "Utility Classes": [[50, "utility-classes"]], "Utility Classes and Functions": [[50, null]], "Utility Functions": [[50, "utility-functions"]], "VanderWeele and Arah (2011): Benchmarking": [[89, "VanderWeele-and-Arah-(2011):-Benchmarking"]], "Variance estimation": [[129, "variance-estimation"]], "Variance estimation and confidence intervals": [[129, null]], "Visualizing Average Potential Outcomes": [[66, "Visualizing-Average-Potential-Outcomes"]], "Visualizing Average Treatment Effects": [[66, "Visualizing-Average-Treatment-Effects"]], "Visualizing the Treatment Effect Structure": [[66, "Visualizing-the-Treatment-Effect-Structure"]], "Weighted Average Treatment Effects": [[97, "weighted-average-treatment-effects"]], "doubleml.data.DoubleMLClusterData": [[4, null]], "doubleml.data.DoubleMLData": [[5, null]], "doubleml.data.DoubleMLPanelData": [[6, null]], "doubleml.datasets.fetch_401K": [[7, null]], "doubleml.datasets.fetch_bonus": [[8, null]], "doubleml.datasets.make_confounded_irm_data": [[9, null]], "doubleml.datasets.make_confounded_plr_data": [[10, null]], "doubleml.datasets.make_heterogeneous_data": [[11, null]], "doubleml.datasets.make_iivm_data": [[12, null]], "doubleml.datasets.make_irm_data": [[13, null]], "doubleml.datasets.make_irm_data_discrete_treatments": [[14, null]], "doubleml.datasets.make_pliv_CHS2015": [[15, null]], "doubleml.datasets.make_pliv_multiway_cluster_CKMS2021": [[16, null]], "doubleml.datasets.make_plr_CCDDHNR2018": [[17, null]], "doubleml.datasets.make_plr_turrell2018": [[18, null]], "doubleml.datasets.make_ssm_data": [[19, null]], "doubleml.did": [[3, "doubleml-did"]], "doubleml.did.DoubleMLDID": [[20, null]], "doubleml.did.DoubleMLDIDAggregation": [[21, null]], "doubleml.did.DoubleMLDIDBinary": [[22, null]], "doubleml.did.DoubleMLDIDCS": [[23, null]], "doubleml.did.DoubleMLDIDMulti": [[24, null]], "doubleml.did.datasets.make_did_CS2021": [[25, null]], "doubleml.did.datasets.make_did_SZ2020": [[26, null]], "doubleml.double_ml_score_mixins.LinearScoreMixin": [[27, null]], "doubleml.double_ml_score_mixins.NonLinearScoreMixin": [[28, null]], "doubleml.irm": [[3, "doubleml-irm"]], "doubleml.irm.DoubleMLAPO": [[29, null]], "doubleml.irm.DoubleMLAPOS": [[30, null]], "doubleml.irm.DoubleMLCVAR": [[31, null]], "doubleml.irm.DoubleMLIIVM": [[32, null]], "doubleml.irm.DoubleMLIRM": [[33, null]], "doubleml.irm.DoubleMLLPQ": [[34, null]], "doubleml.irm.DoubleMLPQ": [[35, null]], "doubleml.irm.DoubleMLQTE": [[36, null]], "doubleml.irm.DoubleMLSSM": [[37, null]], "doubleml.plm": [[3, "doubleml-plm"]], "doubleml.plm.DoubleMLPLIV": [[38, null]], "doubleml.plm.DoubleMLPLR": [[39, null]], "doubleml.rdd": [[3, "doubleml-rdd"]], "doubleml.rdd.RDFlex": [[40, null]], "doubleml.rdd.datasets.make_simple_rdd_data": [[41, null]], "doubleml.utils.DMLDummyClassifier": [[42, null]], "doubleml.utils.DMLDummyRegressor": [[43, null]], "doubleml.utils.DoubleMLBLP": [[44, null]], "doubleml.utils.DoubleMLPolicyTree": [[45, null]], "doubleml.utils.GlobalClassifier": [[46, null]], "doubleml.utils.GlobalRegressor": [[47, null]], "doubleml.utils.gain_statistics": [[48, null]]}, "docnames": ["api/api", "api/data_class", "api/datasets", "api/dml_models", "api/generated/doubleml.data.DoubleMLClusterData", "api/generated/doubleml.data.DoubleMLData", "api/generated/doubleml.data.DoubleMLPanelData", "api/generated/doubleml.datasets.fetch_401K", "api/generated/doubleml.datasets.fetch_bonus", "api/generated/doubleml.datasets.make_confounded_irm_data", "api/generated/doubleml.datasets.make_confounded_plr_data", "api/generated/doubleml.datasets.make_heterogeneous_data", "api/generated/doubleml.datasets.make_iivm_data", "api/generated/doubleml.datasets.make_irm_data", "api/generated/doubleml.datasets.make_irm_data_discrete_treatments", "api/generated/doubleml.datasets.make_pliv_CHS2015", "api/generated/doubleml.datasets.make_pliv_multiway_cluster_CKMS2021", "api/generated/doubleml.datasets.make_plr_CCDDHNR2018", "api/generated/doubleml.datasets.make_plr_turrell2018", "api/generated/doubleml.datasets.make_ssm_data", "api/generated/doubleml.did.DoubleMLDID", "api/generated/doubleml.did.DoubleMLDIDAggregation", "api/generated/doubleml.did.DoubleMLDIDBinary", "api/generated/doubleml.did.DoubleMLDIDCS", "api/generated/doubleml.did.DoubleMLDIDMulti", "api/generated/doubleml.did.datasets.make_did_CS2021", "api/generated/doubleml.did.datasets.make_did_SZ2020", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin", "api/generated/doubleml.irm.DoubleMLAPO", "api/generated/doubleml.irm.DoubleMLAPOS", "api/generated/doubleml.irm.DoubleMLCVAR", "api/generated/doubleml.irm.DoubleMLIIVM", "api/generated/doubleml.irm.DoubleMLIRM", "api/generated/doubleml.irm.DoubleMLLPQ", "api/generated/doubleml.irm.DoubleMLPQ", "api/generated/doubleml.irm.DoubleMLQTE", "api/generated/doubleml.irm.DoubleMLSSM", "api/generated/doubleml.plm.DoubleMLPLIV", "api/generated/doubleml.plm.DoubleMLPLR", "api/generated/doubleml.rdd.RDFlex", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data", "api/generated/doubleml.utils.DMLDummyClassifier", "api/generated/doubleml.utils.DMLDummyRegressor", "api/generated/doubleml.utils.DoubleMLBLP", "api/generated/doubleml.utils.DoubleMLPolicyTree", "api/generated/doubleml.utils.GlobalClassifier", "api/generated/doubleml.utils.GlobalRegressor", "api/generated/doubleml.utils.gain_statistics", "api/mixins", "api/utility", "examples/R_double_ml_basic_iv", "examples/R_double_ml_basics", "examples/R_double_ml_did", "examples/R_double_ml_multiway_cluster", "examples/R_double_ml_pension", "examples/R_double_ml_pipeline", "examples/R_double_ml_ssm", "examples/did/py_did", "examples/did/py_did_pretest", "examples/did/py_panel", "examples/did/py_panel_data_example", "examples/did/py_panel_simple", "examples/did/py_rep_cs", "examples/double_ml_bonus_data", "examples/index", "examples/learners/py_tabpfn", "examples/py_double_ml_apo", "examples/py_double_ml_basic_iv", "examples/py_double_ml_basics", "examples/py_double_ml_cate", "examples/py_double_ml_cate_plr", "examples/py_double_ml_cvar", "examples/py_double_ml_firststage", "examples/py_double_ml_gate", "examples/py_double_ml_gate_plr", "examples/py_double_ml_gate_sensitivity", "examples/py_double_ml_irm_vs_apo", "examples/py_double_ml_learner", "examples/py_double_ml_meets_flaml", "examples/py_double_ml_multiway_cluster", "examples/py_double_ml_pension", "examples/py_double_ml_pension_qte", "examples/py_double_ml_plm_irm_hetfx", "examples/py_double_ml_policy_tree", "examples/py_double_ml_pq", "examples/py_double_ml_rdflex", "examples/py_double_ml_robust_iv", "examples/py_double_ml_sensitivity", "examples/py_double_ml_sensitivity_booking", "examples/py_double_ml_ssm", "guide/algorithms", "guide/basics", "guide/data/base_data", "guide/data/panel_data", "guide/data_backend", "guide/guide", "guide/heterogeneity", "guide/learners", "guide/models", "guide/models/did/did_aggregation", "guide/models/did/did_binary", "guide/models/did/did_cs", "guide/models/did/did_implementation", "guide/models/did/did_pa", "guide/models/did/did_setup", "guide/models/irm/apo", "guide/models/irm/apos", "guide/models/irm/iivm", "guide/models/irm/irm", "guide/models/plm/pliv", "guide/models/plm/plr", "guide/models/ssm/ssm", "guide/resampling", "guide/scores", "guide/scores/did/did_cs_binary_score", "guide/scores/did/did_cs_score", "guide/scores/did/did_pa_binary_score", "guide/scores/did/did_pa_score", "guide/scores/irm/apo_score", "guide/scores/irm/cvar_score", "guide/scores/irm/iivm_score", "guide/scores/irm/irm_score", "guide/scores/irm/lpq_score", "guide/scores/irm/pq_score", "guide/scores/plm/pliv_score", "guide/scores/plm/plr_score", "guide/scores/ssm/mar_score", "guide/scores/ssm/nr_score", "guide/se_confint", "guide/sensitivity", "guide/sensitivity/benchmarking", "guide/sensitivity/did/did_cs_binary_sensitivity", "guide/sensitivity/did/did_cs_sensitivity", "guide/sensitivity/did/did_pa_binary_sensitivity", "guide/sensitivity/did/did_pa_sensitivity", "guide/sensitivity/implementation", "guide/sensitivity/irm/apo_sensitivity", "guide/sensitivity/irm/irm_sensitivity", "guide/sensitivity/plm/plr_sensitivity", "guide/sensitivity/theory", "index", "intro/install", "intro/intro", "literature/literature", "release/release", "workflow/workflow"], "envversion": {"nbsphinx": 4, "sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1}, "filenames": ["api/api.rst", "api/data_class.rst", "api/datasets.rst", "api/dml_models.rst", "api/generated/doubleml.data.DoubleMLClusterData.rst", "api/generated/doubleml.data.DoubleMLData.rst", "api/generated/doubleml.data.DoubleMLPanelData.rst", "api/generated/doubleml.datasets.fetch_401K.rst", "api/generated/doubleml.datasets.fetch_bonus.rst", "api/generated/doubleml.datasets.make_confounded_irm_data.rst", "api/generated/doubleml.datasets.make_confounded_plr_data.rst", "api/generated/doubleml.datasets.make_heterogeneous_data.rst", "api/generated/doubleml.datasets.make_iivm_data.rst", "api/generated/doubleml.datasets.make_irm_data.rst", "api/generated/doubleml.datasets.make_irm_data_discrete_treatments.rst", "api/generated/doubleml.datasets.make_pliv_CHS2015.rst", "api/generated/doubleml.datasets.make_pliv_multiway_cluster_CKMS2021.rst", "api/generated/doubleml.datasets.make_plr_CCDDHNR2018.rst", "api/generated/doubleml.datasets.make_plr_turrell2018.rst", "api/generated/doubleml.datasets.make_ssm_data.rst", "api/generated/doubleml.did.DoubleMLDID.rst", "api/generated/doubleml.did.DoubleMLDIDAggregation.rst", "api/generated/doubleml.did.DoubleMLDIDBinary.rst", "api/generated/doubleml.did.DoubleMLDIDCS.rst", "api/generated/doubleml.did.DoubleMLDIDMulti.rst", "api/generated/doubleml.did.datasets.make_did_CS2021.rst", "api/generated/doubleml.did.datasets.make_did_SZ2020.rst", "api/generated/doubleml.double_ml_score_mixins.LinearScoreMixin.rst", "api/generated/doubleml.double_ml_score_mixins.NonLinearScoreMixin.rst", "api/generated/doubleml.irm.DoubleMLAPO.rst", "api/generated/doubleml.irm.DoubleMLAPOS.rst", "api/generated/doubleml.irm.DoubleMLCVAR.rst", "api/generated/doubleml.irm.DoubleMLIIVM.rst", "api/generated/doubleml.irm.DoubleMLIRM.rst", "api/generated/doubleml.irm.DoubleMLLPQ.rst", "api/generated/doubleml.irm.DoubleMLPQ.rst", "api/generated/doubleml.irm.DoubleMLQTE.rst", "api/generated/doubleml.irm.DoubleMLSSM.rst", "api/generated/doubleml.plm.DoubleMLPLIV.rst", "api/generated/doubleml.plm.DoubleMLPLR.rst", "api/generated/doubleml.rdd.RDFlex.rst", "api/generated/doubleml.rdd.datasets.make_simple_rdd_data.rst", "api/generated/doubleml.utils.DMLDummyClassifier.rst", "api/generated/doubleml.utils.DMLDummyRegressor.rst", "api/generated/doubleml.utils.DoubleMLBLP.rst", "api/generated/doubleml.utils.DoubleMLPolicyTree.rst", "api/generated/doubleml.utils.GlobalClassifier.rst", "api/generated/doubleml.utils.GlobalRegressor.rst", "api/generated/doubleml.utils.gain_statistics.rst", "api/mixins.rst", "api/utility.rst", "examples/R_double_ml_basic_iv.ipynb", "examples/R_double_ml_basics.ipynb", "examples/R_double_ml_did.ipynb", "examples/R_double_ml_multiway_cluster.ipynb", "examples/R_double_ml_pension.ipynb", "examples/R_double_ml_pipeline.ipynb", "examples/R_double_ml_ssm.ipynb", "examples/did/py_did.ipynb", "examples/did/py_did_pretest.ipynb", "examples/did/py_panel.ipynb", "examples/did/py_panel_data_example.ipynb", "examples/did/py_panel_simple.ipynb", "examples/did/py_rep_cs.ipynb", "examples/double_ml_bonus_data.ipynb", "examples/index.rst", "examples/learners/py_tabpfn.ipynb", "examples/py_double_ml_apo.ipynb", "examples/py_double_ml_basic_iv.ipynb", "examples/py_double_ml_basics.ipynb", "examples/py_double_ml_cate.ipynb", "examples/py_double_ml_cate_plr.ipynb", "examples/py_double_ml_cvar.ipynb", "examples/py_double_ml_firststage.ipynb", "examples/py_double_ml_gate.ipynb", "examples/py_double_ml_gate_plr.ipynb", "examples/py_double_ml_gate_sensitivity.ipynb", "examples/py_double_ml_irm_vs_apo.ipynb", "examples/py_double_ml_learner.ipynb", "examples/py_double_ml_meets_flaml.ipynb", "examples/py_double_ml_multiway_cluster.ipynb", "examples/py_double_ml_pension.ipynb", "examples/py_double_ml_pension_qte.ipynb", "examples/py_double_ml_plm_irm_hetfx.ipynb", "examples/py_double_ml_policy_tree.ipynb", "examples/py_double_ml_pq.ipynb", "examples/py_double_ml_rdflex.ipynb", "examples/py_double_ml_robust_iv.ipynb", "examples/py_double_ml_sensitivity.ipynb", "examples/py_double_ml_sensitivity_booking.ipynb", "examples/py_double_ml_ssm.ipynb", "guide/algorithms.rst", "guide/basics.rst", "guide/data/base_data.rst", "guide/data/panel_data.rst", "guide/data_backend.rst", "guide/guide.rst", "guide/heterogeneity.rst", "guide/learners.rst", "guide/models.rst", "guide/models/did/did_aggregation.rst", "guide/models/did/did_binary.rst", "guide/models/did/did_cs.rst", "guide/models/did/did_implementation.rst", "guide/models/did/did_pa.rst", "guide/models/did/did_setup.rst", "guide/models/irm/apo.rst", "guide/models/irm/apos.rst", "guide/models/irm/iivm.rst", "guide/models/irm/irm.rst", "guide/models/plm/pliv.rst", "guide/models/plm/plr.rst", "guide/models/ssm/ssm.rst", "guide/resampling.rst", "guide/scores.rst", "guide/scores/did/did_cs_binary_score.rst", "guide/scores/did/did_cs_score.rst", "guide/scores/did/did_pa_binary_score.rst", "guide/scores/did/did_pa_score.rst", "guide/scores/irm/apo_score.rst", "guide/scores/irm/cvar_score.rst", "guide/scores/irm/iivm_score.rst", "guide/scores/irm/irm_score.rst", "guide/scores/irm/lpq_score.rst", "guide/scores/irm/pq_score.rst", "guide/scores/plm/pliv_score.rst", "guide/scores/plm/plr_score.rst", "guide/scores/ssm/mar_score.rst", "guide/scores/ssm/nr_score.rst", "guide/se_confint.rst", "guide/sensitivity.rst", "guide/sensitivity/benchmarking.rst", "guide/sensitivity/did/did_cs_binary_sensitivity.rst", "guide/sensitivity/did/did_cs_sensitivity.rst", "guide/sensitivity/did/did_pa_binary_sensitivity.rst", "guide/sensitivity/did/did_pa_sensitivity.rst", "guide/sensitivity/implementation.rst", "guide/sensitivity/irm/apo_sensitivity.rst", "guide/sensitivity/irm/irm_sensitivity.rst", "guide/sensitivity/plm/plr_sensitivity.rst", "guide/sensitivity/theory.rst", "index.rst", "intro/install.rst", "intro/intro.rst", "literature/literature.rst", "release/release.rst", "workflow/workflow.rst"], "indexentries": {"aggregate() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.aggregate", false]], "aggregate_over_splits() (doubleml.rdd.rdflex method)": [[40, "doubleml.rdd.RDFlex.aggregate_over_splits", false]], "bootstrap() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.bootstrap", false]], "bootstrap() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.bootstrap", false]], "bootstrap() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.bootstrap", false]], "bootstrap() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.bootstrap", false]], "bootstrap() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.bootstrap", false]], "bootstrap() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.bootstrap", false]], "capo() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.capo", false]], "cate() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.cate", false]], "cate() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.cate", false]], "causal_contrast() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.causal_contrast", false]], "confint() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.confint", false]], "confint() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.confint", false]], "confint() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.confint", false]], "confint() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.confint", false]], "confint() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.confint", false]], "confint() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.confint", false]], "confint() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.confint", false]], "confint() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.confint", false]], "confint() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.confint", false]], "confint() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.confint", false]], "confint() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.confint", false]], "confint() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.confint", false]], "confint() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.confint", false]], "confint() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.confint", false]], "confint() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.confint", false]], "confint() (doubleml.rdd.rdflex method)": [[40, "doubleml.rdd.RDFlex.confint", false]], "confint() (doubleml.utils.doublemlblp method)": [[44, "doubleml.utils.DoubleMLBLP.confint", false]], "construct_framework() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.construct_framework", false]], "construct_framework() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.construct_framework", false]], "construct_framework() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.construct_framework", false]], "construct_framework() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.construct_framework", false]], "construct_framework() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.construct_framework", false]], "construct_framework() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.construct_framework", false]], "dmldummyclassifier (class in doubleml.utils)": [[42, "doubleml.utils.DMLDummyClassifier", false]], "dmldummyregressor (class in doubleml.utils)": [[43, "doubleml.utils.DMLDummyRegressor", false]], "doublemlapo (class in doubleml.irm)": [[29, "doubleml.irm.DoubleMLAPO", false]], "doublemlapos (class in doubleml.irm)": [[30, "doubleml.irm.DoubleMLAPOS", false]], "doublemlblp (class in doubleml.utils)": [[44, "doubleml.utils.DoubleMLBLP", false]], "doublemlclusterdata (class in doubleml.data)": [[4, "doubleml.data.DoubleMLClusterData", false]], "doublemlcvar (class in doubleml.irm)": [[31, "doubleml.irm.DoubleMLCVAR", false]], "doublemldata (class in doubleml.data)": [[5, "doubleml.data.DoubleMLData", false]], "doublemldid (class in doubleml.did)": [[20, "doubleml.did.DoubleMLDID", false]], "doublemldidaggregation (class in doubleml.did)": [[21, "doubleml.did.DoubleMLDIDAggregation", false]], "doublemldidbinary (class in doubleml.did)": [[22, "doubleml.did.DoubleMLDIDBinary", false]], "doublemldidcs (class in doubleml.did)": [[23, "doubleml.did.DoubleMLDIDCS", false]], "doublemldidmulti (class in doubleml.did)": [[24, "doubleml.did.DoubleMLDIDMulti", false]], "doublemliivm (class in doubleml.irm)": [[32, "doubleml.irm.DoubleMLIIVM", false]], "doublemlirm (class in doubleml.irm)": [[33, "doubleml.irm.DoubleMLIRM", false]], "doublemllpq (class in doubleml.irm)": [[34, "doubleml.irm.DoubleMLLPQ", false]], "doublemlpaneldata (class in doubleml.data)": [[6, "doubleml.data.DoubleMLPanelData", false]], "doublemlpliv (class in doubleml.plm)": [[38, "doubleml.plm.DoubleMLPLIV", false]], "doublemlplr (class in doubleml.plm)": [[39, "doubleml.plm.DoubleMLPLR", false]], "doublemlpolicytree (class in doubleml.utils)": [[45, "doubleml.utils.DoubleMLPolicyTree", false]], "doublemlpq (class in doubleml.irm)": [[35, "doubleml.irm.DoubleMLPQ", false]], "doublemlqte (class in doubleml.irm)": [[36, "doubleml.irm.DoubleMLQTE", false]], "doublemlssm (class in doubleml.irm)": [[37, "doubleml.irm.DoubleMLSSM", false]], "draw_sample_splitting() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.draw_sample_splitting", false]], "draw_sample_splitting() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.draw_sample_splitting", false]], "evaluate_learners() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.evaluate_learners", false]], "evaluate_learners() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.evaluate_learners", false]], "evaluate_learners() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.evaluate_learners", false]], "evaluate_learners() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.evaluate_learners", false]], "fetch_401k() (in module doubleml.datasets)": [[7, "doubleml.datasets.fetch_401K", false]], "fetch_bonus() (in module doubleml.datasets)": [[8, "doubleml.datasets.fetch_bonus", false]], "fit() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.fit", false]], "fit() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.fit", false]], "fit() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.fit", false]], "fit() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.fit", false]], "fit() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.fit", false]], "fit() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.fit", false]], "fit() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.fit", false]], "fit() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.fit", false]], "fit() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.fit", false]], "fit() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.fit", false]], "fit() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.fit", false]], "fit() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.fit", false]], "fit() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.fit", false]], "fit() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.fit", false]], "fit() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.fit", false]], "fit() (doubleml.rdd.rdflex method)": [[40, "doubleml.rdd.RDFlex.fit", false]], "fit() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.fit", false]], "fit() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.fit", false]], "fit() (doubleml.utils.doublemlblp method)": [[44, "doubleml.utils.DoubleMLBLP.fit", false]], "fit() (doubleml.utils.doublemlpolicytree method)": [[45, "doubleml.utils.DoubleMLPolicyTree.fit", false]], "fit() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.fit", false]], "fit() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.fit", false]], "from_arrays() (doubleml.data.doublemlclusterdata class method)": [[4, "doubleml.data.DoubleMLClusterData.from_arrays", false]], "from_arrays() (doubleml.data.doublemldata class method)": [[5, "doubleml.data.DoubleMLData.from_arrays", false]], "from_arrays() (doubleml.data.doublemlpaneldata class method)": [[6, "doubleml.data.DoubleMLPanelData.from_arrays", false]], "gain_statistics() (in module doubleml.utils)": [[48, "doubleml.utils.gain_statistics", false]], "gapo() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.gapo", false]], "gate() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.gate", false]], "gate() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.gate", false]], "get_metadata_routing() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.get_metadata_routing", false]], "get_metadata_routing() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.get_metadata_routing", false]], "get_params() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.get_params", false]], "get_params() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.get_params", false]], "get_params() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.get_params", false]], "get_params() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.get_params", false]], "get_params() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.get_params", false]], "get_params() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.get_params", false]], "get_params() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.get_params", false]], "get_params() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.get_params", false]], "get_params() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.get_params", false]], "get_params() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.get_params", false]], "get_params() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.get_params", false]], "get_params() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.get_params", false]], "get_params() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.get_params", false]], "get_params() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.get_params", false]], "get_params() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.get_params", false]], "globalclassifier (class in doubleml.utils)": [[46, "doubleml.utils.GlobalClassifier", false]], "globalregressor (class in doubleml.utils)": [[47, "doubleml.utils.GlobalRegressor", false]], "linearscoremixin (class in doubleml.double_ml_score_mixins)": [[27, "doubleml.double_ml_score_mixins.LinearScoreMixin", false]], "make_confounded_irm_data() (in module doubleml.datasets)": [[9, "doubleml.datasets.make_confounded_irm_data", false]], "make_confounded_plr_data() (in module doubleml.datasets)": [[10, "doubleml.datasets.make_confounded_plr_data", false]], "make_did_cs2021() (in module doubleml.did.datasets)": [[25, "doubleml.did.datasets.make_did_CS2021", false]], "make_did_sz2020() (in module doubleml.did.datasets)": [[26, "doubleml.did.datasets.make_did_SZ2020", false]], "make_heterogeneous_data() (in module doubleml.datasets)": [[11, "doubleml.datasets.make_heterogeneous_data", false]], "make_iivm_data() (in module doubleml.datasets)": [[12, "doubleml.datasets.make_iivm_data", false]], "make_irm_data() (in module doubleml.datasets)": [[13, "doubleml.datasets.make_irm_data", false]], "make_irm_data_discrete_treatments() (in module doubleml.datasets)": [[14, "doubleml.datasets.make_irm_data_discrete_treatments", false]], "make_pliv_chs2015() (in module doubleml.datasets)": [[15, "doubleml.datasets.make_pliv_CHS2015", false]], "make_pliv_multiway_cluster_ckms2021() (in module doubleml.datasets)": [[16, "doubleml.datasets.make_pliv_multiway_cluster_CKMS2021", false]], "make_plr_ccddhnr2018() (in module doubleml.datasets)": [[17, "doubleml.datasets.make_plr_CCDDHNR2018", false]], "make_plr_turrell2018() (in module doubleml.datasets)": [[18, "doubleml.datasets.make_plr_turrell2018", false]], "make_simple_rdd_data() (in module doubleml.rdd.datasets)": [[41, "doubleml.rdd.datasets.make_simple_rdd_data", false]], "make_ssm_data() (in module doubleml.datasets)": [[19, "doubleml.datasets.make_ssm_data", false]], "nonlinearscoremixin (class in doubleml.double_ml_score_mixins)": [[28, "doubleml.double_ml_score_mixins.NonLinearScoreMixin", false]], "p_adjust() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.p_adjust", false]], "p_adjust() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.p_adjust", false]], "p_adjust() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.p_adjust", false]], "p_adjust() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.p_adjust", false]], "p_adjust() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.p_adjust", false]], "p_adjust() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.p_adjust", false]], "plot_effects() (doubleml.did.doublemldidaggregation method)": [[21, "doubleml.did.DoubleMLDIDAggregation.plot_effects", false]], "plot_effects() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.plot_effects", false]], "plot_tree() (doubleml.utils.doublemlpolicytree method)": [[45, "doubleml.utils.DoubleMLPolicyTree.plot_tree", false]], "policy_tree() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.policy_tree", false]], "predict() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.predict", false]], "predict() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.predict", false]], "predict() (doubleml.utils.doublemlpolicytree method)": [[45, "doubleml.utils.DoubleMLPolicyTree.predict", false]], "predict() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.predict", false]], "predict() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.predict", false]], "predict_proba() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.predict_proba", false]], "predict_proba() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.predict_proba", false]], "rdflex (class in doubleml.rdd)": [[40, "doubleml.rdd.RDFlex", false]], "robust_confset() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.robust_confset", false]], "score() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.score", false]], "score() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.score", false]], "score() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.score", false]], "score() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.score", false]], "sensitivity_analysis() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_analysis", false]], "sensitivity_analysis() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_analysis", false]], "sensitivity_benchmark() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_benchmark", false]], "sensitivity_benchmark() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_benchmark", false]], "sensitivity_plot() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.did.doublemldidmulti method)": [[24, "doubleml.did.DoubleMLDIDMulti.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.sensitivity_plot", false]], "sensitivity_plot() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.sensitivity_plot", false]], "sensitivity_plot() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.sensitivity_plot", false]], "set_fit_request() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.set_fit_request", false]], "set_fit_request() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.set_fit_request", false]], "set_ml_nuisance_params() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.set_ml_nuisance_params", false]], "set_ml_nuisance_params() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.set_ml_nuisance_params", false]], "set_params() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.set_params", false]], "set_params() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.set_params", false]], "set_params() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.set_params", false]], "set_params() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.set_params", false]], "set_sample_splitting() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.set_sample_splitting", false]], "set_sample_splitting() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlapos method)": [[30, "doubleml.irm.DoubleMLAPOS.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlqte method)": [[36, "doubleml.irm.DoubleMLQTE.set_sample_splitting", false]], "set_sample_splitting() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.set_sample_splitting", false]], "set_sample_splitting() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.set_sample_splitting", false]], "set_score_request() (doubleml.utils.dmldummyclassifier method)": [[42, "doubleml.utils.DMLDummyClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.dmldummyregressor method)": [[43, "doubleml.utils.DMLDummyRegressor.set_score_request", false]], "set_score_request() (doubleml.utils.globalclassifier method)": [[46, "doubleml.utils.GlobalClassifier.set_score_request", false]], "set_score_request() (doubleml.utils.globalregressor method)": [[47, "doubleml.utils.GlobalRegressor.set_score_request", false]], "set_x_d() (doubleml.data.doublemlclusterdata method)": [[4, "doubleml.data.DoubleMLClusterData.set_x_d", false]], "set_x_d() (doubleml.data.doublemldata method)": [[5, "doubleml.data.DoubleMLData.set_x_d", false]], "set_x_d() (doubleml.data.doublemlpaneldata method)": [[6, "doubleml.data.DoubleMLPanelData.set_x_d", false]], "tune() (doubleml.did.doublemldid method)": [[20, "doubleml.did.DoubleMLDID.tune", false]], "tune() (doubleml.did.doublemldidbinary method)": [[22, "doubleml.did.DoubleMLDIDBinary.tune", false]], "tune() (doubleml.did.doublemldidcs method)": [[23, "doubleml.did.DoubleMLDIDCS.tune", false]], "tune() (doubleml.irm.doublemlapo method)": [[29, "doubleml.irm.DoubleMLAPO.tune", false]], "tune() (doubleml.irm.doublemlcvar method)": [[31, "doubleml.irm.DoubleMLCVAR.tune", false]], "tune() (doubleml.irm.doublemliivm method)": [[32, "doubleml.irm.DoubleMLIIVM.tune", false]], "tune() (doubleml.irm.doublemlirm method)": [[33, "doubleml.irm.DoubleMLIRM.tune", false]], "tune() (doubleml.irm.doublemllpq method)": [[34, "doubleml.irm.DoubleMLLPQ.tune", false]], "tune() (doubleml.irm.doublemlpq method)": [[35, "doubleml.irm.DoubleMLPQ.tune", false]], "tune() (doubleml.irm.doublemlssm method)": [[37, "doubleml.irm.DoubleMLSSM.tune", false]], "tune() (doubleml.plm.doublemlpliv method)": [[38, "doubleml.plm.DoubleMLPLIV.tune", false]], "tune() (doubleml.plm.doublemlplr method)": [[39, "doubleml.plm.DoubleMLPLR.tune", false]]}, "objects": {"doubleml.data": [[4, 0, 1, "", "DoubleMLClusterData"], [5, 0, 1, "", "DoubleMLData"], [6, 0, 1, "", "DoubleMLPanelData"]], "doubleml.data.DoubleMLClusterData": [[4, 1, 1, "", "from_arrays"], [4, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLData": [[5, 1, 1, "", "from_arrays"], [5, 1, 1, "", "set_x_d"]], "doubleml.data.DoubleMLPanelData": [[6, 1, 1, "", "from_arrays"], [6, 1, 1, "", "set_x_d"]], "doubleml.datasets": [[7, 2, 1, "", "fetch_401K"], [8, 2, 1, "", "fetch_bonus"], [9, 2, 1, "", "make_confounded_irm_data"], [10, 2, 1, "", "make_confounded_plr_data"], [11, 2, 1, "", "make_heterogeneous_data"], [12, 2, 1, "", "make_iivm_data"], [13, 2, 1, "", "make_irm_data"], [14, 2, 1, "", "make_irm_data_discrete_treatments"], [15, 2, 1, "", "make_pliv_CHS2015"], [16, 2, 1, "", "make_pliv_multiway_cluster_CKMS2021"], [17, 2, 1, "", "make_plr_CCDDHNR2018"], [18, 2, 1, "", "make_plr_turrell2018"], [19, 2, 1, "", "make_ssm_data"]], "doubleml.did": [[20, 0, 1, "", "DoubleMLDID"], [21, 0, 1, "", "DoubleMLDIDAggregation"], [22, 0, 1, "", "DoubleMLDIDBinary"], [23, 0, 1, "", "DoubleMLDIDCS"], [24, 0, 1, "", "DoubleMLDIDMulti"]], "doubleml.did.DoubleMLDID": [[20, 1, 1, "", "bootstrap"], [20, 1, 1, "", "confint"], [20, 1, 1, "", "construct_framework"], [20, 1, 1, "", "draw_sample_splitting"], [20, 1, 1, "", "evaluate_learners"], [20, 1, 1, "", "fit"], [20, 1, 1, "", "get_params"], [20, 1, 1, "", "p_adjust"], [20, 1, 1, "", "sensitivity_analysis"], [20, 1, 1, "", "sensitivity_benchmark"], [20, 1, 1, "", "sensitivity_plot"], [20, 1, 1, "", "set_ml_nuisance_params"], [20, 1, 1, "", "set_sample_splitting"], [20, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDAggregation": [[21, 1, 1, "", "plot_effects"]], "doubleml.did.DoubleMLDIDBinary": [[22, 1, 1, "", "bootstrap"], [22, 1, 1, "", "confint"], [22, 1, 1, "", "construct_framework"], [22, 1, 1, "", "draw_sample_splitting"], [22, 1, 1, "", "evaluate_learners"], [22, 1, 1, "", "fit"], [22, 1, 1, "", "get_params"], [22, 1, 1, "", "p_adjust"], [22, 1, 1, "", "sensitivity_analysis"], [22, 1, 1, "", "sensitivity_benchmark"], [22, 1, 1, "", "sensitivity_plot"], [22, 1, 1, "", "set_ml_nuisance_params"], [22, 1, 1, "", "set_sample_splitting"], [22, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDCS": [[23, 1, 1, "", "bootstrap"], [23, 1, 1, "", "confint"], [23, 1, 1, "", "construct_framework"], [23, 1, 1, "", "draw_sample_splitting"], [23, 1, 1, "", "evaluate_learners"], [23, 1, 1, "", "fit"], [23, 1, 1, "", "get_params"], [23, 1, 1, "", "p_adjust"], [23, 1, 1, "", "sensitivity_analysis"], [23, 1, 1, "", "sensitivity_benchmark"], [23, 1, 1, "", "sensitivity_plot"], [23, 1, 1, "", "set_ml_nuisance_params"], [23, 1, 1, "", "set_sample_splitting"], [23, 1, 1, "", "tune"]], "doubleml.did.DoubleMLDIDMulti": [[24, 1, 1, "", "aggregate"], [24, 1, 1, "", "bootstrap"], [24, 1, 1, "", "confint"], [24, 1, 1, "", "fit"], [24, 1, 1, "", "p_adjust"], [24, 1, 1, "", "plot_effects"], [24, 1, 1, "", "sensitivity_analysis"], [24, 1, 1, "", "sensitivity_benchmark"], [24, 1, 1, "", "sensitivity_plot"]], "doubleml.did.datasets": [[25, 2, 1, "", "make_did_CS2021"], [26, 2, 1, "", "make_did_SZ2020"]], "doubleml.double_ml_score_mixins": [[27, 0, 1, "", "LinearScoreMixin"], [28, 0, 1, "", "NonLinearScoreMixin"]], "doubleml.irm": [[29, 0, 1, "", "DoubleMLAPO"], [30, 0, 1, "", "DoubleMLAPOS"], [31, 0, 1, "", "DoubleMLCVAR"], [32, 0, 1, "", "DoubleMLIIVM"], [33, 0, 1, "", "DoubleMLIRM"], [34, 0, 1, "", "DoubleMLLPQ"], [35, 0, 1, "", "DoubleMLPQ"], [36, 0, 1, "", "DoubleMLQTE"], [37, 0, 1, "", "DoubleMLSSM"]], "doubleml.irm.DoubleMLAPO": [[29, 1, 1, "", "bootstrap"], [29, 1, 1, "", "capo"], [29, 1, 1, "", "confint"], [29, 1, 1, "", "construct_framework"], [29, 1, 1, "", "draw_sample_splitting"], [29, 1, 1, "", "evaluate_learners"], [29, 1, 1, "", "fit"], [29, 1, 1, "", "gapo"], [29, 1, 1, "", "get_params"], [29, 1, 1, "", "p_adjust"], [29, 1, 1, "", "sensitivity_analysis"], [29, 1, 1, "", "sensitivity_benchmark"], [29, 1, 1, "", "sensitivity_plot"], [29, 1, 1, "", "set_ml_nuisance_params"], [29, 1, 1, "", "set_sample_splitting"], [29, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLAPOS": [[30, 1, 1, "", "bootstrap"], [30, 1, 1, "", "causal_contrast"], [30, 1, 1, "", "confint"], [30, 1, 1, "", "draw_sample_splitting"], [30, 1, 1, "", "fit"], [30, 1, 1, "", "sensitivity_analysis"], [30, 1, 1, "", "sensitivity_benchmark"], [30, 1, 1, "", "sensitivity_plot"], [30, 1, 1, "", "set_sample_splitting"]], "doubleml.irm.DoubleMLCVAR": [[31, 1, 1, "", "bootstrap"], [31, 1, 1, "", "confint"], [31, 1, 1, "", "construct_framework"], [31, 1, 1, "", "draw_sample_splitting"], [31, 1, 1, "", "evaluate_learners"], [31, 1, 1, "", "fit"], [31, 1, 1, "", "get_params"], [31, 1, 1, "", "p_adjust"], [31, 1, 1, "", "sensitivity_analysis"], [31, 1, 1, "", "sensitivity_benchmark"], [31, 1, 1, "", "sensitivity_plot"], [31, 1, 1, "", "set_ml_nuisance_params"], [31, 1, 1, "", "set_sample_splitting"], [31, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLIIVM": [[32, 1, 1, "", "bootstrap"], [32, 1, 1, "", "confint"], [32, 1, 1, "", "construct_framework"], [32, 1, 1, "", "draw_sample_splitting"], [32, 1, 1, "", "evaluate_learners"], [32, 1, 1, "", "fit"], [32, 1, 1, "", "get_params"], [32, 1, 1, "", "p_adjust"], [32, 1, 1, "", "robust_confset"], [32, 1, 1, "", "sensitivity_analysis"], [32, 1, 1, "", "sensitivity_benchmark"], [32, 1, 1, "", "sensitivity_plot"], [32, 1, 1, "", "set_ml_nuisance_params"], [32, 1, 1, "", "set_sample_splitting"], [32, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLIRM": [[33, 1, 1, "", "bootstrap"], [33, 1, 1, "", "cate"], [33, 1, 1, "", "confint"], [33, 1, 1, "", "construct_framework"], [33, 1, 1, "", "draw_sample_splitting"], [33, 1, 1, "", "evaluate_learners"], [33, 1, 1, "", "fit"], [33, 1, 1, "", "gate"], [33, 1, 1, "", "get_params"], [33, 1, 1, "", "p_adjust"], [33, 1, 1, "", "policy_tree"], [33, 1, 1, "", "sensitivity_analysis"], [33, 1, 1, "", "sensitivity_benchmark"], [33, 1, 1, "", "sensitivity_plot"], [33, 1, 1, "", "set_ml_nuisance_params"], [33, 1, 1, "", "set_sample_splitting"], [33, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLLPQ": [[34, 1, 1, "", "bootstrap"], [34, 1, 1, "", "confint"], [34, 1, 1, "", "construct_framework"], [34, 1, 1, "", "draw_sample_splitting"], [34, 1, 1, "", "evaluate_learners"], [34, 1, 1, "", "fit"], [34, 1, 1, "", "get_params"], [34, 1, 1, "", "p_adjust"], [34, 1, 1, "", "sensitivity_analysis"], [34, 1, 1, "", "sensitivity_benchmark"], [34, 1, 1, "", "sensitivity_plot"], [34, 1, 1, "", "set_ml_nuisance_params"], [34, 1, 1, "", "set_sample_splitting"], [34, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLPQ": [[35, 1, 1, "", "bootstrap"], [35, 1, 1, "", "confint"], [35, 1, 1, "", "construct_framework"], [35, 1, 1, "", "draw_sample_splitting"], [35, 1, 1, "", "evaluate_learners"], [35, 1, 1, "", "fit"], [35, 1, 1, "", "get_params"], [35, 1, 1, "", "p_adjust"], [35, 1, 1, "", "sensitivity_analysis"], [35, 1, 1, "", "sensitivity_benchmark"], [35, 1, 1, "", "sensitivity_plot"], [35, 1, 1, "", "set_ml_nuisance_params"], [35, 1, 1, "", "set_sample_splitting"], [35, 1, 1, "", "tune"]], "doubleml.irm.DoubleMLQTE": [[36, 1, 1, "", "bootstrap"], [36, 1, 1, "", "confint"], [36, 1, 1, "", "draw_sample_splitting"], [36, 1, 1, "", "fit"], [36, 1, 1, "", "p_adjust"], [36, 1, 1, "", "set_sample_splitting"]], "doubleml.irm.DoubleMLSSM": [[37, 1, 1, "", "bootstrap"], [37, 1, 1, "", "confint"], [37, 1, 1, "", "construct_framework"], [37, 1, 1, "", "draw_sample_splitting"], [37, 1, 1, "", "evaluate_learners"], [37, 1, 1, "", "fit"], [37, 1, 1, "", "get_params"], [37, 1, 1, "", "p_adjust"], [37, 1, 1, "", "sensitivity_analysis"], [37, 1, 1, "", "sensitivity_benchmark"], [37, 1, 1, "", "sensitivity_plot"], [37, 1, 1, "", "set_ml_nuisance_params"], [37, 1, 1, "", "set_sample_splitting"], [37, 1, 1, "", "tune"]], "doubleml.plm": [[38, 0, 1, "", "DoubleMLPLIV"], [39, 0, 1, "", "DoubleMLPLR"]], "doubleml.plm.DoubleMLPLIV": [[38, 1, 1, "", "bootstrap"], [38, 1, 1, "", "confint"], [38, 1, 1, "", "construct_framework"], [38, 1, 1, "", "draw_sample_splitting"], [38, 1, 1, "", "evaluate_learners"], [38, 1, 1, "", "fit"], [38, 1, 1, "", "get_params"], [38, 1, 1, "", "p_adjust"], [38, 1, 1, "", "sensitivity_analysis"], [38, 1, 1, "", "sensitivity_benchmark"], [38, 1, 1, "", "sensitivity_plot"], [38, 1, 1, "", "set_ml_nuisance_params"], [38, 1, 1, "", "set_sample_splitting"], [38, 1, 1, "", "tune"]], "doubleml.plm.DoubleMLPLR": [[39, 1, 1, "", "bootstrap"], [39, 1, 1, "", "cate"], [39, 1, 1, "", "confint"], [39, 1, 1, "", "construct_framework"], [39, 1, 1, "", "draw_sample_splitting"], [39, 1, 1, "", "evaluate_learners"], [39, 1, 1, "", "fit"], [39, 1, 1, "", "gate"], [39, 1, 1, "", "get_params"], [39, 1, 1, "", "p_adjust"], [39, 1, 1, "", "sensitivity_analysis"], [39, 1, 1, "", "sensitivity_benchmark"], [39, 1, 1, "", "sensitivity_plot"], [39, 1, 1, "", "set_ml_nuisance_params"], [39, 1, 1, "", "set_sample_splitting"], [39, 1, 1, "", "tune"]], "doubleml.rdd": [[40, 0, 1, "", "RDFlex"]], "doubleml.rdd.RDFlex": [[40, 1, 1, "", "aggregate_over_splits"], [40, 1, 1, "", "confint"], [40, 1, 1, "", "fit"]], "doubleml.rdd.datasets": [[41, 2, 1, "", "make_simple_rdd_data"]], "doubleml.utils": [[42, 0, 1, "", "DMLDummyClassifier"], [43, 0, 1, "", "DMLDummyRegressor"], [44, 0, 1, "", "DoubleMLBLP"], [45, 0, 1, "", "DoubleMLPolicyTree"], [46, 0, 1, "", "GlobalClassifier"], [47, 0, 1, "", "GlobalRegressor"], [48, 2, 1, "", "gain_statistics"]], "doubleml.utils.DMLDummyClassifier": [[42, 1, 1, "", "fit"], [42, 1, 1, "", "get_metadata_routing"], [42, 1, 1, "", "get_params"], [42, 1, 1, "", "predict"], [42, 1, 1, "", "predict_proba"], [42, 1, 1, "", "score"], [42, 1, 1, "", "set_params"], [42, 1, 1, "", "set_score_request"]], "doubleml.utils.DMLDummyRegressor": [[43, 1, 1, "", "fit"], [43, 1, 1, "", "get_metadata_routing"], [43, 1, 1, "", "get_params"], [43, 1, 1, "", "predict"], [43, 1, 1, "", "score"], [43, 1, 1, "", "set_params"], [43, 1, 1, "", "set_score_request"]], "doubleml.utils.DoubleMLBLP": [[44, 1, 1, "", "confint"], [44, 1, 1, "", "fit"]], "doubleml.utils.DoubleMLPolicyTree": [[45, 1, 1, "", "fit"], [45, 1, 1, "", "plot_tree"], [45, 1, 1, "", "predict"]], "doubleml.utils.GlobalClassifier": [[46, 1, 1, "", "fit"], [46, 1, 1, "", "get_metadata_routing"], [46, 1, 1, "", "get_params"], [46, 1, 1, "", "predict"], [46, 1, 1, "", "predict_proba"], [46, 1, 1, "", "score"], [46, 1, 1, "", "set_fit_request"], [46, 1, 1, "", "set_params"], [46, 1, 1, "", "set_score_request"]], "doubleml.utils.GlobalRegressor": [[47, 1, 1, "", "fit"], [47, 1, 1, "", "get_metadata_routing"], [47, 1, 1, "", "get_params"], [47, 1, 1, "", "predict"], [47, 1, 1, "", "score"], [47, 1, 1, "", "set_fit_request"], [47, 1, 1, "", "set_params"], [47, 1, 1, "", "set_score_request"]]}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function"}, "terms": {"": [4, 5, 6, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 46, 47, 51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 73, 74, 75, 76, 78, 80, 81, 82, 86, 88, 89, 90, 91, 93, 94, 95, 98, 99, 101, 102, 104, 105, 112, 114, 127, 128, 129, 130, 131, 141, 143, 144, 145, 146], "0": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 142, 143, 145], "00": [60, 75, 77, 81, 82, 113], "000": [87, 129, 146], "00000": 77, "000000": [60, 62, 64, 67, 77, 81, 82, 93, 95, 97, 143], "0000000": 129, "0000000000000010000100": [56, 93, 95, 143], "000000e": [60, 75, 77, 81, 82], "00000591": 85, "000006": [67, 85], "000017": 85, "000023": 63, "000025": 80, "000034": 81, "000039": 80, "000047": 61, "000062": 63, "000064": 68, "000067": 80, "000076": 99, "000084": 61, "000091": 80, "0001": [64, 81], "000117": 61, "000160": 63, "000185": 61, "0002": 61, "000200": 61, "000219": [35, 97], "000242": [36, 97], "000341": 80, "000357": 60, "000413": 63, "000442": 80, "000459": 113, "000462": 63, "00047580260495": 51, "000488": 80, "000489": 60, "000494": 76, "0005": 64, "000511": 60, "000522": 80, "000531": 63, "000546": 60, "000590": 60, "000597": 63, "000599": 60, "0005a80b528f": 56, "000605": 63, "000612": 63, "000614": 61, "000629": 60, "000652": [60, 63], "000666": 63, "000670": 80, "000743": 88, "000915799": 129, "0009157990": 129, "000943": [70, 71], "000957": 61, "000971": 61, "001": [25, 51, 53, 54, 55, 56, 57, 69, 98, 99, 113, 114, 129, 143, 146], "001049": [99, 104], "001051": 80, "001071": 61, "0011": 61, "001234": 82, "001302": 61, "001305": 61, "001312": 60, "00133": 56, "001335": 62, "00138944": [91, 114], "001393": 61, "0014": 61, "001403": 86, "001448": 63, "001470": 61, "001471": 77, "001494": [97, 98, 99], "0016": [55, 81], "001603": [99, 104], "001653": 61, "001698": 77, "001714": 97, "0018": [55, 81], "0019": 64, "001907": 77, "002014": 61, "002169338": 129, "0021693380": 129, "0021693381": 129, "002252": 63, "002277": 70, "002290": 59, "0023": 53, "002388": 79, "002421": 60, "002428": 61, "002436": 76, "002443": 61, "0026": 64, "002601": [99, 100], "002779": 88, "0028": [53, 55, 61, 81], "002821": 89, "00282133350419121": 89, "00290774": [99, 102], "002983": 80, "003": [9, 10, 26, 87], "003045": 77, "003074": 99, "003134": 85, "003187": 70, "003220": 67, "003237": 61, "003328": 85, "0034": [61, 73], "003404": 67, "003415": 67, "003427": 80, "003437": 61, "003512": 61, "003571": 61, "003607": 71, "003654": 63, "003672": 61, "003712": 61, "003779": 76, "003836": 85, "003924": 76, "003944": 70, "003975": 70, "004072": 63, "00409412": [91, 114], "0041": 61, "0042": [55, 81], "004253": 67, "004392": 76, "004512": 61, "004526": 67, "004542": 77, "004552": 60, "0046": 61, "00467589": 63, "004688": 32, "0047": [55, 81], "004846": 89, "00518448": [99, 104], "005319": 61, "005339": [70, 71], "005570": 61, "005697": 61, "005857": 80, "005e": 99, "006046435": 113, "006055": 67, "006267": 71, "006425": 82, "0065": 61, "006578": 63, "0066": 61, "006620": 61, "0068101213851626": 79, "006922": 64, "006928": 61, "006958": [70, 71], "006960578": 113, "007194": 63, "007210e": 82, "00728": 143, "0073": 64, "007332": 72, "007332393760465": 72, "007421": 97, "007482": 61, "00778625": 113, "007810": 60, "0078540263583833": 79, "008": 89, "008023": 82, "008070657": 113, "008216": 61, "008223": [70, 71], "008266e": 82, "008373": 60, "008487": 64, "0084871742256079": 79, "008642": 97, "008754": 63, "008883698": 114, "00888458890362062": 91, "008884589": 91, "008dbd": 83, "008e80": 83, "009": [83, 89], "009098": 63, "009122": 85, "009171": [99, 102], "009255": 70, "009329847": 114, "009428": 72, "00944171905420782": 89, "00950122695463054": 91, "009501226954630540": 91, "009501227": 91, "009645422": 54, "009656": 85, "00972": 64, "009727": [99, 100], "009790": 82, "009896": 83, "009986": [62, 85], "01": [20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 45, 51, 54, 55, 56, 57, 60, 63, 70, 71, 77, 81, 82, 83, 84, 85, 86, 98, 99, 102, 104, 113, 114, 129, 143, 146], "0100": 61, "010213": 88, "010269": 80, "010325": 83, "010450": 54, "0105": 61, "010536": 60, "010707": 60, "010862": 61, "01091843": [99, 102], "010940": 80, "011": 83, "011131": 85, "0112": 53, "011204": 77, "01128": 64, "011323": [99, 100], "011598": 85, "0118095": 54, "011823": 88, "011832": 61, "011852": 61, "011882": 83, "011988e": 85, "012": 83, "012009": 63, "012140": 63, "01219": 56, "012369": 60, "012583": 63, "01274": 89, "012780": 82, "012831": 89, "012879e": 113, "013": 83, "013034": 89, "013128": 70, "013151": 83, "013295": 61, "013313": 79, "013450": 77, "01351638": 54, "013590": 60, "013593": 88, "013617": 82, "013677": 86, "013712": 70, "01398951": 54, "013990": 129, "014": 86, "01403089": 54, "014069": 61, "014080": [70, 71], "014246": 60, "014253": 61, "014301": 60, "014432": 59, "014525": 66, "014637": 80, "014681": 88, "014827": 60, "014873e": 70, "014895": 60, "015": 56, "0150": 61, "015038": 72, "015041": 61, "015063": 61, "01510401": 113, "015152": 60, "015204": 60, "015552": 70, "015565": 85, "0156853566737638": 79, "015698": 85, "015705": 60, "015714": 61, "01574297": 85, "015743": 85, "015748": 60, "015831": 70, "016": 83, "016011": [61, 71], "016154": 80, "01618806": 113, "016200": [70, 71], "016208": 61, "016315": 74, "016392": 61, "0164": 61, "01643": 144, "016475": 61, "016653": 61, "0167": 61, "016752": 61, "016811": 61, "017": 56, "017140": 70, "017167": 63, "017393e": 129, "017660": 77, "01772": 131, "017726": 61, "017777e": 71, "0178": 61, "017800092": 129, "0178000920": 129, "017884": 61, "0179": 61, "018": 56, "018023": 84, "018025": 61, "018092": 97, "018148": 85, "018203": 61, "018296": 63, "018343": 61, "0184989": [99, 102], "018508": 70, "01877828": 63, "01903": [56, 98, 141, 143], "019135": 60, "01916030e": 113, "01925597": 54, "019439633": 129, "0194396330": 129, "0194396331": 129, "019458": 61, "019514": 61, "019516": 62, "019596": 72, "019660": [36, 97], "019678": 61, "0197": 61, "019793": 61, "019814": 61, "01987982": 113, "01990373": 90, "019954": 61, "019974": 82, "02": [60, 63, 70, 71, 81, 82, 85, 97, 99, 102, 104, 113], "020048": 61, "020098": 63, "020123": 66, "02016117": 143, "020166": 85, "020271": 80, "020272": 77, "0203": 61, "020356": 61, "020360838": 129, "0203608380": 129, "0203608381": 129, "02052929": [91, 114], "02079162e": 113, "020819": 97, "0208230": 113, "02092": 143, "020964": 61, "021038": 60, "021050": 61, "021269": [74, 75], "021362": 62, "02150029": 60, "02163217": 54, "021690": 75, "021823": 79, "021866": 84, "021923": 61, "021926": 72, "022181": 70, "022258": 77, "022295e": 70, "0223": 61, "022318": 61, "02247976": 54, "02260304": [99, 104], "022768": 64, "022783": 88, "022915": 80, "022969": 82, "023020e": [81, 82], "023052": 71, "023256": 85, "023283": 61, "0233": 61, "0234": 61, "023435": 61, "023537": 79, "023563": 129, "023661": 63, "02377": 83, "023925": 61, "023955": 82, "024266": 77, "024346": 70, "024355": 59, "024364": 130, "024401": [74, 75], "024422": 61, "024604": 80, "02467": 87, "024700": 63, "024782": 85, "024926": 59, "025": [70, 71, 74, 75, 77, 83], "025077": [71, 129], "02528067": 78, "0253": 56, "025300e": 71, "025443": 64, "025444": 61, "025496": 70, "025652": 60, "0257": 53, "025813114": 129, "0258131140": 129, "025829": 60, "02584": 56, "025841": 77, "025964": 77, "0261": 61, "026181": 61, "02625": 63, "026287": 61, "026292": 66, "026316": 61, "026536": 63, "026559": 61, "026669": 82, "026673": 61, "026723": 72, "026822": 79, "026966": 77, "02697949": 63, "027136": 61, "027509": 61, "02791": 64, "028021": 60, "0281": 56, "028302": 63, "028433": 61, "028447": 63, "028469": 61, "028520": [70, 71], "028630": [99, 100], "028731": 97, "028806": 61, "028952": 63, "02897287": 58, "02900983": 85, "029010": 85, "029022": 71, "029057": 61, "029209": 146, "029324": 61, "029364": [130, 136], "0294": 61, "029831": 85, "029910e": [81, 82], "02e": 55, "03": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 57, 60, 63, 67, 70, 71, 72, 76, 77, 81, 82, 85, 86, 88, 89, 99, 100, 102, 104, 113, 130, 136, 146], "030008": 60, "030059": 97, "0301": 56, "030123": [99, 102], "03018": 34, "0303": 61, "030346": 143, "03045": 57, "030550": 61, "0306": 61, "030685": 60, "0307": [56, 66], "030773": 62, "0308": 61, "0309": 61, "030934": 85, "030962": 85, "0310": 61, "031007": 79, "03113": 90, "031134": 98, "031150": 61, "031156": 71, "031172": 61, "0312": 61, "031269": 64, "031340": 61, "031639": 85, "031820": 71, "03191": 144, "032": 51, "032165": 61, "03220": 145, "0323": 53, "032390": 60, "03244552": 98, "0325": 143, "03258": 79, "032580": 79, "032675": [99, 102], "032738": 79, "032739": 61, "032819": 61, "032941": 70, "032953": 88, "033190": 61, "033265": 79, "033273": 83, "033278": 61, "0333": 61, "033562": 60, "033756": 72, "033858": 61, "033946": [74, 75], "034065": 71, "03411": 143, "034226": 82, "034298": 62, "03438": 57, "034459": 61, "0345": 61, "034690": 72, "034810": 61, "034812763": 129, "0348127630": 129, "0348127631": 129, "034846": 81, "03489": [16, 54, 80], "0349": 61, "035070": 61, "035119185": 129, "0351191850": 129, "0351191851": 129, "035139": 62, "0352": 61, "035264": 71, "035311": 61, "03534806": [99, 102], "03536": 143, "03538": 56, "03539": 56, "035391": 64, "0354": 56, "035411": 143, "035441": 71, "03545": 56, "035539": 61, "035545": 64, "035572": 64, "035689": [99, 104], "035730": 85, "03574": 64, "035762": 85, "035785": 71, "0358": 61, "03589055": 113, "0359": 56, "036129015": 129, "0361290150": 129, "0361290151": 129, "036143": 85, "036147": 85, "036240": 67, "036729": 80, "0368": 53, "036890": 61, "0369": 61, "036945": 82, "03698487": 85, "036985": 85, "037008": [74, 75], "037114": 77, "037148": 63, "037181": 61, "0373": 66, "0374": 56, "037504": 71, "037509": 90, "037529": 77, "037577": [99, 100], "037698": 61, "037747": [70, 71], "03794": 61, "038": 83, "038103": 77, "038265": 63, "038314": 61, "038605": 60, "038629": 83, "038842": 62, "038845": 70, "038978": 61, "039036": 70, "039141": 67, "03917696": [114, 129], "03920960e": 113, "039310e": 72, "0394": 61, "0395": 61, "039661": 77, "039895": 77, "039991e": 70, "04": [10, 40, 55, 60, 63, 67, 70, 71, 81, 82, 85, 86, 88, 99, 100, 102, 104, 113, 146], "040010": 77, "040079": 71, "040112": 129, "040139": [70, 71], "040407": 62, "040487": 61, "040533": [114, 129], "04053339": 129, "040562": 70, "040629": 39, "040688": 70, "040784": 67, "0408": 70, "040912": 71, "040919": 71, "041": 83, "041092": 61, "041147": 72, "041198": 61, "0412": 61, "041284": 72, "0413": 61, "041326": 61, "041331": 60, "041387": 72, "041444": 83, "041459": 82, "041466": 60, "041491e": 72, "04165": 99, "0418": 53, "041831": 72, "041925": 70, "042034": 89, "042170": 63, "0422": 61, "042240": 61, "042249": 71, "042265": 72, "0423": 61, "042436": 61, "0425": 98, "042576": 60, "0428": 90, "042804": 77, "042822e": 82, "042844e": 85, "042978": 61, "043082": [99, 104], "043108": 82, "043145": 61, "0433": 53, "043432": 61, "043457": 61, "0434e374": 56, "043769": 66, "043836": 61, "04387": 98, "043998": 71, "044": [51, 83], "044113": 72, "04415": 56, "044176": 77, "044239": 77, "04424": 56, "044308": 63, "04444978": 129, "044449780": 129, "0445": 98, "04465": 54, "044678": 61, "044704": 71, "04486": 143, "04487585": [130, 136], "04491": 99, "044929": 77, "04497975": [130, 136], "04501612": 129, "04502": [98, 114, 129], "045047": 61, "045144": 80, "045172": 77, "045313": 70, "045379": 143, "045387": 62, "04552": 80, "045553": 72, "045613": 61, "045624": 59, "04563": 98, "045638": 70, "045754": 85, "045782": 62, "04586": 98, "045895": 63, "045932": 85, "045984": 77, "045993": 98, "046204": 61, "04625": 98, "046451": 77, "046507": [99, 104], "046525": 97, "046527": 72, "04653976": 85, "046540": 85, "046587": 71, "046591": 61, "0466028": 54, "046682": 62, "046728": 88, "04682310e": 113, "046922": 98, "047194": 32, "047239": 77, "047288": 97, "047652e": 71, "047682": 60, "0477": 66, "047724": 70, "047873": 77, "047954": 80, "047982": 63, "048090": 66, "048220": 79, "048308": 75, "048476": 77, "048699": 90, "048723": 98, "048841": 63, "048853": 71, "048917": 61, "049010": 61, "049264": 67, "049573": [99, 104], "04973": 71, "05": [40, 51, 53, 54, 55, 56, 57, 60, 63, 70, 71, 72, 73, 78, 80, 81, 82, 83, 85, 86, 89, 98, 99, 100, 102, 104, 113, 114, 129, 143, 146], "050": 83, "050021": 63, "05039": 88, "050400": 62, "050494e": 71, "050538": 71, "050714": 60, "050975": 61, "051": 56, "05122628": 113, "051468": 62, "051549": 63, "051651": 86, "051867e": 72, "051950": 60, "052": 86, "052000e": 82, "052016": 63, "052023": 77, "052298": 85, "0523": 61, "052312": 62, "052356": 61, "052380": 70, "052488": 75, "052502": 85, "052676": 61, "052745": 72, "052932": 60, "053": [56, 99], "053049": 71, "053050": 63, "0533": 53, "053331": 72, "053342": 82, "053389": 129, "053436": 33, "053541": 85, "053558": 72, "053755": 61, "053849e": 70, "054": [56, 86], "054068": 80, "054162": 80, "054348": 129, "054370": 72, "054529": 129, "054771e": 85, "054802": 61, "055165": 88, "055171": 71, "055227": 61, "055439": [79, 82], "055493": 89, "055680": 129, "055907": 63, "056": 86, "0562": 61, "056375": 62, "056499": 75, "056745": 70, "056764": 70, "056794": 61, "056915": 77, "056959": 62, "057033": 61, "057095": 85, "057272": 62, "057498": 62, "0576": [55, 81], "057762": 85, "057792": 70, "057962": 72, "057976": 60, "058042": 129, "058232": 61, "058276": 82, "058375": 67, "058463": 85, "058478": 61, "058508": 90, "058595": 70, "058771": 62, "0588": 61, "05891": 99, "0590": 53, "059128": 70, "059191": 63, "059351": 60, "059384": 85, "059523": 61, "059622": 61, "059627": 82, "059630": 59, "059685": 85, "059970": 61, "06": [9, 10, 26, 60, 63, 67, 70, 71, 72, 81, 82, 85, 97, 98], "0600": 61, "060016": 67, "060030": 61, "06008533": 99, "060201": 85, "060212": [81, 82], "060417": 70, "060581": 78, "060845": 129, "060933": 70, "061": 83, "061090": 61, "0611": 53, "06111111": 56, "0615": 53, "061873": 63, "062": [86, 99], "062414": 82, "062504": 62, "062507": 85, "06269": 87, "062756": 61, "0628": 53, "062915": 62, "062964": 129, "062988": 70, "063": 83, "063017": 67, "0632": 53, "063234e": 71, "063276": 60, "06329768": [99, 102], "063327": 77, "063438": 62, "0635": 53, "063593": 71, "0636": 53, "063700": 70, "0638": 53, "063881": 99, "063882": 60, "0640": 53, "064022": 62, "064161": 82, "064213": 71, "06428": 81, "064280": 81, "0645": 53, "0646222": 55, "0647": 53, "064734": 61, "064756": 62, "064884": 61, "0649": [53, 61], "065": 89, "0653": 53, "065356": [74, 75], "065368": 79, "0654": 53, "065451": 82, "0655": 53, "065514": 62, "065552": 62, "065661": 63, "065725": 72, "0659": 53, "065925": 62, "065969": 99, "065976": 77, "066": 51, "0662": [53, 61], "066292": 62, "066295": 77, "066391": 61, "066464": 88, "066515": 60, "066670": 60, "066889": 85, "066892": 60, "0669": 53, "06692492": 113, "066966": 60, "067046e": 70, "0671": 53, "067212": 77, "067240": 85, "06724028": 85, "0673": 53, "067319": 62, "067425": 62, "0675": 53, "067524": 62, "067528": 89, "067721": 129, "067809": 61, "068073": 71, "06811": 99, "068167": 60, "068175": 62, "06827": 88, "06834315": 58, "068377": 82, "068415": 62, "068514": 70, "068700": 97, "068744": 63, "068934": 67, "06895837": 54, "069208": 63, "069210": 62, "069443": 67, "069567e": 63, "0695854": 54, "069589": 77, "069600": 82, "069882e": 70, "069976": 60, "07": [63, 70, 71, 82, 85, 86, 89, 113], "070020": 85, "070142": 61, "070196": 72, "0701961897676835": 72, "0702127": 54, "0704": [53, 61], "070411": 61, "070433": 77, "070497": 89, "070534": 37, "070552": 70, "070574e": 82, "0707": 53, "070751": 70, "07085301": 99, "070884": 85, "0711": 53, "071285": 129, "071289": 61, "07136": [54, 80], "071362": 70, "071488e": 72, "0716": 53, "07168291": 54, "071777": 98, "071782": [36, 97], "0718": 66, "071809": 60, "071876": 60, "0719": 53, "07202564": [74, 75], "07222222": 56, "072293": 84, "07229774": [99, 104], "072345": 60, "072408": 60, "072456": 61, "072516": 77, "072605": 67, "0727": 99, "072785": 60, "072952": 61, "073": 86, "073013": 85, "073142": 61, "073207": 80, "073275": 70, "073384": 77, "073447": [99, 104], "07347676": 54, "073496": 61, "07350015": [16, 19, 54, 80], "073520": 72, "0736": 53, "07366": [56, 98], "073694": 71, "0739130271918385": 79, "073929": 79, "074": 83, "074036": 60, "074292": 60, "0743": 53, "074304": 129, "07436521": 113, "074426": 85, "07456127": 54, "074599e": 63, "074617": 71, "07479278": 88, "074927": 67, "075": 83, "075261": 59, "075384": 85, "07538443": 85, "07544271e": 113, "07561": 143, "07564554e": 113, "0758": 89, "075809": 67, "075869": 98, "075942": 79, "07596974": 60, "076019": 81, "076136": 61, "076156": 129, "076179312": 129, "0761793120": 129, "0762": 66, "076202": 61, "076322": 85, "076347": 72, "0765": 56, "076596": 70, "076653": [99, 104], "076684": 143, "076778": 61, "07685043": 113, "07689": 56, "07691847": 113, "076953": [74, 75], "076971": 64, "077144e": 71, "077161": 82, "07720872": 63, "077233": 61, "07727773e": 113, "077319": 85, "077502": [130, 136], "077555": 70, "077592": 77, "077702": 67, "077770": 61, "0777777777777778": 98, "07777778": [56, 98], "07781396": 113, "077840": 82, "077883": 85, "07788588": [99, 104], "077923e": 70, "07796": 99, "078017": 70, "078096": 129, "078207": 64, "07828372": 129, "078426": 97, "078474": 129, "078709": 71, "078810": 85, "079085": 64, "07915": 56, "07919896": 113, "07942v3": 144, "079458e": 81, "079500e": 70, "07961": 88, "079707": 61, "07978296": 113, "08": [63, 72, 82, 85, 89, 99], "080048": 60, "080303": 63, "08031571": 113, "080729": 66, "080854": 82, "08091581": 113, "080947": 64, "081": 56, "0810188": 113, "081094": 61, "0811": 61, "081100": 85, "081230": [70, 71], "081396": 75, "081488": 80, "08154161": 113, "081575": 61, "08181827e": 113, "0820": 53, "082197": 77, "082263": 38, "082297": 113, "082399": 63, "082400e": 70, "082468": 62, "082574": 33, "082804": 59, "082858": 71, "082905": [99, 104], "082934": 82, "082973": 80, "083091e": 60, "083258": 129, "083318": 129, "08333333": 56, "08333617": 113, "083421": 61, "0835": 61, "0835771416": 54, "0836": 77, "08364": 77, "083706": 89, "083750": 82, "083822": 62, "083860": 63, "083925": 61, "083949": 89, "084": 54, "084007": 76, "084156": 71, "084184": 72, "0841842065698133": 72, "084269": 82, "084323": 70, "084337": 99, "084633": 74, "08488514": 113, "0853505": 54, "085395": 70, "085566": 72, "085592": 77, "085671": 70, "085713": 60, "085858": 63, "08586641": 61, "085965": 82, "086004": 77, "08602774e": 113, "0862": 141, "086264": 72, "086574": 60, "086616": 61, "08664208": 113, "086679": 98, "086889": 74, "086900": 60, "08711175": 61, "0872": 53, "087222": 71, "087561": 70, "087617": 60, "087634": 70, "087745": 71, "087947": 85, "088048": 85, "088054": 60, "088282": 75, "088336": 60, "088357": 85, "08848": 98, "088482": [36, 97], "088504e": 38, "08888889": 56, "088919": 60, "088935": [99, 102], "089181": 62, "0892494": 113, "0894": 53, "089591": 60, "08968939": 54, "089862": 60, "089964": 79, "08e": 55, "09": [70, 71, 72, 81, 82, 85, 97], "09000000000000001": 98, "090025": 82, "09015": 53, "090255": 85, "090436": 71, "091029": 60, "091121": 60, "091179e": 70, "091263": 79, "091391": 129, "091406": 130, "091535": 70, "0916": 53, "091721": 61, "091824": 71, "091992": 84, "092229": 89, "092247": 85, "092263": 99, "092365": 129, "092919": 131, "092935": 70, "093043": 85, "09310496": 129, "093153": 85, "093474": 85, "09347419": 85, "0935": 99, "09351167": 99, "093525": 62, "093746": 129, "093788": 63, "093950": 80, "093983": 60, "094026": 80, "094118": 85, "094247": 63, "094378e": 70, "094381": 80, "09444444": 56, "09448333": 113, "094581e": 71, "094595": 66, "094829": 99, "094999": 85, "095": 51, "095104": 67, "095587": 60, "095654": 70, "095781": 31, "095785": 67, "096029": 60, "09603": 141, "096337": 80, "096364": 63, "096418": 67, "096550": 74, "096565": 60, "096575": 60, "096599": 60, "096616": 97, "096688": 71, "096741": 58, "09682314": 99, "096915": 89, "097": 86, "097009": 77, "097157": 89, "097404": 63, "097468": 72, "09756": 87, "09779675": 129, "097796750": 129, "098": 55, "098202": 66, "098256": 85, "09830758": 88, "098308": 88, "098317": 82, "098319": 85, "098510": 61, "0986": 53, "098712": 85, "09879814e": 113, "098901": 77, "099": 86, "099001": 71, "099224": 62, "099307": 71, "099485": [99, 100], "099647": 84, "099670": 82, "099731": [70, 71], "09980311": 129, "099822": 60, "09988": 144, "0_": 15, "0ff823b17d45": 56, "0x1747bdd4520": 64, "0x1747bdd6b90": 64, "0x2920d7b7150": 84, "0x7f93b387dac0": 89, "0x7fd0332ac1d0": 146, "0x7fd0335ff0b0": 136, "0x7fd033860e60": 130, "0x7fd033ce0f20": 129, "0x7fd033ce11c0": 129, "0x7fd041190080": 98, "0x7fd0411921b0": 98, "0x7fd0411929c0": 98, "0x7fd0412ec6b0": 100, "0x7fd0415927e0": 129, "0x7fd041593ad0": 129, "0x7fd04181b4d0": 99, "0x7fd0418ee420": 100, "0x7fd041aba030": 99, "0x7fd0423a3fe0": 99, "0x7fd042597470": 99, "0x7fd0429889e0": 99, "0x7fd0433d0b00": 99, "1": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 108, 109, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145], "10": [7, 8, 9, 10, 12, 14, 16, 17, 19, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 130, 136, 141, 143, 144, 145, 146], "100": [16, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 56, 57, 58, 70, 71, 73, 76, 77, 78, 80, 83, 87, 89, 90, 91, 93, 95, 97, 98, 99, 101, 113, 114, 129, 130, 136, 143, 145], "1000": [25, 32, 34, 52, 58, 59, 66, 68, 69, 74, 75, 76, 78, 79, 81, 82, 86, 88, 89, 92, 99], "10000": [51, 59, 70, 71, 81, 82, 85], "100000e": 82, "100044": 75, "100154": 79, "100208": 97, "1003": 66, "100356": 72, "10038": 88, "100385": 79, "10039862": [90, 99], "100517": 129, "10059479": 61, "100715": 70, "100717": 60, "10079785": 99, "1008": 61, "100807": [70, 71], "100817": 61, "100858": 88, "100894": 63, "10089588": 85, "100896": 85, "10092": 82, "100923": 85, "100927": 60, "100_000": 83, "101": [9, 10, 26, 53, 86, 97, 144, 145], "101214": 63, "10126": 82, "10127930": 129, "101279300": 129, "101361": 60, "1015": [55, 81], "1016": [9, 10, 25, 26, 53], "1016010": 55, "1018": [82, 89], "101850": 60, "101862e": 113, "101998": 67, "102": [93, 95, 97, 143, 145], "10235": 82, "10258": 82, "102616": 72, "102775": 72, "10299": 81, "103": [70, 80, 86, 90, 97, 145], "10307": 129, "1031": 82, "103189": 82, "10348": 81, "103497": 85, "103635": 63, "1037458": 113, "1038": 82, "103806": 72, "103951906910721": 72, "103952": 72, "10396": 81, "104": [55, 81, 90, 97, 145], "10406": 82, "104087": 70, "1041": 53, "10414": 82, "104475": 60, "10449": 87, "104492": 77, "1045303": 54, "10477926": 61, "104787": 80, "104849": 70, "105": [15, 54, 77, 80, 97, 145], "105053": 60, "105260": 61, "105318": 85, "1054": 56, "105461": 97, "1055": [53, 87], "106": [56, 97, 145], "10607": [64, 93, 95, 143], "10618": 82, "106205": 62, "10637173e": 113, "106391": 129, "1065": [73, 78, 79], "106595": [99, 101], "106658": 83, "106715": 76, "106746": 85, "1069302": 113, "107": [56, 61, 89, 97, 145], "107073": 72, "107156": 77, "10717876": 61, "107295": 129, "1073": 82, "107413": 70, "10747": [64, 93, 95, 143], "107687": 61, "10799": 82, "108": [97, 141, 144, 145], "1080": [16, 19, 53, 54, 80], "108002": 61, "10824": [64, 93, 95, 143], "108257e": 82, "108259": 67, "10831": [64, 93, 95, 143], "10878571": 85, "108786": 85, "108870": [99, 104], "109": [70, 97], "109005": 85, "10903": 81, "109069": 129, "109079e": 85, "109273": 80, "109277": 77, "10928": 82, "1093": 73, "109454": 82, "109470": 71, "1096": [53, 87], "10967": 81, "109820": 60, "109861": 143, "1099472942084532": 68, "10e": [72, 85], "11": [39, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 85, 86, 88, 89, 91, 93, 95, 97, 98, 99, 100, 101, 104, 113, 114, 129, 130, 136, 143, 146], "110": [97, 145], "110081": 77, "1101": 82, "1101096": 113, "11019365749799062": 89, "110194": 89, "110359": 80, "110365": 89, "110430": 61, "110681": 88, "1107": 82, "11071087": [90, 99], "110717": 129, "1109": 82, "110902": 72, "110902411746278": 72, "111": [71, 97, 99, 145], "111094": 62, "1111": [7, 8, 17, 52, 54, 69, 73, 80, 89, 92, 99, 130, 136, 141], "111164": 84, "11120": 82, "11152982": 61, "1116171825273233354246474950545859687089": 113, "1116171825273233354246474950545859687089514213436394143455152627475768082849092471219202429314048566164667177868794951231028303844536063787981919396979899": 113, "1116171825273233354246474950545859687089514213436394143455152627475768082849092689131522232637555765676972738385881001231028303844536063787981919396979899": 113, "11161718252732333542464749505458596870895142134363941434551526274757680828490926891315222326375557656769727383858810047121920242931404856616466717786879495": 113, "111617182527323335424647495054585968708968913152223263755576567697273838588100471219202429314048566164667177868794951231028303844536063787981919396979899": 113, "1117": [73, 78, 79], "111715": 63, "1118": 55, "11199615e": 113, "112": [56, 97, 145], "1120": 81, "112078": 97, "11208236": [91, 114], "1122": 82, "112216": 72, "112326": 63, "112591": 62, "11289345": 61, "1129": 82, "113": [7, 97, 145], "113022": 77, "11311": 81, "113149": 77, "1131657": 113, "113207": 85, "113270": 72, "113307": 63, "113415": 82, "11375": 82, "113780": 80, "113952": 77, "11399": 81, "114": [97, 145], "114029": 60, "114287": 60, "1144500": 54, "11447": 88, "114530": 74, "1145370": 54, "114570": 71, "11458": 82, "114647": 72, "1146819": 113, "1147": 53, "1148": 82, "114834": 82, "11488": 82, "11495": 82, "11495452": 61, "114989": 67, "115": [97, 145], "11500": [81, 146], "115060e": 85, "1151610541568202": 79, "115193": 60, "115296e": 82, "115297e": 81, "115390": 63, "1155142425200442": 79, "11552911": 88, "11559": 82, "115636": 71, "11570": 81, "115792e": 82, "115901": 67, "115964": 63, "115972": 70, "116": [97, 145], "116027": 72, "116090": [99, 102], "1161589": 113, "11617": 82, "116274": 72, "116526": 61, "116569": 82, "1166": [81, 144], "1167": 81, "11673": 82, "11675": 82, "117": [70, 97], "11700": 146, "117072": 74, "117112": 71, "1172": 113, "117242": 85, "11724226": 85, "117366": 85, "11743": 146, "1175": 113, "11750": 82, "1176": 53, "1177": 53, "117710": 72, "117890": 63, "11792": 55, "11796": 82, "117981": 62, "118": 97, "1180": 61, "11802": 82, "1182": 55, "11823404": 89, "118255": 85, "1186": 55, "118601": 80, "11861": 55, "1187": 88, "1187339840850312": 80, "11879": 82, "118799": 82, "118938e": 99, "118952": 80, "119": [89, 97, 145], "1190355": 113, "11932": 82, "11935": 88, "119362": 60, "1194166": 113, "11942111": [99, 104], "119704": 63, "119766": 85, "1198": [54, 80], "119820": [99, 102], "119864": 63, "12": [21, 24, 25, 37, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 72, 74, 75, 76, 77, 79, 80, 81, 82, 83, 85, 86, 88, 89, 91, 93, 95, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 130, 136, 141, 143, 144, 145, 146], "120": [57, 58, 79, 90, 97, 145], "12002": 81, "120041": 60, "1200x600": [60, 61, 63], "1200x800": [60, 61, 63], "1202": 144, "120399": 60, "120468": 85, "12046836": 85, "120567": [74, 75], "120721": 80, "12080467": [99, 102], "12097": [7, 8, 17, 54, 73, 80, 92, 141], "121": [82, 97, 145], "1210": 82, "12101": 82, "12105472": 129, "121054720": 129, "1211": 82, "1213405": 54, "121399": 82, "1214": 129, "121485": 63, "121584e": 85, "121711": 82, "121774": 76, "121824": 71, "121875": 60, "121918": 113, "121946": 60, "12196389e": 113, "122": [9, 10, 26, 53, 60, 86, 93, 95, 97, 144, 145], "122076": 60, "12214": 55, "12223182e": 113, "122408": 72, "122421": 77, "122777": 129, "123": [40, 55, 56, 60, 63, 81, 89, 97, 145, 146], "1230": 82, "1231028303844536063787981919396979899": 113, "123192": 89, "12323": 82, "1234": [51, 52, 53, 64, 68, 69, 86, 87, 92, 98, 113, 129], "12348": 89, "1235829": 113, "123786": 63, "1238": 82, "123917": 82, "123950": [99, 100, 104], "124": 97, "12410": 82, "124306": 79, "124480": 79, "1246173": 113, "124657": 60, "124805": 81, "124825": 71, "1249385": 113, "125": [83, 97, 145], "12500": 81, "125065": 129, "125153": 60, "12539340": 129, "1255": 82, "125683": 63, "12579": 82, "1258": 54, "126": [97, 145], "12606": 82, "12612": 82, "126557": 60, "126777": 129, "126802": 82, "1268354": 113, "12689": 82, "127": [9, 97, 145], "127006": 82, "12705095": [114, 129], "127059": 60, "12707800": 54, "127152": 60, "12715432": 61, "1272404618426184": 79, "1272572": 113, "127337": 77, "127420": [99, 102], "12752825": 129, "127563": 88, "1277": 83, "127778": 82, "127889": 97, "128": [55, 97, 145], "12802": 55, "12814": 82, "128229": 77, "128300e": 71, "128312": 85, "128408": 80, "1285": 53, "12861": 82, "128651": 71, "129": [51, 80, 97, 145], "129035": 60, "12945": 144, "12947879": [99, 102], "1295": [53, 82], "129514": 82, "12955": 81, "129606": 70, "129798": 70, "1298": 82, "12980769e": 113, "12983057": 99, "13": [10, 12, 14, 25, 26, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 72, 74, 75, 76, 77, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 91, 93, 95, 97, 98, 99, 101, 102, 113, 114, 129, 130, 136, 143, 146], "130": [56, 74, 80, 97, 145], "13003274": [99, 102], "130122": 88, "13034980e": 113, "130370": 72, "1306": 88, "1308": 61, "130829": 85, "13084": 99, "130847": 62, "13091": 82, "1309844442144665": 79, "131": [83, 97, 145], "13102231": 99, "131024": 79, "13119": 88, "1312": 146, "131211": 82, "1313": [55, 146], "13137893e": 113, "1314506": 113, "131475": 60, "131483": 77, "131626": 60, "1318": 53, "131842": 77, "132": [56, 70, 80, 97, 145], "132078": 60, "13208": 146, "1321": [81, 146], "132248": 97, "132329": 63, "1324": [55, 81], "132454": 59, "132481": 97, "1325": 55, "132506": 61, "13257": 81, "132594": 61, "132671": 72, "132756": 62, "1328": 88, "132903": 82, "132982": 70, "133": [56, 93, 95, 97, 144, 145], "13300": 82, "133202": 82, "13339508": 61, "133421": 82, "13356": 82, "133596": 85, "13362729": 61, "133686": 60, "133793": 60, "133839": 77, "13387556": 61, "13398": 89, "133f5a": 83, "134": [80, 90, 97, 145], "134037": 62, "1340371": 53, "134097": 62, "1341": 55, "134146": 82, "1342": 82, "134211": 85, "1343": 81, "134542": 70, "134567": 82, "1346035": 55, "134687": 82, "13474": 82, "134765": 82, "134784": 99, "134784e": 70, "1348": 81, "13490": 82, "135": [56, 97, 99, 145], "13505272": 54, "135142": 71, "135344": 67, "135352": 20, "135379": 129, "135427": 62, "135665": 71, "135707": 98, "135856": 85, "13585644": 85, "135871": 80, "136": [64, 80, 89, 97, 145], "1360": 55, "13602": 89, "136089": 80, "1361": 82, "136102": 70, "1362430723104844": 79, "1364036": 113, "13642": 82, "136442": 80, "136448": 60, "1366": 83, "136836": 80, "137": [9, 56, 64, 97, 145], "1371": 82, "13714951": 63, "137165": 99, "137186": 61, "1372": 61, "137213": 71, "137292": 60, "137396": 85, "137565": 60, "1378": 82, "13781682": 61, "137999": 99, "138": [97, 145], "1380": 81, "138068": 74, "138088": 63, "13809": 82, "138264": 89, "138378": 72, "1384": 81, "13852517": 61, "1386": 53, "13868238": 129, "138682380": 129, "138698": 129, "1387": 53, "138851": 74, "13893": 82, "139": [89, 97, 143], "139117e": 70, "139411": 63, "139491": 129, "139508": 77, "13956": 88, "139797": 61, "1398": 82, "139830": [99, 104], "139840": 60, "1399": 53, "14": [52, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 72, 74, 75, 76, 77, 79, 80, 81, 82, 85, 86, 88, 89, 91, 93, 95, 97, 98, 99, 101, 102, 113, 114, 129, 130, 136, 143, 144, 146], "140": [57, 58, 77, 82, 90, 97, 145], "1400": 82, "14000073": 113, "140073": 70, "140081": 79, "1401": 53, "14018": 99, "140770": [70, 71], "140833": 72, "140861": 54, "140926": 85, "141": [82, 97, 145], "141002": 71, "141023": 63, "14105419": 61, "141098e": 82, "14114": 88, "1412": 61, "14141": 82, "141460": 67, "141546": 129, "141723": 61, "141820": 72, "142": [97, 145], "14200098": 129, "142119": 70, "14216783": 61, "142270": 59, "142382": 70, "1424": 98, "14268": 99, "14281403493938022": 98, "14289": 82, "143": [63, 93, 95, 97, 145], "143342": 71, "143495": 97, "1435": 82, "143534": 70, "14368145": 129, "143969": 63, "144": [97, 145], "14400": 81, "14405": 82, "14406": 82, "144084": 72, "1441": 53, "144137": 58, "144241": 74, "1443": 82, "144500e": 82, "144669": 85, "1447": 82, "144800": 72, "144908": 84, "144971": 81, "145": [97, 145], "145019": 62, "145027": 67, "145245": 85, "14532650": 129, "145625": 85, "14566354": 61, "145748": 129, "14587": 82, "146": [97, 145], "146037": 85, "146087": 143, "146142808990006": 72, "146143": 72, "14625": 82, "146357": 60, "146435": 71, "14647959": 63, "1465": 55, "14660168": 63, "146641": 129, "14667": 82, "146768": 63, "1468115": 54, "146896": 63, "146973": 72, "1469734445741286": 72, "147": [63, 97, 145], "147015e": 82, "14702": 64, "147121": 85, "14744": 82, "14772": 82, "1479": [60, 66, 82], "14790924": 129, "147909240": 129, "147927": 64, "14798": 82, "148": [97, 145], "148005": 67, "14803": 82, "148134": [70, 71], "148161": 85, "148239": 63, "14845": 64, "1485": 82, "148569": 60, "148750e": [81, 82], "148790": 82, "148802": 82, "149": [97, 145], "149087": 63, "1492": 51, "149215e": 71, "149228": 89, "149285": 85, "149472": 89, "149714": 80, "14984": 82, "149858": [35, 97], "149898": 85, "149970": 63, "15": [8, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 51, 52, 54, 55, 56, 58, 60, 63, 66, 67, 69, 70, 71, 72, 74, 76, 77, 78, 79, 80, 81, 82, 85, 86, 87, 88, 89, 93, 95, 97, 98, 99, 101, 102, 113, 114, 129, 130, 136, 143, 146], "150": [15, 56, 89, 97, 145], "15000": [55, 81], "150000": 55, "15000000000000002": [72, 82, 85, 98], "150000e": 82, "150119": 60, "1502": 54, "150200": 80, "150287e": 83, "150334": 82, "150339": 60, "150408": 54, "150614": 64, "150719e": 81, "15072148": 61, "1509": 66, "150985": 60, "151": [97, 145], "151047e": 74, "151063": 70, "15113": 82, "151202": 60, "151440": 62, "15152146": 61, "151636": 72, "151819": 85, "15194": 81, "152": [83, 97, 145], "152034": 82, "15208313": 61, "15211773": 61, "152148": [70, 71], "152353": [60, 71], "152366": 60, "152706": 99, "15285": 82, "152896": 79, "152926": 59, "153": [89, 97, 145], "1530959776797396": 72, "153096": 72, "153119": 72, "1532399": 113, "153314": 71, "15347": 82, "15354": 88, "153587": 80, "153633": 64, "153639": 113, "15365199": 61, "153917": 63, "153935": 71, "153958": 60, "154": [63, 66, 97], "154269": 60, "15430": 146, "154421": 129, "1545": 82, "154525": 60, "154557": 85, "154758": 129, "154821": [99, 102], "154828": 72, "154890": 79, "155": [97, 145], "155000": 81, "155025": 85, "155120": 85, "155160": 67, "155174": 67, "155423": 67, "155516": 84, "15556": 82, "1557093": 54, "156": [97, 145], "1560": 82, "156021": 85, "156169": 71, "156202": [70, 71], "156246": 60, "156317": [70, 71], "1564": 129, "156545": 129, "156684": 71, "156712": 63, "1569": 82, "156969": 72, "156988": 66, "157": [51, 71, 97, 145], "157091": 129, "157151": 63, "157154": 70, "157160": 63, "157327": 61, "1576": 82, "157733": 79, "15774100": 113, "1577657": 54, "1579268": 113, "157e": 99, "158": [97, 145], "158007": 85, "15815035": 55, "158178": 72, "1582": 82, "1586": 82, "158697": 129, "158726": 97, "1589": 82, "158909": 63, "15891559": 85, "158916": 85, "159": [63, 145], "15905872": 61, "1591365": 113, "15915": 62, "15916": [53, 62], "159386": 88, "159451": 63, "1596": 56, "159633e": 71, "159841": 70, "159959": 82, "16": [31, 51, 52, 54, 55, 56, 57, 60, 62, 63, 66, 67, 70, 71, 72, 75, 76, 77, 80, 81, 82, 85, 86, 88, 89, 93, 95, 97, 98, 99, 101, 102, 113, 114, 129, 130, 136, 143, 146], "160": [57, 58, 90, 145], "1604": 55, "160836": 77, "160932": 72, "160959": 63, "161": [56, 66, 144, 145], "161049": 71, "161141": 80, "16119253": 61, "161198": 84, "161236": 85, "161243": 85, "161269": 70, "161288": [71, 79], "16132913": 61, "161543": 82, "1619": 55, "16194239": 61, "161975": 62, "162": [66, 145], "16201": 82, "16205786": 61, "162085": 60, "16211": 81, "162153": 85, "1622": 82, "16241": 82, "162436": 89, "16249735": 61, "162593": 79, "1626685": 54, "162683": 89, "162710": 72, "1628": 81, "162930": 82, "162931": 63, "162970": 60, "163": [82, 145], "163194": 85, "163519": [99, 102], "163566": 82, "16380547": 61, "163895": 72, "164": [66, 67, 86, 145], "1640129": 61, "164034": 129, "164467": 81, "164608": 85, "164698": 76, "1648": 53, "164801": 85, "164805": 72, "164864": 80, "165": [66, 145], "16500": 81, "165081": 63, "165178": 85, "16536299": 129, "165362990": 129, "16539906e": 113, "1654": 82, "165419": 85, "16553": 81, "165549": 143, "165707": 67, "16590": 82, "16597": 82, "166": [63, 145], "1661": 81, "166159808": 113, "166238": 79, "166375": 97, "166517": 77, "166753": 60, "16696076": 113, "167": [55, 63, 81, 145], "167142895": 113, "167174": 60, "16725": 82, "167547": 85, "167581e": 70, "1676": 82, "167765": 82, "167993": 129, "168": 145, "16803512": 129, "168089": 79, "168092": 129, "1681": [53, 81], "168195": 88, "168356": 61, "1684": 61, "168605": 60, "168614": 85, "168689": 60, "168931": 85, "169": [56, 145], "1691": [53, 82], "16910": 82, "169117": 89, "169196": 85, "169230e": 72, "1693": 61, "16951": 82, "169527": 60, "16984": 82, "169915": 60, "17": [52, 54, 55, 56, 60, 63, 66, 67, 70, 71, 76, 77, 80, 81, 82, 85, 86, 88, 89, 93, 95, 97, 98, 99, 101, 113, 114, 129, 130, 136, 143, 146], "170": 145, "1704": 82, "170705": 97, "170709e": 71, "17083": 82, "170933": [99, 104], "171": [66, 145], "1712": 144, "1714": 55, "17142286": 61, "171575": 85, "171696": 97, "1717": 113, "171815": 98, "171833": 71, "171848e": 70, "1719": [60, 61, 62, 63], "171942": 82, "172": [66, 86, 145], "172022": 129, "172062": 63, "172083": 71, "17212659": 61, "172169": 61, "17225791": 61, "17236189": 61, "172628": 79, "172793": 85, "17286026": 61, "173": [66, 145], "17314537": 113, "173400": 66, "173504": 75, "17371855": 61, "17372": 82, "1738": 82, "17385178": 98, "173969": 129, "173986": 60, "173e": 86, "174": 145, "174048": 61, "174185": 85, "174499": 129, "174516e": 85, "17453": 82, "174597": 62, "1746": 82, "174694": 63, "174835": 71, "174968": 81, "17499": 82, "175": [66, 145], "1751": [66, 81], "175176": 85, "17522": 82, "175254": 81, "175284": 72, "175369": 71, "1755": 66, "175635027": 54, "17576": 82, "175894": 89, "175931": [97, 98, 99], "176": 146, "176002": 63, "176278": 60, "176495": 85, "17655394": 85, "176554": 85, "176825": 60, "176929": 129, "177": [144, 145], "177007": 85, "17700723": 85, "177022": 63, "177043": [70, 71], "177094": 66, "1773": 82, "1774": 53, "177463": 84, "177496": 85, "177611": 85, "177740": 70, "177751": 85, "17778": 82, "177830": 71, "17799": 82, "177995": 85, "178": [63, 145], "1780474": 113, "178169": 74, "178218": 71, "17823": 56, "1783968": 113, "178704": 129, "178763": 85, "178934": 129, "179": [63, 74, 83, 145], "179026": 71, "179101": 97, "1795": 66, "1795850": 54, "179588e": 85, "179777": 71, "1798913180930109556": 83, "18": [52, 54, 55, 56, 60, 63, 64, 66, 70, 71, 76, 77, 78, 80, 81, 82, 85, 86, 88, 89, 93, 95, 97, 98, 99, 101, 113, 129, 143, 146], "180": [57, 58, 90, 145], "180143": 67, "18015": 82, "180176e": 82, "180262": 71, "180271": 77, "1803": 53, "18030": 82, "180516": 61, "180575": [74, 75], "1807": [53, 82], "1809": 144, "180951": 85, "181": [63, 145], "1812": 82, "1814": 53, "18141": 82, "181446": 129, "18147168": 61, "181523": 63, "18157439": 61, "181752": 60, "182": 145, "1820": 53, "182427": 71, "182529e": 113, "182633": 85, "18273096": [99, 102], "182849": 85, "183": [56, 66, 81, 99, 145], "183146": 63, "183339": 70, "183373": 99, "183526": 72, "183553": 86, "18356413": 99, "18368": 82, "183855": 98, "183888": 80, "184": [56, 60, 144, 145], "184224": 67, "184247": 70, "184303": [99, 104], "184347": 71, "1844422": 113, "184457": 63, "1845458": 113, "1848624": 60, "184931": 62, "185": [55, 56, 63], "18500": 82, "185130": 86, "18516129": [99, 104], "1855": 82, "185751": 62, "185984": 70, "186": [66, 82, 145], "18604": 82, "1862": 53, "186237": 71, "18631": 82, "18637": 141, "186589": 67, "18666": 82, "186689": 99, "186735": 85, "186775": [99, 100, 104], "18678094e": 113, "186788": 60, "186836": 85, "187": 145, "187153": 129, "187601": 62, "187664": 70, "187690": 85, "18789": 82, "187971": 60, "188": [60, 63, 145], "188175": 85, "1881752": 85, "188223": 85, "188400": 71, "188541": [99, 100], "1887": [97, 98, 99], "188760": 77, "18888149e": 113, "188882": 71, "188991": 129, "189": [56, 86, 145], "189138": 63, "189195": 82, "189248": 70, "189293": 82, "189453": 60, "1895815": [16, 54, 80], "189737": 85, "189739": 67, "189927": 82, "189998": 85, "19": [52, 54, 55, 56, 60, 63, 66, 70, 71, 77, 79, 80, 81, 82, 85, 86, 88, 89, 97, 98, 99, 101, 102, 113, 129, 143, 146], "190": [56, 145], "19000": 82, "190096": 129, "190201": 60, "19031969": 85, "190320": 85, "19033538": 54, "190534": [99, 102], "190596": 60, "190648": 32, "19073905e": 113, "190809": 85, "190892": 89, "1909": [16, 54, 80], "190915": 72, "190918": 60, "190921": 75, "19093": 63, "190976": 86, "190982": 85, "191": [56, 63, 144, 145], "191100": 60, "191192": 70, "1912": 144, "191223": 71, "1912705": 92, "191294": 71, "191534": 81, "191624": 60, "191716": 82, "1918": 53, "192": [60, 145], "192065": 63, "1922": 82, "192240": 129, "192434": 62, "192505": 84, "192526": 88, "19252647": 88, "192539": [36, 97], "192587": 85, "192952": 67, "193": [60, 145], "193060": 85, "193253": 70, "193285": 70, "193308": [36, 97], "193341": 71, "193375": 77, "19374710e": 113, "19382": 82, "193849": 86, "19385": 82, "193f0d909729": 56, "194": [63, 66, 78, 82, 113, 145], "194092": 70, "1941": 55, "19410": 60, "194101": 63, "19413": [81, 82], "194303": 71, "194601": 58, "194785": 62, "194786": [99, 100], "195": [63, 94, 95, 145], "19508": 89, "19508031003642456": 89, "19509680e": 113, "195377": 85, "195396": 85, "195547": 82, "195564": 80, "19559": [55, 81], "195761": 85, "195781": 70, "1959": 144, "195963": 77, "196": 145, "196097": 63, "196189": 85, "196437": 82, "196478e": 71, "196655": 77, "19680840": 129, "196e": 40, "197": [83, 145], "1970": 82, "197000e": 82, "19705": 82, "197225": [64, 93, 95, 143], "1972250000001000100001": [56, 93, 95, 143], "1974": 82, "197424": 98, "197484": 129, "19756": 82, "19758": 82, "197600": 59, "197711": 82, "197920": 70, "19793": 82, "19794": 82, "197962": 61, "198": [60, 63, 145], "198218": 80, "19824": 82, "198351": 85, "198493": 77, "198503": 86, "198549": 64, "198687": 55, "1988": [52, 69, 92, 99], "198948": 63, "199": 145, "1990": [55, 81, 82], "1990504": 113, "1991": [55, 81, 82, 146], "199281e": 85, "199282e": 82, "199412": 71, "199458": 129, "1995": [54, 80], "199547": 60, "1998": 83, "19983954": 90, "199869": 60, "199893": 74, "1999": [83, 90], "1_": [72, 85], "1d": 21, "1e": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 66, 77, 82], "1f77b4": 59, "1m": 98, "1x_4x_3": 59, "2": [7, 9, 10, 11, 12, 13, 14, 15, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 45, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 108, 113, 114, 129, 130, 131, 133, 135, 136, 137, 138, 139, 140, 142, 143, 144, 145], "20": [9, 10, 12, 13, 14, 16, 17, 18, 20, 22, 23, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 55, 56, 57, 58, 60, 63, 66, 70, 71, 72, 74, 76, 77, 78, 80, 81, 82, 85, 88, 89, 90, 91, 92, 97, 98, 99, 101, 113, 114, 129, 130, 136, 143, 146], "200": [11, 14, 15, 25, 53, 57, 58, 60, 63, 72, 73, 78, 83, 84, 85, 90, 92, 98, 144, 145], "2000": [8, 37, 55, 57, 67, 70, 71, 72, 77, 81, 82, 85, 87, 90, 97, 99], "20000": [55, 81], "20000000000000004": [72, 82, 85], "200000e": 82, "200049": 70, "2001": 61, "20010": 82, "200110": 82, "2003": [7, 61, 144], "200303": 143, "2004": [61, 99, 103], "200424": 60, "2005": [58, 61], "20055": 82, "2006": [61, 82, 99, 103], "2007": [61, 99, 103], "20073763": 78, "20074": 82, "20076187": 61, "2008989": 61, "20099453": 61, "201": [56, 82, 145], "2010": [54, 80], "2011": [54, 80, 141, 143], "201297": 63, "2013": [73, 129, 144], "20135158": 61, "2014": [129, 144], "201453": 61, "2015": [15, 144], "201514": [99, 102], "201528": [70, 71], "20158": 82, "2016": 83, "2017": [13, 144], "201768": 80, "201788e": 82, "2018": [7, 8, 17, 18, 52, 54, 55, 58, 69, 73, 78, 80, 81, 82, 87, 88, 92, 99, 105, 113, 114, 122, 129, 141, 144, 145], "2019": [11, 56, 70, 71, 72, 74, 75, 82, 85, 88, 98, 114, 120, 123, 124, 141, 143, 144], "201e": 86, "202": [60, 63, 145], "2020": [9, 10, 12, 14, 20, 22, 23, 25, 26, 53, 56, 58, 60, 63, 89, 98, 99, 101, 105, 130, 131, 144], "2020435": 54, "202071": 60, "20208665": 113, "2021": [16, 25, 53, 54, 56, 60, 61, 62, 63, 70, 71, 80, 99, 100, 103, 105, 144, 145], "20219609": 54, "2022": [88, 89, 99, 101, 105, 130, 131, 140, 141, 144], "2023": [19, 57, 87, 90, 99, 112, 114, 127, 128, 144], "2024": [51, 68, 73, 78, 79, 83, 86, 87, 89, 99, 141, 144], "2025": [25, 60, 63, 87, 99, 100, 102, 104], "202603": 71, "202650e": 72, "20269": 82, "20274": 82, "202846": 71, "203": [55, 60, 70, 81, 145], "203284": 72, "20329": 82, "203490": 60, "2036": 82, "203828": 82, "204": [63, 145], "204007": 85, "20400735": 85, "204362": 89, "204455": 71, "204482": 85, "204486": 60, "204755": 63, "204794": 85, "204893": 67, "205": [86, 88, 145], "205187": 72, "205224": 88, "205333e": 81, "2055": 66, "20554305": 61, "205938": 80, "206": 145, "2061": 82, "206253": [81, 82], "206256": 77, "2064": 82, "206614": 85, "20681944": 61, "206831": 60, "207": [60, 63, 66, 86, 99, 145], "2070037": 113, "2075": 53, "207607": 63, "207834": 71, "20783816": 54, "207840": 75, "207885": 81, "207912": 129, "208": [60, 63, 66, 67, 83, 145], "208034e": 82, "2080787": 54, "20823898": 54, "2086": [66, 82], "208789": 63, "208840": 66, "209": [60, 63, 66, 67], "209014": 85, "2091": 66, "209219e": 88, "209257": 20, "209280": 63, "209281": 60, "209451": 61, "209489": 61, "2095": 61, "209546e": 82, "20957758": 63, "209894": 85, "209950": 61, "21": [7, 8, 17, 52, 54, 55, 56, 60, 63, 66, 70, 71, 73, 80, 81, 82, 85, 87, 88, 89, 92, 97, 98, 99, 101, 113, 129, 141, 143, 144, 146], "210": [10, 14, 25, 26, 60, 63, 66, 67, 83], "210247": 63, "2103": [82, 141], "2103034": 54, "210319": [70, 71], "210323": 85, "2104": 145, "2107": 144, "21078": 82, "211": [60, 63, 66, 67, 86, 145], "211002": [99, 104], "21105": [56, 98, 141, 143], "2112": 89, "21142": 82, "211534": 72, "21155656": 85, "211557": 85, "2116": 66, "211939": 66, "211977": 63, "212": [60, 63, 67, 83, 145], "2122": 82, "21257396e": 113, "212811": 67, "212844": 80, "212863": 70, "212923": 83, "213": [63, 66, 67, 86, 144, 145], "213026": 82, "213070": 71, "213135": 71, "213199": 99, "21361": 82, "213743e": 71, "2139": 12, "213976": 60, "214": [60, 63, 66, 145], "214006": 60, "214006e": 63, "214458": 79, "214764": 88, "214769": 77, "215": [67, 145], "215069": 85, "215342": 85, "2155": 82, "21550": 82, "21562": 82, "21573": 82, "2158211": 63, "215967": 129, "216": [66, 67, 145], "216052": 63, "216207": 98, "21624417": 54, "2162744": 113, "2163": 82, "216344": 85, "216409": 60, "216658": 60, "21669513e": 113, "216761": 84, "216943": 97, "217": [60, 66, 67, 86, 144, 145], "21716": 82, "2171802": [54, 80], "217244": 34, "217270": 60, "217684": 77, "218": [63, 66, 67, 145], "21804": [55, 81], "218383": 67, "21870856": 113, "218767": 82, "2189": 82, "218919": 66, "218938": 82, "219": [9, 10, 26, 51, 53, 66, 67, 144, 145], "2191274": 54, "219230": 60, "219585": 67, "2197237644227434": 79, "22": [52, 54, 55, 56, 60, 63, 70, 71, 79, 80, 81, 85, 86, 88, 89, 97, 98, 99, 101, 113, 129, 143, 146], "220": [40, 63, 66, 67, 145], "220088": 82, "220171": [99, 102], "220398": [63, 70], "220407": 79, "220568": [99, 102], "220772": 85, "220777": 60, "221": [60, 66, 67, 145], "221126": 60, "2213": 80, "2214": 80, "221419": 82, "2215": 80, "2216": 80, "2217": [54, 80], "2218": 144, "221883": 60, "222": [63, 66, 83, 145], "2222": [52, 54, 69, 99], "22222": 82, "222261": 97, "2223": 66, "22272803e": 113, "222843": 85, "222882": [71, 79], "223": [86, 145, 146], "223158": 79, "2233": 113, "22336235": 54, "223485956098176": [74, 75], "223617": 79, "223726": 60, "22375856": 54, "22390": 81, "223928": 79, "224": [60, 63, 86, 145], "2244": 144, "224635": 60, "224655": 63, "224872": 62, "224897": [70, 71], "22497991": 113, "225": [25, 53, 90, 144, 145], "225034": 58, "22505965": 54, "22507006e": 113, "22515044": [99, 102], "225175": 85, "225222": 85, "22522221": 85, "22528": 82, "225350": 71, "225427": 67, "225459760731946": 72, "225460": 72, "225574": 80, "2256": 82, "22562": 82, "225670": 79, "225776": 89, "225899": [99, 104], "226": [60, 145], "22613964": 113, "226158": 60, "2264": 53, "226479": 77, "2264791": 113, "226524": 85, "226598": 80, "226938": 75, "226969": 67, "227": [60, 63, 82, 145], "2271071": 19, "2276": 53, "227789": 63, "2279": 82, "227932e": 81, "228": [63, 145], "228035": 82, "2281": 82, "228404": 79, "228597": 60, "228597e": 71, "22860485": 61, "228630": 71, "228648": 55, "2287": 66, "229": [55, 60, 145], "2290541": 61, "22913518": 61, "22913576": 61, "22913688": 61, "22913957": 61, "229208": 60, "22925": 82, "22931931": 61, "22937": 82, "2293849": 113, "229443": 85, "229452": [97, 98, 99], "229472": 81, "2295": [63, 82], "229759": 98, "229795": 63, "2298": 53, "229961": [70, 71], "229994": [70, 71], "22m": 98, "23": [23, 43, 47, 54, 55, 56, 58, 60, 63, 70, 71, 78, 80, 81, 82, 85, 88, 89, 93, 95, 97, 98, 99, 113, 129, 141, 143, 144, 146], "230": [25, 53, 63, 144, 145], "230009": [74, 75], "230055": [99, 102], "2302": 87, "2307": [54, 80, 87, 92], "2308": 88, "230842": 70, "230956": 59, "230994": 60, "231": [7, 63, 145], "23113": 99, "231153": 71, "231172": 60, "231281": 66, "231310": 85, "231430": 129, "231467": 99, "231734": 99, "231790": 63, "231798": 99, "231986": 85, "231e": 86, "232": [60, 145], "232134": [70, 71], "232157": 71, "232415": 60, "23254142": 61, "2328": 82, "232868e": 71, "232959": [74, 75], "232985": 63, "232e": 99, "233": [13, 60, 145], "233029": 70, "233154": 146, "2335": 53, "233705": 71, "23391813": 61, "234": [144, 145], "234030": 60, "23404391": [99, 102], "234137": 89, "234153": 89, "234205": 82, "234431": 79, "234534": 72, "234605": 64, "234611": 62, "234735": 62, "234798": 82, "2348212": 113, "234910": 80, "235": [63, 144, 145], "235291": 70, "23549489": [99, 102], "235742": 62, "2359": 146, "23590": 82, "235945": 63, "236": 145, "236008": 72, "236015e": 70, "236309": 82, "236688": 63, "236792": 63, "236884": 79, "23690345e": 113, "236940": 60, "237": [56, 145], "237115": 71, "237200e": 70, "237252": 82, "237341": 70, "237461": 88, "23748": 82, "23751359e": 113, "237896": 85, "23789633": 85, "238": [54, 80, 145], "238101": 85, "238225": 129, "238239": 63, "238251": 72, "238529": 33, "23856": 82, "238619": 67, "238794": 85, "239": 145, "239019": 77, "239148": 63, "239243": 70, "239267": 79, "239313": 71, "239471": 66, "23965": 82, "239753": 63, "23e": 55, "24": [54, 55, 56, 60, 63, 70, 71, 78, 79, 80, 81, 82, 85, 88, 89, 90, 97, 98, 99, 113, 129, 143, 144, 145, 146], "240127": [70, 71], "240146": 71, "240295": 88, "2403": 87, "240532": [70, 71], "240556": 63, "240601": [99, 102], "2407": 53, "24080030a4d": 56, "240813": 76, "241": 145, "241049": 85, "241064": 71, "241183": 61, "241503": 97, "2416": 53, "241609": 82, "241645": 71, "241678": 70, "241745": 63, "241827": 71, "241962": 89, "24199": 82, "241e": 86, "242": [144, 145], "242000": 82, "242124": [81, 82], "242139": 129, "242158": [81, 82], "2422": 66, "2424": 77, "242427": 77, "2424596822": 75, "242643": 63, "242815": 129, "242902": 85, "242989": 62, "243": 145, "243056": 62, "2430561": 53, "243246": 85, "2438": 82, "2439": 82, "243e": 86, "244": [40, 82, 145], "244090": 82, "244455": 85, "244622": 129, "24469564": 143, "244915": 60, "245": [144, 145], "245027": 66, "245062": 85, "24506971": 63, "2451": 53, "24510393": 55, "245136": 63, "245370": 80, "245425": 60, "245512": 85, "245531": 70, "245569": 83, "245720": 59, "246": 145, "246093": 63, "246624": 97, "2467506": 54, "246753": 85, "246879": 85, "247": [86, 145], "247020": 72, "247057e": 85, "2471": 82, "247132": 62, "2472": 82, "247497": 60, "247617": 97, "247717": 82, "24774": [81, 82], "247826": 80, "247977": 70, "248171": 85, "248176": 60, "248441": 97, "248638": 72, "249": [54, 80, 83, 145], "2491": 82, "24917": 82, "249240": 60, "249541": 63, "249601": [99, 100, 104], "2499": [61, 94, 95], "249986": 79, "25": [9, 10, 14, 15, 16, 17, 25, 26, 36, 37, 54, 55, 56, 59, 60, 63, 66, 70, 71, 72, 73, 78, 79, 80, 81, 82, 85, 89, 90, 97, 98, 99, 113, 129, 143, 146], "250": [83, 145], "2500": [61, 82, 94, 95, 99, 104], "25000000000000006": [72, 82, 85], "250073": 82, "250210": 72, "2503": 82, "250341": [99, 102], "250354": 85, "250425": 72, "2506": 87, "251": [81, 82, 88], "251412": 71, "2514723": 113, "251480": 71, "251505": 60, "251953": 82, "252133": 82, "252253": 88, "252344": 63, "25240463": 99, "252524": 85, "252601": 129, "253026": [70, 71], "2532": 82, "253437": 84, "253596": 60, "253658": 60, "253662": 60, "253675": 81, "253724": 85, "25374": 82, "25375305": 63, "254": [82, 145], "25401679": 54, "254035": 79, "254038": 75, "254083": 71, "2543": 82, "254324": 72, "254400": 129, "254804": 60, "255": [82, 145], "255034e": 71, "255995": 70, "256": [82, 98], "256082": 97, "2562391": 113, "256290": 63, "256404": 60, "256416": 85, "256567": 80, "256600": 60, "25672": 82, "256944": 85, "256992": 82, "257019": 71, "257207": 54, "257377": 59, "2575": [99, 102], "257523": 70, "2580": 61, "258083": 71, "258158": [70, 71], "2583": 82, "25839958": 61, "25849743": 61, "258522": 70, "258541e": 37, "258659": 63, "258951": 85, "259164": 71, "259367": 97, "259395": 76, "2594": [55, 81], "25949512": 61, "25972892": 61, "259821": 63, "259828": [70, 71], "259875": 71, "25e": 113, "25x_3": 59, "26": [54, 55, 56, 58, 60, 63, 64, 66, 70, 71, 78, 80, 81, 82, 90, 93, 95, 97, 98, 99, 113, 129, 143, 146], "26016": 82, "260161": [35, 97], "260211": [70, 71], "260356": 81, "260360": 85, "260738": [99, 104], "260762": 67, "261": 86, "2610": 82, "2613": 82, "2615": 66, "261624": [81, 82], "261625374": 113, "261685": 82, "26175": 82, "261777": 82, "261903": 80, "2619317": 54, "262094": 60, "262323": 63, "262423e": 82, "262436": 60, "262621": 80, "262829": 113, "263": [7, 82, 145], "2633": 82, "263539": 63, "263581": 60, "263942e": 71, "263974e": 85, "264": [144, 145], "264086": 59, "264274e": 82, "264709": 63, "264847": 60, "264884": 82, "265": 145, "2651": 99, "265119": 84, "2652": [56, 81, 82], "2654974": 113, "265547": 82, "265669": 66, "265744": 79, "2658": 75, "265929": 86, "266": 145, "266147": 86, "266351": 60, "266686": 70, "266922": 129, "267": 83, "2670691": 54, "267359e": 63, "267453": 63, "267500": 80, "267581": 82, "26774380": 113, "267950": 85, "268": 145, "268055": 82, "268110": 63, "26828797": 63, "268343": 79, "268628e": 71, "268942": 85, "268943": 70, "268998": 55, "269043": 85, "269112": 99, "2694928": 113, "269977": 82, "26bd56a6": 56, "26e": 55, "27": [10, 14, 25, 26, 52, 54, 55, 56, 57, 58, 60, 63, 64, 66, 70, 71, 78, 80, 81, 82, 90, 93, 95, 97, 98, 99, 113, 129, 143, 144], "270": [51, 145], "2700": 56, "270248": [99, 104], "270644": [70, 71], "271": 145, "271004": [81, 82], "271083": 82, "271183": 81, "271239": 63, "271556": [99, 100], "271953": 60, "272296": 82, "272332e": 70, "272408": 71, "272610": [99, 102], "272662": 82, "272839": 63, "273": 56, "273299": 71, "273337": 61, "273356": 72, "273616": 60, "27371": [55, 81], "27372": [55, 81], "273933": 63, "274": [56, 82], "2740991": 53, "274251e": 81, "274267": 80, "27429763": [99, 101], "274573": 63, "27461519": [99, 104], "274793": 85, "274825": [36, 97], "27487": 82, "2754": 53, "275430": 63, "275596": 129, "276": [56, 145], "276148": 85, "276189e": 80, "2763": 61, "2764": 82, "276504": 63, "276505": 60, "2766091": 55, "27713": 82, "277179": 66, "277299": 64, "27751": 82, "277512": 71, "277561e": 80, "277599": 60, "277810": 63, "27794072": [99, 102], "277968": 85, "278": [88, 145], "2780": 54, "278000": 80, "278035": 67, "278391": 82, "278434": 74, "278454": 77, "2786": 129, "278724": 66, "278804": 71, "279": 145, "279101": 63, "279130": 60, "27951256e": 113, "279595": 67, "279824": 66, "27986": 82, "279933e": 71, "28": [54, 55, 56, 60, 62, 63, 70, 71, 73, 78, 80, 81, 90, 97, 98, 99, 113, 129, 143, 145, 146], "280196": 75, "280454dd": 56, "280514": 129, "280935": 63, "280963": 84, "281": [86, 145], "281024": 85, "281109e": 60, "28111364": 55, "281437": 60, "2815": 82, "281602": 60, "2818": 53, "2819": 129, "281907": 60, "282": [86, 144, 145], "282200": 75, "2825": [141, 143], "28251": 82, "282870": 82, "2830": [141, 143], "283041": 70, "283189": 60, "283207": 70, "2832466": 113, "28326": 82, "283386": 70, "2836": 53, "2836059": 54, "28382": 82, "2838546": [99, 102], "283974": 85, "283992": 70, "283994": 85, "283e": 86, "284": 145, "28425026": 88, "284271": 76, "284276": 60, "284397": 146, "28452": [55, 81], "284555": 63, "284574": 62, "2847": 61, "2849": 82, "284987": 82, "285": [86, 99, 145], "285001": 67, "285483": 77, "285642": 60, "285783": 66, "285928": 66, "285985": 60, "285e": 86, "286": 145, "286139": 62, "286203": 70, "286371": 70, "2865": [53, 82], "286507": 72, "286563e": 82, "286593": 82, "287": 145, "287041": 85, "287123": 97, "287153": 61, "287196": 70, "287815": 88, "287926": 85, "288": [83, 145], "288336": 63, "288976": 82, "289": [144, 145], "289062": 81, "289357": 71, "289440": [70, 71], "289555": 77, "29": [54, 55, 56, 60, 63, 70, 71, 78, 80, 81, 88, 90, 97, 98, 99, 113, 129, 143, 146], "290": 99, "290146e": 63, "290385": 60, "290565": 71, "290736e": 71, "290901": 67, "290987": 81, "290997": 60, "291": [82, 86, 145], "2910": 82, "291008": 70, "291011": [99, 101], "291071": 85, "29107127": 85, "291405": 85, "291406": 85, "291434": 71, "291500e": [81, 82], "291517": [70, 71], "29168951": [99, 104], "291963": 85, "292": [84, 145], "292028": 72, "292047": 129, "292105": 85, "292302995303554": 72, "292303": 72, "2925": 56, "2927": 82, "2927164": 113, "292997": 85, "29299726": 85, "293218": 85, "293617e": 82, "294": 145, "294067": [70, 71], "294123": 97, "294157": 60, "294449": 70, "294812": 63, "295": [144, 145], "295251": 63, "295307": 70, "295408": 60, "295481": 85, "29548121": 85, "295837": [64, 93, 95, 143], "2958370000000100000100": [56, 93, 95, 143], "2958370001000010011100": [56, 93, 95, 143], "2958371000000010010100": [56, 93, 95, 143], "296099": 67, "2962": 66, "296228": 82, "296729": 80, "29678199": [91, 114], "296901": 70, "297": 145, "297276": [99, 104], "297287": [70, 71], "2973": 82, "297349": [74, 75], "297682": 85, "297687": 82, "297749": 82, "29784405": 88, "297915": 60, "298": [13, 56], "298076": 70, "298120": 72, "298228e": 82, "298934": 61, "299": [56, 86], "29919": 63, "29920": 63, "299398": 66, "299537": 75, "299712": 74, "2999": 67, "29999": 60, "2_": [19, 57, 90, 130, 131, 140], "2_x": [19, 57, 90], "2d": [21, 114, 123], "2dx_5": [72, 85], "2e": [51, 53, 54, 55, 56, 57, 98, 99, 114, 129, 143], "2f": 76, "2m": [130, 136, 140], "2n_t": 59, "2x": 85, "2x_0": [11, 70, 71, 74, 75], "2x_4": 59, "3": [9, 10, 11, 12, 13, 14, 17, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 104, 105, 113, 114, 129, 130, 136, 141, 142, 143, 144, 145], "30": [11, 51, 52, 54, 56, 57, 58, 60, 62, 63, 66, 67, 68, 69, 70, 71, 72, 78, 80, 81, 82, 85, 86, 90, 97, 98, 99, 113, 129, 143, 146], "300": [52, 69, 72, 82, 85, 92, 144], "3000": 67, "30000": 60, "30000000000000004": [72, 82, 85], "300010": 63, "300031": 70, "30031116e": 113, "300318": 63, "300892e": 71, "30093956": 88, "301": 56, "301366": 129, "301371": 85, "3016": 81, "301737": 70, "30189": 82, "302149": 70, "30227357": [99, 102], "302357": 85, "302382": 79, "302648": 80, "302667": 63, "303007": 70, "303324": 80, "303489": 85, "303613": 85, "30361321": 85, "303721": 60, "30383": 82, "303835": 80, "303f00f0bd62": 56, "304130": 85, "304148": 63, "304159": 85, "304201": 59, "304282": 60, "30527": 82, "305341": 85, "305591": 63, "305605": 63, "305612": 80, "305775": 85, "305b": 56, "306297": 70, "30645": 82, "30672815": 54, "306915": 80, "306963": 85, "307181": 63, "307407": 85, "308": 82, "308568": 71, "308774": 70, "30917769": [74, 75], "30932115": 63, "309539": 79, "3096748": 113, "309772": 80, "309823e": 82, "30982972": 85, "309830": 85, "309901": 66, "31": [54, 55, 56, 57, 60, 62, 63, 66, 70, 71, 78, 80, 81, 82, 90, 97, 98, 99, 113, 129, 143, 146], "310000e": 82, "310145": 79, "310761": 84, "311": 86, "311253": 82, "311300": 60, "311321": 71, "311667": 71, "311712": 74, "311869": 97, "3120": 82, "31248689": 63, "312652": 86, "312915": 63, "313": 99, "313056": 129, "313209": 72, "31332": 63, "313324": 82, "31337878": 82, "313535": 85, "313571": 63, "313631732": 113, "31378": 56, "313870": 77, "314": 113, "3141": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 56, 57, 64, 80, 91, 93, 95, 97, 98, 99, 114, 129, 143], "314247": 89, "314341": 70, "3146": 37, "314625": 71, "314651": 74, "31476": [81, 82], "314828": 63, "315": [51, 145], "315003": 60, "315031": 89, "315036": 70, "3151": 82, "315155": 71, "315290": [74, 75], "315310": 70, "315769e": 70, "315798": 63, "3158103": 113, "316": [56, 145], "316193": 85, "31632": 82, "316407": 99, "316540": 80, "316717": [70, 71], "316826": 70, "316863": 71, "316989": 63, "317394": 59, "317487": 85, "317607": 85, "317946": 66, "318": [56, 145], "318000e": 82, "318438": 82, "318552": 82, "318584": 129, "318753": [74, 75], "319": [56, 83, 145], "319100": [74, 75], "31910229": [99, 104], "319420": 77, "319759": 85, "319850": 85, "31e": 113, "32": [39, 54, 55, 56, 60, 62, 63, 70, 71, 78, 79, 80, 81, 82, 90, 97, 98, 99, 101, 113, 114, 129, 143, 146], "320": 82, "320314": 81, "320633": 72, "320719": 60, "320895": 63, "321": 145, "321686": 129, "322186e": 70, "32236455588136": 58, "322404": 88, "322751": 71, "323319": 60, "32336944": 63, "3234": 82, "323636": 81, "32366503": [99, 102], "323679": 80, "324": [55, 82, 145], "324195": 60, "324266": 60, "32446951": 60, "324518": 84, "32458367": 54, "3245837": 85, "324998": 63, "325043": 63, "325056": 85, "325090": 82, "325370": 66, "325486": 70, "325599": 70, "325641": 62, "325677": 63, "326": 86, "326148": 71, "326172": 63, "326183": 60, "3264": 66, "326436": 63, "326721": 71, "326740": 85, "32674263": 62, "326871": 89, "3268714482135149": 89, "326909": 63, "327": 145, "327257": 70, "327803": 97, "327958": 67, "328471": 70, "328744": 63, "32875335": 62, "329210": 60, "329339": 58, "32950022e": 113, "329850": 66, "3299": 66, "33": [54, 55, 56, 60, 63, 70, 71, 74, 78, 79, 80, 81, 82, 90, 97, 98, 99, 113, 129, 143, 144], "330": 145, "3300": [55, 81], "330068": 70, "330100": 70, "330143": 85, "33014346": 85, "330163": 71, "330285": [70, 71], "330385": 63, "3304269": 54, "330615": 85, "330731": [36, 97], "331221": 60, "331365": 74, "331521": 85, "331602": 82, "33175566": 85, "331756": 85, "332502": 71, "332533": 83, "33267407": 63, "332782": [36, 97], "3329": 82, "332996": 80, "3333": [52, 54, 69, 97, 98, 99], "3333333": 56, "33333333": [60, 62, 63], "33335939e": 113, "3334": 66, "3335": 82, "333581": 81, "33365359": 113, "333655": 70, "333704": 71, "333886": 60, "333914": 66, "333955": 71, "334": [55, 83], "334408": 63, "334717": 66, "334750": 72, "33500": 82, "335176": 82, "335379": 60, "335446": 67, "335609e": 85, "335846": 85, "335853": 82, "336": 145, "33613": [99, 100], "336216": 60, "336346": 60, "336382": 71, "336461": 82, "336612": 59, "336870": 61, "336954": 63, "337260": 63, "337380": 85, "3376": 53, "337619": 58, "33763094": 60, "33789954": 60, "338": 88, "33849": 82, "3386": 113, "338603": 70, "338775": 72, "338846": 63, "338908": 72, "338960": 63, "339269": 88, "33928": 82, "339443": 71, "339570": 85, "339595": 63, "339779": 60, "339787": 61, "339875": [74, 75], "339912": 60, "34": [52, 53, 54, 55, 56, 57, 60, 62, 63, 70, 71, 75, 78, 80, 81, 82, 88, 90, 97, 98, 99, 113, 129, 146], "340": [55, 82], "340029": 71, "340142": 99, "340159": 60, "340217": 61, "340235": 77, "340274": 88, "3405076": 113, "341189770": 113, "341336": 34, "341472": 79, "341755e": 70, "341824": 63, "34188358": 113, "3420": 82, "342117": 77, "342158": 60, "342362": 67, "342632": 79, "342675": 54, "342698": 63, "342704": 60, "34287815": 88, "342989": 82, "342992": 80, "343": [82, 86, 145], "343639": 97, "343685": 70, "34375": 81, "343828": 70, "34395649": 63, "344174": 63, "344212": 146, "344440": 97, "34450402": 62, "344505": [81, 82], "344579": [99, 102], "344640": 85, "344787": [70, 71], "344834": 59, "345": 145, "345065e": 82, "345381": 72, "3453813031813522": 72, "3454": 82, "345750": 62, "345852": 71, "345903": 85, "345989": 70, "346016": 60, "346107": 81, "346206": 85, "346238": 88, "346269": 71, "346678": 84, "346964": 70, "347309": 60, "347310": [36, 97], "347696": 72, "34769649731686": 72, "347850848": 113, "347929": 82, "348": 86, "34829644": 61, "348319": 71, "34856881": 61, "34858240261807": 58, "348617": 85, "348622": 86, "348700": 71, "348980e": 71, "349213": 62, "3492131": 53, "349383": 80, "34943627": 78, "349638": 71, "34967621": 54, "349772": 75, "34983025": 61, "34m": 98, "34mglmnet": 98, "34mmlr3": 98, "34mmlr3learner": 98, "34mmlr3pipelin": 98, "34mranger": 98, "34mrpart": 98, "35": [55, 56, 60, 63, 70, 71, 72, 80, 81, 82, 85, 97, 98, 99, 113, 129, 130, 136, 146], "3500000000000001": [72, 82, 85], "350165": 98, "350208": 70, "350518": 85, "350712": [74, 75], "35077502": [130, 136], "35102327": 61, "351220": 71, "351629": 82, "351766": 84, "352": [55, 80], "352250e": 81, "352259e": 82, "3522697": 54, "35292": 82, "3529801": 113, "352990": 82, "352998": 82, "353105": 37, "353412": 85, "35341202": 85, "35365143": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "353748e": 85, "3538": 53, "354": 82, "354188": 59, "354371": 85, "354403": 63, "35444105": 63, "354501": 60, "354688": 38, "355013": 63, "355065": 67, "355083": 60, "355209": 85, "355627": 97, "355651": 71, "35595543": 61, "3559592": 61, "35596309": 61, "35596702": 61, "356136e": 82, "356167": 75, "356183": 82, "35620768e": 113, "3564": 82, "3565": 82, "356693": 63, "3568": 99, "357": 82, "357170": 70, "35731523": [99, 101], "357575e": 113, "357639": 63, "358": 51, "358158": [81, 146], "358289": 80, "358395": 88, "358594": 60, "358799": 129, "359": [86, 146], "359083": 66, "359100": 82, "359161e": 70, "359229": 67, "3593": 88, "359307": 71, "35th": 144, "36": [55, 56, 60, 62, 63, 66, 70, 71, 80, 81, 97, 98, 99, 113, 129], "360004": 85, "360065": 129, "360249": 76, "360475": [70, 71], "360572": 71, "360655": 82, "360683": 72, "360801": 72, "361": 86, "361374": [99, 102], "361518": 72, "361518457569366": 72, "361521": 38, "361623": 77, "3619201": 12, "362157": 70, "36231307e": 113, "362501779": 113, "362745": 63, "362754": 63, "363276": 54, "364197": 60, "364221": 70, "3643": 129, "364340": [99, 102], "364595": 54, "3647": 56, "364800": 85, "36496609": 113, "36501": 82, "365275": 66, "365551": 71, "36557195e": 113, "36566025e": 113, "366": 82, "366124": 60, "36616": 82, "3663": 66, "366310": 70, "366529": 84, "366718627": 54, "366950": 70, "36696349": [99, 104], "367": [40, 86], "367077e": 60, "367181": 70, "367323": 85, "367366": 77, "367398": 79, "367425": 66, "367571": 72, "367625": 85, "368": [60, 62, 63], "368152": 80, "3682": [55, 81, 82], "368324": 80, "368499": 72, "3684990272106954": 72, "368577": 97, "369": 113, "369189e": 63, "369556": 72, "3696": 88, "369796": 85, "369869": 81, "369981": 80, "36m": 98, "37": [55, 62, 66, 70, 71, 77, 80, 81, 82, 97, 98, 99, 113, 129], "370254e": 81, "3702770": 54, "370736": 80, "3707775": 54, "370908": 70, "3710": 82, "371357": [81, 82], "371429": 72, "371850e": 71, "37191227": [99, 102], "372": 144, "37200": [81, 82], "372097": 72, "3722": 82, "37231324": 90, "3724": 82, "372427": 71, "3727679": 54, "373218e": 79, "373451": 97, "373796": 63, "3738573": 54, "373871": 60, "374128": 60, "374350": 63, "374359": 63, "374364": 85, "37436439": 85, "374477": 63, "3745": 82, "374750": 63, "374821e": 82, "374862": 70, "374917e": 70, "375081": 82, "375110": 63, "375274": 70, "375457": 60, "375465": 85, "375621": 79, "375844": 79, "375952": 63, "376399": [99, 102], "3766": 77, "376617": 77, "376760": 71, "376806": 71, "377060": 82, "377147": 97, "377311": 85, "377669": 71, "378161": 61, "37830153": 63, "378351": 32, "378588": 70, "378596": 80, "378688": 85, "378727": 70, "378834": 85, "3788859": 54, "379": 144, "379038": 85, "37939": 82, "379614": 85, "379626": 70, "379981e": 71, "38": [51, 56, 70, 71, 81, 97, 98, 99, 113, 129], "3800694": 54, "380353": 60, "380837": [81, 82], "381": 86, "381072": 85, "381603": 70, "381684e": 81, "381685e": 82, "381689": 85, "3817": 82, "382169": 60, "382286": 82, "382582e": 31, "382872": 72, "383101": 63, "38328122": 62, "383297": 85, "383531": 79, "383852": 62, "384": 82, "384443": 71, "384677": 67, "38470495": [99, 104], "384777": 82, "384865": 71, "384928": 70, "3851": 82, "385119": 60, "385160": 71, "3852": 66, "385240": 129, "385917": 80, "386": [56, 82], "386102": 72, "386502": 82, "386831": 67, "386988": 58, "387": 56, "3871": 53, "387380": 62, "387426": 85, "387780": 85, "388026": 70, "388071": 85, "388145": 63, "388185": 67, "38818693": 113, "388216e": 98, "3882808": 113, "388668": 85, "38866808": 85, "388871": 82, "388959": 60, "389": 56, "389126": 99, "389164": 79, "389489": [99, 100], "389566": 84, "38973512e": 113, "389755": 70, "38990574": 113, "39": [51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 70, 71, 75, 76, 78, 80, 81, 82, 88, 89, 90, 97, 98, 99, 113, 129], "39010121e": 113, "390176": 63, "390304": [99, 102], "390379": 85, "390878": 60, "391377": 89, "391389": 66, "392128": 71, "392242": 76, "39236801": 78, "392400": 82, "39255938": [99, 102], "392623": 71, "392752": 58, "392833": 88, "392864e": [81, 82], "392917": 70, "393019": 60, "393604": 72, "393654": 67, "394014": 63, "394226": 71, "39425708": 54, "3949834": 113, "395076e": 82, "395136": 80, "395268": 97, "395480": 66, "395569": 70, "395603": 70, "395660": 63, "395729": 62, "3958": 99, "395889": 82, "396": [40, 99], "39611477": 55, "396173": 74, "39621961e": 113, "396300": 74, "3964": 82, "396531": 82, "396985": 80, "396992": [70, 71], "397140": 72, "397155": 71, "39727": 82, "397313": 53, "397536": 79, "397578": 76, "397811": 88, "397842": 60, "3979": 62, "398": [93, 95, 143], "39812337": 113, "398166": 67, "3985": 82, "398770": 85, "39878301": 62, "398999": 97, "399": 55, "399056": 85, "399223": 59, "399239": 60, "399343e": 70, "399355": 59, "399470": 66, "399679": 99, "399692": 85, "399858": 89, "39m": 98, "3cd0": 56, "3dx_1": [72, 85], "3e1c": 56, "3ec2": 56, "3f5d93": 83, "3x_": 85, "3x_4": [72, 85], "4": [10, 14, 19, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 130, 136, 143, 145], "40": [54, 57, 58, 70, 71, 72, 79, 81, 82, 83, 85, 90, 93, 95, 97, 98, 99, 113, 129, 130, 136], "400": 80, "4000": [60, 63], "4000000000000001": 98, "40000000000000013": [72, 82, 85], "400113": 77, "400288": 63, "40029364": [130, 136], "4004003": 113, "400823": 85, "400855956463958": 72, "400856": 72, "400905": 67, "401": [7, 146], "401247": [114, 129], "40127723e": 113, "401649": 63, "401677": 60, "401690e": 71, "40186344": 62, "401931": [74, 75], "402077": 82, "402113": 129, "402301e": 98, "402619": 39, "402902": 82, "403": [86, 146], "40334723": 62, "403425": 85, "40349321": 62, "403626490670169": 91, "4036264906701690": 91, "403626491": 91, "403715": 31, "403771948": 114, "4039": 53, "404267": 70, "404300": 67, "404318": 53, "404411": 70, "404467": 63, "40452": 82, "404550": 84, "405050": 71, "405203": 59, "405374": 82, "40538017": 62, "405677": 66, "40583": 53, "405890": [36, 97], "406285": 85, "406353": 83, "406446": 72, "4065173": 113, "40676": 53, "407": 86, "40732": 77, "407558": 70, "407565": 70, "407768": 60, "407877": 61, "408476": [130, 136], "40847623": [130, 136], "408479": 80, "408509": 71, "408539": 85, "408565": 85, "40875847": 62, "408956": 60, "409154": 53, "4093": 88, "409328": 82, "409395": 85, "409746": 72, "409848": [70, 71], "41": [70, 71, 81, 82, 97, 98, 99, 113, 129], "410100": 70, "410393": 72, "410667": 97, "410681": 59, "410682": 70, "4107": 66, "410795": 80, "41093655": 113, "411146e": 71, "411190": [70, 71], "411264": 66, "411291": 84, "411295": 85, "411304": [70, 71], "411447": 82, "411582": 85, "411768": 71, "411977": 66, "412004": 74, "412127": 85, "412304": 89, "41237738": 62, "412466": 63, "412477": 59, "412653": 80, "412714": 72, "412726": 71, "412941e": 71, "413247e": 70, "41336": 98, "413376": 99, "41341040": 54, "413608": 85, "413807": 62, "414": 86, "414073": 33, "41428195": [99, 102], "414533": 71, "415079e": 113, "41525168e": 113, "415375": 70, "415556": 97, "41566": 99, "415812": 146, "415988": 82, "416052": 67, "416132": 71, "41628398": 62, "4166": 82, "4166667": 56, "416757": 85, "416899": 70, "416919": 71, "41698441": 62, "416e": 86, "417502": 66, "417640": 70, "417727": 81, "417736": 79, "417767": [74, 75], "417834": 67, "41798768e": 113, "418": 40, "418056": 85, "41805621": 85, "418400": 77, "41859445": 62, "418741": 67, "418806e": 72, "418969": 97, "41918406e": 113, "419270": 63, "419371": 85, "419871": 67, "41989983e": 113, "4199952": 54, "41e5": 56, "42": [14, 20, 23, 24, 57, 58, 59, 62, 66, 67, 69, 70, 71, 72, 74, 75, 76, 77, 78, 81, 82, 84, 85, 88, 89, 90, 94, 95, 97, 98, 99, 100, 101, 102, 104, 113, 129, 144], "4200": 82, "420316e": 82, "420669": 63, "42068824": 63, "42073312": 54, "42094064": [99, 104], "420967": 72, "421083": 53, "4211349413": 54, "421163": 70, "421200": 89, "421297e": 71, "421357": [74, 75], "421576e": 82, "421735": 63, "421793": 88, "421919": 82, "422007": 88, "422266": 82, "422293e": 97, "422325": 72, "422591": 71, "422835": 60, "423249": 63, "42338": 82, "4235839": [74, 75], "42385": 62, "42388745": 90, "423921e": 97, "423951": 53, "423992": 63, "4240627": 62, "424108": 72, "424127": 113, "42412729": 54, "424292": 70, "424328": 85, "424599": 60, "424651": 99, "424717": 72, "424748": 89, "4249571": 62, "425": 80, "42507964": 62, "425103": 53, "4251501": 113, "4252": 66, "425208": 82, "425325": 77, "425493": 53, "42550": 82, "425636": 77, "426055": 53, "4260942": 62, "426540": 80, "426540301": 54, "426736": 82, "42699342": 62, "427": 82, "42708237": 63, "427486": [70, 71], "42755087": 88, "427551": 88, "427573": 80, "427654": 77, "427725": 85, "427744": 63, "428": [129, 146], "428046": 84, "42811700": 146, "428255": 85, "428411": [81, 82], "428467": 85, "4284675": 85, "428771": [36, 97], "4290": 53, "429057": 71, "4291568": 113, "429230": 70, "429705": 70, "42ba": 56, "43": [55, 62, 67, 70, 71, 83, 97, 98, 99, 113, 129], "430298e": [81, 82], "430465": [99, 104], "430595": 71, "430608": 70, "431061e": 70, "4311947070055128": 98, "431253": 79, "431306": 85, "431437": 88, "431701914": 141, "431998": 67, "4320": 66, "432130e": 81, "432300e": 85, "43231359e": 113, "432454e": 63, "432707": 86, "43294": 56, "432f": 56, "433": [56, 86], "433221": 72, "433229": 63, "433429": 63, "4336": 82, "433630": [99, 102], "43374433": 90, "433750": 70, "433753": 79, "4339": 53, "434054e": 77, "434121": 79, "434207636": 113, "434242": 60, "434535": 85, "43453524": 85, "4347": 66, "435": 56, "43503345": 99, "43511": 82, "4352": 66, "435401": 80, "4357": 82, "435927": 82, "435967": 80, "43597565": 85, "435976": 85, "436": [56, 82], "436016": 79, "43627032": 58, "436327": 82, "436394": 79, "4364": 66, "436764": 86, "436806": 82, "436817": 79, "437635": 60, "437667": 81, "437924": 82, "438": 80, "438219": 85, "438289": 82, "4383819": 113, "438569": 82, "438578e": 82, "43883": 75, "438834": 71, "4389": 82, "438959": 63, "438960": 80, "439401e": 71, "439535": 63, "439541": [81, 82], "439675": 86, "439699": 67, "43989": 97, "439958": 79, "43e": 113, "43f0": 56, "44": [58, 62, 67, 70, 71, 97, 98, 99, 101, 113, 129], "440320": 82, "440605": 98, "440747": 70, "440a": 56, "4410591": 113, "441153": 85, "441209": 85, "441219": 74, "44124313": 99, "441282": 70, "4416552": 54, "441849": 70, "442661": 63, "442847": [99, 100], "443016": 72, "443032": 81, "44312177": 55, "44326741": 62, "443672": [99, 104], "443686": 85, "4437": 82, "443e": 86, "444046": 82, "4444": [52, 54, 69, 99], "444500": [81, 82], "444850": 82, "4449272": 82, "444939": 63, "444988e": 63, "445": 86, "4451214": 113, "445309": 60, "445476": 70, "44563945e": 113, "4460": 66, "4461928741399595": 72, "446193": 72, "4462": [56, 66], "446411": 62, "446452": 63, "44647451": 88, "446979": 62, "44713577e": 113, "447492": 82, "447494": 61, "447624": [70, 71], "447706": 72, "447849": 58, "448": 82, "448252": 71, "448456e": 71, "448569": 70, "448587": 72, "448745": 85, "448842": 71, "4489": 82, "44890536": 113, "448923": 76, "449107": 23, "449150": [36, 97], "44950": 82, "449677": 77, "4499": 61, "44fa97767be8": 56, "45": [70, 71, 72, 74, 76, 79, 82, 85, 97, 98, 99, 113, 129], "4500": 81, "45000000000000007": [72, 82, 85, 98], "450031": 79, "450152": 80, "4505648": [99, 102], "450812e": 71, "450870601": 54, "4509": 66, "4510": 66, "451312e": 70, "452": 56, "452091": 82, "452114": 97, "452219": 63, "452488701": 54, "452489": 80, "452623": 71, "453": 56, "453279": 70, "4535": 82, "4539": 56, "454081": 82, "454188": 60, "454397": 85, "454406": 67, "454489": 83, "45467447": 113, "454712": 63, "455": 56, "45500": 82, "455078": 72, "455091": 71, "455107": 72, "455120": 85, "4552": 56, "455293": 72, "4552b8af": 56, "455448": 88, "455672": 82, "45594267": [99, 102], "455981": 130, "456370": 80, "456458e": 70, "456594": 60, "4566031": 129, "45660310": 129, "4567": 88, "456892": 72, "456972": 83, "457088": 85, "457667": 82, "45802972": 62, "458114": 82, "458196": [99, 102], "458307": 131, "458420": 82, "4584447": 54, "458784": 70, "458814": 77, "458855": 55, "4592": 54, "459200": 80, "459383": 72, "459418": 71, "459436": 79, "45957837": 113, "459760": 82, "459812": 72, "46": [62, 66, 70, 71, 76, 78, 97, 98, 99, 113, 129], "460": 82, "4601": 82, "460207": [70, 71], "460218": 72, "460289": 85, "460593": 63, "460744": 81, "4610": 146, "461227e": 70, "461412": 97, "461469": 61, "461629": 89, "46170477": 63, "462451": 72, "462567": 71, "462979": 70, "4631684": 113, "46328686": [99, 102], "463325": 85, "4634": 82, "463418": 89, "463483": 61, "463668": 82, "463766": 75, "463816": [99, 104], "463857": 82, "463903": 71, "463b": 56, "464": 99, "464076": 72, "4642": 66, "464284": 80, "464479": 63, "46448227": 113, "4645": 66, "464599": 60, "464668": 34, "465": 67, "46507214": 88, "465424": 79, "465476": 66, "465480": 63, "465649": 89, "465730": 89, "4659651": 91, "465965114589023": 91, "4659651145890230": 91, "4660": 66, "466047": 85, "46618738": 113, "466381": 63, "466440": 72, "466756": 85, "467": 82, "46709481": 113, "46722576e": 113, "467464": 63, "467613": 80, "467613401": 54, "467681": [70, 71], "4677055": 113, "467770": 72, "468072": 71, "468075": 85, "46807543": 85, "46811985": 85, "468120": 85, "468406": 82, "468449": [99, 104], "468552": 63, "468907": 67, "468919": 82, "468d": 56, "469": 56, "4690558": 63, "469474": 89, "469676": 67, "469825": 72, "469895": 71, "469905": 71, "47": [55, 58, 66, 67, 70, 71, 81, 88, 97, 98, 99, 113, 129, 145], "470055": 71, "470904": 70, "471": 67, "47121920242931404856616466717786879495": 113, "471246": 62, "471435": 86, "4716": 66, "471622": 70, "472": 82, "47222159": 90, "472255": 82, "472699": 71, "472891": 85, "472e": 56, "473099": 72, "47319": 99, "474062": 62, "47419634": 143, "474214": [74, 75], "474731": 97, "475304": 82, "475517": 79, "475569": 70, "475e": 86, "476121": [99, 102], "476289": 60, "476304": [99, 102], "4767101": 63, "476856": 72, "477": 51, "477130": [70, 71], "477150": 85, "477247": 71, "477357": 86, "477425": 63, "477474": 80, "47759584": 113, "4776": 66, "47761563": 58, "478032": 82, "478059": 71, "478064": 71, "4781": 82, "47857478": 113, "4788": 66, "479337": 60, "479522": 63, "479655": 79, "47966100e": 113, "479722": 71, "479860": 82, "479876": [74, 75], "479882": 71, "479928": 85, "479959": [99, 104], "47be": 56, "48": [56, 67, 70, 71, 75, 81, 82, 83, 97, 98, 99, 113, 129], "480": 67, "480133e": 85, "480199": 77, "48029755": 88, "480579": 71, "48069071": [91, 114, 129], "480691": [114, 129], "480800e": 85, "48103145": 63, "481172": 85, "481218": 82, "481399": [81, 82], "481705": 99, "481713": 77, "481761e": 82, "482": [56, 67, 83], "482012": 74, "482038": 72, "482075": 60, "48208358": 85, "482084": 85, "482179": 70, "4822": 66, "482251": 39, "482461": [130, 136], "48246134": [130, 136], "482483": 85, "482616": 79, "482649": 63, "482677": 62, "482790": 59, "482898e": 71, "48296": 88, "483": [86, 99], "48315": 88, "483186": 59, "483192": [81, 82], "48331": 88, "4835": 82, "483711": 85, "483717": 72, "48390784": 99, "48404": 54, "484303": 71, "4845": 82, "484640": 85, "48489102": [99, 102], "4849": [56, 66], "485": [56, 82], "485197": 70, "485378829": 113, "48550": 89, "485617": [81, 82], "485812e": 82, "48583": [81, 82], "485871": 75, "486": [15, 82], "4860": 66, "486178e": 70, "486202": 72, "486532": 85, "48661": 82, "486847": 60, "486877": 63, "487": [67, 82], "487352": 61, "487467": 82, "487524": 77, "487641e": 85, "487793": 71, "487872": 68, "488150": 63, "488394": 70, "488460": 82, "488485": 82, "48873663": 58, "488811": 85, "488909": [81, 82], "488982e": 72, "489488": [99, 104], "4895498": 85, "489550": 85, "489567": 87, "489699": 72, "489951": 71, "49": [56, 67, 70, 71, 97, 98, 113, 129], "490000e": 82, "490070931": 54, "490488e": 81, "490504e": 82, "490700": 85, "490896": 67, "490941": 82, "49098": 88, "491034": 70, "491073": 62, "491153": 62, "491245": 80, "49135": 37, "4915707": [99, 101], "492": 82, "4923156": 91, "49231564722955": 91, "492315647229550": 91, "492417e": 99, "492637": 76, "492656": 71, "49270769e": 113, "493": [86, 99, 144], "49302": 60, "493102e": 79, "493144": 89, "493195": 77, "493219": 85, "493313": 82, "493325": 20, "493426": 86, "493665e": 60, "493816": 63, "494": 86, "494089": 71, "494129": 85, "4943": 63, "494324": 80, "494324401": 54, "4949": 63, "495": 84, "495108": 79, "495135": 60, "49530782": 54, "495657": 72, "495752": 85, "49596416e": 113, "496": 84, "496209": 83, "49650883": 88, "496551": 85, "496591": [99, 104], "496714": 89, "496777": 146, "49693": 98, "497": [51, 84], "497168": 79, "497193": 83, "497422": 71, "4976": 66, "497655": 23, "497674": 58, "497964": 97, "498": 84, "498122": 77, "498286": 79, "498921": 85, "498979": 82, "498992": 70, "498f": 56, "499": [82, 84, 93, 95, 143], "499000e": [81, 82], "4994529": 113, "499464": 63, "4996": 63, "499776": 82, "49d4": 56, "4a53": 56, "4b8f": 56, "4dba": 56, "4dd2": 56, "4e": [54, 55], "4ecd": 56, "4fee": 56, "4x": 85, "4x_0": [11, 70, 71, 74, 75], "4x_1": [11, 70, 71], "5": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 104, 113, 114, 122, 129, 130, 136, 142, 143, 145], "50": [36, 54, 56, 57, 59, 63, 66, 67, 72, 75, 78, 79, 81, 82, 83, 85, 97, 98, 113, 129], "500": [6, 9, 10, 12, 13, 17, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 52, 56, 60, 61, 62, 63, 64, 69, 70, 71, 74, 75, 78, 81, 84, 86, 88, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 104, 114, 129, 130, 136, 143, 146], "5000": [41, 60, 61, 62, 63, 70, 71, 72, 85, 87], "50000": 80, "500000": [81, 82], "5000000000000001": [72, 82, 85], "500084": 85, "500267": 76, "5003517412": 54, "500458": 60, "500517": 85, "50093148e": 113, "501021": 82, "501047e": 70, "5011": 63, "501765": 63, "501983": 85, "502005": 97, "502016": 79, "502084": 99, "5021": 63, "502205": 70, "502494": 72, "5025850": 54, "502595": 71, "502612": 85, "502807": 83, "502901": 71, "502995": 85, "503": 40, "503109": 63, "503374": 79, "503504": 98, "503511": 82, "503700": 67, "503791": 83, "50398782e": 113, "504": 113, "504286": 80, "5042861": 54, "504548e": 71, "5050973": 54, "505264": 70, "505353": 71, "505795": [99, 102], "506050": 70, "506136": 60, "506644": 70, "506659": 82, "506687": 82, "50672034": 54, "506900e": 85, "506903": 72, "507": 86, "507285": 77, "507315": 63, "50768b": 83, "508047": 63, "508153": 84, "508433": 70, "508459": 80, "5085": 82, "508947": [99, 101], "509059": 82, "509196": 85, "509461": 85, "50967": 89, "5097": 89, "5098": [64, 93, 95, 143], "509853": 85, "5099": [56, 64, 93, 95, 143], "509951": 72, "509958": 80, "51": [53, 55, 56, 62, 67, 74, 79, 97, 98, 113, 129, 145], "510000e": [81, 82], "5101": 66, "510121": 70, "510385": 80, "510555": 67, "51079110": 54, "511257": 79, "511515": 82, "511540": 82, "5115547": 91, "5115547181877": 91, "51155471818770": 91, "511665": 70, "511668": 89, "5116683753999616": 89, "5118": 66, "511862": 85, "512": 80, "512108": 85, "512149": 85, "51214922": 85, "51243406e": 113, "512519": 80, "512572": 85, "512672": [99, 130, 136], "5131": 81, "513222": 77, "5135": 66, "513624": 97, "513992": 85, "514": 56, "514173": 71, "514192": 60, "514213436394143455152627475768082849092": 113, "51421343639414345515262747576808284909268913152223263755576567697273838588100471219202429314048566164667177868794951231028303844536063787981919396979899": 113, "514545": 82, "51494845": [99, 104], "515031": 70, "515338e": 70, "515358": 72, "5154": 82, "5154789948092002": 80, "5155": 56, "515672": 71, "516": 56, "516125": 72, "516222": 85, "516242": 71, "516255": 85, "516256": 85, "516528": 85, "5166": 66, "516797": 71, "517": [51, 56, 80], "517279": 71, "5175": 82, "517753": 70, "517798": 67, "517948": 63, "518175": 80, "518375": 71, "518446": 82, "518478": 67, "518610": 99, "518613": 63, "518782": 82, "518846": 80, "518854": 67, "51966955": 54, "519710": 85, "52": [53, 56, 76, 79, 86, 97, 98, 113, 129], "520": 82, "520415": 70, "520641": 88, "520805": 63, "520930": 72, "521002": 72, "521162": 63, "521233": 67, "5213": 66, "521611": 71, "521632": 70, "521788": 70, "522753": 38, "522835": 59, "523030": 89, "523163": 72, "5232": 78, "52343523e": 113, "523483": 63, "523794e": 85, "523807": 84, "523977545": 54, "52424539": 54, "524657": 85, "524934": [70, 71], "5250": 82, "525064": 67, "52510803": 55, "5251546891842586": 89, "5255": 56, "525722": 70, "52590": [55, 81], "526": 80, "526532": 82, "526582": 77, "526769": [70, 71], "526984": 71, "527226": 70, "52732": 98, "527452": 71, "527540": 70, "5277": 66, "528000e": 88, "528046": 63, "528381e": 90, "528573": 66, "528580": 85, "528763": 71, "528937": [74, 75], "528996901": 54, "528997": 80, "529": 80, "529405": 53, "529468": 97, "529782": 53, "53": [53, 56, 67, 76, 93, 95, 97, 98, 99, 101, 113, 129, 141, 144], "530201": 83, "530659": 76, "530793": 70, "530940": 85, "53094017": 85, "531": 56, "531057": 60, "531098": 60, "531176": 60, "531223": 72, "531594": 82, "531682": 66, "53209683": 99, "532266": 72, "53257": 98, "532738": 85, "53273833": 85, "532751": 74, "5329": 82, "5329569": 113, "533": 86, "533283": 99, "533305": 63, "533489": 59, "533900": 85, "534139": 67, "5346": 56, "535179": 85, "535318": 85, "535356": 60, "535609": 82, "535718e": 82, "53606675": 85, "536067": 85, "536143": 82, "536746": 85, "536778e": 71, "536798e": [81, 82], "537240": 85, "53724023": 85, "537438": [99, 100], "53791422": 99, "538": 56, "538013": 82, "538105": 71, "5382": 88, "538410": 66, "538910": 63, "538937": [81, 82], "539455": 85, "539475": 85, "53947541": 85, "539491": [74, 75], "539767": 72, "54": [53, 55, 56, 58, 66, 86, 92, 97, 98, 113, 129, 145], "54014493": [99, 102], "540240": 82, "540375": 77, "5405": 77, "540549": 77, "5408": 53, "541": 86, "541060": 77, "541159": 85, "541485": 63, "54163": 88, "541660": 60, "5416844": 91, "541684435562712": 91, "541821": 82, "541990": 82, "542": 86, "542136": 70, "542159": 71, "542170": 71, "5422049": 113, "542268": [99, 100, 104], "542333": 82, "5424": 66, "542446": 75, "542451": 85, "542493": 63, "542560": 89, "542584": 72, "5425843074324594": 72, "542647": 85, "542648": 97, "542671": 80, "542883": [130, 136], "5428834": [130, 136], "542919": 97, "542989": 85, "543": [51, 80, 82], "543052": 67, "543075": 72, "543136": 72, "543380": 80, "5434231": 91, "543423145188043": 91, "5436005": 54, "543680": 60, "543691": 71, "54373574": [99, 102], "543764": 75, "54378": 88, "543832": 85, "544097": 85, "544383": 89, "544555": 80, "544669": 74, "544807": 63, "54483": [114, 129], "5448331": [114, 129], "54517706e": 113, "545492": 67, "54550506": 86, "545605e": 85, "545919": 82, "545930": 97, "546294": 82, "5467606094959261": 72, "546761": 72, "546928": [99, 102], "546953": 71, "547039": 70, "54716": 88, "547287": 62, "547324": 71, "547431": 84, "547482": 66, "5476": 82, "5479": 82, "547909": 82, "548705": 63, "549109e": 82, "549412": 60, "55": [55, 56, 72, 81, 82, 85, 97, 98, 113, 129], "5500000000000002": [72, 82, 85], "550242": 79, "551317": 71, "551371": 60, "551548": 62, "551586928482123": 72, "551587": 72, "551592": 63, "551686": 72, "55176": 98, "5518": 82, "552": 82, "552508": 82, "552694e": 70, "552727": 80, "552776": 85, "553004": 67, "55307": 98, "553522": 71, "553754": 97, "553878": [35, 97], "553916": 82, "554076": 72, "554211": 63, "554213": 63, "5544671": 113, "554793e": 113, "554937": [99, 102], "555": 80, "5550": 66, "555137": 71, "555150": 82, "555231": 63, "555445": 84, "555479": 60, "555498": 85, "5555": [52, 69], "555536": 70, "555949e": 82, "555954": 82, "556191": [70, 71], "556344": 60, "55642562": [99, 102], "556530": 63, "556792": 85, "5574dcd4": 56, "557595": 80, "557731": 84, "557999": 80, "558134": [70, 71], "55820564": [99, 102], "558296": 62, "5584": 80, "5585": 80, "55863386": 99, "558655": 72, "5589": 80, "559": 146, "5590": 80, "559144": 72, "559186": 72, "5592": 80, "559394": 85, "559522": 85, "559592e": 70, "559680": 82, "55dc37e31fb1": 56, "55e": 55, "56": [56, 66, 92, 97, 98, 113, 129, 141, 144], "560135": [114, 129], "56018481": 85, "560185": 85, "5602727": 58, "560440": 66, "560530": 71, "560689": 53, "560723": 76, "561348": 71, "5614534": [99, 102], "5616": 81, "561674e": 63, "561711": 82, "561785": [99, 101], "561883": 39, "562013": 85, "56223": 88, "562288": 89, "562390": 97, "562452": 79, "562518": 82, "562556": 62, "5625561": 53, "562712": [70, 71], "563067": 86, "5631": 61, "563374e": 72, "563503": 85, "563528": 82, "563563": 77, "563673": 82, "56387280e": 113, "56390147e": 113, "564": 51, "56403823": [99, 102], "564045": 85, "564073": 82, "5641": 82, "564142": 72, "564232": [70, 71], "56425415": [99, 102], "5644": 66, "564451": 71, "564577": 82, "564619": 63, "565": 99, "5650": 66, "565066": 72, "565163": 60, "565373": 70, "565823": 63, "566": 89, "566024": 85, "566091": 82, "56611407": [99, 102], "566261": 63, "566388": 70, "567004": 88, "567215": 79, "567343": 82, "567364": 71, "567529": 85, "567695": 70, "567945": [74, 75], "568287": 77, "56876517": [99, 102], "568932": [99, 104], "5691743": 113, "569315e": 71, "569449": 97, "569540": 71, "569590": 79, "56965663": 85, "569657": 85, "569911": 54, "5699994715": 54, "57": [56, 81, 86, 97, 98, 113, 129, 146], "570038": 72, "5700384030890744": 72, "570111": 84, "57015458": [99, 102], "5702": [66, 82], "570486": 53, "570562": 53, "570722": 143, "570936": 70, "571778": 53, "5718": 82, "5722": 81, "572408e": 71, "57245066": 85, "572451": 85, "572717": 87, "572730": 66, "572991": 71, "573": 83, "5732": 66, "573689": 66, "573700": 59, "573937": 63, "574": 56, "574160": 77, "5748": 98, "57496671": 54, "575": 8, "575329": 66, "575381": 81, "57572422": 88, "575810": 70, "57585824": 88, "57592948e": 113, "57599221": 88, "575e": 86, "576": 56, "576102": 63, "576301": 63, "576303": 63, "5763996": 54, "57643609": 88, "577": 56, "5770": 81, "57715074": 54, "577271": 80, "577273": 70, "5776971": 88, "57775704": 88, "577807": [70, 71], "577813": 70, "577e": 86, "578081": 82, "578307": 85, "57843836": [99, 102], "578523": 80, "578557": 71, "578846e": 72, "579125": 81, "57914935": 55, "579197": 77, "579213": 89, "579238": 72, "579322e": 81, "579782": 63, "579875e": 70, "57e": 55, "58": [9, 55, 81, 89, 97, 98, 113, 129, 145], "5800": 82, "58000": 81, "5804": 56, "580414": 89, "580751": 81, "58081505": 113, "580853": 70, "580922": 74, "581655": 82, "581739": 62, "581827": 79, "581849": 71, "582031": 81, "582146": 71, "58241568": 113, "5825085": [99, 102], "582754": 87, "582761": 72, "582991": 81, "583034": 74, "583089": 63, "583195": [70, 71], "583201": 71, "5833333": 56, "583534": 85, "583692": 79, "5840": 66, "584012": 82, "584057e": 70, "584742": 75, "584849": 72, "584928": 70, "584942e": 80, "584944": 60, "5852": 82, "585394": [99, 100], "585426": 113, "585479": 77, "585684e": 60, "585793": 72, "586362": 85, "5864": 53, "5866": 82, "586719": 72, "586719493648897": 72, "586794": 70, "5868472": 54, "586921": 79, "587135": 71, "587292": 82, "587801": 66, "588": 82, "58812": 98, "5882": 81, "588233": 70, "588364": 97, "588525e": 60, "588854": 70, "588916": 60, "589147e": 79, "589440": 72, "589810": 63, "589940": 63, "589958": 71, "59": [60, 71, 97, 98, 99, 113, 129], "590320": 59, "5905": 81, "590530": [99, 102], "590736": 85, "590813": 85, "590904": 71, "590911": 72, "590991": 72, "591023": 60, "591080": 59, "591411": 74, "591441": 31, "591652": 81, "591678": 81, "591782": 85, "59199423e": 113, "592186": 71, "592681e": 72, "59300411": [99, 102], "59307502e": 113, "5931003": [99, 102], "593648": 98, "593981": 97, "594": 8, "594316e": 85, "594317": 66, "59518886": 63, "595353": 72, "59549877": 63, "59563003": [99, 104], "595829": 63, "596": 82, "596069e": 82, "596263": [60, 61], "596270": [74, 75], "5964": 78, "596460": 71, "596758": 70, "597": 55, "597098": 82, "5979": 66, "597923": 82, "598178": 82, "59827652": [99, 102], "59849123": 63, "59854797": 99, "5985730": 55, "59861": 82, "598761e": 71, "598865": 60, "599297": [97, 98, 99], "599334": [99, 104], "5999758": 63, "5cb31a99b9cc": 56, "5d": [72, 85], "5x_2": 59, "5x_3": 59, "5z_i": 85, "6": [8, 9, 10, 14, 15, 20, 21, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 93, 95, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 130, 136, 141, 143, 144, 145], "60": [54, 57, 58, 66, 72, 82, 83, 85, 90, 97, 98, 113, 129, 144], "600": 80, "6000": 82, "6000000000000002": [72, 82, 85], "600000e": 82, "600195": 70, "600254": 84, "600694": 99, "600776": 67, "601": 55, "601045": 60, "601061": 72, "60113395": [99, 102], "601514": 63, "601598": 80, "6017": 113, "601783e": 71, "601984": 71, "60206662": 63, "602079": 75, "602168": 72, "6022038": 113, "602322": 97, "602386e": 70, "602492": 70, "602587": 85, "602628": 72, "6029": 82, "603048e": 60, "603349": 63, "604016": 82, "604111": 82, "60458433": [99, 102], "604603": 23, "60481177": [99, 102], "604825": 82, "604841": [81, 82], "60488374": [99, 102], "605": [82, 83], "605195": 75, "60528379": 61, "605913": 83, "605932e": 63, "606034": 85, "606129": 85, "606342": 72, "606345": 63, "606529": 63, "60657873": 61, "606759": 82, "6068": 54, "606800": 80, "60688864": 61, "606954": 72, "607213": 60, "607264": 70, "6075": 146, "60753412": [99, 102], "607600": 82, "607900e": 71, "60795263": [99, 102], "607962e": 113, "608": [73, 86], "60814216": 61, "608219": 63, "608338": 63, "608392": 85, "60857": 53, "608818": 88, "60883428": 61, "60884363": 61, "60885093": 61, "60886858": 61, "609": 86, "609504": 63, "609522": 79, "609575": [99, 101], "61": [60, 67, 97, 98, 113, 129, 145], "610318": 67, "611": [83, 146], "6110": 82, "611269": 80, "61170069": 86, "611722": 60, "611859": 75, "612": 86, "612246": 77, "612792": 82, "612831": 60, "613244": 71, "6133": 55, "613314": 72, "613408": 85, "613498": 82, "613574": 76, "613622": 71, "613691": [99, 101], "614": 86, "61404894": 99, "614188": 80, "614323": 63, "6144": 66, "614678": 82, "615": 67, "615498": 31, "615863": [74, 75], "616224": 63, "616372": 81, "616464": 63, "616511": 60, "616617": 71, "61669761": [130, 136], "616698": [130, 136], "616828": 82, "617": 80, "61728": 97, "617283": 82, "6173": 56, "61771229": 86, "617877": 85, "618069": 81, "61810738": 55, "618574": 71, "618776": 58, "618804": 63, "618881": 71, "619": 86, "619128": 71, "619294": 67, "619351": [70, 71], "619390": [70, 71], "619454": 59, "619613": 81, "619869": 61, "619900": 60, "619903": 71, "61e": [55, 146], "62": [31, 67, 75, 76, 97, 98, 113, 129], "620156": 85, "620270": 63, "620874e": 99, "620995": 90, "62108072": [99, 102], "621094": [81, 82], "621318": 85, "62131806": 85, "621359": 97, "621490": 85, "6215": 81, "622": [82, 86], "622153": 82, "622272": 67, "6224": 54, "622943": 63, "623024": 72, "623084": 60, "623173": 70, "624": 80, "6240": 88, "62403053": 58, "6243811": 54, "624535": 98, "624634": 63, "624764": 71, "624798": 81, "624818": 71, "624919": 82, "624988": 82, "625": [54, 80], "625159": 76, "625477": 85, "625695": 60, "625766": 74, "625767": 70, "625891": [74, 75], "626433": 85, "626501": 63, "6266": 82, "626633": 71, "626998": 66, "627332": 60, "627505": [74, 75], "627560": 85, "627564": 72, "627588e": 82, "627754": 66, "628": 86, "628069": 80, "629346": 82, "629549": 71, "629595": 37, "62970104": 63, "629740": 70, "629908": 63, "629e": 86, "63": [54, 67, 80, 97, 98, 113, 129, 144, 145], "630150e": 85, "630328": 60, "630450": 62, "630880": 86, "630914": 76, "63107687": 63, "631083": 71, "63117637": 99, "631333": 85, "63174826": 63, "6318": [81, 146], "632058": 80, "632407": 66, "63245862e": 113, "632665": 63, "632747e": 85, "632958": 84, "633021": 60, "6330631": 129, "633433": 80, "634": 86, "63407762": 146, "634078": [81, 146], "634566": 63, "634577": 129, "63470619": 63, "63499": 82, "635": 40, "635000e": [81, 82], "635199": [81, 82], "635768": 70, "63593298": 99, "636048": 99, "636453": 34, "636575": 72, "63683228": 63, "637326": 85, "637482": 60, "637868": 63, "6379": 81, "638264": 85, "638461": 79, "638488": 76, "639": 81, "639135": 80, "63916605": 55, "639345": 82, "639580": 71, "639603": 71, "64": [67, 75, 81, 82, 86, 97, 98, 113, 129, 143], "640": 82, "640900": 82, "641528": 85, "641547": 85, "64154727": 85, "641723": 63, "64197957": 85, "641980": 85, "642": [51, 86], "6420": 82, "642016": 85, "642329": 67, "642648": [99, 100], "64269": 88, "643018": 61, "643078": 61, "643133": 82, "64340": 88, "643512": 72, "643679": [99, 104], "643752": 85, "643939": 67, "644065e": 113, "644113": 97, "644371": 71, "644525": 60, "644665": 72, "64476745e": 113, "644799": 59, "644985": 70, "645": 82, "6453463": 113, "645583": 67, "64579": 53, "6458": 54, "645800": 80, "646117": 71, "646134": 66, "646937": 59, "646998": 83, "647002": 82, "647004": 99, "647010": 82, "647196": 59, "64723": 88, "647254e": 70, "6472793": 113, "6473138": 113, "647676e": 60, "647689": 89, "647864": 97, "647873": 85, "64797": 88, "648": 81, "648163": 62, "648355": 70, "648690": 71, "648769": 71, "648827": 63, "649": 144, "649158": 85, "649514": 70, "649738": 70, "65": [67, 72, 76, 82, 85, 86, 97, 98, 113, 129], "650": [73, 99], "6500000000000001": [72, 82, 85], "650000e": 82, "650234": 67, "65033372": 63, "650810": 82, "650867": 72, "65097171": 63, "651127": 71, "6516903": 113, "652071": 82, "652180": 60, "6522": 144, "652312": 74, "652324": 67, "652349": 85, "652350": 80, "652450e": [81, 82], "6527": 73, "652778": 80, "6528": [61, 82], "6530": 82, "653008e": 81, "653276": 60, "653829": 77, "653846": 72, "653901": [70, 71], "654070e": 99, "654628": 63, "6546909": 113, "654755": 59, "65490512": 63, "655284": 85, "6553": 146, "6554": 144, "655422": 82, "655547": 70, "65557405e": 113, "655959": 77, "656095": 63, "656547": 63, "657": 56, "657283": 88, "658": 80, "658021": [99, 104], "658267": 85, "658592": 71, "6586": 53, "658702": 71, "659": 56, "659245": [70, 71], "659339": 71, "659387": 62, "6593871": 53, "659423": [70, 71], "659473": 89, "659636": 72, "65963683": 63, "659735": 70, "659755": 86, "6598": 78, "659835": 71, "66": [67, 77, 78, 83, 97, 98, 113, 129, 143, 145], "660": [56, 99], "660073": 71, "660320": 75, "660479": [99, 101], "6607402": 86, "660776": 85, "660788": [99, 102], "66133": 99, "661338e": 60, "661369": 84, "661388": 70, "66198912": 60, "662": 51, "66201166": 62, "66208601": 62, "6625": 82, "66292174": 62, "662975": 79, "663081975281988": 72, "663082": 72, "663177": 67, "663182": 72, "66321767": 63, "6634357241067574": 89, "6634429": 60, "663529": 85, "663533": 82, "663672": 77, "663765": 71, "663916": 62, "664103e": 82, "664147": 82, "664409": 71, "664797": 70, "664824": 82, "664850": 80, "665264": 85, "66540882": 60, "665653": [99, 104], "66601815": [99, 101], "666104": 85, "66612794": 60, "666307": 59, "6666667": 56, "666742": 97, "666959e": 77, "66696117": 60, "667": 80, "667492e": 82, "667536": 85, "667614": 72, "667614205604159": 72, "6678": 66, "667981": 70, "667985": 76, "668337": 82, "668452": 76, "668584": 59, "668981": 74, "669183": 63, "669579": 71, "66989604": 58, "67": [51, 56, 66, 81, 89, 97, 98, 113, 129, 143], "670867": [36, 97], "67095958": [99, 102], "671224": 71, "671271": [70, 71], "67136": 82, "6716717587835648": 72, "671672": 72, "671690": 70, "671900": 63, "6722": 56, "672234": [70, 71], "672368": 72, "6723684718264447": 72, "672384": [70, 71], "67245350": 54, "672511": 70, "673092": [70, 71], "673302": 80, "673330": 71, "673535": 60, "67410934": 54, "6745349414": 54, "674552": 82, "67456": 89, "674609": 72, "674747": 79, "675233": 71, "675293": 84, "675512": 60, "675625": 97, "675653": [99, 100], "675747": 63, "675775": 79, "67611715": 63, "676293e": 63, "676405": 72, "6765": [55, 81], "676534": 129, "676641": 70, "676756": 85, "676807": 81, "677123": 70, "677614": 85, "677980": 72, "678": [86, 113], "678117": 82, "678826": 72, "6793621": 62, "67936506": [99, 101], "679539": 80, "67954797": 62, "679789e": 70, "67996638": 62, "67ad635a": 56, "68": [56, 67, 83, 87, 88, 97, 98, 113, 129], "680": 82, "680677": [99, 102], "6810775": 88, "681176": 80, "681246": 71, "681448": 82, "681521": 70, "681562": 82, "681817dcfcda": 56, "681850": 60, "682": 99, "682122": 79, "682269": 82, "6826": 81, "682875": 72, "683364": 61, "683487": 71, "683581": 99, "683637e": 77, "683687": 71, "683937": 63, "683942": 85, "683984": 38, "684": 146, "68410364": 55, "68411700": [55, 146], "684128": 71, "684142": 70, "684502": 85, "684899": 66, "685": 51, "685104": 20, "685107": 85, "685310": 63, "68554404e": 113, "68562150e": 113, "685807": 85, "685989": 99, "686270": 71, "686390": 61, "686627": 70, "687345": 85, "687612": 71, "687647": 85, "68770961": 113, "687854": 59, "687871": 80, "6878711": 54, "688": 144, "6882": 66, "688476": 62, "688540": 97, "688641": 79, "688747": 82, "688886": 97, "688918": 82, "688956": 70, "689072": [99, 102], "689088": [70, 71], "68913152223263755576567697273838588100": 113, "689188": 59, "689392": 85, "689600": 79, "689932": 70, "69": [76, 97, 98, 113, 129, 145], "690334": 72, "6903344145051182": 72, "690470": 60, "69065336": 62, "691097": 70, "691157": 58, "69135907": 62, "691368e": 60, "69140475e": 113, "691423": 70, "691495826": 113, "691511": 81, "69158917": 62, "691848e": 71, "691911": 97, "692231": 62, "692297": 71, "692460": 79, "692579": 71, "692725": 85, "692907": 82, "692959": 70, "693316": 82, "693497e": 82, "693690": 82, "693796": 80, "694154": 72, "694561": 86, "694845e": 82, "694919": 80, "6950": 82, "695045": 70, "69508862": 99, "695581": 76, "69562150e": 113, "695702": 60, "695711": 79, "695714": 63, "69572427": [99, 104], "695928": 70, "696011": [35, 97], "696224": 97, "696289": [74, 75], "69684828": 99, "696966": 71, "696995": 60, "697": 80, "697000": 72, "697420": [74, 75], "697545": 85, "697616": 71, "697693": 70, "697776": 63, "697909": 63, "698223": 59, "698244": 59, "69840389e": 113, "698509": 70, "698642": [99, 104], "698657": 60, "698694": 80, "698714": 63, "698751": 79, "699035": 85, "699082": 72, "69921": 56, "699259e": 85, "699333": 72, "699616": 79, "699697": 71, "6_design_1a": 73, "6_r2d_0": 73, "6_r2y_0": 73, "6b": 129, "6cea": 56, "7": [10, 14, 17, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 130, 136, 143, 144, 145], "70": [55, 57, 72, 74, 81, 82, 85, 97, 98, 113, 129, 145], "700": [70, 71, 73, 80], "7000000000000002": [72, 82, 85], "700010": 60, "700015": 85, "700102": 85, "700277": 61, "700314": 67, "700458": 70, "700643": [99, 102], "700672": 63, "701078": 85, "701088": 81, "701106": 76, "701265": 74, "701387": 60, "701413": 82, "701672e": 72, "701841e": 75, "701866": 85, "7018663": 85, "701966": 82, "702248": 66, "702489": 82, "703049": 70, "70305686": [99, 104], "703108e": 39, "703325": 77, "70344386": [99, 104], "703772": 82, "7040": 82, "704482": 77, "7045": 77, "704558": 77, "704814": 70, "704896": 77, "705": 51, "705090": 71, "705354": 70, "70557077": [99, 104], "705581": 82, "7055958": 91, "705595810371231": 91, "7055958103712310": 91, "705794": 71, "70583": 88, "705910": 60, "706056": 82, "706077": 71, "706122": 71, "706430": 70, "706645": 72, "706657": 72, "706721": 63, "706862": 23, "707125": 71, "707279": 63, "707441": 71, "707579": 63, "70757981": 60, "707738": 71, "70774361": [99, 104], "707868": 85, "707963e": 81, "708190": 80, "708235": 70, "708459": 85, "708472": 71, "708821": 67, "708837": 67, "709026": 59, "709596": 70, "709606": [36, 97], "71": [97, 98, 113, 129, 145], "710059": 67, "710319": 71, "710515": 70, "710586e": 80, "710666": 60, "711024": 82, "71116111": 113, "711328": 82, "711383e": 70, "711518": 82, "711638": 99, "712064": 70, "712082": 82, "712095": 67, "712157": 84, "712268": 70, "712372e": 71, "712503": 88, "712592": 81, "712727": 63, "712774": 74, "712846": 97, "712960": 72, "713": 82, "713407": 82, "713457": 70, "713523": 61, "713986": 82, "713993": 71, "714240": 80, "714250": 71, "714321": 81, "714534e": 71, "714651": 85, "71465114": 85, "715013": 82, "715180e": 82, "7154": 82, "715407": 72, "715465": 60, "7155": 82, "7158581": 54, "71585901": 60, "716013e": 70, "716098": 67, "7161": 82, "71618378": 60, "716387": 70, "716424": 63, "716427e": 71, "716456": 85, "716595e": 82, "716615": 67, "716747e": 63, "716762": 72, "716793": 72, "716799": 80, "7167991": 54, "716801": 79, "716920": 63, "717": 82, "717130": 82, "717185": 85, "717314": 60, "71788533": 60, "717916": 66, "718514": 63, "718686": 89, "7188": 61, "71880047": 60, "718838": 61, "71918986": 60, "7194": 63, "719552": 71, "72": [57, 77, 97, 98, 113, 129, 145], "7203548": 60, "7204309": [99, 104], "720559": 70, "720571": 85, "720573": 70, "720589": 86, "720664": 80, "721018": 70, "721071": 85, "721245": 71, "7215093d9089": 56, "72155839e": 113, "721609": 82, "72228103": [99, 102], "722316": 85, "722634": 85, "72269685": [99, 104], "722848": 72, "722881": 85, "7229": 82, "723": 56, "72314033": 60, "72314685": 60, "723314": 85, "723342": 97, "723345e": 85, "72344628": 60, "723657": 70, "723712": 60, "723846": 67, "7239": 82, "7241399": 54, "724338": 85, "724767": [74, 75], "7248": 60, "724918": 89, "725": 56, "725010": 67, "725061": 70, "725087": 82, "725166": 85, "725240": 66, "725431": 83, "725565": 70, "725802": 23, "725820": 79, "725919": 70, "726": [51, 56, 86], "7261258": 113, "726658": 79, "7268131": 54, "726825": 63, "727159e": 71, "727281": 63, "7274697": 113, "727543": 59, "727693": 82, "727704": 82, "727976": 72, "7282094": [99, 101], "728294": 84, "728710": 85, "72875815e": 113, "728852": 82, "728e": 86, "729668": 97, "729867": 70, "72987186": [99, 104], "73": [55, 67, 83, 97, 98, 113, 129], "730023": 82, "7307": 61, "7307449": 113, "7308": 53, "730809": 70, "731174": 70, "731317": 72, "732": 86, "732067": 70, "732137": 70, "732150": 71, "7326": 82, "732638": 85, "73285": 34, "732918": 74, "733": 82, "733047": 71, "733644": 70, "7339": 66, "734017": 61, "734635": 70, "734770": 71, "734948": 85, "735369e": 97, "7357": 82, "735848": 97, "735941": 33, "735964": 59, "736082": [70, 71], "736084": 85, "73608412": 85, "736207": 63, "736823": 71, "7370": 66, "737010": 60, "737052": 82, "737241": 63, "73750654": [99, 102], "7375615": 55, "73764317e": 113, "737798": 63, "737870": 63, "737951": [70, 71], "738": 81, "738065": 71, "738223": 82, "738315": 82, "738659e": 82, "738876": 71, "739": 82, "739063": 70, "739089": [99, 104], "7395359436844482": 72, "739536": 72, "739595": 86, "739720": 82, "739817": 76, "74": [9, 55, 71, 81, 97, 98, 113, 129, 145], "740": 80, "7401": 63, "740180e": 85, "740288": 63, "740367": 70, "740417": 81, "740505": 67, "740738": 60, "740785": 70, "740869": 72, "741104": 72, "741380": 86, "741523": 67, "741702": 85, "7418": 53, "74189": 56, "742128": 85, "742375": 70, "742407": 84, "742411": 70, "742907": 85, "7432": 53, "743203": 66, "743247": 82, "743341": 71, "743609": 70, "7437": 82, "74402577": 85, "744026": 85, "7441735": 113, "744236": 88, "74461783e": 113, "74475816": [99, 104], "745": 82, "745022": 67, "745386": 60, "745444": 70, "745536": 66, "745638": 81, "745881": 70, "746361": 85, "746843": 75, "7470": 82, "747538": 66, "747646": 82, "747945": 54, "747961": 82, "748084": 71, "748310": 63, "748377": 81, "748513": 82, "748653": 83, "748880": 82, "74938952": [99, 101], "749443": 82, "749540": 66, "749854893": 114, "75": [9, 26, 36, 56, 59, 67, 72, 77, 81, 82, 85, 97, 98, 113, 129, 145], "75000": 89, "7500000000000002": [72, 82, 85], "750000e": 82, "750597": 71, "750616": 62, "750701": 67, "751013": 82, "751261": 82, "751530": 63, "751633": 82, "75171": 81, "751710": [72, 81], "751712655588833": 91, "7517126555888330": 91, "751712656": 91, "752015": 32, "752283": 82, "752387": 61, "752450": 63, "752696": 67, "752871": [99, 102], "752909": 77, "75329595": [99, 102], "7533": 81, "753323": 70, "753393": 70, "753523": 85, "7538389": 113, "753866": 71, "7542": 60, "754469": 70, "754499": 71, "754678": 79, "754692": 97, "754770": 60, "7548": [60, 89], "754870": 80, "755298": 63, "755688": 70, "755701e": 70, "755885": 97, "755910": 82, "7559417564883749": 72, "755942": 72, "7560824": 54, "756200": 67, "756641": 63, "756805": 80, "756859": 63, "756867e": 82, "756905": 23, "756969": 72, "757": [86, 144], "757151": [70, 71], "757183": 72, "757411": 85, "757559": 77, "757819": 80, "757917e": 85, "758140": 63, "758391": 82, "758695": [99, 102], "758831": 71, "758845": 60, "75887": 56, "758958": 63, "759": 146, "759006": 58, "759054": 71, "759599": 61, "759833": 71, "76": [97, 98, 113, 129, 144, 145], "760104": 85, "7603": 53, "760386": [99, 101], "760517": 66, "760753": 63, "760778": 80, "760915": 59, "761": [54, 80], "761224": 67, "761344": 63, "761429": 71, "761714": 72, "761827": 60, "762143": 63, "762284": 85, "76228406": 85, "762299": [99, 104], "762748": 82, "7628744": [99, 102], "763691": 82, "764093": [70, 71], "76419024e": 113, "764315": 85, "76444177e": 113, "764478": 84, "7646": 82, "764798": 85, "764953": 81, "765": 146, "7652": 63, "765202": 82, "76535102": [99, 104], "765363": [70, 71], "765500e": [81, 82], "765710e": 90, "765792": 85, "76591188": 54, "765960": 70, "7660": 53, "76608187": 61, "7662": 60, "7663": 82, "766499": 85, "766724": 63, "766940": 67, "76702611e": 113, "767188": [74, 75], "7673": 63, "767435": 89, "767549": 71, "76756064": 63, "768071": 85, "768273": [74, 75], "768763": 71, "768798": 67, "769361": 85, "7696371": 113, "769805": 85, "77": [86, 97, 98, 113, 129], "770222": 60, "770556": 82, "770838": 66, "770944": [74, 75], "7710": 88, "771157": 129, "771390e": 82, "7714": 83, "7716922": 113, "7716982": 55, "771741": 82, "771965": 82, "772": 146, "772104": 70, "772157": 77, "772159": 60, "77227783e": 113, "772396": 71, "772444": 97, "772791": 82, "77289874e": 113, "773": 56, "77314687": 60, "773177": 72, "77329414": [99, 102], "773339": 77, "773488": 85, "77348822": 85, "7737472": 113, "773769": 79, "773995": 61, "77401500e": 113, "774271e": 82, "774296": 63, "774428": 83, "775": [56, 82], "775191": [70, 71], "775285": 70, "775969": 88, "776254e": 70, "7763": 81, "776728e": 80, "776887": 81, "777199": 66, "777343": 62, "77746575": [99, 104], "777585": 63, "7776071": 54, "777608": 60, "777718": 79, "777728": 97, "777849": 63, "777863": 60, "777867": 77, "777e": 86, "778400": 70, "7786": 53, "779": 86, "779068": 77, "779108": 70, "779167": 31, "779375": 60, "779517": [70, 71], "779682": 72, "779739": 63, "7799": 78, "779902": 60, "779912": 82, "78": [86, 97, 98, 113, 129, 145], "780": 56, "780068": 79, "780338": 70, "780458": 85, "780856": 81, "781": 82, "781013": 113, "781049": 60, "781233": 82, "781530": 85, "781681": 85, "782": 56, "782050": 85, "7824": 61, "782555": 82, "783": 56, "783276": 99, "7833": 53, "7838": 53, "78386025": [99, 104], "784": 129, "784238": 80, "784405": 88, "784483": 80, "784594": 61, "7846": 113, "784624": 72, "784792": 79, "78485661": 63, "784872": 67, "785": 56, "785038": 71, "785153": 71, "785418": 62, "785685e": 113, "785815": 67, "785911": 85, "785e": 40, "786": 56, "786090": 79, "786191": 76, "786237": 70, "786242": 60, "786563": 79, "786744": 72, "78691636": 63, "786986": 67, "78711285e": 113, "78726655": 63, "787695": [99, 102], "78777": 88, "788": 144, "78818": 56, "788483": 60, "788522": 63, "788868": 71, "789032": 70, "789039": 71, "789330": 71, "789355e": 88, "789671": 72, "789671060840732": 72, "79": [67, 97, 98, 113, 145], "790039e": 70, "790115": 82, "790261": 97, "790723": [74, 75], "791097": 81, "791241": 85, "791297": [36, 97], "79164735": [99, 102], "7919965": [99, 102], "792": 83, "792308": 60, "792396": 67, "792939": 72, "792972": 97, "79330022": [99, 104], "793315": 97, "79338596e": 113, "793570": 85, "793598": 71, "793613": 83, "793735": 85, "793818": [70, 71], "794": [83, 99], "794166": 60, "794366": 82, "79458848e": 113, "794805": 74, "795": 86, "795647": 85, "7957": 82, "795932": 98, "796014": 71, "796384": 71, "7964": 61, "796444": 82, "796596e": 70, "796e": 86, "797086": 70, "797189": [99, 104], "797280": 85, "79737716": 63, "797454": 99, "797737": 129, "797868": 71, "79792890e": 113, "797965": 129, "798071": 20, "798308": 81, "798411": 60, "7986847": 113, "798783": [74, 75], "799403": 85, "799440": 62, "79953099": [99, 104], "7999": 90, "799900": 60, "7b428990": 56, "7x": 85, "8": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 72, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 93, 95, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 130, 136, 143, 145, 146], "80": [57, 58, 66, 72, 82, 85, 90, 97, 98, 113, 145], "800": 80, "8000": [19, 57, 90], "8000000000000002": [72, 82, 85], "8001": 61, "800124248": 113, "800143": 70, "800202": 83, "800326e": 70, "800351": 70, "800920": 63, "80117909": 60, "801623": 82, "802": 86, "802289": 82, "802738": 97, "803112": 79, "803300": 70, "803492e": 85, "803563": 82, "803902e": 82, "80396726": 63, "804": 82, "804219": 85, "804284": 88, "804316": 85, "804484": 85, "8045884": 113, "8048": 55, "804806": 62, "804828": 85, "804889": 82, "804944": 60, "805007": 80, "8050464": 63, "805153e": [81, 82], "805293": 71, "8055563": 54, "805774": 70, "8059": 81, "806218e": 82, "806389": 60, "806531": 82, "806554": 70, "806732": 76, "80696592e": 113, "80714504e": 113, "807207": 60, "807589": 63, "807879": 85, "808": [55, 129], "808246": 81, "808284": 82, "808306": 60, "808640": 82, "809125": 70, "809259": 61, "809368": 62, "8095": 83, "809913": [70, 71], "80994067": 60, "80a8": 56, "81": [54, 70, 73, 76, 78, 97, 98, 113, 145], "810": 51, "810044": 81, "810134": 85, "8102": [53, 81], "810306": 67, "810322": 70, "810363": 82, "810382": [81, 82], "810419": 71, "810675": 60, "810707": 82, "810895": 71, "8109852": 63, "811011": 71, "811155": 76, "811458": 81, "811513": 71, "8116912": 129, "811696": 70, "811825": 80, "811901": 85, "81190107": 85, "812": 86, "812028": [99, 100], "8130": 66, "8132463": 54, "813293": 85, "813342": 129, "813682": 82, "814136": 72, "814246e": 71, "814351": 72, "814913": 80, "815156": 60, "8152": 82, "815213e": 71, "815224": 129, "815226": [99, 101], "81568484": 85, "815685": 85, "815993": 85, "816176": 89, "816318": 80, "816373": 70, "816752": 82, "816896": 66, "816902": 83, "816982": 70, "817119": 70, "817291": 82, "8173602": 78, "817967": 99, "817989": 63, "81827267": 85, "818273": 85, "818289": 85, "81828926": 85, "818380": [70, 71], "81856": 56, "818953": 60, "819223": 97, "819507": 79, "82": [89, 97, 98, 113, 145], "8202": 55, "820366": 80, "8209": 55, "820963": 67, "821": 144, "8210": 55, "821021": 72, "821457": 82, "821566": 85, "821855": 97, "821970": 79, "821995": 71, "8221": 53, "822289": [81, 146], "82228913": 146, "822482": 72, "8227": 82, "822822": 72, "823247": 85, "823273": [70, 71], "823644": 60, "823708": 63, "823772": [99, 102], "824140": 66, "824350": [70, 71], "824477": 63, "824657": 79, "824701": 72, "824750": 72, "824889": 72, "824961e": 82, "8250": 53, "825587": 71, "825617": 80, "825862": 85, "825980": 72, "8259803249536914": 72, "8260": 81, "826065": [70, 71], "826273": 61, "826426": [99, 101], "826467e": 70, "826492": 85, "826519": [36, 97], "82666866e": 113, "82684324": 88, "82731919": 60, "827375": 58, "827381": 85, "827735": 85, "827938162750831": [74, 75], "827998": 60, "828058": 82, "828157": 67, "828618": 77, "828778e": 70, "828817": 63, "828837": 62, "828912": 70, "828915": [74, 75], "829543": 72, "829730e": 71, "829764": 97, "829781": 63, "83": [97, 98, 113, 145], "830263": 79, "830273": 67, "830301": 84, "830442": 70, "830467": 70, "831": 86, "831019": 72, "831190": 71, "831278": 70, "831741": 70, "831833": [99, 104], "832086": 85, "832091e": 63, "8324": 99, "8326928": 88, "832693": 88, "832875": 85, "83287529": 85, "833024": 80, "833065": 67, "833227e": 98, "833464": 82, "833797": 62, "833907": 80, "834133": 77, "83436056": [99, 102], "834791e": 63, "834916": [99, 102], "835": 86, "8350": 82, "835035": 79, "835239": [99, 104], "835344": 67, "835596": 82, "835750": 77, "835935": 71, "836113": 60, "836234": 99, "837795": 63, "8378867": 63, "838114": 85, "838235": 83, "838457": 82, "83887862": 63, "83905": 20, "84": [56, 76, 86, 97, 98, 99, 113, 145], "840041": 82, "840191": 83, "84029513": 60, "8403": 66, "840303": 85, "84030318": 85, "840345": 61, "84059297": 60, "840673": 70, "840718": [99, 101], "840836": 85, "840995e": 81, "841": [54, 80], "841132": 81, "8415": 55, "841847": 82, "842132": 99, "8422313": 113, "842405": 72, "84243712": 63, "842625": 80, "842746": 85, "8428": 81, "842853": 85, "843018": 97, "8431": 113, "843730": 80, "843796": 70, "844": 51, "8440": 82, "844107e": 71, "844308": 85, "844535": 63, "844549": [74, 75], "844663": 67, "8446645": 113, "844667": 129, "844707": 85, "844889": 80, "8450": 66, "845241": 86, "845534": 79, "845694": 60, "846344e": 63, "846388": 72, "847029": 71, "847452": 63, "847555": 70, "847595": [35, 97], "84767927": 60, "847948": 72, "847962": 70, "847966": 82, "848688e": 79, "848757e": 81, "848868": 72, "84930915e": 113, "849427": 97, "849706e": 113, "849747": 88, "8497f641": 56, "8499": 82, "85": [13, 72, 76, 82, 85, 90, 97, 98, 113], "8500000000000002": [72, 82, 85], "850038": 67, "850321": 80, "850439": 71, "850575": [70, 71], "850656": 77, "850794": 85, "850903": 63, "851": 144, "851198": 82, "8513": 56, "851366": 80, "851429": 63, "852": 82, "85242": 61, "8526": 66, "85265193": 78, "85280376": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "853024": 60, "85355399": 63, "853712": 62, "853744": 63, "85397773": [99, 101], "85441491": 60, "854525": 66, "855035": 70, "855780": 85, "855862": 71, "856404": 75, "856624": 60, "856758": 97, "8571": 53, "857161": 85, "857372": [99, 102], "857515": 97, "857544": 80, "857765": 82, "858212e": 71, "858752": 62, "858805": 63, "858952": 67, "859": 82, "85911521e": 113, "85912862": 129, "859129": [114, 129], "8591362": 113, "8597": 81, "85974356": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "85c5": 56, "85e": 55, "86": [97, 98, 113, 145], "860348": 60, "860663": 129, "860804": 85, "860916": 61, "860992": 82, "861019": 67, "861210": [99, 104], "861505": 66, "861519": 70, "86156218": 60, "861701": 63, "86183959": 63, "862043": [74, 75], "86222106": 63, "862359": 72, "863174": 63, "863772": 81, "863982270": 114, "864": 86, "86415573": 55, "86424193e": 113, "864345": 63, "8644": 56, "864450": 63, "864529": 63, "864664": 67, "864741e": 82, "865313": 82, "865562": [70, 71], "8656": 61, "865854": 82, "865860": [81, 82], "865914": 71, "866102": [70, 71], "866179899731091": 91, "866179900": 91, "866579": 82, "866798": 82, "867201": 79, "867565": 85, "8679": 82, "868": 56, "8685788": 85, "868579": 85, "86879936": 63, "86897905": [99, 104], "869": [56, 86], "869020": 72, "869195": 71, "86922927": 60, "869398": 71, "869477": 70, "869585": 39, "869586": 76, "869658": 66, "87": [55, 70, 76, 80, 97, 98, 113, 145], "870": 62, "8700": 55, "8700071": 113, "870099": [74, 75], "870260": 85, "870332": 85, "870444": 81, "870786": 60, "870857": 85, "8708672": 60, "870870": [99, 102], "871": 56, "871545e": 70, "871923": 71, "871972": 77, "871999": 63, "872": [51, 86], "872132": 71, "872222": 82, "872727": 70, "872768": 85, "872852": 85, "87290240e": 113, "872994": 82, "87309461": [99, 104], "873198": 82, "873275": 66, "873304": 63, "873677": [74, 75], "873848": 62, "87384812361": 53, "87384812362": 53, "87430335": 129, "874303353": 129, "874702": [74, 75], "8750": 82, "87508173": 60, "8759": 82, "876": 86, "876083": 82, "876233": 62, "87623301": 53, "876413": 63, "876431e": 72, "87648198": 60, "876549": 82, "8767": 66, "87674597e": 113, "8768": 53, "8771": 82, "877153": 82, "877213": 60, "877266": 63, "877455": 84, "877833": [70, 71], "877874": 63, "877875": 63, "877903": 67, "878281": 85, "878289": 82, "878402": 70, "878746": 67, "878802": 62, "878847e": 82, "878895": 67, "878968e": 70, "879": 99, "879049": 82, "879058": 79, "879103": 72, "879509": 70, "87972266": 63, "87e": 55, "88": [55, 76, 86, 97, 113], "880106": 80, "880217": [99, 102], "880579": 85, "880591": 84, "880609": 63, "880808e": 82, "880880e": 82, "880886": 81, "8810": 81, "881201": 82, "88125046e": 113, "881465": 59, "881581": 33, "88173062": 54, "881937": 67, "882475": 72, "882641": 81, "882928": 67, "882965": 66, "883485": 71, "883622": 85, "883914": 72, "883953": 79, "88397239": 60, "884132": 85, "8843": 88, "884331": 62, "884499": 63, "88449977": 60, "8845": 53, "884821": 97, "884897": 62, "884996": 72, "8850": 55, "885065": 85, "885832": 86, "885956": 70, "885978": [74, 75], "886041": 71, "886086": [70, 71], "886266": 82, "88629": 53, "886314": 71, "88664": 56, "88711505": 63, "887182": [99, 100], "887345": 82, "887454": 63, "887556": 72, "887648": 71, "887680": 70, "8877": 61, "888146": 80, "8881461": 54, "888352": 67, "888445": 71, "888775": 75, "888804": 82, "889236": 66, "889293": 85, "889326": 71, "889566": 88, "889638": 67, "889733": 85, "889792": 71, "88988263e": 113, "889913": [70, 71], "889963": 85, "88ad": 56, "89": [55, 71, 97, 113, 144, 145], "890": [54, 80], "890204": [99, 104], "89027368": 129, "890273683": 129, "890318": 70, "89035917": 76, "890372": [64, 93, 95, 143], "8903720000100010000010": [56, 93, 95, 143], "8904": 51, "890454": 98, "890665": 77, "890673": 63, "890718": 60, "890855": 67, "8909": [54, 81, 146], "890933": 66, "891527": [99, 104], "891570e": 60, "891606": 81, "891752": 71, "891997": 70, "892": 56, "892648": 85, "892796": [70, 71], "892828": 77, "893": 56, "8932105": 54, "893461": 67, "893649": [70, 71], "893851": 85, "894": 56, "894307e": 82, "894448": 71, "8946549": 57, "89472978": [99, 104], "895106": [70, 71], "895308": 82, "895333": 85, "895442": 81, "89551067": 63, "895690": [70, 71], "895768e": 72, "8957998": 63, "896023": 85, "896182e": 77, "896263": 77, "89639464": 60, "896541": 60, "896761": 61, "897220": 85, "897240": 82, "8974": 81, "8974226": [99, 102], "897451": 70, "897495e": 71, "89756162": 60, "897811": 60, "898183": 67, "898270": 63, "898722": 85, "899021": 97, "899250": 67, "899296": [99, 102], "899460": 85, "899654": 67, "899662e": 70, "899716": 71, "8bdee1a1d83d": 56, "8da924c": 56, "8e3aa840": 56, "9": [20, 22, 23, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 91, 93, 95, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 130, 136, 142, 143, 145, 146], "90": [15, 55, 57, 58, 72, 82, 85, 90, 97, 113, 145], "9000000000000002": [72, 82, 85], "900000e": 82, "900021": 98, "900127": [99, 104], "900829": 77, "901013": 71, "901148": 85, "90136": 81, "901360": 81, "90145324": [99, 104], "901526": 76, "901683": 82, "901684": 63, "901705": 70, "902": [51, 129], "902573": 72, "90283043": 60, "902920": 97, "903056e": 85, "903339": 72, "903351e": 72, "903418": 80, "903674": 70, "903681": 85, "903767": [70, 71], "904156": 72, "9041560442482157": 72, "904315": 70, "904396": 70, "9045594": 113, "905042": 71, "905494": 72, "905724": 66, "905858": 89, "905951": 88, "906072": 97, "9061": 82, "906195": [99, 102], "906716732639898": [74, 75], "906757": 68, "907115": 85, "907176": 85, "9073": 82, "907301": 63, "907491": 72, "907801": 80, "907879": 67, "90794478": 129, "907944783": 129, "907961": 82, "908024": 89, "908663": 71, "908767": 79, "909184": 60, "90925087": 60, "909304": [70, 71], "909571": 67, "90963122e": 113, "909896": 83, "909942e": 97, "909975": 82, "909997": [81, 146], "91": [87, 97, 113, 145], "910000e": 82, "91053356": [99, 102], "910895": 71, "9109": 56, "91102953": 85, "911030": 85, "911047": 60, "9112": 81, "911277": 71, "9115645": 113, "911662": 74, "912230": [70, 71], "9126": [55, 146], "9127": [55, 146], "912903": 70, "913": 56, "91315015": 54, "913280": 89, "913371": 71, "913415e": 70, "913485": 82, "913585": 77, "913774": 72, "91389564": 63, "914": 83, "9142": 82, "914226": 63, "91438767e": 113, "9145": 53, "914598": 67, "914784": 60, "915": [55, 56, 81, 82], "915000e": [81, 82], "915225": 66, "915260e": 70, "915488": [74, 75], "9158080176561963": 79, "915918": 62, "916236": 53, "916359": 67, "916528": 74, "9166667": 56, "91683613": [99, 102], "916914": 85, "916930": 70, "917": 56, "917000": 71, "917066": 82, "917248": 85, "91724807": 85, "917436": 85, "918": 86, "918196": 60, "918227": 72, "918293": [99, 104], "918607": 60, "918657": 60, "919432": 85, "9197": 82, "919969": 70, "91e": 55, "92": [60, 87, 97, 98, 113, 145], "920052": 71, "920335": 82, "920337": 75, "920422": 83, "920645": 82, "9209": 53, "9210": 82, "921061": 89, "921198": 77, "921256e": 71, "921372": 72, "921778": 77, "921913": 80, "921956": [70, 71], "921e4f0d": 56, "922160": 82, "922251": 70, "9223": 82, "922656": 66, "922668": 77, "92270892": 60, "922996": 80, "9230344": 113, "923074e": 72, "923517": 90, "923607": 85, "92369755": 54, "923804": 72, "923943": 146, "923977": 82, "924002": 85, "9243": 82, "924385": 60, "924396": [74, 75], "924443": 67, "924634": 59, "9248": 56, "924818": 60, "924821": 72, "924843": 80, "924920": 62, "924921": 97, "925": 58, "925248": [74, 75], "92536544": 63, "925410": 66, "92552658": 113, "92566": 99, "925660": 70, "92566844": 60, "925697": 61, "9257": 61, "925736": 72, "925957": 74, "925994": 81, "925995": 71, "926227": 71, "926621": 72, "926901": 79, "926944": 63, "927": 52, "927074": 85, "927232": 82, "9274": 82, "927837": 62, "927906": 63, "927950": 82, "92818559": 63, "92827999": 99, "928792": 62, "92881435e": 113, "928947": 80, "92905": 54, "92957159": 63, "929643": 70, "92964644": [99, 102], "92972925e": 129, "929729e": [114, 129], "93": [55, 77, 87, 97, 98, 113, 145], "930105": 63, "93028391": 63, "9304028": 54, "93074943": [99, 104], "931": 92, "931037": 63, "93134081": [99, 102], "931479": 85, "931487": 63, "931630": 62, "93163715": 63, "93191324": 60, "931978": 143, "932027": 72, "932404e": 82, "9325": 53, "9327": 53, "932973": 85, "93300": 99, "933322": 71, "933517": 61, "933642": 61, "933671": 71, "933810": 60, "933857": 71, "933996": 72, "934058": 70, "934243": 71, "9344184": 63, "934433": [70, 71], "9345": 56, "934500": 71, "934511": 129, "934549": 82, "93458": 88, "93469253": 63, "934963": 71, "934992": 72, "935": [40, 78, 99], "935220": [99, 100], "935591": 85, "935666e": 113, "935730": 85, "935764": 71, "935793": [99, 102], "935989": 80, "9359891": 54, "936332": 62, "93648": 90, "936494": 70, "936739": 85, "937116": 80, "93742834": 63, "937586": 82, "938": 129, "938263": [99, 104], "9385089": 60, "938836": [99, 104], "939068": [74, 75], "9392": 82, "939250": 70, "939458": 70, "9395": 82, "939501": 60, "93958082416": 146, "94": [58, 78, 97, 113, 145, 146], "94034181": 63, "940354721701296": 72, "940355": 72, "940373": 82, "94060006": 113, "940634": 63, "941440": 70, "941487": 63, "941724": 82, "941788": 74, "942139": 75, "942312": 85, "942460e": 85, "942489": 82, "9425": 53, "942550": 82, "942661": 80, "942775": 63, "942823": 82, "942864": 62, "94309994e": 113, "943112": 63, "943548": 77, "943693": [99, 100], "943938": 85, "943949e": 85, "94420632": 60, "944253e": 85, "944266": [74, 75], "94426663": 63, "94427158": [99, 104], "944280": 82, "94441007e": 113, "94473": 57, "945": 51, "945193": 63, "94561711": 63, "945881": 70, "946180": 77, "94629": 90, "946297": 72, "946406": 75, "946433": 85, "946462": 63, "946533": 70, "946658": 82, "946968": 72, "947440": 84, "947466": 98, "947613": 71, "947854": 60, "947855": 67, "9480": 82, "948112": 86, "948154e": 74, "948194": 60, "948785e": 70, "948868": 82, "948975": 76, "94906344": 54, "949241": 129, "949456": 85, "9494716": 63, "949866": 71, "949912": 66, "95": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 53, 55, 57, 58, 59, 60, 61, 62, 63, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 85, 86, 88, 89, 90, 97, 99, 113, 129, 130, 136, 145, 146], "950": 83, "9500": 82, "950158": 70, "950545": 68, "95062986e": 113, "95079708": 60, "951": 83, "95104742": 60, "951270": 60, "951502": 85, "951532": 80, "951920": 84, "952": [55, 86, 146], "952146": [99, 104], "9523": 53, "952839": 85, "95305": 57, "95311164": [99, 104], "953368": 61, "9534": 82, "953683": 80, "953704": 70, "95372559e": 113, "953884": 99, "954": 129, "95401167e": 113, "955005e": 82, "9551": 82, "9552": 53, "955500": 60, "955541": [36, 97], "95559917": 98, "955701": 70, "955e": 99, "956047": 54, "9561": 53, "956241": 62, "956574": 82, "956588": 99, "956724": 72, "9567242535070148": 72, "956877": 71, "956892": 82, "957229": 75, "957375": 80, "957420": 63, "957745": 72, "9579": 55, "957996": 72, "958": 129, "9580": 55, "958105": 97, "958133": 62, "958541": 82, "958568": 60, "958608": 66, "958612": 66, "958636": 76, "958789": [99, 102], "95883291": 113, "959132": 71, "959384": 71, "959613": 97, "959699": 63, "959786": 63, "959983e": 63, "95e": 55, "96": [55, 70, 71, 83, 97, 113, 145], "960236": 99, "9605": 82, "960808": 72, "960834": 71, "9609": 53, "961181": 62, "961539": 82, "961962": 72, "962364": 67, "962373": 71, "962954": 71, "963055": 82, "963148": 61, "963367": 66, "963427e": 71, "9636": 66, "963619": 63, "963884": 60, "964025e": 85, "964065e": 71, "964261e": 80, "964318": 82, "9647": 53, "965322": 63, "965341": 71, "965531": 99, "965696": 70, "965774": 82, "96582": 98, "96586194": 60, "966015": 85, "966031": 66, "966097": 37, "966659": 72, "9666592590622916": 72, "966929": 60, "967092": 71, "967206": 66, "967223": 61, "967467": 88, "968": 83, "968127": 67, "968134e": 85, "968258e": 70, "968577": 58, "969141": [97, 98, 99], "969315": 61, "969509": 60, "9699": 81, "969925e": 71, "97": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 143, 145, 146], "970065": 85, "9701": 66, "970150": 71, "970926": 63, "971058": [74, 75], "971059": 66, "97161134": 63, "971967": [99, 102], "972509": 71, "972583": 83, "972735": 66, "972745": 70, "972748": 72, "97276281": 85, "972763": 85, "973140": 66, "97314470": 54, "973156": 97, "973229": 71, "973241": 85, "973331": 82, "973741": 71, "973890": 71, "974202": 72, "974213": 71, "97441062": [74, 75], "974414": 72, "974487": 70, "97470872": 88, "974712": 63, "9748910611": 54, "975": [70, 71, 74, 75, 77, 78, 83, 99, 102], "9750238": 113, "97506313": 63, "975289": 67, "9753": 56, "975447": 58, "975450": 71, "975461": 80, "975592": 67, "9757": 113, "975825": 62, "976": 83, "976088": 85, "97619643": [99, 104], "97632595": 63, "9764": 66, "976548e": 71, "976562": 85, "977202": 71, "977280": [70, 71], "977295": 82, "977507": 71, "977799": 62, "977820": 70, "978": 83, "9783": 66, "978303": 79, "978618": 66, "9787": 82, "978977": 85, "979": 86, "979031": 66, "979384": 77, "979475": 70, "979702": 70, "979857": 70, "979966": 77, "979971e": 70, "98": [70, 71, 82, 97, 113, 145], "980": 83, "980026": 82, "98023747": [99, 102], "9802393": 54, "980440": 71, "980643e": 72, "981104": 84, "981403": 71, "981438": 70, "981672": 72, "981715": 70, "982019e": 71, "982353e": 82, "982417": 72, "982720": 70, "982797": 84, "983192": 85, "983253": 70, "983759": 146, "98393441": 88, "984024": 84, "984083": [74, 75], "984430": 62, "984551": 32, "984562": 85, "984866": 129, "984872": [70, 71], "984937": 72, "98505871e": 113, "985207": [70, 71], "985654": 71, "985672": 62, "9862": 63, "986249": 81, "986383": 82, "986403": 60, "986417": 70, "98673": 76, "9870004": 56, "987220": 82, "987307": 79, "9875": 53, "987726": 71, "987942": 60, "9880384": 56, "988421": [70, 71], "988463": 85, "988541": 70, "988709": 82, "988780": 82, "989120": 63, "989675": 83, "99": [55, 67, 70, 71, 83, 97, 113, 145], "990104": 83, "990210": 82, "990903": 70, "991": 56, "9914": [81, 82, 88], "991444e": 74, "9915": [55, 81, 82, 88], "991512": 55, "991963": [70, 71], "991977": 82, "991988": 70, "992": 86, "992047": 62, "99232145": 88, "992582": [70, 71], "992610550": 113, "993": 83, "993201": 71, "993416": 77, "993512": [99, 102], "993575": 82, "994": [83, 86], "994168239": 54, "994208": 67, "994214": 82, "994332": 68, "994377": 70, "9944": [78, 99, 101], "9948104": 57, "994851": 82, "994937": 75, "995015": 82, "9951": 53, "995193": 60, "995248": 85, "99549118e": 113, "99571372e": 113, "996": 83, "9961": 81, "996130": 60, "9961392": 54, "996313": 70, "996646": 62, "996892": 79, "996934": 80, "9970": 82, "997034": 90, "997494": 90, "997571": 80, "997621": 72, "997934": [74, 75], "998": 83, "998063": 68, "99864670889": 146, "998766": 82, "999": [58, 59, 66, 76, 88, 146], "999207": 85, "9995": [59, 70, 71], "9996": [59, 70, 71], "9996553": 55, "9997": [59, 70, 71], "9998": [59, 70, 71], "9999": [59, 70, 71], "99c8": 56, "A": [7, 9, 10, 13, 14, 18, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 51, 52, 53, 55, 56, 60, 61, 62, 63, 66, 68, 69, 73, 75, 78, 79, 83, 84, 86, 87, 88, 89, 92, 93, 95, 97, 98, 99, 100, 102, 104, 112, 129, 130, 131, 137, 138, 139, 140, 141, 143, 144, 146], "ATE": [9, 33, 37, 55, 64, 66, 67, 77, 81, 88, 89, 97, 99, 109, 112, 114, 122, 130, 138], "ATEs": [66, 67, 83], "And": [57, 83, 90, 130, 132, 133], "As": [52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 66, 67, 69, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 85, 89, 90, 91, 98, 99, 102, 103, 104, 113, 114, 116, 118, 120, 129, 130, 131, 140, 146], "At": [9, 10, 26, 54, 58, 59, 61, 67, 76, 78, 80, 82, 85, 146], "Being": 146, "But": [77, 78], "By": [53, 54, 80, 86, 89, 98, 99, 102, 104, 130, 136], "For": [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 33, 39, 43, 47, 51, 53, 54, 56, 58, 60, 61, 62, 63, 66, 67, 68, 76, 77, 78, 79, 80, 82, 84, 86, 88, 89, 91, 92, 93, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 138, 140, 141, 142, 143, 146], "ITE": [14, 66, 67], "ITEs": [66, 67], "If": [4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 52, 54, 58, 60, 61, 62, 63, 66, 69, 70, 71, 77, 78, 80, 82, 86, 92, 93, 95, 97, 98, 99, 101, 107, 114, 115, 116, 117, 118, 119, 122, 129, 130, 131, 132, 133, 134, 135, 136, 139, 140, 141, 146], "In": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 46, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 113, 114, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146], "It": [21, 53, 54, 55, 66, 70, 71, 73, 74, 75, 80, 81, 82, 86, 87, 89, 98, 99, 100, 113, 141, 145], "No": [12, 51, 53, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 76, 81, 82, 86, 88, 90, 93, 94, 95, 98, 99, 101, 102, 104, 114, 129, 143, 144], "Not": [99, 105], "Of": [78, 129, 146], "On": [52, 69, 79, 83, 87, 92, 144], "One": [55, 60, 63, 81, 82, 89, 97, 129], "Or": 40, "Such": [89, 98], "That": [40, 146], "The": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 105, 108, 110, 111, 112, 113, 114, 119, 122, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 144, 145, 146], "Then": [26, 72, 85, 87, 99, 129, 130, 140, 141, 142], "There": [55, 81, 89, 99, 105, 142, 146], "These": [32, 55, 56, 61, 65, 79, 81, 84, 86, 88, 97, 99, 146], "To": [28, 51, 52, 54, 55, 56, 57, 58, 60, 61, 62, 63, 66, 67, 68, 69, 70, 71, 72, 74, 75, 77, 78, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 92, 93, 95, 97, 98, 99, 103, 113, 129, 130, 131, 136, 140, 141, 142, 143, 146], "With": [13, 70, 71, 98, 144], "_": [52, 54, 59, 60, 62, 63, 69, 70, 71, 72, 73, 74, 75, 80, 81, 82, 84, 85, 86, 87, 91, 92, 97, 99, 102, 105, 113, 114, 116, 129, 130, 131, 136], "_0": [52, 54, 69, 73, 80, 91, 92, 113, 114, 127, 128, 129, 130, 140], "_1": [9, 10, 14, 25, 26, 57, 83, 90, 114, 127, 128], "_2": [9, 10, 14, 25, 26, 83], "_3": [9, 10, 14, 25, 26], "_4": [9, 10, 14, 25, 26], "_5": [9, 14], "__": [46, 47], "__init__": [79, 87], "__version__": 142, "_a": [114, 116, 118], "_all_coef": 113, "_all_s": 113, "_b": [114, 116, 118], "_check": 66, "_compute_scor": 28, "_compute_score_deriv": 28, "_coordinate_desc": 80, "_d": [66, 86, 99], "_est_causal_pars_and_s": 145, "_estimator_typ": 79, "_h": [86, 99], "_i": [52, 57, 69, 85, 90, 92, 99, 105], "_id": 113, "_j": [9, 10, 14, 16, 25, 26, 54, 80, 129], "_l": 98, "_lower_quantil": [60, 63], "_m": [98, 113], "_mean": [60, 63], "_n": [114, 115, 116, 117, 118, 122, 129, 130, 136, 139], "_n_folds_per_clust": 80, "_offset": 98, "_pred": 98, "_rmse": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "_upper_quantil": [60, 63], "_x": 41, "_y": [86, 99], "a0": 79, "a09a": 56, "a09b": 56, "a1": 79, "a3d9": 56, "a4a147": 83, "a5e6": 56, "a5e7": 56, "a6ba": 56, "a79359d2da46": 56, "a840": 56, "a_": [57, 90], "a_0": 17, "a_1": 17, "a_j": [99, 106], "ab": [53, 87, 141], "ab71": 56, "abadi": [7, 58], "abb0fd28": 56, "abdt": [64, 93, 95, 143], "abl": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 52, 69, 78, 82, 83, 98, 130, 131, 140], "about": [22, 24, 55, 58, 59, 66, 78, 81, 87, 99, 141, 143, 146], "abov": [52, 55, 61, 67, 69, 70, 71, 74, 75, 78, 79, 81, 83, 84, 85, 86, 89, 92, 97, 98, 99, 103, 105, 142], "absolut": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 78, 98], "abstract": [28, 53, 54, 80, 114, 141, 145], "abus": [60, 63, 99, 102, 104, 105], "acc": [30, 53], "acceler": 66, "accept": [25, 97, 98], "access": [42, 43, 53, 55, 60, 61, 62, 63, 66, 74, 75, 76, 78, 88, 98, 130, 136, 146], "accompani": 141, "accord": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 55, 57, 58, 61, 67, 69, 72, 81, 85, 86, 89, 90, 98, 99, 100, 105, 129, 130, 132, 133, 134, 135, 137, 138, 146], "accordingli": [57, 58, 78, 79, 81, 86, 90], "account": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 54, 55, 60, 63, 80, 81, 82, 88, 89, 130, 136, 140, 146], "accumul": [55, 81, 82, 88], "accur": 66, "accuraci": [42, 46, 53, 99], "acemoglu": 144, "achiev": [54, 66, 77, 80, 84, 89, 99, 129], "acic_2024_post": 83, "acknowledg": [55, 56, 81], "acm": 144, "acov": 144, "across": [21, 25, 55, 66, 81, 83, 146], "action": 145, "activ": [4, 5, 6, 142, 145], "actual": [40, 60, 63, 76, 89], "acycl": [57, 90, 146], "ad": [4, 5, 6, 7, 8, 28, 42, 43, 46, 47, 76, 93, 95, 98, 99, 129, 130, 131, 145], "adapt": [32, 81, 145], "add": [53, 54, 57, 58, 59, 64, 66, 67, 74, 75, 76, 83, 85, 86, 88, 89, 90, 98, 99, 144, 145], "add_trac": 89, "addit": [14, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 44, 60, 61, 62, 63, 73, 89, 94, 95, 98, 99, 100, 114, 123, 130, 137, 138, 140, 144, 145], "addition": [9, 10, 63, 67, 72, 82, 88, 98, 99, 113, 129, 130, 136, 143], "additional_inform": 21, "additional_paramet": 21, "address": 89, "adel": 144, "adj": [86, 89], "adj_coef_bench": 89, "adj_est": 89, "adj_vanderweelearah": 89, "adjust": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 45, 54, 59, 77, 80, 82, 88, 89, 97, 99, 105, 129, 130, 136, 144, 145, 146], "adopt": [58, 99, 101, 105], "advanc": [79, 96, 113, 144], "advantag": [52, 53, 55, 67, 69, 81, 82, 92, 142], "advers": [130, 131], "adversari": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 88, 130, 136, 140], "ae": [52, 54, 55, 57], "ae56": 56, "ae89": 56, "aesthet": 52, "aeturrel": 18, "afd9e4": 83, "affect": [66, 67, 73, 99, 145, 146], "after": [53, 55, 56, 57, 58, 73, 81, 82, 89, 90, 97, 98, 99, 105, 130, 132, 136, 142, 146], "after_stat": 52, "ag": [55, 81, 82, 84, 88, 146], "again": [52, 53, 54, 55, 57, 58, 60, 63, 67, 69, 76, 79, 80, 81, 86, 87, 88, 89, 90, 92, 130, 132, 133], "against": [58, 66, 76, 78, 84, 98], "agebra": 97, "agegt54": [56, 64, 93, 95, 143], "agelt35": [56, 64, 93, 95, 143], "agg": [53, 60, 63, 87], "agg_df": [60, 63], "agg_df_anticip": [60, 63], "agg_dict": [60, 63], "agg_dictionari": [60, 63], "agg_did_obj": [99, 100], "aggrag": [99, 100], "aggreg": [21, 24, 53, 91, 100, 113, 145], "aggregate_over_split": 40, "aggregated_eventstudi": [60, 61, 62, 63], "aggregated_framework": [60, 61, 62, 63, 99, 100], "aggregated_group": [60, 63], "aggregated_tim": [60, 62, 63], "aggregation_0": 21, "aggregation_1": 21, "aggregation_color_idx": 21, "aggregation_method_nam": 21, "aggregation_nam": 21, "aggregation_weight": [21, 60, 61, 62, 63], "aggt": 53, "ai": [62, 87, 144], "aim": 86, "aipw": 83, "aipw_est_1": 83, "aipw_est_2": 83, "aipw_obj_1": 83, "aipw_obj_2": 83, "air": [54, 80], "al": [7, 8, 11, 13, 16, 17, 52, 54, 55, 56, 58, 69, 70, 71, 72, 73, 74, 75, 78, 80, 81, 82, 85, 88, 92, 99, 101, 105, 113, 114, 120, 122, 123, 124, 129, 130, 131, 140, 141, 143, 145], "alexandr": [73, 144], "algebra": 99, "algorithm": [51, 53, 54, 56, 57, 58, 60, 61, 62, 63, 67, 69, 72, 77, 78, 80, 82, 85, 88, 90, 96, 98, 99, 101, 102, 104, 113, 114, 129, 145, 146], "alia": [42, 43, 46, 47], "align": [52, 54, 57, 59, 60, 63, 69, 72, 78, 80, 81, 83, 84, 85, 90, 99, 102, 104, 105, 114, 116, 118, 145], "all": [4, 5, 6, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 46, 47, 48, 52, 53, 54, 55, 57, 58, 66, 67, 69, 76, 77, 78, 79, 80, 81, 82, 84, 86, 89, 90, 92, 93, 94, 95, 97, 98, 99, 100, 101, 103, 105, 113, 114, 116, 118, 129, 130, 140, 141, 142, 145], "all_coef": 113, "all_dml1_coef": 91, "all_s": 113, "all_smpl": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 77], "all_smpls_clust": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "all_z_col": [54, 80], "allow": [4, 5, 6, 20, 21, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 55, 60, 63, 66, 67, 81, 82, 86, 95, 97, 98, 99, 113, 114, 129, 141, 145, 146], "almqvist": 144, "along": [66, 98], "alpha": [15, 17, 20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 55, 57, 60, 63, 64, 66, 67, 69, 70, 71, 72, 73, 77, 78, 79, 80, 81, 82, 85, 91, 92, 97, 98, 99, 113, 114, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140], "alpha_": [16, 54, 80, 98], "alpha_0": [130, 140], "alpha_ml_l": 73, "alpha_ml_m": 73, "alpha_x": [12, 32, 99], "alreadi": [26, 57, 58, 60, 63, 87, 90, 98, 99, 101], "also": [20, 22, 23, 24, 29, 32, 33, 39, 51, 52, 53, 54, 55, 56, 58, 60, 61, 63, 66, 67, 68, 69, 70, 71, 74, 75, 76, 78, 79, 80, 81, 82, 84, 86, 88, 89, 92, 97, 98, 99, 113, 114, 129, 130, 131, 142, 143, 145, 146], "alter": [54, 80], "altern": [53, 55, 56, 61, 81, 84, 96, 98, 129, 141, 143], "although": 89, "alwai": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 47, 53, 86, 145], "always_tak": [32, 55, 81], "alyssa": 144, "amamb": 80, "american": [15, 83], "amgrem": 80, "amhorn": 80, "amit": [89, 144], "amjavl": 80, "ammata": 80, "among": [55, 73, 81, 82, 88, 89], "amount": [24, 55, 79, 81, 82, 146], "amp": [51, 54, 56, 57, 58, 60, 61, 62, 63, 80, 82, 88, 90], "an": [4, 5, 6, 9, 10, 14, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 52, 53, 54, 55, 56, 57, 60, 61, 62, 63, 67, 69, 70, 71, 73, 76, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 97, 98, 99, 100, 103, 105, 106, 113, 114, 129, 130, 131, 136, 141, 143, 144, 145, 146], "analog": [27, 28, 54, 60, 61, 63, 80, 82, 88, 97, 99, 101, 114, 115, 116, 117, 118, 129, 130, 136], "analys": 146, "analysi": [20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 52, 54, 55, 69, 80, 81, 82, 87, 92, 96, 97, 99, 104, 131, 136, 140, 141, 145], "analyst": 87, "analyt": [83, 85], "analyz": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 55, 81, 82, 88, 146], "ancillari": 89, "andrea": 144, "angl": 55, "angrist": 83, "ani": [51, 52, 53, 56, 57, 58, 66, 68, 69, 87, 89, 90, 92, 99, 142, 146], "anna": [9, 10, 14, 20, 22, 23, 25, 26, 53, 58, 60, 61, 62, 63, 99, 100, 101, 103, 105, 144], "annal": [129, 144], "anneal": 98, "annot": 52, "annual": 144, "anoth": [52, 53, 54, 55, 69, 78, 79, 80, 87, 92, 98, 99, 107], "anticip": [22, 24, 25, 53, 61, 62, 99, 100, 102, 103, 104, 105], "anticipation_period": [22, 24, 25, 60, 63], "anymor": [54, 80], "aos1161": 129, "aos1230": 129, "aos1671": 129, "ap": [55, 81], "ape_e401_uncond": 55, "ape_p401_uncond": 55, "api": [66, 93, 95, 141, 145], "apo": [29, 30, 66, 106, 119, 137], "apo_result": 66, "apoorva": 145, "apoorva__l": 83, "apoorval": 83, "app": 145, "appdata": 66, "appeal": 89, "append": [66, 69, 78, 87, 92], "appendix": [13, 19, 57, 60, 63, 88, 90, 130, 131], "appli": [8, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 54, 55, 56, 57, 58, 59, 60, 62, 63, 69, 77, 78, 80, 81, 82, 86, 87, 89, 90, 92, 99, 113, 114, 129, 141, 143, 144, 145, 146], "applic": [52, 58, 66, 69, 83, 89, 92, 97, 113, 144, 146], "applicatoin": 61, "apply_along_axi": 84, "apply_cross_fit": [52, 113], "apply_crossfit": 145, "appreci": 141, "approach": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 53, 54, 66, 67, 80, 86, 88, 89, 96, 98, 99, 113, 129, 130, 131, 142, 144, 146], "appropri": [55, 73, 81, 99, 113, 146], "approx": [97, 99, 102, 104], "approxim": [52, 63, 69, 70, 71, 72, 78, 85, 89, 92, 97, 99, 129, 145, 146], "april": 60, "apt": 142, "ar": [4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 69, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 122, 125, 126, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146], "arang": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 59, 69, 72, 82, 84, 85, 88, 89, 98], "arbitrarili": [43, 47], "architectur": [66, 114, 144], "arellano": 144, "arg": [79, 86, 97, 99], "argmax": 87, "argmin": 78, "argu": [52, 55, 69, 81, 82, 88, 92, 146], "argument": [16, 17, 18, 20, 22, 23, 25, 26, 29, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 44, 55, 58, 60, 61, 62, 63, 70, 71, 76, 78, 81, 82, 87, 91, 97, 98, 99, 100, 145, 146], "aris": [52, 53, 54, 61, 69, 80, 89, 92, 146], "aronow": 83, "around": [53, 55, 66, 81, 82, 86, 99, 114], "arr": 84, "arrai": [4, 5, 6, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 45, 46, 47, 57, 58, 60, 61, 62, 63, 66, 67, 69, 70, 71, 72, 78, 80, 83, 84, 87, 88, 89, 90, 91, 92, 97, 98, 113, 129, 130, 136, 143, 145, 146], "arrang": 54, "array_lik": 36, "articl": [18, 141], "arxiv": [16, 53, 54, 80, 87, 89, 141, 144, 145], "as_learn": [56, 98], "asarrai": [70, 71], "aspect": [55, 81, 82], "assert": 98, "assess": 53, "asset": [82, 88, 146], "assign": [4, 5, 6, 25, 55, 60, 63, 66, 75, 81, 86, 97, 98, 99, 100, 112, 130, 146], "assmput": [99, 112], "associ": [55, 73, 81, 99, 129, 144], "assum": [51, 54, 58, 68, 80, 83, 84, 87, 89, 99, 100, 101, 103, 105, 114, 115, 116, 117, 118, 129, 130, 140, 146], "assumpt": [53, 54, 55, 57, 58, 59, 60, 61, 63, 78, 80, 81, 83, 86, 90, 99, 100, 101, 103, 105, 112, 129, 146], "assur": 145, "astyp": [60, 62, 68, 86, 89], "asymptot": [27, 28, 52, 54, 66, 69, 80, 92, 113, 129, 144], "ate": [66, 67], "ate_estim": [57, 90], "ates": [66, 67], "athei": 144, "att": [9, 24, 25, 33, 53, 59, 76, 77, 84, 89, 97, 99, 100, 101, 102, 103, 104, 105, 109, 114, 122, 130, 138, 145], "att_": [99, 103], "att_gt": [53, 61, 62], "attach": 53, "atte_estim": 58, "attempt": [42, 43], "attenu": [55, 81], "attr": 55, "attribut": [4, 5, 6, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 45, 46, 47, 78, 79, 91, 94, 95, 98, 113, 114, 129], "attributeerror": [42, 43], "attrict": 99, "attrit": [37, 57, 90, 99, 112], "au": [56, 98, 141, 143], "auc": 53, "author": [53, 89, 141], "auto_ml": 79, "autodoubleml": 79, "autom": 79, "automat": [21, 24, 52, 60, 62, 63, 69, 76, 92, 97, 130, 136], "automl": 145, "automl_l": 79, "automl_l_lesstim": 79, "automl_m": 79, "automl_m_lesstim": 79, "automobil": [54, 80], "autos": 73, "autosklearn": 79, "auxiliari": [52, 69, 92], "avail": [12, 53, 55, 56, 58, 60, 61, 63, 66, 67, 73, 78, 81, 82, 83, 84, 86, 89, 92, 97, 98, 99, 100, 101, 102, 104, 130, 140, 141, 142, 145, 146], "avaiv": 45, "aver": 67, "averag": [9, 10, 25, 26, 29, 30, 32, 33, 39, 51, 53, 56, 57, 58, 59, 60, 62, 63, 68, 76, 82, 83, 84, 86, 87, 88, 89, 90, 96, 100, 101, 105, 106, 107, 108, 109, 112, 119, 122, 129, 137, 138, 144, 145, 146], "average_it": [66, 67], "avoid": [52, 53, 69, 86, 99, 113, 142, 145], "awai": 88, "ax": [21, 24, 59, 60, 61, 62, 63, 66, 67, 69, 70, 71, 72, 74, 75, 78, 79, 80, 81, 82, 83, 85, 86], "ax1": [66, 67, 72, 77, 82, 85], "ax2": [66, 67, 72, 77, 82, 85], "axhlin": [59, 66, 79, 86], "axi": [21, 24, 54, 55, 66, 67, 73, 77, 78, 80, 81, 83, 84, 86], "axvlin": [66, 67, 69], "b": [18, 20, 22, 23, 24, 25, 52, 54, 56, 69, 70, 71, 80, 83, 85, 86, 89, 92, 97, 98, 99, 105, 129, 130, 140, 141, 143, 144], "b208": 56, "b371": 56, "b5d34a6f42b": 56, "b5d7": 56, "b_": 99, "b_0": 17, "b_1": 17, "b_j": 18, "bach": [73, 78, 79, 89, 141, 144, 145], "back": 66, "backbon": 78, "backend": [4, 5, 6, 53, 82, 88, 89, 93, 94, 96, 145], "backward": 145, "bad": 83, "bag": 66, "balanc": [55, 60, 61, 63, 81, 82], "bam5698": 66, "band": [53, 96, 146], "bandwidth": [34, 35, 36, 40, 86, 99, 145], "bar": [76, 79, 81, 97, 99, 114, 119, 122, 130, 137], "base": [11, 14, 20, 22, 23, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 45, 52, 53, 54, 55, 57, 58, 59, 61, 62, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 97, 98, 99, 100, 103, 105, 112, 113, 114, 129, 130, 131, 136, 141, 143, 144, 145, 146], "base_estim": [46, 47, 86], "baseestim": 87, "baselin": [14, 22, 55, 79, 81], "basi": [29, 33, 39, 44, 63, 70, 71, 77, 97], "basic": [53, 54, 55, 58, 62, 80, 81, 82, 83, 86, 88, 89, 96, 98], "basis_df": 77, "basis_matrix": 77, "batch": 56, "battocchi": 144, "bay": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 129], "bb2913dc": 56, "bbotk": [56, 98, 145], "bbox_inch": 69, "bbox_to_anchor": 69, "bcallaway11": [53, 61], "bd929a9e": 56, "bde4": 56, "becam": [55, 81, 82], "becaus": [43, 47, 51, 52, 53, 54, 68, 69, 75, 76, 80, 83, 87, 89, 92, 146], "becker": [56, 98], "becom": [54, 75, 79, 80, 97, 113], "bee": 59, "been": [21, 24, 54, 55, 60, 62, 63, 79, 80, 81, 82, 88, 89, 97, 98, 99, 105, 141, 145], "befor": [25, 53, 55, 59, 60, 62, 63, 67, 76, 81, 85, 89, 99, 101, 146], "begin": [12, 15, 16, 52, 54, 55, 56, 57, 59, 60, 63, 69, 72, 78, 80, 81, 83, 84, 85, 90, 91, 93, 95, 98, 99, 102, 104, 105, 113, 114, 116, 118, 129, 143, 146], "behav": [60, 62, 63, 75, 87], "behavior": [55, 83, 98], "behaviour": 75, "behind": [61, 99], "being": [14, 19, 25, 27, 28, 41, 46, 47, 54, 63, 80, 86, 89, 99, 104, 105, 113, 114, 120, 129, 130, 136, 141], "belloni": [13, 73, 129, 144], "below": [51, 55, 61, 68, 81, 83, 87, 99, 142, 143], "bench_x1": 89, "bench_x2": 89, "benchmark": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 66, 67, 76, 131, 145], "benchmark_dict": [48, 88], "benchmark_inc": 88, "benchmark_pira": 88, "benchmark_result": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39], "benchmark_twoearn": 88, "benchmarking_set": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 67, 76, 88, 89, 130, 131], "benchmarking_vari": 76, "benefit": [52, 55, 69, 81, 92], "bernoulli": 12, "berri": [54, 80], "besid": 143, "best": [29, 33, 39, 43, 44, 47, 60, 63, 66, 70, 71, 74, 75, 79, 142], "best_loss": 79, "beta": [12, 13, 15, 19, 37, 55, 57, 81, 84, 86, 90, 99], "beta_": [57, 90], "beta_0": [11, 57, 84, 90, 97], "beta_a": [9, 10, 89], "beta_j": [12, 13, 15, 19], "better": [53, 60, 63, 66, 67, 78, 89, 99, 105], "between": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 57, 59, 60, 63, 66, 67, 68, 72, 73, 79, 83, 85, 87, 88, 89, 90, 99, 107, 114, 115, 116, 117, 118, 122, 125, 126, 129, 130, 140, 143, 145], "betwen": [51, 68], "beyond": 144, "bia": [19, 51, 57, 68, 73, 86, 89, 90, 96, 99, 112, 113, 114, 127, 128, 130, 140, 144, 145], "bias": [51, 55, 60, 63, 68, 81, 82, 88, 146], "bias_bench": 89, "bibtex": 141, "big": [73, 91, 113, 114, 115, 116, 123, 129, 130, 132, 133, 134, 135, 138, 139, 140], "bigg": [54, 80, 114, 121, 122, 130, 133, 138], "bilia": 8, "bilinski": 144, "bin": [52, 66, 67, 69, 142], "binari": [11, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 37, 39, 41, 51, 53, 55, 56, 58, 68, 76, 77, 78, 81, 83, 84, 89, 97, 98, 101, 105, 108, 109, 112, 130, 137, 138, 145, 146], "binary_outcom": 41, "binary_treat": [11, 70, 74, 76], "bind": 145, "binder": [56, 98, 141, 143, 145], "binomi": [68, 83, 84, 85, 87], "bischl": [56, 98, 141, 143], "black": [52, 56, 64, 93, 95, 143], "blob": 53, "blog": 18, "blondel": [141, 143], "blp": [44, 54, 80], "blp_data": [54, 80], "blp_model": [74, 75], "blue": [52, 54, 57, 80], "bodori": 144, "bond": [55, 81, 82], "bonferroni": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 129], "bonu": [8, 56, 93, 95, 143], "book": [56, 87, 89, 98, 144], "bool": [4, 5, 6, 9, 11, 14, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 76, 86], "boolean": [19, 74, 75, 93, 95, 113], "boost": [51, 55, 58, 60, 63, 66, 68, 78, 81], "boost_class": [55, 81], "boost_summari": 81, "boostrap": [72, 145], "bootstrap": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 60, 61, 62, 63, 67, 70, 71, 72, 74, 75, 82, 85, 96, 97, 99, 100, 113, 114, 141, 143, 145, 146], "both": [10, 11, 20, 21, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 55, 56, 58, 59, 66, 77, 78, 79, 81, 82, 84, 86, 87, 88, 89, 93, 95, 98, 99, 102, 103, 104, 129, 130, 131, 136, 139, 140, 145, 146], "bottom": [54, 55, 78, 80, 81, 82], "bound": [14, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 60, 63, 66, 67, 76, 77, 81, 88, 89, 130, 131, 136, 140, 145, 146], "branch": 56, "brantli": [53, 144], "break": [52, 145], "breviti": 146, "brew": 142, "brewer": 54, "bridg": 89, "brief": 92, "briefli": 61, "bring": [51, 68], "brucher": [141, 143], "bsd": [141, 145], "bst": 81, "budget": [79, 98], "bug": [141, 145], "build": [54, 78, 80, 84], "build_design_matric": [70, 71], "build_sim_dataset": 53, "built": [45, 79, 98, 141], "bureau": [89, 113, 144], "busi": [16, 19, 54, 80, 89, 144], "b\u00fchlmann": 144, "c": [7, 8, 10, 13, 15, 17, 25, 26, 51, 52, 53, 54, 55, 56, 59, 64, 66, 68, 69, 73, 74, 75, 80, 81, 83, 86, 87, 92, 93, 95, 98, 99, 105, 114, 116, 118, 130, 133, 135, 141, 142, 143, 144, 146], "c1": [7, 8, 17, 54, 73, 80, 87, 92, 141, 144], "c68": [7, 8, 17, 54, 73, 80, 87, 92, 141, 144], "c895": 56, "c_": [24, 99, 102, 104, 105, 114, 116, 118, 129], "c_d": [13, 130, 138, 139, 140], "c_i": [99, 105], "c_y": [13, 130, 140], "ca1af7be64b2": 56, "caac5a95": 56, "calcualt": 84, "calcul": [29, 33, 39, 53, 55, 60, 63, 66, 67, 70, 71, 72, 74, 75, 78, 79, 81, 85, 88, 130, 136, 140], "calendar": [60, 61, 62, 63], "calibr": [78, 79, 89], "call": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 43, 46, 47, 51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 66, 68, 70, 71, 72, 74, 75, 80, 81, 82, 84, 85, 86, 88, 89, 90, 93, 95, 98, 113, 114, 129, 130, 136, 140, 143, 145, 146], "callabl": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 69, 70, 71, 78, 96, 98, 141], "callawai": [25, 53, 60, 61, 62, 63, 99, 100, 103, 105, 144], "camera": 73, "cameron": [54, 80], "can": [4, 5, 6, 9, 11, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 45, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 103, 105, 106, 112, 113, 114, 115, 116, 117, 118, 119, 122, 125, 126, 129, 130, 131, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146], "candid": 89, "cannot": [78, 86, 89, 99, 146], "capabl": [4, 5, 6, 51, 68], "capo": [29, 77], "capo0": 77, "capo1": 77, "capsiz": [66, 67, 79, 83, 86], "capthick": [66, 67, 86], "cardin": [54, 80], "care": [61, 87, 98], "carlo": [9, 10, 11, 14, 70, 71, 74, 75, 89, 144], "casalicchio": [56, 98, 141, 143], "case": [4, 5, 6, 8, 11, 22, 29, 32, 33, 40, 51, 54, 55, 60, 61, 63, 68, 70, 71, 72, 73, 75, 76, 77, 79, 80, 84, 85, 86, 87, 88, 89, 93, 95, 97, 98, 99, 101, 105, 112, 113, 114, 116, 118, 129, 130, 136, 143, 145, 146], "cat": [52, 145], "catboost": 78, "cate": [33, 39, 44, 77, 96, 145], "cate_obj": 97, "categori": 66, "cattaneo": [99, 144], "caus": [52, 69, 86, 92], "causal": [12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 54, 55, 56, 57, 68, 69, 77, 78, 79, 80, 81, 83, 87, 88, 90, 91, 92, 93, 95, 96, 99, 105, 113, 129, 130, 136, 144], "causal_contrast": [30, 66, 67, 77, 99], "causal_contrast_att": 77, "causal_contrast_c": 77, "causal_contrast_model": [66, 67, 99], "causal_contrast_result": 66, "causaldml": 144, "causalml": [87, 144], "causalweight": 144, "caution": 129, "caveat": [75, 89], "cbind": 54, "cbook": [60, 61, 62, 63], "cc": 81, "ccp_alpha": [33, 45, 81], "cd": 142, "cd_fast": 80, "cda85647": 56, "cdf": 97, "cdid": [54, 80], "cdot": [9, 10, 14, 25, 26, 41, 54, 59, 60, 63, 72, 76, 80, 83, 85, 86, 89, 97, 99, 100, 102, 104, 105, 113, 114, 116, 118, 119, 122, 123, 127, 128, 129, 130, 133, 135, 137], "cdot1": 76, "cell": 79, "center": [60, 61, 63, 66, 73], "central": [113, 145], "certain": [75, 99, 114, 116, 118], "cexcol": 54, "cexrow": 54, "cf_d": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 60, 63, 67, 76, 77, 88, 89, 130, 131, 136, 137, 138, 139, 140, 146], "cf_y": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 60, 63, 67, 76, 77, 88, 89, 130, 131, 136, 137, 138, 139, 140, 146], "cff": 145, "chad": 89, "chain": 75, "chainedassignmenterror": 75, "challeng": [54, 80, 130, 131], "chanc": 63, "chang": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 55, 57, 58, 62, 75, 82, 88, 89, 90, 99, 104, 105, 114, 118, 122, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 142, 144, 145], "channel": 146, "chapter": [27, 28, 56, 87, 98, 130, 140], "charact": [55, 56, 98, 145], "characterist": [88, 146], "chart": 79, "check": [42, 43, 46, 47, 52, 55, 58, 59, 60, 63, 69, 78, 79, 81, 82, 87, 91, 92, 99, 100, 141, 142, 145], "check_data": 145, "check_scor": 145, "checkmat": 145, "chernozhukov": [7, 8, 13, 15, 17, 52, 54, 55, 69, 73, 78, 79, 80, 81, 82, 87, 88, 92, 113, 114, 122, 129, 130, 131, 140, 141, 144, 145], "chetverikov": [7, 8, 17, 54, 73, 80, 87, 92, 129, 141, 144], "chiang": [16, 54, 80, 144], "chieh": 144, "choic": [4, 5, 6, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 55, 60, 61, 62, 63, 73, 81, 84, 97, 98, 99, 103, 114, 116, 118, 130, 131, 136, 140, 145], "cholecyst": 87, "choos": [51, 55, 61, 68, 69, 73, 78, 81, 82, 91, 99, 103, 113, 114, 115, 116, 117, 118, 122, 125, 126, 129, 143, 146], "chosen": [10, 14, 29, 78, 98, 99], "chou": 83, "chr": 55, "christian": [73, 144], "christoph": 144, "chunk": 98, "ci": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 58, 59, 60, 61, 62, 63, 66, 67, 70, 71, 72, 74, 75, 76, 77, 79, 81, 82, 85, 86, 88, 89, 97, 99, 130, 136, 145, 146], "ci_at": [66, 67], "ci_cvar": [72, 82], "ci_cvar_0": 72, "ci_cvar_1": 72, "ci_joint": [60, 61, 62, 63, 67], "ci_joint_cvar": 72, "ci_joint_lqt": 85, "ci_joint_qt": 85, "ci_length": 58, "ci_low": [66, 67], "ci_lpq_0": 85, "ci_lpq_1": 85, "ci_lqt": [82, 85], "ci_pointwis": [66, 67], "ci_pq_0": [82, 85], "ci_pq_1": [82, 85], "ci_qt": [82, 85], "ci_upp": [66, 67], "cinelli": [89, 130, 131, 144], "circumv": 146, "citat": 145, "cite": 141, "claim": 56, "clarifi": [60, 61, 62, 63], "clash": 53, "class": [0, 4, 5, 6, 20, 21, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 76, 77, 79, 81, 82, 87, 88, 90, 91, 93, 94, 95, 97, 98, 99, 100, 113, 114, 129, 141, 143, 145], "class_estim": 86, "class_learn": 82, "class_learner_1": 78, "class_learner_2": 78, "classes_": [79, 87], "classic": [53, 54, 61, 80, 146], "classif": [33, 42, 46, 51, 53, 55, 56, 57, 60, 61, 62, 63, 78, 79, 84, 88, 97, 98, 99, 101, 102, 104, 146], "classifavg": 56, "classifi": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 39, 40, 42, 46, 56, 66, 67, 79, 86, 98, 145], "classifiermixin": 87, "classmethod": [4, 5, 6], "claudia": [144, 145], "claus": [141, 145], "clean": 145, "cleaner": 78, "cleanup": 145, "clear": [54, 66, 80], "clearli": [60, 63, 86], "clever": 78, "client": 66, "clone": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 56, 69, 78, 80, 82, 91, 98, 99, 113, 114, 129, 130, 136, 142, 143], "close": [53, 55, 66, 81, 87, 89, 130, 131], "cluster": [4, 16, 20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 62, 144, 145], "cluster_col": [4, 54, 80], "cluster_var": [4, 16], "cluster_var_i": [4, 54, 80], "cluster_var_j": [4, 54, 80], "cmap": 80, "cmd": 145, "co": [18, 59], "codaci": 145, "code": [18, 29, 33, 39, 51, 53, 54, 55, 56, 57, 68, 73, 81, 92, 97, 98, 99, 113, 114, 129, 142, 143, 145, 146], "codecov": 145, "coef": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 143, 146], "coef_": 89, "coef_df": 54, "coef_valu": 79, "coeffici": [9, 10, 11, 24, 43, 44, 47, 55, 57, 74, 75, 78, 81, 83, 84, 86, 89, 90, 97, 129, 130, 136, 146], "coefs_t": 84, "coefs_w": 84, "coffici": [130, 136], "cofid": 44, "coincid": [59, 77, 82], "col": [52, 54, 75, 81], "col_nam": [60, 63], "collect": [56, 57, 58, 66, 80, 90], "colnam": [54, 78], "color": [21, 24, 55, 57, 59, 63, 66, 67, 69, 70, 71, 72, 79, 80, 81, 82, 83, 85, 86, 89], "color_palett": [21, 24, 60, 63, 66, 67, 69, 80, 81, 82], "colorbar": 80, "colorblind": [21, 24, 60, 63, 66, 67], "colorramppalett": 54, "colorscal": [70, 71], "colour": [52, 54], "column": [4, 5, 6, 58, 59, 60, 61, 62, 63, 64, 66, 67, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 86, 88, 89, 90, 93, 94, 95, 97, 98, 99, 102, 104, 113, 143, 145, 146], "column_stack": [59, 66, 67, 74, 75, 86, 88, 89, 99], "colv": 54, "com": [18, 53, 55, 56, 61, 62, 73, 83, 89, 98, 141, 142], "comb": 73, "combin": [22, 24, 53, 54, 56, 58, 61, 62, 66, 67, 77, 78, 79, 80, 89, 98, 99, 103, 113, 130, 136, 145], "combind": 82, "combined_loss": 73, "come": [91, 98, 114, 130, 131, 141, 146], "command": [142, 145], "comment": [93, 95], "commit": 145, "common": [78, 88, 89, 97, 99, 144], "commonli": 66, "companion": 144, "compar": [52, 54, 59, 60, 63, 66, 69, 70, 71, 72, 74, 75, 77, 80, 83, 85, 86, 87, 89, 92, 98, 99, 130, 131, 145], "comparevers": 55, "comparison": [60, 63, 67, 78, 83], "compat": [51, 53, 68, 145], "complement": 89, "complet": [79, 92, 130, 136, 142], "complex": [33, 53, 79], "compli": [86, 99], "complianc": [85, 86, 99, 114, 123], "complic": [56, 146], "complier": [55, 81, 82, 85, 86, 97, 99], "compon": [25, 46, 47, 53, 55, 61, 66, 73, 78, 79, 81, 84, 97, 98, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 125, 126, 145], "compont": 53, "composit": 144, "comprehens": 66, "compris": 129, "comput": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 48, 52, 53, 55, 56, 60, 61, 62, 63, 66, 69, 81, 82, 87, 88, 89, 113, 114, 130, 131, 132, 133, 134, 135, 136, 137, 138, 141, 144, 145, 146], "computation": [130, 131], "concat": [66, 79, 80, 81, 84, 129], "concaten": [59, 81, 129], "concentr": 129, "concern": 89, "conclud": [86, 89, 146], "cond": [99, 101, 112], "conda": [80, 144, 145], "condit": [9, 10, 11, 27, 28, 29, 31, 33, 39, 52, 54, 55, 57, 58, 59, 60, 63, 67, 69, 76, 77, 80, 81, 84, 86, 89, 90, 92, 96, 99, 102, 103, 104, 105, 129, 130, 137, 138, 140, 143, 144, 145, 146], "conduct": [97, 99, 101, 102, 104, 146], "conf": [53, 85], "confer": 144, "confid": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 53, 54, 55, 57, 58, 60, 61, 62, 63, 66, 67, 70, 71, 72, 74, 75, 77, 80, 82, 85, 86, 88, 90, 96, 97, 99, 113, 114, 130, 136, 143, 144, 145, 146], "confidenceband": 72, "confidenti": 89, "config": 83, "configur": [56, 79], "confint": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 55, 57, 58, 59, 60, 61, 62, 63, 66, 67, 70, 71, 72, 74, 75, 77, 78, 82, 84, 85, 86, 87, 88, 90, 97, 113, 129, 141, 143, 146], "conflict": 142, "confound": [9, 10, 11, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 51, 55, 60, 61, 63, 68, 76, 81, 85, 88, 89, 93, 95, 99, 110, 111, 129, 130, 131, 136, 139, 140, 143, 144, 145, 146], "congress": 144, "connect": [55, 81, 82], "consequ": [9, 10, 54, 76, 80, 88, 97, 99, 101, 130, 131, 137, 138, 140], "conserv": [88, 89, 130, 140], "consid": [31, 32, 33, 34, 35, 41, 52, 54, 55, 57, 58, 59, 60, 61, 62, 63, 69, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 97, 98, 99, 100, 105, 108, 109, 113, 114, 129, 130, 131, 141, 146], "consider": [89, 99], "consist": [38, 39, 43, 47, 55, 58, 79, 81, 82, 83, 89, 92, 93, 95, 99, 105, 110, 111, 143, 145], "consol": [52, 145], "constant": [13, 25, 43, 47, 60, 63, 73, 84, 97, 99, 129], "constrained_layout": 69, "construct": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 37, 38, 39, 44, 56, 59, 60, 63, 70, 71, 72, 77, 82, 87, 88, 91, 97, 114, 120, 128, 129, 145, 146], "construct_framework": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "construct_iv": 80, "constructiv": 54, "constructor": 56, "consum": [54, 80], "cont": 14, "cont_d": [66, 67], "contain": [14, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 46, 47, 52, 54, 55, 60, 61, 63, 67, 69, 70, 71, 74, 75, 78, 80, 81, 87, 92, 94, 95, 97, 98, 99, 100, 102, 104, 129, 130, 131, 136, 145], "context": [89, 99, 112, 146], "contin": [14, 79], "continu": [14, 51, 56, 66, 67, 68, 73, 83, 86, 99, 130, 140, 145, 146], "contour": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 73, 76, 88, 89, 130, 136], "contour_plot": 89, "contours_z": [70, 71], "contrast": [30, 58, 66, 72, 77, 99, 107], "contribut": [142, 145], "contributor": 145, "control": [15, 22, 24, 25, 41, 53, 61, 62, 73, 77, 82, 84, 86, 87, 89, 99, 100, 102, 103, 104, 105, 114, 116, 118, 130, 133, 135, 146], "control_group": [22, 24, 60, 61, 62, 63, 99, 100, 102, 104], "convent": [40, 55, 61, 81, 82, 86, 99, 105], "converg": [52, 69, 78, 80, 92], "convergencewarn": 80, "convers": 80, "convert": [66, 72, 80, 85], "convex": 83, "cooper": 145, "coor": [56, 98, 141, 143], "coordin": [66, 89], "copi": [63, 75, 79, 81, 84, 89], "cor": [130, 140], "core": [25, 58, 60, 61, 62, 63, 64, 66, 67, 72, 76, 80, 81, 82, 85, 88, 90, 93, 94, 95, 98, 143, 145], "cores_us": [72, 82, 85], "correct": [76, 77, 89, 97, 129, 145], "correctli": [42, 46, 58, 83, 88, 130, 140], "correl": [20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 54, 57, 73, 80, 87, 88, 90, 99, 130, 131, 140], "correpond": [60, 63, 99], "correspond": [9, 10, 14, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 55, 56, 58, 59, 60, 61, 62, 63, 67, 69, 70, 71, 73, 77, 78, 80, 81, 82, 84, 85, 88, 89, 92, 97, 98, 99, 101, 103, 105, 106, 112, 113, 114, 116, 118, 129, 130, 131, 133, 135, 136, 138, 140, 145, 146], "correspondingli": [60, 63], "cosh": 18, "coul": 54, "could": [51, 56, 60, 61, 63, 68, 70, 71, 79, 89, 145, 146], "counfound": [9, 10, 85, 88, 97, 130, 140], "count": [66, 67, 81, 82], "counti": 61, "countour": [130, 136], "countyr": 61, "coupl": [55, 81, 82], "cournapeau": [141, 143], "cours": [55, 78, 81, 89, 129, 146], "cov": [9, 37, 41, 86], "cov_nam": [86, 99], "cov_typ": [29, 33, 39, 44, 145], "covari": [4, 5, 6, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 33, 38, 39, 40, 41, 44, 45, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 64, 66, 67, 69, 70, 71, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 87, 88, 89, 90, 92, 93, 94, 95, 97, 98, 99, 101, 102, 104, 110, 111, 112, 114, 115, 116, 117, 118, 129, 130, 131, 143, 144, 145], "cover": [53, 73, 88], "coverag": [60, 63, 78, 86, 87, 97, 145], "cp": [55, 56, 98], "cpu": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 66], "cpu_count": [72, 82, 85], "cran": [56, 144, 145], "creat": [11, 21, 24, 25, 51, 54, 56, 60, 63, 67, 68, 69, 70, 71, 72, 74, 75, 80, 82, 84, 85, 89, 98, 130, 131, 136, 140, 142, 145], "create_synthetic_group_data": 84, "crictial": 113, "critic": [89, 146], "cross": [20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 53, 55, 56, 57, 61, 66, 69, 78, 79, 81, 82, 86, 89, 92, 96, 98, 102, 103, 105, 117, 128, 129, 132, 133, 136, 145, 146], "cross_sectional_data": [23, 26, 58, 99, 101], "crossfit": [78, 99], "crosstab": 83, "crucial": [73, 99, 146], "csail": [141, 143], "csdid": 62, "csv": [62, 73], "cuda": 66, "cumul": 99, "current": [45, 53, 58, 59, 60, 63, 75, 77, 114, 130, 140, 141, 142, 146], "custom": [21, 24, 52, 53, 61, 69, 89, 98], "custom_measur": 53, "cut": 84, "cutoff": [40, 41, 86, 99], "cv": [56, 81, 98, 113], "cv_glmnet": [54, 55, 56, 57, 98, 99, 129, 143], "cvar": [31, 36, 96, 120, 145], "cvar_0": 72, "cvar_1": 72, "d": [4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 104, 106, 108, 109, 110, 111, 112, 113, 114, 115, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 136, 137, 138, 139, 140, 141, 143, 144, 146], "d0": [72, 85, 129], "d0_true": 85, "d0cdb0ea4795": 56, "d1": [72, 83, 85, 129], "d10": 129, "d1_true": 85, "d2": [83, 129], "d21ee5775b5f": 56, "d2cml": 62, "d3": 129, "d4": 129, "d5": 129, "d5a0c70f1d98": 56, "d6": 129, "d7": 129, "d8": 129, "d9": 129, "d_": [14, 16, 54, 59, 67, 80, 99, 101, 105, 129], "d_0": [99, 106], "d_1": [83, 129], "d_2": 83, "d_col": [4, 5, 6, 24, 51, 52, 54, 55, 56, 57, 60, 61, 62, 63, 68, 70, 71, 74, 75, 77, 80, 81, 82, 84, 86, 87, 88, 91, 92, 93, 94, 95, 98, 99, 100, 102, 104, 113, 114, 143, 145, 146], "d_i": [11, 12, 13, 15, 17, 18, 19, 52, 57, 58, 67, 69, 72, 83, 85, 86, 90, 92, 99, 101, 112], "d_j": [67, 99, 106, 107, 129], "d_k": [99, 107, 129], "d_l": [99, 106], "d_w": 84, "da1440": 83, "dag": [57, 89, 90, 146], "dai": 60, "dark": [52, 69], "darkblu": 54, "darkr": 54, "dash": [57, 60, 63, 66, 67], "dat": [93, 95], "data": [0, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 46, 47, 53, 59, 73, 78, 83, 87, 91, 93, 94, 96, 97, 98, 100, 102, 103, 104, 105, 113, 129, 133, 134, 135, 136, 144, 145], "data_apo": [66, 67], "data_cvar": 82, "data_dict": [40, 70, 71, 74, 75, 76, 86, 99], "data_dml": 88, "data_dml_bas": [55, 70, 71, 74, 75, 81, 82, 84], "data_dml_base_iv": [55, 81, 82], "data_dml_flex": [55, 81], "data_dml_flex_iv": 55, "data_dml_iv_flex": 81, "data_dml_new": 84, "data_fram": 146, "data_lqt": 82, "data_pq": 82, "data_qt": 82, "data_transf": [54, 80, 81], "datafram": [4, 5, 6, 7, 8, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 45, 54, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 94, 97, 98, 99, 101, 114, 129, 130, 131, 136, 143, 146], "dataset": [0, 4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 57, 58, 60, 63, 66, 67, 69, 70, 71, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 130, 136, 143, 146], "datatyp": [62, 145], "date": [24, 25, 98], "date_format": 24, "datetim": [6, 24, 25, 60, 61, 63, 94, 95], "datetime64": [60, 94, 95], "datetime_unit": [6, 24, 60, 63, 94, 95, 99, 100, 102, 104], "david": 145, "db": [55, 81, 82, 88, 146], "dbl": [53, 54, 55, 56, 93, 95, 129, 143, 146], "dc13a11076b3": 56, "ddc9": 56, "de": [51, 68, 144], "deal": [51, 68], "debias": [7, 8, 16, 17, 54, 73, 80, 87, 96, 98, 113, 141, 144, 145], "debt": [55, 81, 82], "decai": [57, 90], "decid": [55, 81, 87], "decis": [33, 51, 55, 68, 81, 82, 97, 99, 144, 146], "decision_effect": 51, "decision_impact": [51, 68], "decisiontreeclassifi": [33, 45, 81], "decisiontreeregressor": 81, "declar": 146, "decomposit": [99, 100], "decreas": 86, "deep": [42, 43, 46, 47, 79], "deeper": 33, "def": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 60, 63, 69, 72, 78, 79, 80, 83, 84, 85, 87, 89, 98, 114], "default": [4, 5, 6, 9, 10, 11, 14, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 53, 54, 57, 58, 60, 61, 62, 63, 74, 75, 78, 80, 84, 86, 88, 89, 90, 91, 97, 98, 99, 113, 129, 130, 136, 137, 143, 146], "default_arg": [60, 63], "default_convert": 80, "default_jitt": 24, "defier": [86, 99], "defin": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 40, 41, 43, 47, 52, 55, 56, 58, 61, 66, 67, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 81, 82, 84, 85, 86, 87, 88, 89, 97, 98, 99, 101, 102, 104, 105, 112, 114, 115, 117, 118, 130, 131, 136, 140, 145], "definit": [18, 60, 63, 74, 75, 77, 99, 103, 130, 137, 138], "defint": 130, "degre": [41, 55, 70, 71, 77, 80, 81, 86, 97, 130, 131], "dekel": 144, "delete_origin": 56, "deliber": 83, "delta": [15, 24, 53, 58, 60, 63, 89, 99, 101, 102, 103, 104, 105, 114, 116, 118], "delta_bench": 89, "delta_i": 53, "delta_j": 15, "delta_t": [25, 60, 63], "delta_theta": [48, 67, 76, 88, 89, 130, 131], "delta_v": 89, "demand": [54, 80, 130, 131], "demir": [7, 8, 17, 54, 73, 80, 87, 92, 113, 141, 144], "demo": [61, 89], "demonstr": [52, 53, 54, 61, 66, 69, 80, 86, 89, 93, 95, 99, 129, 141, 143], "deni": 144, "denomin": [130, 131, 137, 138], "denot": [38, 54, 55, 57, 58, 59, 63, 80, 81, 86, 89, 90, 97, 99, 101, 102, 104, 105, 106, 110, 114, 130, 131, 136, 138, 140], "dens_net_tfa": 55, "densiti": [34, 35, 36, 52, 57, 66, 67, 69], "dep": 64, "dep1": [56, 64, 93, 95, 143], "dep2": [56, 64, 93, 95, 143], "depend": [11, 25, 29, 31, 33, 34, 36, 56, 58, 60, 63, 70, 71, 74, 75, 76, 78, 79, 84, 86, 91, 97, 98, 99, 102, 103, 104, 114, 123, 124, 130, 131, 137, 140, 143, 144], "deprec": [58, 59, 60, 61, 62, 63, 91, 99, 113, 114, 130], "depreci": 145, "depth": [33, 45, 55, 56, 84, 91, 97, 98, 99, 113, 114, 129, 143, 146], "deriv": [20, 22, 23, 25, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 99, 129], "describ": [21, 53, 54, 60, 63, 80, 81, 82, 89, 98, 113, 142, 145], "descript": [55, 62, 64, 88, 98, 113, 114, 116, 118, 130, 131, 133, 135], "deserv": 99, "design": [25, 40, 41, 66, 67, 79, 96, 144, 145], "design_info": [70, 71], "design_matrix": [70, 71, 97], "desir": [10, 56, 84, 99, 142], "detail": [20, 22, 23, 27, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 52, 55, 56, 58, 59, 66, 67, 69, 73, 79, 82, 86, 88, 89, 92, 93, 95, 97, 98, 100, 101, 102, 103, 104, 105, 112, 114, 120, 122, 123, 124, 127, 128, 129, 130, 131, 133, 135, 140, 141, 142, 143, 145, 146], "determin": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 47, 55, 63, 72, 81, 82, 85, 86, 88, 99, 129, 130, 140], "determinist": [84, 86, 97, 99], "deutsch": 141, "dev": [142, 145], "develop": [53, 54, 56, 80, 89, 99, 101, 105, 145], "deviat": [78, 99, 130, 140], "devic": 66, "dezeur": 144, "df": [4, 5, 6, 24, 51, 52, 54, 57, 59, 60, 63, 66, 67, 68, 70, 71, 72, 75, 77, 80, 83, 85, 86, 88, 89, 90, 92, 94, 95, 97, 99, 100, 102, 104], "df_agg": 73, "df_all_apo": 66, "df_all_at": 66, "df_anticip": [60, 63], "df_apo": [66, 67], "df_apo_ci": 67, "df_apos_ci": 67, "df_ate": [66, 67], "df_bench": 89, "df_binari": 89, "df_bonu": [56, 93, 95, 143], "df_capo0": 77, "df_capo1": 77, "df_cate": [70, 71, 77], "df_causal_contrast_c": 77, "df_ci": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44], "df_coef": 78, "df_cvar": 82, "df_fuzzi": 86, "df_lqte": 82, "df_ml_g0": 78, "df_ml_g1": 78, "df_ml_m": 78, "df_pa": [58, 90], "df_perform": 66, "df_plot": 54, "df_post_treat": [60, 63], "df_pq": 82, "df_qte": 82, "df_result": 73, "df_sharp": 86, "df_sort": [66, 67], "df_summari": 81, "df_treat": 63, "df_wide": 80, "dfg": 141, "dgp": [25, 26, 54, 57, 59, 60, 63, 72, 73, 80, 83, 84, 85, 89, 90], "dgp1": [25, 26], "dgp2": [25, 26], "dgp3": [25, 26], "dgp4": [25, 26], "dgp5": [25, 26], "dgp6": [25, 26], "dgp_dict": 89, "dgp_tpye": 58, "dgp_type": [25, 26, 58, 60, 63], "diagnost": 61, "diagon": 89, "diagram": [51, 68, 99], "dichotom": [51, 68], "dict": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 45, 46, 47, 48, 60, 63, 70, 71, 73, 79, 89, 98], "dict_kei": [130, 136], "dictionari": [9, 10, 11, 14, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 48, 60, 63, 70, 71, 74, 75, 88, 97, 98, 99, 100, 130, 136], "dictonari": [55, 81], "did": [0, 4, 5, 6, 52, 58, 59, 60, 61, 62, 63, 80, 94, 95, 96, 100, 101, 102, 104, 105, 114, 116, 118, 145, 146], "did_aggreg": [60, 62, 63], "did_multi": [99, 100], "diff": 81, "differ": [9, 10, 11, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 51, 52, 54, 55, 56, 57, 60, 62, 63, 66, 67, 68, 69, 72, 74, 75, 76, 77, 79, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 96, 97, 98, 100, 101, 102, 104, 105, 113, 115, 116, 117, 118, 142, 143, 144, 145, 146], "differenti": 99, "difficult": 89, "dillon": 144, "dim": [41, 55], "dim_x": [12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 25, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 52, 54, 56, 69, 77, 78, 79, 80, 92, 97, 98, 99, 130, 136], "dim_z": [15, 38, 99], "dimens": [11, 16, 25, 54, 80, 84, 113], "dimension": [11, 13, 21, 38, 39, 73, 87, 97, 99, 110, 111, 113, 129, 130, 136, 143, 144], "direct": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 57, 59, 69, 77, 90, 92, 99, 146], "directli": [40, 52, 53, 55, 67, 69, 78, 88, 92, 130, 136, 143, 146], "discontinu": [40, 41, 96, 144, 145], "discret": [14, 30, 66, 67, 80, 99, 106, 145], "discretis": 82, "discuss": [12, 54, 55, 80, 81, 99, 100, 105, 144, 145, 146], "disjoint": [54, 74, 75, 80], "displai": [21, 54, 60, 61, 62, 63, 66, 67, 80, 89, 97, 98, 130, 136], "displot": 81, "disproportion": [55, 81], "disregard": [43, 47], "dist": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "distinguish": [24, 60, 63], "distr": 98, "distribut": [41, 52, 58, 66, 67, 69, 78, 89, 92, 99, 101, 105, 130, 138, 142, 144, 145], "diverg": [40, 52, 69, 92], "divid": [60, 63, 99], "dmatrix": [70, 71, 97], "dml": [20, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 56, 57, 58, 61, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 114, 129, 130, 136, 142], "dml1": [96, 143, 145, 146], "dml2": [51, 54, 56, 57, 58, 64, 80, 82, 96, 99, 114, 129, 143, 145, 146], "dml_apo": 77, "dml_apo_obj": 99, "dml_apos_att": 77, "dml_apos_obj": 99, "dml_base": 80, "dml_combin": 129, "dml_cover": 87, "dml_cv_predict": 145, "dml_cvar": [72, 82], "dml_cvar_0": 72, "dml_cvar_1": 72, "dml_cvar_obj": [31, 97], "dml_data": [6, 24, 53, 54, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 76, 77, 78, 80, 83, 87, 88, 89, 90, 94, 95, 97, 98, 99, 100, 102, 104, 129, 146], "dml_data_anticip": [60, 63], "dml_data_bench": 89, "dml_data_bonu": [56, 143], "dml_data_df": 146, "dml_data_fuzzi": 86, "dml_data_lasso": 64, "dml_data_sharp": 86, "dml_data_sim": [56, 143], "dml_df": [54, 80], "dml_did": [58, 59], "dml_did_obj": [20, 23, 24, 99, 100, 101, 102, 104], "dml_doc": 66, "dml_iivm": 87, "dml_iivm_boost": [55, 81], "dml_iivm_forest": [55, 81], "dml_iivm_lasso": [55, 81], "dml_iivm_obj": [32, 68, 99], "dml_iivm_tre": [55, 81], "dml_irm": [70, 74, 77, 78, 84], "dml_irm_at": 76, "dml_irm_att": 77, "dml_irm_boost": [55, 81], "dml_irm_forest": [55, 81], "dml_irm_gat": 76, "dml_irm_gatet": 76, "dml_irm_lasso": [55, 64, 81], "dml_irm_new": 84, "dml_irm_obj": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 88, 97, 98, 99], "dml_irm_obj_ext": 98, "dml_irm_rf": 64, "dml_irm_tre": [55, 81], "dml_irm_weighted_att": 77, "dml_kwarg": 77, "dml_length": 87, "dml_long": 48, "dml_lpq_0": 85, "dml_lpq_1": 85, "dml_lpq_obj": [34, 97], "dml_lqte": [82, 85], "dml_obj": [53, 60, 61, 62, 63, 66, 67, 88, 89], "dml_obj_al": [60, 63], "dml_obj_anticip": [60, 63], "dml_obj_bench": 89, "dml_obj_lasso": 61, "dml_obj_linear": [60, 63], "dml_obj_linear_logist": 61, "dml_obj_nyt": [60, 63], "dml_obj_univers": [60, 63], "dml_pliv": [54, 80], "dml_pliv_obj": [38, 54, 80, 99], "dml_plr": [71, 75, 129], "dml_plr_1": 129, "dml_plr_2": 129, "dml_plr_boost": [55, 81], "dml_plr_forest": [55, 81, 146], "dml_plr_lasso": [55, 64, 81], "dml_plr_no_split": 113, "dml_plr_obj": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 88, 91, 97, 98, 99, 113, 114, 129, 130, 131, 136], "dml_plr_obj_extern": 113, "dml_plr_obj_intern": 113, "dml_plr_obj_onfold": 79, "dml_plr_obj_untun": 79, "dml_plr_rf": 64, "dml_plr_tree": [55, 81, 146], "dml_pq_0": [82, 85], "dml_pq_1": [82, 85], "dml_pq_obj": [35, 97], "dml_procedur": [64, 91, 143, 145, 146], "dml_qte": [82, 85], "dml_qte_obj": [36, 97], "dml_robust_confset": 87, "dml_robust_length": 87, "dml_short": 48, "dml_ssm": [57, 90, 99], "dml_standard_ci": 87, "dml_tune": 145, "dmldummyclassifi": 98, "dmldummyregressor": 98, "dmlmt": 144, "dnorm": 52, "do": [53, 54, 55, 56, 60, 61, 63, 77, 78, 80, 81, 82, 83, 89, 97, 98, 113, 130, 140, 143, 146], "doabl": 114, "doc": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 141, 145], "docu": 145, "document": [59, 60, 61, 62, 63, 65, 70, 71, 74, 75, 77, 79, 89, 99, 105, 114, 130, 141, 145], "doe": [24, 30, 36, 53, 54, 55, 66, 67, 77, 80, 81, 83, 88, 89, 99, 114, 118, 130, 140, 146], "doesn": [51, 68], "doi": [7, 8, 9, 10, 12, 16, 17, 19, 25, 26, 53, 54, 56, 73, 80, 89, 92, 98, 113, 129, 141, 143, 145], "domain": 84, "don": [53, 79], "done": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 61, 79, 82, 98, 113, 130, 131], "dosag": [66, 67], "dot": [37, 59, 60, 63, 84, 93, 95, 97, 98, 99, 103, 105, 106, 129, 143], "doubl": [4, 5, 6, 7, 8, 16, 17, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 55, 61, 73, 78, 79, 81, 83, 87, 96, 98, 113, 114, 129, 130, 131, 145], "double_ml": [60, 66], "double_ml_bonus_data": 64, "double_ml_data_from_data_fram": [52, 92, 93, 95, 146], "double_ml_data_from_matrix": [53, 56, 93, 95, 98, 129, 143], "double_ml_framework": [99, 100], "double_ml_irm": [64, 84], "double_ml_score_mixin": 0, "doubleiivm": 141, "doubleml": [0, 52, 54, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 82, 83, 84, 85, 86, 87, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 113, 114, 130, 136, 143, 144, 145], "doubleml2022python": 141, "doubleml2024r": 141, "doubleml_did_eval_linear": 53, "doubleml_did_eval_rf": 53, "doubleml_did_linear": 53, "doubleml_did_rf": 53, "doubleml_framework": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "doublemlapo": [66, 67, 77, 99, 114, 119, 145], "doublemlblp": [29, 33, 39, 70, 71, 77, 97, 145], "doublemlclusterdata": [0, 16], "doublemlcvar": [72, 97, 114, 120, 145], "doublemldata": [0, 6, 7, 8, 12, 13, 15, 17, 18, 19, 20, 22, 23, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 54, 56, 57, 58, 59, 64, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 96, 97, 98, 99, 101, 113, 114, 129, 130, 136, 145, 146], "doublemldid": [58, 59, 99, 101, 114, 117, 145], "doublemldidaggreg": [60, 61, 62, 63, 99, 100], "doublemldidc": [58, 99, 101, 114, 115, 145], "doublemldidmulti": [60, 61, 62, 63, 99, 100, 102, 103, 104, 114, 116, 118, 145], "doublemlframework": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 61, 62, 63, 99, 100, 113, 129, 145], "doublemlframwork": 30, "doublemlidid": [99, 101], "doublemlididc": [99, 101], "doublemliivm": [51, 55, 68, 81, 87, 98, 99, 113, 114, 121, 145], "doublemlirm": [20, 22, 23, 29, 31, 32, 34, 35, 37, 38, 39, 53, 55, 64, 67, 70, 74, 76, 77, 78, 81, 83, 84, 88, 89, 97, 98, 99, 113, 114, 122, 141, 145], "doublemllpq": [85, 97, 114, 123, 145], "doublemlpaneldata": [0, 22, 24, 61, 62, 94, 99, 100, 102, 103, 104, 145], "doublemlpliv": [98, 99, 113, 114, 125, 141, 145], "doublemlplr": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 52, 55, 56, 64, 69, 71, 75, 79, 81, 83, 88, 91, 92, 97, 98, 99, 113, 114, 126, 129, 130, 136, 141, 143, 145, 146], "doublemlpolicytre": [33, 97], "doublemlpq": [82, 85, 97, 114, 124, 145], "doublemlqt": [72, 82, 85, 97, 129, 145], "doublemlresampl": [77, 78], "doublemlsmm": 145, "doublemlssm": [57, 90, 99, 114, 127, 128], "doubli": [9, 10, 26, 53, 61, 87, 144], "down": 89, "download": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 93, 95, 142, 143], "download_fil": 61, "downward": 89, "dpg_dict": 88, "dpi": [52, 69, 83], "dr": [99, 103], "dramat": 53, "draw": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 89, 113, 145], "draw_sample_split": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 77, 78, 113], "drawn": [11, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 55, 60, 63, 81, 82, 84, 113], "drive": [52, 69, 92], "driven": [89, 146], "drop": [53, 79, 80, 83, 93, 95, 98, 114, 115, 116, 117, 118, 129], "dropna": [60, 63], "dt": [60, 114, 115, 130, 132], "dt_bonu": [93, 95], "dta": [53, 62], "dtrain": 81, "dtype": [58, 60, 61, 62, 63, 64, 66, 67, 74, 75, 76, 78, 80, 81, 82, 87, 88, 90, 93, 94, 95, 97, 143], "dualiti": 80, "dubourg": [141, 143], "duchesnai": [141, 143], "due": [52, 53, 61, 69, 70, 71, 76, 88, 89, 92, 99, 112, 130, 131, 145, 146], "duflo": [7, 8, 17, 54, 73, 80, 87, 92, 113, 141, 144], "dummi": [29, 33, 39, 42, 43, 44, 61, 79, 89, 97, 98, 99, 101, 145], "dummyclassifi": [42, 61], "dummyregressor": [43, 61], "duplic": 145, "durabl": [56, 64, 93, 95, 143], "durat": 8, "dure": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 54, 55, 56, 57, 79, 80, 81, 98, 113, 143, 145, 146], "dx": 12, "dynam": [53, 144], "e": [5, 6, 7, 8, 9, 10, 14, 17, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 52, 53, 54, 55, 57, 58, 60, 61, 62, 63, 66, 67, 69, 70, 71, 73, 76, 78, 79, 80, 81, 82, 83, 86, 87, 88, 89, 90, 92, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 146], "e20ea26": 56, "e401": [55, 81, 82, 88, 146], "e4016553": 146, "e45228": 83, "e57c": 56, "each": [14, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 54, 56, 59, 60, 61, 62, 63, 66, 67, 74, 75, 78, 79, 80, 82, 83, 84, 87, 88, 89, 91, 93, 94, 95, 98, 99, 105, 113, 129, 130, 136, 146], "earlier": [60, 63, 146], "earn": [55, 81, 82], "earner": [55, 81, 88], "easi": [56, 87, 114], "easier": 79, "easili": [56, 78, 79, 82, 145], "ec973f": 83, "ecolor": [59, 66, 67, 81, 83], "econ": 144, "econml": 144, "econom": [15, 16, 18, 19, 54, 73, 80, 83, 89, 113, 144], "econometr": [7, 8, 9, 10, 17, 18, 25, 26, 53, 54, 73, 80, 87, 92, 141, 144], "econometrica": [13, 54, 80, 83, 87, 92, 144], "ecosystem": [141, 146], "ectj": [7, 8, 17, 54, 73, 80, 92, 141], "ed": 144, "edge_color": 69, "edgecolor": 69, "edit": [142, 144], "edu": [141, 143], "educ": [55, 81, 82, 88, 146], "ee97bda7": 56, "effect": [9, 10, 11, 14, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 46, 47, 51, 52, 53, 54, 56, 57, 58, 59, 67, 68, 69, 73, 76, 80, 84, 86, 87, 90, 92, 96, 98, 100, 101, 103, 105, 107, 108, 109, 112, 113, 114, 122, 129, 130, 131, 143, 144, 145, 146], "effici": [66, 99, 144], "effort": 114, "eight": [54, 80], "either": [11, 20, 22, 23, 24, 25, 29, 31, 32, 33, 34, 35, 37, 38, 39, 56, 59, 60, 63, 73, 84, 86, 97, 98, 99, 102, 105, 146], "eleanor": 144, "element": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 57, 58, 60, 61, 62, 63, 70, 71, 72, 78, 80, 82, 85, 88, 90, 99, 102, 103, 104, 114, 115, 116, 117, 118, 119, 130, 133, 135, 136, 139, 140, 145], "element_text": [54, 55], "elementari": 144, "elif": [74, 75, 84], "elig": [82, 88, 146], "eligibl": [55, 81, 88], "ell": [52, 54, 69, 73, 80, 92, 114, 125, 126, 143], "ell_0": [32, 38, 39, 52, 69, 73, 79, 92, 99, 108, 114, 126], "ell_1": 61, "ell_2": 78, "els": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 44, 53, 54, 55, 59, 66, 74, 75, 80, 84, 89], "em": 144, "emphas": [54, 80], "empir": [27, 28, 52, 54, 61, 69, 80, 83, 89, 92, 99, 102, 103, 104, 113, 114, 129], "emploi": [54, 73, 80, 89, 114, 121], "employ": [55, 61, 81, 82], "employe": 146, "empti": 80, "emul": [130, 131], "enabl": [21, 66, 67, 84, 88, 97, 130, 131, 145], "enable_metadata_rout": [42, 43, 46, 47], "encapsul": [42, 43, 46, 47, 98], "encod": 83, "encount": [63, 66], "end": [12, 15, 16, 52, 53, 54, 55, 57, 59, 60, 63, 69, 72, 73, 78, 80, 81, 83, 84, 85, 90, 91, 93, 95, 98, 99, 102, 104, 105, 113, 114, 116, 118, 129, 143, 146], "endogen": [55, 81, 82, 146], "enet_coordinate_descent_gram": 80, "enforc": 61, "engin": [56, 144], "enrol": [55, 81, 82], "ensembl": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 64, 66, 67, 69, 70, 71, 74, 75, 76, 78, 81, 84, 88, 89, 91, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 130, 136, 143, 146], "ensemble_learner_pipelin": 98, "ensemble_pipe_classif": 56, "ensemble_pipe_regr": 56, "ensur": [46, 47, 54, 66, 75, 77, 79, 80, 84, 87], "entir": [24, 52, 55, 69, 81, 92, 130, 131], "entri": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 52, 54, 58, 60, 61, 62, 63, 64, 66, 67, 69, 76, 80, 81, 82, 88, 90, 92, 93, 94, 95, 98, 141, 143, 145], "enumer": [59, 66, 67, 72, 74, 75, 78, 80, 81, 82, 85, 91, 98, 113], "env": [66, 80, 142], "environ": 142, "ep": [66, 83], "epanechnikov": 40, "epsilon": [55, 58, 59, 72, 81, 85, 97, 99, 101, 105], "epsilon_": [54, 59, 60, 63, 80], "epsilon_0": 41, "epsilon_1": 41, "epsilon_i": [11, 72, 83, 84, 85], "epsilon_sampl": 84, "epsilon_tru": [72, 85], "eqnarrai": 55, "equal": [21, 25, 29, 33, 54, 57, 60, 63, 80, 83, 87, 90, 97, 98, 99, 130, 138], "equat": [41, 54, 55, 80, 81, 89, 91, 129, 146], "equilibrium": [54, 80], "equiv": [99, 105, 114, 116, 118], "equival": [73, 77, 113], "err": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 58, 60, 61, 62, 63, 64, 66, 67, 68, 70, 71, 72, 74, 75, 76, 77, 79, 80, 81, 82, 83, 85, 88, 89, 90, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 143, 146], "error": [4, 5, 6, 20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 51, 52, 53, 55, 56, 57, 59, 66, 69, 73, 74, 75, 78, 79, 81, 86, 89, 92, 98, 99, 110, 111, 113, 114, 129, 130, 136, 143, 145, 146], "errorbar": [59, 66, 67, 74, 75, 79, 81, 83, 86], "erstellt": [54, 55, 56], "es_linear_logist": 61, "es_rf": 61, "esim": 86, "especi": [78, 79], "essenti": 89, "est": 86, "est_method": 53, "esther": [113, 144], "estim": [4, 5, 7, 9, 10, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 52, 53, 54, 56, 59, 67, 69, 70, 71, 72, 74, 75, 77, 78, 80, 84, 86, 87, 91, 92, 96, 97, 98, 99, 100, 101, 102, 103, 104, 107, 109, 115, 116, 117, 120, 123, 124, 128, 130, 131, 136, 141, 144, 145], "estimand": 87, "estimatior": [4, 5], "estimator_list": 79, "et": [7, 8, 11, 13, 16, 17, 52, 54, 55, 56, 58, 69, 70, 71, 72, 73, 74, 75, 78, 80, 81, 82, 85, 88, 92, 99, 101, 105, 113, 114, 120, 122, 123, 124, 129, 130, 131, 140, 141, 143, 145], "eta": [27, 28, 52, 54, 55, 59, 79, 80, 81, 85, 86, 91, 97, 99, 102, 103, 104, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 140, 143, 146], "eta1": 83, "eta2": 83, "eta_": [129, 130, 140], "eta_0": [40, 91, 99, 102, 103, 104, 114, 129], "eta_d": [86, 99], "eta_i": [11, 25, 59, 60, 63, 84, 85, 86, 99], "eta_sampl": 84, "eta_tru": 85, "etc": [20, 21, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 78, 79, 80, 145], "ev": [52, 69, 92], "eval": [24, 56, 60, 61, 62, 63, 98, 99, 102, 103, 104, 114, 116, 118], "eval_metr": [55, 81, 146], "eval_pr": 53, "eval_predict": 53, "evalu": [13, 20, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 56, 59, 60, 61, 62, 63, 70, 71, 72, 76, 77, 82, 85, 88, 91, 99, 103, 105, 144, 145], "evaluate_learn": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 78, 79, 98, 145], "evalut": 98, "even": [55, 56, 60, 63, 81, 83, 86, 98, 99, 146], "event": [99, 100], "eventstudi": [24, 60, 61, 62, 63, 99, 100], "eventu": [54, 80], "everi": [54, 61, 80], "everyth": 141, "evid": [76, 79], "exact": [77, 89], "exactli": [86, 89, 99], "examin": 66, "exampl": [4, 5, 6, 11, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 55, 56, 57, 58, 59, 60, 62, 63, 64, 66, 67, 68, 69, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 90, 91, 92, 97, 98, 99, 100, 102, 103, 104, 112, 113, 114, 129, 130, 136, 141, 143, 145, 146], "example_attgt": 53, "example_attgt_dml_eval_linear": 53, "example_attgt_dml_eval_rf": 53, "example_attgt_dml_linear": 53, "example_attgt_dml_rf": 53, "except": [43, 47, 73, 89, 145], "excess": 78, "exclud": 48, "exclus": [29, 33, 39, 74, 75, 97], "execut": [56, 146], "exemplarili": 143, "exemplatori": 84, "exhaust": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 98], "exhibit": [54, 80], "exist": [42, 43, 46, 47, 77, 99, 101, 105, 130, 140], "exogen": [55, 81, 82, 99, 146], "exp": [9, 10, 11, 13, 14, 17, 25, 26, 52, 59, 69, 70, 71, 74, 75, 83, 84, 92], "expect": [9, 10, 43, 47, 53, 57, 58, 60, 61, 63, 67, 76, 78, 79, 86, 89, 90, 97, 99, 113, 129, 130, 137, 143], "experi": [8, 12, 13, 52, 55, 69, 81, 87, 89, 92, 93, 95, 113, 143, 144], "experiment": [20, 22, 23, 24, 25, 26, 114, 115, 116, 117, 118, 130, 132, 133, 134, 135], "expertis": 89, "explain": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 88, 130, 131, 139, 140], "explan": [54, 58, 80, 88, 130, 139, 141, 146], "explanatori": [89, 129], "explicit": 89, "explicitli": [60, 62, 63, 76, 146], "exploit": [52, 69, 92, 99, 146], "explor": 79, "exponenti": 129, "export": [79, 145], "expos": [99, 105], "exposur": [6, 25, 59, 60, 61, 62, 63], "express": [54, 73, 86, 130, 140], "ext": 24, "extend": [89, 95, 98, 141, 145], "extendend": [130, 140], "extens": [98, 114, 141, 144, 145], "extent": 73, "extern": [52, 69, 77, 79, 96, 130, 131, 145], "external_predict": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 69, 98], "externalptr": 55, "extra": 56, "extract": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 66, 79], "extralearn": 56, "extrem": [55, 81], "ey": 73, "ezequiel": 145, "f": [55, 56, 58, 59, 60, 63, 66, 67, 69, 72, 73, 78, 80, 81, 82, 84, 85, 88, 89, 90, 98, 130, 140, 141, 143], "f00584a57972": 56, "f1718fdeb9b0": 56, "f2e7": 56, "f3d24993": 56, "f6ebc": 83, "f_": [9, 25, 26, 59, 97], "f_loc": [72, 85], "f_p": 59, "f_scale": [72, 85], "f_t": [60, 63], "f_x": 99, "face_color": 69, "facet_wrap": 55, "facilit": 79, "fact": [55, 81, 82], "factor": [41, 52, 53, 54, 55, 56, 69, 78, 92, 98, 130, 133, 135, 146], "faculti": 144, "fail": 145, "fair": 78, "fake": [51, 68, 87], "fall": 66, "fallback": 98, "fals": [4, 5, 6, 7, 8, 9, 11, 14, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 52, 55, 56, 57, 58, 60, 63, 66, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 89, 90, 93, 95, 98, 99, 102, 113, 114, 115, 116, 117, 118, 129, 130, 132, 133, 134, 135, 146], "famili": [55, 81, 98], "familiar": 87, "fanci": 53, "far": [55, 81], "farbmach": 12, "fast": [78, 84, 98], "faster": 73, "fb5c25fa": 56, "fc9e": 56, "fd8a": 56, "featur": [7, 8, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 43, 45, 47, 53, 60, 63, 64, 66, 76, 77, 78, 81, 84, 97, 98, 99], "featureless": [56, 98], "features_bas": [55, 81, 82, 88], "features_flex": 55, "featureunion": 56, "februari": [60, 89], "feder": 61, "femal": [56, 64, 93, 95, 143], "fern\u00e1ndez": [13, 113, 144], "fetch": [55, 80, 81, 82, 93, 95], "fetch_401k": [55, 81, 82, 88, 146], "fetch_bonu": [56, 64, 93, 95, 143], "few": [55, 81, 82], "ff7f0e": 59, "field": [54, 80, 98, 146], "fifteenth": 144, "fifth": [54, 60, 63], "fig": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 59, 60, 61, 62, 63, 66, 67, 70, 71, 72, 73, 77, 78, 79, 82, 83, 85, 86, 89], "fig_al": 69, "fig_dml": 69, "fig_non_orth": 69, "fig_orth_nosplit": 69, "fig_po_al": 69, "fig_po_dml": 69, "fig_po_nosplit": 69, "figsiz": [21, 24, 59, 60, 63, 64, 66, 67, 70, 71, 72, 74, 75, 77, 78, 79, 80, 81, 82, 83, 85, 86], "figur": [17, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 54, 59, 60, 61, 63, 64, 66, 67, 69, 70, 71, 72, 73, 74, 75, 77, 79, 80, 81, 82, 85, 89, 92], "figure_format": 83, "file": [7, 8, 61, 73, 83, 144, 145], "filenam": 52, "fill": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 54, 55, 57, 58, 78, 81, 90], "fill_between": [70, 71, 72, 77, 82, 85], "fill_valu": 78, "fillna": 60, "filter": 56, "filterwarn": [66, 69], "final": [52, 56, 57, 59, 60, 62, 63, 67, 69, 70, 71, 72, 74, 75, 76, 82, 85, 86, 90, 92, 99, 112, 114, 116, 118, 146], "final_estim": 86, "financi": [7, 88, 146], "find": [55, 58, 59, 66, 81, 89, 97, 98, 146], "finish": 56, "finit": [52, 55], "firm": [54, 80, 88], "firmid": 80, "first": [6, 9, 10, 16, 20, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 67, 68, 69, 70, 71, 74, 75, 76, 77, 78, 80, 81, 82, 84, 85, 86, 89, 90, 92, 97, 99, 100, 103, 105, 113, 129, 130, 136, 142, 143, 145, 146], "fit": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 96, 97, 98, 99, 100, 101, 102, 104, 105, 114, 117, 128, 129, 130, 131, 136, 141, 145, 146], "fit_arg": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39], "fit_transform": [77, 80, 81], "five": 80, "fix": [59, 60, 63, 78, 145], "flag": [26, 113, 142], "flake8": 145, "flamlclassifierdoubleml": 79, "flamlregressordoubleml": 79, "flatten": [79, 83], "flexibl": [40, 51, 53, 55, 56, 58, 68, 81, 99, 141, 144, 145, 146], "flexibli": [55, 61, 81, 88], "float": [9, 10, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 60, 61, 62, 63, 94, 95], "float32": [81, 82, 88], "float64": [58, 60, 61, 62, 63, 64, 66, 67, 75, 76, 80, 81, 88, 90, 93, 94, 95, 98, 143], "floor": 56, "floor_divid": 80, "flt": 56, "flush": 52, "fmt": [59, 66, 67, 74, 75, 79, 81, 83, 86], "fobj": 81, "focu": [54, 55, 61, 77, 80, 81, 82, 89, 97, 99, 101, 105, 112, 146], "focus": [82, 88, 89, 146], "fold": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 54, 55, 56, 57, 58, 60, 61, 62, 63, 78, 80, 81, 82, 88, 90, 91, 96, 98, 99, 101, 102, 104, 114, 117, 129, 143, 146], "follow": [9, 10, 11, 14, 25, 26, 52, 54, 55, 57, 58, 59, 60, 61, 62, 63, 69, 70, 71, 72, 74, 75, 79, 80, 81, 82, 85, 86, 87, 88, 89, 90, 92, 93, 95, 97, 98, 99, 100, 102, 103, 104, 105, 113, 114, 116, 118, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 146], "font_scal": [80, 81, 82], "fontsiz": [60, 63, 72, 82, 85], "force_all_d_finit": [5, 6], "force_all_x_finit": [4, 5, 6], "forest": [12, 51, 52, 53, 55, 56, 58, 66, 68, 69, 76, 78, 81, 88, 92, 98, 143, 146], "forest_summari": 81, "forg": [142, 144, 145], "form": [9, 10, 11, 20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 46, 47, 55, 57, 58, 59, 70, 71, 72, 74, 75, 76, 78, 81, 85, 86, 88, 90, 97, 99, 100, 101, 105, 108, 109, 110, 111, 114, 116, 118, 119, 122, 130, 131, 136, 137, 138, 139, 140, 142, 143], "format": [6, 24, 61, 66, 69, 76, 130, 136, 145], "former": [61, 87], "formula": [54, 55, 80, 81, 86, 89, 145], "formula_flex": 55, "forschungsgemeinschaft": 141, "forthcom": [89, 144], "forum": 145, "forward": [33, 45], "found": [62, 70, 71, 73, 74, 75, 79, 92, 93, 95, 98, 99, 112, 143], "foundat": [66, 141, 144], "four": [55, 66, 78, 81, 99, 102, 145], "fourth": [54, 80], "frac": [9, 10, 12, 13, 15, 17, 18, 19, 25, 26, 28, 32, 43, 47, 52, 54, 56, 59, 63, 69, 73, 76, 80, 83, 86, 91, 92, 97, 99, 102, 103, 104, 108, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140], "fraction": 56, "frame": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 51, 52, 54, 55, 57, 58, 60, 61, 62, 63, 64, 66, 67, 70, 71, 74, 75, 76, 80, 81, 82, 83, 84, 88, 90, 92, 93, 94, 95, 143, 146], "framealpha": [60, 63], "frameon": [60, 63], "framework": [21, 24, 28, 52, 54, 56, 66, 69, 78, 79, 80, 83, 89, 92, 98, 129, 141, 143, 145, 146], "freez": 142, "fribourg": 144, "friendli": [60, 63, 66, 67], "from": [4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 47, 51, 52, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 97, 98, 99, 100, 101, 102, 103, 104, 105, 113, 114, 129, 130, 136, 143, 145, 146], "from_arrai": [4, 5, 6, 37, 40, 58, 59, 69, 72, 85, 92, 93, 95, 98, 129, 143], "from_product": 80, "front": 67, "fr\u00e9chet": [130, 140], "fs_kernel": [40, 99], "fs_specif": [40, 99], "fsize": [55, 81, 82, 88, 146], "full": [58, 59, 66, 67, 69, 72, 74, 75, 78, 81, 82, 85, 86, 87, 90, 92, 99], "fulli": [33, 55, 65, 79, 81, 87, 99, 109], "fun": 52, "func": 53, "function": [0, 4, 5, 6, 17, 18, 20, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 55, 56, 57, 58, 60, 61, 62, 63, 66, 68, 69, 70, 71, 72, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 96, 98, 99, 100, 101, 102, 103, 104, 105, 108, 113, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 132, 133, 134, 135, 140, 141, 144, 145, 146], "fund": [55, 81, 82, 141], "further": [9, 10, 11, 14, 16, 21, 24, 25, 26, 54, 56, 57, 58, 59, 60, 61, 62, 63, 67, 70, 71, 72, 76, 77, 78, 80, 82, 84, 85, 86, 88, 89, 90, 98, 99, 101, 104, 112, 114, 120, 123, 124, 127, 128, 129, 130, 131, 136, 139, 140, 141, 143, 145, 146], "furthermor": [69, 94, 95, 114, 119, 122], "futur": [60, 61, 62, 63, 99, 114, 130], "futurewarn": [60, 61, 62, 63, 66, 75], "fuzzi": [40, 41], "g": [5, 6, 9, 10, 20, 22, 23, 24, 25, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 52, 53, 56, 58, 59, 60, 61, 62, 63, 64, 69, 70, 71, 73, 76, 78, 82, 83, 84, 87, 88, 90, 92, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 114, 115, 116, 117, 118, 119, 121, 122, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 146], "g0": 66, "g1": 66, "g_": [41, 67, 99, 104, 114, 115, 117, 118, 120, 123, 124, 129], "g_0": [17, 18, 20, 22, 23, 24, 29, 32, 33, 35, 38, 39, 40, 41, 52, 54, 55, 66, 69, 78, 80, 81, 92, 97, 98, 99, 106, 107, 108, 109, 110, 111, 114, 116, 118, 119, 126, 127, 128, 130, 137, 138, 140, 143, 146], "g_1": [41, 78], "g_all": [52, 55], "g_all_po": 52, "g_ci": 55, "g_d": [114, 120, 124], "g_dml": 52, "g_dml_po": 52, "g_hat": [38, 39, 52, 69, 114], "g_hat0": [32, 33], "g_hat1": [32, 33], "g_i": [25, 99, 102, 104, 105, 114, 116, 118], "g_k": 97, "g_nonorth": 52, "g_nosplit": 52, "g_nosplit_po": 52, "g_valu": 22, "g_x": 59, "gain": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 78, 130, 131, 138, 145], "gain_statist": 145, "galleri": [92, 97, 98, 99, 100, 102, 104, 112, 141, 145], "gama": 79, "gamma": [15, 18, 19, 54, 80, 83, 84, 86, 89, 99, 114, 120, 123], "gamma_0": [11, 57, 84, 90, 114, 120, 123], "gamma_a": [9, 10, 89], "gamma_bench": 89, "gamma_v": 89, "gap": [80, 89], "gapo": 29, "gate": [29, 33, 39, 44, 83, 84, 96, 145], "gate_obj": 97, "gatet": 97, "gaussian": [34, 35, 36, 52, 69, 92, 97, 98, 129, 144], "ge": [9, 11, 26, 76, 84, 97, 99, 103, 105], "geer": 144, "gelbach": [54, 80], "gener": [0, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 43, 47, 51, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 93, 95, 96, 97, 98, 99, 102, 104, 105, 106, 113, 114, 116, 118, 119, 122, 129, 131, 132, 133, 134, 135, 137, 138, 140, 144, 145, 146], "generate_treat": 85, "generate_weakiv_data": 87, "geom_bar": 55, "geom_dens": [55, 57], "geom_errorbar": 55, "geom_funct": 52, "geom_histogram": 52, "geom_hlin": 55, "geom_point": 55, "geom_til": 54, "geom_vlin": [52, 57], "geq": [25, 86, 99], "german": 141, "get": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 56, 60, 62, 63, 66, 67, 78, 83, 88, 89, 130, 131, 141, 142], "get_dummi": 83, "get_feature_names_out": [77, 80, 81], "get_legend_handles_label": [66, 67], "get_level_valu": 79, "get_logg": [52, 53, 54, 55, 56, 57, 91, 98, 99, 113, 114, 129, 143], "get_metadata_rout": [42, 43, 46, 47], "get_param": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 79, 98], "get_ylim": 77, "ggdid": 53, "ggplot": [52, 54, 55, 57], "ggplot2": [52, 54, 55, 57], "ggsave": 52, "ggtitl": 55, "gh": 145, "git": 142, "github": [53, 55, 61, 73, 79, 83, 141, 144, 145], "githubusercont": [62, 73], "give": [55, 77, 81], "given": [9, 10, 13, 17, 18, 20, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 46, 47, 52, 54, 57, 59, 61, 62, 67, 69, 74, 75, 80, 82, 83, 86, 89, 90, 92, 97, 99, 102, 103, 104, 114, 119, 129, 130, 136, 137, 138, 139, 140, 143, 145], "glmnet": [55, 56, 98, 145], "global": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 46, 47, 98, 99], "globalclassifi": 86, "globallearn": 86, "globalregressor": 86, "glrn": 56, "glrn_lasso": 56, "gmm": 87, "gname": 53, "go": [70, 71, 73, 77, 79, 86, 89], "goal": [67, 74, 75, 99], "goe": 99, "goldman": 144, "good": [73, 130, 131, 146], "gpu": 66, "gradient": [55, 66, 81], "gradientboostingclassifi": 78, "gradientboostingregressor": 78, "gradual": 89, "gramfort": [141, 143], "graph": [56, 57, 90, 146], "graph_ensemble_classif": 56, "graph_ensemble_regr": 56, "graph_obj": 86, "graph_object": [70, 71, 73, 89], "graphlearn": [56, 98], "grasp": [67, 130, 131], "great": [59, 146], "greater": 146, "green": [52, 70, 71, 72, 85], "greg": 144, "grei": [55, 66, 67], "grenand": 144, "grey50": 54, "grid": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 54, 56, 60, 63, 66, 67, 70, 71, 72, 73, 77, 82, 83, 85, 89, 98, 130, 136], "grid_arrai": [70, 71], "grid_basi": 77, "grid_bound": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 89], "grid_search": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 56, 98], "grid_siz": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 70, 71], "gridextra": 54, "gridsearchcv": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 98], "grisel": [141, 143], "grob": 54, "group": [6, 22, 24, 25, 29, 33, 39, 51, 53, 61, 66, 67, 68, 76, 77, 82, 83, 84, 89, 96, 99, 100, 102, 103, 104, 105, 114, 116, 118, 130, 133, 135], "group_0": 97, "group_1": [74, 75, 97], "group_2": [74, 75, 97], "group_3": [74, 75], "group_effect": 84, "group_ind": 76, "group_treat": 76, "groupbi": [60, 63, 66, 73, 81, 87], "gruber": 12, "gt": [51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 68, 70, 71, 72, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 93, 95, 143], "gt_combin": [24, 60, 61, 63, 99, 100, 102, 103, 104], "gt_dict": [60, 63], "guarante": [54, 80], "guber": 12, "guess": [88, 130, 131], "guid": [27, 28, 42, 43, 46, 47, 52, 53, 54, 56, 59, 60, 61, 62, 63, 67, 69, 76, 77, 80, 86, 88, 98, 141, 143, 145], "guidelin": 145, "gunion": [56, 98], "gxidclusterperiodytreat": 53, "h": [9, 10, 12, 16, 25, 26, 53, 54, 80, 86, 87, 94, 95, 99, 144], "h20": 79, "h_0": [60, 63, 67, 76, 77, 88, 89, 130, 136, 146], "h_f": [40, 99], "ha": [20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 45, 46, 47, 52, 53, 54, 55, 61, 63, 69, 73, 78, 79, 80, 81, 82, 83, 86, 87, 88, 89, 97, 98, 99, 101, 105, 130, 131, 136, 137, 138, 139, 140, 141, 146], "had": 61, "half": [52, 69, 83, 92, 113], "hand": [40, 78, 79, 83, 87, 146], "handbook": 83, "handl": [53, 61, 66, 67, 78, 94, 95, 98, 145], "hansen": [7, 8, 13, 15, 17, 54, 73, 80, 87, 92, 141, 144], "happend": 78, "hard": [88, 130, 131], "harold": 144, "harsh": [42, 46], "hasn": [21, 24, 60, 62, 63], "hat": [52, 54, 69, 73, 76, 80, 83, 86, 91, 92, 97, 99, 113, 114, 129, 130, 131, 136, 139], "have": [11, 14, 24, 29, 30, 33, 36, 39, 44, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 66, 67, 68, 70, 71, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 93, 95, 97, 98, 99, 103, 114, 116, 118, 129, 130, 131, 137, 140, 142, 143, 145, 146], "hazlett": [89, 130, 131], "hc": [53, 144], "hc0": [44, 145], "hdm": [54, 80], "he": [57, 90], "head": [53, 54, 56, 60, 61, 62, 63, 64, 70, 71, 74, 75, 77, 79, 80, 81, 83, 86, 89, 93, 95, 97, 143], "heat": [54, 80], "heatmap": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 54, 80, 89], "heavili": 78, "hei": 144, "height": [21, 24, 52, 54, 73, 79], "help": [53, 55, 61, 72, 78, 82, 84, 89, 99, 100, 113, 146], "helper": 145, "henc": [53, 55, 56, 81, 89, 98, 114, 146], "here": [34, 35, 36, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 70, 71, 72, 74, 75, 76, 78, 80, 81, 82, 84, 85, 86, 89, 90, 93, 95, 98, 99, 102, 142], "heterogen": [11, 25, 33, 55, 76, 81, 82, 84, 96, 99, 109, 113, 144, 145, 146], "heteroskedast": [74, 75], "heurist": [52, 69, 92], "high": [13, 38, 39, 55, 59, 73, 81, 82, 87, 91, 99, 110, 111, 129, 141, 143, 144], "higher": [53, 55, 73, 81, 82, 83, 86, 87, 145, 146], "highli": [55, 81, 141], "highlight": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 58, 77, 79, 89, 145], "highlightcolor": [70, 71], "hint": 79, "hispan": 64, "hist": [66, 67], "hist_e401": 55, "hist_p401": 55, "histogram": [66, 67], "histplot": 69, "hjust": 55, "hline": [93, 95, 129, 143, 146], "hold": [19, 54, 55, 57, 79, 80, 81, 90, 97, 98, 99, 103], "holdout": [98, 113], "holm": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39], "home": [55, 81], "homogen": 99, "hook": 145, "hopefulli": 82, "horizont": [54, 59, 66, 80], "hostedtoolcach": [60, 61, 62, 63, 81, 89], "hot": 83, "hotstart_backward": [56, 98], "hotstart_forward": [56, 98], "household": [55, 81, 82, 88], "how": [21, 25, 42, 43, 46, 47, 51, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 92, 98, 99, 141, 142], "howev": [52, 55, 57, 69, 79, 81, 86, 89, 90, 92, 99, 146], "hown": [55, 81, 82, 88, 146], "hpwt": [54, 80], "hpwt0": 54, "hpwtairmpdspac": 54, "href": 141, "hspace": 78, "hstack": [37, 59], "html": [56, 75, 141, 143, 145], "http": [12, 18, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 98, 141, 142, 143, 145], "huber": [19, 57, 90, 99, 112, 114, 127, 128, 144], "hue": [60, 63, 81], "huge": 78, "hugo": 144, "husd": [56, 64, 93, 95, 143], "hyperparamet": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 55, 56, 64, 66, 73, 78, 79, 81, 96, 143], "hypothes": [129, 144], "hypothesi": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 81, 88, 130, 136, 144], "hypothet": 89, "i": [4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 128, 129, 130, 131, 133, 135, 136, 137, 138, 140, 141, 142, 143, 145, 146], "i0": [58, 59, 99, 101], "i03": 141, "i1": [58, 99, 101], "i_": [15, 80, 84], "i_1": [54, 80], "i_2": [54, 80], "i_3": [54, 80], "i_4": 59, "i_est": 69, "i_fold": 54, "i_k": [54, 80, 91, 113, 129], "i_learn": 78, "i_level": 67, "i_rep": [52, 57, 58, 69, 78, 90, 92], "i_split": 80, "i_train": 69, "icp": 144, "id": [6, 24, 53, 54, 56, 60, 61, 62, 63, 80, 94, 95, 99, 100, 102, 104, 130, 135], "id_col": [6, 24, 60, 61, 62, 63, 94, 95, 99, 100, 102, 104], "id_var": 80, "idea": [55, 56, 81, 82, 89, 98, 99, 130, 131, 146], "ident": [9, 10, 11, 14, 15, 25, 26, 45, 56, 61, 67, 77, 79, 86, 98, 99, 105, 114, 122, 130, 136], "identfi": 89, "identif": [60, 61, 62, 63, 86, 87, 99, 146], "identifi": [6, 54, 55, 58, 61, 66, 76, 80, 81, 82, 86, 89, 94, 95, 97, 99, 101, 103, 105, 112, 130, 140, 145], "identifii": 97, "idnam": 53, "idx_gt_att": 24, "idx_learn": 66, "idx_tau": [72, 82, 85], "idx_treat": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 66, 67, 130, 136], "ieee": 144, "ifels": 53, "ignor": [42, 43, 46, 47, 61, 66, 69, 86], "ignore_index": 66, "ii": [54, 80], "iid": [60, 63, 99, 101], "iivm": [12, 27, 28, 32, 82, 91, 97, 108, 121, 141, 145], "iivm_summari": 81, "iivmglmnet": 55, "iivmrang": 55, "iivmrpart": 55, "iivmxgboost11861": 55, "ij": [16, 54, 57, 67, 80, 90], "ilia": 144, "illustr": [52, 54, 55, 56, 57, 58, 59, 61, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 80, 81, 82, 84, 85, 88, 89, 90, 92, 98, 146], "iloc": [58, 59, 60, 61, 62, 63, 66, 67, 78, 80, 83, 87], "immedi": 142, "immun": [113, 144], "impact": [51, 68, 78, 83, 88], "implement": [20, 22, 23, 24, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 69, 73, 77, 78, 80, 81, 83, 86, 88, 89, 90, 92, 96, 97, 98, 100, 101, 103, 105, 112, 113, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 143, 144, 145, 146], "impli": [9, 10, 54, 55, 60, 63, 80, 81, 82, 86, 97, 99, 105, 130, 132, 133, 134, 135, 137, 138], "implicitli": [99, 103], "implment": [59, 99, 100], "import": [4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 130, 136, 142, 143, 145, 146], "importlib": 73, "impos": 89, "improv": [58, 60, 63, 78, 84, 99, 145], "in_sample_norm": [20, 22, 23, 24, 58, 114, 115, 116, 117, 118, 130, 132, 133, 134, 135], "inbuild": 78, "inbuilt": 78, "inc": [55, 81, 82, 88, 146], "includ": [20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 53, 55, 59, 60, 61, 62, 63, 66, 67, 74, 75, 77, 81, 86, 88, 89, 97, 99, 129, 130, 136, 137, 138, 140, 142, 145, 146], "include_bia": [77, 80, 81], "include_never_tr": [25, 63], "include_scenario": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 89], "incom": [55, 81, 82, 84, 88, 146], "incorpor": [56, 88, 130, 136], "increas": [60, 63, 76, 78, 80, 89, 146], "increment": 145, "ind": 81, "independ": [9, 10, 11, 20, 22, 23, 24, 26, 41, 54, 56, 59, 76, 80, 84, 99, 105, 112, 114, 115, 116, 117, 118, 145], "index": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 54, 59, 64, 66, 69, 73, 74, 75, 79, 80, 81, 83, 84, 92, 93, 95, 113, 114, 115, 116, 117, 118, 141, 143], "index_col": 73, "india": [113, 144], "indic": [4, 5, 6, 11, 14, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 54, 55, 57, 59, 60, 61, 62, 63, 76, 80, 81, 82, 86, 87, 89, 90, 91, 93, 95, 97, 99, 101, 105, 106, 112, 113], "individu": [25, 29, 33, 46, 47, 53, 55, 59, 60, 63, 66, 67, 74, 75, 76, 79, 81, 82, 86, 88, 97, 99, 146], "individual_df": 59, "induc": [96, 113], "industri": [54, 80], "inf": [4, 5, 6, 53, 60, 61, 62, 63, 87], "inf_model": 114, "infer": [13, 15, 51, 52, 54, 61, 66, 68, 69, 73, 78, 79, 80, 87, 92, 96, 99, 113, 141, 143, 144, 145], "inferenti": 146, "infinit": [4, 5, 6, 87, 145], "influenc": [32, 43, 47, 66, 99], "info": [51, 56, 58, 60, 61, 62, 63, 64, 66, 67, 76, 79, 80, 81, 82, 88, 90, 93, 94, 95, 143, 145, 146], "inform": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 46, 47, 51, 56, 60, 61, 62, 63, 68, 70, 71, 78, 86, 87, 88, 89, 99, 100, 130, 131, 144], "infti": [52, 69, 92, 99, 105], "inher": 89, "inherit": [83, 94, 95, 145], "initi": [4, 5, 6, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 55, 56, 57, 58, 60, 61, 62, 63, 66, 72, 81, 82, 85, 86, 88, 89, 90, 93, 94, 95, 97, 98, 99, 113, 143, 145, 146], "inlin": [64, 83], "inlinebackend": 83, "inner": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 98], "innermost": 98, "input": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 56, 61, 88, 91, 99, 103, 129, 130, 131, 136], "insensit": 99, "insid": [42, 43, 46, 47], "insight": [73, 89], "insignific": 88, "inspect": 143, "inspir": [9, 12, 13, 19, 60, 63, 89], "instal": [55, 66, 79, 86, 99, 145], "install_github": 142, "instanc": [46, 47, 55, 56, 66, 81, 98], "instanti": [54, 55, 80, 81, 98, 113], "instead": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 51, 53, 55, 60, 61, 62, 63, 66, 67, 68, 75, 76, 79, 81, 82, 97, 98, 99, 114, 118, 130, 132, 133, 134, 135, 138, 139, 145], "instruct": [66, 142, 145], "instrument": [4, 5, 6, 7, 12, 15, 32, 38, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 76, 80, 81, 82, 85, 88, 90, 93, 94, 95, 98, 99, 101, 102, 104, 108, 110, 114, 123, 129, 143, 146], "instrument_effect": 51, "instrument_impact": 68, "instrument_strength": 87, "insuffienct": 79, "int": [9, 10, 11, 14, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 53, 54, 55, 58, 61, 66, 68, 72, 84, 85, 87, 89, 90, 94, 95], "int32": 61, "int64": [60, 62, 63, 64, 78, 80, 93, 94, 95, 143], "int8": [81, 82, 88], "integ": [26, 56, 98], "integr": [66, 79, 89, 130, 140, 145], "intend": [40, 56, 89, 146], "intent": [99, 146], "inter": 98, "interact": [9, 12, 13, 14, 29, 30, 32, 33, 40, 41, 67, 89, 96, 98, 108, 109, 137, 138, 141, 145, 146], "interchang": 129, "interest": [9, 10, 32, 33, 38, 39, 52, 55, 57, 58, 69, 73, 81, 82, 86, 87, 90, 92, 97, 99, 101, 103, 106, 107, 108, 109, 110, 111, 112, 114, 129, 143, 146], "interfac": [53, 55, 56, 66, 93, 95, 98, 113, 143], "intermedi": [75, 89], "intern": [53, 55, 56, 67, 79, 82, 98, 99, 100, 144], "internet": [55, 81, 82], "interpret": [60, 61, 62, 63, 74, 75, 87, 89, 97, 130, 131, 137, 138, 139, 140, 142, 146], "intersect": [89, 130, 136, 145], "interv": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 53, 54, 55, 57, 58, 60, 61, 62, 63, 66, 67, 70, 71, 72, 74, 75, 77, 80, 82, 85, 86, 88, 90, 96, 97, 113, 114, 130, 136, 143, 144, 145, 146], "intial": 86, "introduc": [52, 69, 92, 93, 95, 129, 145, 146], "introduct": [52, 54, 56, 69, 80, 82, 88, 98, 99, 101, 105, 130, 131], "introductori": [53, 89], "intrument": [57, 90], "intspecifi": 40, "intuit": 89, "inuidur1": [56, 64, 93, 95, 143], "inuidur1femaleblackothracedep1dep2q2q3q4q5q6agelt35agegt54durablelusdhusdtg": [56, 93, 95, 143], "inuidur2": [64, 93, 95, 143], "inv_sigmoid": 83, "invalid": [52, 63, 69, 92], "invari": [99, 101, 105], "invers": [29, 31, 32, 33, 34, 35, 36, 37, 57, 90, 130, 137, 138], "invert": [32, 87], "invert_yaxi": 80, "investig": [73, 79, 89], "involv": [97, 98, 114, 146], "io": [83, 145], "ipw_norm": 145, "ipykernel_46248": 75, "ipynb": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90], "ira": [55, 81, 82], "irm": [0, 13, 14, 20, 22, 23, 27, 28, 38, 39, 44, 45, 78, 89, 91, 96, 98, 102, 104, 109, 122, 137, 138, 141, 145, 146], "irm_summari": 81, "irmglmnet": 55, "irmrang": 55, "irmrpart": 55, "irmxgboost8047": 55, "irrespect": 89, "irrevers": [99, 105], "is_classifi": [20, 22, 23, 29, 32, 33, 39], "is_gat": [29, 33, 39, 44], "isfinit": [60, 61, 62, 63], "isnan": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 78, 98], "isoton": 89, "isotonicregress": 89, "issn": 73, "issu": [21, 24, 89, 141, 144, 145], "ite": [60, 63, 66, 67, 74, 75, 76], "ite_lower_quantil": [60, 63], "ite_mean": [60, 63], "ite_upper_quantil": [60, 63], "item": [32, 66, 81, 91, 98, 113], "iter": [40, 51, 57, 58, 80, 81, 86, 90, 98, 129, 146], "itertool": 73, "its": [42, 43, 66, 89, 91, 97, 98, 99, 101, 102, 104, 113, 114, 129], "iv": [12, 15, 16, 32, 38, 39, 52, 54, 69, 80, 92, 93, 95, 108, 110, 125, 126, 130, 139, 141, 145, 146], "iv_2": 51, "iv_var": [54, 80], "iv\u00e1n": [113, 144], "j": [7, 8, 9, 10, 12, 13, 15, 16, 17, 18, 19, 25, 26, 52, 53, 54, 56, 57, 67, 69, 73, 80, 83, 87, 90, 92, 98, 99, 106, 129, 141, 143], "j_": [54, 80], "j_0": 129, "j_1": [54, 80], "j_2": [54, 80], "j_3": [54, 80], "j_k": [54, 80], "jame": 144, "janari": [60, 63], "janni": [55, 81], "januari": 60, "jasenakova": 145, "javanmard": 144, "jbe": [54, 80], "jeconom": [9, 10, 25, 26, 53], "jerzi": 144, "jia": 89, "jitter": [24, 66], "jitter_strength": 66, "jitter_valu": 24, "jk": [99, 107], "jmlr": [56, 141, 143, 145], "job": [55, 81, 82], "john": 144, "joint": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 60, 61, 62, 63, 67, 70, 71, 72, 74, 75, 77, 82, 85, 87, 99, 101, 129, 145, 146], "jointli": [85, 97], "jonathan": 144, "joss": [56, 98, 141, 143], "journal": [7, 8, 9, 10, 16, 17, 19, 25, 26, 53, 54, 56, 73, 80, 83, 87, 89, 92, 98, 141, 143, 144, 145], "jss": 141, "jump": [84, 86, 99], "jun": [53, 144], "jupyt": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90], "juraj": 144, "just": [53, 56, 58, 59, 60, 63, 67, 72, 74, 75, 76, 77, 84, 85, 99, 114, 115, 116, 117, 118, 130, 131], "justif": [113, 130, 131], "k": [7, 10, 12, 13, 15, 16, 17, 19, 25, 26, 52, 54, 56, 69, 78, 79, 80, 86, 87, 91, 92, 96, 97, 99, 129, 146], "k_h": [86, 99], "kaggl": [55, 81], "kallu": [72, 82, 85, 87, 88, 114, 120, 123, 124, 144], "kappa": 99, "kato": [16, 54, 80, 129, 144], "kb": [58, 61, 62, 66, 67, 76, 80, 81, 82, 88, 93, 94, 95, 143], "kde": [34, 35, 36, 81], "kdeplot": [58, 78, 90], "kdeunivari": [34, 35, 36], "kecsk\u00e9sov\u00e1": 145, "keel": 87, "keep": [43, 47, 53, 75, 77, 89, 146], "kei": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 45, 54, 55, 70, 71, 74, 75, 79, 80, 81, 82, 86, 89, 98, 99, 114, 130, 136, 145], "keith": 144, "kelz": 87, "kengo": 144, "kennedi": 87, "kernel": [34, 35, 36, 40, 43, 47, 86, 99], "kernel_regress": 86, "kernelreg": 86, "keyword": [16, 17, 18, 25, 26, 29, 33, 39, 41, 44], "kf": 113, "kfold": [80, 113], "kind": [51, 68, 81], "kj": [10, 12, 13, 15, 16, 17, 19, 25, 26, 52, 54, 69, 80, 92], "klaassen": [12, 73, 78, 79, 89, 141, 144], "klaa\u00dfen": 12, "knau": 144, "know": [58, 84, 87], "knowledg": [51, 68, 78, 83, 84], "known": [76, 78, 86, 87, 89, 98, 99, 105], "kohei": 144, "kotthof": 56, "kotthoff": [56, 98, 141, 143], "krueger": 83, "kueck": [55, 81], "kurz": [141, 144, 145], "kwarg": [9, 10, 14, 16, 17, 18, 25, 26, 29, 33, 39, 40, 41, 42, 44, 79, 99], "l": [54, 56, 57, 64, 70, 71, 80, 87, 89, 90, 98, 130, 139, 141, 143], "l1": [81, 90, 99], "l_hat": [38, 39, 52, 69, 114], "lab": 57, "label": [21, 24, 42, 46, 59, 66, 67, 69, 70, 71, 72, 74, 75, 77, 79, 82, 83, 85, 86], "labor": 83, "laffer": 144, "laff\u00e9r": [19, 57, 90, 99, 112, 114, 127, 128], "lal": [83, 145], "lambda": [54, 55, 56, 57, 60, 63, 81, 83, 84, 98, 99, 114, 115, 116, 129, 143], "lambda_": 73, "lambda_0": [114, 115, 116], "lambda_t": [26, 63], "land": 84, "lang": [56, 98, 141, 143], "langl": [11, 84], "lanni": 87, "lappli": 113, "larg": [52, 69, 76, 78, 79, 83, 89, 99], "larger": [33, 53, 86, 89, 130, 136], "largest": 78, "largli": 78, "lasso": [54, 55, 56, 57, 61, 81, 90, 98, 143, 144], "lasso_class": [55, 81], "lasso_pip": [56, 98], "lasso_summari": 81, "lassocv": [37, 61, 73, 80, 81, 90, 98, 99, 129, 143], "last": [26, 56, 142], "late": [32, 51, 55, 81, 87, 99, 108, 114, 121], "latent": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 88, 130, 139, 140], "later": [55, 56, 86, 89, 98, 146], "latest": 141, "latter": [46, 47, 87, 99], "layout": 73, "lbrace": [12, 13, 19, 32, 33, 54, 80, 91, 99, 106, 108, 109, 113, 114, 119, 129, 130, 137], "ldot": [38, 39, 54, 57, 80, 90, 91, 99, 110, 111, 113, 129, 143], "le": [26, 58, 84, 97, 99, 101, 114, 123, 124], "lead": [53, 89, 99], "leadsto": 129, "lear": [56, 98, 141, 143], "learn": [4, 5, 6, 7, 8, 16, 17, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 55, 56, 61, 64, 67, 68, 72, 73, 77, 78, 79, 81, 82, 83, 85, 86, 87, 89, 93, 95, 96, 98, 113, 114, 129, 130, 131, 145, 146], "learner": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55, 57, 58, 60, 61, 62, 63, 66, 69, 70, 71, 73, 80, 81, 82, 88, 89, 90, 91, 92, 96, 99, 101, 102, 104, 113, 114, 129, 130, 136, 145, 146], "learner_class": [37, 145], "learner_cv": 56, "learner_dict": 66, "learner_forest_classif": 56, "learner_forest_regr": 56, "learner_l": 88, "learner_lasso": 56, "learner_list": 78, "learner_m": 88, "learner_nam": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 66, 98], "learner_pair": 66, "learner_param_v": 56, "learner_rf": 129, "learnerclassif": 56, "learnerregr": 56, "learnerregrcvglmnet": 56, "learnerregrrang": [56, 98], "learning_r": [60, 63, 69, 72, 82, 85, 86, 89, 92], "least": [51, 55, 68, 81, 82, 88, 99, 103, 113], "leav": [57, 89, 90], "left": [12, 13, 15, 16, 19, 25, 52, 54, 66, 67, 69, 78, 80, 81, 82, 83, 85, 86, 92, 99, 114, 115, 116, 117, 118, 129, 130, 132, 133, 134, 135, 137, 138], "legend": [55, 59, 60, 63, 66, 67, 69, 70, 71, 72, 74, 75, 77, 78, 82, 83, 85], "lemp": 61, "len": [66, 67, 72, 78, 79, 80, 82, 85, 87], "length": [20, 21, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 56, 58, 60, 63, 87, 98], "leq": [54, 80], "less": [53, 55, 81, 82, 86, 89], "lester": 144, "let": [9, 10, 14, 25, 26, 52, 53, 55, 56, 57, 58, 60, 63, 66, 67, 69, 72, 74, 75, 77, 78, 81, 82, 85, 89, 90, 91, 92, 98, 99, 101, 105, 112, 130, 131, 140, 146], "level": [14, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 54, 55, 57, 58, 59, 60, 61, 62, 63, 66, 67, 70, 71, 72, 74, 75, 76, 77, 80, 81, 82, 85, 87, 88, 89, 90, 98, 106, 107, 114, 119, 130, 136, 137, 146], "level_0": [56, 80], "level_1": 80, "level_bound": [66, 67], "leverag": 66, "levi": 87, "levinsohn": [54, 80], "lewi": 144, "lgbm": 66, "lgbmclassifi": [58, 59, 60, 63, 66, 72, 78, 82, 85, 86, 89], "lgbmregressor": [58, 59, 60, 63, 66, 69, 72, 78, 82, 86, 89, 92], "lgr": [52, 53, 54, 55, 56, 57, 91, 98, 99, 113, 114, 129, 143], "lib": [60, 61, 62, 63, 66, 80, 81, 89], "liblinear": [81, 90, 99], "librari": [51, 52, 53, 54, 55, 56, 57, 66, 91, 92, 93, 95, 98, 99, 113, 114, 129, 142, 143, 146], "licens": [141, 145], "lie": 144, "lightgbm": [58, 59, 60, 63, 66, 69, 72, 78, 82, 85, 86, 89], "like": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 53, 55, 56, 60, 61, 62, 63, 73, 75, 81, 82, 89, 98, 99, 100, 113, 143, 146], "lim": 83, "lim_": [86, 99], "limegreen": [70, 71], "limit": [83, 99, 105, 144], "limits_": 97, "lin": [86, 89, 99], "line": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 59, 61, 66, 67, 89], "line_width": 66, "linear": [9, 10, 14, 15, 16, 17, 18, 25, 27, 28, 29, 33, 38, 39, 44, 51, 52, 53, 54, 56, 58, 59, 61, 66, 67, 68, 69, 70, 71, 73, 74, 77, 78, 79, 80, 87, 88, 89, 91, 92, 96, 97, 98, 102, 103, 104, 110, 111, 113, 115, 116, 117, 118, 119, 121, 122, 125, 126, 129, 136, 138, 139, 140, 141, 143, 144, 145, 146], "linear_learn": [60, 63], "linear_model": [29, 33, 37, 39, 44, 60, 61, 62, 63, 64, 66, 67, 68, 73, 77, 78, 80, 81, 86, 87, 89, 90, 98, 99, 129, 143], "linearli": [86, 99], "linearregress": [51, 60, 61, 62, 63, 66, 67, 68, 77, 78, 86, 87, 89], "linearscoremixin": [0, 114], "lineplot": [60, 63, 66, 67], "linestyl": [59, 60, 63, 66, 67, 79, 86], "linetyp": 57, "linewidth": [59, 60, 63, 66], "link": [89, 145], "linspac": [70, 71, 77, 89], "lint": 145, "linux": 142, "list": [4, 5, 6, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 43, 47, 52, 53, 54, 55, 56, 60, 61, 63, 69, 70, 71, 80, 82, 84, 92, 98, 113, 114, 142, 145], "list_confset": 32, "listedcolormap": 80, "literatur": [89, 99, 101, 105], "littl": [76, 87], "ll": [56, 129, 146], "lllllllllllllllll": [93, 95, 143], "lm": [51, 53, 89], "ln_alpha_ml_l": 73, "ln_alpha_ml_m": 73, "load": [51, 53, 55, 56, 73, 81, 82, 93, 95, 142, 143], "loader": 0, "loc": [59, 60, 61, 62, 63, 66, 67, 69, 72, 73, 75, 80, 83, 85, 88, 89], "local": [32, 34, 66, 87, 97, 99, 108, 144, 145], "localconvert": 80, "locat": [72, 85, 99], "log": [54, 60, 61, 62, 63, 66, 73, 78, 80, 83, 88, 98, 99, 101, 102, 104], "log_odd": 84, "log_p": [54, 80], "log_reg": [51, 53], "logarithm": [66, 73], "logic": [32, 56, 98], "logical_not": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 78, 98], "logist": [9, 41, 51, 53, 55, 57, 61, 66, 67, 68, 81, 87, 89, 90, 146], "logisticregress": [51, 60, 61, 62, 63, 64, 66, 67, 68, 77, 86, 87, 89], "logisticregressioncv": [37, 61, 78, 81, 90, 99], "logit": [78, 83], "loglik": 56, "logloss": [55, 66, 81, 146], "logloss_m": 66, "logo": 145, "logspac": 81, "long": [6, 9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 61, 69, 78, 88, 89, 130, 131, 140, 144], "longer": [60, 63, 99, 103], "look": [53, 55, 56, 58, 59, 60, 61, 62, 63, 72, 78, 81, 82, 85, 86, 88], "loop": [66, 67, 87], "loss": [60, 61, 62, 63, 66, 78, 79, 86, 88, 98, 99, 101, 102, 104], "loss_ml_g0": 78, "loss_ml_g1": 78, "loss_ml_m": 78, "low": [59, 76, 87, 97, 144], "lower": [32, 55, 56, 59, 60, 63, 66, 67, 72, 73, 76, 77, 82, 83, 85, 86, 87, 88, 89, 98, 130, 136, 140, 146], "lower_bound": [70, 71], "lpop": 61, "lpq": [34, 36, 82, 97, 123, 145], "lpq_0": 85, "lpq_1": 85, "lqte": 97, "lr": 86, "lrn": [51, 52, 53, 54, 55, 56, 57, 91, 98, 99, 113, 114, 129, 143, 146], "lrn_0": 56, "lt": [51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 76, 80, 81, 82, 84, 88, 89, 90, 93, 95, 143], "lucien": 145, "luka": 144, "luk\u00e1\u0161": 19, "lusd": [56, 64, 93, 95, 143], "lvert": 73, "m": [6, 7, 8, 9, 15, 16, 17, 24, 37, 52, 54, 56, 60, 63, 64, 66, 69, 73, 76, 78, 79, 80, 83, 87, 92, 94, 95, 96, 97, 98, 99, 100, 102, 104, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145], "m_": [66, 67, 99, 102, 104, 106, 114, 116, 118, 119, 123, 129], "m_0": [17, 18, 20, 22, 23, 24, 29, 31, 32, 33, 35, 38, 39, 40, 52, 54, 55, 69, 73, 76, 79, 80, 81, 92, 97, 98, 99, 108, 109, 110, 111, 114, 115, 116, 117, 118, 120, 123, 124, 126, 127, 128, 143, 146], "m_hat": [32, 33, 38, 39, 52, 69, 77, 114], "m_i": [86, 99], "ma": [16, 54, 80, 87, 99, 100, 144], "mac": 142, "machin": [4, 5, 6, 7, 8, 16, 17, 18, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 55, 56, 57, 58, 60, 61, 62, 63, 64, 67, 68, 72, 73, 77, 78, 79, 81, 82, 83, 85, 86, 87, 88, 89, 90, 96, 98, 99, 101, 102, 104, 113, 114, 129, 130, 131, 145, 146], "machineri": [73, 144], "mackei": 144, "maco": 142, "made": [99, 112, 146], "mae": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 78, 98], "maggi": 144, "magnitud": [130, 131], "mai": [43, 47, 57, 58, 90], "main": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 62, 66, 73, 82, 89, 99, 129, 130, 131, 144, 146], "mainli": 89, "maintain": [53, 141, 145], "mainten": 145, "major": [56, 89, 145], "make": [51, 66, 67, 68, 78, 79, 89, 97, 98, 145, 146], "make_confounded_irm_data": [89, 145], "make_confounded_plr_data": 88, "make_did_cs2021": [6, 24, 60, 94, 95, 99, 100, 104], "make_did_cs_cs2021": [63, 99, 102], "make_did_sz2020": [20, 23, 58, 99, 101], "make_heterogeneous_data": [70, 71, 74, 75, 76], "make_iivm_data": [32, 34, 97, 99], "make_irm_data": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 77, 78, 97, 98, 99], "make_irm_data_discrete_treat": [66, 67], "make_pipelin": 81, "make_pliv_chs2015": [38, 99], "make_pliv_multiway_cluster_ckms2021": [4, 54, 80], "make_plr_ccddhnr2018": [5, 6, 20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 69, 79, 91, 92, 97, 98, 99, 113, 114, 129, 130, 136], "make_simple_rdd_data": [40, 86, 99], "make_spd_matrix": 18, "make_ssm_data": [57, 90, 99], "malt": [141, 144], "man": [51, 68], "manag": [98, 142], "mani": [15, 27, 28, 52, 53, 54, 56, 58, 69, 79, 80, 92, 114, 129, 146], "manili": 44, "manipul": [55, 56, 86, 99], "manual": [55, 77, 79, 88, 146], "mao": 144, "map": [32, 42, 43, 46, 47, 53, 54, 80, 97, 99, 108], "mapsto": [91, 97], "mar": [19, 99], "march": [73, 78, 79], "margin": [70, 71, 89], "marit": [55, 81], "marker": [60, 63, 67, 89], "markers": 83, "market": 83, "markettwo": 54, "markov": [18, 144], "marr": [55, 81, 82, 88, 146], "marshal": 98, "martin": [19, 89, 141, 144, 145], "masatoshi": 144, "masip": [87, 145], "mask": 24, "maskedarrai": [99, 100], "master": [53, 61], "mat": 54, "match": [98, 130, 139], "math": [37, 60, 61, 62, 63], "mathbb": [9, 10, 14, 25, 26, 27, 28, 32, 33, 38, 39, 54, 57, 58, 59, 60, 63, 66, 67, 76, 78, 79, 80, 83, 86, 90, 97, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 126, 127, 128, 129, 130, 132, 133, 134, 135, 136, 137, 138, 139, 140, 143, 146], "mathcal": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 41, 52, 54, 57, 59, 69, 72, 80, 84, 85, 90, 92, 99, 100, 103, 105], "mathop": 97, "mathrm": [9, 10, 60, 61, 62, 63, 86, 99, 100, 102, 103, 104, 105, 114, 116, 118, 130, 133, 135], "matia": 144, "matplotlib": [21, 24, 58, 59, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 72, 74, 75, 77, 78, 79, 80, 81, 82, 83, 85, 86, 89, 90], "matric": [84, 145], "matrix": [9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 43, 47, 52, 54, 55, 56, 57, 69, 80, 90, 92, 93, 95, 98, 129, 143, 145, 146], "matt": 144, "matter": [78, 83], "max": [25, 55, 56, 66, 77, 81, 82, 87, 91, 97, 98, 99, 113, 114, 116, 118, 120, 129, 130, 133, 135, 143, 146], "max_depth": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 64, 81, 88, 91, 97, 98, 99, 101, 113, 114, 129, 130, 136, 143, 146], "max_featur": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 64, 81, 88, 91, 97, 98, 99, 113, 114, 129, 130, 136, 143, 146], "max_it": [66, 80, 81, 89], "maxim": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 84, 97, 99], "maxima": 129, "maximum": [97, 98], "mb": [60, 63, 64, 90, 93, 95, 143], "mb706": 145, "mea": 12, "mean": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 51, 52, 54, 55, 58, 60, 63, 66, 67, 68, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 85, 87, 88, 89, 92, 98, 99, 129, 146], "mean_absolute_error": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 78, 98], "meant": [61, 97, 145], "measir": 88, "measur": [53, 56, 61, 73, 79, 87, 88, 89, 98, 99, 100, 112, 130, 131, 137, 138, 139, 140], "measure_col": 73, "measure_func": 53, "measure_pr": 53, "measures_r": 53, "mechan": [42, 43, 46, 47, 89, 99, 105], "median": [87, 89, 113], "medium": 66, "melt": 54, "membership": 89, "memori": [58, 60, 61, 62, 63, 64, 66, 67, 76, 80, 81, 82, 88, 90, 93, 94, 95, 143], "mention": [76, 97], "merg": [55, 81], "mert": [113, 144], "meshgrid": [70, 71, 89], "messag": [52, 53, 54, 55, 56, 57, 66, 143, 145], "meta": [42, 43, 46, 47, 98, 143], "metadata": [42, 43, 46, 47], "metadata_rout": [42, 43, 46, 47], "metadatarequest": [42, 43, 46, 47], "method": [4, 5, 6, 20, 21, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 67, 68, 69, 70, 71, 72, 74, 75, 77, 78, 80, 81, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 130, 131, 136, 141, 143, 145], "methodolog": 144, "methodologi": 89, "metric": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 46, 66, 98], "michael": 144, "michaela": 145, "michel": [141, 143], "michela": [19, 144], "mid": [55, 81, 83, 86, 99, 114, 126], "mid_point": [66, 67], "might": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 54, 61, 72, 77, 78, 80, 84, 86, 88, 89, 98, 99], "mild": [52, 69, 92], "militari": 83, "miller": [54, 80], "mimic": 89, "min": [54, 55, 56, 57, 60, 61, 62, 63, 66, 72, 77, 80, 81, 82, 85, 86, 91, 98, 99, 103, 113, 114, 129, 143, 146], "min_": 97, "min_samples_leaf": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 76, 81, 88, 91, 97, 98, 99, 101, 113, 114, 129, 130, 136, 146], "min_samples_split": [81, 99, 100, 102, 104], "miniconda3": 66, "minim": [33, 45, 55, 78, 81, 86, 99], "minimum": 61, "minor": [52, 61, 69, 92, 114, 145], "minsplit": 55, "minut": 79, "miruna": 144, "mislead": 145, "miss": [4, 5, 6, 37, 56, 98, 99, 114, 127, 145], "missing": [19, 57, 90], "misspecif": 58, "misspecifi": 58, "mit": [141, 143], "mixin": [0, 27, 28, 114], "ml": [18, 54, 55, 56, 61, 73, 79, 80, 81, 86, 87, 91, 96, 98, 99, 113, 141, 144, 145], "ml_g": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 55, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 72, 74, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 97, 98, 99, 100, 101, 102, 104, 145], "ml_g0": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 53, 55, 58, 60, 61, 62, 64, 78, 81, 88, 98, 99, 101, 104], "ml_g1": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 53, 55, 58, 60, 61, 62, 64, 78, 81, 88, 98, 99, 101, 104], "ml_g_d0": [90, 99], "ml_g_d0_t0": [58, 63, 99, 101, 102], "ml_g_d0_t1": [58, 63, 99, 101, 102], "ml_g_d1": [90, 99], "ml_g_d1_t0": [58, 63, 99, 101, 102], "ml_g_d1_t1": [58, 63, 99, 101, 102], "ml_g_d_lvl0": [66, 99], "ml_g_d_lvl1": [66, 99], "ml_g_sim": 37, "ml_l": [38, 39, 52, 54, 55, 56, 64, 69, 71, 75, 79, 80, 81, 83, 88, 91, 92, 98, 99, 113, 114, 129, 130, 136, 143, 145, 146], "ml_l_bonu": 143, "ml_l_forest": 56, "ml_l_forest_pip": 56, "ml_l_lasso": 56, "ml_l_lasso_pip": 56, "ml_l_rf": 146, "ml_l_sim": 143, "ml_l_tune": 98, "ml_l_xgb": 146, "ml_m": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 130, 136, 143, 145, 146], "ml_m_bench_control": 89, "ml_m_bench_treat": 89, "ml_m_bonu": 143, "ml_m_forest": 56, "ml_m_forest_pip": 56, "ml_m_lasso": 56, "ml_m_lasso_pip": 56, "ml_m_rf": 146, "ml_m_sim": [37, 143], "ml_m_tune": 98, "ml_m_xgb": 146, "ml_pi": [37, 57, 90, 99], "ml_pi_sim": 37, "ml_r": [32, 38, 51, 54, 55, 68, 80, 81, 87, 99, 145], "ml_r0": 99, "ml_r1": [55, 81, 99], "mlr": [56, 98], "mlr3": [51, 52, 53, 54, 55, 57, 91, 98, 99, 113, 114, 129, 141, 143, 145, 146], "mlr3book": [56, 98], "mlr3extralearn": [55, 98], "mlr3filter": 56, "mlr3learner": [51, 52, 53, 54, 55, 91, 98, 99, 113, 114, 129, 143, 146], "mlr3measur": 53, "mlr3pipelin": [98, 145], "mlr3tune": [56, 98, 145], "mlr3vers": 55, "mlrmeasur": 53, "mode": [89, 142], "model": [0, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 47, 48, 51, 52, 53, 54, 56, 58, 59, 60, 61, 62, 63, 68, 69, 72, 73, 76, 78, 80, 82, 85, 88, 91, 92, 93, 94, 95, 96, 98, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 115, 116, 117, 118, 119, 121, 122, 125, 126, 131, 136, 137, 138, 139, 140, 141, 144, 145], "model_data": [55, 81], "model_label": 79, "model_list": 66, "model_select": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 69, 80, 98, 113], "modellist": [66, 77], "modelmlestimatelowerupp": 55, "modern": [56, 98, 141, 143], "modul": [66, 86, 99, 142], "moment": [27, 28, 54, 80, 99, 102, 103, 104, 114, 129, 130, 131, 140, 143], "monoton": 99, "mont": [9, 10, 11, 14, 70, 71, 74, 75], "montanari": 144, "month": [60, 63], "more": [33, 44, 51, 53, 55, 60, 61, 62, 63, 66, 67, 68, 70, 71, 73, 77, 78, 79, 81, 82, 86, 88, 89, 91, 97, 98, 99, 100, 101, 102, 103, 104, 105, 112, 114, 122, 129, 130, 131, 136, 140, 143, 146], "moreov": [55, 56, 61, 73, 98, 129, 146], "mortgag": [55, 81, 82], "most": [55, 72, 78, 81, 82, 85, 89, 97, 98, 99, 130, 136, 142], "motiv": [89, 92], "motivation_example_bch": 73, "mp": 53, "mpd": [54, 80], "mpdta": 61, "mpg": 80, "mse": [56, 73, 98], "mserd": 86, "msg": [60, 66], "msr": [56, 98], "mtry": [55, 56, 91, 98, 99, 113, 114, 129, 146], "mu": 59, "mu_": 59, "mu_0": 99, "mu_mean": 59, "much": [55, 56, 66, 81, 86, 87, 89, 146], "muld": [64, 93, 95, 143], "multi": [24, 42, 46, 53, 54, 70, 71, 80, 114, 116, 118, 145], "multiclass": [56, 79], "multiindex": 80, "multioutput": [43, 47], "multioutputregressor": [43, 47], "multipl": [4, 5, 6, 14, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 54, 55, 57, 58, 61, 62, 66, 77, 80, 81, 88, 89, 90, 93, 95, 98, 103, 107, 110, 113, 129, 130, 131, 144, 145, 146], "multipletest": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39], "multipli": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 69, 96, 97, 114, 146], "multiprocess": [72, 82, 85], "multitest": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39], "multivariate_norm": 37, "multiwai": [16, 54, 80, 144], "music": 144, "must": [20, 21, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 98, 99], "mutat": 56, "mutual": [29, 33, 39, 55, 74, 75, 81, 82, 97], "my_sampl": 113, "my_task": 113, "n": [9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 25, 26, 29, 31, 32, 33, 34, 35, 37, 38, 39, 41, 51, 52, 54, 56, 57, 59, 60, 63, 66, 67, 68, 69, 72, 73, 76, 80, 83, 84, 85, 86, 87, 90, 91, 92, 97, 98, 99, 105, 113, 129, 141, 142], "n_": [14, 59, 63, 130, 133, 135], "n_aggreg": 21, "n_coef": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 130, 136], "n_color": [60, 63], "n_complier": 85, "n_core": [72, 82, 85], "n_estim": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 58, 59, 60, 63, 64, 66, 69, 70, 71, 72, 74, 75, 76, 81, 82, 84, 85, 86, 88, 89, 91, 92, 97, 98, 99, 101, 113, 114, 129, 130, 136, 143, 146], "n_eval": [56, 98], "n_featur": [42, 43, 46, 47], "n_fold": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 54, 55, 58, 60, 61, 63, 64, 69, 70, 71, 72, 74, 75, 77, 78, 80, 81, 82, 83, 84, 85, 86, 88, 89, 92, 98, 113, 143, 146], "n_folds_per_clust": [54, 80], "n_folds_tun": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "n_framework": 21, "n_iter": [40, 86, 99], "n_iter_randomized_search": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 98], "n_job": 81, "n_jobs_cv": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 78], "n_jobs_model": [24, 30, 36, 72, 82, 85], "n_learner": 66, "n_level": [14, 66, 67], "n_ob": [6, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 44, 45, 52, 56, 57, 58, 59, 60, 63, 66, 67, 69, 70, 71, 74, 75, 76, 77, 78, 79, 86, 88, 89, 90, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 104, 113, 129, 130, 136, 143], "n_output": [42, 43, 46, 47], "n_period": [25, 60, 63], "n_pre_treat_period": [25, 60, 63], "n_rep": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 54, 57, 58, 60, 63, 64, 66, 67, 69, 76, 77, 78, 80, 86, 88, 89, 90, 92, 98, 113, 130, 136, 143, 146], "n_rep_boot": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 60, 61, 62, 63, 67, 70, 71, 72, 74, 75, 77, 82, 85, 129], "n_sampl": [42, 43, 46, 47, 84, 87], "n_samples_fit": [43, 47], "n_split": 113, "n_t": 59, "n_target": [46, 47], "n_theta": 21, "n_time_period": 59, "n_true": [72, 85], "n_var": [52, 56, 69, 92, 93, 95, 98, 129, 143], "n_w": 84, "n_x": [11, 70, 71, 74, 75, 76], "na": [4, 5, 6, 52, 54, 57, 92, 145], "na_real_": [54, 145], "naiv": [52, 69, 92], "name": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 52, 53, 54, 60, 61, 63, 66, 74, 75, 76, 79, 80, 86, 88, 89, 98, 142, 145], "namespac": 53, "nan": [4, 5, 6, 20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 58, 59, 66, 67, 69, 72, 74, 75, 78, 79, 81, 82, 85, 90, 92, 98], "nanmean": 69, "narita": 144, "nat": [60, 63], "nathan": 144, "nation": [89, 113, 144], "nativ": 53, "natt": 84, "natur": 89, "nbest": 66, "ncol": [54, 55, 56, 86, 93, 95, 98, 129, 143], "ncoverag": 78, "ndarrai": [4, 5, 6, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 93, 95], "nearli": 78, "necess": [54, 80], "necessari": [53, 54, 66, 79, 80, 86, 99, 142], "need": [20, 22, 23, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 53, 55, 57, 66, 68, 69, 79, 82, 87, 90, 98, 113, 130, 140, 145, 146], "neg": [43, 47], "neighborhood": [86, 129], "neither": [4, 5, 6, 54, 80, 93, 95], "neng": 144, "neq": [66, 86, 99], "nest": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 46, 47, 98, 114, 128, 130, 136], "net": [82, 88, 146], "net_tfa": [55, 81, 82, 88, 146], "network": 66, "nev": [99, 102, 104, 105], "never": [25, 32, 53, 54, 60, 61, 62, 63, 75, 80, 99, 105, 145], "never_tak": [32, 55, 81], "never_tr": [22, 24, 60, 61, 62, 63, 99, 100, 102, 104], "nevertheless": 77, "new": [51, 52, 53, 54, 55, 56, 57, 70, 71, 79, 81, 84, 91, 92, 93, 95, 97, 98, 99, 113, 114, 129, 141, 143, 144, 145, 146], "new_data": [70, 71, 84], "newei": [7, 8, 17, 54, 73, 80, 89, 92, 141, 144], "newest": 145, "next": [53, 55, 56, 70, 71, 72, 76, 78, 81, 82, 84, 85, 87, 89, 145], "neyman": [54, 80, 91, 96, 130, 140, 141, 144], "nfold": [54, 55, 57, 99], "nh": 99, "nice": [53, 66], "nifa": [81, 82, 88], "nil": 89, "nine": [54, 80], "nlogloss": 66, "nn": 86, "noack": [86, 99, 144, 145], "node": [55, 56, 91, 99, 113, 114, 129, 143, 146], "nois": [41, 83, 84], "nomin": 87, "non": [16, 17, 18, 22, 25, 26, 32, 40, 51, 52, 55, 59, 60, 63, 66, 68, 69, 81, 82, 84, 86, 98, 113, 114, 116, 118, 129, 130, 133, 135], "non_orth_scor": [52, 69, 114], "nondur": 64, "none": [4, 5, 6, 14, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 54, 55, 58, 60, 61, 62, 63, 64, 66, 67, 68, 76, 81, 82, 87, 88, 89, 90, 93, 94, 95, 98, 99, 101, 102, 104, 114, 129, 142, 143], "nonignor": [37, 128], "nonlinear": [25, 28, 55, 60, 63, 81, 86, 99, 114, 123, 124, 145], "nonlinearscoremixin": [0, 114], "nonparametr": [34, 35, 36, 86, 89, 114, 130, 131, 137, 138, 139, 140, 144], "nop": 56, "nor": [4, 5, 6, 54, 80, 93, 95], "norm": 69, "normal": [20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 57, 58, 59, 60, 62, 63, 68, 69, 72, 76, 82, 83, 84, 85, 86, 87, 90, 92, 93, 95, 98, 99, 114, 115, 116, 117, 118, 129, 143], "normalize_ipw": [29, 30, 31, 32, 33, 34, 35, 36, 37, 57, 77, 82, 90], "not_yet_tr": [22, 24, 60, 63], "notat": [54, 57, 58, 80, 90, 99, 101, 102, 103, 104, 105, 112, 114, 116, 118], "note": [4, 5, 6, 21, 24, 27, 28, 32, 33, 37, 38, 39, 42, 43, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 113, 114, 141, 143], "notebook": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 97, 98, 99, 146], "notic": [51, 68, 87], "now": [53, 54, 55, 57, 61, 62, 66, 70, 71, 78, 80, 81, 84, 87, 89, 90, 143, 145], "np": [4, 5, 6, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 58, 59, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 130, 136, 143, 146], "nrmse": 66, "nround": [52, 55, 146], "nrow": [53, 54, 56, 86, 93, 95, 98, 129, 143], "nu": [18, 26, 32, 57, 90, 99, 108, 130, 131, 133, 135, 136, 139, 140], "nu2": [60, 66, 130, 136], "nu_0": [130, 140], "nu_i": [57, 90], "nuis_g0": 51, "nuis_g1": 51, "nuis_l": 146, "nuis_m": [51, 146], "nuis_r0": 51, "nuis_r1": 51, "nuis_rmse_ml_l": 73, "nuis_rmse_ml_m": 73, "nuisanc": [4, 5, 6, 17, 18, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 52, 53, 54, 55, 56, 57, 58, 61, 66, 69, 70, 71, 72, 73, 76, 78, 80, 81, 82, 85, 87, 88, 89, 90, 91, 92, 98, 99, 102, 103, 104, 113, 114, 115, 116, 117, 118, 119, 123, 129, 130, 133, 135, 140, 141, 145, 146], "nuisance_el": [130, 132, 133, 134, 135, 137, 138, 139], "nuisance_loss": [66, 78, 98, 145], "nuisance_target": 78, "null": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 53, 88, 98, 130, 136, 145], "null_hypothesi": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 88, 130, 136], "num": [55, 56, 91, 98, 99, 113, 114, 129, 143], "num_leav": [59, 72, 82, 85], "number": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 47, 52, 54, 59, 60, 61, 63, 69, 70, 71, 72, 73, 74, 75, 78, 80, 82, 84, 85, 86, 89, 99, 103, 113, 129, 141, 143, 146], "numer": [28, 51, 56, 77, 83, 98, 114, 130, 137, 138, 145], "numeric_onli": 73, "numpi": [4, 5, 6, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 45, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 95, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 130, 136, 143], "nuniqu": [60, 63], "ny": 144, "nyt": [99, 102, 104, 105], "o": [59, 66, 67, 73, 74, 75, 78, 79, 81, 83, 86, 129, 141, 143], "ob": [53, 55, 59, 63, 86, 130, 133], "obei": 114, "obj": 81, "obj_dml_data": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 54, 61, 62, 68, 69, 72, 77, 79, 80, 85, 91, 92, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 130, 136, 145], "obj_dml_data_bonu": [93, 95], "obj_dml_data_bonus_df": [93, 95], "obj_dml_data_from_arrai": [4, 5, 6], "obj_dml_data_from_df": [4, 5], "obj_dml_data_sim": [93, 95], "obj_dml_plr": [52, 69, 92], "obj_dml_plr_bonu": [56, 143], "obj_dml_plr_bonus_pip": 56, "obj_dml_plr_bonus_pipe2": 56, "obj_dml_plr_bonus_pipe3": 56, "obj_dml_plr_bonus_pipe_ensembl": 56, "obj_dml_plr_fullsampl": 79, "obj_dml_plr_lesstim": 79, "obj_dml_plr_nonorth": [52, 69], "obj_dml_plr_orth_nosplit": [52, 69], "obj_dml_plr_sim": [56, 143], "obj_dml_plr_sim_pip": 56, "obj_dml_plr_sim_pipe_ensembl": 56, "obj_dml_plr_sim_pipe_tun": 56, "obj_dml_sim": 37, "object": [4, 5, 6, 7, 8, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 51, 55, 56, 57, 58, 60, 61, 62, 63, 64, 67, 70, 71, 72, 75, 76, 77, 79, 81, 82, 85, 86, 90, 93, 94, 95, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 141, 143, 144, 145, 146], "obs_confound": [51, 68], "observ": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 37, 38, 39, 41, 44, 45, 48, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 76, 78, 79, 80, 81, 82, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 104, 105, 112, 113, 114, 115, 116, 117, 118, 129, 130, 131, 132, 133, 134, 135, 143, 144, 146], "obtain": [10, 32, 51, 52, 53, 54, 57, 58, 60, 61, 62, 63, 68, 69, 70, 71, 72, 73, 78, 80, 85, 87, 89, 90, 91, 92, 97, 98, 113, 114, 129, 130, 131, 136, 142, 143], "obvious": [60, 63], "occur": [25, 60, 63, 79, 145], "off": [84, 144], "offer": [25, 53, 55, 81, 82, 89, 146], "offici": 142, "offset": 98, "often": 85, "oka": 144, "ol": [29, 33, 39, 44], "olma": [86, 99, 144, 145], "omega": [76, 97, 99, 100, 114, 119, 122, 130, 137, 138], "omega_": [16, 54, 80], "omega_1": [16, 54, 80], "omega_2": [16, 54, 80], "omega_epsilon": [54, 80], "omega_v": [16, 54, 80], "omega_x": [16, 54, 80], "omit": [60, 63, 88, 89, 99, 103, 114, 116, 118, 130, 131, 140, 144, 145, 146], "ommit": 89, "onc": [53, 79, 89, 99, 105, 146], "one": [21, 24, 38, 48, 51, 52, 53, 54, 55, 56, 60, 61, 62, 63, 66, 67, 68, 69, 70, 71, 78, 80, 82, 83, 86, 87, 88, 89, 92, 97, 98, 99, 100, 103, 105, 110, 113, 114, 115, 116, 117, 118, 122, 125, 126, 129, 130, 131, 136, 137, 138, 139, 143, 145], "ones": [56, 59, 72, 79, 85, 88, 97], "ones_lik": [66, 67, 85], "onli": [4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 46, 47, 53, 54, 55, 60, 63, 70, 71, 74, 75, 76, 78, 79, 80, 81, 82, 86, 91, 97, 98, 99, 102, 104, 112, 114, 116, 118, 120, 123, 124, 129, 130, 131, 133, 135, 137, 138, 140, 145], "onlin": 146, "onto": 78, "oo": 79, "oob_error": [56, 98], "oop": 145, "opac": [70, 71], "open": [56, 98, 141, 143], "oper": [56, 145], "opposit": [84, 86, 99], "oprescu": [11, 70, 71, 74, 75, 144], "opt": [60, 61, 62, 63, 81, 89], "optim": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 40, 56, 70, 71, 79, 84, 97, 98, 144], "option": [20, 21, 22, 23, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 51, 52, 54, 55, 57, 60, 61, 63, 67, 70, 71, 74, 75, 76, 78, 80, 81, 82, 90, 98, 99, 113, 114, 120, 123, 124, 129, 145], "oracl": [14, 41, 60, 63, 66, 67], "oracle_valu": [9, 10, 14, 41, 66, 67], "orang": 52, "orcal": [9, 10], "order": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 46, 53, 54, 55, 56, 61, 77, 80, 81, 86, 98, 99, 113, 114], "org": [12, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 98, 141, 142, 144, 145], "orient": [56, 98, 114, 141, 143, 144, 145], "origin": [42, 43, 45, 46, 47, 53, 56, 60, 61, 62, 63, 75, 84, 88, 89, 97, 114, 122], "orign": [55, 81], "orth_sign": [44, 45, 77], "orthogon": [44, 45, 54, 55, 60, 66, 80, 81, 91, 96, 99, 129, 130, 140, 141, 144], "orthongon": [130, 140], "osx": 142, "other": [4, 5, 6, 38, 39, 42, 43, 46, 47, 52, 54, 55, 56, 57, 58, 60, 61, 62, 63, 67, 69, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 92, 93, 95, 97, 98, 99, 110, 111, 113, 114, 122, 129, 130, 140, 141, 142, 143, 144, 145, 146], "other_ind": 80, "otherwis": [20, 22, 23, 29, 32, 33, 39, 42, 43, 46, 47, 55, 81, 82, 84, 99, 101, 114, 116, 118], "othrac": [56, 64, 93, 95, 143], "our": [52, 53, 55, 56, 58, 60, 63, 66, 69, 70, 71, 72, 78, 79, 81, 82, 85, 86, 88, 89, 92, 99, 141, 143, 145, 146], "ourselv": 78, "out": [38, 39, 54, 56, 58, 59, 60, 61, 62, 63, 64, 73, 78, 79, 80, 82, 88, 89, 90, 91, 93, 95, 96, 97, 98, 99, 100, 101, 102, 104, 114, 125, 126, 129, 130, 131, 136, 139, 141, 143, 145, 146], "outcom": [4, 5, 6, 9, 10, 14, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 51, 53, 54, 55, 56, 59, 60, 61, 62, 63, 64, 68, 73, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 93, 94, 95, 98, 101, 102, 104, 105, 106, 110, 111, 112, 116, 118, 119, 129, 131, 136, 137, 139, 140, 143, 145, 146], "outcome_0": 68, "outcome_1": 68, "outer": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 98], "outperform": 66, "output": [53, 60, 62, 63, 78, 87, 91, 99, 102, 104, 129, 146], "output_list": 87, "outshr": 80, "outsid": 52, "over": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 37, 38, 39, 52, 60, 61, 63, 67, 69, 73, 78, 92, 96, 98, 99, 100, 130, 136, 145], "overal": [21, 60, 61, 62, 63, 84, 89, 99, 100], "overall_aggregation_weight": [21, 60, 61, 62, 63], "overcom": [96, 114], "overfit": [79, 96, 113], "overlap": [58, 89, 99, 101, 105], "overrid": [98, 145], "overridden": 99, "overst": [55, 81, 82], "overview": [78, 129, 130, 136, 144], "overwrit": 145, "ownership": [55, 81], "p": [9, 10, 11, 13, 20, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 92, 97, 98, 99, 100, 101, 102, 104, 105, 113, 114, 115, 116, 117, 118, 119, 120, 123, 124, 127, 128, 129, 130, 137, 138, 141, 142, 143, 145], "p401": [55, 81, 82], "p_0": [114, 115, 116, 117], "p_1": 129, "p_adjust": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 113, 129, 141, 143], "p_dbl": [56, 98], "p_hat": 77, "p_int": 98, "p_n": 15, "p_val": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39], "p_x": [16, 54, 80], "p_x0": 83, "p_x1": 83, "packag": [51, 52, 54, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 79, 80, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 101, 105, 112, 113, 114, 129, 130, 131, 141, 143, 144, 145, 146], "packagedata": 80, "packagevers": 55, "page": [89, 141, 144], "pair": [51, 68], "pake": [54, 80], "paket": [54, 55, 56], "pal": 54, "palett": [21, 24, 60, 63, 66, 67], "pand": [60, 63], "panda": [4, 5, 6, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 44, 45, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 93, 94, 95, 97, 99, 130, 131, 143], "pandas2ri": 80, "panel": [5, 6, 20, 22, 24, 25, 26, 61, 63, 94, 95, 102, 103, 105, 134, 135, 144, 145], "paper": [12, 15, 56, 79, 83, 86, 88, 89, 130, 140, 141, 143, 144, 145], "par": 64, "par_grid": [56, 98], "paradox": [56, 98, 145], "parallel": [53, 58, 59, 60, 63, 67, 72, 78, 85, 99, 101, 103, 105], "param": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 79, 98], "param_grid": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 98], "param_nam": 53, "param_set": [56, 98], "param_v": 56, "paramet": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 55, 57, 58, 60, 61, 63, 66, 67, 69, 70, 71, 72, 73, 76, 77, 78, 80, 81, 84, 85, 86, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 101, 102, 103, 104, 105, 106, 107, 108, 109, 112, 113, 123, 124, 129, 130, 131, 136, 138, 140, 141, 143, 144, 145, 146], "parametr": [32, 53, 89, 92, 98, 146], "params_exact": 98, "params_nam": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 53], "parenttoc": 141, "part": [18, 52, 54, 55, 56, 62, 69, 78, 79, 80, 81, 92, 98, 113, 130, 140, 145, 146], "parti": 18, "partial": [10, 15, 16, 17, 18, 28, 38, 39, 54, 56, 64, 73, 79, 80, 88, 91, 96, 98, 110, 111, 113, 125, 126, 129, 136, 137, 138, 139, 140, 141, 143, 145, 146], "partial_": [114, 129], "partiallli": 88, "particip": [7, 82, 88, 146], "particular": [99, 141], "particularli": [66, 79], "partion": [54, 80], "partit": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 80, 91, 96], "partli": 146, "pass": [29, 33, 39, 42, 43, 44, 46, 47, 53, 56, 61, 79, 98, 146], "passo": [141, 143], "past": 54, "paste0": [54, 57], "pastel": 69, "path": [98, 99], "path_to_r": 73, "patsi": [70, 71, 97], "pattern": 89, "paul": 144, "pd": [4, 5, 6, 7, 8, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 58, 59, 60, 61, 62, 63, 66, 67, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 97, 99], "pdf": [69, 83], "pedregosa": [141, 143], "pedregosa11a": [141, 143], "pedro": [53, 144], "penal": [57, 61, 90], "penalti": [55, 56, 61, 68, 81, 87, 89, 90, 98, 99], "pennsylvania": [8, 93, 95, 143], "pension": [55, 81, 82, 146], "peopl": [55, 81, 82], "pep8": 145, "per": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 54, 60, 61, 62, 63, 80], "percent": 98, "percentag": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39], "perf_count": 78, "perfectli": [86, 99], "perform": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 45, 52, 54, 56, 58, 60, 61, 62, 63, 69, 73, 75, 76, 78, 79, 80, 82, 88, 89, 90, 92, 98, 99, 101, 102, 104, 113, 114, 129, 141, 143, 144, 146], "performance_result": 66, "perfrom": 76, "perhap": 146, "period": [20, 22, 24, 25, 53, 58, 59, 62, 94, 95, 100, 101, 102, 103, 104, 105, 116, 118, 144, 145], "perp": [99, 112], "perrot": [141, 143], "person": 146, "pessimist": 89, "peter": 144, "petra": 145, "petronelaj": 145, "pfister": [56, 98, 141, 143], "phi": [54, 80, 97, 129], "philipp": [89, 141, 144], "philippbach": [141, 145], "pi": [13, 15, 18, 37, 97, 99, 114, 127, 128], "pi_": [16, 54, 80], "pi_0": [114, 127, 128], "pi_i": [57, 90, 99], "pick": [86, 146], "pip": [86, 99], "pip3": 142, "pipe": 56, "pipe_forest_classif": 56, "pipe_forest_regr": 56, "pipe_lasso": 56, "pipelin": [42, 43, 46, 47, 56, 81, 145], "pipeop": 56, "pira": [55, 81, 82, 88, 146], "pivot": [66, 73, 80, 144], "pivot_logloss": 66, "pivot_rmse_g0": 66, "pivot_rmse_g1": 66, "plai": [79, 146], "plan": [7, 55, 81, 82, 146], "plausibl": [89, 130], "pleas": [42, 43, 46, 47, 53, 58, 59, 61, 67, 79, 89, 113, 141, 142], "plim": 83, "pliv": [27, 28, 38, 54, 80, 91, 97, 110, 125, 141, 145], "plm": [0, 96, 98, 129, 136, 146], "plot": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 45, 52, 53, 55, 56, 57, 59, 60, 61, 62, 63, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 81, 82, 83, 85, 86, 88, 89, 90, 97, 130, 136], "plot_data": [60, 63], "plot_effect": [21, 24, 60, 61, 62, 63], "plot_tre": [45, 84, 97], "plotli": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 70, 71, 73, 86, 89], "plr": [27, 28, 39, 56, 79, 83, 88, 91, 98, 111, 113, 126, 129, 136, 138, 139, 140, 141, 143, 145, 146], "plr_est": 83, "plr_est1": 83, "plr_est2": 83, "plr_obj": 83, "plr_obj_1": 83, "plr_obj_2": 83, "plr_summari": 81, "plrglmnet": 55, "plrranger": 55, "plrrpart": 55, "plrxgboost8700": 55, "plt": [58, 59, 60, 63, 64, 66, 67, 69, 70, 71, 72, 74, 75, 77, 78, 79, 80, 81, 82, 83, 85, 86, 89, 90], "plt_smpl": [54, 80], "plt_smpls_cluster": [54, 80], "plug": [76, 130, 132, 133, 134, 135, 136, 137, 138], "pm": [40, 54, 80, 129, 130, 136, 140], "pmatrix": [57, 90], "pmlr": [73, 78, 79], "po": [56, 98], "poe": 144, "point": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 53, 54, 66, 74, 75, 80, 89, 97, 99, 146], "pointwis": [44, 72, 74, 75, 85], "poli": [55, 77, 80, 81], "polici": [33, 38, 39, 45, 96, 99, 110, 111, 143, 144, 145], "policy_tre": [33, 84, 97], "policy_tree_2": 84, "policy_tree_obj": 97, "policytre": 84, "polit": 83, "poly_dict": 81, "polynomi": [7, 8, 41, 55, 64, 77, 81, 86], "polynomial_featur": [7, 8, 55, 64], "polynomialfeatur": [77, 80, 81], "poor": 87, "pop": [114, 116], "popul": [89, 99, 102, 104, 114, 118], "popular": [78, 99, 130, 131], "porport": 88, "posit": [18, 55, 60, 62, 63, 66, 83, 89, 146], "posixct": [56, 98], "possibl": [4, 5, 6, 43, 46, 47, 53, 56, 60, 62, 63, 70, 71, 74, 75, 76, 77, 78, 79, 84, 86, 87, 88, 89, 98, 99, 103, 106, 107, 129, 130, 131, 145, 146], "possibli": [87, 130, 131], "post": [15, 18, 24, 99, 101, 103, 105, 129, 144], "postdoubl": 144, "poster": 83, "potenti": [9, 14, 25, 29, 30, 31, 34, 35, 37, 41, 57, 58, 60, 63, 77, 83, 86, 90, 101, 105, 106, 112, 119, 120, 129, 137, 142, 145, 146], "potential_level": [66, 67], "power": [56, 79, 87, 89, 98, 144], "pp": [53, 73, 78, 79], "pq": [34, 35, 36, 82, 124, 145], "pq_0": [82, 85], "pq_1": [82, 85], "pr": [37, 51, 54, 55, 56, 57, 98, 99, 113, 114, 129, 143, 146], "practic": [78, 89, 144], "pre": [22, 24, 25, 53, 57, 58, 60, 61, 62, 63, 90, 98, 99, 101, 102, 103, 104, 114, 116, 118, 145], "precis": [53, 99, 130, 138, 146], "precomput": [43, 47], "pred": [53, 79], "pred_df": 84, "pred_dict": 98, "pred_treat": 84, "predict": [4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 52, 54, 55, 56, 61, 66, 69, 72, 73, 77, 78, 79, 80, 81, 84, 87, 89, 92, 97, 113, 130, 131, 136, 138, 145, 146], "predict_proba": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 39, 40, 42, 46, 79, 87, 98], "predictor": [29, 33, 39, 44, 45, 70, 71, 74, 75, 89, 91], "prefer": [55, 81, 82, 146], "preliminari": [31, 52, 69, 86, 114, 120, 123, 124, 128], "prepar": [53, 54, 80, 145], "preprint": [87, 144], "preprocess": [55, 61, 77, 80, 81, 82, 98], "presenc": [55, 81, 82], "present": [53, 61, 89, 98, 114, 122, 146], "prespecifi": 88, "pretreat": [20, 22, 23, 24, 53, 58], "prettenhof": [141, 143], "preval": 89, "prevent": [113, 145], "previou": [59, 76, 77, 83, 142, 146], "previous": [61, 87, 98, 146], "price": [54, 80], "priliminari": [34, 36], "primari": [66, 67], "principl": [130, 131], "print": [22, 24, 40, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 93, 94, 95, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 130, 136, 142, 143, 145, 146], "print_detail": 53, "print_period": [22, 24], "prior": [66, 78, 99, 112], "privat": 145, "prob": 56, "prob_dist": 87, "prob_dist_": 87, "probabilit": 76, "probabl": [14, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 46, 52, 53, 57, 58, 60, 63, 67, 69, 76, 83, 85, 86, 89, 90, 92, 99, 114, 115, 116, 117, 118, 123, 144], "problem": [55, 61, 81, 82, 97, 98], "procedur": [52, 54, 55, 69, 78, 80, 81, 88, 89, 98, 129, 142, 145], "proceed": [15, 144], "process": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 41, 53, 57, 58, 59, 62, 63, 70, 71, 72, 73, 74, 75, 78, 79, 84, 85, 89, 90, 96, 129, 130, 131, 144, 145], "produc": 83, "product": [70, 71, 73, 78, 89, 130, 140], "producton": 54, "program": [13, 55, 81, 82, 144, 146], "progress": 65, "project": [56, 70, 71, 97, 141, 145], "project_z": [70, 71], "prone": 114, "pronounc": 86, "propens": [9, 10, 25, 34, 36, 55, 57, 58, 60, 61, 63, 66, 76, 77, 78, 81, 82, 89, 90, 97, 99, 102, 104, 106, 114, 116, 118, 130, 137], "proper": 66, "properli": [66, 79, 146], "properti": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 56, 60, 61, 62, 63, 78, 81, 82, 83, 87, 88, 98, 99, 130, 136, 143, 145], "proport": [88, 130, 131, 139, 140], "propos": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 54, 56, 80, 86, 130, 131, 144, 145], "provid": [4, 5, 6, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 53, 54, 55, 56, 61, 66, 70, 71, 74, 75, 77, 79, 80, 81, 86, 89, 91, 92, 93, 94, 95, 96, 98, 129, 141, 143, 145, 146], "prune": [33, 45], "ps911c": 80, "ps944": 80, "pscore1": 83, "pscore2": 83, "psi": [27, 28, 52, 53, 54, 80, 91, 99, 102, 103, 104, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 140, 143], "psi_": [129, 130, 133, 135, 136, 139, 140], "psi_a": [27, 32, 33, 38, 39, 52, 54, 69, 80, 99, 102, 103, 104, 113, 114, 115, 116, 117, 118, 119, 121, 122, 125, 126, 129], "psi_b": [27, 32, 33, 38, 39, 52, 69, 97, 99, 102, 103, 104, 113, 114, 115, 116, 117, 118, 119, 121, 122, 125, 126], "psi_el": [113, 114], "psi_j": 129, "psi_nu2": [130, 136], "psi_sigma2": [130, 136], "public": [51, 68, 145], "publish": [89, 141, 145], "pull": [55, 145], "purchas": 89, "pure": 89, "purp": [70, 71], "purpos": [52, 69, 76, 88, 89, 114, 116, 118, 130, 131, 143], "pval": 129, "px": [73, 86], "py": [60, 61, 62, 63, 66, 75, 80, 81, 89, 141, 142, 145], "py3": 142, "py_al": 69, "py_did": 58, "py_did_pretest": 59, "py_dml": 69, "py_dml_nosplit": 69, "py_dml_po": 69, "py_dml_po_nosplit": 69, "py_double_ml_apo": 67, "py_double_ml_bas": 69, "py_double_ml_basic_iv": 68, "py_double_ml_c": 70, "py_double_ml_cate_plr": 71, "py_double_ml_cvar": 72, "py_double_ml_firststag": 73, "py_double_ml_g": 74, "py_double_ml_gate_plr": 75, "py_double_ml_gate_sensit": 76, "py_double_ml_irm_vs_apo": 77, "py_double_ml_learn": 78, "py_double_ml_meets_flaml": 79, "py_double_ml_multiway_clust": 80, "py_double_ml_pens": 81, "py_double_ml_pension_qt": 82, "py_double_ml_plm_irm_hetfx": 83, "py_double_ml_policy_tre": 84, "py_double_ml_pq": 85, "py_double_ml_rdflex": 86, "py_double_ml_robust_iv": 87, "py_double_ml_sensit": 88, "py_double_ml_sensitivity_book": 89, "py_double_ml_ssm": 90, "py_non_orthogon": 69, "py_panel": 60, "py_panel_data_exampl": 61, "py_panel_simpl": 62, "py_po_al": 69, "py_rep_c": 63, "py_tabpfn": 66, "pydata": 75, "pypi": [144, 145], "pyplot": [58, 59, 60, 63, 64, 66, 67, 69, 70, 71, 72, 74, 75, 77, 78, 79, 80, 81, 82, 83, 85, 86, 89, 90], "pyproject": 145, "pyreadr": 61, "python": [18, 53, 79, 89, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 104, 113, 114, 129, 130, 131, 136, 141, 143, 144, 145, 146], "python3": [60, 61, 62, 63, 81, 89, 142], "pytorch": 66, "q": [56, 72, 85, 86, 98, 141, 143], "q2": [56, 64, 93, 95, 143], "q3": [56, 64, 93, 95, 143], "q4": [56, 64, 93, 95, 143], "q5": [56, 64, 93, 95, 143], "q6": [56, 64, 93, 95, 143], "q_i": [86, 99], "qquad": 13, "qte": [72, 82, 145], "quad": [25, 26, 55, 57, 58, 81, 84, 86, 90, 97, 99, 101, 105, 112, 114, 123, 129, 130, 132, 133], "quadrat": [57, 90], "qualiti": [88, 91, 145], "quanitl": 82, "quant": 72, "quantifi": [89, 99, 105], "quantil": [14, 24, 30, 31, 34, 35, 36, 60, 63, 67, 72, 77, 88, 96, 98, 120, 123, 124, 144, 145], "quantiti": [51, 68, 89], "queri": 81, "question": [89, 146], "quick": 82, "quit": [66, 78, 84, 88, 130, 131], "r": [12, 32, 42, 43, 46, 47, 59, 60, 61, 62, 63, 69, 70, 71, 73, 80, 83, 86, 87, 89, 91, 92, 93, 95, 96, 99, 108, 113, 114, 121, 125, 129, 130, 131, 137, 138, 139, 140, 141, 143, 144, 145, 146], "r2_d": [13, 78], "r2_score": [43, 47], "r2_y": [13, 78], "r6": [56, 145], "r_0": [32, 38, 55, 81, 99, 108], "r_all": 52, "r_d": 13, "r_df": 80, "r_dml": 52, "r_dml_nosplit": 52, "r_dml_po": 52, "r_dml_po_nosplit": 52, "r_double_ml_bas": 52, "r_double_ml_basic_iv": 51, "r_double_ml_did": 53, "r_double_ml_multiway_clust": 54, "r_double_ml_pens": 55, "r_double_ml_pipelin": 56, "r_double_ml_ssm": 57, "r_hat": 38, "r_hat0": 32, "r_hat1": 32, "r_non_orthogon": 52, "r_po_al": 52, "r_y": 13, "rais": [4, 5, 6, 42, 43, 46, 47, 60, 61, 62, 63, 98], "randint": 83, "randn": 37, "random": [11, 14, 18, 19, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 51, 52, 53, 55, 56, 58, 59, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 97, 98, 100, 101, 102, 104, 105, 113, 127, 129, 130, 136, 140, 143, 144, 146], "random_search": 98, "random_st": [14, 60, 63, 69, 76, 77, 84], "randomforest": [55, 66, 78, 81], "randomforest_class": [55, 70, 81, 84], "randomforest_reg": [70, 84], "randomforestclassifi": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 64, 66, 67, 70, 71, 74, 75, 76, 78, 81, 84, 86, 88, 89, 97, 98, 99, 100, 101, 102, 104, 146], "randomforestregressor": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 64, 66, 67, 69, 70, 71, 74, 75, 76, 78, 81, 84, 86, 88, 89, 91, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 130, 136, 143, 146], "randomized_search": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 98], "randomizedsearchcv": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 98], "randomli": [52, 54, 69, 80, 92, 113, 146], "rang": [52, 58, 59, 66, 67, 69, 72, 74, 75, 78, 79, 80, 82, 84, 85, 86, 87, 89, 90, 92, 98, 99], "rangeindex": [58, 60, 61, 62, 63, 64, 66, 67, 76, 80, 81, 82, 88, 90, 93, 94, 95, 143], "ranger": [53, 55, 56, 91, 98, 99, 113, 114, 129, 143, 146], "rangl": [11, 84], "rank": 145, "rate": [73, 78, 99], "rather": [86, 89, 99], "ratio": [98, 113, 130, 131], "rational": 61, "ravel": [70, 71], "raw": [55, 61, 62, 73, 81], "raw_res_manual_lasso_r_100_n_100_p_200_rho_0": 73, "rbind": 55, "rbindlist": 55, "rbinom": 51, "rbrace": [12, 13, 19, 32, 33, 54, 80, 91, 99, 106, 108, 109, 113, 114, 119, 129, 130, 137], "rcolorbrew": 54, "rcparam": [59, 64, 70, 71, 72, 74, 75, 77, 80, 81, 82, 85], "rd": [99, 145], "rda": 61, "rdbu": 54, "rdbu_r": 80, "rdbwselect": 99, "rdd": [0, 4, 5, 6, 96, 142], "rdflex": [86, 99, 145], "rdflex_fuzzi": 86, "rdflex_fuzzy_stack": 86, "rdflex_obj": [40, 99], "rdflex_sharp": 86, "rdflex_sharp_stack": 86, "rdrobust": [40, 86, 99, 142, 145], "rdrobust_fuzzi": 86, "rdrobust_fuzzy_noadj": 86, "rdrobust_sharp": 86, "rdrobust_sharp_noadj": 86, "rdt044": 73, "re": [60, 66, 80, 89, 142], "read": [61, 142], "read_csv": [62, 73], "read_r": 61, "readabl": [66, 145], "reader": [61, 87], "readili": 141, "real": [55, 81, 82, 88, 130, 131], "realat": 99, "realiz": [86, 99, 112], "reason": [4, 5, 6, 51, 66, 68, 73, 78, 79, 88, 89, 130, 131, 146], "recal": [64, 130, 140], "receiv": [25, 60, 63, 67, 86, 99, 101, 103], "recent": [79, 99, 101, 105, 144], "recogn": [55, 81, 82], "recommend": [56, 60, 63, 66, 78, 86, 89, 91, 99, 113, 130, 142, 144, 145], "recov": [51, 53, 68, 83], "recreat": 66, "recsi": 144, "red": [54, 57, 66, 74, 75, 79, 80], "reduc": [55, 76, 79, 81, 86, 88, 89, 99, 145], "redund": 145, "reemploy": [8, 93, 95, 143], "ref": 61, "refactor": 145, "refer": [7, 8, 9, 10, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 59, 60, 63, 66, 67, 76, 81, 82, 86, 88, 93, 95, 96, 97, 99, 100, 102, 104, 105, 114, 130, 131, 136, 144, 145], "reference_level": [30, 66, 67, 77, 99], "refin": 145, "refit": [130, 131], "reflect": [84, 89, 97], "reg": [25, 26, 55, 81, 146], "reg_estim": 86, "reg_learn": 82, "reg_learner_1": 78, "reg_learner_2": 78, "regard": [89, 141], "regener": 145, "region": [54, 72, 80, 129, 144], "regr": [51, 52, 53, 54, 55, 56, 57, 91, 98, 99, 113, 114, 116, 129, 143, 146], "regravg": [56, 98], "regress": [9, 10, 12, 13, 14, 15, 16, 17, 18, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 44, 51, 53, 54, 56, 57, 60, 61, 62, 63, 66, 67, 68, 73, 79, 80, 83, 87, 88, 89, 90, 91, 92, 96, 97, 98, 101, 102, 104, 108, 109, 110, 111, 113, 118, 129, 131, 136, 137, 138, 139, 140, 141, 143, 144, 145, 146], "regressor": [43, 47, 52, 55, 66, 67, 69, 72, 78, 79, 81, 92], "regular": [15, 96, 98, 114, 129, 144], "reich": [56, 98], "reinforc": 144, "reject": [55, 81], "rel": [55, 61, 66, 81, 87, 99, 100, 130, 131, 137, 138], "relat": [77, 89, 146], "relationship": [51, 66, 68, 73, 89, 129], "relev": [4, 5, 6, 11, 20, 22, 23, 24, 42, 43, 44, 46, 47, 60, 63, 72, 84, 85, 99, 130, 146], "reli": [24, 58, 59, 60, 63, 70, 71, 76, 77, 97, 98, 99, 101, 114, 118, 130, 131, 146], "reload": 55, "remain": [53, 129, 146], "remark": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 52, 58, 59, 60, 61, 62, 63, 67, 69, 70, 71, 72, 74, 75, 76, 77, 78, 82, 88, 97, 98, 99, 100, 101, 102, 104, 113, 114, 115, 116, 117, 118, 123, 124, 129, 130, 133, 135, 138], "remot": 142, "remov": [55, 77, 89, 96, 99, 113, 114, 130, 145], "renam": [60, 63, 81, 145], "render": [88, 89], "reorgan": 145, "rep": [52, 57, 92, 98, 129], "repeat": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 54, 55, 56, 57, 60, 61, 62, 69, 76, 80, 81, 82, 83, 86, 88, 90, 92, 96, 98, 102, 103, 104, 105, 129, 132, 133, 143, 145, 146], "repeatedkfold": 80, "repet": 88, "repetit": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 61, 70, 71, 73, 74, 75, 76, 78, 96, 98, 129, 143, 145, 146], "replac": [84, 89, 145], "replic": [7, 8, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 55, 61, 62, 69, 73, 87, 89], "repo": 145, "report": [55, 79, 81, 141, 145], "repositori": [60, 63, 73, 86, 145], "repr": [52, 54], "repres": [25, 66, 83, 89, 99], "represent": [10, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 66, 88, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 143, 145], "reproduc": 14, "request": [42, 43, 46, 47, 145], "requir": [38, 39, 42, 46, 51, 55, 56, 60, 61, 62, 63, 66, 67, 76, 81, 82, 88, 99, 100, 102, 103, 104, 114, 116, 118, 129, 130, 131, 136, 142, 145, 146], "requirenamespac": 53, "rerun": 61, "res_df": 80, "res_dict": [9, 10, 11, 14, 41], "resampl": [51, 54, 56, 57, 58, 60, 61, 62, 63, 80, 82, 88, 90, 98, 99, 101, 102, 104, 113, 114, 129, 141, 143, 146], "resdat": 63, "research": [54, 56, 80, 83, 89, 113, 141, 143, 144, 146], "resembl": [57, 90], "reset": 53, "reset_index": [60, 63, 73, 80, 81], "reshap": [59, 69, 70, 71, 77], "reshape2": 54, "residu": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 88, 130, 131, 139, 140], "resolut": [56, 98], "resourc": 78, "resourcewis": 78, "respect": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 55, 60, 61, 62, 63, 67, 81, 82, 86, 97, 99, 103, 108, 112, 113, 130, 140, 146], "respons": [7, 56, 98], "rest": 99, "restart": 142, "restrict": 78, "restructur": 145, "restud": 73, "result": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 51, 52, 53, 56, 57, 58, 59, 60, 61, 62, 63, 66, 67, 69, 70, 71, 73, 76, 77, 78, 84, 86, 87, 88, 89, 90, 92, 98, 113, 114, 115, 116, 117, 118, 130, 131, 136, 143, 145], "result_iivm": 55, "result_irm": 55, "result_plr": 55, "results_df": 87, "retain": [42, 43, 46, 47], "retina": 83, "retir": [55, 81, 82, 88], "return": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 56, 57, 60, 61, 62, 63, 69, 72, 75, 78, 79, 80, 83, 84, 85, 87, 88, 89, 90, 91, 98, 114, 130, 131, 145], "return_count": [66, 67, 78], "return_tune_r": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "return_typ": [4, 5, 6, 7, 8, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 55, 56, 57, 58, 69, 77, 78, 79, 81, 82, 88, 90, 91, 92, 93, 95, 97, 98, 99, 101, 113, 114, 129, 130, 136, 143, 146], "rev": 54, "reveal": 76, "review": [15, 73, 144], "revist": [54, 80], "reweight": [99, 100], "rf": 86, "rho": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 60, 63, 67, 76, 77, 86, 88, 89, 130, 131, 136, 140, 146], "rho_val": 89, "richter": [56, 98, 141, 143], "riesz": [10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 66, 88, 130, 131, 132, 133, 134, 135, 136, 139, 140], "riesz_rep": [130, 136], "right": [12, 13, 15, 16, 19, 25, 52, 54, 69, 78, 80, 81, 82, 83, 85, 86, 89, 92, 99, 114, 115, 116, 117, 118, 129, 130, 132, 133, 134, 135, 137, 138], "rightarrow_": [52, 69, 92], "risk": [31, 96, 145], "ritov": 144, "rival": 80, "rival_ind": 80, "rmd": 53, "rmse": [53, 58, 60, 61, 62, 63, 66, 78, 79, 82, 88, 90, 98, 99, 101, 102, 104, 114, 129, 143, 145], "rmse_dml_ml_l_fullsampl": 79, "rmse_dml_ml_l_lesstim": 79, "rmse_dml_ml_l_onfold": 79, "rmse_dml_ml_l_untun": 79, "rmse_dml_ml_m_fullsampl": 79, "rmse_dml_ml_m_lesstim": 79, "rmse_dml_ml_m_onfold": 79, "rmse_dml_ml_m_untun": 79, "rmse_g0": 66, "rmse_g1": 66, "rmse_oos_ml_l": 79, "rmse_oos_ml_m": 79, "rmse_oos_onfolds_ml_l": 79, "rmse_oos_onfolds_ml_m": 79, "rnorm": [51, 56, 93, 95, 98, 129, 143], "robin": [7, 8, 17, 54, 73, 80, 92, 141, 144], "robinson": [52, 69, 92], "robject": 80, "robu": [74, 75], "robust": [9, 10, 16, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 53, 60, 61, 63, 67, 76, 77, 86, 88, 89, 99, 130, 136, 144, 145, 146], "robust_confset": [32, 87, 145], "robust_cov": 87, "robust_length": 87, "roc\u00edo": 144, "role": [4, 5, 6, 52, 69, 79, 92, 146], "romano": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 129], "root": [20, 22, 23, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 66, 73, 92, 98, 114, 144], "rotat": [79, 86], "roth": [86, 99, 101, 105, 144], "rough": [89, 146], "roughli": [60, 63, 89], "round": [55, 61, 66, 67, 77, 78, 83, 89], "rout": [42, 43, 46, 47], "row": [21, 52, 55, 59, 61, 64, 70, 71, 79, 80, 84, 93, 95, 99, 102, 104, 113, 143, 146], "row_index": 75, "rownam": 54, "rowv": 54, "roxygen2": 145, "royal": [89, 144], "rpart": [55, 56, 98], "rpart_cv": 56, "rprocess": 78, "rpy2": 80, "rpy2pi": 80, "rskf": 77, "rsmp": [56, 98, 113], "rsmp_tune": [56, 98], "rssb": 89, "rtype": 30, "ruben": 144, "ruiz": [51, 68], "rule": [53, 97], "run": [53, 61, 66, 86, 99, 142, 145], "runif": 51, "runtime_learn": 56, "runtimewarn": 63, "rv": [60, 63, 67, 76, 77, 88, 89, 130, 136, 146], "rva": [60, 63, 67, 76, 77, 88, 89, 130, 136, 146], "rvert": 73, "rvert_": 73, "s1": 79, "s2": 79, "s_": [16, 54, 80, 99, 112], "s_1": 17, "s_2": 17, "s_col": [4, 5, 57, 86, 90, 99], "s_i": [19, 57, 86, 90, 99], "s_x": [16, 54, 80], "safeguard": [58, 98], "sake": [55, 81, 89, 146], "same": [14, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 44, 52, 54, 57, 60, 61, 63, 69, 70, 71, 76, 77, 78, 80, 82, 84, 86, 87, 88, 89, 90, 98, 99, 102, 104, 114, 117, 118, 129, 130, 138, 145], "samii": 83, "sampl": [9, 10, 16, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 51, 53, 54, 56, 58, 60, 61, 62, 63, 68, 74, 75, 77, 78, 80, 82, 84, 87, 88, 96, 98, 101, 102, 104, 105, 112, 115, 129, 130, 133, 135, 143, 144, 145], "sample_weight": [40, 42, 43, 46, 47, 86], "sant": [9, 10, 14, 20, 22, 23, 25, 26, 53, 58, 60, 61, 62, 63, 99, 100, 101, 103, 105, 144], "sara": 144, "sasaki": [16, 54, 80, 144], "satisfi": [57, 61, 90, 98, 114, 129], "save": [52, 55, 61, 69, 74, 75, 78, 79, 81, 82, 98, 130, 136, 146], "savefig": 69, "saveguard": 78, "saver": [55, 81, 82], "sc": [60, 63], "scalar": 99, "scale": [25, 52, 54, 59, 72, 77, 83, 85, 89, 129, 130, 133, 135, 140], "scale_color_manu": 52, "scale_fill_manu": [52, 54], "scaled_psi": 77, "scatter": [59, 66, 67, 74, 75, 83, 86, 89], "scatterplot": [66, 67], "scenario": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 63, 67, 76, 77, 88, 89, 99, 130, 136, 146], "scene": [70, 71, 73], "scene_camera": 73, "schacht": [73, 78, 79], "schaefer": 83, "schedul": 145, "scheme": [54, 80, 98, 99, 100, 113, 141], "schneider": 56, "schratz": [56, 98, 141, 143], "scienc": [18, 51, 68, 83, 144], "scikit": [61, 78, 81, 98, 141, 143, 145, 146], "scipi": 69, "score": [0, 4, 5, 6, 9, 10, 20, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 85, 86, 87, 88, 89, 90, 91, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 113, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 132, 133, 134, 135, 136, 138, 139, 140, 141, 145, 146], "scoring_method": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "script": 142, "sd": 51, "se": [52, 54, 69, 88, 92, 98, 113, 129, 130, 136, 144, 146], "se_df": 54, "se_dml": [52, 69, 92], "se_dml_po": [52, 69, 92], "se_nonorth": [52, 69], "se_orth_nosplit": [52, 69], "se_orth_po_nosplit": [52, 69], "seaborn": [21, 24, 58, 60, 63, 64, 66, 67, 69, 78, 80, 81, 82, 89, 90], "seamlessli": 66, "search": [20, 22, 23, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 98, 114], "search_mod": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 98], "searchabl": 55, "second": [16, 20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 56, 69, 78, 79, 80, 91, 92, 113, 129, 130, 131, 140, 143], "secondari": [66, 67], "section": [23, 24, 26, 53, 54, 55, 56, 59, 76, 79, 80, 82, 89, 100, 102, 103, 104, 105, 122, 132, 133, 145], "secur": 83, "see": [7, 8, 13, 19, 20, 22, 23, 27, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 46, 47, 51, 53, 54, 55, 56, 58, 60, 61, 62, 63, 66, 67, 68, 70, 71, 75, 77, 79, 80, 82, 83, 84, 86, 87, 88, 89, 98, 99, 101, 103, 105, 113, 114, 120, 122, 123, 124, 127, 128, 130, 131, 133, 135, 136, 140, 142, 143, 145], "seed": [14, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55, 56, 57, 58, 59, 64, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 130, 136, 143, 146], "seek": 83, "seem": [53, 55, 76, 81, 82, 130, 146], "seen": [74, 75, 77], "sel_cols_chiang": 80, "select": [4, 5, 6, 14, 15, 19, 25, 37, 51, 66, 73, 78, 86, 89, 91, 93, 95, 96, 98, 112, 129, 143, 144, 145, 146], "selected_coef": 78, "selected_featur": [56, 98], "selected_learn": 78, "self": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 78, 79, 87, 146], "selfref": 55, "semenova": [70, 71, 144], "semi": 92, "semiparametr": 7, "sens": [88, 89], "sensemakr": [130, 131], "sensit": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 96, 97, 99, 104, 131, 136, 140, 145], "sensitivity_analysi": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 63, 67, 76, 77, 88, 89, 130, 136, 146], "sensitivity_benchmark": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 67, 76, 88, 89, 130, 131], "sensitivity_el": [130, 136], "sensitivity_param": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 88, 89, 130, 131, 136], "sensitivity_plot": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 67, 76, 88, 89, 130, 136], "sensitivity_summari": [60, 63, 67, 76, 77, 88, 89, 130, 136, 146], "sensitv": 77, "sensitvity_benchmark": 67, "sensiv": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39], "senstiv": [130, 139], "sep": 52, "separ": [21, 66, 83, 88, 98, 99, 113, 145], "seper": [79, 86, 88, 129, 130, 131], "seq_len": [52, 57, 92], "sequenti": 8, "ser": [60, 61, 62, 63], "seri": [60, 61, 62, 63, 75, 89, 144], "serv": [25, 93, 94, 95, 143, 145], "serverless": [144, 145], "servic": 83, "set": [4, 5, 6, 7, 8, 9, 11, 16, 17, 18, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 45, 46, 47, 51, 52, 53, 54, 55, 57, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 86, 87, 88, 89, 90, 91, 92, 93, 95, 97, 99, 100, 102, 103, 105, 113, 114, 115, 116, 117, 118, 119, 122, 129, 130, 131, 137, 138, 139, 142, 143, 145, 146], "set_as_param": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "set_config": [42, 43, 46, 47], "set_fit_request": [46, 47], "set_fold_specif": 98, "set_index": 81, "set_ml_nuisance_param": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 55, 64, 81, 98, 145], "set_param": [42, 43, 46, 47, 79, 98], "set_sample_split": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 77, 78, 113, 145], "set_score_request": [42, 43, 46, 47], "set_styl": [81, 82], "set_text": 78, "set_threshold": [52, 53, 54, 55, 56, 57, 91, 98, 99, 113, 114, 129, 143], "set_tick": 80, "set_ticklabel": 80, "set_titl": [66, 67, 77, 79, 80, 86], "set_x_d": [4, 5, 6], "set_xlabel": [66, 67, 69, 77, 79, 80, 86], "set_xlim": 69, "set_xtick": 83, "set_xticklabel": 83, "set_ylabel": [66, 67, 77, 79, 80, 83, 86], "set_ylim": [72, 77, 79, 80, 85], "setdiff": 145, "setdiff1d": 80, "setminu": [54, 80, 129], "settings_l": 79, "settings_m": 79, "setup": [142, 145], "seven": [54, 80], "sever": [48, 55, 56, 60, 61, 62, 63, 66, 78, 79, 81, 82, 88, 89, 92, 98, 146], "shape": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 44, 45, 46, 47, 59, 60, 63, 66, 67, 70, 71, 74, 75, 78, 80, 81, 84, 86, 88, 89, 98, 99], "share": [54, 55, 80, 81], "sharma": [89, 144], "sharp": 40, "shift": [60, 63], "shock": [54, 80], "short": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 88, 89, 99, 103, 130, 131, 144, 145, 146], "shortcut": 55, "shortli": [54, 56, 80, 98], "shota": 144, "should": [4, 5, 6, 20, 21, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 55, 57, 60, 61, 63, 67, 74, 75, 78, 81, 86, 87, 88, 90, 93, 95, 97, 98, 99, 100, 107, 129, 130, 131, 141], "show": [51, 52, 54, 57, 58, 60, 63, 64, 66, 67, 68, 69, 70, 71, 73, 76, 77, 78, 79, 80, 83, 86, 87, 89, 90, 92, 130, 139, 142], "showcas": 84, "showlabel": 89, "showlegend": 89, "shown": [51, 68, 83, 143], "showscal": [70, 71, 73], "shrink": 86, "shuffl": 113, "side": [86, 99, 130, 136], "sigma": [9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 37, 52, 54, 57, 69, 80, 90, 92, 97, 113, 129, 130, 131, 133, 135, 136, 139, 140], "sigma2": [130, 136], "sigma_": [10, 12, 13, 15, 16, 17, 19, 25, 26, 52, 54, 69, 80, 92], "sigma_0": [130, 140], "sigma_j": 129, "sigmoid": 83, "sign": [87, 89], "signal": [44, 45], "signatur": [32, 33, 34, 35, 36, 38, 39, 114], "signif": [51, 53, 54, 55, 56, 57, 98, 99, 113, 114, 129, 143, 146], "signific": [51, 54, 55, 56, 57, 60, 63, 67, 76, 77, 81, 84, 86, 88, 89, 98, 99, 113, 114, 129, 130, 136, 143, 146], "significantli": 66, "silverman": [34, 35, 36], "sim": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 41, 52, 53, 54, 57, 59, 69, 72, 80, 84, 85, 90, 92, 99], "sim_data": 62, "similar": [10, 14, 53, 56, 61, 70, 71, 76, 79, 82, 86, 87, 88, 89, 99, 114, 115, 116], "similarli": [61, 79, 87], "simpl": [11, 20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 46, 47, 53, 56, 70, 71, 74, 75, 76, 77, 84, 89, 96, 99, 130, 131], "simplest": 97, "simpli": [56, 58, 146], "simplic": [55, 78, 81, 84, 89], "simplif": [130, 132], "simplifi": [60, 63, 77, 83, 89, 97, 130, 139], "simul": [9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 25, 26, 52, 56, 57, 60, 62, 63, 69, 70, 71, 72, 73, 74, 75, 78, 79, 85, 86, 89, 90, 92, 98, 129, 143], "simul_data": 37, "simulaten": [99, 107], "simulation_run": 73, "simult": 53, "simultan": [61, 96, 146], "sin": [11, 14, 18, 59, 70, 71, 74, 75], "sinc": [9, 10, 42, 46, 55, 57, 58, 59, 67, 74, 75, 76, 78, 79, 81, 83, 90, 98, 99, 101, 130, 136, 138, 142, 145], "singl": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 58, 60, 61, 62, 63, 74, 75, 82, 83, 98, 129], "single_learner_pipelin": 98, "singleton": 113, "sinh": 18, "sipp": [55, 81, 82], "site": [60, 61, 62, 63, 66, 80, 81, 89], "situat": [54, 80], "six": [25, 54], "sixth": 80, "size": [21, 24, 37, 52, 54, 55, 56, 59, 60, 61, 62, 63, 66, 69, 72, 73, 76, 78, 79, 81, 83, 84, 85, 87, 89, 91, 93, 95, 98, 99, 100, 113, 114, 129, 130, 133, 135, 143, 146], "sizeabl": 89, "skill": 144, "sklearn": [18, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 45, 46, 47, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 86, 87, 88, 89, 90, 91, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 130, 136, 143, 146], "skotara": 89, "slide": 83, "slight": [99, 102, 104], "slightli": [20, 22, 23, 59, 60, 63, 74, 75, 76, 78, 97, 99, 103, 105, 114, 115, 116, 117, 118, 130, 131], "slow": [52, 69, 92], "slower": [52, 69, 92], "small": [11, 57, 58, 59, 66, 77, 84, 90, 99, 130, 131, 138], "smaller": [55, 58, 74, 75, 76, 79, 81, 86, 89, 99, 146], "smallest": [22, 66, 78], "smpl": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 69, 78, 80, 113, 114], "smpls_cluster": [54, 80], "smucler": [87, 145], "sn": [58, 60, 63, 64, 66, 67, 69, 78, 80, 81, 82, 89, 90], "so": [46, 47, 51, 55, 56, 57, 58, 60, 61, 63, 68, 79, 81, 83, 89, 90, 98, 129, 146], "social": [83, 144], "societi": [54, 80, 89, 144], "softwar": [56, 98, 141, 143, 144, 145], "solari": 145, "sole": [61, 89], "solut": [91, 97, 114], "solv": [27, 54, 80, 97, 98, 99, 102, 103, 104, 129], "solver": [81, 90, 99], "some": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 55, 56, 57, 58, 59, 61, 64, 78, 79, 81, 82, 86, 87, 88, 90, 97, 98, 99, 108, 142, 145], "sometim": [78, 99, 105], "sonabend": [56, 98], "sophist": 98, "sort": [21, 66, 81, 87, 99], "sort_bi": 21, "sort_valu": [66, 67], "sourc": [56, 98, 143, 145], "sourcefileload": 73, "sp": 53, "space": [54, 66, 80, 98], "spars": [73, 98, 129, 143, 144], "sparsiti": 144, "spec": 144, "special": [54, 80, 96, 99], "specif": [20, 22, 23, 25, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 40, 54, 55, 60, 61, 62, 63, 66, 67, 77, 78, 80, 81, 89, 93, 95, 96, 97, 98, 99, 102, 104, 113, 114, 122, 129, 136, 140, 141, 143], "specifi": [4, 5, 6, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 54, 55, 56, 57, 58, 60, 61, 62, 63, 66, 67, 68, 70, 71, 72, 74, 75, 77, 79, 80, 81, 82, 84, 85, 86, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 119, 122, 142, 143, 145, 146], "specifii": 82, "speed": [24, 30, 36, 78], "speedup": 78, "spefici": 32, "spindler": [15, 73, 78, 79, 87, 89, 141, 144, 145], "spine": [81, 82], "spline": [70, 71, 97], "spline_basi": [70, 71, 97], "spline_grid": [70, 71], "split": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 54, 56, 57, 58, 60, 61, 62, 63, 77, 78, 80, 82, 84, 88, 90, 96, 97, 98, 99, 101, 102, 104, 114, 129, 143, 145], "split_sampl": [77, 78], "sponsor": [55, 81, 82], "sprintf": 52, "sq_error": 73, "sqrt": [9, 10, 13, 14, 25, 26, 52, 54, 56, 64, 69, 72, 80, 85, 92, 113, 129, 130, 131, 143], "squar": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 55, 66, 73, 81, 98, 99, 130, 140, 144], "squarederror": [55, 81, 146], "squeez": [58, 72, 85, 90], "src": 81, "ssm": [4, 5, 6, 19, 96, 112], "ssrn": 12, "stabil": 76, "stabl": [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 99, 100, 141], "stack": [56, 98], "stackingclassifi": 86, "stackingregressor": 86, "stacklrn": 56, "stackrel": 99, "stage": [40, 61, 70, 71, 74, 75, 84, 86, 98, 99, 145, 146], "stagger": [99, 105], "stai": [99, 105], "standard": [24, 26, 53, 56, 60, 61, 62, 63, 72, 74, 75, 86, 87, 99, 100, 102, 103, 104, 113, 114, 129, 130, 136, 140, 145, 146], "standard_norm": [93, 95, 98, 129, 143], "standardscal": 81, "star": 99, "start": [25, 53, 55, 56, 60, 61, 62, 63, 66, 70, 71, 73, 76, 78, 79, 80, 81, 85, 89, 99, 101, 141, 146], "start_dat": 25, "stat": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 69, 86, 93, 95, 98, 99, 129, 141, 144], "stat_bin": 52, "stat_dens": 55, "state": 146, "stationar": 58, "stationari": [99, 101], "statist": [16, 19, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 48, 54, 80, 87, 88, 89, 129, 130, 136, 141, 143, 144, 145, 146], "statsmodel": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 86], "statu": [53, 55, 57, 58, 81, 83, 86, 90, 99, 105], "std": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 68, 70, 71, 72, 74, 75, 76, 77, 79, 80, 81, 82, 83, 85, 88, 89, 90, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 143, 146], "stefan": 144, "step": [52, 55, 56, 69, 74, 75, 76, 81, 84, 92, 98, 99, 129, 141, 146], "stepdown": 129, "stick": [55, 81], "still": [57, 58, 70, 71, 74, 75, 76, 82, 86, 88, 90, 98, 99, 103], "stochast": [38, 39, 99, 110, 111, 143], "stock": [55, 81, 82, 87], "store": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 66, 87, 91, 98, 113, 114, 129, 130, 136, 145], "store_model": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 79], "store_predict": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 53, 81, 84], "stori": [89, 144], "str": [4, 5, 6, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 55, 60, 63, 66, 67, 74, 75, 85, 86, 97, 99, 145], "straightforward": [60, 63, 74, 75, 78, 97], "strategi": [83, 89, 99, 146], "stratifi": [77, 78], "stratum": 83, "strength": [9, 10, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 87, 88, 89, 130, 131, 136, 139], "strftime": 60, "strictli": 99, "string": [20, 21, 22, 23, 24, 29, 31, 32, 33, 34, 35, 37, 38, 39, 97, 129, 130, 136, 143, 145], "string_label": 83, "strong": [57, 87, 90, 130, 131], "stronger": [87, 99, 103, 129, 146], "structur": [7, 8, 17, 24, 25, 54, 55, 57, 61, 73, 80, 81, 87, 90, 92, 98, 141, 144, 146], "student": 144, "studi": [19, 54, 55, 73, 78, 79, 80, 81, 82, 87, 88, 99, 100, 143, 146], "style": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 60, 63, 79, 145], "styler": 145, "styliz": 89, "sub": [42, 43, 46, 47, 54, 80], "subclass": [94, 95, 145], "subfold": 98, "subgroup": [32, 55, 81, 145], "subject": [54, 80], "submiss": 145, "submit": [60, 63], "submodul": 145, "subobject": [42, 43, 46, 47], "subplot": [54, 59, 66, 67, 69, 70, 71, 72, 74, 75, 77, 78, 79, 80, 81, 82, 83, 85, 86], "subplots_adjust": 78, "subpopul": [99, 112], "subsampl": [56, 78, 114, 116], "subscript": [99, 103, 114, 116, 118, 130, 131], "subsequ": [54, 66, 80], "subset": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 42, 46, 54, 78, 80, 84, 91, 97, 98, 130, 131, 133, 135], "subseteq": 97, "substanti": [55, 81, 83], "substract": 129, "subtract": 129, "sudo": 142, "suffic": 89, "suffici": [78, 79, 89], "suggest": [54, 55, 80, 81, 89, 145], "suitabl": [57, 70, 71, 90, 99, 103], "sum": [43, 47, 54, 55, 80, 81, 82, 85, 86, 97, 129], "sum_": [25, 41, 52, 54, 69, 80, 86, 91, 92, 97, 99, 100, 105, 129], "sum_i": 83, "sum_oth": 80, "sum_riv": 80, "summar": [21, 53, 60, 61, 62, 63, 66, 67, 83, 89, 91, 130, 136], "summari": [20, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 53, 54, 56, 57, 58, 60, 61, 62, 63, 64, 67, 68, 70, 71, 72, 74, 75, 76, 77, 80, 82, 85, 87, 88, 89, 90, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 104, 113, 114, 129, 130, 143, 145, 146], "summary_df": 87, "summary_result": 55, "summary_stat": 66, "superior": 66, "suppli": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 70, 71, 74, 75, 76, 84, 97, 130, 131, 136, 137], "support": [11, 32, 40, 53, 54, 60, 61, 62, 63, 78, 80, 84, 86, 95, 98, 99, 108, 146], "support_s": [11, 70, 71, 74, 75, 84], "support_t": 84, "support_w": 84, "suppos": 89, "suppress": [53, 55, 56, 57], "suppresswarn": 52, "suprema": 129, "suptitl": [72, 78, 79, 82, 85], "supxlabel": [72, 82, 85], "supylabel": [72, 82, 85], "sure": [66, 67, 98, 145], "surfac": [70, 71, 73], "surgic": 87, "surpress": [54, 143], "survei": [55, 81, 82, 146], "susan": 144, "sven": [89, 141, 144], "svenk": 80, "svenklaassen": [141, 145], "svg": [52, 69], "switch": [52, 69, 89, 92], "symbol": 89, "symmetr": 18, "syntax": [86, 99], "synthesi": 144, "synthet": [11, 25, 41, 51, 66, 68, 70, 71, 72, 74, 75, 79, 84, 85, 87], "syrgkani": [87, 89, 144], "system": 144, "szita": 144, "t": [4, 5, 6, 9, 10, 14, 20, 21, 23, 24, 25, 26, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 70, 71, 72, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 113, 114, 115, 116, 129, 130, 132, 133, 143, 146], "t_": [24, 60, 61, 62, 63, 99, 102, 103, 104, 114, 116, 118], "t_1_start": 78, "t_1_stop": 78, "t_2_start": 78, "t_2_stop": 78, "t_3_start": 78, "t_3_stop": 78, "t_col": [4, 5, 6, 23, 24, 60, 61, 62, 63, 94, 95, 99, 100, 101, 102, 104], "t_df": 84, "t_diff": 59, "t_dml": 52, "t_g": 25, "t_i": [58, 84, 86, 99, 101, 102, 105, 114, 116], "t_idx": 59, "t_nonorth": 52, "t_orth_nosplit": 52, "t_sigmoid": 84, "t_stat": 129, "t_value_ev": 22, "t_value_pr": 22, "tabl": [52, 54, 55, 56, 57, 66, 67, 87, 91, 93, 95, 98, 99, 113, 114, 129, 143, 146], "tabpfnclassifi": 66, "tabpfnregressor": 66, "tabular": [66, 78, 93, 95, 129, 143, 146], "taddi": 144, "takatsu": 87, "take": [9, 10, 11, 32, 33, 38, 39, 57, 58, 59, 60, 61, 62, 63, 70, 71, 72, 73, 74, 75, 78, 82, 85, 86, 87, 88, 90, 91, 97, 98, 99, 100, 105, 108, 109, 110, 111, 114, 119, 122, 130, 137, 138, 139, 143], "taken": [55, 81, 82, 146], "taker": [32, 145], "talk": 146, "target": [20, 22, 23, 27, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 46, 47, 51, 54, 55, 56, 57, 70, 71, 78, 80, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 112, 113, 114, 123, 124, 129, 130, 138, 140, 141, 143, 145, 146], "task": [51, 66, 79, 93, 95, 113, 146], "task_typ": 145, "tau": [41, 59, 72, 82, 83, 85, 86, 97, 99, 114, 120, 123, 124], "tau_": [83, 86, 99], "tau_0": [86, 99], "tau_1": 83, "tau_2": 83, "tau_vec": [72, 82, 85], "tax": [55, 81, 82], "te": [53, 70, 71, 84], "techniqu": [52, 69, 92, 113, 146], "teen": 61, "templat": 145, "ten": 79, "tend": [55, 81, 82, 99], "tensor": [70, 71], "tenth": 144, "term": [6, 22, 52, 54, 55, 56, 59, 69, 73, 80, 81, 83, 89, 92, 99, 105, 141, 146], "termin": [56, 98], "terminatorev": 56, "test": [8, 12, 20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 60, 61, 62, 63, 69, 76, 80, 87, 89, 92, 98, 99, 113, 114, 129, 143, 144, 145, 146], "test_id": [54, 113], "test_ind": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "test_set": 113, "test_siz": 69, "text": [9, 10, 12, 14, 24, 25, 26, 40, 41, 54, 55, 60, 61, 62, 63, 66, 72, 73, 83, 84, 85, 86, 89, 97, 99, 102, 103, 104, 105, 113, 114, 116, 118, 130, 133, 135], "textbf": [91, 98, 146], "textposit": 89, "textrm": [130, 131, 137, 138, 139, 140], "tg": [56, 64, 93, 95, 143], "th": [54, 80], "than": [33, 52, 53, 55, 69, 73, 77, 78, 81, 82, 83, 86, 87, 88, 89, 92, 99, 103, 130, 136, 146], "thank": [53, 55, 56, 81, 145], "thatw": 59, "thei": [53, 55, 59, 74, 75, 81, 83, 87, 99, 130, 140], "them": [21, 55, 56, 70, 71, 72, 76, 79, 81, 85, 99], "theme": [54, 55], "theme_minim": [52, 55, 57], "theorem": [99, 105, 130, 140], "theoret": [78, 89, 113, 144], "theori": [97, 144], "therebi": [54, 56, 80, 146], "therefor": [60, 62, 63, 67, 83, 86, 88, 113, 114, 130, 139], "theta": [9, 10, 12, 13, 14, 16, 18, 19, 20, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 54, 56, 57, 58, 59, 60, 63, 66, 67, 69, 73, 76, 77, 78, 80, 86, 88, 89, 90, 91, 92, 93, 95, 97, 98, 99, 100, 102, 103, 104, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 136, 139, 140, 143, 146], "theta_": [25, 60, 63, 67, 86, 89, 97, 99, 106, 107, 129, 130, 140], "theta_0": [11, 32, 33, 38, 39, 52, 54, 55, 57, 67, 69, 70, 71, 73, 74, 75, 80, 81, 89, 90, 92, 97, 99, 101, 108, 109, 110, 111, 112, 114, 123, 124, 126, 129, 130, 137, 138, 140, 143], "theta_d": 66, "theta_dml": [52, 69, 92], "theta_dml_po": [52, 69, 92], "theta_initi": 69, "theta_nonorth": [52, 69], "theta_orth_nosplit": [52, 69], "theta_orth_po_nosplit": [52, 69], "theta_resc": 52, "theta_t": 59, "thi": [9, 10, 20, 21, 22, 23, 24, 25, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 97, 98, 99, 103, 105, 106, 107, 108, 109, 113, 114, 115, 116, 117, 118, 120, 122, 123, 124, 129, 130, 131, 136, 137, 138, 141, 142, 143, 144, 145, 146], "think": 56, "third": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 52, 61, 62, 69, 80, 92, 113], "thirion": [141, 143], "this_df": [73, 81], "this_split_ind": 80, "those": [53, 55, 61, 81, 82, 87], "though": [51, 68, 83], "thread": [83, 98], "three": [54, 56, 74, 75, 142, 145], "threshold": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 86, 89, 99], "through": [53, 61, 72, 74, 75, 85, 86, 98, 99], "throughout": [76, 87], "thu": [79, 86, 97, 99], "tibbl": 53, "tick": 24, "tick_param": 86, "tight": 69, "tight_layout": [60, 63, 79, 80, 86], "tighter": 86, "tild": [9, 10, 14, 25, 26, 54, 80, 83, 91, 97, 113, 114, 116, 118, 123, 124, 127, 128, 129, 130, 139, 140], "tile": 87, "time": [4, 5, 6, 15, 16, 20, 22, 24, 25, 52, 53, 54, 55, 57, 58, 59, 61, 69, 73, 74, 75, 80, 81, 82, 86, 88, 89, 90, 94, 95, 99, 100, 101, 102, 103, 104, 105, 114, 130, 144, 145, 146], "time_budget": 79, "time_df": 59, "time_period": 59, "time_typ": [25, 60, 63], "titiunik": [99, 144], "titl": [21, 24, 54, 55, 57, 60, 61, 63, 66, 67, 70, 71, 72, 73, 74, 75, 78, 79, 80, 81, 82, 83, 85, 86, 89, 141], "title_fonts": [60, 63], "tmp": 75, "tname": 53, "tnr": [56, 98], "to_datetim": 60, "to_fram": 84, "to_numpi": [72, 76, 82, 85], "to_str": 66, "todo": [54, 64], "toeplitz": 73, "togeth": [74, 75, 129], "toler": 80, "tomasz": [144, 145], "toml": 145, "too": 78, "tool": [53, 56, 88, 146], "top": [54, 78, 80, 81, 82, 86, 89, 99, 141], "total": [43, 47, 55, 79, 81, 99, 100], "total_width": 66, "tpot": 79, "tracker": 141, "tradit": [66, 129], "train": [20, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 46, 47, 52, 54, 56, 66, 69, 70, 71, 72, 74, 75, 77, 78, 80, 81, 84, 85, 91, 92, 113], "train_id": [54, 113], "train_ind": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "train_set": 113, "train_test_split": 69, "transact": 144, "transform": [9, 10, 41, 66, 77, 83, 89, 146], "translat": [66, 73], "transpos": 59, "treament": 84, "treat": [24, 25, 26, 33, 53, 58, 59, 60, 61, 62, 63, 67, 76, 84, 86, 89, 97, 99, 101, 102, 104, 105, 109, 114, 116, 118, 129, 146], "treat1_param": 83, "treat2_param": 83, "treat_var": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 98], "treatment": [4, 5, 6, 7, 8, 9, 10, 11, 14, 17, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 51, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 67, 68, 73, 76, 78, 79, 80, 84, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 112, 113, 115, 116, 117, 118, 119, 120, 122, 123, 124, 129, 133, 135, 136, 137, 139, 141, 143, 144, 145, 146], "treatment_df": 59, "treatment_effect": [11, 70, 71], "treatment_level": [29, 30, 66, 67, 77, 99], "treatment_var": [4, 5, 6], "tree": [33, 45, 55, 56, 58, 59, 60, 63, 78, 81, 91, 96, 98, 99, 113, 114, 129, 143, 145], "tree_param": [33, 45], "tree_summari": 81, "trees_class": [55, 81], "trend": [53, 58, 59, 60, 63, 80, 99, 101, 103, 105, 144], "tri": [73, 130, 131], "triangular": [40, 86, 99], "trim": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 55, 81, 82, 89], "trimming_rul": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 82], "trimming_threshold": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 55, 70, 77, 81, 82, 84, 85, 89], "trm": [56, 98], "true": [4, 5, 6, 7, 8, 9, 14, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 95, 98, 99, 101, 113, 114, 119, 120, 123, 124, 127, 128, 129, 130, 132, 133, 134, 135, 140, 143, 146], "true_effect": [59, 70, 71, 74, 75, 87], "true_gatet_effect": 76, "true_group_effect": 76, "true_tau": 86, "truemfunct": 87, "truncat": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 82], "try": [78, 88], "tune": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 66, 73, 78, 86, 96, 99, 141, 143, 145], "tune_on_fold": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 98], "tune_r": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39], "tune_set": [56, 98], "tuned_model": 79, "tuner": 98, "tunergridsearch": 56, "tupl": [20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 60, 63, 99, 102, 103, 104], "turn": 89, "turrel": 18, "tutori": 55, "tw": [81, 82], "twice": 99, "twinx": [66, 67], "two": [11, 20, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 51, 52, 55, 56, 58, 60, 61, 62, 63, 68, 69, 72, 77, 78, 79, 81, 82, 83, 84, 85, 87, 88, 89, 91, 92, 97, 98, 101, 104, 113, 123, 129, 146], "twoclass": 56, "twoearn": [55, 81, 82, 88, 146], "type": [6, 9, 10, 11, 14, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 55, 56, 60, 63, 69, 78, 79, 80, 86, 89, 92, 96, 98, 99, 114, 125, 126, 129, 130, 139, 145, 146], "typeerror": [60, 61, 62, 63], "typic": [66, 75, 99, 105, 141], "u": [9, 10, 11, 13, 19, 26, 32, 33, 34, 35, 36, 43, 47, 52, 53, 54, 55, 58, 59, 60, 61, 63, 66, 67, 69, 72, 74, 75, 77, 78, 80, 81, 82, 84, 85, 87, 88, 89, 92, 99, 106, 108, 109, 130, 131, 142, 146], "u_hat": [52, 69, 114], "u_i": [12, 15, 18, 19], "u_t": 26, "uehara": 144, "uhash": 56, "ulf": 144, "unambigu": 89, "uncertainti": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 74, 75, 77, 86, 88, 130, 136, 146], "unchang": [42, 43, 46, 47], "uncondit": [55, 60, 63, 81, 146], "unconfounded": [89, 144], "under": [32, 37, 52, 55, 58, 61, 69, 81, 84, 86, 89, 92, 99, 103, 105, 112, 129, 144], "underbrac": [52, 59, 69, 92, 97], "underfit": 79, "underli": [9, 14, 55, 56, 60, 63, 66, 67, 74, 75, 83, 84, 99, 105, 130, 131, 146], "underlin": [54, 80], "underset": [86, 99], "understand": [60, 61, 63, 66, 89], "undesir": 98, "unevenli": 113, "uniform": [26, 40, 41, 59, 68, 70, 71, 72, 84, 85, 129], "uniform_averag": [43, 47], "uniformli": [32, 60, 63, 72, 82, 129], "union": 32, "uniqu": [6, 51, 60, 61, 62, 63, 66, 67, 68, 78, 86, 94, 95, 99, 102, 104, 114, 130, 140], "unique_label": 79, "unit": [6, 25, 52, 53, 57, 58, 59, 60, 61, 62, 63, 76, 86, 90, 94, 95, 99, 101, 102, 103, 104, 105, 114, 115, 116, 117, 118, 130, 133, 135, 145], "univari": [11, 70, 71], "univers": [24, 144], "unknown": 99, "unlik": [55, 81, 82, 89], "unobserv": [9, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 51, 55, 60, 63, 68, 81, 82, 88, 89, 99, 130, 131, 140, 146], "unpen": 53, "unstabl": [130, 131], "unter": [54, 55, 56], "untest": 89, "until": [99, 101, 145], "untreat": [89, 99, 101], "untun": 66, "up": [24, 30, 36, 55, 73, 78, 79, 81, 82, 88, 89, 98, 99, 101, 113, 130, 131, 142, 145, 146], "upcom": 145, "updat": [42, 43, 46, 47, 54, 75, 80, 81, 144, 145], "update_layout": [70, 71, 73, 86, 89], "update_trac": [70, 71], "upload": 145, "upon": [114, 145], "upper": [32, 55, 56, 59, 60, 63, 66, 67, 69, 72, 76, 77, 82, 85, 86, 88, 89, 98, 130, 136, 140, 146], "upper_bound": [70, 71], "upsilon": [57, 90], "upsilon_i": [57, 90], "upward": [55, 81, 82, 89], "upweight": 83, "url": [61, 73, 141, 144], "us": [4, 5, 6, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 52, 54, 55, 57, 58, 59, 60, 61, 62, 63, 66, 67, 69, 70, 71, 72, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 95, 97, 99, 100, 102, 104, 113, 114, 115, 116, 117, 118, 129, 130, 131, 136, 138, 139, 140, 141, 142, 143, 145, 146], "usa": 144, "usabl": 78, "usag": [53, 58, 60, 61, 62, 63, 64, 66, 67, 76, 80, 81, 82, 88, 90, 93, 94, 95, 143, 145], "use_label_encod": [81, 146], "use_other_treat_as_covari": [4, 5, 6, 93, 95], "use_pred_offset": 98, "use_weight": 98, "usecolormap": [70, 71], "user": [27, 28, 42, 43, 46, 47, 52, 53, 54, 55, 56, 61, 66, 67, 69, 76, 77, 78, 80, 81, 86, 88, 97, 98, 99, 114, 129, 141, 142, 143, 145, 146], "user_guid": 75, "userwarn": [60, 62, 63, 66, 81, 89], "usual": [54, 58, 60, 61, 62, 63, 66, 70, 71, 78, 80, 86, 88, 89, 97, 98, 113, 130, 140], "util": [0, 28, 66, 77, 78, 79, 83, 86, 98, 99, 145], "v": [7, 8, 13, 15, 16, 17, 19, 32, 33, 38, 39, 43, 47, 52, 54, 55, 60, 63, 66, 67, 69, 76, 77, 78, 79, 80, 81, 83, 86, 87, 91, 92, 97, 99, 106, 108, 109, 110, 111, 129, 141, 143, 144, 145, 146], "v108": 141, "v12": [141, 143], "v22": 56, "v23": 141, "v_": [16, 54, 80, 99], "v_i": [12, 13, 17, 18, 19, 52, 69, 92, 99], "v_j": 129, "val": [13, 60, 61, 62, 63, 113, 144], "val_list": 73, "valid": [4, 5, 6, 12, 20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 52, 53, 54, 55, 58, 61, 66, 69, 72, 78, 79, 80, 81, 82, 85, 92, 96, 97, 98, 113, 114, 120, 123, 124, 130, 131, 144, 146], "valu": [4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 46, 47, 48, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 66, 67, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 86, 87, 88, 89, 91, 94, 95, 96, 98, 99, 100, 105, 106, 112, 113, 120, 123, 124, 127, 128, 129, 130, 131, 136, 140, 143, 145, 146], "value_count": 81, "van": 144, "vanderpla": [141, 143], "vanish": [52, 69, 92], "var": [9, 10, 14, 25, 26, 54, 80, 83, 86, 130, 131, 137, 138, 139, 140], "var_ep": 89, "varepsilon": [9, 10, 16, 32, 54, 57, 80, 90, 97, 99, 108], "varepsilon_": [16, 25, 54, 60, 63, 80], "varepsilon_0": 26, "varepsilon_1": 26, "varepsilon_d": [10, 14], "varepsilon_i": [14, 15, 57, 72, 85, 90], "vari": [25, 55, 59, 60, 63, 78, 81, 83, 89], "variabl": [4, 5, 6, 7, 10, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 73, 76, 79, 80, 81, 82, 86, 88, 89, 90, 93, 94, 95, 97, 98, 99, 101, 102, 104, 105, 106, 108, 109, 110, 111, 113, 114, 129, 130, 131, 136, 140, 143, 144, 145, 146], "varianc": [27, 28, 54, 56, 80, 86, 88, 89, 96, 99, 113, 130, 131, 136, 138, 139, 140, 143, 145], "variant": [53, 61, 77], "variat": [10, 20, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 77, 88, 130, 131, 140], "variou": [53, 79, 89, 98, 146], "varoquaux": [141, 143], "vasili": [89, 144], "vast": 66, "vector": [11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 25, 29, 31, 32, 33, 34, 35, 37, 38, 39, 51, 54, 55, 57, 58, 68, 74, 75, 76, 80, 81, 84, 87, 90, 99, 101, 110, 111, 112, 129, 143, 145], "vee": [114, 116, 118], "venv": 142, "verbos": [55, 59, 60, 63, 66, 69, 78, 79, 86, 89], "veri": [53, 54, 56, 61, 76, 78, 80, 87, 89, 114, 141], "verifi": 83, "versa": [78, 83, 130, 136], "version": [9, 42, 43, 46, 47, 54, 55, 56, 58, 59, 89, 91, 97, 99, 102, 103, 104, 114, 129, 130, 132, 133, 134, 135, 137, 138, 141, 145], "versoin": 89, "versu": 75, "vertic": [54, 66, 67, 80], "via": [9, 10, 20, 22, 23, 26, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 53, 57, 58, 59, 60, 61, 62, 63, 72, 73, 74, 75, 76, 77, 78, 86, 88, 90, 91, 93, 95, 96, 97, 98, 99, 100, 101, 102, 104, 113, 120, 128, 129, 130, 131, 136, 140, 141, 142, 143, 144, 145, 146], "viabl": [20, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], "vice": [78, 83, 130, 136], "victor": [73, 89, 113, 141, 144], "view": 75, "vignett": [53, 145], "villa": [51, 68], "violat": [60, 63], "violet": [72, 82, 85], "vira": 144, "virtual": [61, 142], "virtualenv": 142, "visibl": [82, 86, 89], "visit": [141, 146], "visual": [21, 54, 60, 61, 62, 63, 76, 77, 79, 80, 86], "vmax": 63, "vmin": 63, "vol": 53, "volum": [89, 141], "voluntari": 83, "vv740": 80, "vv760g": 80, "w": [7, 8, 9, 10, 17, 25, 26, 27, 28, 42, 43, 46, 47, 54, 73, 80, 83, 84, 87, 91, 92, 99, 102, 103, 104, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 143], "w24678": 113, "w30302": 144, "w_": [25, 26, 54, 80, 84, 99], "w_1": [25, 26, 84], "w_2": [25, 26, 84], "w_3": [25, 26], "w_4": [25, 26], "w_df": 84, "w_i": [19, 58, 84, 86, 91, 97, 99, 113, 114, 116, 118, 129], "wa": [54, 59, 79, 80, 89, 145], "wage": 61, "wager": 144, "wai": [55, 78, 79, 81, 87, 89, 98, 114, 142], "wander": 18, "wang": 144, "want": [51, 54, 55, 56, 58, 68, 72, 78, 80, 85, 86, 98, 99, 141, 142, 144], "warn": [21, 24, 51, 52, 53, 54, 55, 56, 57, 60, 62, 63, 66, 69, 81, 89, 91, 98, 99, 113, 114, 129, 143, 145], "wayon": 54, "we": [33, 45, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 97, 98, 99, 100, 102, 103, 104, 105, 109, 113, 114, 116, 118, 121, 129, 130, 131, 140, 142, 143, 145, 146], "weak": [32, 130, 131, 144, 145], "weakest": [60, 63], "wealth": [7, 88], "websit": [55, 56, 61, 98, 141], "wedg": [54, 80], "week": 145, "wei": 129, "weight": [21, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 42, 43, 46, 47, 54, 55, 56, 57, 60, 61, 62, 63, 66, 67, 76, 77, 80, 81, 86, 90, 96, 98, 99, 100, 114, 119, 122, 129, 130, 137, 138, 145], "weights_bar": [29, 33, 77], "weights_dict": 77, "weiss": [141, 143], "well": [4, 5, 6, 46, 47, 52, 54, 69, 73, 78, 79, 80, 87, 91, 92, 95, 113, 142, 143], "were": [55, 57, 81, 82, 90, 146], "what": [53, 73, 78, 144], "when": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 42, 43, 46, 47, 55, 58, 61, 66, 75, 77, 81, 83, 87, 99, 109, 112, 114, 129, 141, 142, 143, 145], "whenev": [55, 81], "whera": [130, 138], "where": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 43, 44, 45, 47, 51, 52, 54, 55, 57, 58, 59, 60, 61, 62, 63, 67, 68, 69, 72, 76, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 97, 98, 99, 100, 101, 103, 104, 105, 106, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 136, 137, 138, 140, 142, 143, 145, 146], "wherea": [11, 57, 58, 60, 63, 67, 87, 89, 90, 99, 102, 104, 114, 122, 130, 137, 146], "whether": [4, 5, 6, 11, 14, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 44, 55, 59, 78, 81, 82, 86, 87, 89, 93, 95, 98, 99, 130, 131, 145], "which": [4, 5, 6, 9, 11, 20, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 46, 51, 52, 53, 55, 56, 57, 58, 60, 61, 62, 63, 65, 66, 67, 68, 69, 73, 75, 76, 78, 79, 81, 82, 84, 86, 88, 89, 90, 92, 93, 95, 97, 98, 99, 103, 114, 129, 130, 131, 136, 137, 138, 140, 142, 145, 146], "while": [51, 68, 99], "white": [54, 74, 75, 80, 89], "whitegrid": [81, 82], "whitnei": [89, 144], "who": [53, 55, 81, 89], "whole": [52, 58, 69, 86, 92, 98, 114, 115, 130, 131], "whom": 99, "why": [61, 66], "widehat": [60, 61, 62, 63, 99], "width": [21, 24, 52, 54, 66, 70, 71, 73], "wiki": 145, "wiksel": 144, "wild": [20, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 129], "window": 142, "wise": [74, 75], "wish": 142, "within": [40, 54, 60, 61, 62, 63, 66, 74, 75, 80, 84, 86], "without": [14, 20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 40, 51, 52, 60, 61, 62, 63, 66, 68, 69, 78, 79, 89, 92, 96, 98, 99, 130, 131, 142, 145], "wolf": [20, 22, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 129], "won": 89, "word": [40, 86, 99, 145, 146], "work": [42, 43, 46, 47, 61, 62, 65, 66, 67, 75, 76, 78, 83, 88, 98, 99, 129, 142, 144], "workflow": [141, 145], "workspac": 81, "world": 144, "worri": 89, "wors": [43, 47], "would": [43, 47, 53, 55, 56, 60, 61, 62, 63, 70, 71, 73, 78, 81, 82, 86, 88, 89, 97, 98, 130, 140, 146], "wrapper": [53, 86, 98], "wright": 87, "write": [52, 53, 57, 58, 69, 75, 90, 92, 130, 140], "written": [99, 114, 130, 137, 138], "wrong": [78, 83], "wspace": 78, "wurd": [54, 55, 56], "www": [141, 142], "x": [4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 29, 31, 32, 33, 35, 37, 38, 39, 40, 41, 42, 43, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 95, 97, 98, 99, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 132, 133, 134, 135, 137, 138, 139, 140, 143, 146], "x0": [66, 67, 83, 86], "x1": [54, 56, 57, 58, 66, 67, 77, 79, 80, 83, 86, 88, 89, 90, 93, 95, 97, 98, 99, 114, 129, 130, 131, 143], "x10": [54, 56, 57, 77, 79, 80, 90, 93, 95, 98, 99, 114, 129, 143], "x100": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x11": [54, 56, 57, 80, 90, 93, 95, 98, 99, 114, 129, 143], "x12": [54, 56, 57, 80, 90, 93, 95, 98, 99, 114, 129, 143], "x13": [54, 56, 57, 80, 90, 93, 95, 98, 99, 114, 129, 143], "x14": [54, 56, 57, 80, 90, 93, 95, 98, 99, 114, 129, 143], "x15": [54, 56, 57, 80, 90, 93, 95, 98, 99, 114, 129, 143], "x16": [54, 56, 57, 80, 90, 93, 95, 98, 99, 114, 129, 143], "x17": [54, 56, 57, 80, 90, 93, 95, 98, 99, 114, 129, 143], "x18": [54, 56, 57, 80, 90, 93, 95, 98, 99, 114, 129, 143], "x19": [54, 56, 57, 80, 90, 93, 95, 98, 99, 114, 129, 143], "x1x2x3x4x5x6x7x8x9x10": 54, "x2": [54, 56, 57, 58, 66, 67, 77, 79, 80, 86, 88, 89, 90, 93, 95, 97, 98, 99, 114, 129, 143], "x20": [54, 56, 57, 80, 90, 93, 95, 98, 99, 114, 129, 143], "x21": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x22": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x23": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x24": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x25": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x26": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x27": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x28": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x29": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x2_dummi": 89, "x2_preds_control": 89, "x2_preds_treat": 89, "x3": [54, 56, 57, 58, 66, 67, 77, 79, 80, 88, 89, 90, 93, 95, 97, 98, 99, 114, 129, 143], "x30": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x31": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x32": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x33": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x34": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x35": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x36": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x37": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x38": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x39": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x4": [54, 56, 57, 58, 66, 67, 77, 79, 80, 88, 89, 90, 93, 95, 98, 99, 114, 129, 143], "x40": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x41": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x42": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x43": [54, 56, 57, 79, 80, 90, 93, 95, 99, 143], "x44": [54, 56, 57, 79, 80, 90, 93, 95, 99, 143], "x45": [54, 56, 57, 79, 80, 90, 93, 95, 99, 143], "x46": [54, 56, 57, 79, 80, 90, 93, 95, 99, 143], "x47": [54, 56, 57, 79, 80, 90, 93, 95, 99, 143], "x48": [54, 56, 57, 79, 80, 90, 93, 95, 99, 143], "x49": [54, 56, 57, 79, 80, 90, 93, 95, 99, 143], "x5": [54, 56, 57, 77, 79, 80, 89, 90, 93, 95, 98, 99, 114, 129, 143], "x50": [54, 56, 57, 79, 80, 90, 93, 95, 99, 143], "x51": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x52": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x53": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x54": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x55": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x56": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x57": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x58": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x59": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x6": [54, 56, 57, 77, 79, 80, 90, 93, 95, 98, 99, 114, 129, 143], "x60": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x61": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x62": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x63": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x64": [54, 56, 57, 60, 61, 62, 63, 80, 81, 89, 90, 93, 95, 99, 143], "x65": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x66": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x67": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x68": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x69": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x7": [54, 56, 57, 77, 79, 80, 90, 93, 95, 98, 99, 114, 129, 143], "x70": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x71": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x72": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x73": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x74": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x75": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x76": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x77": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x78": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x79": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x8": [54, 56, 57, 77, 79, 80, 90, 93, 95, 98, 99, 114, 129, 143], "x80": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x81": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x82": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x83": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x84": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x85": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x86": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x87": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x88": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x89": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x9": [54, 56, 57, 77, 79, 80, 90, 93, 95, 98, 99, 114, 129, 143], "x90": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x91": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x92": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x93": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x94": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x95": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x96": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x96x97x98x99x100ydcluster_var_icluster_var_jz": 54, "x97": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x98": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x99": [54, 56, 57, 80, 90, 93, 95, 99, 143], "x_": [16, 17, 25, 52, 54, 59, 69, 80, 89, 92], "x_0": [59, 70, 71, 74, 75, 76], "x_1": [9, 10, 14, 25, 26, 38, 39, 59, 70, 71, 72, 74, 75, 76, 85, 89, 99, 110, 111, 130, 131, 143], "x_1x_3": [72, 85], "x_2": [9, 10, 14, 25, 26, 59, 70, 71, 72, 74, 75, 76, 85, 89, 130, 131], "x_3": [9, 10, 14, 25, 26, 59, 70, 71, 74, 75, 76, 130, 131], "x_4": [9, 10, 14, 25, 26, 70, 71, 72, 74, 75, 76, 85], "x_5": [9, 10, 14, 70, 71, 74, 75], "x_6": [70, 71, 74, 75], "x_7": [70, 71, 74, 75], "x_8": [70, 71, 74, 75], "x_9": [70, 71, 74, 75], "x_binary_control": 89, "x_binary_tr": 89, "x_center": 66, "x_col": [4, 5, 6, 24, 51, 54, 55, 56, 60, 61, 62, 63, 68, 73, 80, 81, 82, 84, 86, 87, 88, 89, 93, 94, 95, 98, 99, 100, 102, 104, 143, 145, 146], "x_cols_bench": 89, "x_cols_binari": 89, "x_cols_poli": 80, "x_conf": 85, "x_conf_tru": 85, "x_df": 59, "x_domain": 56, "x_end": 66, "x_i": [11, 12, 13, 15, 17, 18, 19, 41, 52, 57, 58, 69, 72, 74, 75, 83, 85, 86, 90, 92, 97, 99, 101, 102, 104, 105, 112, 114, 116, 118], "x_jitter": 66, "x_p": [38, 39, 99, 110, 111, 143], "x_rang": 66, "x_start": 66, "x_train": 79, "x_true": [72, 85], "x_var": 56, "xaxis_titl": [70, 71, 73, 86, 89], "xformla": 53, "xgb": 79, "xgb_untuned_l": 79, "xgb_untuned_m": 79, "xgbclassifi": [78, 81, 83, 146], "xgboost": [52, 55, 78, 81, 83, 146], "xgbregressor": [78, 79, 81, 83, 146], "xi": [14, 25, 26, 99], "xi_": 129, "xi_0": [16, 54, 80], "xi_i": [57, 90], "xiaoji": 144, "xintercept": [52, 57], "xlab": [52, 54, 55], "xlabel": [59, 60, 61, 63, 66, 67, 70, 71, 72, 74, 75, 79, 81, 82, 85], "xlim": [52, 55, 66], "xmax": 66, "xmax_rel": 66, "xmin": 66, "xmin_rel": 66, "xtick": [66, 67, 79], "xval": [56, 98], "xx": 69, "y": [4, 5, 6, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 104, 106, 108, 109, 110, 111, 113, 114, 115, 116, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 136, 137, 138, 139, 140, 143, 146], "y0": [53, 60, 63, 66, 67, 72, 85], "y0_cvar": 72, "y0_quant": [72, 85], "y1": [53, 60, 63, 72, 85], "y1_cvar": 72, "y1_quant": [72, 85], "y_": [16, 24, 25, 54, 57, 58, 59, 60, 63, 80, 90, 99, 101, 102, 104, 105, 112, 114, 116, 118], "y_0": [20, 22, 26, 41, 114, 117], "y_1": [20, 22, 26, 41, 114, 117], "y_col": [4, 5, 6, 24, 51, 52, 54, 55, 56, 57, 60, 61, 62, 63, 68, 70, 71, 73, 74, 75, 77, 80, 81, 82, 84, 86, 87, 88, 91, 92, 93, 94, 95, 98, 99, 100, 102, 104, 113, 114, 143, 145, 146], "y_df": [59, 84], "y_diff": 59, "y_i": [11, 12, 13, 15, 17, 18, 19, 52, 57, 58, 69, 72, 83, 84, 85, 86, 90, 92, 99, 101, 112], "y_label": [21, 24], "y_lower_quantil": [60, 63], "y_mean": [60, 63], "y_pred": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 78, 98], "y_train": 79, "y_true": [20, 22, 23, 29, 31, 32, 33, 34, 35, 37, 38, 39, 43, 47, 78, 98], "y_upper_quantil": [60, 63], "ya": 144, "yasui": 144, "yata": 144, "yaxis_titl": [70, 71, 73, 86, 89], "year": [61, 141], "yerr": [59, 66, 67, 74, 75, 79, 81, 83, 86], "yet": [54, 60, 62, 63, 65, 99, 102, 104, 105], "yggvpl": 80, "yield": 99, "yintercept": 55, "ylab": [52, 54, 55], "ylabel": [59, 60, 61, 63, 66, 67, 70, 71, 72, 74, 75, 79, 81, 82, 85], "ylim": 81, "ymax": 55, "ymin": 55, "yname": 53, "york": 144, "you": [42, 43, 46, 47, 51, 52, 59, 60, 61, 62, 63, 66, 68, 75, 80, 88, 99, 141, 142, 146], "your": [78, 142], "ython": 141, "yukun": 144, "yusuk": 144, "yuya": 144, "yy": 69, "z": [4, 5, 6, 9, 10, 12, 14, 15, 16, 19, 25, 26, 32, 34, 37, 38, 51, 54, 55, 57, 60, 63, 68, 70, 71, 73, 80, 81, 85, 87, 89, 90, 97, 99, 108, 110, 114, 121, 123, 125, 128, 129, 145], "z1": [6, 24, 38, 60, 63, 94, 95, 99, 100, 101, 102, 104], "z2": [6, 24, 60, 63, 94, 95, 99, 100, 101, 102, 104], "z3": [6, 24, 60, 63, 94, 95, 99, 100, 101, 102, 104], "z4": [6, 24, 60, 63, 94, 95, 99, 100, 101, 102, 104], "z_": [16, 54, 80], "z_1": [9, 10, 14, 60, 63], "z_2": [9, 10, 14], "z_3": [9, 10, 14], "z_4": [9, 10, 14, 60, 63], "z_5": 9, "z_col": [4, 5, 6, 32, 34, 38, 51, 54, 55, 57, 68, 80, 81, 82, 87, 90, 93, 95, 97, 99, 145], "z_i": [15, 19, 57, 85, 90, 99], "z_j": [9, 10, 14, 25, 26], "z_true": 85, "zadik": 144, "zaxis_titl": [70, 71, 73], "zero": [22, 26, 41, 58, 59, 60, 63, 66, 72, 77, 78, 84, 85, 88, 89, 99, 114, 116, 118, 129, 130, 133, 135], "zeros_lik": 85, "zeta": [32, 38, 39, 55, 81, 97, 99, 108, 110, 111, 143], "zeta_": [16, 54, 80], "zeta_0": [16, 54, 80], "zeta_i": [13, 15, 17, 52, 69, 92], "zeta_j": 129, "zhang": 144, "zhao": [9, 10, 14, 20, 22, 23, 26, 53, 58, 60, 63, 99, 101, 105, 144], "zimmert": [58, 99, 105, 144], "zip": [70, 71], "zorder": [66, 67], "\u03c4_x0": 83, "\u03c4_x1": 83, "\u2139": 52}, "titles": ["API Reference", "<span class=\"section-number\">1. </span>DoubleML Data Class", "<span class=\"section-number\">3. </span>Datasets", "<span class=\"section-number\">2. </span>DoubleML Models", "<span class=\"section-number\">1.2. </span>doubleml.data.DoubleMLClusterData", "<span class=\"section-number\">1.1. </span>doubleml.data.DoubleMLData", "<span class=\"section-number\">1.3. </span>doubleml.data.DoubleMLPanelData", "<span class=\"section-number\">3.1.1. </span>doubleml.datasets.fetch_401K", "<span class=\"section-number\">3.1.2. </span>doubleml.datasets.fetch_bonus", "<span class=\"section-number\">3.2.9. </span>doubleml.datasets.make_confounded_irm_data", "<span class=\"section-number\">3.2.8. </span>doubleml.datasets.make_confounded_plr_data", "<span class=\"section-number\">3.2.10. </span>doubleml.datasets.make_heterogeneous_data", "<span class=\"section-number\">3.2.4. </span>doubleml.datasets.make_iivm_data", "<span class=\"section-number\">3.2.3. </span>doubleml.datasets.make_irm_data", "<span class=\"section-number\">3.2.11. </span>doubleml.datasets.make_irm_data_discrete_treatments", "<span class=\"section-number\">3.2.2. </span>doubleml.datasets.make_pliv_CHS2015", "<span class=\"section-number\">3.2.6. </span>doubleml.datasets.make_pliv_multiway_cluster_CKMS2021", "<span class=\"section-number\">3.2.1. </span>doubleml.datasets.make_plr_CCDDHNR2018", "<span class=\"section-number\">3.2.5. </span>doubleml.datasets.make_plr_turrell2018", "<span class=\"section-number\">3.2.7. </span>doubleml.datasets.make_ssm_data", "<span class=\"section-number\">2.3.4. </span>doubleml.did.DoubleMLDID", "<span class=\"section-number\">2.3.2. </span>doubleml.did.DoubleMLDIDAggregation", "<span class=\"section-number\">2.3.3. </span>doubleml.did.DoubleMLDIDBinary", "<span class=\"section-number\">2.3.5. </span>doubleml.did.DoubleMLDIDCS", "<span class=\"section-number\">2.3.1. </span>doubleml.did.DoubleMLDIDMulti", "<span class=\"section-number\">3.2.13. </span>doubleml.did.datasets.make_did_CS2021", "<span class=\"section-number\">3.2.12. </span>doubleml.did.datasets.make_did_SZ2020", "<span class=\"section-number\">5.1. </span>doubleml.double_ml_score_mixins.LinearScoreMixin", "<span class=\"section-number\">5.2. </span>doubleml.double_ml_score_mixins.NonLinearScoreMixin", "<span class=\"section-number\">2.2.2. </span>doubleml.irm.DoubleMLAPO", "<span class=\"section-number\">2.2.3. </span>doubleml.irm.DoubleMLAPOS", "<span class=\"section-number\">2.2.7. </span>doubleml.irm.DoubleMLCVAR", "<span class=\"section-number\">2.2.4. </span>doubleml.irm.DoubleMLIIVM", "<span class=\"section-number\">2.2.1. </span>doubleml.irm.DoubleMLIRM", "<span class=\"section-number\">2.2.6. </span>doubleml.irm.DoubleMLLPQ", "<span class=\"section-number\">2.2.5. </span>doubleml.irm.DoubleMLPQ", "<span class=\"section-number\">2.2.8. </span>doubleml.irm.DoubleMLQTE", "<span class=\"section-number\">2.2.9. </span>doubleml.irm.DoubleMLSSM", "<span class=\"section-number\">2.1.2. </span>doubleml.plm.DoubleMLPLIV", "<span class=\"section-number\">2.1.1. </span>doubleml.plm.DoubleMLPLR", "<span class=\"section-number\">2.4.1. </span>doubleml.rdd.RDFlex", "<span class=\"section-number\">3.2.14. </span>doubleml.rdd.datasets.make_simple_rdd_data", "<span class=\"section-number\">4.1.2. </span>doubleml.utils.DMLDummyClassifier", "<span class=\"section-number\">4.1.1. </span>doubleml.utils.DMLDummyRegressor", "<span class=\"section-number\">4.1.3. </span>doubleml.utils.DoubleMLBLP", "<span class=\"section-number\">4.1.4. </span>doubleml.utils.DoubleMLPolicyTree", "<span class=\"section-number\">4.1.6. </span>doubleml.utils.GlobalClassifier", "<span class=\"section-number\">4.1.5. </span>doubleml.utils.GlobalRegressor", "<span class=\"section-number\">4.2.1. </span>doubleml.utils.gain_statistics", "<span class=\"section-number\">5. </span>Score Mixin Classes for DoubleML Models", "<span class=\"section-number\">4. </span>Utility Classes and Functions", "R: Basic Instrumental Variables Calculation", "R: Basics of Double Machine Learning", "R: DoubleML for Difference-in-Differences", "R: Cluster Robust Double Machine Learning", "R: Impact of 401(k) on Financial Wealth", "R: Ensemble Learners and More with <code class=\"docutils literal notranslate\"><span class=\"pre\">mlr3pipelines</span></code>", "R: Sample Selection Models", "Python: Difference-in-Differences", "Python: Difference-in-Differences Pre-Testing", "Python: Panel Data with Multiple Time Periods", "Python: Real-Data Example for Multi-Period Difference-in-Differences", "Python: Panel Data Introduction", "Python: Repeated Cross-Sectional Data with Multiple Time Periods", "DML: Bonus Data", "Examples", "Python: Causal Machine Learning with TabPFN", "Python: Average Potential Outcome (APO) Models", "Python: Basic Instrumental Variables calculation", "Python: Basics of Double Machine Learning", "Python: Conditional Average Treatment Effects (CATEs) for IRM models", "Python: Conditional Average Treatment Effects (CATEs) for PLR models", "Python: Conditional Value at Risk of potential outcomes", "Python: First Stage and Causal Estimation", "Python: Group Average Treatment Effects (GATEs) for IRM models", "Python: Group Average Treatment Effects (GATEs) for PLR models", "Python: GATE Sensitivity Analysis", "Python: IRM and APO Model Comparison", "Python: Choice of learners", "DoubleML meets FLAML - How to tune learners automatically within <code class=\"docutils literal notranslate\"><span class=\"pre\">DoubleML</span></code>", "Python: Cluster Robust Double Machine Learning", "Python: Impact of 401(k) on Financial Wealth", "Python: Impact of 401(k) on Financial Wealth (Quantile Effects)", "Python: PLM and IRM for Multiple Treatments", "Python: Policy Learning with Trees", "Python: Potential Quantiles and Quantile Treatment Effects", "Flexible Covariate Adjustment in Regression Discontinuity Designs (RDD)", "Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments", "Python: Sensitivity Analysis", "Example: Sensitivity Analysis for Causal ML", "Python: Sample Selection Models", "<span class=\"section-number\">6. </span>Double machine learning algorithms", "<span class=\"section-number\">1. </span>The basics of double/debiased machine learning", "DoubleMLData from dataframes", "&lt;no title&gt;", "<span class=\"section-number\">2. </span>Data Backend", "User Guide", "<span class=\"section-number\">4. </span>Heterogeneous treatment effects", "<span class=\"section-number\">7. </span>Learners, hyperparameters and hyperparameter tuning", "<span class=\"section-number\">3. </span>Models", "&lt;no title&gt;", "Panel Data", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">9. </span>Sample-splitting, cross-fitting and repeated cross-fitting", "<span class=\"section-number\">5. </span>Score functions", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "<span class=\"section-number\">8. </span>Variance estimation and confidence intervals", "<span class=\"section-number\">10. </span>Sensitivity analysis", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "DoubleML", "Installing DoubleML", "Getting Started", "Double Machine Learning Literature", "Release Notes", "DoubleML Workflow"], "titleterms": {"": 79, "0": 146, "1": [79, 89, 146], "2": [79, 89, 146], "2011": 89, "2023": 89, "3": [79, 89, 146], "4": [89, 146], "401": [55, 81, 82, 88], "5": [89, 146], "6": 146, "7": 146, "95": 79, "A": [54, 80], "ATE": [57, 76, 83, 90], "No": [54, 80], "One": [54, 70, 71, 80], "That": 87, "The": [55, 81, 83, 92, 143], "acknowledg": [53, 141], "acycl": [51, 68], "addit": 83, "adjust": [60, 63, 86], "advanc": [86, 98, 129], "aggreg": [60, 61, 62, 63, 99], "al": 89, "algorithm": [91, 130, 141, 143], "all": [60, 63], "altern": 114, "analysi": [60, 63, 67, 76, 77, 88, 89, 130, 146], "anticip": [60, 63], "api": [0, 79], "apo": [67, 77, 99, 114, 130], "applic": [54, 80, 88], "approach": [52, 69, 78, 92], "ar": 87, "arah": 89, "arbitrari": 83, "archiv": 65, "arrai": [93, 95], "asset": [55, 81], "assumpt": 89, "att": [58, 60, 61, 62, 63], "augment": 83, "automat": 79, "automl": 79, "averag": [55, 66, 67, 70, 71, 74, 75, 77, 81, 97, 99, 114, 130], "backend": [54, 55, 80, 81, 95, 143, 146], "band": 129, "base": [56, 60, 63], "basic": [51, 52, 60, 63, 68, 69, 92], "benchmark": [88, 89, 130], "bia": [52, 69, 92], "binari": [99, 114], "bonu": 64, "bootstrap": 129, "build": 142, "calcul": [51, 68], "call": 79, "callabl": 114, "case": 65, "cate": [70, 71, 83, 97], "causal": [61, 64, 66, 67, 73, 89, 114, 143, 146], "chernozhukov": 89, "choic": 78, "citat": 141, "class": [1, 49, 50, 54, 80], "cluster": [54, 80], "code": 141, "coeffici": 79, "combin": [60, 63, 73], "compar": [78, 79], "comparison": [53, 66, 77, 79], "comput": [78, 79], "conclus": [79, 89], "conda": 142, "condit": [61, 70, 71, 72, 82, 97, 114], "confid": [79, 87, 129], "construct": 98, "contrast": 67, "control": [60, 63], "covari": [60, 63, 86], "coverag": [58, 73], "cran": 142, "creat": [66, 79], "cross": [54, 58, 63, 80, 99, 101, 113, 114, 130, 143], "custom": [78, 79], "cvar": [72, 82, 97, 114], "dag": [51, 68], "data": [1, 4, 5, 6, 51, 52, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 88, 89, 90, 92, 95, 99, 101, 114, 130, 143, 146], "datafram": [93, 95], "dataset": [2, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 25, 26, 41, 64], "debias": [52, 69, 92, 143], "default": 79, "defin": [54, 80], "demo": 53, "depend": 142, "descript": [60, 63], "design": [86, 99], "detail": [53, 60, 61, 62, 63, 99], "develop": 142, "dgp": [52, 66, 67, 69], "did": [3, 20, 21, 22, 23, 24, 25, 26, 53, 99], "differ": [53, 58, 59, 61, 65, 78, 99, 114, 129, 130], "dimension": [70, 71], "direct": [51, 68], "disclaim": 89, "discontinu": [86, 99], "distribut": [57, 90], "dml": [54, 64, 80, 113, 143, 146], "dml1": 91, "dml2": 91, "dmldummyclassifi": 42, "dmldummyregressor": 43, "doubl": [52, 54, 69, 80, 91, 92, 141, 143, 144], "double_ml_score_mixin": [27, 28], "doubleml": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 53, 55, 56, 66, 68, 79, 81, 88, 89, 129, 141, 142, 146], "doublemlapo": [29, 30], "doublemlblp": 44, "doublemlclusterdata": [4, 54, 80], "doublemlcvar": 31, "doublemldata": [5, 55, 66, 81, 93, 95, 143], "doublemldid": 20, "doublemldidaggreg": 21, "doublemldidbinari": 22, "doublemldidc": 23, "doublemldidmulti": 24, "doublemliivm": 32, "doublemlirm": 33, "doublemllpq": 34, "doublemlpaneldata": [6, 60, 63, 95], "doublemlpliv": [38, 54, 80], "doublemlplr": 39, "doublemlpolicytre": 45, "doublemlpq": 35, "doublemlqt": 36, "doublemlssm": 37, "effect": [55, 60, 61, 62, 63, 65, 66, 70, 71, 72, 74, 75, 77, 81, 82, 83, 85, 88, 89, 97, 99], "elig": [55, 81], "empir": 73, "ensembl": [56, 86], "error": [54, 80], "estim": [51, 55, 57, 58, 60, 61, 62, 63, 64, 66, 68, 73, 76, 79, 81, 82, 83, 85, 88, 89, 90, 113, 114, 129, 143, 146], "et": 89, "evalu": [66, 78, 79, 98], "event": [60, 61, 62, 63], "exampl": [53, 54, 61, 65, 70, 71, 80, 88, 89], "exploit": [53, 56], "extern": [98, 113], "featur": [56, 141], "fetch_401k": 7, "fetch_bonu": 8, "figur": 83, "file": 142, "final": 53, "financi": [55, 81, 82], "first": 73, "fit": [54, 79, 80, 113, 143], "flaml": 79, "flexibl": 86, "fold": [79, 113], "forest": 64, "formul": [89, 146], "from": [53, 56, 93, 95, 142], "full": 79, "function": [50, 53, 54, 80, 114, 143], "fuzzi": [86, 99], "gain_statist": 48, "gate": [74, 75, 76, 97], "gatet": 76, "gener": [2, 52, 65, 66, 67, 69, 79, 86, 92, 130], "get": 143, "github": 142, "global": 86, "globalclassifi": 46, "globalregressor": 47, "graph": [51, 68], "group": [60, 62, 63, 74, 75, 97], "guid": 96, "helper": [54, 80], "heterogen": [65, 77, 83, 97], "how": [56, 79], "hyperparamet": [77, 98], "identif": 89, "iivm": [55, 81, 99, 114], "impact": [55, 81, 82], "implement": [91, 99, 114, 130], "import": 66, "induc": [52, 69, 92], "infer": [129, 146], "initi": [54, 79, 80], "insight": 66, "instal": 142, "instrument": [51, 68, 87], "integr": 53, "interact": [55, 74, 81, 84, 99, 114, 130], "interv": [79, 87, 129], "introduct": 62, "invers": 83, "irm": [3, 29, 30, 31, 32, 33, 34, 35, 36, 37, 55, 64, 70, 74, 77, 81, 83, 84, 88, 97, 99, 114, 130], "iv": [51, 55, 68, 81, 99, 114], "k": [55, 81, 82, 88, 113], "kei": 66, "lambda": 73, "lasso": [64, 73], "latest": 142, "lear": [54, 80], "learn": [52, 54, 66, 69, 80, 84, 91, 92, 97, 141, 143, 144], "learner": [56, 64, 77, 78, 79, 86, 98, 143], "less": 79, "level": 99, "linear": [55, 60, 63, 75, 81, 83, 86, 99, 114, 130], "linearscoremixin": 27, "literatur": 144, "load": [54, 64, 80, 89], "loader": 2, "local": [55, 81, 82, 85, 86, 114], "loss": 73, "lpq": [85, 114], "lqte": [82, 85], "m": 113, "machin": [52, 54, 66, 69, 80, 91, 92, 141, 143, 144], "main": 141, "mainten": 141, "make_confounded_irm_data": 9, "make_confounded_plr_data": 10, "make_did_cs2021": 25, "make_did_sz2020": 26, "make_heterogeneous_data": 11, "make_iivm_data": 12, "make_irm_data": 13, "make_irm_data_discrete_treat": 14, "make_pliv_chs2015": 15, "make_pliv_multiway_cluster_ckms2021": 16, "make_plr_ccddhnr2018": 17, "make_plr_turrell2018": 18, "make_simple_rdd_data": 41, "make_ssm_data": 19, "mar": [57, 90], "market": [54, 80], "matric": [93, 95], "meet": 79, "method": [66, 79, 146], "metric": [78, 79], "minimum": 98, "miss": [57, 90], "missing": [99, 114], "mixin": 49, "ml": [52, 53, 69, 89, 92, 146], "mlr3": 56, "mlr3extralearn": 56, "mlr3learner": 56, "mlr3pipelin": 56, "model": [3, 49, 55, 57, 64, 66, 67, 70, 71, 74, 75, 77, 79, 81, 83, 84, 87, 89, 90, 97, 99, 113, 114, 129, 130, 143, 146], "modul": 64, "more": 56, "motiv": [54, 80], "multi": 61, "multipl": [60, 63, 67, 83, 99], "multipli": 129, "naiv": [51, 68], "net": [55, 81], "neyman": [114, 143], "nonignor": [57, 90, 99, 114], "nonlinearscoremixin": 28, "nonrespons": [57, 90, 99, 114], "note": 145, "nuisanc": [79, 143], "object": [54, 66, 80, 88], "option": 142, "orthogon": [52, 69, 92, 114, 143], "out": [52, 69, 92], "outcom": [57, 58, 66, 67, 72, 90, 97, 99, 114, 130], "over": 129, "overcom": [52, 69, 92], "overfit": [52, 69, 92], "overlap": 83, "packag": [53, 55, 81, 142], "panel": [58, 60, 62, 99, 101, 114, 130], "parallel": 61, "paramet": [56, 64, 79, 99, 114], "partial": [52, 55, 69, 75, 81, 83, 92, 99, 114, 130], "particip": [55, 81], "partit": 113, "penalti": 73, "perform": [53, 66, 83], "period": [60, 61, 63, 99, 114, 130], "pip": 142, "pipelin": 98, "pliv": [99, 114], "plm": [3, 38, 39, 83, 99, 114, 130], "plot": [54, 79, 80], "plr": [55, 64, 71, 75, 81, 97, 99, 114, 130], "polici": [84, 97], "potenti": [66, 67, 72, 82, 85, 97, 99, 114, 130], "pq": [85, 97, 114], "pre": 59, "predict": [53, 98], "preprocess": 56, "problem": 146, "process": [52, 54, 66, 67, 69, 80, 92], "product": [54, 80], "propens": 83, "provid": 113, "python": [58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 87, 88, 90, 98, 142], "qte": [85, 97], "qualiti": 73, "quantil": [82, 85, 97, 114], "question": 61, "r": [51, 52, 53, 54, 55, 56, 57, 65, 98, 142], "random": [57, 64, 90, 99, 114], "rank": 83, "rdd": [3, 40, 41, 86, 99], "rdflex": 40, "real": [54, 61, 80], "refer": [0, 51, 53, 54, 56, 68, 73, 78, 79, 80, 83, 87, 89, 92, 98, 113, 129, 141, 143], "regress": [55, 74, 75, 81, 84, 86, 99, 114, 130], "regular": [52, 69, 92], "releas": [142, 145], "remark": 53, "remov": [52, 69, 92], "repeat": [58, 63, 99, 101, 113, 114, 130], "repetit": 113, "requir": 98, "research": 61, "respect": [54, 80], "result": [54, 55, 80, 81, 83], "risk": [72, 82, 97, 114], "robust": [54, 80, 87], "run": 87, "sampl": [52, 57, 69, 79, 90, 92, 99, 113, 114], "sandbox": 65, "score": [49, 52, 69, 83, 92, 114, 143], "section": [58, 63, 99, 101, 114, 130], "select": [57, 60, 63, 90, 99, 114], "sensit": [60, 63, 67, 76, 77, 88, 89, 130, 146], "set": [56, 98], "setup": 66, "sharp": [86, 99], "simpl": [52, 69, 92], "simul": [51, 54, 58, 68, 80, 87, 88], "simultan": 129, "singl": 67, "small": 87, "sourc": [141, 142], "special": 95, "specif": [130, 146], "specifi": [64, 98, 114], "split": [52, 69, 92, 113], "ssm": 99, "stack": 86, "stage": 73, "standard": [54, 78, 80], "start": 143, "step": 79, "structur": 66, "studi": [60, 61, 62, 63, 65], "summari": [55, 66, 79, 81, 83], "tabpfn": 66, "takeawai": 66, "test": 59, "theori": 130, "time": [60, 62, 63, 78, 79], "train": 79, "treat": 77, "treatment": [55, 66, 70, 71, 72, 74, 75, 77, 81, 82, 83, 85, 97, 99, 114, 130], "tree": [84, 97], "trend": 61, "tune": [56, 79, 98], "two": [54, 70, 71, 80, 99, 114, 130], "type": 95, "uncondit": 61, "under": [57, 83, 90], "univers": [60, 63], "untun": 79, "up": 56, "us": [51, 53, 56, 64, 68, 79, 98], "user": 96, "util": [42, 43, 44, 45, 46, 47, 48, 50], "v": 73, "valid": 129, "valu": [72, 82, 97, 114], "vanderweel": 89, "variabl": [51, 68, 87], "varianc": 129, "version": 142, "via": 114, "visual": 66, "wai": [54, 80], "weak": 87, "wealth": [55, 81, 82], "weight": [83, 97], "when": 79, "whl": 142, "within": 79, "without": [86, 113], "workflow": 146, "xgboost": 79, "zero": [54, 80]}})